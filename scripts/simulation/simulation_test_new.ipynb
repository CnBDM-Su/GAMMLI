{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0630 22:58:42.768287 22800 deprecation.py:323] From C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "../..\\lvxnn\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.86 MB\n",
      "Memory usage after optimization is: 0.26 MB\n",
      "Decreased by 69.6%\n",
      "Memory usage of dataframe is 0.21 MB\n",
      "Memory usage after optimization is: 0.07 MB\n",
      "Decreased by 69.6%\n",
      "test cold start user: 0\n",
      "test cold start item: 3\n",
      "validation cold start user: 0\n",
      "validation cold start item: 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error,roc_auc_score,mean_absolute_error,log_loss\n",
    "import sys\n",
    "sys.path.append('../benchmark/')\n",
    "from lvxnn_test import lvxnn\n",
    "from xgb_test import xgb\n",
    "from svd_test import svd\n",
    "from deepfm_fm_test import deepfm_fm\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from lvxnn.LVXNN import LV_XNN\n",
    "from lvxnn.DataReader import data_initialize\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "random_state = 0\n",
    "data= pd.read_csv('../simulation/data/sim_0.9.csv')\n",
    "train , test = train_test_split(data,test_size=0.2 ,random_state=0)\n",
    "task_type = \"Regression\"\n",
    "\n",
    "meta_info = OrderedDict()\n",
    "\n",
    "meta_info['uf_1']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_2']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_3']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_4']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_5']={'type': 'continues','source':'user'}\n",
    "meta_info['if_1']={'type': 'continues','source':'item'}\n",
    "meta_info['if_2']={'type': 'continues','source':'item'}\n",
    "meta_info['if_3']={'type': 'continues','source':'item'}\n",
    "meta_info['if_4']={'type': 'continues','source':'item'}\n",
    "meta_info['if_5']={'type': 'continues','source':'item'}\n",
    "meta_info['user_id']={\"type\":\"id\",'source':'user'}\n",
    "meta_info['item_id']={\"type\":\"id\",'source':'item'}\n",
    "meta_info['target']={\"type\":\"target\",'source':''}\n",
    "\n",
    "lx_params = {\n",
    "        \"main_effect_epochs\":300,\n",
    "        \"interaction_epochs\" : 200 ,\n",
    "        \"tuning_epochs\" : 50 , \n",
    "        \"mf_training_iters\": 200,\n",
    "        \"u_group_num\":30,\n",
    "        \"i_group_num\":50\n",
    "    }\n",
    "\n",
    "tr_x, tr_Xi, tr_y, tr_idx, te_x, te_Xi, te_y, val_x, val_Xi, val_y, val_idx, meta_info, model_info = data_initialize(train,test,meta_info,task_type ,'warm', random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.02581, val loss: 4.15569\n",
      "Main effects training epoch: 2, train loss: 3.86663, val loss: 4.00300\n",
      "Main effects training epoch: 3, train loss: 3.66700, val loss: 3.81314\n",
      "Main effects training epoch: 4, train loss: 3.48207, val loss: 3.63384\n",
      "Main effects training epoch: 5, train loss: 3.38250, val loss: 3.50337\n",
      "Main effects training epoch: 6, train loss: 3.28135, val loss: 3.35840\n",
      "Main effects training epoch: 7, train loss: 3.28186, val loss: 3.35070\n",
      "Main effects training epoch: 8, train loss: 3.27336, val loss: 3.36040\n",
      "Main effects training epoch: 9, train loss: 3.25283, val loss: 3.34172\n",
      "Main effects training epoch: 10, train loss: 3.15250, val loss: 3.23796\n",
      "Main effects training epoch: 11, train loss: 3.05938, val loss: 3.14667\n",
      "Main effects training epoch: 12, train loss: 3.05048, val loss: 3.13776\n",
      "Main effects training epoch: 13, train loss: 2.97453, val loss: 3.06166\n",
      "Main effects training epoch: 14, train loss: 2.90463, val loss: 2.99088\n",
      "Main effects training epoch: 15, train loss: 2.85090, val loss: 2.93330\n",
      "Main effects training epoch: 16, train loss: 2.80705, val loss: 2.88247\n",
      "Main effects training epoch: 17, train loss: 2.72197, val loss: 2.79421\n",
      "Main effects training epoch: 18, train loss: 2.70970, val loss: 2.78070\n",
      "Main effects training epoch: 19, train loss: 2.64818, val loss: 2.71541\n",
      "Main effects training epoch: 20, train loss: 2.60938, val loss: 2.68330\n",
      "Main effects training epoch: 21, train loss: 2.57306, val loss: 2.65275\n",
      "Main effects training epoch: 22, train loss: 2.51936, val loss: 2.59316\n",
      "Main effects training epoch: 23, train loss: 2.51905, val loss: 2.60367\n",
      "Main effects training epoch: 24, train loss: 2.44088, val loss: 2.52255\n",
      "Main effects training epoch: 25, train loss: 2.41554, val loss: 2.50227\n",
      "Main effects training epoch: 26, train loss: 2.38866, val loss: 2.46996\n",
      "Main effects training epoch: 27, train loss: 2.36765, val loss: 2.45355\n",
      "Main effects training epoch: 28, train loss: 2.33153, val loss: 2.41112\n",
      "Main effects training epoch: 29, train loss: 2.28303, val loss: 2.36249\n",
      "Main effects training epoch: 30, train loss: 2.26930, val loss: 2.35093\n",
      "Main effects training epoch: 31, train loss: 2.21173, val loss: 2.29201\n",
      "Main effects training epoch: 32, train loss: 2.24060, val loss: 2.32154\n",
      "Main effects training epoch: 33, train loss: 2.17058, val loss: 2.25252\n",
      "Main effects training epoch: 34, train loss: 2.18446, val loss: 2.25689\n",
      "Main effects training epoch: 35, train loss: 2.16814, val loss: 2.24363\n",
      "Main effects training epoch: 36, train loss: 2.13803, val loss: 2.21848\n",
      "Main effects training epoch: 37, train loss: 2.07828, val loss: 2.15467\n",
      "Main effects training epoch: 38, train loss: 2.10415, val loss: 2.17893\n",
      "Main effects training epoch: 39, train loss: 2.06347, val loss: 2.14620\n",
      "Main effects training epoch: 40, train loss: 2.05131, val loss: 2.12958\n",
      "Main effects training epoch: 41, train loss: 2.04399, val loss: 2.11796\n",
      "Main effects training epoch: 42, train loss: 1.99832, val loss: 2.07447\n",
      "Main effects training epoch: 43, train loss: 1.99686, val loss: 2.07381\n",
      "Main effects training epoch: 44, train loss: 1.98219, val loss: 2.05986\n",
      "Main effects training epoch: 45, train loss: 1.98538, val loss: 2.06555\n",
      "Main effects training epoch: 46, train loss: 1.93199, val loss: 2.00777\n",
      "Main effects training epoch: 47, train loss: 1.92633, val loss: 2.00292\n",
      "Main effects training epoch: 48, train loss: 1.93781, val loss: 2.01180\n",
      "Main effects training epoch: 49, train loss: 1.90558, val loss: 1.97629\n",
      "Main effects training epoch: 50, train loss: 1.89436, val loss: 1.97301\n",
      "Main effects training epoch: 51, train loss: 1.88707, val loss: 1.95539\n",
      "Main effects training epoch: 52, train loss: 1.87418, val loss: 1.95203\n",
      "Main effects training epoch: 53, train loss: 1.88074, val loss: 1.95203\n",
      "Main effects training epoch: 54, train loss: 1.84766, val loss: 1.91670\n",
      "Main effects training epoch: 55, train loss: 1.86362, val loss: 1.93316\n",
      "Main effects training epoch: 56, train loss: 1.84504, val loss: 1.91578\n",
      "Main effects training epoch: 57, train loss: 1.83871, val loss: 1.90598\n",
      "Main effects training epoch: 58, train loss: 1.83635, val loss: 1.90794\n",
      "Main effects training epoch: 59, train loss: 1.81987, val loss: 1.88301\n",
      "Main effects training epoch: 60, train loss: 1.81411, val loss: 1.87753\n",
      "Main effects training epoch: 61, train loss: 1.79435, val loss: 1.85428\n",
      "Main effects training epoch: 62, train loss: 1.80737, val loss: 1.87724\n",
      "Main effects training epoch: 63, train loss: 1.78433, val loss: 1.84476\n",
      "Main effects training epoch: 64, train loss: 1.77484, val loss: 1.83380\n",
      "Main effects training epoch: 65, train loss: 1.76469, val loss: 1.82684\n",
      "Main effects training epoch: 66, train loss: 1.75610, val loss: 1.81696\n",
      "Main effects training epoch: 67, train loss: 1.74015, val loss: 1.80136\n",
      "Main effects training epoch: 68, train loss: 1.74855, val loss: 1.80547\n",
      "Main effects training epoch: 69, train loss: 1.73357, val loss: 1.79520\n",
      "Main effects training epoch: 70, train loss: 1.74203, val loss: 1.79823\n",
      "Main effects training epoch: 71, train loss: 1.73003, val loss: 1.79278\n",
      "Main effects training epoch: 72, train loss: 1.72470, val loss: 1.77914\n",
      "Main effects training epoch: 73, train loss: 1.72740, val loss: 1.78702\n",
      "Main effects training epoch: 74, train loss: 1.71997, val loss: 1.77884\n",
      "Main effects training epoch: 75, train loss: 1.72546, val loss: 1.77572\n",
      "Main effects training epoch: 76, train loss: 1.71292, val loss: 1.77190\n",
      "Main effects training epoch: 77, train loss: 1.71444, val loss: 1.77190\n",
      "Main effects training epoch: 78, train loss: 1.71019, val loss: 1.76935\n",
      "Main effects training epoch: 79, train loss: 1.70637, val loss: 1.75924\n",
      "Main effects training epoch: 80, train loss: 1.70559, val loss: 1.76570\n",
      "Main effects training epoch: 81, train loss: 1.70424, val loss: 1.76163\n",
      "Main effects training epoch: 82, train loss: 1.69625, val loss: 1.75276\n",
      "Main effects training epoch: 83, train loss: 1.70362, val loss: 1.76509\n",
      "Main effects training epoch: 84, train loss: 1.69010, val loss: 1.74551\n",
      "Main effects training epoch: 85, train loss: 1.69095, val loss: 1.75028\n",
      "Main effects training epoch: 86, train loss: 1.68340, val loss: 1.74746\n",
      "Main effects training epoch: 87, train loss: 1.68064, val loss: 1.74529\n",
      "Main effects training epoch: 88, train loss: 1.67380, val loss: 1.74022\n",
      "Main effects training epoch: 89, train loss: 1.67360, val loss: 1.74665\n",
      "Main effects training epoch: 90, train loss: 1.65844, val loss: 1.73033\n",
      "Main effects training epoch: 91, train loss: 1.65355, val loss: 1.72703\n",
      "Main effects training epoch: 92, train loss: 1.64214, val loss: 1.71970\n",
      "Main effects training epoch: 93, train loss: 1.64936, val loss: 1.73062\n",
      "Main effects training epoch: 94, train loss: 1.63636, val loss: 1.71888\n",
      "Main effects training epoch: 95, train loss: 1.63488, val loss: 1.72091\n",
      "Main effects training epoch: 96, train loss: 1.63066, val loss: 1.70495\n",
      "Main effects training epoch: 97, train loss: 1.63836, val loss: 1.72648\n",
      "Main effects training epoch: 98, train loss: 1.63448, val loss: 1.71359\n",
      "Main effects training epoch: 99, train loss: 1.62205, val loss: 1.71122\n",
      "Main effects training epoch: 100, train loss: 1.62696, val loss: 1.71339\n",
      "Main effects training epoch: 101, train loss: 1.62468, val loss: 1.70969\n",
      "Main effects training epoch: 102, train loss: 1.62361, val loss: 1.71057\n",
      "Main effects training epoch: 103, train loss: 1.61811, val loss: 1.70932\n",
      "Main effects training epoch: 104, train loss: 1.61437, val loss: 1.69638\n",
      "Main effects training epoch: 105, train loss: 1.61687, val loss: 1.70838\n",
      "Main effects training epoch: 106, train loss: 1.61267, val loss: 1.69917\n",
      "Main effects training epoch: 107, train loss: 1.60933, val loss: 1.69456\n",
      "Main effects training epoch: 108, train loss: 1.60899, val loss: 1.69789\n",
      "Main effects training epoch: 109, train loss: 1.61071, val loss: 1.70213\n",
      "Main effects training epoch: 110, train loss: 1.60834, val loss: 1.69356\n",
      "Main effects training epoch: 111, train loss: 1.60903, val loss: 1.69018\n",
      "Main effects training epoch: 112, train loss: 1.61192, val loss: 1.71417\n",
      "Main effects training epoch: 113, train loss: 1.60321, val loss: 1.68930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 114, train loss: 1.60681, val loss: 1.69810\n",
      "Main effects training epoch: 115, train loss: 1.61161, val loss: 1.69664\n",
      "Main effects training epoch: 116, train loss: 1.60462, val loss: 1.69884\n",
      "Main effects training epoch: 117, train loss: 1.60153, val loss: 1.68634\n",
      "Main effects training epoch: 118, train loss: 1.59715, val loss: 1.69219\n",
      "Main effects training epoch: 119, train loss: 1.59567, val loss: 1.68591\n",
      "Main effects training epoch: 120, train loss: 1.59747, val loss: 1.68979\n",
      "Main effects training epoch: 121, train loss: 1.59275, val loss: 1.68562\n",
      "Main effects training epoch: 122, train loss: 1.58952, val loss: 1.68187\n",
      "Main effects training epoch: 123, train loss: 1.59299, val loss: 1.69392\n",
      "Main effects training epoch: 124, train loss: 1.58823, val loss: 1.67589\n",
      "Main effects training epoch: 125, train loss: 1.58752, val loss: 1.67897\n",
      "Main effects training epoch: 126, train loss: 1.58643, val loss: 1.67941\n",
      "Main effects training epoch: 127, train loss: 1.58719, val loss: 1.68306\n",
      "Main effects training epoch: 128, train loss: 1.58105, val loss: 1.67735\n",
      "Main effects training epoch: 129, train loss: 1.58516, val loss: 1.67262\n",
      "Main effects training epoch: 130, train loss: 1.57827, val loss: 1.67244\n",
      "Main effects training epoch: 131, train loss: 1.57854, val loss: 1.66952\n",
      "Main effects training epoch: 132, train loss: 1.58368, val loss: 1.67841\n",
      "Main effects training epoch: 133, train loss: 1.57787, val loss: 1.66421\n",
      "Main effects training epoch: 134, train loss: 1.57550, val loss: 1.67620\n",
      "Main effects training epoch: 135, train loss: 1.57558, val loss: 1.66555\n",
      "Main effects training epoch: 136, train loss: 1.57565, val loss: 1.66808\n",
      "Main effects training epoch: 137, train loss: 1.57215, val loss: 1.65359\n",
      "Main effects training epoch: 138, train loss: 1.57597, val loss: 1.67812\n",
      "Main effects training epoch: 139, train loss: 1.56594, val loss: 1.65260\n",
      "Main effects training epoch: 140, train loss: 1.56379, val loss: 1.65452\n",
      "Main effects training epoch: 141, train loss: 1.56809, val loss: 1.65819\n",
      "Main effects training epoch: 142, train loss: 1.56959, val loss: 1.67084\n",
      "Main effects training epoch: 143, train loss: 1.56391, val loss: 1.65611\n",
      "Main effects training epoch: 144, train loss: 1.56557, val loss: 1.66064\n",
      "Main effects training epoch: 145, train loss: 1.56139, val loss: 1.65606\n",
      "Main effects training epoch: 146, train loss: 1.55847, val loss: 1.65215\n",
      "Main effects training epoch: 147, train loss: 1.55328, val loss: 1.65161\n",
      "Main effects training epoch: 148, train loss: 1.55521, val loss: 1.65337\n",
      "Main effects training epoch: 149, train loss: 1.55394, val loss: 1.64666\n",
      "Main effects training epoch: 150, train loss: 1.55689, val loss: 1.64452\n",
      "Main effects training epoch: 151, train loss: 1.55483, val loss: 1.64596\n",
      "Main effects training epoch: 152, train loss: 1.55640, val loss: 1.65974\n",
      "Main effects training epoch: 153, train loss: 1.55465, val loss: 1.64418\n",
      "Main effects training epoch: 154, train loss: 1.54827, val loss: 1.64315\n",
      "Main effects training epoch: 155, train loss: 1.54425, val loss: 1.63498\n",
      "Main effects training epoch: 156, train loss: 1.54566, val loss: 1.63837\n",
      "Main effects training epoch: 157, train loss: 1.54784, val loss: 1.64139\n",
      "Main effects training epoch: 158, train loss: 1.55597, val loss: 1.64081\n",
      "Main effects training epoch: 159, train loss: 1.54733, val loss: 1.64121\n",
      "Main effects training epoch: 160, train loss: 1.54370, val loss: 1.64175\n",
      "Main effects training epoch: 161, train loss: 1.54700, val loss: 1.63689\n",
      "Main effects training epoch: 162, train loss: 1.53894, val loss: 1.64314\n",
      "Main effects training epoch: 163, train loss: 1.55071, val loss: 1.63990\n",
      "Main effects training epoch: 164, train loss: 1.53857, val loss: 1.62758\n",
      "Main effects training epoch: 165, train loss: 1.53582, val loss: 1.63364\n",
      "Main effects training epoch: 166, train loss: 1.53784, val loss: 1.62337\n",
      "Main effects training epoch: 167, train loss: 1.53668, val loss: 1.61628\n",
      "Main effects training epoch: 168, train loss: 1.54187, val loss: 1.64968\n",
      "Main effects training epoch: 169, train loss: 1.53233, val loss: 1.61626\n",
      "Main effects training epoch: 170, train loss: 1.53301, val loss: 1.62002\n",
      "Main effects training epoch: 171, train loss: 1.53462, val loss: 1.63271\n",
      "Main effects training epoch: 172, train loss: 1.53059, val loss: 1.62420\n",
      "Main effects training epoch: 173, train loss: 1.52675, val loss: 1.61587\n",
      "Main effects training epoch: 174, train loss: 1.52574, val loss: 1.61472\n",
      "Main effects training epoch: 175, train loss: 1.52522, val loss: 1.61609\n",
      "Main effects training epoch: 176, train loss: 1.52517, val loss: 1.61189\n",
      "Main effects training epoch: 177, train loss: 1.52300, val loss: 1.61130\n",
      "Main effects training epoch: 178, train loss: 1.52251, val loss: 1.61256\n",
      "Main effects training epoch: 179, train loss: 1.52604, val loss: 1.62370\n",
      "Main effects training epoch: 180, train loss: 1.51909, val loss: 1.60909\n",
      "Main effects training epoch: 181, train loss: 1.52000, val loss: 1.60844\n",
      "Main effects training epoch: 182, train loss: 1.52126, val loss: 1.60803\n",
      "Main effects training epoch: 183, train loss: 1.51478, val loss: 1.60948\n",
      "Main effects training epoch: 184, train loss: 1.52037, val loss: 1.60605\n",
      "Main effects training epoch: 185, train loss: 1.51414, val loss: 1.59444\n",
      "Main effects training epoch: 186, train loss: 1.51951, val loss: 1.61258\n",
      "Main effects training epoch: 187, train loss: 1.52691, val loss: 1.60462\n",
      "Main effects training epoch: 188, train loss: 1.51070, val loss: 1.60830\n",
      "Main effects training epoch: 189, train loss: 1.50982, val loss: 1.59836\n",
      "Main effects training epoch: 190, train loss: 1.51110, val loss: 1.59181\n",
      "Main effects training epoch: 191, train loss: 1.50860, val loss: 1.60462\n",
      "Main effects training epoch: 192, train loss: 1.51019, val loss: 1.59339\n",
      "Main effects training epoch: 193, train loss: 1.50871, val loss: 1.59399\n",
      "Main effects training epoch: 194, train loss: 1.51010, val loss: 1.59995\n",
      "Main effects training epoch: 195, train loss: 1.50732, val loss: 1.58955\n",
      "Main effects training epoch: 196, train loss: 1.51943, val loss: 1.59980\n",
      "Main effects training epoch: 197, train loss: 1.52664, val loss: 1.61026\n",
      "Main effects training epoch: 198, train loss: 1.52697, val loss: 1.60535\n",
      "Main effects training epoch: 199, train loss: 1.51058, val loss: 1.59071\n",
      "Main effects training epoch: 200, train loss: 1.50977, val loss: 1.59562\n",
      "Main effects training epoch: 201, train loss: 1.50855, val loss: 1.58794\n",
      "Main effects training epoch: 202, train loss: 1.50613, val loss: 1.59355\n",
      "Main effects training epoch: 203, train loss: 1.50374, val loss: 1.58254\n",
      "Main effects training epoch: 204, train loss: 1.50417, val loss: 1.59863\n",
      "Main effects training epoch: 205, train loss: 1.49828, val loss: 1.57278\n",
      "Main effects training epoch: 206, train loss: 1.50065, val loss: 1.59706\n",
      "Main effects training epoch: 207, train loss: 1.50225, val loss: 1.57431\n",
      "Main effects training epoch: 208, train loss: 1.50469, val loss: 1.59505\n",
      "Main effects training epoch: 209, train loss: 1.49897, val loss: 1.58897\n",
      "Main effects training epoch: 210, train loss: 1.50055, val loss: 1.57327\n",
      "Main effects training epoch: 211, train loss: 1.51061, val loss: 1.59110\n",
      "Main effects training epoch: 212, train loss: 1.50414, val loss: 1.57268\n",
      "Main effects training epoch: 213, train loss: 1.50270, val loss: 1.59163\n",
      "Main effects training epoch: 214, train loss: 1.49817, val loss: 1.57566\n",
      "Main effects training epoch: 215, train loss: 1.49571, val loss: 1.57164\n",
      "Main effects training epoch: 216, train loss: 1.49560, val loss: 1.57660\n",
      "Main effects training epoch: 217, train loss: 1.49150, val loss: 1.57137\n",
      "Main effects training epoch: 218, train loss: 1.49647, val loss: 1.58332\n",
      "Main effects training epoch: 219, train loss: 1.49084, val loss: 1.56828\n",
      "Main effects training epoch: 220, train loss: 1.49340, val loss: 1.57736\n",
      "Main effects training epoch: 221, train loss: 1.50009, val loss: 1.56919\n",
      "Main effects training epoch: 222, train loss: 1.49716, val loss: 1.58686\n",
      "Main effects training epoch: 223, train loss: 1.49415, val loss: 1.56762\n",
      "Main effects training epoch: 224, train loss: 1.49731, val loss: 1.57543\n",
      "Main effects training epoch: 225, train loss: 1.49069, val loss: 1.56735\n",
      "Main effects training epoch: 226, train loss: 1.48744, val loss: 1.56514\n",
      "Main effects training epoch: 227, train loss: 1.48606, val loss: 1.56522\n",
      "Main effects training epoch: 228, train loss: 1.48761, val loss: 1.57127\n",
      "Main effects training epoch: 229, train loss: 1.48698, val loss: 1.56841\n",
      "Main effects training epoch: 230, train loss: 1.49084, val loss: 1.57460\n",
      "Main effects training epoch: 231, train loss: 1.48533, val loss: 1.56179\n",
      "Main effects training epoch: 232, train loss: 1.48691, val loss: 1.56537\n",
      "Main effects training epoch: 233, train loss: 1.49704, val loss: 1.57297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 234, train loss: 1.48505, val loss: 1.57149\n",
      "Main effects training epoch: 235, train loss: 1.49092, val loss: 1.56129\n",
      "Main effects training epoch: 236, train loss: 1.48097, val loss: 1.56570\n",
      "Main effects training epoch: 237, train loss: 1.48239, val loss: 1.55212\n",
      "Main effects training epoch: 238, train loss: 1.48273, val loss: 1.55997\n",
      "Main effects training epoch: 239, train loss: 1.48013, val loss: 1.55569\n",
      "Main effects training epoch: 240, train loss: 1.48035, val loss: 1.56105\n",
      "Main effects training epoch: 241, train loss: 1.48436, val loss: 1.55253\n",
      "Main effects training epoch: 242, train loss: 1.48507, val loss: 1.55589\n",
      "Main effects training epoch: 243, train loss: 1.48884, val loss: 1.56125\n",
      "Main effects training epoch: 244, train loss: 1.48401, val loss: 1.57373\n",
      "Main effects training epoch: 245, train loss: 1.49051, val loss: 1.55337\n",
      "Main effects training epoch: 246, train loss: 1.48160, val loss: 1.56023\n",
      "Main effects training epoch: 247, train loss: 1.47777, val loss: 1.55312\n",
      "Main effects training epoch: 248, train loss: 1.49156, val loss: 1.55815\n",
      "Main effects training epoch: 249, train loss: 1.48098, val loss: 1.55937\n",
      "Main effects training epoch: 250, train loss: 1.48385, val loss: 1.56199\n",
      "Main effects training epoch: 251, train loss: 1.48659, val loss: 1.56128\n",
      "Main effects training epoch: 252, train loss: 1.47685, val loss: 1.56157\n",
      "Main effects training epoch: 253, train loss: 1.48808, val loss: 1.54859\n",
      "Main effects training epoch: 254, train loss: 1.48407, val loss: 1.55806\n",
      "Main effects training epoch: 255, train loss: 1.48563, val loss: 1.55742\n",
      "Main effects training epoch: 256, train loss: 1.47918, val loss: 1.54952\n",
      "Main effects training epoch: 257, train loss: 1.47606, val loss: 1.55121\n",
      "Main effects training epoch: 258, train loss: 1.47372, val loss: 1.54436\n",
      "Main effects training epoch: 259, train loss: 1.47893, val loss: 1.55576\n",
      "Main effects training epoch: 260, train loss: 1.47852, val loss: 1.54907\n",
      "Main effects training epoch: 261, train loss: 1.49550, val loss: 1.55619\n",
      "Main effects training epoch: 262, train loss: 1.48585, val loss: 1.56521\n",
      "Main effects training epoch: 263, train loss: 1.47815, val loss: 1.52881\n",
      "Main effects training epoch: 264, train loss: 1.46899, val loss: 1.55029\n",
      "Main effects training epoch: 265, train loss: 1.46937, val loss: 1.54180\n",
      "Main effects training epoch: 266, train loss: 1.47614, val loss: 1.54285\n",
      "Main effects training epoch: 267, train loss: 1.46799, val loss: 1.53840\n",
      "Main effects training epoch: 268, train loss: 1.46506, val loss: 1.53744\n",
      "Main effects training epoch: 269, train loss: 1.46442, val loss: 1.53535\n",
      "Main effects training epoch: 270, train loss: 1.46777, val loss: 1.52940\n",
      "Main effects training epoch: 271, train loss: 1.46353, val loss: 1.53767\n",
      "Main effects training epoch: 272, train loss: 1.47036, val loss: 1.52646\n",
      "Main effects training epoch: 273, train loss: 1.46460, val loss: 1.53165\n",
      "Main effects training epoch: 274, train loss: 1.46643, val loss: 1.51927\n",
      "Main effects training epoch: 275, train loss: 1.46682, val loss: 1.54881\n",
      "Main effects training epoch: 276, train loss: 1.47766, val loss: 1.54330\n",
      "Main effects training epoch: 277, train loss: 1.46014, val loss: 1.53089\n",
      "Main effects training epoch: 278, train loss: 1.46303, val loss: 1.53656\n",
      "Main effects training epoch: 279, train loss: 1.46443, val loss: 1.52143\n",
      "Main effects training epoch: 280, train loss: 1.46422, val loss: 1.54040\n",
      "Main effects training epoch: 281, train loss: 1.45312, val loss: 1.51418\n",
      "Main effects training epoch: 282, train loss: 1.45666, val loss: 1.52099\n",
      "Main effects training epoch: 283, train loss: 1.45174, val loss: 1.51592\n",
      "Main effects training epoch: 284, train loss: 1.46646, val loss: 1.51831\n",
      "Main effects training epoch: 285, train loss: 1.45664, val loss: 1.51366\n",
      "Main effects training epoch: 286, train loss: 1.45831, val loss: 1.50973\n",
      "Main effects training epoch: 287, train loss: 1.46764, val loss: 1.54868\n",
      "Main effects training epoch: 288, train loss: 1.46280, val loss: 1.50655\n",
      "Main effects training epoch: 289, train loss: 1.45112, val loss: 1.52111\n",
      "Main effects training epoch: 290, train loss: 1.45658, val loss: 1.51546\n",
      "Main effects training epoch: 291, train loss: 1.45599, val loss: 1.51696\n",
      "Main effects training epoch: 292, train loss: 1.44961, val loss: 1.51482\n",
      "Main effects training epoch: 293, train loss: 1.46067, val loss: 1.51332\n",
      "Main effects training epoch: 294, train loss: 1.47385, val loss: 1.51202\n",
      "Main effects training epoch: 295, train loss: 1.45615, val loss: 1.52349\n",
      "Main effects training epoch: 296, train loss: 1.47318, val loss: 1.53020\n",
      "Main effects training epoch: 297, train loss: 1.46001, val loss: 1.50907\n",
      "Main effects training epoch: 298, train loss: 1.47136, val loss: 1.51133\n",
      "Main effects training epoch: 299, train loss: 1.46982, val loss: 1.53516\n",
      "Main effects training epoch: 300, train loss: 1.44960, val loss: 1.50548\n",
      "##########Stage 1: main effect training stop.##########\n",
      "3 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.47981, val loss: 1.52267\n",
      "Main effects tuning epoch: 2, train loss: 1.46331, val loss: 1.51472\n",
      "Main effects tuning epoch: 3, train loss: 1.46830, val loss: 1.51161\n",
      "Main effects tuning epoch: 4, train loss: 1.46896, val loss: 1.51178\n",
      "Main effects tuning epoch: 5, train loss: 1.47528, val loss: 1.51366\n",
      "Main effects tuning epoch: 6, train loss: 1.46808, val loss: 1.51692\n",
      "Main effects tuning epoch: 7, train loss: 1.46537, val loss: 1.51874\n",
      "Main effects tuning epoch: 8, train loss: 1.47975, val loss: 1.50987\n",
      "Main effects tuning epoch: 9, train loss: 1.46065, val loss: 1.51071\n",
      "Main effects tuning epoch: 10, train loss: 1.46917, val loss: 1.50453\n",
      "Main effects tuning epoch: 11, train loss: 1.47333, val loss: 1.50563\n",
      "Main effects tuning epoch: 12, train loss: 1.46157, val loss: 1.50754\n",
      "Main effects tuning epoch: 13, train loss: 1.48088, val loss: 1.52535\n",
      "Main effects tuning epoch: 14, train loss: 1.46625, val loss: 1.51370\n",
      "Main effects tuning epoch: 15, train loss: 1.46376, val loss: 1.51038\n",
      "Main effects tuning epoch: 16, train loss: 1.47410, val loss: 1.50254\n",
      "Main effects tuning epoch: 17, train loss: 1.46502, val loss: 1.52004\n",
      "Main effects tuning epoch: 18, train loss: 1.46385, val loss: 1.49553\n",
      "Main effects tuning epoch: 19, train loss: 1.46368, val loss: 1.50912\n",
      "Main effects tuning epoch: 20, train loss: 1.46234, val loss: 1.50258\n",
      "Main effects tuning epoch: 21, train loss: 1.46433, val loss: 1.49670\n",
      "Main effects tuning epoch: 22, train loss: 1.46370, val loss: 1.49430\n",
      "Main effects tuning epoch: 23, train loss: 1.46845, val loss: 1.49861\n",
      "Main effects tuning epoch: 24, train loss: 1.46385, val loss: 1.50414\n",
      "Main effects tuning epoch: 25, train loss: 1.45635, val loss: 1.50218\n",
      "Main effects tuning epoch: 26, train loss: 1.45837, val loss: 1.49553\n",
      "Main effects tuning epoch: 27, train loss: 1.46347, val loss: 1.50165\n",
      "Main effects tuning epoch: 28, train loss: 1.46517, val loss: 1.49487\n",
      "Main effects tuning epoch: 29, train loss: 1.45550, val loss: 1.49383\n",
      "Main effects tuning epoch: 30, train loss: 1.45760, val loss: 1.50101\n",
      "Main effects tuning epoch: 31, train loss: 1.47132, val loss: 1.49669\n",
      "Main effects tuning epoch: 32, train loss: 1.45995, val loss: 1.49988\n",
      "Main effects tuning epoch: 33, train loss: 1.47518, val loss: 1.50785\n",
      "Main effects tuning epoch: 34, train loss: 1.45653, val loss: 1.49390\n",
      "Main effects tuning epoch: 35, train loss: 1.46586, val loss: 1.48579\n",
      "Main effects tuning epoch: 36, train loss: 1.45584, val loss: 1.50565\n",
      "Main effects tuning epoch: 37, train loss: 1.45403, val loss: 1.48898\n",
      "Main effects tuning epoch: 38, train loss: 1.45523, val loss: 1.49846\n",
      "Main effects tuning epoch: 39, train loss: 1.45269, val loss: 1.48773\n",
      "Main effects tuning epoch: 40, train loss: 1.45165, val loss: 1.49717\n",
      "Main effects tuning epoch: 41, train loss: 1.45082, val loss: 1.49373\n",
      "Main effects tuning epoch: 42, train loss: 1.45648, val loss: 1.49967\n",
      "Main effects tuning epoch: 43, train loss: 1.45135, val loss: 1.48556\n",
      "Main effects tuning epoch: 44, train loss: 1.45106, val loss: 1.48879\n",
      "Main effects tuning epoch: 45, train loss: 1.45430, val loss: 1.49438\n",
      "Main effects tuning epoch: 46, train loss: 1.44805, val loss: 1.47819\n",
      "Main effects tuning epoch: 47, train loss: 1.45053, val loss: 1.49323\n",
      "Main effects tuning epoch: 48, train loss: 1.45437, val loss: 1.49447\n",
      "Main effects tuning epoch: 49, train loss: 1.46905, val loss: 1.50304\n",
      "Main effects tuning epoch: 50, train loss: 1.45347, val loss: 1.49706\n",
      "##########Stage 2: interaction training start.##########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 1, train loss: 1.38751, val loss: 1.42972\n",
      "Interaction training epoch: 2, train loss: 1.29788, val loss: 1.29251\n",
      "Interaction training epoch: 3, train loss: 1.29616, val loss: 1.32320\n",
      "Interaction training epoch: 4, train loss: 1.04085, val loss: 1.05417\n",
      "Interaction training epoch: 5, train loss: 1.03009, val loss: 1.03709\n",
      "Interaction training epoch: 6, train loss: 1.05054, val loss: 1.05218\n",
      "Interaction training epoch: 7, train loss: 1.02948, val loss: 1.02686\n",
      "Interaction training epoch: 8, train loss: 0.99902, val loss: 1.00530\n",
      "Interaction training epoch: 9, train loss: 0.98387, val loss: 0.98691\n",
      "Interaction training epoch: 10, train loss: 0.99080, val loss: 1.01337\n",
      "Interaction training epoch: 11, train loss: 0.96049, val loss: 0.96233\n",
      "Interaction training epoch: 12, train loss: 0.94945, val loss: 0.94754\n",
      "Interaction training epoch: 13, train loss: 0.95982, val loss: 0.96210\n",
      "Interaction training epoch: 14, train loss: 0.95414, val loss: 0.94884\n",
      "Interaction training epoch: 15, train loss: 0.94345, val loss: 0.94976\n",
      "Interaction training epoch: 16, train loss: 0.93793, val loss: 0.92820\n",
      "Interaction training epoch: 17, train loss: 0.93178, val loss: 0.93546\n",
      "Interaction training epoch: 18, train loss: 0.91425, val loss: 0.89675\n",
      "Interaction training epoch: 19, train loss: 0.91632, val loss: 0.92731\n",
      "Interaction training epoch: 20, train loss: 0.92236, val loss: 0.91942\n",
      "Interaction training epoch: 21, train loss: 0.90920, val loss: 0.90217\n",
      "Interaction training epoch: 22, train loss: 0.90385, val loss: 0.89343\n",
      "Interaction training epoch: 23, train loss: 0.89987, val loss: 0.89575\n",
      "Interaction training epoch: 24, train loss: 0.90721, val loss: 0.89862\n",
      "Interaction training epoch: 25, train loss: 0.92542, val loss: 0.91327\n",
      "Interaction training epoch: 26, train loss: 0.90072, val loss: 0.91308\n",
      "Interaction training epoch: 27, train loss: 0.90686, val loss: 0.91623\n",
      "Interaction training epoch: 28, train loss: 0.89684, val loss: 0.88755\n",
      "Interaction training epoch: 29, train loss: 0.89574, val loss: 0.88784\n",
      "Interaction training epoch: 30, train loss: 0.89629, val loss: 0.90104\n",
      "Interaction training epoch: 31, train loss: 0.88658, val loss: 0.88759\n",
      "Interaction training epoch: 32, train loss: 0.89231, val loss: 0.87914\n",
      "Interaction training epoch: 33, train loss: 0.88231, val loss: 0.88828\n",
      "Interaction training epoch: 34, train loss: 0.88436, val loss: 0.87844\n",
      "Interaction training epoch: 35, train loss: 0.89044, val loss: 0.88305\n",
      "Interaction training epoch: 36, train loss: 0.88164, val loss: 0.88330\n",
      "Interaction training epoch: 37, train loss: 0.87723, val loss: 0.87402\n",
      "Interaction training epoch: 38, train loss: 0.87424, val loss: 0.87164\n",
      "Interaction training epoch: 39, train loss: 0.88113, val loss: 0.88409\n",
      "Interaction training epoch: 40, train loss: 0.87380, val loss: 0.86950\n",
      "Interaction training epoch: 41, train loss: 0.87787, val loss: 0.87971\n",
      "Interaction training epoch: 42, train loss: 0.87474, val loss: 0.86993\n",
      "Interaction training epoch: 43, train loss: 0.87210, val loss: 0.86398\n",
      "Interaction training epoch: 44, train loss: 0.86534, val loss: 0.86351\n",
      "Interaction training epoch: 45, train loss: 0.87172, val loss: 0.86373\n",
      "Interaction training epoch: 46, train loss: 0.86930, val loss: 0.86788\n",
      "Interaction training epoch: 47, train loss: 0.87310, val loss: 0.86793\n",
      "Interaction training epoch: 48, train loss: 0.87626, val loss: 0.87352\n",
      "Interaction training epoch: 49, train loss: 0.86374, val loss: 0.86011\n",
      "Interaction training epoch: 50, train loss: 0.86237, val loss: 0.86165\n",
      "Interaction training epoch: 51, train loss: 0.85959, val loss: 0.85691\n",
      "Interaction training epoch: 52, train loss: 0.87314, val loss: 0.87168\n",
      "Interaction training epoch: 53, train loss: 0.86557, val loss: 0.85709\n",
      "Interaction training epoch: 54, train loss: 0.88728, val loss: 0.89249\n",
      "Interaction training epoch: 55, train loss: 0.86669, val loss: 0.86887\n",
      "Interaction training epoch: 56, train loss: 0.87323, val loss: 0.87294\n",
      "Interaction training epoch: 57, train loss: 0.85961, val loss: 0.85659\n",
      "Interaction training epoch: 58, train loss: 0.86651, val loss: 0.86275\n",
      "Interaction training epoch: 59, train loss: 0.86838, val loss: 0.87083\n",
      "Interaction training epoch: 60, train loss: 0.87023, val loss: 0.86088\n",
      "Interaction training epoch: 61, train loss: 0.85704, val loss: 0.85348\n",
      "Interaction training epoch: 62, train loss: 0.85447, val loss: 0.85588\n",
      "Interaction training epoch: 63, train loss: 0.85908, val loss: 0.85408\n",
      "Interaction training epoch: 64, train loss: 0.86018, val loss: 0.86011\n",
      "Interaction training epoch: 65, train loss: 0.85534, val loss: 0.84887\n",
      "Interaction training epoch: 66, train loss: 0.85576, val loss: 0.85693\n",
      "Interaction training epoch: 67, train loss: 0.85598, val loss: 0.84631\n",
      "Interaction training epoch: 68, train loss: 0.85123, val loss: 0.85383\n",
      "Interaction training epoch: 69, train loss: 0.84728, val loss: 0.84808\n",
      "Interaction training epoch: 70, train loss: 0.85633, val loss: 0.85398\n",
      "Interaction training epoch: 71, train loss: 0.86387, val loss: 0.85482\n",
      "Interaction training epoch: 72, train loss: 0.86148, val loss: 0.86960\n",
      "Interaction training epoch: 73, train loss: 0.85043, val loss: 0.84455\n",
      "Interaction training epoch: 74, train loss: 0.85069, val loss: 0.85530\n",
      "Interaction training epoch: 75, train loss: 0.84891, val loss: 0.84545\n",
      "Interaction training epoch: 76, train loss: 0.86129, val loss: 0.86666\n",
      "Interaction training epoch: 77, train loss: 0.84883, val loss: 0.84464\n",
      "Interaction training epoch: 78, train loss: 0.84876, val loss: 0.85347\n",
      "Interaction training epoch: 79, train loss: 0.84878, val loss: 0.84226\n",
      "Interaction training epoch: 80, train loss: 0.85118, val loss: 0.85842\n",
      "Interaction training epoch: 81, train loss: 0.85010, val loss: 0.84725\n",
      "Interaction training epoch: 82, train loss: 0.85496, val loss: 0.85152\n",
      "Interaction training epoch: 83, train loss: 0.84734, val loss: 0.84694\n",
      "Interaction training epoch: 84, train loss: 0.83982, val loss: 0.83801\n",
      "Interaction training epoch: 85, train loss: 0.85011, val loss: 0.84656\n",
      "Interaction training epoch: 86, train loss: 0.85688, val loss: 0.84358\n",
      "Interaction training epoch: 87, train loss: 0.86353, val loss: 0.86145\n",
      "Interaction training epoch: 88, train loss: 0.85386, val loss: 0.84938\n",
      "Interaction training epoch: 89, train loss: 0.84869, val loss: 0.85860\n",
      "Interaction training epoch: 90, train loss: 0.85809, val loss: 0.85626\n",
      "Interaction training epoch: 91, train loss: 0.84737, val loss: 0.84347\n",
      "Interaction training epoch: 92, train loss: 0.88299, val loss: 0.87539\n",
      "Interaction training epoch: 93, train loss: 0.85565, val loss: 0.84627\n",
      "Interaction training epoch: 94, train loss: 0.86188, val loss: 0.87089\n",
      "Interaction training epoch: 95, train loss: 0.86285, val loss: 0.85021\n",
      "Interaction training epoch: 96, train loss: 0.85232, val loss: 0.85564\n",
      "Interaction training epoch: 97, train loss: 0.84748, val loss: 0.84804\n",
      "Interaction training epoch: 98, train loss: 0.84486, val loss: 0.84667\n",
      "Interaction training epoch: 99, train loss: 0.85361, val loss: 0.85675\n",
      "Interaction training epoch: 100, train loss: 0.84448, val loss: 0.84568\n",
      "Interaction training epoch: 101, train loss: 0.84034, val loss: 0.84930\n",
      "Interaction training epoch: 102, train loss: 0.84522, val loss: 0.84170\n",
      "Interaction training epoch: 103, train loss: 0.84307, val loss: 0.84549\n",
      "Interaction training epoch: 104, train loss: 0.84887, val loss: 0.84818\n",
      "Interaction training epoch: 105, train loss: 0.84111, val loss: 0.84438\n",
      "Interaction training epoch: 106, train loss: 0.84481, val loss: 0.85072\n",
      "Interaction training epoch: 107, train loss: 0.83311, val loss: 0.83184\n",
      "Interaction training epoch: 108, train loss: 0.84402, val loss: 0.83933\n",
      "Interaction training epoch: 109, train loss: 0.84446, val loss: 0.84765\n",
      "Interaction training epoch: 110, train loss: 0.84550, val loss: 0.83716\n",
      "Interaction training epoch: 111, train loss: 0.84113, val loss: 0.85107\n",
      "Interaction training epoch: 112, train loss: 0.83914, val loss: 0.84731\n",
      "Interaction training epoch: 113, train loss: 0.83822, val loss: 0.83572\n",
      "Interaction training epoch: 114, train loss: 0.84078, val loss: 0.85020\n",
      "Interaction training epoch: 115, train loss: 0.83684, val loss: 0.84174\n",
      "Interaction training epoch: 116, train loss: 0.84198, val loss: 0.84449\n",
      "Interaction training epoch: 117, train loss: 0.84988, val loss: 0.85086\n",
      "Interaction training epoch: 118, train loss: 0.84723, val loss: 0.84895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 119, train loss: 0.84825, val loss: 0.85438\n",
      "Interaction training epoch: 120, train loss: 0.83353, val loss: 0.84249\n",
      "Interaction training epoch: 121, train loss: 0.84568, val loss: 0.84169\n",
      "Interaction training epoch: 122, train loss: 0.84632, val loss: 0.84663\n",
      "Interaction training epoch: 123, train loss: 0.84193, val loss: 0.84624\n",
      "Interaction training epoch: 124, train loss: 0.82580, val loss: 0.82487\n",
      "Interaction training epoch: 125, train loss: 0.84564, val loss: 0.85498\n",
      "Interaction training epoch: 126, train loss: 0.84185, val loss: 0.85316\n",
      "Interaction training epoch: 127, train loss: 0.82878, val loss: 0.83275\n",
      "Interaction training epoch: 128, train loss: 0.82934, val loss: 0.83971\n",
      "Interaction training epoch: 129, train loss: 0.84837, val loss: 0.85086\n",
      "Interaction training epoch: 130, train loss: 0.83089, val loss: 0.84414\n",
      "Interaction training epoch: 131, train loss: 0.83600, val loss: 0.84247\n",
      "Interaction training epoch: 132, train loss: 0.83097, val loss: 0.83826\n",
      "Interaction training epoch: 133, train loss: 0.83711, val loss: 0.84705\n",
      "Interaction training epoch: 134, train loss: 0.83164, val loss: 0.83379\n",
      "Interaction training epoch: 135, train loss: 0.82984, val loss: 0.83505\n",
      "Interaction training epoch: 136, train loss: 0.83540, val loss: 0.82933\n",
      "Interaction training epoch: 137, train loss: 0.83534, val loss: 0.84802\n",
      "Interaction training epoch: 138, train loss: 0.84032, val loss: 0.84233\n",
      "Interaction training epoch: 139, train loss: 0.82736, val loss: 0.83227\n",
      "Interaction training epoch: 140, train loss: 0.82745, val loss: 0.83496\n",
      "Interaction training epoch: 141, train loss: 0.82917, val loss: 0.83139\n",
      "Interaction training epoch: 142, train loss: 0.83277, val loss: 0.83556\n",
      "Interaction training epoch: 143, train loss: 0.83873, val loss: 0.83588\n",
      "Interaction training epoch: 144, train loss: 0.83220, val loss: 0.84184\n",
      "Interaction training epoch: 145, train loss: 0.83137, val loss: 0.83206\n",
      "Interaction training epoch: 146, train loss: 0.83064, val loss: 0.83581\n",
      "Interaction training epoch: 147, train loss: 0.83673, val loss: 0.84164\n",
      "Interaction training epoch: 148, train loss: 0.83926, val loss: 0.83894\n",
      "Interaction training epoch: 149, train loss: 0.82296, val loss: 0.82527\n",
      "Interaction training epoch: 150, train loss: 0.83546, val loss: 0.83520\n",
      "Interaction training epoch: 151, train loss: 0.83130, val loss: 0.83714\n",
      "Interaction training epoch: 152, train loss: 0.83576, val loss: 0.84786\n",
      "Interaction training epoch: 153, train loss: 0.84551, val loss: 0.85060\n",
      "Interaction training epoch: 154, train loss: 0.83219, val loss: 0.82960\n",
      "Interaction training epoch: 155, train loss: 0.83138, val loss: 0.84068\n",
      "Interaction training epoch: 156, train loss: 0.82575, val loss: 0.82219\n",
      "Interaction training epoch: 157, train loss: 0.83437, val loss: 0.83265\n",
      "Interaction training epoch: 158, train loss: 0.83791, val loss: 0.84165\n",
      "Interaction training epoch: 159, train loss: 0.82527, val loss: 0.81928\n",
      "Interaction training epoch: 160, train loss: 0.84681, val loss: 0.84812\n",
      "Interaction training epoch: 161, train loss: 0.84271, val loss: 0.83922\n",
      "Interaction training epoch: 162, train loss: 0.82548, val loss: 0.83312\n",
      "Interaction training epoch: 163, train loss: 0.84106, val loss: 0.84076\n",
      "Interaction training epoch: 164, train loss: 0.83649, val loss: 0.84125\n",
      "Interaction training epoch: 165, train loss: 0.83301, val loss: 0.83403\n",
      "Interaction training epoch: 166, train loss: 0.82190, val loss: 0.81532\n",
      "Interaction training epoch: 167, train loss: 0.82201, val loss: 0.82740\n",
      "Interaction training epoch: 168, train loss: 0.82725, val loss: 0.82684\n",
      "Interaction training epoch: 169, train loss: 0.82403, val loss: 0.82240\n",
      "Interaction training epoch: 170, train loss: 0.82601, val loss: 0.82373\n",
      "Interaction training epoch: 171, train loss: 0.82168, val loss: 0.83330\n",
      "Interaction training epoch: 172, train loss: 0.83113, val loss: 0.82711\n",
      "Interaction training epoch: 173, train loss: 0.82023, val loss: 0.81578\n",
      "Interaction training epoch: 174, train loss: 0.82378, val loss: 0.82676\n",
      "Interaction training epoch: 175, train loss: 0.82887, val loss: 0.83708\n",
      "Interaction training epoch: 176, train loss: 0.82882, val loss: 0.83049\n",
      "Interaction training epoch: 177, train loss: 0.83148, val loss: 0.84407\n",
      "Interaction training epoch: 178, train loss: 0.82600, val loss: 0.82280\n",
      "Interaction training epoch: 179, train loss: 0.83135, val loss: 0.83668\n",
      "Interaction training epoch: 180, train loss: 0.83581, val loss: 0.84194\n",
      "Interaction training epoch: 181, train loss: 0.82044, val loss: 0.82287\n",
      "Interaction training epoch: 182, train loss: 0.82135, val loss: 0.82401\n",
      "Interaction training epoch: 183, train loss: 0.82940, val loss: 0.82729\n",
      "Interaction training epoch: 184, train loss: 0.82159, val loss: 0.82257\n",
      "Interaction training epoch: 185, train loss: 0.82134, val loss: 0.81587\n",
      "Interaction training epoch: 186, train loss: 0.82022, val loss: 0.81890\n",
      "Interaction training epoch: 187, train loss: 0.82263, val loss: 0.82265\n",
      "Interaction training epoch: 188, train loss: 0.82953, val loss: 0.83362\n",
      "Interaction training epoch: 189, train loss: 0.83232, val loss: 0.83031\n",
      "Interaction training epoch: 190, train loss: 0.81332, val loss: 0.81501\n",
      "Interaction training epoch: 191, train loss: 0.81952, val loss: 0.82189\n",
      "Interaction training epoch: 192, train loss: 0.82166, val loss: 0.82530\n",
      "Interaction training epoch: 193, train loss: 0.83151, val loss: 0.84141\n",
      "Interaction training epoch: 194, train loss: 0.82638, val loss: 0.81517\n",
      "Interaction training epoch: 195, train loss: 0.83199, val loss: 0.82848\n",
      "Interaction training epoch: 196, train loss: 0.82739, val loss: 0.83803\n",
      "Interaction training epoch: 197, train loss: 0.82264, val loss: 0.82194\n",
      "Interaction training epoch: 198, train loss: 0.82310, val loss: 0.83420\n",
      "Interaction training epoch: 199, train loss: 0.81675, val loss: 0.81584\n",
      "Interaction training epoch: 200, train loss: 0.83348, val loss: 0.82797\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########1 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.82371, val loss: 0.82202\n",
      "Interaction tuning epoch: 2, train loss: 0.82096, val loss: 0.82463\n",
      "Interaction tuning epoch: 3, train loss: 0.81892, val loss: 0.82484\n",
      "Interaction tuning epoch: 4, train loss: 0.82479, val loss: 0.82251\n",
      "Interaction tuning epoch: 5, train loss: 0.81839, val loss: 0.82672\n",
      "Interaction tuning epoch: 6, train loss: 0.81689, val loss: 0.81462\n",
      "Interaction tuning epoch: 7, train loss: 0.81898, val loss: 0.82051\n",
      "Interaction tuning epoch: 8, train loss: 0.81312, val loss: 0.81754\n",
      "Interaction tuning epoch: 9, train loss: 0.81539, val loss: 0.80975\n",
      "Interaction tuning epoch: 10, train loss: 0.81745, val loss: 0.82210\n",
      "Interaction tuning epoch: 11, train loss: 0.81418, val loss: 0.82079\n",
      "Interaction tuning epoch: 12, train loss: 0.82485, val loss: 0.82715\n",
      "Interaction tuning epoch: 13, train loss: 0.81825, val loss: 0.81712\n",
      "Interaction tuning epoch: 14, train loss: 0.82252, val loss: 0.82478\n",
      "Interaction tuning epoch: 15, train loss: 0.82245, val loss: 0.82111\n",
      "Interaction tuning epoch: 16, train loss: 0.81510, val loss: 0.81301\n",
      "Interaction tuning epoch: 17, train loss: 0.84084, val loss: 0.84370\n",
      "Interaction tuning epoch: 18, train loss: 0.82361, val loss: 0.83073\n",
      "Interaction tuning epoch: 19, train loss: 0.81988, val loss: 0.81411\n",
      "Interaction tuning epoch: 20, train loss: 0.82053, val loss: 0.82244\n",
      "Interaction tuning epoch: 21, train loss: 0.81551, val loss: 0.81523\n",
      "Interaction tuning epoch: 22, train loss: 0.82296, val loss: 0.82491\n",
      "Interaction tuning epoch: 23, train loss: 0.81978, val loss: 0.81538\n",
      "Interaction tuning epoch: 24, train loss: 0.81054, val loss: 0.81135\n",
      "Interaction tuning epoch: 25, train loss: 0.81548, val loss: 0.81068\n",
      "Interaction tuning epoch: 26, train loss: 0.81618, val loss: 0.81822\n",
      "Interaction tuning epoch: 27, train loss: 0.80571, val loss: 0.81150\n",
      "Interaction tuning epoch: 28, train loss: 0.81969, val loss: 0.82267\n",
      "Interaction tuning epoch: 29, train loss: 0.81576, val loss: 0.80756\n",
      "Interaction tuning epoch: 30, train loss: 0.81227, val loss: 0.81788\n",
      "Interaction tuning epoch: 31, train loss: 0.82466, val loss: 0.81669\n",
      "Interaction tuning epoch: 32, train loss: 0.82289, val loss: 0.82492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 33, train loss: 0.81647, val loss: 0.81767\n",
      "Interaction tuning epoch: 34, train loss: 0.82142, val loss: 0.82184\n",
      "Interaction tuning epoch: 35, train loss: 0.84295, val loss: 0.83084\n",
      "Interaction tuning epoch: 36, train loss: 0.81647, val loss: 0.81656\n",
      "Interaction tuning epoch: 37, train loss: 0.81434, val loss: 0.80192\n",
      "Interaction tuning epoch: 38, train loss: 0.81647, val loss: 0.81865\n",
      "Interaction tuning epoch: 39, train loss: 0.80969, val loss: 0.80605\n",
      "Interaction tuning epoch: 40, train loss: 0.81152, val loss: 0.81092\n",
      "Interaction tuning epoch: 41, train loss: 0.81000, val loss: 0.80475\n",
      "Interaction tuning epoch: 42, train loss: 0.80874, val loss: 0.80624\n",
      "Interaction tuning epoch: 43, train loss: 0.80997, val loss: 0.80494\n",
      "Interaction tuning epoch: 44, train loss: 0.81356, val loss: 0.80658\n",
      "Interaction tuning epoch: 45, train loss: 0.81452, val loss: 0.81623\n",
      "Interaction tuning epoch: 46, train loss: 0.80775, val loss: 0.81179\n",
      "Interaction tuning epoch: 47, train loss: 0.81477, val loss: 0.80927\n",
      "Interaction tuning epoch: 48, train loss: 0.81802, val loss: 0.81413\n",
      "Interaction tuning epoch: 49, train loss: 0.81307, val loss: 0.81744\n",
      "Interaction tuning epoch: 50, train loss: 0.81885, val loss: 0.80675\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 39.11222290992737\n",
      "After the gam stage, training error is 0.81885 , validation error is 0.80675\n",
      "missing value counts: 92783\n",
      "#####start auto_tuning#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best shrinkage is 0.929865\n",
      "the best combination is 0.634472\n",
      "[SoftImpute] Max Singular Value of X_init = 20.277104\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed MAE=0.676237 validation MAE=0.762611,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.629384 validation MAE=0.742539,rank=5\n",
      "[SoftImpute] Iter 3: observed MAE=0.590575 validation MAE=0.725300,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.556756 validation MAE=0.709607,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.528293 validation MAE=0.695647,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.503478 validation MAE=0.682433,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.481580 validation MAE=0.670839,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.462879 validation MAE=0.660293,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.446128 validation MAE=0.650988,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.430982 validation MAE=0.642562,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.418070 validation MAE=0.635380,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.406084 validation MAE=0.628565,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.395869 validation MAE=0.622462,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.385994 validation MAE=0.616958,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.378024 validation MAE=0.611986,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.369819 validation MAE=0.607516,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.361537 validation MAE=0.603198,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.355661 validation MAE=0.599650,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.349308 validation MAE=0.596382,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.343633 validation MAE=0.593265,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.339368 validation MAE=0.590518,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.334472 validation MAE=0.588019,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.329957 validation MAE=0.585049,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.324986 validation MAE=0.583087,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.321959 validation MAE=0.580429,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.317959 validation MAE=0.578283,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.314557 validation MAE=0.577190,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.310840 validation MAE=0.574860,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.308277 validation MAE=0.573587,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.305565 validation MAE=0.571676,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.302325 validation MAE=0.569749,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.299181 validation MAE=0.568541,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.297505 validation MAE=0.567166,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.294803 validation MAE=0.566239,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.291698 validation MAE=0.564321,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.290534 validation MAE=0.562729,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.288764 validation MAE=0.561741,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.286443 validation MAE=0.561462,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.285152 validation MAE=0.559922,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.283152 validation MAE=0.559080,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.280992 validation MAE=0.558073,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.279909 validation MAE=0.557186,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.278484 validation MAE=0.555820,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.276281 validation MAE=0.554670,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.274956 validation MAE=0.554391,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.273190 validation MAE=0.553523,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.272372 validation MAE=0.552826,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.270996 validation MAE=0.552409,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.269536 validation MAE=0.551730,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.267991 validation MAE=0.550547,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.267825 validation MAE=0.550472,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.266042 validation MAE=0.549190,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.265130 validation MAE=0.548977,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.264205 validation MAE=0.547595,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.264083 validation MAE=0.548057,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.261811 validation MAE=0.547290,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.261614 validation MAE=0.546323,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.260211 validation MAE=0.545841,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.259537 validation MAE=0.545434,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.258532 validation MAE=0.545318,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.257616 validation MAE=0.544433,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.256927 validation MAE=0.543769,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.257493 validation MAE=0.543164,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.256845 validation MAE=0.542582,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.256263 validation MAE=0.542571,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.255619 validation MAE=0.541797,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.255313 validation MAE=0.541736,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.253503 validation MAE=0.540363,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.253958 validation MAE=0.540650,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.252829 validation MAE=0.539773,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.251762 validation MAE=0.539458,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.250816 validation MAE=0.538705,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.251116 validation MAE=0.538919,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.250131 validation MAE=0.538204,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.248904 validation MAE=0.537400,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.249060 validation MAE=0.537729,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.248457 validation MAE=0.536663,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.247255 validation MAE=0.536282,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.247613 validation MAE=0.536056,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.246173 validation MAE=0.535641,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.245634 validation MAE=0.535239,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.245497 validation MAE=0.534439,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.244696 validation MAE=0.533873,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.244124 validation MAE=0.534104,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.242855 validation MAE=0.533334,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.243129 validation MAE=0.533050,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.242685 validation MAE=0.532537,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.242356 validation MAE=0.532320,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.241928 validation MAE=0.531635,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.241122 validation MAE=0.531710,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.240409 validation MAE=0.531488,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.240139 validation MAE=0.530426,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.240355 validation MAE=0.529812,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.239565 validation MAE=0.530023,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.238553 validation MAE=0.529420,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.237852 validation MAE=0.529041,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.238293 validation MAE=0.528591,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.238062 validation MAE=0.527990,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.236714 validation MAE=0.527660,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.236134 validation MAE=0.527684,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.236221 validation MAE=0.527292,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.236202 validation MAE=0.526860,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.235854 validation MAE=0.526484,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.235606 validation MAE=0.526527,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.234981 validation MAE=0.526041,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.234482 validation MAE=0.526168,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.233617 validation MAE=0.525402,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.234488 validation MAE=0.525563,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 109: observed MAE=0.233575 validation MAE=0.524864,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.232872 validation MAE=0.524824,rank=5\n",
      "[SoftImpute] Iter 111: observed MAE=0.232167 validation MAE=0.524191,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.231960 validation MAE=0.524005,rank=5\n",
      "[SoftImpute] Iter 113: observed MAE=0.232226 validation MAE=0.523858,rank=5\n",
      "[SoftImpute] Iter 114: observed MAE=0.231755 validation MAE=0.523428,rank=5\n",
      "[SoftImpute] Iter 115: observed MAE=0.231009 validation MAE=0.523073,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.231779 validation MAE=0.523457,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.230296 validation MAE=0.522458,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.229968 validation MAE=0.522267,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.230351 validation MAE=0.522320,rank=5\n",
      "[SoftImpute] Iter 120: observed MAE=0.230335 validation MAE=0.522342,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.229452 validation MAE=0.521949,rank=5\n",
      "[SoftImpute] Iter 122: observed MAE=0.227594 validation MAE=0.521183,rank=5\n",
      "[SoftImpute] Iter 123: observed MAE=0.227972 validation MAE=0.520620,rank=5\n",
      "[SoftImpute] Iter 124: observed MAE=0.228075 validation MAE=0.520448,rank=5\n",
      "[SoftImpute] Iter 125: observed MAE=0.228366 validation MAE=0.520787,rank=5\n",
      "[SoftImpute] Iter 126: observed MAE=0.227141 validation MAE=0.520161,rank=5\n",
      "[SoftImpute] Iter 127: observed MAE=0.226803 validation MAE=0.519498,rank=5\n",
      "[SoftImpute] Iter 128: observed MAE=0.227223 validation MAE=0.519956,rank=5\n",
      "[SoftImpute] Iter 129: observed MAE=0.226587 validation MAE=0.519937,rank=5\n",
      "[SoftImpute] Iter 130: observed MAE=0.225701 validation MAE=0.519145,rank=5\n",
      "[SoftImpute] Iter 131: observed MAE=0.226400 validation MAE=0.519289,rank=5\n",
      "[SoftImpute] Iter 132: observed MAE=0.225892 validation MAE=0.518898,rank=5\n",
      "[SoftImpute] Iter 133: observed MAE=0.225694 validation MAE=0.518690,rank=5\n",
      "[SoftImpute] Iter 134: observed MAE=0.224708 validation MAE=0.517833,rank=5\n",
      "[SoftImpute] Iter 135: observed MAE=0.224554 validation MAE=0.517780,rank=5\n",
      "[SoftImpute] Iter 136: observed MAE=0.224349 validation MAE=0.517837,rank=5\n",
      "[SoftImpute] Iter 137: observed MAE=0.224356 validation MAE=0.517215,rank=5\n",
      "[SoftImpute] Iter 138: observed MAE=0.223519 validation MAE=0.517491,rank=5\n",
      "[SoftImpute] Iter 139: observed MAE=0.223556 validation MAE=0.516860,rank=5\n",
      "[SoftImpute] Iter 140: observed MAE=0.224341 validation MAE=0.517039,rank=5\n",
      "[SoftImpute] Iter 141: observed MAE=0.223143 validation MAE=0.516671,rank=5\n",
      "[SoftImpute] Iter 142: observed MAE=0.222715 validation MAE=0.516459,rank=5\n",
      "[SoftImpute] Iter 143: observed MAE=0.222699 validation MAE=0.515971,rank=5\n",
      "[SoftImpute] Iter 144: observed MAE=0.222852 validation MAE=0.516207,rank=5\n",
      "[SoftImpute] Iter 145: observed MAE=0.222285 validation MAE=0.515401,rank=5\n",
      "[SoftImpute] Iter 146: observed MAE=0.221874 validation MAE=0.515886,rank=5\n",
      "[SoftImpute] Iter 147: observed MAE=0.221900 validation MAE=0.515566,rank=5\n",
      "[SoftImpute] Iter 148: observed MAE=0.221514 validation MAE=0.515357,rank=5\n",
      "[SoftImpute] Iter 149: observed MAE=0.220702 validation MAE=0.514624,rank=5\n",
      "[SoftImpute] Iter 150: observed MAE=0.221459 validation MAE=0.515124,rank=5\n",
      "[SoftImpute] Iter 151: observed MAE=0.220730 validation MAE=0.514398,rank=5\n",
      "[SoftImpute] Iter 152: observed MAE=0.220429 validation MAE=0.514484,rank=5\n",
      "[SoftImpute] Iter 153: observed MAE=0.220679 validation MAE=0.514067,rank=5\n",
      "[SoftImpute] Iter 154: observed MAE=0.219735 validation MAE=0.514148,rank=5\n",
      "[SoftImpute] Iter 155: observed MAE=0.219897 validation MAE=0.513910,rank=5\n",
      "[SoftImpute] Iter 156: observed MAE=0.219079 validation MAE=0.513483,rank=5\n",
      "[SoftImpute] Iter 157: observed MAE=0.219359 validation MAE=0.513607,rank=5\n",
      "[SoftImpute] Iter 158: observed MAE=0.219161 validation MAE=0.513320,rank=5\n",
      "[SoftImpute] Iter 159: observed MAE=0.218953 validation MAE=0.513060,rank=5\n",
      "[SoftImpute] Iter 160: observed MAE=0.219177 validation MAE=0.513164,rank=5\n",
      "[SoftImpute] Iter 161: observed MAE=0.218700 validation MAE=0.512867,rank=5\n",
      "[SoftImpute] Iter 162: observed MAE=0.218240 validation MAE=0.512806,rank=5\n",
      "[SoftImpute] Iter 163: observed MAE=0.218457 validation MAE=0.512366,rank=5\n",
      "[SoftImpute] Iter 164: observed MAE=0.218523 validation MAE=0.512305,rank=5\n",
      "[SoftImpute] Iter 165: observed MAE=0.217878 validation MAE=0.512024,rank=5\n",
      "[SoftImpute] Iter 166: observed MAE=0.217171 validation MAE=0.511900,rank=5\n",
      "[SoftImpute] Iter 167: observed MAE=0.217633 validation MAE=0.511877,rank=5\n",
      "[SoftImpute] Iter 168: observed MAE=0.217169 validation MAE=0.511944,rank=5\n",
      "[SoftImpute] Iter 169: observed MAE=0.216499 validation MAE=0.511415,rank=5\n",
      "[SoftImpute] Iter 170: observed MAE=0.217156 validation MAE=0.511834,rank=5\n",
      "[SoftImpute] Iter 171: observed MAE=0.217262 validation MAE=0.511413,rank=5\n",
      "[SoftImpute] Iter 172: observed MAE=0.216728 validation MAE=0.510937,rank=5\n",
      "[SoftImpute] Iter 173: observed MAE=0.215895 validation MAE=0.510688,rank=5\n",
      "[SoftImpute] Iter 174: observed MAE=0.215410 validation MAE=0.510109,rank=5\n",
      "[SoftImpute] Iter 175: observed MAE=0.216247 validation MAE=0.510740,rank=5\n",
      "[SoftImpute] Iter 176: observed MAE=0.215812 validation MAE=0.510810,rank=5\n",
      "[SoftImpute] Iter 177: observed MAE=0.215877 validation MAE=0.510405,rank=5\n",
      "[SoftImpute] Iter 178: observed MAE=0.215543 validation MAE=0.510221,rank=5\n",
      "[SoftImpute] Iter 179: observed MAE=0.215242 validation MAE=0.509771,rank=5\n",
      "[SoftImpute] Iter 180: observed MAE=0.214906 validation MAE=0.509662,rank=5\n",
      "[SoftImpute] Iter 181: observed MAE=0.215094 validation MAE=0.510152,rank=5\n",
      "[SoftImpute] Iter 182: observed MAE=0.214906 validation MAE=0.509901,rank=5\n",
      "[SoftImpute] Iter 183: observed MAE=0.214493 validation MAE=0.510081,rank=5\n",
      "[SoftImpute] Iter 184: observed MAE=0.214810 validation MAE=0.509041,rank=5\n",
      "[SoftImpute] Iter 185: observed MAE=0.214623 validation MAE=0.509182,rank=5\n",
      "[SoftImpute] Iter 186: observed MAE=0.214376 validation MAE=0.509244,rank=5\n",
      "[SoftImpute] Iter 187: observed MAE=0.214419 validation MAE=0.509427,rank=5\n",
      "[SoftImpute] Stopped after iteration 187 for lambda=0.405542\n",
      "final num of user group: 28\n",
      "final num of item group: 48\n",
      "change mode state : True\n",
      "time cost: 401.66000056266785\n",
      "After the matrix factor stage, training error is 0.21442, validation error is 0.50943\n",
      "0\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.02581, val loss: 4.15569\n",
      "Main effects training epoch: 2, train loss: 3.86663, val loss: 4.00300\n",
      "Main effects training epoch: 3, train loss: 3.66700, val loss: 3.81314\n",
      "Main effects training epoch: 4, train loss: 3.48207, val loss: 3.63384\n",
      "Main effects training epoch: 5, train loss: 3.38250, val loss: 3.50337\n",
      "Main effects training epoch: 6, train loss: 3.28135, val loss: 3.35840\n",
      "Main effects training epoch: 7, train loss: 3.28186, val loss: 3.35070\n",
      "Main effects training epoch: 8, train loss: 3.27336, val loss: 3.36040\n",
      "Main effects training epoch: 9, train loss: 3.25283, val loss: 3.34172\n",
      "Main effects training epoch: 10, train loss: 3.15250, val loss: 3.23796\n",
      "Main effects training epoch: 11, train loss: 3.05938, val loss: 3.14667\n",
      "Main effects training epoch: 12, train loss: 3.05048, val loss: 3.13776\n",
      "Main effects training epoch: 13, train loss: 2.97453, val loss: 3.06166\n",
      "Main effects training epoch: 14, train loss: 2.90463, val loss: 2.99088\n",
      "Main effects training epoch: 15, train loss: 2.85090, val loss: 2.93330\n",
      "Main effects training epoch: 16, train loss: 2.80705, val loss: 2.88247\n",
      "Main effects training epoch: 17, train loss: 2.72197, val loss: 2.79421\n",
      "Main effects training epoch: 18, train loss: 2.70970, val loss: 2.78070\n",
      "Main effects training epoch: 19, train loss: 2.64818, val loss: 2.71541\n",
      "Main effects training epoch: 20, train loss: 2.60938, val loss: 2.68330\n",
      "Main effects training epoch: 21, train loss: 2.57306, val loss: 2.65275\n",
      "Main effects training epoch: 22, train loss: 2.51936, val loss: 2.59316\n",
      "Main effects training epoch: 23, train loss: 2.51905, val loss: 2.60367\n",
      "Main effects training epoch: 24, train loss: 2.44088, val loss: 2.52255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 25, train loss: 2.41554, val loss: 2.50227\n",
      "Main effects training epoch: 26, train loss: 2.38866, val loss: 2.46996\n",
      "Main effects training epoch: 27, train loss: 2.36765, val loss: 2.45355\n",
      "Main effects training epoch: 28, train loss: 2.33153, val loss: 2.41112\n",
      "Main effects training epoch: 29, train loss: 2.28303, val loss: 2.36249\n",
      "Main effects training epoch: 30, train loss: 2.26930, val loss: 2.35093\n",
      "Main effects training epoch: 31, train loss: 2.21173, val loss: 2.29201\n",
      "Main effects training epoch: 32, train loss: 2.24060, val loss: 2.32154\n",
      "Main effects training epoch: 33, train loss: 2.17058, val loss: 2.25252\n",
      "Main effects training epoch: 34, train loss: 2.18446, val loss: 2.25689\n",
      "Main effects training epoch: 35, train loss: 2.16814, val loss: 2.24363\n",
      "Main effects training epoch: 36, train loss: 2.13803, val loss: 2.21848\n",
      "Main effects training epoch: 37, train loss: 2.07828, val loss: 2.15467\n",
      "Main effects training epoch: 38, train loss: 2.10415, val loss: 2.17893\n",
      "Main effects training epoch: 39, train loss: 2.06347, val loss: 2.14620\n",
      "Main effects training epoch: 40, train loss: 2.05131, val loss: 2.12958\n",
      "Main effects training epoch: 41, train loss: 2.04399, val loss: 2.11796\n",
      "Main effects training epoch: 42, train loss: 1.99832, val loss: 2.07447\n",
      "Main effects training epoch: 43, train loss: 1.99686, val loss: 2.07381\n",
      "Main effects training epoch: 44, train loss: 1.98219, val loss: 2.05986\n",
      "Main effects training epoch: 45, train loss: 1.98538, val loss: 2.06555\n",
      "Main effects training epoch: 46, train loss: 1.93199, val loss: 2.00777\n",
      "Main effects training epoch: 47, train loss: 1.92633, val loss: 2.00292\n",
      "Main effects training epoch: 48, train loss: 1.93781, val loss: 2.01180\n",
      "Main effects training epoch: 49, train loss: 1.90558, val loss: 1.97629\n",
      "Main effects training epoch: 50, train loss: 1.89436, val loss: 1.97301\n",
      "Main effects training epoch: 51, train loss: 1.88707, val loss: 1.95539\n",
      "Main effects training epoch: 52, train loss: 1.87418, val loss: 1.95203\n",
      "Main effects training epoch: 53, train loss: 1.88074, val loss: 1.95203\n",
      "Main effects training epoch: 54, train loss: 1.84766, val loss: 1.91670\n",
      "Main effects training epoch: 55, train loss: 1.86362, val loss: 1.93316\n",
      "Main effects training epoch: 56, train loss: 1.84504, val loss: 1.91578\n",
      "Main effects training epoch: 57, train loss: 1.83871, val loss: 1.90598\n",
      "Main effects training epoch: 58, train loss: 1.83635, val loss: 1.90794\n",
      "Main effects training epoch: 59, train loss: 1.81987, val loss: 1.88301\n",
      "Main effects training epoch: 60, train loss: 1.81411, val loss: 1.87753\n",
      "Main effects training epoch: 61, train loss: 1.79435, val loss: 1.85428\n",
      "Main effects training epoch: 62, train loss: 1.80737, val loss: 1.87724\n",
      "Main effects training epoch: 63, train loss: 1.78433, val loss: 1.84476\n",
      "Main effects training epoch: 64, train loss: 1.77484, val loss: 1.83380\n",
      "Main effects training epoch: 65, train loss: 1.76469, val loss: 1.82684\n",
      "Main effects training epoch: 66, train loss: 1.75610, val loss: 1.81696\n",
      "Main effects training epoch: 67, train loss: 1.74015, val loss: 1.80136\n",
      "Main effects training epoch: 68, train loss: 1.74855, val loss: 1.80547\n",
      "Main effects training epoch: 69, train loss: 1.73357, val loss: 1.79520\n",
      "Main effects training epoch: 70, train loss: 1.74203, val loss: 1.79823\n",
      "Main effects training epoch: 71, train loss: 1.73003, val loss: 1.79278\n",
      "Main effects training epoch: 72, train loss: 1.72470, val loss: 1.77914\n",
      "Main effects training epoch: 73, train loss: 1.72740, val loss: 1.78702\n",
      "Main effects training epoch: 74, train loss: 1.71997, val loss: 1.77884\n",
      "Main effects training epoch: 75, train loss: 1.72546, val loss: 1.77572\n",
      "Main effects training epoch: 76, train loss: 1.71292, val loss: 1.77190\n",
      "Main effects training epoch: 77, train loss: 1.71444, val loss: 1.77190\n",
      "Main effects training epoch: 78, train loss: 1.71019, val loss: 1.76935\n",
      "Main effects training epoch: 79, train loss: 1.70637, val loss: 1.75924\n",
      "Main effects training epoch: 80, train loss: 1.70559, val loss: 1.76570\n",
      "Main effects training epoch: 81, train loss: 1.70424, val loss: 1.76163\n",
      "Main effects training epoch: 82, train loss: 1.69625, val loss: 1.75276\n",
      "Main effects training epoch: 83, train loss: 1.70362, val loss: 1.76509\n",
      "Main effects training epoch: 84, train loss: 1.69010, val loss: 1.74551\n",
      "Main effects training epoch: 85, train loss: 1.69095, val loss: 1.75028\n",
      "Main effects training epoch: 86, train loss: 1.68340, val loss: 1.74746\n",
      "Main effects training epoch: 87, train loss: 1.68064, val loss: 1.74529\n",
      "Main effects training epoch: 88, train loss: 1.67380, val loss: 1.74022\n",
      "Main effects training epoch: 89, train loss: 1.67360, val loss: 1.74665\n",
      "Main effects training epoch: 90, train loss: 1.65844, val loss: 1.73033\n",
      "Main effects training epoch: 91, train loss: 1.65355, val loss: 1.72703\n",
      "Main effects training epoch: 92, train loss: 1.64214, val loss: 1.71970\n",
      "Main effects training epoch: 93, train loss: 1.64936, val loss: 1.73062\n",
      "Main effects training epoch: 94, train loss: 1.63636, val loss: 1.71888\n",
      "Main effects training epoch: 95, train loss: 1.63488, val loss: 1.72091\n",
      "Main effects training epoch: 96, train loss: 1.63066, val loss: 1.70495\n",
      "Main effects training epoch: 97, train loss: 1.63836, val loss: 1.72648\n",
      "Main effects training epoch: 98, train loss: 1.63448, val loss: 1.71359\n",
      "Main effects training epoch: 99, train loss: 1.62205, val loss: 1.71122\n",
      "Main effects training epoch: 100, train loss: 1.62696, val loss: 1.71339\n",
      "Main effects training epoch: 101, train loss: 1.62468, val loss: 1.70969\n",
      "Main effects training epoch: 102, train loss: 1.62361, val loss: 1.71057\n",
      "Main effects training epoch: 103, train loss: 1.61811, val loss: 1.70932\n",
      "Main effects training epoch: 104, train loss: 1.61437, val loss: 1.69638\n",
      "Main effects training epoch: 105, train loss: 1.61687, val loss: 1.70838\n",
      "Main effects training epoch: 106, train loss: 1.61267, val loss: 1.69917\n",
      "Main effects training epoch: 107, train loss: 1.60933, val loss: 1.69456\n",
      "Main effects training epoch: 108, train loss: 1.60899, val loss: 1.69789\n",
      "Main effects training epoch: 109, train loss: 1.61071, val loss: 1.70213\n",
      "Main effects training epoch: 110, train loss: 1.60834, val loss: 1.69356\n",
      "Main effects training epoch: 111, train loss: 1.60903, val loss: 1.69018\n",
      "Main effects training epoch: 112, train loss: 1.61192, val loss: 1.71417\n",
      "Main effects training epoch: 113, train loss: 1.60321, val loss: 1.68930\n",
      "Main effects training epoch: 114, train loss: 1.60681, val loss: 1.69810\n",
      "Main effects training epoch: 115, train loss: 1.61161, val loss: 1.69664\n",
      "Main effects training epoch: 116, train loss: 1.60462, val loss: 1.69884\n",
      "Main effects training epoch: 117, train loss: 1.60153, val loss: 1.68634\n",
      "Main effects training epoch: 118, train loss: 1.59715, val loss: 1.69219\n",
      "Main effects training epoch: 119, train loss: 1.59567, val loss: 1.68591\n",
      "Main effects training epoch: 120, train loss: 1.59747, val loss: 1.68979\n",
      "Main effects training epoch: 121, train loss: 1.59275, val loss: 1.68562\n",
      "Main effects training epoch: 122, train loss: 1.58952, val loss: 1.68187\n",
      "Main effects training epoch: 123, train loss: 1.59299, val loss: 1.69392\n",
      "Main effects training epoch: 124, train loss: 1.58823, val loss: 1.67589\n",
      "Main effects training epoch: 125, train loss: 1.58752, val loss: 1.67897\n",
      "Main effects training epoch: 126, train loss: 1.58643, val loss: 1.67941\n",
      "Main effects training epoch: 127, train loss: 1.58719, val loss: 1.68306\n",
      "Main effects training epoch: 128, train loss: 1.58105, val loss: 1.67735\n",
      "Main effects training epoch: 129, train loss: 1.58516, val loss: 1.67262\n",
      "Main effects training epoch: 130, train loss: 1.57827, val loss: 1.67244\n",
      "Main effects training epoch: 131, train loss: 1.57854, val loss: 1.66952\n",
      "Main effects training epoch: 132, train loss: 1.58368, val loss: 1.67841\n",
      "Main effects training epoch: 133, train loss: 1.57787, val loss: 1.66421\n",
      "Main effects training epoch: 134, train loss: 1.57550, val loss: 1.67620\n",
      "Main effects training epoch: 135, train loss: 1.57558, val loss: 1.66555\n",
      "Main effects training epoch: 136, train loss: 1.57565, val loss: 1.66808\n",
      "Main effects training epoch: 137, train loss: 1.57215, val loss: 1.65359\n",
      "Main effects training epoch: 138, train loss: 1.57597, val loss: 1.67812\n",
      "Main effects training epoch: 139, train loss: 1.56594, val loss: 1.65260\n",
      "Main effects training epoch: 140, train loss: 1.56379, val loss: 1.65452\n",
      "Main effects training epoch: 141, train loss: 1.56809, val loss: 1.65819\n",
      "Main effects training epoch: 142, train loss: 1.56959, val loss: 1.67084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 143, train loss: 1.56391, val loss: 1.65611\n",
      "Main effects training epoch: 144, train loss: 1.56557, val loss: 1.66064\n",
      "Main effects training epoch: 145, train loss: 1.56139, val loss: 1.65606\n",
      "Main effects training epoch: 146, train loss: 1.55847, val loss: 1.65215\n",
      "Main effects training epoch: 147, train loss: 1.55328, val loss: 1.65161\n",
      "Main effects training epoch: 148, train loss: 1.55521, val loss: 1.65337\n",
      "Main effects training epoch: 149, train loss: 1.55394, val loss: 1.64666\n",
      "Main effects training epoch: 150, train loss: 1.55689, val loss: 1.64452\n",
      "Main effects training epoch: 151, train loss: 1.55483, val loss: 1.64596\n",
      "Main effects training epoch: 152, train loss: 1.55640, val loss: 1.65974\n",
      "Main effects training epoch: 153, train loss: 1.55465, val loss: 1.64418\n",
      "Main effects training epoch: 154, train loss: 1.54827, val loss: 1.64315\n",
      "Main effects training epoch: 155, train loss: 1.54425, val loss: 1.63498\n",
      "Main effects training epoch: 156, train loss: 1.54566, val loss: 1.63837\n",
      "Main effects training epoch: 157, train loss: 1.54784, val loss: 1.64139\n",
      "Main effects training epoch: 158, train loss: 1.55597, val loss: 1.64081\n",
      "Main effects training epoch: 159, train loss: 1.54733, val loss: 1.64121\n",
      "Main effects training epoch: 160, train loss: 1.54370, val loss: 1.64175\n",
      "Main effects training epoch: 161, train loss: 1.54700, val loss: 1.63689\n",
      "Main effects training epoch: 162, train loss: 1.53894, val loss: 1.64314\n",
      "Main effects training epoch: 163, train loss: 1.55071, val loss: 1.63990\n",
      "Main effects training epoch: 164, train loss: 1.53857, val loss: 1.62758\n",
      "Main effects training epoch: 165, train loss: 1.53582, val loss: 1.63364\n",
      "Main effects training epoch: 166, train loss: 1.53784, val loss: 1.62337\n",
      "Main effects training epoch: 167, train loss: 1.53668, val loss: 1.61628\n",
      "Main effects training epoch: 168, train loss: 1.54187, val loss: 1.64968\n",
      "Main effects training epoch: 169, train loss: 1.53233, val loss: 1.61626\n",
      "Main effects training epoch: 170, train loss: 1.53301, val loss: 1.62002\n",
      "Main effects training epoch: 171, train loss: 1.53462, val loss: 1.63271\n",
      "Main effects training epoch: 172, train loss: 1.53059, val loss: 1.62420\n",
      "Main effects training epoch: 173, train loss: 1.52675, val loss: 1.61587\n",
      "Main effects training epoch: 174, train loss: 1.52574, val loss: 1.61472\n",
      "Main effects training epoch: 175, train loss: 1.52522, val loss: 1.61609\n",
      "Main effects training epoch: 176, train loss: 1.52517, val loss: 1.61189\n",
      "Main effects training epoch: 177, train loss: 1.52300, val loss: 1.61130\n",
      "Main effects training epoch: 178, train loss: 1.52251, val loss: 1.61256\n",
      "Main effects training epoch: 179, train loss: 1.52604, val loss: 1.62370\n",
      "Main effects training epoch: 180, train loss: 1.51909, val loss: 1.60909\n",
      "Main effects training epoch: 181, train loss: 1.52000, val loss: 1.60844\n",
      "Main effects training epoch: 182, train loss: 1.52126, val loss: 1.60803\n",
      "Main effects training epoch: 183, train loss: 1.51478, val loss: 1.60948\n",
      "Main effects training epoch: 184, train loss: 1.52037, val loss: 1.60605\n",
      "Main effects training epoch: 185, train loss: 1.51414, val loss: 1.59444\n",
      "Main effects training epoch: 186, train loss: 1.51951, val loss: 1.61258\n",
      "Main effects training epoch: 187, train loss: 1.52691, val loss: 1.60462\n",
      "Main effects training epoch: 188, train loss: 1.51070, val loss: 1.60830\n",
      "Main effects training epoch: 189, train loss: 1.50982, val loss: 1.59836\n",
      "Main effects training epoch: 190, train loss: 1.51110, val loss: 1.59181\n",
      "Main effects training epoch: 191, train loss: 1.50860, val loss: 1.60462\n",
      "Main effects training epoch: 192, train loss: 1.51019, val loss: 1.59339\n",
      "Main effects training epoch: 193, train loss: 1.50871, val loss: 1.59399\n",
      "Main effects training epoch: 194, train loss: 1.51010, val loss: 1.59995\n",
      "Main effects training epoch: 195, train loss: 1.50732, val loss: 1.58955\n",
      "Main effects training epoch: 196, train loss: 1.51943, val loss: 1.59980\n",
      "Main effects training epoch: 197, train loss: 1.52664, val loss: 1.61026\n",
      "Main effects training epoch: 198, train loss: 1.52697, val loss: 1.60535\n",
      "Main effects training epoch: 199, train loss: 1.51058, val loss: 1.59071\n",
      "Main effects training epoch: 200, train loss: 1.50977, val loss: 1.59562\n",
      "Main effects training epoch: 201, train loss: 1.50855, val loss: 1.58794\n",
      "Main effects training epoch: 202, train loss: 1.50613, val loss: 1.59355\n",
      "Main effects training epoch: 203, train loss: 1.50374, val loss: 1.58254\n",
      "Main effects training epoch: 204, train loss: 1.50417, val loss: 1.59863\n",
      "Main effects training epoch: 205, train loss: 1.49828, val loss: 1.57278\n",
      "Main effects training epoch: 206, train loss: 1.50065, val loss: 1.59706\n",
      "Main effects training epoch: 207, train loss: 1.50225, val loss: 1.57431\n",
      "Main effects training epoch: 208, train loss: 1.50469, val loss: 1.59505\n",
      "Main effects training epoch: 209, train loss: 1.49897, val loss: 1.58897\n",
      "Main effects training epoch: 210, train loss: 1.50055, val loss: 1.57327\n",
      "Main effects training epoch: 211, train loss: 1.51061, val loss: 1.59110\n",
      "Main effects training epoch: 212, train loss: 1.50414, val loss: 1.57268\n",
      "Main effects training epoch: 213, train loss: 1.50270, val loss: 1.59163\n",
      "Main effects training epoch: 214, train loss: 1.49817, val loss: 1.57566\n",
      "Main effects training epoch: 215, train loss: 1.49571, val loss: 1.57164\n",
      "Main effects training epoch: 216, train loss: 1.49560, val loss: 1.57660\n",
      "Main effects training epoch: 217, train loss: 1.49150, val loss: 1.57137\n",
      "Main effects training epoch: 218, train loss: 1.49647, val loss: 1.58332\n",
      "Main effects training epoch: 219, train loss: 1.49084, val loss: 1.56828\n",
      "Main effects training epoch: 220, train loss: 1.49340, val loss: 1.57736\n",
      "Main effects training epoch: 221, train loss: 1.50009, val loss: 1.56919\n",
      "Main effects training epoch: 222, train loss: 1.49716, val loss: 1.58686\n",
      "Main effects training epoch: 223, train loss: 1.49415, val loss: 1.56762\n",
      "Main effects training epoch: 224, train loss: 1.49731, val loss: 1.57543\n",
      "Main effects training epoch: 225, train loss: 1.49069, val loss: 1.56735\n",
      "Main effects training epoch: 226, train loss: 1.48744, val loss: 1.56514\n",
      "Main effects training epoch: 227, train loss: 1.48606, val loss: 1.56522\n",
      "Main effects training epoch: 228, train loss: 1.48761, val loss: 1.57127\n",
      "Main effects training epoch: 229, train loss: 1.48698, val loss: 1.56841\n",
      "Main effects training epoch: 230, train loss: 1.49084, val loss: 1.57460\n",
      "Main effects training epoch: 231, train loss: 1.48533, val loss: 1.56179\n",
      "Main effects training epoch: 232, train loss: 1.48691, val loss: 1.56537\n",
      "Main effects training epoch: 233, train loss: 1.49704, val loss: 1.57297\n",
      "Main effects training epoch: 234, train loss: 1.48505, val loss: 1.57149\n",
      "Main effects training epoch: 235, train loss: 1.49092, val loss: 1.56129\n",
      "Main effects training epoch: 236, train loss: 1.48097, val loss: 1.56570\n",
      "Main effects training epoch: 237, train loss: 1.48239, val loss: 1.55212\n",
      "Main effects training epoch: 238, train loss: 1.48273, val loss: 1.55997\n",
      "Main effects training epoch: 239, train loss: 1.48013, val loss: 1.55569\n",
      "Main effects training epoch: 240, train loss: 1.48035, val loss: 1.56105\n",
      "Main effects training epoch: 241, train loss: 1.48436, val loss: 1.55253\n",
      "Main effects training epoch: 242, train loss: 1.48507, val loss: 1.55589\n",
      "Main effects training epoch: 243, train loss: 1.48884, val loss: 1.56125\n",
      "Main effects training epoch: 244, train loss: 1.48401, val loss: 1.57373\n",
      "Main effects training epoch: 245, train loss: 1.49051, val loss: 1.55337\n",
      "Main effects training epoch: 246, train loss: 1.48160, val loss: 1.56023\n",
      "Main effects training epoch: 247, train loss: 1.47777, val loss: 1.55312\n",
      "Main effects training epoch: 248, train loss: 1.49156, val loss: 1.55815\n",
      "Main effects training epoch: 249, train loss: 1.48098, val loss: 1.55937\n",
      "Main effects training epoch: 250, train loss: 1.48385, val loss: 1.56199\n",
      "Main effects training epoch: 251, train loss: 1.48659, val loss: 1.56128\n",
      "Main effects training epoch: 252, train loss: 1.47685, val loss: 1.56157\n",
      "Main effects training epoch: 253, train loss: 1.48808, val loss: 1.54859\n",
      "Main effects training epoch: 254, train loss: 1.48407, val loss: 1.55806\n",
      "Main effects training epoch: 255, train loss: 1.48563, val loss: 1.55742\n",
      "Main effects training epoch: 256, train loss: 1.47918, val loss: 1.54952\n",
      "Main effects training epoch: 257, train loss: 1.47606, val loss: 1.55121\n",
      "Main effects training epoch: 258, train loss: 1.47372, val loss: 1.54436\n",
      "Main effects training epoch: 259, train loss: 1.47893, val loss: 1.55576\n",
      "Main effects training epoch: 260, train loss: 1.47852, val loss: 1.54907\n",
      "Main effects training epoch: 261, train loss: 1.49550, val loss: 1.55619\n",
      "Main effects training epoch: 262, train loss: 1.48585, val loss: 1.56521\n",
      "Main effects training epoch: 263, train loss: 1.47815, val loss: 1.52881\n",
      "Main effects training epoch: 264, train loss: 1.46899, val loss: 1.55029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 265, train loss: 1.46937, val loss: 1.54180\n",
      "Main effects training epoch: 266, train loss: 1.47614, val loss: 1.54285\n",
      "Main effects training epoch: 267, train loss: 1.46799, val loss: 1.53840\n",
      "Main effects training epoch: 268, train loss: 1.46506, val loss: 1.53744\n",
      "Main effects training epoch: 269, train loss: 1.46442, val loss: 1.53535\n",
      "Main effects training epoch: 270, train loss: 1.46777, val loss: 1.52940\n",
      "Main effects training epoch: 271, train loss: 1.46353, val loss: 1.53767\n",
      "Main effects training epoch: 272, train loss: 1.47036, val loss: 1.52646\n",
      "Main effects training epoch: 273, train loss: 1.46460, val loss: 1.53165\n",
      "Main effects training epoch: 274, train loss: 1.46643, val loss: 1.51927\n",
      "Main effects training epoch: 275, train loss: 1.46682, val loss: 1.54881\n",
      "Main effects training epoch: 276, train loss: 1.47766, val loss: 1.54330\n",
      "Main effects training epoch: 277, train loss: 1.46014, val loss: 1.53089\n",
      "Main effects training epoch: 278, train loss: 1.46303, val loss: 1.53656\n",
      "Main effects training epoch: 279, train loss: 1.46443, val loss: 1.52143\n",
      "Main effects training epoch: 280, train loss: 1.46422, val loss: 1.54040\n",
      "Main effects training epoch: 281, train loss: 1.45312, val loss: 1.51418\n",
      "Main effects training epoch: 282, train loss: 1.45666, val loss: 1.52099\n",
      "Main effects training epoch: 283, train loss: 1.45174, val loss: 1.51592\n",
      "Main effects training epoch: 284, train loss: 1.46646, val loss: 1.51831\n",
      "Main effects training epoch: 285, train loss: 1.45664, val loss: 1.51366\n",
      "Main effects training epoch: 286, train loss: 1.45831, val loss: 1.50973\n",
      "Main effects training epoch: 287, train loss: 1.46764, val loss: 1.54868\n",
      "Main effects training epoch: 288, train loss: 1.46280, val loss: 1.50655\n",
      "Main effects training epoch: 289, train loss: 1.45112, val loss: 1.52111\n",
      "Main effects training epoch: 290, train loss: 1.45658, val loss: 1.51546\n",
      "Main effects training epoch: 291, train loss: 1.45599, val loss: 1.51696\n",
      "Main effects training epoch: 292, train loss: 1.44961, val loss: 1.51482\n",
      "Main effects training epoch: 293, train loss: 1.46067, val loss: 1.51332\n",
      "Main effects training epoch: 294, train loss: 1.47385, val loss: 1.51202\n",
      "Main effects training epoch: 295, train loss: 1.45615, val loss: 1.52349\n",
      "Main effects training epoch: 296, train loss: 1.47318, val loss: 1.53020\n",
      "Main effects training epoch: 297, train loss: 1.46001, val loss: 1.50907\n",
      "Main effects training epoch: 298, train loss: 1.47136, val loss: 1.51133\n",
      "Main effects training epoch: 299, train loss: 1.46982, val loss: 1.53516\n",
      "Main effects training epoch: 300, train loss: 1.44960, val loss: 1.50548\n",
      "##########Stage 1: main effect training stop.##########\n",
      "3 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.47981, val loss: 1.52267\n",
      "Main effects tuning epoch: 2, train loss: 1.46331, val loss: 1.51472\n",
      "Main effects tuning epoch: 3, train loss: 1.46830, val loss: 1.51161\n",
      "Main effects tuning epoch: 4, train loss: 1.46896, val loss: 1.51178\n",
      "Main effects tuning epoch: 5, train loss: 1.47528, val loss: 1.51366\n",
      "Main effects tuning epoch: 6, train loss: 1.46808, val loss: 1.51692\n",
      "Main effects tuning epoch: 7, train loss: 1.46537, val loss: 1.51874\n",
      "Main effects tuning epoch: 8, train loss: 1.47975, val loss: 1.50987\n",
      "Main effects tuning epoch: 9, train loss: 1.46065, val loss: 1.51071\n",
      "Main effects tuning epoch: 10, train loss: 1.46917, val loss: 1.50453\n",
      "Main effects tuning epoch: 11, train loss: 1.47333, val loss: 1.50563\n",
      "Main effects tuning epoch: 12, train loss: 1.46157, val loss: 1.50754\n",
      "Main effects tuning epoch: 13, train loss: 1.48088, val loss: 1.52535\n",
      "Main effects tuning epoch: 14, train loss: 1.46625, val loss: 1.51370\n",
      "Main effects tuning epoch: 15, train loss: 1.46376, val loss: 1.51038\n",
      "Main effects tuning epoch: 16, train loss: 1.47410, val loss: 1.50254\n",
      "Main effects tuning epoch: 17, train loss: 1.46502, val loss: 1.52004\n",
      "Main effects tuning epoch: 18, train loss: 1.46385, val loss: 1.49553\n",
      "Main effects tuning epoch: 19, train loss: 1.46368, val loss: 1.50912\n",
      "Main effects tuning epoch: 20, train loss: 1.46234, val loss: 1.50258\n",
      "Main effects tuning epoch: 21, train loss: 1.46433, val loss: 1.49670\n",
      "Main effects tuning epoch: 22, train loss: 1.46370, val loss: 1.49430\n",
      "Main effects tuning epoch: 23, train loss: 1.46845, val loss: 1.49861\n",
      "Main effects tuning epoch: 24, train loss: 1.46385, val loss: 1.50414\n",
      "Main effects tuning epoch: 25, train loss: 1.45635, val loss: 1.50218\n",
      "Main effects tuning epoch: 26, train loss: 1.45837, val loss: 1.49553\n",
      "Main effects tuning epoch: 27, train loss: 1.46347, val loss: 1.50165\n",
      "Main effects tuning epoch: 28, train loss: 1.46517, val loss: 1.49487\n",
      "Main effects tuning epoch: 29, train loss: 1.45550, val loss: 1.49383\n",
      "Main effects tuning epoch: 30, train loss: 1.45760, val loss: 1.50101\n",
      "Main effects tuning epoch: 31, train loss: 1.47132, val loss: 1.49669\n",
      "Main effects tuning epoch: 32, train loss: 1.45995, val loss: 1.49988\n",
      "Main effects tuning epoch: 33, train loss: 1.47518, val loss: 1.50785\n",
      "Main effects tuning epoch: 34, train loss: 1.45653, val loss: 1.49390\n",
      "Main effects tuning epoch: 35, train loss: 1.46586, val loss: 1.48579\n",
      "Main effects tuning epoch: 36, train loss: 1.45584, val loss: 1.50565\n",
      "Main effects tuning epoch: 37, train loss: 1.45403, val loss: 1.48898\n",
      "Main effects tuning epoch: 38, train loss: 1.45523, val loss: 1.49846\n",
      "Main effects tuning epoch: 39, train loss: 1.45269, val loss: 1.48773\n",
      "Main effects tuning epoch: 40, train loss: 1.45165, val loss: 1.49717\n",
      "Main effects tuning epoch: 41, train loss: 1.45082, val loss: 1.49373\n",
      "Main effects tuning epoch: 42, train loss: 1.45648, val loss: 1.49967\n",
      "Main effects tuning epoch: 43, train loss: 1.45135, val loss: 1.48556\n",
      "Main effects tuning epoch: 44, train loss: 1.45106, val loss: 1.48879\n",
      "Main effects tuning epoch: 45, train loss: 1.45430, val loss: 1.49438\n",
      "Main effects tuning epoch: 46, train loss: 1.44805, val loss: 1.47819\n",
      "Main effects tuning epoch: 47, train loss: 1.45053, val loss: 1.49323\n",
      "Main effects tuning epoch: 48, train loss: 1.45437, val loss: 1.49447\n",
      "Main effects tuning epoch: 49, train loss: 1.46905, val loss: 1.50304\n",
      "Main effects tuning epoch: 50, train loss: 1.45347, val loss: 1.49706\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.38751, val loss: 1.42972\n",
      "Interaction training epoch: 2, train loss: 1.29788, val loss: 1.29251\n",
      "Interaction training epoch: 3, train loss: 1.29616, val loss: 1.32320\n",
      "Interaction training epoch: 4, train loss: 1.04085, val loss: 1.05417\n",
      "Interaction training epoch: 5, train loss: 1.03009, val loss: 1.03709\n",
      "Interaction training epoch: 6, train loss: 1.05054, val loss: 1.05218\n",
      "Interaction training epoch: 7, train loss: 1.02948, val loss: 1.02686\n",
      "Interaction training epoch: 8, train loss: 0.99902, val loss: 1.00530\n",
      "Interaction training epoch: 9, train loss: 0.98387, val loss: 0.98691\n",
      "Interaction training epoch: 10, train loss: 0.99080, val loss: 1.01337\n",
      "Interaction training epoch: 11, train loss: 0.96049, val loss: 0.96233\n",
      "Interaction training epoch: 12, train loss: 0.94945, val loss: 0.94754\n",
      "Interaction training epoch: 13, train loss: 0.95982, val loss: 0.96210\n",
      "Interaction training epoch: 14, train loss: 0.95414, val loss: 0.94884\n",
      "Interaction training epoch: 15, train loss: 0.94345, val loss: 0.94976\n",
      "Interaction training epoch: 16, train loss: 0.93793, val loss: 0.92820\n",
      "Interaction training epoch: 17, train loss: 0.93178, val loss: 0.93546\n",
      "Interaction training epoch: 18, train loss: 0.91425, val loss: 0.89675\n",
      "Interaction training epoch: 19, train loss: 0.91632, val loss: 0.92731\n",
      "Interaction training epoch: 20, train loss: 0.92236, val loss: 0.91942\n",
      "Interaction training epoch: 21, train loss: 0.90920, val loss: 0.90217\n",
      "Interaction training epoch: 22, train loss: 0.90385, val loss: 0.89343\n",
      "Interaction training epoch: 23, train loss: 0.89987, val loss: 0.89575\n",
      "Interaction training epoch: 24, train loss: 0.90721, val loss: 0.89862\n",
      "Interaction training epoch: 25, train loss: 0.92542, val loss: 0.91327\n",
      "Interaction training epoch: 26, train loss: 0.90072, val loss: 0.91308\n",
      "Interaction training epoch: 27, train loss: 0.90686, val loss: 0.91623\n",
      "Interaction training epoch: 28, train loss: 0.89684, val loss: 0.88755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 29, train loss: 0.89574, val loss: 0.88784\n",
      "Interaction training epoch: 30, train loss: 0.89629, val loss: 0.90104\n",
      "Interaction training epoch: 31, train loss: 0.88658, val loss: 0.88759\n",
      "Interaction training epoch: 32, train loss: 0.89231, val loss: 0.87914\n",
      "Interaction training epoch: 33, train loss: 0.88231, val loss: 0.88828\n",
      "Interaction training epoch: 34, train loss: 0.88436, val loss: 0.87844\n",
      "Interaction training epoch: 35, train loss: 0.89044, val loss: 0.88305\n",
      "Interaction training epoch: 36, train loss: 0.88164, val loss: 0.88330\n",
      "Interaction training epoch: 37, train loss: 0.87723, val loss: 0.87402\n",
      "Interaction training epoch: 38, train loss: 0.87424, val loss: 0.87164\n",
      "Interaction training epoch: 39, train loss: 0.88113, val loss: 0.88409\n",
      "Interaction training epoch: 40, train loss: 0.87380, val loss: 0.86950\n",
      "Interaction training epoch: 41, train loss: 0.87787, val loss: 0.87971\n",
      "Interaction training epoch: 42, train loss: 0.87474, val loss: 0.86993\n",
      "Interaction training epoch: 43, train loss: 0.87210, val loss: 0.86398\n",
      "Interaction training epoch: 44, train loss: 0.86534, val loss: 0.86351\n",
      "Interaction training epoch: 45, train loss: 0.87172, val loss: 0.86373\n",
      "Interaction training epoch: 46, train loss: 0.86930, val loss: 0.86788\n",
      "Interaction training epoch: 47, train loss: 0.87310, val loss: 0.86793\n",
      "Interaction training epoch: 48, train loss: 0.87626, val loss: 0.87352\n",
      "Interaction training epoch: 49, train loss: 0.86374, val loss: 0.86011\n",
      "Interaction training epoch: 50, train loss: 0.86237, val loss: 0.86165\n",
      "Interaction training epoch: 51, train loss: 0.85959, val loss: 0.85691\n",
      "Interaction training epoch: 52, train loss: 0.87314, val loss: 0.87168\n",
      "Interaction training epoch: 53, train loss: 0.86557, val loss: 0.85709\n",
      "Interaction training epoch: 54, train loss: 0.88728, val loss: 0.89249\n",
      "Interaction training epoch: 55, train loss: 0.86669, val loss: 0.86887\n",
      "Interaction training epoch: 56, train loss: 0.87323, val loss: 0.87294\n",
      "Interaction training epoch: 57, train loss: 0.85961, val loss: 0.85659\n",
      "Interaction training epoch: 58, train loss: 0.86651, val loss: 0.86275\n",
      "Interaction training epoch: 59, train loss: 0.86838, val loss: 0.87083\n",
      "Interaction training epoch: 60, train loss: 0.87023, val loss: 0.86088\n",
      "Interaction training epoch: 61, train loss: 0.85704, val loss: 0.85348\n",
      "Interaction training epoch: 62, train loss: 0.85447, val loss: 0.85588\n",
      "Interaction training epoch: 63, train loss: 0.85908, val loss: 0.85408\n",
      "Interaction training epoch: 64, train loss: 0.86018, val loss: 0.86011\n",
      "Interaction training epoch: 65, train loss: 0.85534, val loss: 0.84887\n",
      "Interaction training epoch: 66, train loss: 0.85576, val loss: 0.85693\n",
      "Interaction training epoch: 67, train loss: 0.85598, val loss: 0.84631\n",
      "Interaction training epoch: 68, train loss: 0.85123, val loss: 0.85383\n",
      "Interaction training epoch: 69, train loss: 0.84728, val loss: 0.84808\n",
      "Interaction training epoch: 70, train loss: 0.85633, val loss: 0.85398\n",
      "Interaction training epoch: 71, train loss: 0.86387, val loss: 0.85482\n",
      "Interaction training epoch: 72, train loss: 0.86148, val loss: 0.86960\n",
      "Interaction training epoch: 73, train loss: 0.85043, val loss: 0.84455\n",
      "Interaction training epoch: 74, train loss: 0.85069, val loss: 0.85530\n",
      "Interaction training epoch: 75, train loss: 0.84891, val loss: 0.84545\n",
      "Interaction training epoch: 76, train loss: 0.86129, val loss: 0.86666\n",
      "Interaction training epoch: 77, train loss: 0.84883, val loss: 0.84464\n",
      "Interaction training epoch: 78, train loss: 0.84876, val loss: 0.85347\n",
      "Interaction training epoch: 79, train loss: 0.84878, val loss: 0.84226\n",
      "Interaction training epoch: 80, train loss: 0.85118, val loss: 0.85842\n",
      "Interaction training epoch: 81, train loss: 0.85010, val loss: 0.84725\n",
      "Interaction training epoch: 82, train loss: 0.85496, val loss: 0.85152\n",
      "Interaction training epoch: 83, train loss: 0.84734, val loss: 0.84694\n",
      "Interaction training epoch: 84, train loss: 0.83982, val loss: 0.83801\n",
      "Interaction training epoch: 85, train loss: 0.85011, val loss: 0.84656\n",
      "Interaction training epoch: 86, train loss: 0.85688, val loss: 0.84358\n",
      "Interaction training epoch: 87, train loss: 0.86353, val loss: 0.86145\n",
      "Interaction training epoch: 88, train loss: 0.85386, val loss: 0.84938\n",
      "Interaction training epoch: 89, train loss: 0.84869, val loss: 0.85860\n",
      "Interaction training epoch: 90, train loss: 0.85809, val loss: 0.85626\n",
      "Interaction training epoch: 91, train loss: 0.84737, val loss: 0.84347\n",
      "Interaction training epoch: 92, train loss: 0.88299, val loss: 0.87539\n",
      "Interaction training epoch: 93, train loss: 0.85565, val loss: 0.84627\n",
      "Interaction training epoch: 94, train loss: 0.86188, val loss: 0.87089\n",
      "Interaction training epoch: 95, train loss: 0.86285, val loss: 0.85021\n",
      "Interaction training epoch: 96, train loss: 0.85232, val loss: 0.85564\n",
      "Interaction training epoch: 97, train loss: 0.84748, val loss: 0.84804\n",
      "Interaction training epoch: 98, train loss: 0.84486, val loss: 0.84667\n",
      "Interaction training epoch: 99, train loss: 0.85361, val loss: 0.85675\n",
      "Interaction training epoch: 100, train loss: 0.84448, val loss: 0.84568\n",
      "Interaction training epoch: 101, train loss: 0.84034, val loss: 0.84930\n",
      "Interaction training epoch: 102, train loss: 0.84522, val loss: 0.84170\n",
      "Interaction training epoch: 103, train loss: 0.84307, val loss: 0.84549\n",
      "Interaction training epoch: 104, train loss: 0.84887, val loss: 0.84818\n",
      "Interaction training epoch: 105, train loss: 0.84111, val loss: 0.84438\n",
      "Interaction training epoch: 106, train loss: 0.84481, val loss: 0.85072\n",
      "Interaction training epoch: 107, train loss: 0.83311, val loss: 0.83184\n",
      "Interaction training epoch: 108, train loss: 0.84402, val loss: 0.83933\n",
      "Interaction training epoch: 109, train loss: 0.84446, val loss: 0.84765\n",
      "Interaction training epoch: 110, train loss: 0.84550, val loss: 0.83716\n",
      "Interaction training epoch: 111, train loss: 0.84113, val loss: 0.85107\n",
      "Interaction training epoch: 112, train loss: 0.83914, val loss: 0.84731\n",
      "Interaction training epoch: 113, train loss: 0.83822, val loss: 0.83572\n",
      "Interaction training epoch: 114, train loss: 0.84078, val loss: 0.85020\n",
      "Interaction training epoch: 115, train loss: 0.83684, val loss: 0.84174\n",
      "Interaction training epoch: 116, train loss: 0.84198, val loss: 0.84449\n",
      "Interaction training epoch: 117, train loss: 0.84988, val loss: 0.85086\n",
      "Interaction training epoch: 118, train loss: 0.84723, val loss: 0.84895\n",
      "Interaction training epoch: 119, train loss: 0.84825, val loss: 0.85438\n",
      "Interaction training epoch: 120, train loss: 0.83353, val loss: 0.84249\n",
      "Interaction training epoch: 121, train loss: 0.84568, val loss: 0.84169\n",
      "Interaction training epoch: 122, train loss: 0.84632, val loss: 0.84663\n",
      "Interaction training epoch: 123, train loss: 0.84193, val loss: 0.84624\n",
      "Interaction training epoch: 124, train loss: 0.82580, val loss: 0.82487\n",
      "Interaction training epoch: 125, train loss: 0.84564, val loss: 0.85498\n",
      "Interaction training epoch: 126, train loss: 0.84185, val loss: 0.85316\n",
      "Interaction training epoch: 127, train loss: 0.82878, val loss: 0.83275\n",
      "Interaction training epoch: 128, train loss: 0.82934, val loss: 0.83971\n",
      "Interaction training epoch: 129, train loss: 0.84837, val loss: 0.85086\n",
      "Interaction training epoch: 130, train loss: 0.83089, val loss: 0.84414\n",
      "Interaction training epoch: 131, train loss: 0.83600, val loss: 0.84247\n",
      "Interaction training epoch: 132, train loss: 0.83097, val loss: 0.83826\n",
      "Interaction training epoch: 133, train loss: 0.83711, val loss: 0.84705\n",
      "Interaction training epoch: 134, train loss: 0.83164, val loss: 0.83379\n",
      "Interaction training epoch: 135, train loss: 0.82984, val loss: 0.83505\n",
      "Interaction training epoch: 136, train loss: 0.83540, val loss: 0.82933\n",
      "Interaction training epoch: 137, train loss: 0.83534, val loss: 0.84802\n",
      "Interaction training epoch: 138, train loss: 0.84032, val loss: 0.84233\n",
      "Interaction training epoch: 139, train loss: 0.82736, val loss: 0.83227\n",
      "Interaction training epoch: 140, train loss: 0.82745, val loss: 0.83496\n",
      "Interaction training epoch: 141, train loss: 0.82917, val loss: 0.83139\n",
      "Interaction training epoch: 142, train loss: 0.83277, val loss: 0.83556\n",
      "Interaction training epoch: 143, train loss: 0.83873, val loss: 0.83588\n",
      "Interaction training epoch: 144, train loss: 0.83220, val loss: 0.84184\n",
      "Interaction training epoch: 145, train loss: 0.83137, val loss: 0.83206\n",
      "Interaction training epoch: 146, train loss: 0.83064, val loss: 0.83581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 147, train loss: 0.83673, val loss: 0.84164\n",
      "Interaction training epoch: 148, train loss: 0.83926, val loss: 0.83894\n",
      "Interaction training epoch: 149, train loss: 0.82296, val loss: 0.82527\n",
      "Interaction training epoch: 150, train loss: 0.83546, val loss: 0.83520\n",
      "Interaction training epoch: 151, train loss: 0.83130, val loss: 0.83714\n",
      "Interaction training epoch: 152, train loss: 0.83576, val loss: 0.84786\n",
      "Interaction training epoch: 153, train loss: 0.84551, val loss: 0.85060\n",
      "Interaction training epoch: 154, train loss: 0.83219, val loss: 0.82960\n",
      "Interaction training epoch: 155, train loss: 0.83138, val loss: 0.84068\n",
      "Interaction training epoch: 156, train loss: 0.82575, val loss: 0.82219\n",
      "Interaction training epoch: 157, train loss: 0.83437, val loss: 0.83265\n",
      "Interaction training epoch: 158, train loss: 0.83791, val loss: 0.84165\n",
      "Interaction training epoch: 159, train loss: 0.82527, val loss: 0.81928\n",
      "Interaction training epoch: 160, train loss: 0.84681, val loss: 0.84812\n",
      "Interaction training epoch: 161, train loss: 0.84271, val loss: 0.83922\n",
      "Interaction training epoch: 162, train loss: 0.82548, val loss: 0.83312\n",
      "Interaction training epoch: 163, train loss: 0.84106, val loss: 0.84076\n",
      "Interaction training epoch: 164, train loss: 0.83649, val loss: 0.84125\n",
      "Interaction training epoch: 165, train loss: 0.83301, val loss: 0.83403\n",
      "Interaction training epoch: 166, train loss: 0.82190, val loss: 0.81532\n",
      "Interaction training epoch: 167, train loss: 0.82201, val loss: 0.82740\n",
      "Interaction training epoch: 168, train loss: 0.82725, val loss: 0.82684\n",
      "Interaction training epoch: 169, train loss: 0.82403, val loss: 0.82240\n",
      "Interaction training epoch: 170, train loss: 0.82601, val loss: 0.82373\n",
      "Interaction training epoch: 171, train loss: 0.82168, val loss: 0.83330\n",
      "Interaction training epoch: 172, train loss: 0.83113, val loss: 0.82711\n",
      "Interaction training epoch: 173, train loss: 0.82023, val loss: 0.81578\n",
      "Interaction training epoch: 174, train loss: 0.82378, val loss: 0.82676\n",
      "Interaction training epoch: 175, train loss: 0.82887, val loss: 0.83708\n",
      "Interaction training epoch: 176, train loss: 0.82882, val loss: 0.83049\n",
      "Interaction training epoch: 177, train loss: 0.83148, val loss: 0.84407\n",
      "Interaction training epoch: 178, train loss: 0.82600, val loss: 0.82280\n",
      "Interaction training epoch: 179, train loss: 0.83135, val loss: 0.83668\n",
      "Interaction training epoch: 180, train loss: 0.83581, val loss: 0.84194\n",
      "Interaction training epoch: 181, train loss: 0.82044, val loss: 0.82287\n",
      "Interaction training epoch: 182, train loss: 0.82135, val loss: 0.82401\n",
      "Interaction training epoch: 183, train loss: 0.82940, val loss: 0.82729\n",
      "Interaction training epoch: 184, train loss: 0.82159, val loss: 0.82257\n",
      "Interaction training epoch: 185, train loss: 0.82134, val loss: 0.81587\n",
      "Interaction training epoch: 186, train loss: 0.82022, val loss: 0.81890\n",
      "Interaction training epoch: 187, train loss: 0.82263, val loss: 0.82265\n",
      "Interaction training epoch: 188, train loss: 0.82953, val loss: 0.83362\n",
      "Interaction training epoch: 189, train loss: 0.83232, val loss: 0.83031\n",
      "Interaction training epoch: 190, train loss: 0.81332, val loss: 0.81501\n",
      "Interaction training epoch: 191, train loss: 0.81952, val loss: 0.82189\n",
      "Interaction training epoch: 192, train loss: 0.82166, val loss: 0.82530\n",
      "Interaction training epoch: 193, train loss: 0.83151, val loss: 0.84141\n",
      "Interaction training epoch: 194, train loss: 0.82638, val loss: 0.81517\n",
      "Interaction training epoch: 195, train loss: 0.83199, val loss: 0.82848\n",
      "Interaction training epoch: 196, train loss: 0.82739, val loss: 0.83803\n",
      "Interaction training epoch: 197, train loss: 0.82264, val loss: 0.82194\n",
      "Interaction training epoch: 198, train loss: 0.82310, val loss: 0.83420\n",
      "Interaction training epoch: 199, train loss: 0.81675, val loss: 0.81584\n",
      "Interaction training epoch: 200, train loss: 0.83348, val loss: 0.82797\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########1 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.82371, val loss: 0.82202\n",
      "Interaction tuning epoch: 2, train loss: 0.82096, val loss: 0.82463\n",
      "Interaction tuning epoch: 3, train loss: 0.81892, val loss: 0.82484\n",
      "Interaction tuning epoch: 4, train loss: 0.82479, val loss: 0.82251\n",
      "Interaction tuning epoch: 5, train loss: 0.81839, val loss: 0.82672\n",
      "Interaction tuning epoch: 6, train loss: 0.81689, val loss: 0.81462\n",
      "Interaction tuning epoch: 7, train loss: 0.81898, val loss: 0.82051\n",
      "Interaction tuning epoch: 8, train loss: 0.81312, val loss: 0.81754\n",
      "Interaction tuning epoch: 9, train loss: 0.81539, val loss: 0.80975\n",
      "Interaction tuning epoch: 10, train loss: 0.81745, val loss: 0.82210\n",
      "Interaction tuning epoch: 11, train loss: 0.81418, val loss: 0.82079\n",
      "Interaction tuning epoch: 12, train loss: 0.82485, val loss: 0.82715\n",
      "Interaction tuning epoch: 13, train loss: 0.81825, val loss: 0.81712\n",
      "Interaction tuning epoch: 14, train loss: 0.82252, val loss: 0.82478\n",
      "Interaction tuning epoch: 15, train loss: 0.82245, val loss: 0.82111\n",
      "Interaction tuning epoch: 16, train loss: 0.81510, val loss: 0.81301\n",
      "Interaction tuning epoch: 17, train loss: 0.84084, val loss: 0.84370\n",
      "Interaction tuning epoch: 18, train loss: 0.82361, val loss: 0.83073\n",
      "Interaction tuning epoch: 19, train loss: 0.81988, val loss: 0.81411\n",
      "Interaction tuning epoch: 20, train loss: 0.82053, val loss: 0.82244\n",
      "Interaction tuning epoch: 21, train loss: 0.81551, val loss: 0.81523\n",
      "Interaction tuning epoch: 22, train loss: 0.82296, val loss: 0.82491\n",
      "Interaction tuning epoch: 23, train loss: 0.81978, val loss: 0.81538\n",
      "Interaction tuning epoch: 24, train loss: 0.81054, val loss: 0.81135\n",
      "Interaction tuning epoch: 25, train loss: 0.81548, val loss: 0.81068\n",
      "Interaction tuning epoch: 26, train loss: 0.81618, val loss: 0.81822\n",
      "Interaction tuning epoch: 27, train loss: 0.80571, val loss: 0.81150\n",
      "Interaction tuning epoch: 28, train loss: 0.81969, val loss: 0.82267\n",
      "Interaction tuning epoch: 29, train loss: 0.81576, val loss: 0.80756\n",
      "Interaction tuning epoch: 30, train loss: 0.81227, val loss: 0.81788\n",
      "Interaction tuning epoch: 31, train loss: 0.82466, val loss: 0.81669\n",
      "Interaction tuning epoch: 32, train loss: 0.82289, val loss: 0.82492\n",
      "Interaction tuning epoch: 33, train loss: 0.81647, val loss: 0.81767\n",
      "Interaction tuning epoch: 34, train loss: 0.82142, val loss: 0.82184\n",
      "Interaction tuning epoch: 35, train loss: 0.84295, val loss: 0.83084\n",
      "Interaction tuning epoch: 36, train loss: 0.81647, val loss: 0.81656\n",
      "Interaction tuning epoch: 37, train loss: 0.81434, val loss: 0.80192\n",
      "Interaction tuning epoch: 38, train loss: 0.81647, val loss: 0.81865\n",
      "Interaction tuning epoch: 39, train loss: 0.80969, val loss: 0.80605\n",
      "Interaction tuning epoch: 40, train loss: 0.81152, val loss: 0.81092\n",
      "Interaction tuning epoch: 41, train loss: 0.81000, val loss: 0.80475\n",
      "Interaction tuning epoch: 42, train loss: 0.80874, val loss: 0.80624\n",
      "Interaction tuning epoch: 43, train loss: 0.80997, val loss: 0.80494\n",
      "Interaction tuning epoch: 44, train loss: 0.81356, val loss: 0.80658\n",
      "Interaction tuning epoch: 45, train loss: 0.81452, val loss: 0.81623\n",
      "Interaction tuning epoch: 46, train loss: 0.80775, val loss: 0.81179\n",
      "Interaction tuning epoch: 47, train loss: 0.81477, val loss: 0.80927\n",
      "Interaction tuning epoch: 48, train loss: 0.81802, val loss: 0.81413\n",
      "Interaction tuning epoch: 49, train loss: 0.81307, val loss: 0.81744\n",
      "Interaction tuning epoch: 50, train loss: 0.81885, val loss: 0.80675\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 37.37449359893799\n",
      "After the gam stage, training error is 0.81885 , validation error is 0.80675\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 20.277104\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.676198 validation MAE=0.762506,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.628743 validation MAE=0.741494,rank=5\n",
      "[SoftImpute] Iter 3: observed MAE=0.589156 validation MAE=0.724039,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.555485 validation MAE=0.708972,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.526486 validation MAE=0.694986,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.500173 validation MAE=0.681519,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.477215 validation MAE=0.669432,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.457474 validation MAE=0.658522,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.440192 validation MAE=0.648503,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.425215 validation MAE=0.639140,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.411393 validation MAE=0.630897,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.399589 validation MAE=0.623139,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.388829 validation MAE=0.616523,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.379322 validation MAE=0.610143,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.369248 validation MAE=0.604673,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.360159 validation MAE=0.599419,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.351989 validation MAE=0.594829,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.345365 validation MAE=0.590140,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.338475 validation MAE=0.585370,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.331230 validation MAE=0.581356,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.325536 validation MAE=0.577612,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.319234 validation MAE=0.573940,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.314238 validation MAE=0.570313,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.308966 validation MAE=0.567112,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.304751 validation MAE=0.563955,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.300418 validation MAE=0.560679,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.296329 validation MAE=0.557846,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.292372 validation MAE=0.554543,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.288367 validation MAE=0.552269,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.284731 validation MAE=0.550445,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.282277 validation MAE=0.547951,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.278395 validation MAE=0.545834,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.274939 validation MAE=0.543175,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.272268 validation MAE=0.541084,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.269911 validation MAE=0.539508,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.267134 validation MAE=0.537671,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.264899 validation MAE=0.536318,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.262187 validation MAE=0.534754,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.260864 validation MAE=0.533205,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.258306 validation MAE=0.531900,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.256069 validation MAE=0.530532,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.254243 validation MAE=0.528992,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.251914 validation MAE=0.527657,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.250140 validation MAE=0.527157,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.248716 validation MAE=0.526397,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.246935 validation MAE=0.525104,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.245779 validation MAE=0.523931,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.243037 validation MAE=0.523124,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.241688 validation MAE=0.522574,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.241469 validation MAE=0.521343,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.239220 validation MAE=0.520354,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.238261 validation MAE=0.519116,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.236031 validation MAE=0.518979,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.234647 validation MAE=0.518049,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.234726 validation MAE=0.516890,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.233006 validation MAE=0.516739,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.232010 validation MAE=0.515257,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.230595 validation MAE=0.514474,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.228754 validation MAE=0.513848,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.227520 validation MAE=0.512709,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.227230 validation MAE=0.512344,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.226405 validation MAE=0.511828,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.225047 validation MAE=0.510286,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.222819 validation MAE=0.509903,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.222750 validation MAE=0.508841,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.221497 validation MAE=0.508213,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.221046 validation MAE=0.507871,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.219607 validation MAE=0.507288,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.219580 validation MAE=0.506279,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.218247 validation MAE=0.505874,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.217194 validation MAE=0.504951,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.215442 validation MAE=0.504326,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.215513 validation MAE=0.503149,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.214581 validation MAE=0.502801,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.213988 validation MAE=0.501930,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.212331 validation MAE=0.501877,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.212091 validation MAE=0.500762,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.210730 validation MAE=0.500470,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.210616 validation MAE=0.499838,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.210009 validation MAE=0.499530,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.210415 validation MAE=0.498722,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.208931 validation MAE=0.498915,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.207775 validation MAE=0.498042,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.207539 validation MAE=0.498041,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.206916 validation MAE=0.497157,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.205278 validation MAE=0.496457,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.205189 validation MAE=0.495826,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.204542 validation MAE=0.495819,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.204224 validation MAE=0.495460,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.203826 validation MAE=0.494636,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.203821 validation MAE=0.494521,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.202476 validation MAE=0.494270,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.202079 validation MAE=0.493377,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.200999 validation MAE=0.492960,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.200365 validation MAE=0.492578,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.199632 validation MAE=0.492370,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.199760 validation MAE=0.492351,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.199287 validation MAE=0.491702,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.198313 validation MAE=0.491048,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.197833 validation MAE=0.490863,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.196981 validation MAE=0.490062,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.197313 validation MAE=0.489757,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.196093 validation MAE=0.489727,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.195593 validation MAE=0.488935,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.195086 validation MAE=0.488675,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.194973 validation MAE=0.488256,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.194445 validation MAE=0.487562,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.194499 validation MAE=0.487307,rank=5\n",
      "[SoftImpute] Iter 109: observed MAE=0.193289 validation MAE=0.487003,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.193307 validation MAE=0.486501,rank=5\n",
      "[SoftImpute] Iter 111: observed MAE=0.193096 validation MAE=0.486104,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 112: observed MAE=0.192767 validation MAE=0.486381,rank=5\n",
      "[SoftImpute] Iter 113: observed MAE=0.192545 validation MAE=0.485865,rank=5\n",
      "[SoftImpute] Iter 114: observed MAE=0.192390 validation MAE=0.485887,rank=5\n",
      "[SoftImpute] Iter 115: observed MAE=0.192454 validation MAE=0.485287,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.191338 validation MAE=0.484721,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.191454 validation MAE=0.484210,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.190378 validation MAE=0.484172,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.190355 validation MAE=0.483637,rank=5\n",
      "[SoftImpute] Iter 120: observed MAE=0.189666 validation MAE=0.482852,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.189020 validation MAE=0.483172,rank=5\n",
      "[SoftImpute] Iter 122: observed MAE=0.188948 validation MAE=0.482636,rank=5\n",
      "[SoftImpute] Iter 123: observed MAE=0.188558 validation MAE=0.482903,rank=5\n",
      "[SoftImpute] Iter 124: observed MAE=0.188393 validation MAE=0.482193,rank=5\n",
      "[SoftImpute] Iter 125: observed MAE=0.188176 validation MAE=0.481978,rank=5\n",
      "[SoftImpute] Iter 126: observed MAE=0.187753 validation MAE=0.481688,rank=5\n",
      "[SoftImpute] Iter 127: observed MAE=0.187231 validation MAE=0.481848,rank=5\n",
      "[SoftImpute] Iter 128: observed MAE=0.187195 validation MAE=0.480844,rank=5\n",
      "[SoftImpute] Iter 129: observed MAE=0.186716 validation MAE=0.480700,rank=5\n",
      "[SoftImpute] Iter 130: observed MAE=0.185864 validation MAE=0.480735,rank=5\n",
      "[SoftImpute] Iter 131: observed MAE=0.186194 validation MAE=0.480269,rank=5\n",
      "[SoftImpute] Iter 132: observed MAE=0.185578 validation MAE=0.479597,rank=5\n",
      "[SoftImpute] Iter 133: observed MAE=0.185680 validation MAE=0.479628,rank=5\n",
      "[SoftImpute] Iter 134: observed MAE=0.185019 validation MAE=0.479344,rank=5\n",
      "[SoftImpute] Iter 135: observed MAE=0.185367 validation MAE=0.479181,rank=5\n",
      "[SoftImpute] Iter 136: observed MAE=0.185055 validation MAE=0.478549,rank=5\n",
      "[SoftImpute] Iter 137: observed MAE=0.184993 validation MAE=0.478460,rank=5\n",
      "[SoftImpute] Iter 138: observed MAE=0.184168 validation MAE=0.478151,rank=5\n",
      "[SoftImpute] Iter 139: observed MAE=0.183854 validation MAE=0.478378,rank=5\n",
      "[SoftImpute] Iter 140: observed MAE=0.183936 validation MAE=0.477829,rank=5\n",
      "[SoftImpute] Iter 141: observed MAE=0.183668 validation MAE=0.477686,rank=5\n",
      "[SoftImpute] Iter 142: observed MAE=0.183283 validation MAE=0.477250,rank=5\n",
      "[SoftImpute] Iter 143: observed MAE=0.183156 validation MAE=0.477154,rank=5\n",
      "[SoftImpute] Iter 144: observed MAE=0.182708 validation MAE=0.476583,rank=5\n",
      "[SoftImpute] Iter 145: observed MAE=0.182571 validation MAE=0.476149,rank=5\n",
      "[SoftImpute] Iter 146: observed MAE=0.182407 validation MAE=0.475715,rank=5\n",
      "[SoftImpute] Iter 147: observed MAE=0.182166 validation MAE=0.475704,rank=5\n",
      "[SoftImpute] Iter 148: observed MAE=0.181822 validation MAE=0.475079,rank=5\n",
      "[SoftImpute] Iter 149: observed MAE=0.181224 validation MAE=0.475229,rank=5\n",
      "[SoftImpute] Iter 150: observed MAE=0.181099 validation MAE=0.474625,rank=5\n",
      "[SoftImpute] Iter 151: observed MAE=0.181118 validation MAE=0.474560,rank=5\n",
      "[SoftImpute] Iter 152: observed MAE=0.180800 validation MAE=0.474582,rank=5\n",
      "[SoftImpute] Iter 153: observed MAE=0.181068 validation MAE=0.474105,rank=5\n",
      "[SoftImpute] Iter 154: observed MAE=0.180020 validation MAE=0.473730,rank=5\n",
      "[SoftImpute] Iter 155: observed MAE=0.180047 validation MAE=0.473040,rank=5\n",
      "[SoftImpute] Iter 156: observed MAE=0.180320 validation MAE=0.473184,rank=5\n",
      "[SoftImpute] Iter 157: observed MAE=0.180101 validation MAE=0.472682,rank=5\n",
      "[SoftImpute] Iter 158: observed MAE=0.179313 validation MAE=0.472696,rank=5\n",
      "[SoftImpute] Iter 159: observed MAE=0.178476 validation MAE=0.472680,rank=5\n",
      "[SoftImpute] Iter 160: observed MAE=0.179266 validation MAE=0.472173,rank=5\n",
      "[SoftImpute] Iter 161: observed MAE=0.179117 validation MAE=0.471882,rank=5\n",
      "[SoftImpute] Iter 162: observed MAE=0.178832 validation MAE=0.471297,rank=5\n",
      "[SoftImpute] Iter 163: observed MAE=0.178398 validation MAE=0.471092,rank=5\n",
      "[SoftImpute] Iter 164: observed MAE=0.178304 validation MAE=0.470794,rank=5\n",
      "[SoftImpute] Iter 165: observed MAE=0.177853 validation MAE=0.471057,rank=5\n",
      "[SoftImpute] Iter 166: observed MAE=0.177793 validation MAE=0.469992,rank=5\n",
      "[SoftImpute] Iter 167: observed MAE=0.177710 validation MAE=0.469687,rank=5\n",
      "[SoftImpute] Iter 168: observed MAE=0.177778 validation MAE=0.469770,rank=5\n",
      "[SoftImpute] Iter 169: observed MAE=0.176915 validation MAE=0.469822,rank=5\n",
      "[SoftImpute] Iter 170: observed MAE=0.176747 validation MAE=0.469924,rank=5\n",
      "[SoftImpute] Stopped after iteration 170 for lambda=0.405542\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 5.115399599075317\n",
      "After the matrix factor stage, training error is 0.17675, validation error is 0.46992\n",
      "1\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.05122, val loss: 4.17539\n",
      "Main effects training epoch: 2, train loss: 3.89054, val loss: 4.01662\n",
      "Main effects training epoch: 3, train loss: 3.67039, val loss: 3.80311\n",
      "Main effects training epoch: 4, train loss: 3.51425, val loss: 3.65380\n",
      "Main effects training epoch: 5, train loss: 3.32941, val loss: 3.42293\n",
      "Main effects training epoch: 6, train loss: 3.35628, val loss: 3.42105\n",
      "Main effects training epoch: 7, train loss: 3.34245, val loss: 3.42762\n",
      "Main effects training epoch: 8, train loss: 3.21952, val loss: 3.31185\n",
      "Main effects training epoch: 9, train loss: 3.19315, val loss: 3.28617\n",
      "Main effects training epoch: 10, train loss: 3.16334, val loss: 3.25000\n",
      "Main effects training epoch: 11, train loss: 3.12922, val loss: 3.21085\n",
      "Main effects training epoch: 12, train loss: 3.01683, val loss: 3.10206\n",
      "Main effects training epoch: 13, train loss: 2.96310, val loss: 3.05149\n",
      "Main effects training epoch: 14, train loss: 2.94512, val loss: 3.03595\n",
      "Main effects training epoch: 15, train loss: 2.88656, val loss: 2.97575\n",
      "Main effects training epoch: 16, train loss: 2.86009, val loss: 2.95121\n",
      "Main effects training epoch: 17, train loss: 2.78296, val loss: 2.87053\n",
      "Main effects training epoch: 18, train loss: 2.71433, val loss: 2.80078\n",
      "Main effects training epoch: 19, train loss: 2.69217, val loss: 2.76384\n",
      "Main effects training epoch: 20, train loss: 2.61815, val loss: 2.69017\n",
      "Main effects training epoch: 21, train loss: 2.54445, val loss: 2.61737\n",
      "Main effects training epoch: 22, train loss: 2.50412, val loss: 2.57289\n",
      "Main effects training epoch: 23, train loss: 2.51888, val loss: 2.59870\n",
      "Main effects training epoch: 24, train loss: 2.46480, val loss: 2.52592\n",
      "Main effects training epoch: 25, train loss: 2.45437, val loss: 2.53148\n",
      "Main effects training epoch: 26, train loss: 2.37783, val loss: 2.44283\n",
      "Main effects training epoch: 27, train loss: 2.38827, val loss: 2.46942\n",
      "Main effects training epoch: 28, train loss: 2.32040, val loss: 2.38521\n",
      "Main effects training epoch: 29, train loss: 2.29763, val loss: 2.37535\n",
      "Main effects training epoch: 30, train loss: 2.27153, val loss: 2.33515\n",
      "Main effects training epoch: 31, train loss: 2.27511, val loss: 2.35111\n",
      "Main effects training epoch: 32, train loss: 2.23499, val loss: 2.30761\n",
      "Main effects training epoch: 33, train loss: 2.17665, val loss: 2.24862\n",
      "Main effects training epoch: 34, train loss: 2.16289, val loss: 2.23185\n",
      "Main effects training epoch: 35, train loss: 2.14011, val loss: 2.21092\n",
      "Main effects training epoch: 36, train loss: 2.14090, val loss: 2.21334\n",
      "Main effects training epoch: 37, train loss: 2.10857, val loss: 2.18094\n",
      "Main effects training epoch: 38, train loss: 2.07884, val loss: 2.14635\n",
      "Main effects training epoch: 39, train loss: 2.06025, val loss: 2.13426\n",
      "Main effects training epoch: 40, train loss: 2.05419, val loss: 2.12066\n",
      "Main effects training epoch: 41, train loss: 2.03042, val loss: 2.10381\n",
      "Main effects training epoch: 42, train loss: 2.02547, val loss: 2.09393\n",
      "Main effects training epoch: 43, train loss: 1.99387, val loss: 2.06681\n",
      "Main effects training epoch: 44, train loss: 1.98023, val loss: 2.05536\n",
      "Main effects training epoch: 45, train loss: 1.97411, val loss: 2.03856\n",
      "Main effects training epoch: 46, train loss: 1.95575, val loss: 2.03520\n",
      "Main effects training epoch: 47, train loss: 1.93443, val loss: 2.00397\n",
      "Main effects training epoch: 48, train loss: 1.91945, val loss: 1.99848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 49, train loss: 1.92196, val loss: 1.99578\n",
      "Main effects training epoch: 50, train loss: 1.89427, val loss: 1.96965\n",
      "Main effects training epoch: 51, train loss: 1.89176, val loss: 1.96506\n",
      "Main effects training epoch: 52, train loss: 1.87986, val loss: 1.95799\n",
      "Main effects training epoch: 53, train loss: 1.87138, val loss: 1.94226\n",
      "Main effects training epoch: 54, train loss: 1.86308, val loss: 1.93851\n",
      "Main effects training epoch: 55, train loss: 1.84218, val loss: 1.91686\n",
      "Main effects training epoch: 56, train loss: 1.85196, val loss: 1.91690\n",
      "Main effects training epoch: 57, train loss: 1.82458, val loss: 1.89456\n",
      "Main effects training epoch: 58, train loss: 1.84287, val loss: 1.91340\n",
      "Main effects training epoch: 59, train loss: 1.81760, val loss: 1.87985\n",
      "Main effects training epoch: 60, train loss: 1.81969, val loss: 1.89644\n",
      "Main effects training epoch: 61, train loss: 1.81745, val loss: 1.89077\n",
      "Main effects training epoch: 62, train loss: 1.79967, val loss: 1.86452\n",
      "Main effects training epoch: 63, train loss: 1.78831, val loss: 1.85189\n",
      "Main effects training epoch: 64, train loss: 1.77949, val loss: 1.84166\n",
      "Main effects training epoch: 65, train loss: 1.77663, val loss: 1.83376\n",
      "Main effects training epoch: 66, train loss: 1.75694, val loss: 1.82533\n",
      "Main effects training epoch: 67, train loss: 1.75134, val loss: 1.80929\n",
      "Main effects training epoch: 68, train loss: 1.73404, val loss: 1.79394\n",
      "Main effects training epoch: 69, train loss: 1.73895, val loss: 1.80041\n",
      "Main effects training epoch: 70, train loss: 1.72830, val loss: 1.78929\n",
      "Main effects training epoch: 71, train loss: 1.72903, val loss: 1.78694\n",
      "Main effects training epoch: 72, train loss: 1.72318, val loss: 1.78012\n",
      "Main effects training epoch: 73, train loss: 1.72374, val loss: 1.77831\n",
      "Main effects training epoch: 74, train loss: 1.71705, val loss: 1.77679\n",
      "Main effects training epoch: 75, train loss: 1.72542, val loss: 1.77743\n",
      "Main effects training epoch: 76, train loss: 1.71159, val loss: 1.76334\n",
      "Main effects training epoch: 77, train loss: 1.71650, val loss: 1.77340\n",
      "Main effects training epoch: 78, train loss: 1.71035, val loss: 1.76499\n",
      "Main effects training epoch: 79, train loss: 1.70923, val loss: 1.76368\n",
      "Main effects training epoch: 80, train loss: 1.70744, val loss: 1.76335\n",
      "Main effects training epoch: 81, train loss: 1.70137, val loss: 1.74764\n",
      "Main effects training epoch: 82, train loss: 1.69669, val loss: 1.74769\n",
      "Main effects training epoch: 83, train loss: 1.69794, val loss: 1.75657\n",
      "Main effects training epoch: 84, train loss: 1.69367, val loss: 1.74546\n",
      "Main effects training epoch: 85, train loss: 1.69200, val loss: 1.74872\n",
      "Main effects training epoch: 86, train loss: 1.68610, val loss: 1.73575\n",
      "Main effects training epoch: 87, train loss: 1.68905, val loss: 1.74951\n",
      "Main effects training epoch: 88, train loss: 1.68562, val loss: 1.74116\n",
      "Main effects training epoch: 89, train loss: 1.68199, val loss: 1.73522\n",
      "Main effects training epoch: 90, train loss: 1.67973, val loss: 1.73674\n",
      "Main effects training epoch: 91, train loss: 1.67390, val loss: 1.73258\n",
      "Main effects training epoch: 92, train loss: 1.66398, val loss: 1.71823\n",
      "Main effects training epoch: 93, train loss: 1.66298, val loss: 1.73048\n",
      "Main effects training epoch: 94, train loss: 1.65575, val loss: 1.71456\n",
      "Main effects training epoch: 95, train loss: 1.65169, val loss: 1.71831\n",
      "Main effects training epoch: 96, train loss: 1.65361, val loss: 1.72022\n",
      "Main effects training epoch: 97, train loss: 1.63556, val loss: 1.70666\n",
      "Main effects training epoch: 98, train loss: 1.63402, val loss: 1.70615\n",
      "Main effects training epoch: 99, train loss: 1.63450, val loss: 1.71246\n",
      "Main effects training epoch: 100, train loss: 1.63355, val loss: 1.70491\n",
      "Main effects training epoch: 101, train loss: 1.62450, val loss: 1.69697\n",
      "Main effects training epoch: 102, train loss: 1.62749, val loss: 1.70275\n",
      "Main effects training epoch: 103, train loss: 1.62695, val loss: 1.70228\n",
      "Main effects training epoch: 104, train loss: 1.63104, val loss: 1.69833\n",
      "Main effects training epoch: 105, train loss: 1.62334, val loss: 1.70307\n",
      "Main effects training epoch: 106, train loss: 1.61798, val loss: 1.68742\n",
      "Main effects training epoch: 107, train loss: 1.62737, val loss: 1.70303\n",
      "Main effects training epoch: 108, train loss: 1.61757, val loss: 1.69609\n",
      "Main effects training epoch: 109, train loss: 1.61746, val loss: 1.69638\n",
      "Main effects training epoch: 110, train loss: 1.61699, val loss: 1.69339\n",
      "Main effects training epoch: 111, train loss: 1.61188, val loss: 1.68732\n",
      "Main effects training epoch: 112, train loss: 1.61781, val loss: 1.69905\n",
      "Main effects training epoch: 113, train loss: 1.61176, val loss: 1.68437\n",
      "Main effects training epoch: 114, train loss: 1.61224, val loss: 1.68956\n",
      "Main effects training epoch: 115, train loss: 1.61214, val loss: 1.69700\n",
      "Main effects training epoch: 116, train loss: 1.60783, val loss: 1.68702\n",
      "Main effects training epoch: 117, train loss: 1.61487, val loss: 1.68942\n",
      "Main effects training epoch: 118, train loss: 1.60885, val loss: 1.69163\n",
      "Main effects training epoch: 119, train loss: 1.60320, val loss: 1.68369\n",
      "Main effects training epoch: 120, train loss: 1.60652, val loss: 1.68578\n",
      "Main effects training epoch: 121, train loss: 1.60171, val loss: 1.68059\n",
      "Main effects training epoch: 122, train loss: 1.60621, val loss: 1.69022\n",
      "Main effects training epoch: 123, train loss: 1.60211, val loss: 1.68299\n",
      "Main effects training epoch: 124, train loss: 1.60210, val loss: 1.68114\n",
      "Main effects training epoch: 125, train loss: 1.60803, val loss: 1.69249\n",
      "Main effects training epoch: 126, train loss: 1.60826, val loss: 1.68603\n",
      "Main effects training epoch: 127, train loss: 1.59899, val loss: 1.68737\n",
      "Main effects training epoch: 128, train loss: 1.59876, val loss: 1.68052\n",
      "Main effects training epoch: 129, train loss: 1.60290, val loss: 1.68809\n",
      "Main effects training epoch: 130, train loss: 1.59884, val loss: 1.68320\n",
      "Main effects training epoch: 131, train loss: 1.59378, val loss: 1.67801\n",
      "Main effects training epoch: 132, train loss: 1.59402, val loss: 1.67753\n",
      "Main effects training epoch: 133, train loss: 1.59592, val loss: 1.68585\n",
      "Main effects training epoch: 134, train loss: 1.59288, val loss: 1.67225\n",
      "Main effects training epoch: 135, train loss: 1.59135, val loss: 1.68761\n",
      "Main effects training epoch: 136, train loss: 1.58755, val loss: 1.67321\n",
      "Main effects training epoch: 137, train loss: 1.58742, val loss: 1.67012\n",
      "Main effects training epoch: 138, train loss: 1.58533, val loss: 1.67460\n",
      "Main effects training epoch: 139, train loss: 1.58405, val loss: 1.67406\n",
      "Main effects training epoch: 140, train loss: 1.58212, val loss: 1.67662\n",
      "Main effects training epoch: 141, train loss: 1.58283, val loss: 1.66493\n",
      "Main effects training epoch: 142, train loss: 1.58336, val loss: 1.66959\n",
      "Main effects training epoch: 143, train loss: 1.57956, val loss: 1.66842\n",
      "Main effects training epoch: 144, train loss: 1.59220, val loss: 1.68104\n",
      "Main effects training epoch: 145, train loss: 1.58573, val loss: 1.67042\n",
      "Main effects training epoch: 146, train loss: 1.59082, val loss: 1.68184\n",
      "Main effects training epoch: 147, train loss: 1.57770, val loss: 1.66017\n",
      "Main effects training epoch: 148, train loss: 1.57924, val loss: 1.66927\n",
      "Main effects training epoch: 149, train loss: 1.57511, val loss: 1.66126\n",
      "Main effects training epoch: 150, train loss: 1.57628, val loss: 1.65853\n",
      "Main effects training epoch: 151, train loss: 1.57022, val loss: 1.67016\n",
      "Main effects training epoch: 152, train loss: 1.57556, val loss: 1.65449\n",
      "Main effects training epoch: 153, train loss: 1.57674, val loss: 1.66353\n",
      "Main effects training epoch: 154, train loss: 1.57095, val loss: 1.65577\n",
      "Main effects training epoch: 155, train loss: 1.57220, val loss: 1.65539\n",
      "Main effects training epoch: 156, train loss: 1.57714, val loss: 1.65510\n",
      "Main effects training epoch: 157, train loss: 1.57018, val loss: 1.65411\n",
      "Main effects training epoch: 158, train loss: 1.56298, val loss: 1.65054\n",
      "Main effects training epoch: 159, train loss: 1.56806, val loss: 1.64302\n",
      "Main effects training epoch: 160, train loss: 1.55841, val loss: 1.64151\n",
      "Main effects training epoch: 161, train loss: 1.57269, val loss: 1.65807\n",
      "Main effects training epoch: 162, train loss: 1.57109, val loss: 1.65769\n",
      "Main effects training epoch: 163, train loss: 1.55597, val loss: 1.63278\n",
      "Main effects training epoch: 164, train loss: 1.56180, val loss: 1.63894\n",
      "Main effects training epoch: 165, train loss: 1.55765, val loss: 1.64254\n",
      "Main effects training epoch: 166, train loss: 1.55941, val loss: 1.63575\n",
      "Main effects training epoch: 167, train loss: 1.55623, val loss: 1.64678\n",
      "Main effects training epoch: 168, train loss: 1.55504, val loss: 1.63655\n",
      "Main effects training epoch: 169, train loss: 1.55468, val loss: 1.64108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 170, train loss: 1.55596, val loss: 1.63317\n",
      "Main effects training epoch: 171, train loss: 1.55862, val loss: 1.63404\n",
      "Main effects training epoch: 172, train loss: 1.55832, val loss: 1.64816\n",
      "Main effects training epoch: 173, train loss: 1.55019, val loss: 1.63020\n",
      "Main effects training epoch: 174, train loss: 1.55631, val loss: 1.61903\n",
      "Main effects training epoch: 175, train loss: 1.55746, val loss: 1.65080\n",
      "Main effects training epoch: 176, train loss: 1.55890, val loss: 1.63209\n",
      "Main effects training epoch: 177, train loss: 1.55825, val loss: 1.65377\n",
      "Main effects training epoch: 178, train loss: 1.54642, val loss: 1.62883\n",
      "Main effects training epoch: 179, train loss: 1.54447, val loss: 1.62314\n",
      "Main effects training epoch: 180, train loss: 1.54371, val loss: 1.63221\n",
      "Main effects training epoch: 181, train loss: 1.54701, val loss: 1.62164\n",
      "Main effects training epoch: 182, train loss: 1.54495, val loss: 1.62960\n",
      "Main effects training epoch: 183, train loss: 1.54462, val loss: 1.62382\n",
      "Main effects training epoch: 184, train loss: 1.56359, val loss: 1.65699\n",
      "Main effects training epoch: 185, train loss: 1.54548, val loss: 1.61993\n",
      "Main effects training epoch: 186, train loss: 1.54284, val loss: 1.62437\n",
      "Main effects training epoch: 187, train loss: 1.54138, val loss: 1.63205\n",
      "Main effects training epoch: 188, train loss: 1.54761, val loss: 1.62665\n",
      "Main effects training epoch: 189, train loss: 1.54938, val loss: 1.62171\n",
      "Main effects training epoch: 190, train loss: 1.54009, val loss: 1.62626\n",
      "Main effects training epoch: 191, train loss: 1.53771, val loss: 1.60940\n",
      "Main effects training epoch: 192, train loss: 1.54604, val loss: 1.63776\n",
      "Main effects training epoch: 193, train loss: 1.56676, val loss: 1.64683\n",
      "Main effects training epoch: 194, train loss: 1.54804, val loss: 1.63446\n",
      "Main effects training epoch: 195, train loss: 1.53625, val loss: 1.60674\n",
      "Main effects training epoch: 196, train loss: 1.54960, val loss: 1.63717\n",
      "Main effects training epoch: 197, train loss: 1.53624, val loss: 1.61455\n",
      "Main effects training epoch: 198, train loss: 1.53642, val loss: 1.61290\n",
      "Main effects training epoch: 199, train loss: 1.53174, val loss: 1.60991\n",
      "Main effects training epoch: 200, train loss: 1.53576, val loss: 1.62582\n",
      "Main effects training epoch: 201, train loss: 1.53233, val loss: 1.61019\n",
      "Main effects training epoch: 202, train loss: 1.53374, val loss: 1.61289\n",
      "Main effects training epoch: 203, train loss: 1.53312, val loss: 1.62118\n",
      "Main effects training epoch: 204, train loss: 1.53771, val loss: 1.62743\n",
      "Main effects training epoch: 205, train loss: 1.53032, val loss: 1.60170\n",
      "Main effects training epoch: 206, train loss: 1.53967, val loss: 1.62736\n",
      "Main effects training epoch: 207, train loss: 1.52945, val loss: 1.61658\n",
      "Main effects training epoch: 208, train loss: 1.53349, val loss: 1.60320\n",
      "Main effects training epoch: 209, train loss: 1.53051, val loss: 1.62660\n",
      "Main effects training epoch: 210, train loss: 1.54654, val loss: 1.61289\n",
      "Main effects training epoch: 211, train loss: 1.54255, val loss: 1.61670\n",
      "Main effects training epoch: 212, train loss: 1.53013, val loss: 1.61371\n",
      "Main effects training epoch: 213, train loss: 1.52838, val loss: 1.61152\n",
      "Main effects training epoch: 214, train loss: 1.52604, val loss: 1.61503\n",
      "Main effects training epoch: 215, train loss: 1.52634, val loss: 1.59984\n",
      "Main effects training epoch: 216, train loss: 1.52570, val loss: 1.61450\n",
      "Main effects training epoch: 217, train loss: 1.52961, val loss: 1.61247\n",
      "Main effects training epoch: 218, train loss: 1.53552, val loss: 1.59693\n",
      "Main effects training epoch: 219, train loss: 1.53059, val loss: 1.62281\n",
      "Main effects training epoch: 220, train loss: 1.53058, val loss: 1.61633\n",
      "Main effects training epoch: 221, train loss: 1.52119, val loss: 1.60882\n",
      "Main effects training epoch: 222, train loss: 1.52655, val loss: 1.60851\n",
      "Main effects training epoch: 223, train loss: 1.53162, val loss: 1.60837\n",
      "Main effects training epoch: 224, train loss: 1.52437, val loss: 1.61377\n",
      "Main effects training epoch: 225, train loss: 1.52713, val loss: 1.61368\n",
      "Main effects training epoch: 226, train loss: 1.52227, val loss: 1.60883\n",
      "Main effects training epoch: 227, train loss: 1.52429, val loss: 1.60823\n",
      "Main effects training epoch: 228, train loss: 1.52369, val loss: 1.60400\n",
      "Main effects training epoch: 229, train loss: 1.52148, val loss: 1.60374\n",
      "Main effects training epoch: 230, train loss: 1.53306, val loss: 1.61484\n",
      "Main effects training epoch: 231, train loss: 1.54442, val loss: 1.63792\n",
      "Main effects training epoch: 232, train loss: 1.53282, val loss: 1.61109\n",
      "Main effects training epoch: 233, train loss: 1.53469, val loss: 1.61935\n",
      "Main effects training epoch: 234, train loss: 1.51884, val loss: 1.60298\n",
      "Main effects training epoch: 235, train loss: 1.52142, val loss: 1.60394\n",
      "Main effects training epoch: 236, train loss: 1.52014, val loss: 1.60015\n",
      "Main effects training epoch: 237, train loss: 1.52087, val loss: 1.60212\n",
      "Main effects training epoch: 238, train loss: 1.51590, val loss: 1.60071\n",
      "Main effects training epoch: 239, train loss: 1.52471, val loss: 1.59869\n",
      "Main effects training epoch: 240, train loss: 1.51832, val loss: 1.60909\n",
      "Main effects training epoch: 241, train loss: 1.52616, val loss: 1.60342\n",
      "Main effects training epoch: 242, train loss: 1.52842, val loss: 1.62451\n",
      "Main effects training epoch: 243, train loss: 1.52694, val loss: 1.60999\n",
      "Main effects training epoch: 244, train loss: 1.52874, val loss: 1.61694\n",
      "Main effects training epoch: 245, train loss: 1.53049, val loss: 1.59473\n",
      "Main effects training epoch: 246, train loss: 1.52559, val loss: 1.60280\n",
      "Main effects training epoch: 247, train loss: 1.51893, val loss: 1.59359\n",
      "Main effects training epoch: 248, train loss: 1.52525, val loss: 1.60635\n",
      "Main effects training epoch: 249, train loss: 1.52247, val loss: 1.59922\n",
      "Main effects training epoch: 250, train loss: 1.53047, val loss: 1.62460\n",
      "Main effects training epoch: 251, train loss: 1.51583, val loss: 1.59392\n",
      "Main effects training epoch: 252, train loss: 1.52183, val loss: 1.61455\n",
      "Main effects training epoch: 253, train loss: 1.52286, val loss: 1.59670\n",
      "Main effects training epoch: 254, train loss: 1.52093, val loss: 1.61828\n",
      "Main effects training epoch: 255, train loss: 1.52087, val loss: 1.59533\n",
      "Main effects training epoch: 256, train loss: 1.51369, val loss: 1.58740\n",
      "Main effects training epoch: 257, train loss: 1.50975, val loss: 1.59729\n",
      "Main effects training epoch: 258, train loss: 1.51241, val loss: 1.59326\n",
      "Main effects training epoch: 259, train loss: 1.50930, val loss: 1.58947\n",
      "Main effects training epoch: 260, train loss: 1.51141, val loss: 1.59002\n",
      "Main effects training epoch: 261, train loss: 1.51911, val loss: 1.60096\n",
      "Main effects training epoch: 262, train loss: 1.51845, val loss: 1.58967\n",
      "Main effects training epoch: 263, train loss: 1.51846, val loss: 1.60890\n",
      "Main effects training epoch: 264, train loss: 1.51840, val loss: 1.58899\n",
      "Main effects training epoch: 265, train loss: 1.51741, val loss: 1.59519\n",
      "Main effects training epoch: 266, train loss: 1.51739, val loss: 1.59927\n",
      "Main effects training epoch: 267, train loss: 1.51725, val loss: 1.58183\n",
      "Main effects training epoch: 268, train loss: 1.51975, val loss: 1.59169\n",
      "Main effects training epoch: 269, train loss: 1.51525, val loss: 1.59788\n",
      "Main effects training epoch: 270, train loss: 1.51611, val loss: 1.59871\n",
      "Main effects training epoch: 271, train loss: 1.51253, val loss: 1.58532\n",
      "Main effects training epoch: 272, train loss: 1.52196, val loss: 1.59734\n",
      "Main effects training epoch: 273, train loss: 1.51085, val loss: 1.59456\n",
      "Main effects training epoch: 274, train loss: 1.51424, val loss: 1.57850\n",
      "Main effects training epoch: 275, train loss: 1.50602, val loss: 1.58749\n",
      "Main effects training epoch: 276, train loss: 1.50236, val loss: 1.58772\n",
      "Main effects training epoch: 277, train loss: 1.50289, val loss: 1.59051\n",
      "Main effects training epoch: 278, train loss: 1.50613, val loss: 1.59041\n",
      "Main effects training epoch: 279, train loss: 1.50142, val loss: 1.58434\n",
      "Main effects training epoch: 280, train loss: 1.50141, val loss: 1.57827\n",
      "Main effects training epoch: 281, train loss: 1.49898, val loss: 1.58516\n",
      "Main effects training epoch: 282, train loss: 1.49792, val loss: 1.58660\n",
      "Main effects training epoch: 283, train loss: 1.49719, val loss: 1.56868\n",
      "Main effects training epoch: 284, train loss: 1.50092, val loss: 1.58170\n",
      "Main effects training epoch: 285, train loss: 1.50180, val loss: 1.58024\n",
      "Main effects training epoch: 286, train loss: 1.49872, val loss: 1.57155\n",
      "Main effects training epoch: 287, train loss: 1.49427, val loss: 1.57515\n",
      "Main effects training epoch: 288, train loss: 1.49866, val loss: 1.58673\n",
      "Main effects training epoch: 289, train loss: 1.48984, val loss: 1.57179\n",
      "Main effects training epoch: 290, train loss: 1.52312, val loss: 1.59115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 291, train loss: 1.51199, val loss: 1.59631\n",
      "Main effects training epoch: 292, train loss: 1.48848, val loss: 1.57138\n",
      "Main effects training epoch: 293, train loss: 1.49524, val loss: 1.57229\n",
      "Main effects training epoch: 294, train loss: 1.49913, val loss: 1.58078\n",
      "Main effects training epoch: 295, train loss: 1.48412, val loss: 1.56349\n",
      "Main effects training epoch: 296, train loss: 1.48543, val loss: 1.56815\n",
      "Main effects training epoch: 297, train loss: 1.49614, val loss: 1.56678\n",
      "Main effects training epoch: 298, train loss: 1.48698, val loss: 1.58104\n",
      "Main effects training epoch: 299, train loss: 1.48634, val loss: 1.56696\n",
      "Main effects training epoch: 300, train loss: 1.50293, val loss: 1.59658\n",
      "##########Stage 1: main effect training stop.##########\n",
      "3 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.50201, val loss: 1.57887\n",
      "Main effects tuning epoch: 2, train loss: 1.51322, val loss: 1.57754\n",
      "Main effects tuning epoch: 3, train loss: 1.50647, val loss: 1.59067\n",
      "Main effects tuning epoch: 4, train loss: 1.50697, val loss: 1.58564\n",
      "Main effects tuning epoch: 5, train loss: 1.50994, val loss: 1.58717\n",
      "Main effects tuning epoch: 6, train loss: 1.50217, val loss: 1.58635\n",
      "Main effects tuning epoch: 7, train loss: 1.50956, val loss: 1.56722\n",
      "Main effects tuning epoch: 8, train loss: 1.49900, val loss: 1.57584\n",
      "Main effects tuning epoch: 9, train loss: 1.49715, val loss: 1.58560\n",
      "Main effects tuning epoch: 10, train loss: 1.49506, val loss: 1.56475\n",
      "Main effects tuning epoch: 11, train loss: 1.49638, val loss: 1.56967\n",
      "Main effects tuning epoch: 12, train loss: 1.49678, val loss: 1.57440\n",
      "Main effects tuning epoch: 13, train loss: 1.50130, val loss: 1.58070\n",
      "Main effects tuning epoch: 14, train loss: 1.49886, val loss: 1.57659\n",
      "Main effects tuning epoch: 15, train loss: 1.49823, val loss: 1.57670\n",
      "Main effects tuning epoch: 16, train loss: 1.49898, val loss: 1.58388\n",
      "Main effects tuning epoch: 17, train loss: 1.49770, val loss: 1.58001\n",
      "Main effects tuning epoch: 18, train loss: 1.49552, val loss: 1.57220\n",
      "Main effects tuning epoch: 19, train loss: 1.49272, val loss: 1.56959\n",
      "Main effects tuning epoch: 20, train loss: 1.49605, val loss: 1.57495\n",
      "Main effects tuning epoch: 21, train loss: 1.49602, val loss: 1.57136\n",
      "Main effects tuning epoch: 22, train loss: 1.49187, val loss: 1.56780\n",
      "Main effects tuning epoch: 23, train loss: 1.49516, val loss: 1.57431\n",
      "Main effects tuning epoch: 24, train loss: 1.48949, val loss: 1.56656\n",
      "Main effects tuning epoch: 25, train loss: 1.49384, val loss: 1.56912\n",
      "Main effects tuning epoch: 26, train loss: 1.49551, val loss: 1.58487\n",
      "Main effects tuning epoch: 27, train loss: 1.50251, val loss: 1.57910\n",
      "Main effects tuning epoch: 28, train loss: 1.50970, val loss: 1.59883\n",
      "Main effects tuning epoch: 29, train loss: 1.48927, val loss: 1.56225\n",
      "Main effects tuning epoch: 30, train loss: 1.49112, val loss: 1.56849\n",
      "Main effects tuning epoch: 31, train loss: 1.48908, val loss: 1.56345\n",
      "Main effects tuning epoch: 32, train loss: 1.49016, val loss: 1.56673\n",
      "Main effects tuning epoch: 33, train loss: 1.49331, val loss: 1.57740\n",
      "Main effects tuning epoch: 34, train loss: 1.49478, val loss: 1.56284\n",
      "Main effects tuning epoch: 35, train loss: 1.49213, val loss: 1.56920\n",
      "Main effects tuning epoch: 36, train loss: 1.49277, val loss: 1.56703\n",
      "Main effects tuning epoch: 37, train loss: 1.48937, val loss: 1.56266\n",
      "Main effects tuning epoch: 38, train loss: 1.49335, val loss: 1.57233\n",
      "Main effects tuning epoch: 39, train loss: 1.48769, val loss: 1.55247\n",
      "Main effects tuning epoch: 40, train loss: 1.48620, val loss: 1.56524\n",
      "Main effects tuning epoch: 41, train loss: 1.49863, val loss: 1.57520\n",
      "Main effects tuning epoch: 42, train loss: 1.48723, val loss: 1.56371\n",
      "Main effects tuning epoch: 43, train loss: 1.48533, val loss: 1.55475\n",
      "Main effects tuning epoch: 44, train loss: 1.48768, val loss: 1.55845\n",
      "Main effects tuning epoch: 45, train loss: 1.49451, val loss: 1.57121\n",
      "Main effects tuning epoch: 46, train loss: 1.48530, val loss: 1.56457\n",
      "Main effects tuning epoch: 47, train loss: 1.48656, val loss: 1.55917\n",
      "Main effects tuning epoch: 48, train loss: 1.49864, val loss: 1.57142\n",
      "Main effects tuning epoch: 49, train loss: 1.50054, val loss: 1.56049\n",
      "Main effects tuning epoch: 50, train loss: 1.49805, val loss: 1.57155\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.40382, val loss: 1.47338\n",
      "Interaction training epoch: 2, train loss: 1.52073, val loss: 1.55351\n",
      "Interaction training epoch: 3, train loss: 1.13840, val loss: 1.17310\n",
      "Interaction training epoch: 4, train loss: 1.11488, val loss: 1.12391\n",
      "Interaction training epoch: 5, train loss: 1.09997, val loss: 1.12080\n",
      "Interaction training epoch: 6, train loss: 1.10403, val loss: 1.11054\n",
      "Interaction training epoch: 7, train loss: 1.03991, val loss: 1.05972\n",
      "Interaction training epoch: 8, train loss: 1.07378, val loss: 1.09772\n",
      "Interaction training epoch: 9, train loss: 1.08760, val loss: 1.10006\n",
      "Interaction training epoch: 10, train loss: 1.00810, val loss: 1.02516\n",
      "Interaction training epoch: 11, train loss: 0.99691, val loss: 1.01643\n",
      "Interaction training epoch: 12, train loss: 0.98846, val loss: 0.99432\n",
      "Interaction training epoch: 13, train loss: 0.97892, val loss: 0.98295\n",
      "Interaction training epoch: 14, train loss: 0.95949, val loss: 0.96193\n",
      "Interaction training epoch: 15, train loss: 0.94384, val loss: 0.94677\n",
      "Interaction training epoch: 16, train loss: 0.94905, val loss: 0.95640\n",
      "Interaction training epoch: 17, train loss: 0.94156, val loss: 0.95738\n",
      "Interaction training epoch: 18, train loss: 0.94161, val loss: 0.93737\n",
      "Interaction training epoch: 19, train loss: 0.93488, val loss: 0.94487\n",
      "Interaction training epoch: 20, train loss: 0.93952, val loss: 0.95484\n",
      "Interaction training epoch: 21, train loss: 0.93905, val loss: 0.93999\n",
      "Interaction training epoch: 22, train loss: 0.92998, val loss: 0.92812\n",
      "Interaction training epoch: 23, train loss: 0.94011, val loss: 0.94058\n",
      "Interaction training epoch: 24, train loss: 0.94810, val loss: 0.96443\n",
      "Interaction training epoch: 25, train loss: 0.90587, val loss: 0.90948\n",
      "Interaction training epoch: 26, train loss: 0.89568, val loss: 0.89910\n",
      "Interaction training epoch: 27, train loss: 0.90931, val loss: 0.91100\n",
      "Interaction training epoch: 28, train loss: 0.89390, val loss: 0.89926\n",
      "Interaction training epoch: 29, train loss: 0.89149, val loss: 0.89765\n",
      "Interaction training epoch: 30, train loss: 0.90251, val loss: 0.90618\n",
      "Interaction training epoch: 31, train loss: 0.92756, val loss: 0.92297\n",
      "Interaction training epoch: 32, train loss: 0.90935, val loss: 0.91585\n",
      "Interaction training epoch: 33, train loss: 0.90199, val loss: 0.89836\n",
      "Interaction training epoch: 34, train loss: 0.90859, val loss: 0.90905\n",
      "Interaction training epoch: 35, train loss: 0.89657, val loss: 0.90455\n",
      "Interaction training epoch: 36, train loss: 0.90580, val loss: 0.90536\n",
      "Interaction training epoch: 37, train loss: 0.89353, val loss: 0.89554\n",
      "Interaction training epoch: 38, train loss: 0.88136, val loss: 0.88052\n",
      "Interaction training epoch: 39, train loss: 0.89927, val loss: 0.89981\n",
      "Interaction training epoch: 40, train loss: 0.88152, val loss: 0.88296\n",
      "Interaction training epoch: 41, train loss: 0.88039, val loss: 0.87933\n",
      "Interaction training epoch: 42, train loss: 0.87104, val loss: 0.86172\n",
      "Interaction training epoch: 43, train loss: 0.87900, val loss: 0.88453\n",
      "Interaction training epoch: 44, train loss: 0.87637, val loss: 0.88604\n",
      "Interaction training epoch: 45, train loss: 0.87715, val loss: 0.86282\n",
      "Interaction training epoch: 46, train loss: 0.87572, val loss: 0.87693\n",
      "Interaction training epoch: 47, train loss: 0.86861, val loss: 0.86460\n",
      "Interaction training epoch: 48, train loss: 0.87535, val loss: 0.86805\n",
      "Interaction training epoch: 49, train loss: 0.86463, val loss: 0.85809\n",
      "Interaction training epoch: 50, train loss: 0.86908, val loss: 0.86634\n",
      "Interaction training epoch: 51, train loss: 0.87506, val loss: 0.86301\n",
      "Interaction training epoch: 52, train loss: 0.88847, val loss: 0.88512\n",
      "Interaction training epoch: 53, train loss: 0.87220, val loss: 0.87624\n",
      "Interaction training epoch: 54, train loss: 0.87729, val loss: 0.86306\n",
      "Interaction training epoch: 55, train loss: 0.87071, val loss: 0.87121\n",
      "Interaction training epoch: 56, train loss: 0.87024, val loss: 0.86426\n",
      "Interaction training epoch: 57, train loss: 0.86730, val loss: 0.87588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 58, train loss: 0.85885, val loss: 0.84707\n",
      "Interaction training epoch: 59, train loss: 0.86789, val loss: 0.87141\n",
      "Interaction training epoch: 60, train loss: 0.86870, val loss: 0.85564\n",
      "Interaction training epoch: 61, train loss: 0.86284, val loss: 0.86515\n",
      "Interaction training epoch: 62, train loss: 0.86927, val loss: 0.87993\n",
      "Interaction training epoch: 63, train loss: 0.87083, val loss: 0.86640\n",
      "Interaction training epoch: 64, train loss: 0.86966, val loss: 0.86589\n",
      "Interaction training epoch: 65, train loss: 0.85333, val loss: 0.85369\n",
      "Interaction training epoch: 66, train loss: 0.85865, val loss: 0.86001\n",
      "Interaction training epoch: 67, train loss: 0.85279, val loss: 0.85464\n",
      "Interaction training epoch: 68, train loss: 0.85225, val loss: 0.85316\n",
      "Interaction training epoch: 69, train loss: 0.84742, val loss: 0.84106\n",
      "Interaction training epoch: 70, train loss: 0.84663, val loss: 0.85053\n",
      "Interaction training epoch: 71, train loss: 0.85652, val loss: 0.85750\n",
      "Interaction training epoch: 72, train loss: 0.85251, val loss: 0.85574\n",
      "Interaction training epoch: 73, train loss: 0.84674, val loss: 0.84250\n",
      "Interaction training epoch: 74, train loss: 0.84703, val loss: 0.84610\n",
      "Interaction training epoch: 75, train loss: 0.85189, val loss: 0.85385\n",
      "Interaction training epoch: 76, train loss: 0.84398, val loss: 0.83649\n",
      "Interaction training epoch: 77, train loss: 0.84674, val loss: 0.85244\n",
      "Interaction training epoch: 78, train loss: 0.85655, val loss: 0.84153\n",
      "Interaction training epoch: 79, train loss: 0.84194, val loss: 0.84928\n",
      "Interaction training epoch: 80, train loss: 0.85914, val loss: 0.85954\n",
      "Interaction training epoch: 81, train loss: 0.85143, val loss: 0.84621\n",
      "Interaction training epoch: 82, train loss: 0.85465, val loss: 0.85640\n",
      "Interaction training epoch: 83, train loss: 0.84963, val loss: 0.85377\n",
      "Interaction training epoch: 84, train loss: 0.84891, val loss: 0.84386\n",
      "Interaction training epoch: 85, train loss: 0.84171, val loss: 0.84355\n",
      "Interaction training epoch: 86, train loss: 0.84154, val loss: 0.83939\n",
      "Interaction training epoch: 87, train loss: 0.84429, val loss: 0.84208\n",
      "Interaction training epoch: 88, train loss: 0.84083, val loss: 0.83528\n",
      "Interaction training epoch: 89, train loss: 0.84365, val loss: 0.86220\n",
      "Interaction training epoch: 90, train loss: 0.84576, val loss: 0.84145\n",
      "Interaction training epoch: 91, train loss: 0.84655, val loss: 0.85111\n",
      "Interaction training epoch: 92, train loss: 0.83797, val loss: 0.83998\n",
      "Interaction training epoch: 93, train loss: 0.84303, val loss: 0.84609\n",
      "Interaction training epoch: 94, train loss: 0.83631, val loss: 0.83887\n",
      "Interaction training epoch: 95, train loss: 0.83968, val loss: 0.83512\n",
      "Interaction training epoch: 96, train loss: 0.83244, val loss: 0.83891\n",
      "Interaction training epoch: 97, train loss: 0.83974, val loss: 0.84128\n",
      "Interaction training epoch: 98, train loss: 0.84378, val loss: 0.84829\n",
      "Interaction training epoch: 99, train loss: 0.84303, val loss: 0.84943\n",
      "Interaction training epoch: 100, train loss: 0.84618, val loss: 0.84491\n",
      "Interaction training epoch: 101, train loss: 0.84280, val loss: 0.85092\n",
      "Interaction training epoch: 102, train loss: 0.83756, val loss: 0.83309\n",
      "Interaction training epoch: 103, train loss: 0.83661, val loss: 0.84492\n",
      "Interaction training epoch: 104, train loss: 0.84362, val loss: 0.85180\n",
      "Interaction training epoch: 105, train loss: 0.84636, val loss: 0.83768\n",
      "Interaction training epoch: 106, train loss: 0.84340, val loss: 0.84591\n",
      "Interaction training epoch: 107, train loss: 0.84586, val loss: 0.84980\n",
      "Interaction training epoch: 108, train loss: 0.84393, val loss: 0.85047\n",
      "Interaction training epoch: 109, train loss: 0.83581, val loss: 0.83953\n",
      "Interaction training epoch: 110, train loss: 0.83923, val loss: 0.84094\n",
      "Interaction training epoch: 111, train loss: 0.83423, val loss: 0.84781\n",
      "Interaction training epoch: 112, train loss: 0.83792, val loss: 0.84129\n",
      "Interaction training epoch: 113, train loss: 0.83295, val loss: 0.84008\n",
      "Interaction training epoch: 114, train loss: 0.83523, val loss: 0.83793\n",
      "Interaction training epoch: 115, train loss: 0.83825, val loss: 0.84225\n",
      "Interaction training epoch: 116, train loss: 0.82809, val loss: 0.83396\n",
      "Interaction training epoch: 117, train loss: 0.83819, val loss: 0.84117\n",
      "Interaction training epoch: 118, train loss: 0.84574, val loss: 0.85693\n",
      "Interaction training epoch: 119, train loss: 0.83091, val loss: 0.82609\n",
      "Interaction training epoch: 120, train loss: 0.84543, val loss: 0.84849\n",
      "Interaction training epoch: 121, train loss: 0.84205, val loss: 0.84309\n",
      "Interaction training epoch: 122, train loss: 0.83363, val loss: 0.84470\n",
      "Interaction training epoch: 123, train loss: 0.83547, val loss: 0.83747\n",
      "Interaction training epoch: 124, train loss: 0.83471, val loss: 0.84205\n",
      "Interaction training epoch: 125, train loss: 0.83940, val loss: 0.83500\n",
      "Interaction training epoch: 126, train loss: 0.83702, val loss: 0.84805\n",
      "Interaction training epoch: 127, train loss: 0.83263, val loss: 0.83244\n",
      "Interaction training epoch: 128, train loss: 0.83403, val loss: 0.84042\n",
      "Interaction training epoch: 129, train loss: 0.83157, val loss: 0.83672\n",
      "Interaction training epoch: 130, train loss: 0.83262, val loss: 0.83390\n",
      "Interaction training epoch: 131, train loss: 0.83080, val loss: 0.83816\n",
      "Interaction training epoch: 132, train loss: 0.83542, val loss: 0.83336\n",
      "Interaction training epoch: 133, train loss: 0.82560, val loss: 0.82734\n",
      "Interaction training epoch: 134, train loss: 0.83294, val loss: 0.83845\n",
      "Interaction training epoch: 135, train loss: 0.85268, val loss: 0.86734\n",
      "Interaction training epoch: 136, train loss: 0.84320, val loss: 0.84240\n",
      "Interaction training epoch: 137, train loss: 0.83809, val loss: 0.84898\n",
      "Interaction training epoch: 138, train loss: 0.83300, val loss: 0.83486\n",
      "Interaction training epoch: 139, train loss: 0.83335, val loss: 0.83286\n",
      "Interaction training epoch: 140, train loss: 0.83547, val loss: 0.84025\n",
      "Interaction training epoch: 141, train loss: 0.83554, val loss: 0.83793\n",
      "Interaction training epoch: 142, train loss: 0.84191, val loss: 0.85536\n",
      "Interaction training epoch: 143, train loss: 0.83560, val loss: 0.83018\n",
      "Interaction training epoch: 144, train loss: 0.83922, val loss: 0.84183\n",
      "Interaction training epoch: 145, train loss: 0.83672, val loss: 0.84130\n",
      "Interaction training epoch: 146, train loss: 0.83045, val loss: 0.83885\n",
      "Interaction training epoch: 147, train loss: 0.83855, val loss: 0.83665\n",
      "Interaction training epoch: 148, train loss: 0.83498, val loss: 0.84374\n",
      "Interaction training epoch: 149, train loss: 0.82837, val loss: 0.82705\n",
      "Interaction training epoch: 150, train loss: 0.82919, val loss: 0.83620\n",
      "Interaction training epoch: 151, train loss: 0.82660, val loss: 0.82976\n",
      "Interaction training epoch: 152, train loss: 0.83134, val loss: 0.83485\n",
      "Interaction training epoch: 153, train loss: 0.83043, val loss: 0.84160\n",
      "Interaction training epoch: 154, train loss: 0.82945, val loss: 0.83159\n",
      "Interaction training epoch: 155, train loss: 0.83748, val loss: 0.84555\n",
      "Interaction training epoch: 156, train loss: 0.82810, val loss: 0.82963\n",
      "Interaction training epoch: 157, train loss: 0.83499, val loss: 0.83886\n",
      "Interaction training epoch: 158, train loss: 0.83054, val loss: 0.82661\n",
      "Interaction training epoch: 159, train loss: 0.83178, val loss: 0.84351\n",
      "Interaction training epoch: 160, train loss: 0.83198, val loss: 0.83017\n",
      "Interaction training epoch: 161, train loss: 0.82417, val loss: 0.83497\n",
      "Interaction training epoch: 162, train loss: 0.83108, val loss: 0.83354\n",
      "Interaction training epoch: 163, train loss: 0.83115, val loss: 0.83543\n",
      "Interaction training epoch: 164, train loss: 0.83194, val loss: 0.83691\n",
      "Interaction training epoch: 165, train loss: 0.83141, val loss: 0.83474\n",
      "Interaction training epoch: 166, train loss: 0.83177, val loss: 0.84075\n",
      "Interaction training epoch: 167, train loss: 0.82822, val loss: 0.82809\n",
      "Interaction training epoch: 168, train loss: 0.82782, val loss: 0.83574\n",
      "Interaction training epoch: 169, train loss: 0.82720, val loss: 0.83189\n",
      "Interaction training epoch: 170, train loss: 0.83017, val loss: 0.82878\n",
      "Interaction training epoch: 171, train loss: 0.82689, val loss: 0.82914\n",
      "Interaction training epoch: 172, train loss: 0.82686, val loss: 0.83172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 173, train loss: 0.83324, val loss: 0.84303\n",
      "Interaction training epoch: 174, train loss: 0.82761, val loss: 0.82527\n",
      "Interaction training epoch: 175, train loss: 0.82898, val loss: 0.82986\n",
      "Interaction training epoch: 176, train loss: 0.82964, val loss: 0.83067\n",
      "Interaction training epoch: 177, train loss: 0.82654, val loss: 0.83149\n",
      "Interaction training epoch: 178, train loss: 0.82977, val loss: 0.83203\n",
      "Interaction training epoch: 179, train loss: 0.82845, val loss: 0.83521\n",
      "Interaction training epoch: 180, train loss: 0.83419, val loss: 0.84177\n",
      "Interaction training epoch: 181, train loss: 0.82870, val loss: 0.83170\n",
      "Interaction training epoch: 182, train loss: 0.83265, val loss: 0.83122\n",
      "Interaction training epoch: 183, train loss: 0.83091, val loss: 0.83755\n",
      "Interaction training epoch: 184, train loss: 0.82997, val loss: 0.83409\n",
      "Interaction training epoch: 185, train loss: 0.82725, val loss: 0.83505\n",
      "Interaction training epoch: 186, train loss: 0.83070, val loss: 0.83544\n",
      "Interaction training epoch: 187, train loss: 0.82909, val loss: 0.83411\n",
      "Interaction training epoch: 188, train loss: 0.83652, val loss: 0.83808\n",
      "Interaction training epoch: 189, train loss: 0.82605, val loss: 0.83232\n",
      "Interaction training epoch: 190, train loss: 0.82594, val loss: 0.82710\n",
      "Interaction training epoch: 191, train loss: 0.82751, val loss: 0.82884\n",
      "Interaction training epoch: 192, train loss: 0.82973, val loss: 0.82988\n",
      "Interaction training epoch: 193, train loss: 0.83172, val loss: 0.83302\n",
      "Interaction training epoch: 194, train loss: 0.82799, val loss: 0.83659\n",
      "Interaction training epoch: 195, train loss: 0.82785, val loss: 0.82460\n",
      "Interaction training epoch: 196, train loss: 0.82686, val loss: 0.83204\n",
      "Interaction training epoch: 197, train loss: 0.83463, val loss: 0.83813\n",
      "Interaction training epoch: 198, train loss: 0.82767, val loss: 0.83402\n",
      "Interaction training epoch: 199, train loss: 0.83250, val loss: 0.83720\n",
      "Interaction training epoch: 200, train loss: 0.82782, val loss: 0.83185\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########No main interaction is pruned, the tuning step is skipped.\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 33.60362482070923\n",
      "After the gam stage, training error is 0.82782 , validation error is 0.83185\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 20.851936\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.683456 validation MAE=0.787973,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.634516 validation MAE=0.767620,rank=5\n",
      "[SoftImpute] Iter 3: observed MAE=0.593258 validation MAE=0.748884,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.557948 validation MAE=0.731907,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.528501 validation MAE=0.716807,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.503690 validation MAE=0.703439,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.481858 validation MAE=0.691307,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.462471 validation MAE=0.680137,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.445811 validation MAE=0.670608,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.432707 validation MAE=0.662973,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.420710 validation MAE=0.655715,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.410700 validation MAE=0.649630,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.401014 validation MAE=0.643980,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.392299 validation MAE=0.638012,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.385690 validation MAE=0.634088,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.378358 validation MAE=0.630174,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.371799 validation MAE=0.625767,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.365392 validation MAE=0.622478,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.360320 validation MAE=0.619536,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.356251 validation MAE=0.615489,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.353080 validation MAE=0.613692,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.349774 validation MAE=0.611877,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.344521 validation MAE=0.608923,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.340612 validation MAE=0.606560,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.335813 validation MAE=0.603817,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.332618 validation MAE=0.601251,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.328601 validation MAE=0.599416,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.325780 validation MAE=0.597179,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.322288 validation MAE=0.595403,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.319388 validation MAE=0.593446,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.316046 validation MAE=0.591210,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.314249 validation MAE=0.589735,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.311220 validation MAE=0.588464,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.309568 validation MAE=0.586219,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.306500 validation MAE=0.585251,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.304192 validation MAE=0.583995,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.302857 validation MAE=0.582764,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.300210 validation MAE=0.581051,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.297676 validation MAE=0.579774,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.296286 validation MAE=0.578573,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.294307 validation MAE=0.577732,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.292813 validation MAE=0.576628,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.291053 validation MAE=0.575328,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.289517 validation MAE=0.574158,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.287809 validation MAE=0.573640,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.286235 validation MAE=0.572647,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.284843 validation MAE=0.571440,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.283146 validation MAE=0.570619,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.282856 validation MAE=0.569902,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.280968 validation MAE=0.569556,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.279705 validation MAE=0.569195,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.278761 validation MAE=0.568457,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.278387 validation MAE=0.567339,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.276022 validation MAE=0.566384,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.274881 validation MAE=0.566225,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.274669 validation MAE=0.565606,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.273452 validation MAE=0.564688,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.272394 validation MAE=0.563705,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.271319 validation MAE=0.563598,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.270356 validation MAE=0.563010,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.269717 validation MAE=0.562718,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.269073 validation MAE=0.561778,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.267483 validation MAE=0.561429,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.266875 validation MAE=0.561088,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.265504 validation MAE=0.559620,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.264776 validation MAE=0.559660,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.264121 validation MAE=0.559330,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.263383 validation MAE=0.559366,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.263259 validation MAE=0.558688,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.262454 validation MAE=0.557924,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.261239 validation MAE=0.557743,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.260409 validation MAE=0.557150,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.260127 validation MAE=0.556646,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.259503 validation MAE=0.557085,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.259116 validation MAE=0.556047,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.258176 validation MAE=0.556153,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.257769 validation MAE=0.555574,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.257338 validation MAE=0.554562,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.255925 validation MAE=0.554445,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.255814 validation MAE=0.554636,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.255448 validation MAE=0.553784,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.255305 validation MAE=0.553706,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.254546 validation MAE=0.553781,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.253518 validation MAE=0.553440,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.253147 validation MAE=0.553818,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.252957 validation MAE=0.553546,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.252927 validation MAE=0.552544,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.252101 validation MAE=0.552608,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.251528 validation MAE=0.552028,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.250993 validation MAE=0.551325,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.250496 validation MAE=0.551461,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.250134 validation MAE=0.551518,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.249288 validation MAE=0.551683,rank=5\n",
      "[SoftImpute] Stopped after iteration 93 for lambda=0.417039\n",
      "final num of user group: 4\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 3.5565085411071777\n",
      "After the matrix factor stage, training error is 0.24929, validation error is 0.55168\n",
      "2\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.01156, val loss: 4.13188\n",
      "Main effects training epoch: 2, train loss: 3.81184, val loss: 3.93024\n",
      "Main effects training epoch: 3, train loss: 3.63454, val loss: 3.76612\n",
      "Main effects training epoch: 4, train loss: 3.58081, val loss: 3.69630\n",
      "Main effects training epoch: 5, train loss: 3.40727, val loss: 3.48164\n",
      "Main effects training epoch: 6, train loss: 3.31163, val loss: 3.38527\n",
      "Main effects training epoch: 7, train loss: 3.23578, val loss: 3.31998\n",
      "Main effects training epoch: 8, train loss: 3.26583, val loss: 3.35273\n",
      "Main effects training epoch: 9, train loss: 3.20997, val loss: 3.30030\n",
      "Main effects training epoch: 10, train loss: 3.13436, val loss: 3.22268\n",
      "Main effects training epoch: 11, train loss: 3.07565, val loss: 3.15766\n",
      "Main effects training epoch: 12, train loss: 3.05091, val loss: 3.13357\n",
      "Main effects training epoch: 13, train loss: 2.98359, val loss: 3.06703\n",
      "Main effects training epoch: 14, train loss: 2.91460, val loss: 3.00088\n",
      "Main effects training epoch: 15, train loss: 2.84827, val loss: 2.93188\n",
      "Main effects training epoch: 16, train loss: 2.79767, val loss: 2.87998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 17, train loss: 2.72163, val loss: 2.79889\n",
      "Main effects training epoch: 18, train loss: 2.70443, val loss: 2.78181\n",
      "Main effects training epoch: 19, train loss: 2.59996, val loss: 2.66920\n",
      "Main effects training epoch: 20, train loss: 2.55199, val loss: 2.61625\n",
      "Main effects training epoch: 21, train loss: 2.50150, val loss: 2.56434\n",
      "Main effects training epoch: 22, train loss: 2.45267, val loss: 2.52240\n",
      "Main effects training epoch: 23, train loss: 2.40016, val loss: 2.47495\n",
      "Main effects training epoch: 24, train loss: 2.36947, val loss: 2.44015\n",
      "Main effects training epoch: 25, train loss: 2.33913, val loss: 2.41546\n",
      "Main effects training epoch: 26, train loss: 2.34308, val loss: 2.41919\n",
      "Main effects training epoch: 27, train loss: 2.27595, val loss: 2.35851\n",
      "Main effects training epoch: 28, train loss: 2.25511, val loss: 2.33313\n",
      "Main effects training epoch: 29, train loss: 2.22530, val loss: 2.30695\n",
      "Main effects training epoch: 30, train loss: 2.20839, val loss: 2.29165\n",
      "Main effects training epoch: 31, train loss: 2.18500, val loss: 2.27309\n",
      "Main effects training epoch: 32, train loss: 2.15846, val loss: 2.23920\n",
      "Main effects training epoch: 33, train loss: 2.10452, val loss: 2.18300\n",
      "Main effects training epoch: 34, train loss: 2.12973, val loss: 2.21140\n",
      "Main effects training epoch: 35, train loss: 2.08174, val loss: 2.15788\n",
      "Main effects training epoch: 36, train loss: 2.05017, val loss: 2.13012\n",
      "Main effects training epoch: 37, train loss: 2.04798, val loss: 2.12772\n",
      "Main effects training epoch: 38, train loss: 2.01218, val loss: 2.09233\n",
      "Main effects training epoch: 39, train loss: 2.00603, val loss: 2.08053\n",
      "Main effects training epoch: 40, train loss: 1.96716, val loss: 2.04704\n",
      "Main effects training epoch: 41, train loss: 1.97735, val loss: 2.05075\n",
      "Main effects training epoch: 42, train loss: 1.94721, val loss: 2.02261\n",
      "Main effects training epoch: 43, train loss: 1.92709, val loss: 2.00363\n",
      "Main effects training epoch: 44, train loss: 1.93875, val loss: 2.00794\n",
      "Main effects training epoch: 45, train loss: 1.90079, val loss: 1.97669\n",
      "Main effects training epoch: 46, train loss: 1.89560, val loss: 1.96291\n",
      "Main effects training epoch: 47, train loss: 1.89328, val loss: 1.97001\n",
      "Main effects training epoch: 48, train loss: 1.88197, val loss: 1.95118\n",
      "Main effects training epoch: 49, train loss: 1.85841, val loss: 1.92544\n",
      "Main effects training epoch: 50, train loss: 1.85590, val loss: 1.92914\n",
      "Main effects training epoch: 51, train loss: 1.83971, val loss: 1.90862\n",
      "Main effects training epoch: 52, train loss: 1.82851, val loss: 1.89208\n",
      "Main effects training epoch: 53, train loss: 1.82679, val loss: 1.89282\n",
      "Main effects training epoch: 54, train loss: 1.80935, val loss: 1.87578\n",
      "Main effects training epoch: 55, train loss: 1.81048, val loss: 1.87820\n",
      "Main effects training epoch: 56, train loss: 1.78480, val loss: 1.84562\n",
      "Main effects training epoch: 57, train loss: 1.79908, val loss: 1.86719\n",
      "Main effects training epoch: 58, train loss: 1.77702, val loss: 1.83930\n",
      "Main effects training epoch: 59, train loss: 1.77793, val loss: 1.84381\n",
      "Main effects training epoch: 60, train loss: 1.76725, val loss: 1.83031\n",
      "Main effects training epoch: 61, train loss: 1.77160, val loss: 1.83739\n",
      "Main effects training epoch: 62, train loss: 1.75614, val loss: 1.81753\n",
      "Main effects training epoch: 63, train loss: 1.76112, val loss: 1.82492\n",
      "Main effects training epoch: 64, train loss: 1.75032, val loss: 1.80944\n",
      "Main effects training epoch: 65, train loss: 1.76113, val loss: 1.83528\n",
      "Main effects training epoch: 66, train loss: 1.74814, val loss: 1.80187\n",
      "Main effects training epoch: 67, train loss: 1.74741, val loss: 1.81483\n",
      "Main effects training epoch: 68, train loss: 1.72934, val loss: 1.78352\n",
      "Main effects training epoch: 69, train loss: 1.74420, val loss: 1.81141\n",
      "Main effects training epoch: 70, train loss: 1.73315, val loss: 1.79543\n",
      "Main effects training epoch: 71, train loss: 1.72711, val loss: 1.78532\n",
      "Main effects training epoch: 72, train loss: 1.72482, val loss: 1.78428\n",
      "Main effects training epoch: 73, train loss: 1.72895, val loss: 1.79314\n",
      "Main effects training epoch: 74, train loss: 1.71333, val loss: 1.77186\n",
      "Main effects training epoch: 75, train loss: 1.72468, val loss: 1.78798\n",
      "Main effects training epoch: 76, train loss: 1.71459, val loss: 1.77710\n",
      "Main effects training epoch: 77, train loss: 1.70535, val loss: 1.76099\n",
      "Main effects training epoch: 78, train loss: 1.71375, val loss: 1.78090\n",
      "Main effects training epoch: 79, train loss: 1.70743, val loss: 1.76799\n",
      "Main effects training epoch: 80, train loss: 1.70381, val loss: 1.76880\n",
      "Main effects training epoch: 81, train loss: 1.70140, val loss: 1.76810\n",
      "Main effects training epoch: 82, train loss: 1.70002, val loss: 1.76238\n",
      "Main effects training epoch: 83, train loss: 1.69528, val loss: 1.75276\n",
      "Main effects training epoch: 84, train loss: 1.69168, val loss: 1.75476\n",
      "Main effects training epoch: 85, train loss: 1.69840, val loss: 1.75396\n",
      "Main effects training epoch: 86, train loss: 1.69058, val loss: 1.75216\n",
      "Main effects training epoch: 87, train loss: 1.68967, val loss: 1.74774\n",
      "Main effects training epoch: 88, train loss: 1.68791, val loss: 1.74111\n",
      "Main effects training epoch: 89, train loss: 1.68345, val loss: 1.74114\n",
      "Main effects training epoch: 90, train loss: 1.68324, val loss: 1.74105\n",
      "Main effects training epoch: 91, train loss: 1.68439, val loss: 1.73959\n",
      "Main effects training epoch: 92, train loss: 1.68006, val loss: 1.73721\n",
      "Main effects training epoch: 93, train loss: 1.67653, val loss: 1.73046\n",
      "Main effects training epoch: 94, train loss: 1.67448, val loss: 1.73422\n",
      "Main effects training epoch: 95, train loss: 1.67212, val loss: 1.72963\n",
      "Main effects training epoch: 96, train loss: 1.66840, val loss: 1.72660\n",
      "Main effects training epoch: 97, train loss: 1.66817, val loss: 1.72383\n",
      "Main effects training epoch: 98, train loss: 1.66389, val loss: 1.71549\n",
      "Main effects training epoch: 99, train loss: 1.65986, val loss: 1.72304\n",
      "Main effects training epoch: 100, train loss: 1.66380, val loss: 1.72058\n",
      "Main effects training epoch: 101, train loss: 1.65146, val loss: 1.70800\n",
      "Main effects training epoch: 102, train loss: 1.65256, val loss: 1.70762\n",
      "Main effects training epoch: 103, train loss: 1.64817, val loss: 1.70959\n",
      "Main effects training epoch: 104, train loss: 1.63533, val loss: 1.70164\n",
      "Main effects training epoch: 105, train loss: 1.63857, val loss: 1.69215\n",
      "Main effects training epoch: 106, train loss: 1.62170, val loss: 1.68649\n",
      "Main effects training epoch: 107, train loss: 1.61561, val loss: 1.68562\n",
      "Main effects training epoch: 108, train loss: 1.61681, val loss: 1.68804\n",
      "Main effects training epoch: 109, train loss: 1.60578, val loss: 1.68061\n",
      "Main effects training epoch: 110, train loss: 1.60430, val loss: 1.67285\n",
      "Main effects training epoch: 111, train loss: 1.60367, val loss: 1.68678\n",
      "Main effects training epoch: 112, train loss: 1.60683, val loss: 1.67735\n",
      "Main effects training epoch: 113, train loss: 1.59141, val loss: 1.67976\n",
      "Main effects training epoch: 114, train loss: 1.60306, val loss: 1.67426\n",
      "Main effects training epoch: 115, train loss: 1.58979, val loss: 1.66507\n",
      "Main effects training epoch: 116, train loss: 1.58438, val loss: 1.66640\n",
      "Main effects training epoch: 117, train loss: 1.59445, val loss: 1.67548\n",
      "Main effects training epoch: 118, train loss: 1.58427, val loss: 1.66652\n",
      "Main effects training epoch: 119, train loss: 1.57772, val loss: 1.65397\n",
      "Main effects training epoch: 120, train loss: 1.58224, val loss: 1.66366\n",
      "Main effects training epoch: 121, train loss: 1.57915, val loss: 1.66098\n",
      "Main effects training epoch: 122, train loss: 1.57304, val loss: 1.65303\n",
      "Main effects training epoch: 123, train loss: 1.57881, val loss: 1.65764\n",
      "Main effects training epoch: 124, train loss: 1.57421, val loss: 1.65945\n",
      "Main effects training epoch: 125, train loss: 1.57383, val loss: 1.65186\n",
      "Main effects training epoch: 126, train loss: 1.57470, val loss: 1.65305\n",
      "Main effects training epoch: 127, train loss: 1.57006, val loss: 1.64339\n",
      "Main effects training epoch: 128, train loss: 1.56755, val loss: 1.64699\n",
      "Main effects training epoch: 129, train loss: 1.57268, val loss: 1.64550\n",
      "Main effects training epoch: 130, train loss: 1.57520, val loss: 1.65649\n",
      "Main effects training epoch: 131, train loss: 1.56562, val loss: 1.64618\n",
      "Main effects training epoch: 132, train loss: 1.57565, val loss: 1.65474\n",
      "Main effects training epoch: 133, train loss: 1.56291, val loss: 1.63836\n",
      "Main effects training epoch: 134, train loss: 1.56127, val loss: 1.63724\n",
      "Main effects training epoch: 135, train loss: 1.56155, val loss: 1.64171\n",
      "Main effects training epoch: 136, train loss: 1.56116, val loss: 1.63233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 137, train loss: 1.57210, val loss: 1.65457\n",
      "Main effects training epoch: 138, train loss: 1.56037, val loss: 1.62353\n",
      "Main effects training epoch: 139, train loss: 1.57551, val loss: 1.66517\n",
      "Main effects training epoch: 140, train loss: 1.56154, val loss: 1.63180\n",
      "Main effects training epoch: 141, train loss: 1.55686, val loss: 1.63875\n",
      "Main effects training epoch: 142, train loss: 1.55974, val loss: 1.62836\n",
      "Main effects training epoch: 143, train loss: 1.55589, val loss: 1.63888\n",
      "Main effects training epoch: 144, train loss: 1.55726, val loss: 1.62721\n",
      "Main effects training epoch: 145, train loss: 1.56383, val loss: 1.62461\n",
      "Main effects training epoch: 146, train loss: 1.55828, val loss: 1.65192\n",
      "Main effects training epoch: 147, train loss: 1.55257, val loss: 1.61462\n",
      "Main effects training epoch: 148, train loss: 1.55500, val loss: 1.63568\n",
      "Main effects training epoch: 149, train loss: 1.55295, val loss: 1.62642\n",
      "Main effects training epoch: 150, train loss: 1.55130, val loss: 1.62678\n",
      "Main effects training epoch: 151, train loss: 1.57070, val loss: 1.63460\n",
      "Main effects training epoch: 152, train loss: 1.55719, val loss: 1.64094\n",
      "Main effects training epoch: 153, train loss: 1.54948, val loss: 1.62193\n",
      "Main effects training epoch: 154, train loss: 1.54995, val loss: 1.62659\n",
      "Main effects training epoch: 155, train loss: 1.55148, val loss: 1.63084\n",
      "Main effects training epoch: 156, train loss: 1.54791, val loss: 1.61980\n",
      "Main effects training epoch: 157, train loss: 1.54419, val loss: 1.61332\n",
      "Main effects training epoch: 158, train loss: 1.54769, val loss: 1.63358\n",
      "Main effects training epoch: 159, train loss: 1.54534, val loss: 1.61921\n",
      "Main effects training epoch: 160, train loss: 1.54207, val loss: 1.61551\n",
      "Main effects training epoch: 161, train loss: 1.53936, val loss: 1.61474\n",
      "Main effects training epoch: 162, train loss: 1.54232, val loss: 1.61546\n",
      "Main effects training epoch: 163, train loss: 1.53878, val loss: 1.61941\n",
      "Main effects training epoch: 164, train loss: 1.55344, val loss: 1.62601\n",
      "Main effects training epoch: 165, train loss: 1.55540, val loss: 1.62364\n",
      "Main effects training epoch: 166, train loss: 1.54157, val loss: 1.61911\n",
      "Main effects training epoch: 167, train loss: 1.53977, val loss: 1.60997\n",
      "Main effects training epoch: 168, train loss: 1.53679, val loss: 1.61230\n",
      "Main effects training epoch: 169, train loss: 1.54625, val loss: 1.62328\n",
      "Main effects training epoch: 170, train loss: 1.53927, val loss: 1.61810\n",
      "Main effects training epoch: 171, train loss: 1.53250, val loss: 1.59995\n",
      "Main effects training epoch: 172, train loss: 1.53705, val loss: 1.61830\n",
      "Main effects training epoch: 173, train loss: 1.53537, val loss: 1.61031\n",
      "Main effects training epoch: 174, train loss: 1.53033, val loss: 1.61283\n",
      "Main effects training epoch: 175, train loss: 1.52998, val loss: 1.59556\n",
      "Main effects training epoch: 176, train loss: 1.53059, val loss: 1.60541\n",
      "Main effects training epoch: 177, train loss: 1.53269, val loss: 1.59205\n",
      "Main effects training epoch: 178, train loss: 1.53352, val loss: 1.61473\n",
      "Main effects training epoch: 179, train loss: 1.54179, val loss: 1.61654\n",
      "Main effects training epoch: 180, train loss: 1.54929, val loss: 1.61806\n",
      "Main effects training epoch: 181, train loss: 1.54738, val loss: 1.60886\n",
      "Main effects training epoch: 182, train loss: 1.54348, val loss: 1.62340\n",
      "Main effects training epoch: 183, train loss: 1.52900, val loss: 1.60071\n",
      "Main effects training epoch: 184, train loss: 1.52391, val loss: 1.59319\n",
      "Main effects training epoch: 185, train loss: 1.53682, val loss: 1.60736\n",
      "Main effects training epoch: 186, train loss: 1.52704, val loss: 1.59625\n",
      "Main effects training epoch: 187, train loss: 1.52400, val loss: 1.59273\n",
      "Main effects training epoch: 188, train loss: 1.51772, val loss: 1.59140\n",
      "Main effects training epoch: 189, train loss: 1.51645, val loss: 1.57918\n",
      "Main effects training epoch: 190, train loss: 1.52106, val loss: 1.60360\n",
      "Main effects training epoch: 191, train loss: 1.52330, val loss: 1.58191\n",
      "Main effects training epoch: 192, train loss: 1.51263, val loss: 1.59259\n",
      "Main effects training epoch: 193, train loss: 1.51940, val loss: 1.58913\n",
      "Main effects training epoch: 194, train loss: 1.51359, val loss: 1.57782\n",
      "Main effects training epoch: 195, train loss: 1.52123, val loss: 1.60143\n",
      "Main effects training epoch: 196, train loss: 1.51222, val loss: 1.58582\n",
      "Main effects training epoch: 197, train loss: 1.51648, val loss: 1.58135\n",
      "Main effects training epoch: 198, train loss: 1.52573, val loss: 1.60515\n",
      "Main effects training epoch: 199, train loss: 1.51626, val loss: 1.57840\n",
      "Main effects training epoch: 200, train loss: 1.51642, val loss: 1.58938\n",
      "Main effects training epoch: 201, train loss: 1.51380, val loss: 1.58348\n",
      "Main effects training epoch: 202, train loss: 1.51491, val loss: 1.58058\n",
      "Main effects training epoch: 203, train loss: 1.51597, val loss: 1.58061\n",
      "Main effects training epoch: 204, train loss: 1.51727, val loss: 1.58092\n",
      "Main effects training epoch: 205, train loss: 1.50791, val loss: 1.58639\n",
      "Main effects training epoch: 206, train loss: 1.50755, val loss: 1.57711\n",
      "Main effects training epoch: 207, train loss: 1.50424, val loss: 1.56548\n",
      "Main effects training epoch: 208, train loss: 1.51033, val loss: 1.58461\n",
      "Main effects training epoch: 209, train loss: 1.50183, val loss: 1.57029\n",
      "Main effects training epoch: 210, train loss: 1.50023, val loss: 1.56272\n",
      "Main effects training epoch: 211, train loss: 1.50437, val loss: 1.57446\n",
      "Main effects training epoch: 212, train loss: 1.49852, val loss: 1.55223\n",
      "Main effects training epoch: 213, train loss: 1.49635, val loss: 1.56643\n",
      "Main effects training epoch: 214, train loss: 1.50573, val loss: 1.56716\n",
      "Main effects training epoch: 215, train loss: 1.50598, val loss: 1.57390\n",
      "Main effects training epoch: 216, train loss: 1.49691, val loss: 1.56212\n",
      "Main effects training epoch: 217, train loss: 1.51106, val loss: 1.56065\n",
      "Main effects training epoch: 218, train loss: 1.50945, val loss: 1.56819\n",
      "Main effects training epoch: 219, train loss: 1.50015, val loss: 1.55477\n",
      "Main effects training epoch: 220, train loss: 1.49192, val loss: 1.55303\n",
      "Main effects training epoch: 221, train loss: 1.50005, val loss: 1.56195\n",
      "Main effects training epoch: 222, train loss: 1.49564, val loss: 1.55273\n",
      "Main effects training epoch: 223, train loss: 1.49945, val loss: 1.54736\n",
      "Main effects training epoch: 224, train loss: 1.50365, val loss: 1.56749\n",
      "Main effects training epoch: 225, train loss: 1.49344, val loss: 1.56031\n",
      "Main effects training epoch: 226, train loss: 1.49294, val loss: 1.55534\n",
      "Main effects training epoch: 227, train loss: 1.49410, val loss: 1.55022\n",
      "Main effects training epoch: 228, train loss: 1.49007, val loss: 1.55232\n",
      "Main effects training epoch: 229, train loss: 1.48666, val loss: 1.55247\n",
      "Main effects training epoch: 230, train loss: 1.48672, val loss: 1.53660\n",
      "Main effects training epoch: 231, train loss: 1.49489, val loss: 1.56093\n",
      "Main effects training epoch: 232, train loss: 1.49271, val loss: 1.53255\n",
      "Main effects training epoch: 233, train loss: 1.49039, val loss: 1.55251\n",
      "Main effects training epoch: 234, train loss: 1.48968, val loss: 1.53425\n",
      "Main effects training epoch: 235, train loss: 1.49617, val loss: 1.54244\n",
      "Main effects training epoch: 236, train loss: 1.49556, val loss: 1.54519\n",
      "Main effects training epoch: 237, train loss: 1.50157, val loss: 1.54036\n",
      "Main effects training epoch: 238, train loss: 1.49286, val loss: 1.54025\n",
      "Main effects training epoch: 239, train loss: 1.48348, val loss: 1.54075\n",
      "Main effects training epoch: 240, train loss: 1.47866, val loss: 1.52798\n",
      "Main effects training epoch: 241, train loss: 1.48454, val loss: 1.53785\n",
      "Main effects training epoch: 242, train loss: 1.48915, val loss: 1.53385\n",
      "Main effects training epoch: 243, train loss: 1.47804, val loss: 1.52226\n",
      "Main effects training epoch: 244, train loss: 1.47633, val loss: 1.52852\n",
      "Main effects training epoch: 245, train loss: 1.48122, val loss: 1.54567\n",
      "Main effects training epoch: 246, train loss: 1.49038, val loss: 1.53486\n",
      "Main effects training epoch: 247, train loss: 1.47888, val loss: 1.53396\n",
      "Main effects training epoch: 248, train loss: 1.48218, val loss: 1.54290\n",
      "Main effects training epoch: 249, train loss: 1.47999, val loss: 1.54030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 250, train loss: 1.47879, val loss: 1.52604\n",
      "Main effects training epoch: 251, train loss: 1.47895, val loss: 1.53793\n",
      "Main effects training epoch: 252, train loss: 1.48080, val loss: 1.52918\n",
      "Main effects training epoch: 253, train loss: 1.47489, val loss: 1.51043\n",
      "Main effects training epoch: 254, train loss: 1.48689, val loss: 1.56151\n",
      "Main effects training epoch: 255, train loss: 1.49374, val loss: 1.52245\n",
      "Main effects training epoch: 256, train loss: 1.48346, val loss: 1.55561\n",
      "Main effects training epoch: 257, train loss: 1.47276, val loss: 1.51593\n",
      "Main effects training epoch: 258, train loss: 1.47398, val loss: 1.52060\n",
      "Main effects training epoch: 259, train loss: 1.46937, val loss: 1.51195\n",
      "Main effects training epoch: 260, train loss: 1.46840, val loss: 1.53292\n",
      "Main effects training epoch: 261, train loss: 1.47059, val loss: 1.50329\n",
      "Main effects training epoch: 262, train loss: 1.46711, val loss: 1.52141\n",
      "Main effects training epoch: 263, train loss: 1.46987, val loss: 1.50722\n",
      "Main effects training epoch: 264, train loss: 1.46661, val loss: 1.52007\n",
      "Main effects training epoch: 265, train loss: 1.47238, val loss: 1.52087\n",
      "Main effects training epoch: 266, train loss: 1.47517, val loss: 1.52755\n",
      "Main effects training epoch: 267, train loss: 1.47257, val loss: 1.51720\n",
      "Main effects training epoch: 268, train loss: 1.46696, val loss: 1.53558\n",
      "Main effects training epoch: 269, train loss: 1.46074, val loss: 1.49852\n",
      "Main effects training epoch: 270, train loss: 1.46629, val loss: 1.52216\n",
      "Main effects training epoch: 271, train loss: 1.45585, val loss: 1.50371\n",
      "Main effects training epoch: 272, train loss: 1.46514, val loss: 1.52077\n",
      "Main effects training epoch: 273, train loss: 1.46275, val loss: 1.51426\n",
      "Main effects training epoch: 274, train loss: 1.45829, val loss: 1.50718\n",
      "Main effects training epoch: 275, train loss: 1.45746, val loss: 1.50669\n",
      "Main effects training epoch: 276, train loss: 1.46218, val loss: 1.49900\n",
      "Main effects training epoch: 277, train loss: 1.46026, val loss: 1.50077\n",
      "Main effects training epoch: 278, train loss: 1.46856, val loss: 1.51782\n",
      "Main effects training epoch: 279, train loss: 1.46544, val loss: 1.50195\n",
      "Main effects training epoch: 280, train loss: 1.45430, val loss: 1.49798\n",
      "Main effects training epoch: 281, train loss: 1.45636, val loss: 1.50303\n",
      "Main effects training epoch: 282, train loss: 1.46012, val loss: 1.50786\n",
      "Main effects training epoch: 283, train loss: 1.45667, val loss: 1.49180\n",
      "Main effects training epoch: 284, train loss: 1.46529, val loss: 1.50067\n",
      "Main effects training epoch: 285, train loss: 1.45483, val loss: 1.49814\n",
      "Main effects training epoch: 286, train loss: 1.45827, val loss: 1.52077\n",
      "Main effects training epoch: 287, train loss: 1.45682, val loss: 1.51010\n",
      "Main effects training epoch: 288, train loss: 1.44900, val loss: 1.49536\n",
      "Main effects training epoch: 289, train loss: 1.45049, val loss: 1.49063\n",
      "Main effects training epoch: 290, train loss: 1.45399, val loss: 1.50077\n",
      "Main effects training epoch: 291, train loss: 1.45799, val loss: 1.49651\n",
      "Main effects training epoch: 292, train loss: 1.45468, val loss: 1.51145\n",
      "Main effects training epoch: 293, train loss: 1.45942, val loss: 1.47844\n",
      "Main effects training epoch: 294, train loss: 1.44727, val loss: 1.49723\n",
      "Main effects training epoch: 295, train loss: 1.44395, val loss: 1.49513\n",
      "Main effects training epoch: 296, train loss: 1.44926, val loss: 1.49267\n",
      "Main effects training epoch: 297, train loss: 1.45201, val loss: 1.49799\n",
      "Main effects training epoch: 298, train loss: 1.45513, val loss: 1.50176\n",
      "Main effects training epoch: 299, train loss: 1.44854, val loss: 1.50216\n",
      "Main effects training epoch: 300, train loss: 1.44739, val loss: 1.48551\n",
      "##########Stage 1: main effect training stop.##########\n",
      "3 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.46663, val loss: 1.50210\n",
      "Main effects tuning epoch: 2, train loss: 1.47040, val loss: 1.49827\n",
      "Main effects tuning epoch: 3, train loss: 1.46733, val loss: 1.49980\n",
      "Main effects tuning epoch: 4, train loss: 1.47006, val loss: 1.49289\n",
      "Main effects tuning epoch: 5, train loss: 1.46417, val loss: 1.49379\n",
      "Main effects tuning epoch: 6, train loss: 1.46684, val loss: 1.50928\n",
      "Main effects tuning epoch: 7, train loss: 1.47818, val loss: 1.50387\n",
      "Main effects tuning epoch: 8, train loss: 1.46547, val loss: 1.49443\n",
      "Main effects tuning epoch: 9, train loss: 1.46926, val loss: 1.49150\n",
      "Main effects tuning epoch: 10, train loss: 1.46575, val loss: 1.49988\n",
      "Main effects tuning epoch: 11, train loss: 1.46048, val loss: 1.49189\n",
      "Main effects tuning epoch: 12, train loss: 1.46263, val loss: 1.49274\n",
      "Main effects tuning epoch: 13, train loss: 1.47037, val loss: 1.49916\n",
      "Main effects tuning epoch: 14, train loss: 1.48534, val loss: 1.52617\n",
      "Main effects tuning epoch: 15, train loss: 1.47336, val loss: 1.51263\n",
      "Main effects tuning epoch: 16, train loss: 1.47558, val loss: 1.49158\n",
      "Main effects tuning epoch: 17, train loss: 1.46992, val loss: 1.48996\n",
      "Main effects tuning epoch: 18, train loss: 1.47654, val loss: 1.50080\n",
      "Main effects tuning epoch: 19, train loss: 1.47729, val loss: 1.49772\n",
      "Main effects tuning epoch: 20, train loss: 1.46194, val loss: 1.49663\n",
      "Main effects tuning epoch: 21, train loss: 1.46297, val loss: 1.49150\n",
      "Main effects tuning epoch: 22, train loss: 1.45946, val loss: 1.48869\n",
      "Main effects tuning epoch: 23, train loss: 1.45708, val loss: 1.49168\n",
      "Main effects tuning epoch: 24, train loss: 1.45580, val loss: 1.48903\n",
      "Main effects tuning epoch: 25, train loss: 1.46081, val loss: 1.49226\n",
      "Main effects tuning epoch: 26, train loss: 1.46099, val loss: 1.51029\n",
      "Main effects tuning epoch: 27, train loss: 1.46626, val loss: 1.48389\n",
      "Main effects tuning epoch: 28, train loss: 1.47109, val loss: 1.48983\n",
      "Main effects tuning epoch: 29, train loss: 1.46498, val loss: 1.49008\n",
      "Main effects tuning epoch: 30, train loss: 1.47223, val loss: 1.51165\n",
      "Main effects tuning epoch: 31, train loss: 1.46215, val loss: 1.48963\n",
      "Main effects tuning epoch: 32, train loss: 1.45897, val loss: 1.49698\n",
      "Main effects tuning epoch: 33, train loss: 1.46120, val loss: 1.48922\n",
      "Main effects tuning epoch: 34, train loss: 1.45962, val loss: 1.49014\n",
      "Main effects tuning epoch: 35, train loss: 1.46845, val loss: 1.50302\n",
      "Main effects tuning epoch: 36, train loss: 1.45754, val loss: 1.49544\n",
      "Main effects tuning epoch: 37, train loss: 1.45547, val loss: 1.49106\n",
      "Main effects tuning epoch: 38, train loss: 1.46464, val loss: 1.50560\n",
      "Main effects tuning epoch: 39, train loss: 1.45800, val loss: 1.50060\n",
      "Main effects tuning epoch: 40, train loss: 1.45685, val loss: 1.49910\n",
      "Main effects tuning epoch: 41, train loss: 1.45964, val loss: 1.49062\n",
      "Main effects tuning epoch: 42, train loss: 1.45300, val loss: 1.49123\n",
      "Main effects tuning epoch: 43, train loss: 1.46163, val loss: 1.49243\n",
      "Main effects tuning epoch: 44, train loss: 1.46911, val loss: 1.49207\n",
      "Main effects tuning epoch: 45, train loss: 1.45826, val loss: 1.49389\n",
      "Main effects tuning epoch: 46, train loss: 1.45918, val loss: 1.49226\n",
      "Main effects tuning epoch: 47, train loss: 1.46510, val loss: 1.48911\n",
      "Main effects tuning epoch: 48, train loss: 1.45575, val loss: 1.48617\n",
      "Main effects tuning epoch: 49, train loss: 1.45134, val loss: 1.49274\n",
      "Main effects tuning epoch: 50, train loss: 1.46566, val loss: 1.51290\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.41516, val loss: 1.42834\n",
      "Interaction training epoch: 2, train loss: 1.55572, val loss: 1.54991\n",
      "Interaction training epoch: 3, train loss: 1.16793, val loss: 1.16959\n",
      "Interaction training epoch: 4, train loss: 1.07333, val loss: 1.07712\n",
      "Interaction training epoch: 5, train loss: 1.06547, val loss: 1.06367\n",
      "Interaction training epoch: 6, train loss: 1.04701, val loss: 1.05401\n",
      "Interaction training epoch: 7, train loss: 1.03642, val loss: 1.05066\n",
      "Interaction training epoch: 8, train loss: 1.03701, val loss: 1.04922\n",
      "Interaction training epoch: 9, train loss: 1.04310, val loss: 1.05474\n",
      "Interaction training epoch: 10, train loss: 1.01375, val loss: 1.03043\n",
      "Interaction training epoch: 11, train loss: 1.00115, val loss: 1.02061\n",
      "Interaction training epoch: 12, train loss: 0.96872, val loss: 0.97810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 13, train loss: 0.96140, val loss: 0.96743\n",
      "Interaction training epoch: 14, train loss: 0.95477, val loss: 0.95348\n",
      "Interaction training epoch: 15, train loss: 0.94748, val loss: 0.94551\n",
      "Interaction training epoch: 16, train loss: 0.94507, val loss: 0.94008\n",
      "Interaction training epoch: 17, train loss: 0.95756, val loss: 0.95194\n",
      "Interaction training epoch: 18, train loss: 0.93276, val loss: 0.92951\n",
      "Interaction training epoch: 19, train loss: 0.91915, val loss: 0.91056\n",
      "Interaction training epoch: 20, train loss: 0.92465, val loss: 0.91385\n",
      "Interaction training epoch: 21, train loss: 0.90788, val loss: 0.89621\n",
      "Interaction training epoch: 22, train loss: 0.92427, val loss: 0.90516\n",
      "Interaction training epoch: 23, train loss: 0.89179, val loss: 0.88000\n",
      "Interaction training epoch: 24, train loss: 0.91545, val loss: 0.89518\n",
      "Interaction training epoch: 25, train loss: 0.90416, val loss: 0.89270\n",
      "Interaction training epoch: 26, train loss: 0.89133, val loss: 0.87810\n",
      "Interaction training epoch: 27, train loss: 0.90213, val loss: 0.87904\n",
      "Interaction training epoch: 28, train loss: 0.91357, val loss: 0.89532\n",
      "Interaction training epoch: 29, train loss: 0.88852, val loss: 0.87214\n",
      "Interaction training epoch: 30, train loss: 0.88124, val loss: 0.86182\n",
      "Interaction training epoch: 31, train loss: 0.88936, val loss: 0.87819\n",
      "Interaction training epoch: 32, train loss: 0.87767, val loss: 0.86483\n",
      "Interaction training epoch: 33, train loss: 0.87680, val loss: 0.86053\n",
      "Interaction training epoch: 34, train loss: 0.87208, val loss: 0.86188\n",
      "Interaction training epoch: 35, train loss: 0.87614, val loss: 0.85943\n",
      "Interaction training epoch: 36, train loss: 0.86847, val loss: 0.85323\n",
      "Interaction training epoch: 37, train loss: 0.87060, val loss: 0.85665\n",
      "Interaction training epoch: 38, train loss: 0.86376, val loss: 0.85304\n",
      "Interaction training epoch: 39, train loss: 0.87419, val loss: 0.86978\n",
      "Interaction training epoch: 40, train loss: 0.87804, val loss: 0.86315\n",
      "Interaction training epoch: 41, train loss: 0.87861, val loss: 0.86641\n",
      "Interaction training epoch: 42, train loss: 0.86275, val loss: 0.84800\n",
      "Interaction training epoch: 43, train loss: 0.87209, val loss: 0.86390\n",
      "Interaction training epoch: 44, train loss: 0.86572, val loss: 0.85177\n",
      "Interaction training epoch: 45, train loss: 0.86386, val loss: 0.84461\n",
      "Interaction training epoch: 46, train loss: 0.86336, val loss: 0.85573\n",
      "Interaction training epoch: 47, train loss: 0.86246, val loss: 0.84851\n",
      "Interaction training epoch: 48, train loss: 0.86861, val loss: 0.85385\n",
      "Interaction training epoch: 49, train loss: 0.85798, val loss: 0.85648\n",
      "Interaction training epoch: 50, train loss: 0.85640, val loss: 0.84289\n",
      "Interaction training epoch: 51, train loss: 0.85892, val loss: 0.84579\n",
      "Interaction training epoch: 52, train loss: 0.84348, val loss: 0.83405\n",
      "Interaction training epoch: 53, train loss: 0.86365, val loss: 0.85266\n",
      "Interaction training epoch: 54, train loss: 0.85443, val loss: 0.84129\n",
      "Interaction training epoch: 55, train loss: 0.84644, val loss: 0.84724\n",
      "Interaction training epoch: 56, train loss: 0.85410, val loss: 0.84215\n",
      "Interaction training epoch: 57, train loss: 0.85385, val loss: 0.84726\n",
      "Interaction training epoch: 58, train loss: 0.85332, val loss: 0.84241\n",
      "Interaction training epoch: 59, train loss: 0.84677, val loss: 0.83744\n",
      "Interaction training epoch: 60, train loss: 0.84968, val loss: 0.84736\n",
      "Interaction training epoch: 61, train loss: 0.84933, val loss: 0.83585\n",
      "Interaction training epoch: 62, train loss: 0.85567, val loss: 0.84830\n",
      "Interaction training epoch: 63, train loss: 0.84782, val loss: 0.83980\n",
      "Interaction training epoch: 64, train loss: 0.85100, val loss: 0.84098\n",
      "Interaction training epoch: 65, train loss: 0.84795, val loss: 0.84032\n",
      "Interaction training epoch: 66, train loss: 0.84235, val loss: 0.83570\n",
      "Interaction training epoch: 67, train loss: 0.83941, val loss: 0.82893\n",
      "Interaction training epoch: 68, train loss: 0.84191, val loss: 0.84360\n",
      "Interaction training epoch: 69, train loss: 0.83572, val loss: 0.83383\n",
      "Interaction training epoch: 70, train loss: 0.83862, val loss: 0.83321\n",
      "Interaction training epoch: 71, train loss: 0.84398, val loss: 0.83857\n",
      "Interaction training epoch: 72, train loss: 0.85112, val loss: 0.84787\n",
      "Interaction training epoch: 73, train loss: 0.84684, val loss: 0.84774\n",
      "Interaction training epoch: 74, train loss: 0.83538, val loss: 0.82524\n",
      "Interaction training epoch: 75, train loss: 0.84994, val loss: 0.85005\n",
      "Interaction training epoch: 76, train loss: 0.84109, val loss: 0.83468\n",
      "Interaction training epoch: 77, train loss: 0.84777, val loss: 0.83998\n",
      "Interaction training epoch: 78, train loss: 0.84609, val loss: 0.84088\n",
      "Interaction training epoch: 79, train loss: 0.84260, val loss: 0.84098\n",
      "Interaction training epoch: 80, train loss: 0.83344, val loss: 0.83002\n",
      "Interaction training epoch: 81, train loss: 0.83637, val loss: 0.83616\n",
      "Interaction training epoch: 82, train loss: 0.83472, val loss: 0.82974\n",
      "Interaction training epoch: 83, train loss: 0.83589, val loss: 0.83403\n",
      "Interaction training epoch: 84, train loss: 0.83736, val loss: 0.83466\n",
      "Interaction training epoch: 85, train loss: 0.83774, val loss: 0.83221\n",
      "Interaction training epoch: 86, train loss: 0.83458, val loss: 0.82449\n",
      "Interaction training epoch: 87, train loss: 0.83148, val loss: 0.83435\n",
      "Interaction training epoch: 88, train loss: 0.82865, val loss: 0.82753\n",
      "Interaction training epoch: 89, train loss: 0.84322, val loss: 0.82999\n",
      "Interaction training epoch: 90, train loss: 0.85321, val loss: 0.85237\n",
      "Interaction training epoch: 91, train loss: 0.83219, val loss: 0.82857\n",
      "Interaction training epoch: 92, train loss: 0.83957, val loss: 0.82915\n",
      "Interaction training epoch: 93, train loss: 0.83655, val loss: 0.83411\n",
      "Interaction training epoch: 94, train loss: 0.83517, val loss: 0.82686\n",
      "Interaction training epoch: 95, train loss: 0.84059, val loss: 0.83417\n",
      "Interaction training epoch: 96, train loss: 0.84039, val loss: 0.84303\n",
      "Interaction training epoch: 97, train loss: 0.82586, val loss: 0.82989\n",
      "Interaction training epoch: 98, train loss: 0.83923, val loss: 0.83296\n",
      "Interaction training epoch: 99, train loss: 0.83086, val loss: 0.82543\n",
      "Interaction training epoch: 100, train loss: 0.84010, val loss: 0.84295\n",
      "Interaction training epoch: 101, train loss: 0.83598, val loss: 0.82665\n",
      "Interaction training epoch: 102, train loss: 0.83389, val loss: 0.83074\n",
      "Interaction training epoch: 103, train loss: 0.82808, val loss: 0.82505\n",
      "Interaction training epoch: 104, train loss: 0.83040, val loss: 0.82955\n",
      "Interaction training epoch: 105, train loss: 0.82557, val loss: 0.82333\n",
      "Interaction training epoch: 106, train loss: 0.82816, val loss: 0.82476\n",
      "Interaction training epoch: 107, train loss: 0.82661, val loss: 0.82198\n",
      "Interaction training epoch: 108, train loss: 0.83568, val loss: 0.84318\n",
      "Interaction training epoch: 109, train loss: 0.83398, val loss: 0.82509\n",
      "Interaction training epoch: 110, train loss: 0.82881, val loss: 0.83473\n",
      "Interaction training epoch: 111, train loss: 0.83337, val loss: 0.82912\n",
      "Interaction training epoch: 112, train loss: 0.82888, val loss: 0.83200\n",
      "Interaction training epoch: 113, train loss: 0.82878, val loss: 0.82463\n",
      "Interaction training epoch: 114, train loss: 0.83110, val loss: 0.82951\n",
      "Interaction training epoch: 115, train loss: 0.82457, val loss: 0.83287\n",
      "Interaction training epoch: 116, train loss: 0.83000, val loss: 0.82665\n",
      "Interaction training epoch: 117, train loss: 0.83332, val loss: 0.83881\n",
      "Interaction training epoch: 118, train loss: 0.83366, val loss: 0.83617\n",
      "Interaction training epoch: 119, train loss: 0.83064, val loss: 0.82877\n",
      "Interaction training epoch: 120, train loss: 0.83549, val loss: 0.84452\n",
      "Interaction training epoch: 121, train loss: 0.85031, val loss: 0.84424\n",
      "Interaction training epoch: 122, train loss: 0.83550, val loss: 0.83348\n",
      "Interaction training epoch: 123, train loss: 0.83262, val loss: 0.82668\n",
      "Interaction training epoch: 124, train loss: 0.82232, val loss: 0.82843\n",
      "Interaction training epoch: 125, train loss: 0.82491, val loss: 0.82613\n",
      "Interaction training epoch: 126, train loss: 0.82847, val loss: 0.82597\n",
      "Interaction training epoch: 127, train loss: 0.82719, val loss: 0.83081\n",
      "Interaction training epoch: 128, train loss: 0.83626, val loss: 0.82446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 129, train loss: 0.83520, val loss: 0.83481\n",
      "Interaction training epoch: 130, train loss: 0.83193, val loss: 0.84108\n",
      "Interaction training epoch: 131, train loss: 0.82562, val loss: 0.82023\n",
      "Interaction training epoch: 132, train loss: 0.82708, val loss: 0.82618\n",
      "Interaction training epoch: 133, train loss: 0.82524, val loss: 0.82760\n",
      "Interaction training epoch: 134, train loss: 0.83546, val loss: 0.84443\n",
      "Interaction training epoch: 135, train loss: 0.83538, val loss: 0.82261\n",
      "Interaction training epoch: 136, train loss: 0.83697, val loss: 0.84186\n",
      "Interaction training epoch: 137, train loss: 0.82910, val loss: 0.83016\n",
      "Interaction training epoch: 138, train loss: 0.82363, val loss: 0.82933\n",
      "Interaction training epoch: 139, train loss: 0.82781, val loss: 0.83245\n",
      "Interaction training epoch: 140, train loss: 0.82277, val loss: 0.82159\n",
      "Interaction training epoch: 141, train loss: 0.82450, val loss: 0.83187\n",
      "Interaction training epoch: 142, train loss: 0.82675, val loss: 0.82052\n",
      "Interaction training epoch: 143, train loss: 0.81935, val loss: 0.82380\n",
      "Interaction training epoch: 144, train loss: 0.82791, val loss: 0.83338\n",
      "Interaction training epoch: 145, train loss: 0.82794, val loss: 0.82861\n",
      "Interaction training epoch: 146, train loss: 0.82923, val loss: 0.83261\n",
      "Interaction training epoch: 147, train loss: 0.82973, val loss: 0.83055\n",
      "Interaction training epoch: 148, train loss: 0.83217, val loss: 0.83880\n",
      "Interaction training epoch: 149, train loss: 0.82730, val loss: 0.82859\n",
      "Interaction training epoch: 150, train loss: 0.82351, val loss: 0.82627\n",
      "Interaction training epoch: 151, train loss: 0.82329, val loss: 0.82748\n",
      "Interaction training epoch: 152, train loss: 0.82523, val loss: 0.83019\n",
      "Interaction training epoch: 153, train loss: 0.82586, val loss: 0.82046\n",
      "Interaction training epoch: 154, train loss: 0.82222, val loss: 0.83021\n",
      "Interaction training epoch: 155, train loss: 0.82329, val loss: 0.82451\n",
      "Interaction training epoch: 156, train loss: 0.82303, val loss: 0.82332\n",
      "Interaction training epoch: 157, train loss: 0.82323, val loss: 0.82896\n",
      "Interaction training epoch: 158, train loss: 0.82649, val loss: 0.82779\n",
      "Interaction training epoch: 159, train loss: 0.83252, val loss: 0.83357\n",
      "Interaction training epoch: 160, train loss: 0.82862, val loss: 0.83090\n",
      "Interaction training epoch: 161, train loss: 0.83443, val loss: 0.83204\n",
      "Interaction training epoch: 162, train loss: 0.83315, val loss: 0.83543\n",
      "Interaction training epoch: 163, train loss: 0.82676, val loss: 0.83089\n",
      "Interaction training epoch: 164, train loss: 0.82563, val loss: 0.83283\n",
      "Interaction training epoch: 165, train loss: 0.82574, val loss: 0.82861\n",
      "Interaction training epoch: 166, train loss: 0.82225, val loss: 0.82204\n",
      "Interaction training epoch: 167, train loss: 0.82089, val loss: 0.83060\n",
      "Interaction training epoch: 168, train loss: 0.82632, val loss: 0.83141\n",
      "Interaction training epoch: 169, train loss: 0.82518, val loss: 0.82454\n",
      "Interaction training epoch: 170, train loss: 0.82446, val loss: 0.83109\n",
      "Interaction training epoch: 171, train loss: 0.82009, val loss: 0.82345\n",
      "Interaction training epoch: 172, train loss: 0.82488, val loss: 0.82865\n",
      "Interaction training epoch: 173, train loss: 0.82345, val loss: 0.82496\n",
      "Interaction training epoch: 174, train loss: 0.81726, val loss: 0.82154\n",
      "Interaction training epoch: 175, train loss: 0.82529, val loss: 0.83467\n",
      "Interaction training epoch: 176, train loss: 0.81863, val loss: 0.82310\n",
      "Interaction training epoch: 177, train loss: 0.83452, val loss: 0.83177\n",
      "Interaction training epoch: 178, train loss: 0.81879, val loss: 0.82171\n",
      "Interaction training epoch: 179, train loss: 0.82819, val loss: 0.83331\n",
      "Interaction training epoch: 180, train loss: 0.81862, val loss: 0.82206\n",
      "Interaction training epoch: 181, train loss: 0.82677, val loss: 0.83215\n",
      "Interaction training epoch: 182, train loss: 0.82118, val loss: 0.81525\n",
      "Interaction training epoch: 183, train loss: 0.82312, val loss: 0.82716\n",
      "Interaction training epoch: 184, train loss: 0.82128, val loss: 0.82421\n",
      "Interaction training epoch: 185, train loss: 0.82238, val loss: 0.82776\n",
      "Interaction training epoch: 186, train loss: 0.83009, val loss: 0.82531\n",
      "Interaction training epoch: 187, train loss: 0.82194, val loss: 0.82887\n",
      "Interaction training epoch: 188, train loss: 0.81824, val loss: 0.82804\n",
      "Interaction training epoch: 189, train loss: 0.81875, val loss: 0.82188\n",
      "Interaction training epoch: 190, train loss: 0.82082, val loss: 0.82855\n",
      "Interaction training epoch: 191, train loss: 0.82353, val loss: 0.82774\n",
      "Interaction training epoch: 192, train loss: 0.82478, val loss: 0.82088\n",
      "Interaction training epoch: 193, train loss: 0.82091, val loss: 0.82727\n",
      "Interaction training epoch: 194, train loss: 0.82049, val loss: 0.82170\n",
      "Interaction training epoch: 195, train loss: 0.82083, val loss: 0.82535\n",
      "Interaction training epoch: 196, train loss: 0.82049, val loss: 0.81906\n",
      "Interaction training epoch: 197, train loss: 0.81800, val loss: 0.82593\n",
      "Interaction training epoch: 198, train loss: 0.82277, val loss: 0.82581\n",
      "Interaction training epoch: 199, train loss: 0.82197, val loss: 0.83102\n",
      "Interaction training epoch: 200, train loss: 0.82953, val loss: 0.82411\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########No main interaction is pruned, the tuning step is skipped.\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 35.599629402160645\n",
      "After the gam stage, training error is 0.82953 , validation error is 0.82411\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 20.985671\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.686951 validation MAE=0.781831,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.638516 validation MAE=0.763159,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 3: observed MAE=0.597875 validation MAE=0.746237,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.563471 validation MAE=0.730694,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.534221 validation MAE=0.717217,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.509830 validation MAE=0.705055,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.489475 validation MAE=0.694336,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.471337 validation MAE=0.684166,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.454204 validation MAE=0.674835,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.438879 validation MAE=0.666565,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.426366 validation MAE=0.658722,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.413635 validation MAE=0.651487,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.402740 validation MAE=0.644964,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.393194 validation MAE=0.638652,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.384120 validation MAE=0.633183,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.375692 validation MAE=0.628438,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.368135 validation MAE=0.624323,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.361444 validation MAE=0.620026,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.356994 validation MAE=0.615866,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.352902 validation MAE=0.611676,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.346797 validation MAE=0.607581,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.341111 validation MAE=0.603401,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.336219 validation MAE=0.600495,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.332564 validation MAE=0.596988,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.327410 validation MAE=0.594580,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.324608 validation MAE=0.591977,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.320470 validation MAE=0.589258,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.316292 validation MAE=0.586289,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.313602 validation MAE=0.583976,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.311317 validation MAE=0.582734,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.307065 validation MAE=0.580194,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.303626 validation MAE=0.577401,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.301894 validation MAE=0.576153,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.299227 validation MAE=0.574538,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.296210 validation MAE=0.572008,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.293874 validation MAE=0.570474,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.291703 validation MAE=0.568757,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.289257 validation MAE=0.567680,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.287380 validation MAE=0.566096,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.285370 validation MAE=0.565025,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.283828 validation MAE=0.563471,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.281756 validation MAE=0.562532,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.279802 validation MAE=0.561668,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.277901 validation MAE=0.560343,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.276471 validation MAE=0.558924,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.274217 validation MAE=0.557955,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.273042 validation MAE=0.556709,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.272005 validation MAE=0.556482,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.270789 validation MAE=0.555397,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.268690 validation MAE=0.553782,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.267946 validation MAE=0.553160,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.266447 validation MAE=0.552578,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.264350 validation MAE=0.551693,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.262926 validation MAE=0.550996,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.262624 validation MAE=0.550563,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.260375 validation MAE=0.549518,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.260136 validation MAE=0.548766,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.259096 validation MAE=0.548697,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.258471 validation MAE=0.547735,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.256868 validation MAE=0.547416,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.256370 validation MAE=0.546411,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.254358 validation MAE=0.545825,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.254508 validation MAE=0.545818,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.252895 validation MAE=0.544899,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.251427 validation MAE=0.544205,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.250787 validation MAE=0.544432,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.249533 validation MAE=0.543427,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.247797 validation MAE=0.542837,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.247847 validation MAE=0.542519,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.247065 validation MAE=0.542651,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.246522 validation MAE=0.542113,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.246614 validation MAE=0.541637,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.245320 validation MAE=0.541160,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.244599 validation MAE=0.541223,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.243956 validation MAE=0.541133,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.242358 validation MAE=0.540627,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.242557 validation MAE=0.540273,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.241798 validation MAE=0.539744,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.240287 validation MAE=0.539311,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.239494 validation MAE=0.539383,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.238910 validation MAE=0.538801,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.237958 validation MAE=0.538229,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.237926 validation MAE=0.538536,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.237520 validation MAE=0.538316,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.236928 validation MAE=0.537959,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.235935 validation MAE=0.537357,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.236164 validation MAE=0.537377,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.235837 validation MAE=0.537365,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.234117 validation MAE=0.536507,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.234116 validation MAE=0.536846,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.234072 validation MAE=0.536886,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.232948 validation MAE=0.536356,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.231427 validation MAE=0.536512,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.231023 validation MAE=0.535907,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.230470 validation MAE=0.535968,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.230896 validation MAE=0.535647,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.229801 validation MAE=0.535130,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.230089 validation MAE=0.535062,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.228845 validation MAE=0.535357,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.228640 validation MAE=0.534710,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.227874 validation MAE=0.534526,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.228099 validation MAE=0.534174,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.226976 validation MAE=0.533334,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.226419 validation MAE=0.533360,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.225725 validation MAE=0.533339,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.225900 validation MAE=0.533744,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.225520 validation MAE=0.533380,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.225016 validation MAE=0.532875,rank=5\n",
      "[SoftImpute] Iter 109: observed MAE=0.224456 validation MAE=0.532665,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.224764 validation MAE=0.532774,rank=5\n",
      "[SoftImpute] Iter 111: observed MAE=0.224357 validation MAE=0.532529,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.224571 validation MAE=0.532251,rank=5\n",
      "[SoftImpute] Iter 113: observed MAE=0.223416 validation MAE=0.531997,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 114: observed MAE=0.222958 validation MAE=0.531977,rank=5\n",
      "[SoftImpute] Iter 115: observed MAE=0.222208 validation MAE=0.531587,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.222035 validation MAE=0.531077,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.222189 validation MAE=0.531543,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.221374 validation MAE=0.531105,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.221039 validation MAE=0.530123,rank=5\n",
      "[SoftImpute] Iter 120: observed MAE=0.220279 validation MAE=0.530626,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.219965 validation MAE=0.530169,rank=5\n",
      "[SoftImpute] Iter 122: observed MAE=0.219870 validation MAE=0.530386,rank=5\n",
      "[SoftImpute] Iter 123: observed MAE=0.219810 validation MAE=0.530106,rank=5\n",
      "[SoftImpute] Iter 124: observed MAE=0.219771 validation MAE=0.529651,rank=5\n",
      "[SoftImpute] Iter 125: observed MAE=0.218537 validation MAE=0.529485,rank=5\n",
      "[SoftImpute] Iter 126: observed MAE=0.218053 validation MAE=0.529309,rank=5\n",
      "[SoftImpute] Iter 127: observed MAE=0.218506 validation MAE=0.529062,rank=5\n",
      "[SoftImpute] Iter 128: observed MAE=0.218499 validation MAE=0.529247,rank=5\n",
      "[SoftImpute] Iter 129: observed MAE=0.218223 validation MAE=0.528754,rank=5\n",
      "[SoftImpute] Iter 130: observed MAE=0.217984 validation MAE=0.528989,rank=5\n",
      "[SoftImpute] Iter 131: observed MAE=0.217429 validation MAE=0.528386,rank=5\n",
      "[SoftImpute] Iter 132: observed MAE=0.216937 validation MAE=0.528068,rank=5\n",
      "[SoftImpute] Iter 133: observed MAE=0.217050 validation MAE=0.528399,rank=5\n",
      "[SoftImpute] Iter 134: observed MAE=0.216635 validation MAE=0.527988,rank=5\n",
      "[SoftImpute] Iter 135: observed MAE=0.216650 validation MAE=0.527734,rank=5\n",
      "[SoftImpute] Iter 136: observed MAE=0.216638 validation MAE=0.528050,rank=5\n",
      "[SoftImpute] Iter 137: observed MAE=0.215731 validation MAE=0.527190,rank=5\n",
      "[SoftImpute] Iter 138: observed MAE=0.214899 validation MAE=0.527014,rank=5\n",
      "[SoftImpute] Iter 139: observed MAE=0.215168 validation MAE=0.527863,rank=5\n",
      "[SoftImpute] Iter 140: observed MAE=0.215553 validation MAE=0.527036,rank=5\n",
      "[SoftImpute] Iter 141: observed MAE=0.214553 validation MAE=0.526913,rank=5\n",
      "[SoftImpute] Iter 142: observed MAE=0.214835 validation MAE=0.526503,rank=5\n",
      "[SoftImpute] Iter 143: observed MAE=0.214506 validation MAE=0.526548,rank=5\n",
      "[SoftImpute] Iter 144: observed MAE=0.213015 validation MAE=0.526422,rank=5\n",
      "[SoftImpute] Iter 145: observed MAE=0.213682 validation MAE=0.526456,rank=5\n",
      "[SoftImpute] Iter 146: observed MAE=0.213416 validation MAE=0.526181,rank=5\n",
      "[SoftImpute] Iter 147: observed MAE=0.212628 validation MAE=0.526013,rank=5\n",
      "[SoftImpute] Iter 148: observed MAE=0.213178 validation MAE=0.526108,rank=5\n",
      "[SoftImpute] Iter 149: observed MAE=0.211936 validation MAE=0.525584,rank=5\n",
      "[SoftImpute] Iter 150: observed MAE=0.211941 validation MAE=0.525298,rank=5\n",
      "[SoftImpute] Iter 151: observed MAE=0.212263 validation MAE=0.525400,rank=5\n",
      "[SoftImpute] Iter 152: observed MAE=0.212024 validation MAE=0.525232,rank=5\n",
      "[SoftImpute] Iter 153: observed MAE=0.211225 validation MAE=0.525123,rank=5\n",
      "[SoftImpute] Iter 154: observed MAE=0.210206 validation MAE=0.524808,rank=5\n",
      "[SoftImpute] Iter 155: observed MAE=0.211178 validation MAE=0.525110,rank=5\n",
      "[SoftImpute] Iter 156: observed MAE=0.211426 validation MAE=0.525401,rank=5\n",
      "[SoftImpute] Iter 157: observed MAE=0.210795 validation MAE=0.524731,rank=5\n",
      "[SoftImpute] Iter 158: observed MAE=0.211960 validation MAE=0.524866,rank=5\n",
      "[SoftImpute] Iter 159: observed MAE=0.210772 validation MAE=0.524407,rank=5\n",
      "[SoftImpute] Iter 160: observed MAE=0.210053 validation MAE=0.524190,rank=5\n",
      "[SoftImpute] Iter 161: observed MAE=0.210022 validation MAE=0.524121,rank=5\n",
      "[SoftImpute] Iter 162: observed MAE=0.210490 validation MAE=0.523533,rank=5\n",
      "[SoftImpute] Iter 163: observed MAE=0.209335 validation MAE=0.523551,rank=5\n",
      "[SoftImpute] Iter 164: observed MAE=0.208993 validation MAE=0.523263,rank=5\n",
      "[SoftImpute] Iter 165: observed MAE=0.209862 validation MAE=0.523456,rank=5\n",
      "[SoftImpute] Iter 166: observed MAE=0.208958 validation MAE=0.523410,rank=5\n",
      "[SoftImpute] Iter 167: observed MAE=0.208479 validation MAE=0.523178,rank=5\n",
      "[SoftImpute] Iter 168: observed MAE=0.208403 validation MAE=0.523089,rank=5\n",
      "[SoftImpute] Iter 169: observed MAE=0.208175 validation MAE=0.522745,rank=5\n",
      "[SoftImpute] Iter 170: observed MAE=0.208470 validation MAE=0.523041,rank=5\n",
      "[SoftImpute] Iter 171: observed MAE=0.207900 validation MAE=0.522422,rank=5\n",
      "[SoftImpute] Iter 172: observed MAE=0.207756 validation MAE=0.522624,rank=5\n",
      "[SoftImpute] Iter 173: observed MAE=0.207358 validation MAE=0.522239,rank=5\n",
      "[SoftImpute] Iter 174: observed MAE=0.207077 validation MAE=0.522025,rank=5\n",
      "[SoftImpute] Iter 175: observed MAE=0.207119 validation MAE=0.521945,rank=5\n",
      "[SoftImpute] Iter 176: observed MAE=0.207107 validation MAE=0.521262,rank=5\n",
      "[SoftImpute] Iter 177: observed MAE=0.207954 validation MAE=0.521505,rank=5\n",
      "[SoftImpute] Iter 178: observed MAE=0.207182 validation MAE=0.521390,rank=5\n",
      "[SoftImpute] Iter 179: observed MAE=0.206505 validation MAE=0.521327,rank=5\n",
      "[SoftImpute] Iter 180: observed MAE=0.206898 validation MAE=0.521312,rank=5\n",
      "[SoftImpute] Iter 181: observed MAE=0.206968 validation MAE=0.521038,rank=5\n",
      "[SoftImpute] Iter 182: observed MAE=0.205906 validation MAE=0.520797,rank=5\n",
      "[SoftImpute] Iter 183: observed MAE=0.205839 validation MAE=0.520569,rank=5\n",
      "[SoftImpute] Iter 184: observed MAE=0.205562 validation MAE=0.520453,rank=5\n",
      "[SoftImpute] Iter 185: observed MAE=0.205316 validation MAE=0.520269,rank=5\n",
      "[SoftImpute] Iter 186: observed MAE=0.205081 validation MAE=0.519894,rank=5\n",
      "[SoftImpute] Iter 187: observed MAE=0.205660 validation MAE=0.520405,rank=5\n",
      "[SoftImpute] Iter 188: observed MAE=0.204689 validation MAE=0.520098,rank=5\n",
      "[SoftImpute] Iter 189: observed MAE=0.204251 validation MAE=0.519918,rank=5\n",
      "[SoftImpute] Iter 190: observed MAE=0.205033 validation MAE=0.519931,rank=5\n",
      "[SoftImpute] Iter 191: observed MAE=0.204900 validation MAE=0.519681,rank=5\n",
      "[SoftImpute] Iter 192: observed MAE=0.204738 validation MAE=0.519567,rank=5\n",
      "[SoftImpute] Iter 193: observed MAE=0.205296 validation MAE=0.519601,rank=5\n",
      "[SoftImpute] Iter 194: observed MAE=0.204517 validation MAE=0.519398,rank=5\n",
      "[SoftImpute] Iter 195: observed MAE=0.203315 validation MAE=0.519130,rank=5\n",
      "[SoftImpute] Iter 196: observed MAE=0.203815 validation MAE=0.519022,rank=5\n",
      "[SoftImpute] Iter 197: observed MAE=0.204240 validation MAE=0.518540,rank=5\n",
      "[SoftImpute] Iter 198: observed MAE=0.204236 validation MAE=0.518173,rank=5\n",
      "[SoftImpute] Iter 199: observed MAE=0.204093 validation MAE=0.518636,rank=5\n",
      "[SoftImpute] Iter 200: observed MAE=0.203243 validation MAE=0.518132,rank=5\n",
      "[SoftImpute] Stopped after iteration 200 for lambda=0.419713\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 6.976736545562744\n",
      "After the matrix factor stage, training error is 0.20324, validation error is 0.51813\n",
      "3\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.01008, val loss: 4.13662\n",
      "Main effects training epoch: 2, train loss: 3.83438, val loss: 3.96263\n",
      "Main effects training epoch: 3, train loss: 3.64332, val loss: 3.79151\n",
      "Main effects training epoch: 4, train loss: 3.46972, val loss: 3.61136\n",
      "Main effects training epoch: 5, train loss: 3.37485, val loss: 3.48669\n",
      "Main effects training epoch: 6, train loss: 3.39722, val loss: 3.47354\n",
      "Main effects training epoch: 7, train loss: 3.32330, val loss: 3.39752\n",
      "Main effects training epoch: 8, train loss: 3.26542, val loss: 3.35246\n",
      "Main effects training epoch: 9, train loss: 3.23888, val loss: 3.33416\n",
      "Main effects training epoch: 10, train loss: 3.17076, val loss: 3.25758\n",
      "Main effects training epoch: 11, train loss: 3.09188, val loss: 3.17983\n",
      "Main effects training epoch: 12, train loss: 3.06223, val loss: 3.14993\n",
      "Main effects training epoch: 13, train loss: 3.02836, val loss: 3.11668\n",
      "Main effects training epoch: 14, train loss: 2.93695, val loss: 3.01732\n",
      "Main effects training epoch: 15, train loss: 2.85998, val loss: 2.94066\n",
      "Main effects training epoch: 16, train loss: 2.81922, val loss: 2.90332\n",
      "Main effects training epoch: 17, train loss: 2.77821, val loss: 2.85763\n",
      "Main effects training epoch: 18, train loss: 2.70667, val loss: 2.77550\n",
      "Main effects training epoch: 19, train loss: 2.63216, val loss: 2.69929\n",
      "Main effects training epoch: 20, train loss: 2.59348, val loss: 2.66088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 21, train loss: 2.56106, val loss: 2.63352\n",
      "Main effects training epoch: 22, train loss: 2.48510, val loss: 2.55894\n",
      "Main effects training epoch: 23, train loss: 2.42074, val loss: 2.49563\n",
      "Main effects training epoch: 24, train loss: 2.39598, val loss: 2.46710\n",
      "Main effects training epoch: 25, train loss: 2.35650, val loss: 2.43674\n",
      "Main effects training epoch: 26, train loss: 2.32587, val loss: 2.39291\n",
      "Main effects training epoch: 27, train loss: 2.30366, val loss: 2.37580\n",
      "Main effects training epoch: 28, train loss: 2.24406, val loss: 2.32070\n",
      "Main effects training epoch: 29, train loss: 2.23595, val loss: 2.31750\n",
      "Main effects training epoch: 30, train loss: 2.17289, val loss: 2.24483\n",
      "Main effects training epoch: 31, train loss: 2.16895, val loss: 2.24479\n",
      "Main effects training epoch: 32, train loss: 2.13964, val loss: 2.22006\n",
      "Main effects training epoch: 33, train loss: 2.13501, val loss: 2.21590\n",
      "Main effects training epoch: 34, train loss: 2.10980, val loss: 2.18814\n",
      "Main effects training epoch: 35, train loss: 2.06976, val loss: 2.14543\n",
      "Main effects training epoch: 36, train loss: 2.07312, val loss: 2.14640\n",
      "Main effects training epoch: 37, train loss: 2.01774, val loss: 2.09917\n",
      "Main effects training epoch: 38, train loss: 2.02325, val loss: 2.09864\n",
      "Main effects training epoch: 39, train loss: 2.01191, val loss: 2.08692\n",
      "Main effects training epoch: 40, train loss: 1.97449, val loss: 2.04983\n",
      "Main effects training epoch: 41, train loss: 1.95981, val loss: 2.03253\n",
      "Main effects training epoch: 42, train loss: 1.95400, val loss: 2.02772\n",
      "Main effects training epoch: 43, train loss: 1.92912, val loss: 2.00637\n",
      "Main effects training epoch: 44, train loss: 1.93674, val loss: 2.00829\n",
      "Main effects training epoch: 45, train loss: 1.89341, val loss: 1.96585\n",
      "Main effects training epoch: 46, train loss: 1.89721, val loss: 1.96561\n",
      "Main effects training epoch: 47, train loss: 1.88754, val loss: 1.95864\n",
      "Main effects training epoch: 48, train loss: 1.87411, val loss: 1.94294\n",
      "Main effects training epoch: 49, train loss: 1.87471, val loss: 1.94426\n",
      "Main effects training epoch: 50, train loss: 1.83864, val loss: 1.90715\n",
      "Main effects training epoch: 51, train loss: 1.83387, val loss: 1.90094\n",
      "Main effects training epoch: 52, train loss: 1.83329, val loss: 1.89941\n",
      "Main effects training epoch: 53, train loss: 1.82325, val loss: 1.88923\n",
      "Main effects training epoch: 54, train loss: 1.79851, val loss: 1.86556\n",
      "Main effects training epoch: 55, train loss: 1.81969, val loss: 1.89144\n",
      "Main effects training epoch: 56, train loss: 1.78887, val loss: 1.84654\n",
      "Main effects training epoch: 57, train loss: 1.79608, val loss: 1.86159\n",
      "Main effects training epoch: 58, train loss: 1.77720, val loss: 1.83668\n",
      "Main effects training epoch: 59, train loss: 1.78721, val loss: 1.85368\n",
      "Main effects training epoch: 60, train loss: 1.76637, val loss: 1.82636\n",
      "Main effects training epoch: 61, train loss: 1.77125, val loss: 1.83238\n",
      "Main effects training epoch: 62, train loss: 1.76741, val loss: 1.83018\n",
      "Main effects training epoch: 63, train loss: 1.75331, val loss: 1.81085\n",
      "Main effects training epoch: 64, train loss: 1.75347, val loss: 1.81466\n",
      "Main effects training epoch: 65, train loss: 1.75553, val loss: 1.81297\n",
      "Main effects training epoch: 66, train loss: 1.75003, val loss: 1.81211\n",
      "Main effects training epoch: 67, train loss: 1.74433, val loss: 1.80204\n",
      "Main effects training epoch: 68, train loss: 1.73634, val loss: 1.79232\n",
      "Main effects training epoch: 69, train loss: 1.74074, val loss: 1.79763\n",
      "Main effects training epoch: 70, train loss: 1.73224, val loss: 1.78887\n",
      "Main effects training epoch: 71, train loss: 1.73058, val loss: 1.79045\n",
      "Main effects training epoch: 72, train loss: 1.72857, val loss: 1.78041\n",
      "Main effects training epoch: 73, train loss: 1.72995, val loss: 1.78798\n",
      "Main effects training epoch: 74, train loss: 1.72388, val loss: 1.78007\n",
      "Main effects training epoch: 75, train loss: 1.71900, val loss: 1.77217\n",
      "Main effects training epoch: 76, train loss: 1.71715, val loss: 1.77396\n",
      "Main effects training epoch: 77, train loss: 1.72331, val loss: 1.78106\n",
      "Main effects training epoch: 78, train loss: 1.71420, val loss: 1.76510\n",
      "Main effects training epoch: 79, train loss: 1.71033, val loss: 1.76369\n",
      "Main effects training epoch: 80, train loss: 1.70895, val loss: 1.76599\n",
      "Main effects training epoch: 81, train loss: 1.70803, val loss: 1.76185\n",
      "Main effects training epoch: 82, train loss: 1.71015, val loss: 1.76129\n",
      "Main effects training epoch: 83, train loss: 1.70508, val loss: 1.76206\n",
      "Main effects training epoch: 84, train loss: 1.70433, val loss: 1.75898\n",
      "Main effects training epoch: 85, train loss: 1.70108, val loss: 1.75609\n",
      "Main effects training epoch: 86, train loss: 1.69990, val loss: 1.75251\n",
      "Main effects training epoch: 87, train loss: 1.69436, val loss: 1.75190\n",
      "Main effects training epoch: 88, train loss: 1.69222, val loss: 1.75144\n",
      "Main effects training epoch: 89, train loss: 1.69127, val loss: 1.74184\n",
      "Main effects training epoch: 90, train loss: 1.69106, val loss: 1.75686\n",
      "Main effects training epoch: 91, train loss: 1.67662, val loss: 1.73277\n",
      "Main effects training epoch: 92, train loss: 1.67593, val loss: 1.73787\n",
      "Main effects training epoch: 93, train loss: 1.67035, val loss: 1.73987\n",
      "Main effects training epoch: 94, train loss: 1.66675, val loss: 1.73812\n",
      "Main effects training epoch: 95, train loss: 1.65734, val loss: 1.72574\n",
      "Main effects training epoch: 96, train loss: 1.64604, val loss: 1.71636\n",
      "Main effects training epoch: 97, train loss: 1.64210, val loss: 1.71756\n",
      "Main effects training epoch: 98, train loss: 1.63777, val loss: 1.71863\n",
      "Main effects training epoch: 99, train loss: 1.64075, val loss: 1.72462\n",
      "Main effects training epoch: 100, train loss: 1.64097, val loss: 1.73094\n",
      "Main effects training epoch: 101, train loss: 1.62623, val loss: 1.70886\n",
      "Main effects training epoch: 102, train loss: 1.63179, val loss: 1.72331\n",
      "Main effects training epoch: 103, train loss: 1.62307, val loss: 1.70865\n",
      "Main effects training epoch: 104, train loss: 1.63476, val loss: 1.72128\n",
      "Main effects training epoch: 105, train loss: 1.62479, val loss: 1.71080\n",
      "Main effects training epoch: 106, train loss: 1.63386, val loss: 1.72354\n",
      "Main effects training epoch: 107, train loss: 1.61614, val loss: 1.70360\n",
      "Main effects training epoch: 108, train loss: 1.61996, val loss: 1.70861\n",
      "Main effects training epoch: 109, train loss: 1.61510, val loss: 1.70769\n",
      "Main effects training epoch: 110, train loss: 1.61013, val loss: 1.69963\n",
      "Main effects training epoch: 111, train loss: 1.61404, val loss: 1.70016\n",
      "Main effects training epoch: 112, train loss: 1.60828, val loss: 1.69691\n",
      "Main effects training epoch: 113, train loss: 1.61411, val loss: 1.70515\n",
      "Main effects training epoch: 114, train loss: 1.61299, val loss: 1.70323\n",
      "Main effects training epoch: 115, train loss: 1.60491, val loss: 1.69459\n",
      "Main effects training epoch: 116, train loss: 1.60704, val loss: 1.69810\n",
      "Main effects training epoch: 117, train loss: 1.60046, val loss: 1.69229\n",
      "Main effects training epoch: 118, train loss: 1.60832, val loss: 1.70092\n",
      "Main effects training epoch: 119, train loss: 1.60460, val loss: 1.69852\n",
      "Main effects training epoch: 120, train loss: 1.60459, val loss: 1.70358\n",
      "Main effects training epoch: 121, train loss: 1.60527, val loss: 1.69641\n",
      "Main effects training epoch: 122, train loss: 1.59869, val loss: 1.69208\n",
      "Main effects training epoch: 123, train loss: 1.60236, val loss: 1.68868\n",
      "Main effects training epoch: 124, train loss: 1.59573, val loss: 1.69877\n",
      "Main effects training epoch: 125, train loss: 1.59249, val loss: 1.68132\n",
      "Main effects training epoch: 126, train loss: 1.59162, val loss: 1.68683\n",
      "Main effects training epoch: 127, train loss: 1.58863, val loss: 1.68187\n",
      "Main effects training epoch: 128, train loss: 1.59070, val loss: 1.69147\n",
      "Main effects training epoch: 129, train loss: 1.59212, val loss: 1.67968\n",
      "Main effects training epoch: 130, train loss: 1.59291, val loss: 1.69241\n",
      "Main effects training epoch: 131, train loss: 1.58371, val loss: 1.66885\n",
      "Main effects training epoch: 132, train loss: 1.58090, val loss: 1.67261\n",
      "Main effects training epoch: 133, train loss: 1.59349, val loss: 1.68499\n",
      "Main effects training epoch: 134, train loss: 1.59356, val loss: 1.67296\n",
      "Main effects training epoch: 135, train loss: 1.58808, val loss: 1.68170\n",
      "Main effects training epoch: 136, train loss: 1.58745, val loss: 1.68078\n",
      "Main effects training epoch: 137, train loss: 1.57418, val loss: 1.66148\n",
      "Main effects training epoch: 138, train loss: 1.57784, val loss: 1.67755\n",
      "Main effects training epoch: 139, train loss: 1.57309, val loss: 1.66224\n",
      "Main effects training epoch: 140, train loss: 1.57317, val loss: 1.65716\n",
      "Main effects training epoch: 141, train loss: 1.57348, val loss: 1.66582\n",
      "Main effects training epoch: 142, train loss: 1.57709, val loss: 1.67126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 143, train loss: 1.56945, val loss: 1.65663\n",
      "Main effects training epoch: 144, train loss: 1.56849, val loss: 1.65957\n",
      "Main effects training epoch: 145, train loss: 1.57037, val loss: 1.66280\n",
      "Main effects training epoch: 146, train loss: 1.57114, val loss: 1.65615\n",
      "Main effects training epoch: 147, train loss: 1.56874, val loss: 1.65538\n",
      "Main effects training epoch: 148, train loss: 1.57278, val loss: 1.67000\n",
      "Main effects training epoch: 149, train loss: 1.57144, val loss: 1.65717\n",
      "Main effects training epoch: 150, train loss: 1.56982, val loss: 1.66195\n",
      "Main effects training epoch: 151, train loss: 1.58456, val loss: 1.66503\n",
      "Main effects training epoch: 152, train loss: 1.57764, val loss: 1.66355\n",
      "Main effects training epoch: 153, train loss: 1.56997, val loss: 1.65873\n",
      "Main effects training epoch: 154, train loss: 1.58657, val loss: 1.67511\n",
      "Main effects training epoch: 155, train loss: 1.56456, val loss: 1.65323\n",
      "Main effects training epoch: 156, train loss: 1.56164, val loss: 1.64575\n",
      "Main effects training epoch: 157, train loss: 1.55945, val loss: 1.64827\n",
      "Main effects training epoch: 158, train loss: 1.56203, val loss: 1.64334\n",
      "Main effects training epoch: 159, train loss: 1.57293, val loss: 1.65891\n",
      "Main effects training epoch: 160, train loss: 1.58478, val loss: 1.67515\n",
      "Main effects training epoch: 161, train loss: 1.56302, val loss: 1.64808\n",
      "Main effects training epoch: 162, train loss: 1.55895, val loss: 1.64389\n",
      "Main effects training epoch: 163, train loss: 1.56342, val loss: 1.65602\n",
      "Main effects training epoch: 164, train loss: 1.55439, val loss: 1.63978\n",
      "Main effects training epoch: 165, train loss: 1.55559, val loss: 1.64381\n",
      "Main effects training epoch: 166, train loss: 1.55190, val loss: 1.64297\n",
      "Main effects training epoch: 167, train loss: 1.55196, val loss: 1.64180\n",
      "Main effects training epoch: 168, train loss: 1.55281, val loss: 1.63364\n",
      "Main effects training epoch: 169, train loss: 1.55090, val loss: 1.63697\n",
      "Main effects training epoch: 170, train loss: 1.55894, val loss: 1.65752\n",
      "Main effects training epoch: 171, train loss: 1.55219, val loss: 1.62887\n",
      "Main effects training epoch: 172, train loss: 1.55061, val loss: 1.64670\n",
      "Main effects training epoch: 173, train loss: 1.55039, val loss: 1.64197\n",
      "Main effects training epoch: 174, train loss: 1.54940, val loss: 1.62832\n",
      "Main effects training epoch: 175, train loss: 1.54779, val loss: 1.63216\n",
      "Main effects training epoch: 176, train loss: 1.55336, val loss: 1.64292\n",
      "Main effects training epoch: 177, train loss: 1.55531, val loss: 1.64572\n",
      "Main effects training epoch: 178, train loss: 1.54074, val loss: 1.62613\n",
      "Main effects training epoch: 179, train loss: 1.54452, val loss: 1.62016\n",
      "Main effects training epoch: 180, train loss: 1.54018, val loss: 1.62570\n",
      "Main effects training epoch: 181, train loss: 1.54184, val loss: 1.62609\n",
      "Main effects training epoch: 182, train loss: 1.53680, val loss: 1.61910\n",
      "Main effects training epoch: 183, train loss: 1.53915, val loss: 1.62459\n",
      "Main effects training epoch: 184, train loss: 1.54172, val loss: 1.62861\n",
      "Main effects training epoch: 185, train loss: 1.53865, val loss: 1.62333\n",
      "Main effects training epoch: 186, train loss: 1.53820, val loss: 1.62788\n",
      "Main effects training epoch: 187, train loss: 1.53883, val loss: 1.62119\n",
      "Main effects training epoch: 188, train loss: 1.53756, val loss: 1.61977\n",
      "Main effects training epoch: 189, train loss: 1.53503, val loss: 1.61705\n",
      "Main effects training epoch: 190, train loss: 1.54219, val loss: 1.62399\n",
      "Main effects training epoch: 191, train loss: 1.53927, val loss: 1.62930\n",
      "Main effects training epoch: 192, train loss: 1.54086, val loss: 1.62904\n",
      "Main effects training epoch: 193, train loss: 1.53828, val loss: 1.62035\n",
      "Main effects training epoch: 194, train loss: 1.54299, val loss: 1.62049\n",
      "Main effects training epoch: 195, train loss: 1.54058, val loss: 1.63851\n",
      "Main effects training epoch: 196, train loss: 1.54558, val loss: 1.60536\n",
      "Main effects training epoch: 197, train loss: 1.53678, val loss: 1.63658\n",
      "Main effects training epoch: 198, train loss: 1.54230, val loss: 1.62145\n",
      "Main effects training epoch: 199, train loss: 1.55104, val loss: 1.62156\n",
      "Main effects training epoch: 200, train loss: 1.54656, val loss: 1.64457\n",
      "Main effects training epoch: 201, train loss: 1.54330, val loss: 1.62311\n",
      "Main effects training epoch: 202, train loss: 1.53971, val loss: 1.62405\n",
      "Main effects training epoch: 203, train loss: 1.53104, val loss: 1.61792\n",
      "Main effects training epoch: 204, train loss: 1.52771, val loss: 1.61524\n",
      "Main effects training epoch: 205, train loss: 1.53082, val loss: 1.60632\n",
      "Main effects training epoch: 206, train loss: 1.53429, val loss: 1.62259\n",
      "Main effects training epoch: 207, train loss: 1.52622, val loss: 1.61061\n",
      "Main effects training epoch: 208, train loss: 1.54630, val loss: 1.62156\n",
      "Main effects training epoch: 209, train loss: 1.53413, val loss: 1.62973\n",
      "Main effects training epoch: 210, train loss: 1.53416, val loss: 1.60276\n",
      "Main effects training epoch: 211, train loss: 1.52383, val loss: 1.60696\n",
      "Main effects training epoch: 212, train loss: 1.52285, val loss: 1.60015\n",
      "Main effects training epoch: 213, train loss: 1.53035, val loss: 1.61564\n",
      "Main effects training epoch: 214, train loss: 1.52727, val loss: 1.60243\n",
      "Main effects training epoch: 215, train loss: 1.52914, val loss: 1.61820\n",
      "Main effects training epoch: 216, train loss: 1.52540, val loss: 1.60739\n",
      "Main effects training epoch: 217, train loss: 1.52182, val loss: 1.60742\n",
      "Main effects training epoch: 218, train loss: 1.52492, val loss: 1.59121\n",
      "Main effects training epoch: 219, train loss: 1.52343, val loss: 1.61487\n",
      "Main effects training epoch: 220, train loss: 1.53339, val loss: 1.60253\n",
      "Main effects training epoch: 221, train loss: 1.52536, val loss: 1.61168\n",
      "Main effects training epoch: 222, train loss: 1.52107, val loss: 1.58800\n",
      "Main effects training epoch: 223, train loss: 1.52247, val loss: 1.60657\n",
      "Main effects training epoch: 224, train loss: 1.52092, val loss: 1.60220\n",
      "Main effects training epoch: 225, train loss: 1.52468, val loss: 1.60481\n",
      "Main effects training epoch: 226, train loss: 1.52080, val loss: 1.60559\n",
      "Main effects training epoch: 227, train loss: 1.51791, val loss: 1.59571\n",
      "Main effects training epoch: 228, train loss: 1.52379, val loss: 1.60224\n",
      "Main effects training epoch: 229, train loss: 1.52711, val loss: 1.60197\n",
      "Main effects training epoch: 230, train loss: 1.51952, val loss: 1.60032\n",
      "Main effects training epoch: 231, train loss: 1.51969, val loss: 1.60313\n",
      "Main effects training epoch: 232, train loss: 1.52589, val loss: 1.60080\n",
      "Main effects training epoch: 233, train loss: 1.52770, val loss: 1.61419\n",
      "Main effects training epoch: 234, train loss: 1.51596, val loss: 1.58807\n",
      "Main effects training epoch: 235, train loss: 1.51419, val loss: 1.59642\n",
      "Main effects training epoch: 236, train loss: 1.51737, val loss: 1.59924\n",
      "Main effects training epoch: 237, train loss: 1.51549, val loss: 1.59327\n",
      "Main effects training epoch: 238, train loss: 1.50929, val loss: 1.59533\n",
      "Main effects training epoch: 239, train loss: 1.51052, val loss: 1.58860\n",
      "Main effects training epoch: 240, train loss: 1.51746, val loss: 1.59300\n",
      "Main effects training epoch: 241, train loss: 1.51639, val loss: 1.60694\n",
      "Main effects training epoch: 242, train loss: 1.51298, val loss: 1.59281\n",
      "Main effects training epoch: 243, train loss: 1.50673, val loss: 1.58497\n",
      "Main effects training epoch: 244, train loss: 1.51807, val loss: 1.60684\n",
      "Main effects training epoch: 245, train loss: 1.51572, val loss: 1.58826\n",
      "Main effects training epoch: 246, train loss: 1.50772, val loss: 1.58337\n",
      "Main effects training epoch: 247, train loss: 1.50497, val loss: 1.58862\n",
      "Main effects training epoch: 248, train loss: 1.50637, val loss: 1.58709\n",
      "Main effects training epoch: 249, train loss: 1.50471, val loss: 1.57694\n",
      "Main effects training epoch: 250, train loss: 1.50513, val loss: 1.58677\n",
      "Main effects training epoch: 251, train loss: 1.50510, val loss: 1.59216\n",
      "Main effects training epoch: 252, train loss: 1.51170, val loss: 1.58077\n",
      "Main effects training epoch: 253, train loss: 1.50897, val loss: 1.58760\n",
      "Main effects training epoch: 254, train loss: 1.50879, val loss: 1.60047\n",
      "Main effects training epoch: 255, train loss: 1.50173, val loss: 1.57266\n",
      "Main effects training epoch: 256, train loss: 1.52687, val loss: 1.60018\n",
      "Main effects training epoch: 257, train loss: 1.52517, val loss: 1.60292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 258, train loss: 1.50662, val loss: 1.57837\n",
      "Main effects training epoch: 259, train loss: 1.50770, val loss: 1.57949\n",
      "Main effects training epoch: 260, train loss: 1.50088, val loss: 1.58024\n",
      "Main effects training epoch: 261, train loss: 1.50425, val loss: 1.58559\n",
      "Main effects training epoch: 262, train loss: 1.50053, val loss: 1.57961\n",
      "Main effects training epoch: 263, train loss: 1.49406, val loss: 1.56602\n",
      "Main effects training epoch: 264, train loss: 1.49790, val loss: 1.57809\n",
      "Main effects training epoch: 265, train loss: 1.50012, val loss: 1.56876\n",
      "Main effects training epoch: 266, train loss: 1.49776, val loss: 1.58430\n",
      "Main effects training epoch: 267, train loss: 1.49626, val loss: 1.56177\n",
      "Main effects training epoch: 268, train loss: 1.50312, val loss: 1.57595\n",
      "Main effects training epoch: 269, train loss: 1.49805, val loss: 1.57945\n",
      "Main effects training epoch: 270, train loss: 1.49667, val loss: 1.56669\n",
      "Main effects training epoch: 271, train loss: 1.49736, val loss: 1.57228\n",
      "Main effects training epoch: 272, train loss: 1.48794, val loss: 1.56660\n",
      "Main effects training epoch: 273, train loss: 1.49282, val loss: 1.56984\n",
      "Main effects training epoch: 274, train loss: 1.49137, val loss: 1.56917\n",
      "Main effects training epoch: 275, train loss: 1.49457, val loss: 1.56706\n",
      "Main effects training epoch: 276, train loss: 1.48623, val loss: 1.56579\n",
      "Main effects training epoch: 277, train loss: 1.48854, val loss: 1.56631\n",
      "Main effects training epoch: 278, train loss: 1.49012, val loss: 1.56645\n",
      "Main effects training epoch: 279, train loss: 1.48877, val loss: 1.56618\n",
      "Main effects training epoch: 280, train loss: 1.49609, val loss: 1.56399\n",
      "Main effects training epoch: 281, train loss: 1.48363, val loss: 1.55693\n",
      "Main effects training epoch: 282, train loss: 1.48100, val loss: 1.54719\n",
      "Main effects training epoch: 283, train loss: 1.48251, val loss: 1.55853\n",
      "Main effects training epoch: 284, train loss: 1.48259, val loss: 1.55624\n",
      "Main effects training epoch: 285, train loss: 1.49057, val loss: 1.57554\n",
      "Main effects training epoch: 286, train loss: 1.48825, val loss: 1.55087\n",
      "Main effects training epoch: 287, train loss: 1.48309, val loss: 1.55754\n",
      "Main effects training epoch: 288, train loss: 1.48276, val loss: 1.54920\n",
      "Main effects training epoch: 289, train loss: 1.47989, val loss: 1.54322\n",
      "Main effects training epoch: 290, train loss: 1.47943, val loss: 1.56234\n",
      "Main effects training epoch: 291, train loss: 1.48027, val loss: 1.55931\n",
      "Main effects training epoch: 292, train loss: 1.48571, val loss: 1.54766\n",
      "Main effects training epoch: 293, train loss: 1.47570, val loss: 1.54256\n",
      "Main effects training epoch: 294, train loss: 1.48293, val loss: 1.55359\n",
      "Main effects training epoch: 295, train loss: 1.47286, val loss: 1.53423\n",
      "Main effects training epoch: 296, train loss: 1.47958, val loss: 1.54060\n",
      "Main effects training epoch: 297, train loss: 1.47272, val loss: 1.54107\n",
      "Main effects training epoch: 298, train loss: 1.48184, val loss: 1.56164\n",
      "Main effects training epoch: 299, train loss: 1.47643, val loss: 1.54182\n",
      "Main effects training epoch: 300, train loss: 1.48828, val loss: 1.54435\n",
      "##########Stage 1: main effect training stop.##########\n",
      "3 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.50261, val loss: 1.56040\n",
      "Main effects tuning epoch: 2, train loss: 1.50895, val loss: 1.56079\n",
      "Main effects tuning epoch: 3, train loss: 1.51376, val loss: 1.54779\n",
      "Main effects tuning epoch: 4, train loss: 1.50924, val loss: 1.55230\n",
      "Main effects tuning epoch: 5, train loss: 1.50418, val loss: 1.55497\n",
      "Main effects tuning epoch: 6, train loss: 1.49873, val loss: 1.54958\n",
      "Main effects tuning epoch: 7, train loss: 1.49703, val loss: 1.55129\n",
      "Main effects tuning epoch: 8, train loss: 1.48887, val loss: 1.53308\n",
      "Main effects tuning epoch: 9, train loss: 1.50084, val loss: 1.53758\n",
      "Main effects tuning epoch: 10, train loss: 1.49293, val loss: 1.54628\n",
      "Main effects tuning epoch: 11, train loss: 1.49997, val loss: 1.53410\n",
      "Main effects tuning epoch: 12, train loss: 1.49210, val loss: 1.53754\n",
      "Main effects tuning epoch: 13, train loss: 1.50153, val loss: 1.55917\n",
      "Main effects tuning epoch: 14, train loss: 1.49807, val loss: 1.53152\n",
      "Main effects tuning epoch: 15, train loss: 1.49053, val loss: 1.54335\n",
      "Main effects tuning epoch: 16, train loss: 1.48658, val loss: 1.52807\n",
      "Main effects tuning epoch: 17, train loss: 1.50053, val loss: 1.53107\n",
      "Main effects tuning epoch: 18, train loss: 1.50763, val loss: 1.55494\n",
      "Main effects tuning epoch: 19, train loss: 1.48866, val loss: 1.53065\n",
      "Main effects tuning epoch: 20, train loss: 1.49559, val loss: 1.54052\n",
      "Main effects tuning epoch: 21, train loss: 1.48946, val loss: 1.53366\n",
      "Main effects tuning epoch: 22, train loss: 1.50447, val loss: 1.53888\n",
      "Main effects tuning epoch: 23, train loss: 1.49270, val loss: 1.55550\n",
      "Main effects tuning epoch: 24, train loss: 1.48783, val loss: 1.51867\n",
      "Main effects tuning epoch: 25, train loss: 1.48826, val loss: 1.53702\n",
      "Main effects tuning epoch: 26, train loss: 1.48852, val loss: 1.52367\n",
      "Main effects tuning epoch: 27, train loss: 1.48854, val loss: 1.54573\n",
      "Main effects tuning epoch: 28, train loss: 1.48208, val loss: 1.51541\n",
      "Main effects tuning epoch: 29, train loss: 1.48777, val loss: 1.52532\n",
      "Main effects tuning epoch: 30, train loss: 1.48924, val loss: 1.53662\n",
      "Main effects tuning epoch: 31, train loss: 1.49011, val loss: 1.52282\n",
      "Main effects tuning epoch: 32, train loss: 1.49044, val loss: 1.52337\n",
      "Main effects tuning epoch: 33, train loss: 1.49034, val loss: 1.53381\n",
      "Main effects tuning epoch: 34, train loss: 1.48967, val loss: 1.52476\n",
      "Main effects tuning epoch: 35, train loss: 1.47857, val loss: 1.52587\n",
      "Main effects tuning epoch: 36, train loss: 1.50786, val loss: 1.54828\n",
      "Main effects tuning epoch: 37, train loss: 1.49955, val loss: 1.53157\n",
      "Main effects tuning epoch: 38, train loss: 1.49203, val loss: 1.53074\n",
      "Main effects tuning epoch: 39, train loss: 1.49557, val loss: 1.54176\n",
      "Main effects tuning epoch: 40, train loss: 1.48861, val loss: 1.52978\n",
      "Main effects tuning epoch: 41, train loss: 1.48055, val loss: 1.53246\n",
      "Main effects tuning epoch: 42, train loss: 1.47456, val loss: 1.51216\n",
      "Main effects tuning epoch: 43, train loss: 1.48726, val loss: 1.52028\n",
      "Main effects tuning epoch: 44, train loss: 1.48178, val loss: 1.53076\n",
      "Main effects tuning epoch: 45, train loss: 1.47285, val loss: 1.50916\n",
      "Main effects tuning epoch: 46, train loss: 1.47905, val loss: 1.52197\n",
      "Main effects tuning epoch: 47, train loss: 1.47413, val loss: 1.52250\n",
      "Main effects tuning epoch: 48, train loss: 1.48409, val loss: 1.50861\n",
      "Main effects tuning epoch: 49, train loss: 1.47337, val loss: 1.51054\n",
      "Main effects tuning epoch: 50, train loss: 1.47883, val loss: 1.50694\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.36988, val loss: 1.40212\n",
      "Interaction training epoch: 2, train loss: 1.47527, val loss: 1.49025\n",
      "Interaction training epoch: 3, train loss: 1.07121, val loss: 1.10505\n",
      "Interaction training epoch: 4, train loss: 1.07792, val loss: 1.07761\n",
      "Interaction training epoch: 5, train loss: 1.05348, val loss: 1.07224\n",
      "Interaction training epoch: 6, train loss: 1.01342, val loss: 1.00926\n",
      "Interaction training epoch: 7, train loss: 1.11298, val loss: 1.12187\n",
      "Interaction training epoch: 8, train loss: 1.01438, val loss: 1.00464\n",
      "Interaction training epoch: 9, train loss: 1.00664, val loss: 1.00326\n",
      "Interaction training epoch: 10, train loss: 1.01130, val loss: 1.01304\n",
      "Interaction training epoch: 11, train loss: 1.00133, val loss: 0.99646\n",
      "Interaction training epoch: 12, train loss: 0.98888, val loss: 0.98021\n",
      "Interaction training epoch: 13, train loss: 0.99702, val loss: 0.99984\n",
      "Interaction training epoch: 14, train loss: 1.01281, val loss: 1.00353\n",
      "Interaction training epoch: 15, train loss: 0.97276, val loss: 0.97039\n",
      "Interaction training epoch: 16, train loss: 0.96633, val loss: 0.95983\n",
      "Interaction training epoch: 17, train loss: 0.95055, val loss: 0.95514\n",
      "Interaction training epoch: 18, train loss: 0.93959, val loss: 0.93997\n",
      "Interaction training epoch: 19, train loss: 0.94005, val loss: 0.93632\n",
      "Interaction training epoch: 20, train loss: 0.95697, val loss: 0.94543\n",
      "Interaction training epoch: 21, train loss: 0.94791, val loss: 0.94808\n",
      "Interaction training epoch: 22, train loss: 0.90874, val loss: 0.90864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 23, train loss: 0.90661, val loss: 0.90545\n",
      "Interaction training epoch: 24, train loss: 0.90826, val loss: 0.91300\n",
      "Interaction training epoch: 25, train loss: 0.90741, val loss: 0.90218\n",
      "Interaction training epoch: 26, train loss: 0.89351, val loss: 0.89637\n",
      "Interaction training epoch: 27, train loss: 0.89189, val loss: 0.89648\n",
      "Interaction training epoch: 28, train loss: 0.89368, val loss: 0.89442\n",
      "Interaction training epoch: 29, train loss: 0.88380, val loss: 0.88938\n",
      "Interaction training epoch: 30, train loss: 0.88955, val loss: 0.89600\n",
      "Interaction training epoch: 31, train loss: 0.87871, val loss: 0.87594\n",
      "Interaction training epoch: 32, train loss: 0.89171, val loss: 0.89273\n",
      "Interaction training epoch: 33, train loss: 0.87652, val loss: 0.89024\n",
      "Interaction training epoch: 34, train loss: 0.90112, val loss: 0.90906\n",
      "Interaction training epoch: 35, train loss: 0.88078, val loss: 0.87620\n",
      "Interaction training epoch: 36, train loss: 0.90977, val loss: 0.91392\n",
      "Interaction training epoch: 37, train loss: 0.87990, val loss: 0.89447\n",
      "Interaction training epoch: 38, train loss: 0.87256, val loss: 0.87525\n",
      "Interaction training epoch: 39, train loss: 0.87964, val loss: 0.87688\n",
      "Interaction training epoch: 40, train loss: 0.85794, val loss: 0.87380\n",
      "Interaction training epoch: 41, train loss: 0.86450, val loss: 0.87640\n",
      "Interaction training epoch: 42, train loss: 0.85732, val loss: 0.86921\n",
      "Interaction training epoch: 43, train loss: 0.85903, val loss: 0.87304\n",
      "Interaction training epoch: 44, train loss: 0.84703, val loss: 0.84529\n",
      "Interaction training epoch: 45, train loss: 0.85252, val loss: 0.86844\n",
      "Interaction training epoch: 46, train loss: 0.84767, val loss: 0.85374\n",
      "Interaction training epoch: 47, train loss: 0.84630, val loss: 0.85315\n",
      "Interaction training epoch: 48, train loss: 0.84094, val loss: 0.84820\n",
      "Interaction training epoch: 49, train loss: 0.84780, val loss: 0.85247\n",
      "Interaction training epoch: 50, train loss: 0.85174, val loss: 0.87410\n",
      "Interaction training epoch: 51, train loss: 0.84312, val loss: 0.85239\n",
      "Interaction training epoch: 52, train loss: 0.84213, val loss: 0.85214\n",
      "Interaction training epoch: 53, train loss: 0.83729, val loss: 0.83918\n",
      "Interaction training epoch: 54, train loss: 0.83503, val loss: 0.84389\n",
      "Interaction training epoch: 55, train loss: 0.83529, val loss: 0.83149\n",
      "Interaction training epoch: 56, train loss: 0.85838, val loss: 0.87183\n",
      "Interaction training epoch: 57, train loss: 0.83064, val loss: 0.83928\n",
      "Interaction training epoch: 58, train loss: 0.82514, val loss: 0.83205\n",
      "Interaction training epoch: 59, train loss: 0.83012, val loss: 0.83348\n",
      "Interaction training epoch: 60, train loss: 0.83214, val loss: 0.84542\n",
      "Interaction training epoch: 61, train loss: 0.84417, val loss: 0.85275\n",
      "Interaction training epoch: 62, train loss: 0.83151, val loss: 0.84302\n",
      "Interaction training epoch: 63, train loss: 0.83424, val loss: 0.83461\n",
      "Interaction training epoch: 64, train loss: 0.82330, val loss: 0.82344\n",
      "Interaction training epoch: 65, train loss: 0.82372, val loss: 0.83435\n",
      "Interaction training epoch: 66, train loss: 0.82488, val loss: 0.81936\n",
      "Interaction training epoch: 67, train loss: 0.83425, val loss: 0.83745\n",
      "Interaction training epoch: 68, train loss: 0.82716, val loss: 0.83353\n",
      "Interaction training epoch: 69, train loss: 0.82534, val loss: 0.82893\n",
      "Interaction training epoch: 70, train loss: 0.82024, val loss: 0.81682\n",
      "Interaction training epoch: 71, train loss: 0.84530, val loss: 0.84671\n",
      "Interaction training epoch: 72, train loss: 0.80980, val loss: 0.81400\n",
      "Interaction training epoch: 73, train loss: 0.82359, val loss: 0.82822\n",
      "Interaction training epoch: 74, train loss: 0.81850, val loss: 0.82146\n",
      "Interaction training epoch: 75, train loss: 0.82706, val loss: 0.83267\n",
      "Interaction training epoch: 76, train loss: 0.82626, val loss: 0.83144\n",
      "Interaction training epoch: 77, train loss: 0.84054, val loss: 0.84409\n",
      "Interaction training epoch: 78, train loss: 0.85002, val loss: 0.85514\n",
      "Interaction training epoch: 79, train loss: 0.82440, val loss: 0.81328\n",
      "Interaction training epoch: 80, train loss: 0.82947, val loss: 0.83159\n",
      "Interaction training epoch: 81, train loss: 0.81695, val loss: 0.82424\n",
      "Interaction training epoch: 82, train loss: 0.80486, val loss: 0.80297\n",
      "Interaction training epoch: 83, train loss: 0.82158, val loss: 0.83102\n",
      "Interaction training epoch: 84, train loss: 0.82727, val loss: 0.83459\n",
      "Interaction training epoch: 85, train loss: 0.81705, val loss: 0.81683\n",
      "Interaction training epoch: 86, train loss: 0.82537, val loss: 0.82844\n",
      "Interaction training epoch: 87, train loss: 0.82908, val loss: 0.83337\n",
      "Interaction training epoch: 88, train loss: 0.80039, val loss: 0.79523\n",
      "Interaction training epoch: 89, train loss: 0.81238, val loss: 0.81552\n",
      "Interaction training epoch: 90, train loss: 0.81167, val loss: 0.81807\n",
      "Interaction training epoch: 91, train loss: 0.82147, val loss: 0.82174\n",
      "Interaction training epoch: 92, train loss: 0.83118, val loss: 0.84021\n",
      "Interaction training epoch: 93, train loss: 0.80684, val loss: 0.80127\n",
      "Interaction training epoch: 94, train loss: 0.79966, val loss: 0.79558\n",
      "Interaction training epoch: 95, train loss: 0.81801, val loss: 0.81638\n",
      "Interaction training epoch: 96, train loss: 0.80288, val loss: 0.80296\n",
      "Interaction training epoch: 97, train loss: 0.82486, val loss: 0.82827\n",
      "Interaction training epoch: 98, train loss: 0.81008, val loss: 0.80844\n",
      "Interaction training epoch: 99, train loss: 0.82479, val loss: 0.82513\n",
      "Interaction training epoch: 100, train loss: 0.80863, val loss: 0.81340\n",
      "Interaction training epoch: 101, train loss: 0.79491, val loss: 0.78906\n",
      "Interaction training epoch: 102, train loss: 0.81112, val loss: 0.80853\n",
      "Interaction training epoch: 103, train loss: 0.79637, val loss: 0.80469\n",
      "Interaction training epoch: 104, train loss: 0.81381, val loss: 0.81663\n",
      "Interaction training epoch: 105, train loss: 0.80868, val loss: 0.81112\n",
      "Interaction training epoch: 106, train loss: 0.80870, val loss: 0.80195\n",
      "Interaction training epoch: 107, train loss: 0.80866, val loss: 0.81417\n",
      "Interaction training epoch: 108, train loss: 0.82758, val loss: 0.82522\n",
      "Interaction training epoch: 109, train loss: 0.79106, val loss: 0.79419\n",
      "Interaction training epoch: 110, train loss: 0.83621, val loss: 0.83270\n",
      "Interaction training epoch: 111, train loss: 0.79739, val loss: 0.79985\n",
      "Interaction training epoch: 112, train loss: 0.80717, val loss: 0.81317\n",
      "Interaction training epoch: 113, train loss: 0.79769, val loss: 0.79960\n",
      "Interaction training epoch: 114, train loss: 0.80528, val loss: 0.80598\n",
      "Interaction training epoch: 115, train loss: 0.81667, val loss: 0.81866\n",
      "Interaction training epoch: 116, train loss: 0.81369, val loss: 0.82065\n",
      "Interaction training epoch: 117, train loss: 0.78996, val loss: 0.78246\n",
      "Interaction training epoch: 118, train loss: 0.82218, val loss: 0.81648\n",
      "Interaction training epoch: 119, train loss: 0.78339, val loss: 0.77695\n",
      "Interaction training epoch: 120, train loss: 0.80320, val loss: 0.80596\n",
      "Interaction training epoch: 121, train loss: 0.79669, val loss: 0.79862\n",
      "Interaction training epoch: 122, train loss: 0.81366, val loss: 0.81811\n",
      "Interaction training epoch: 123, train loss: 0.80449, val loss: 0.80739\n",
      "Interaction training epoch: 124, train loss: 0.80489, val loss: 0.81308\n",
      "Interaction training epoch: 125, train loss: 0.80118, val loss: 0.79745\n",
      "Interaction training epoch: 126, train loss: 0.79158, val loss: 0.78995\n",
      "Interaction training epoch: 127, train loss: 0.78908, val loss: 0.79534\n",
      "Interaction training epoch: 128, train loss: 0.79522, val loss: 0.79619\n",
      "Interaction training epoch: 129, train loss: 0.82476, val loss: 0.82787\n",
      "Interaction training epoch: 130, train loss: 0.78836, val loss: 0.79205\n",
      "Interaction training epoch: 131, train loss: 0.79619, val loss: 0.79921\n",
      "Interaction training epoch: 132, train loss: 0.80706, val loss: 0.80689\n",
      "Interaction training epoch: 133, train loss: 0.79953, val loss: 0.80404\n",
      "Interaction training epoch: 134, train loss: 0.78305, val loss: 0.78651\n",
      "Interaction training epoch: 135, train loss: 0.81486, val loss: 0.81826\n",
      "Interaction training epoch: 136, train loss: 0.81312, val loss: 0.82181\n",
      "Interaction training epoch: 137, train loss: 0.85201, val loss: 0.84923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 138, train loss: 0.81038, val loss: 0.80984\n",
      "Interaction training epoch: 139, train loss: 0.78361, val loss: 0.78691\n",
      "Interaction training epoch: 140, train loss: 0.79259, val loss: 0.78814\n",
      "Interaction training epoch: 141, train loss: 0.78694, val loss: 0.79218\n",
      "Interaction training epoch: 142, train loss: 0.83567, val loss: 0.83612\n",
      "Interaction training epoch: 143, train loss: 0.80218, val loss: 0.81192\n",
      "Interaction training epoch: 144, train loss: 0.79544, val loss: 0.79416\n",
      "Interaction training epoch: 145, train loss: 0.80692, val loss: 0.80521\n",
      "Interaction training epoch: 146, train loss: 0.78771, val loss: 0.79811\n",
      "Interaction training epoch: 147, train loss: 0.80907, val loss: 0.80623\n",
      "Interaction training epoch: 148, train loss: 0.78951, val loss: 0.79660\n",
      "Interaction training epoch: 149, train loss: 0.80076, val loss: 0.79380\n",
      "Interaction training epoch: 150, train loss: 0.79536, val loss: 0.79780\n",
      "Interaction training epoch: 151, train loss: 0.78711, val loss: 0.78515\n",
      "Interaction training epoch: 152, train loss: 0.80086, val loss: 0.80472\n",
      "Interaction training epoch: 153, train loss: 0.78688, val loss: 0.78632\n",
      "Interaction training epoch: 154, train loss: 0.77726, val loss: 0.77507\n",
      "Interaction training epoch: 155, train loss: 0.81193, val loss: 0.80956\n",
      "Interaction training epoch: 156, train loss: 0.79463, val loss: 0.79972\n",
      "Interaction training epoch: 157, train loss: 0.79390, val loss: 0.78858\n",
      "Interaction training epoch: 158, train loss: 0.83467, val loss: 0.83932\n",
      "Interaction training epoch: 159, train loss: 0.78504, val loss: 0.78820\n",
      "Interaction training epoch: 160, train loss: 0.79780, val loss: 0.79373\n",
      "Interaction training epoch: 161, train loss: 0.77726, val loss: 0.78449\n",
      "Interaction training epoch: 162, train loss: 0.78769, val loss: 0.79071\n",
      "Interaction training epoch: 163, train loss: 0.82546, val loss: 0.83465\n",
      "Interaction training epoch: 164, train loss: 0.78662, val loss: 0.77748\n",
      "Interaction training epoch: 165, train loss: 0.78094, val loss: 0.78418\n",
      "Interaction training epoch: 166, train loss: 0.80223, val loss: 0.80141\n",
      "Interaction training epoch: 167, train loss: 0.78219, val loss: 0.78465\n",
      "Interaction training epoch: 168, train loss: 0.78685, val loss: 0.79508\n",
      "Interaction training epoch: 169, train loss: 0.78148, val loss: 0.78702\n",
      "Interaction training epoch: 170, train loss: 0.81043, val loss: 0.81186\n",
      "Interaction training epoch: 171, train loss: 0.79174, val loss: 0.79633\n",
      "Interaction training epoch: 172, train loss: 0.82023, val loss: 0.81905\n",
      "Interaction training epoch: 173, train loss: 0.79546, val loss: 0.80691\n",
      "Interaction training epoch: 174, train loss: 0.78412, val loss: 0.78062\n",
      "Interaction training epoch: 175, train loss: 0.79316, val loss: 0.80286\n",
      "Interaction training epoch: 176, train loss: 0.79471, val loss: 0.79751\n",
      "Interaction training epoch: 177, train loss: 0.78783, val loss: 0.78811\n",
      "Interaction training epoch: 178, train loss: 0.79497, val loss: 0.79538\n",
      "Interaction training epoch: 179, train loss: 0.80058, val loss: 0.80293\n",
      "Interaction training epoch: 180, train loss: 0.78341, val loss: 0.79643\n",
      "Interaction training epoch: 181, train loss: 0.78483, val loss: 0.77949\n",
      "Interaction training epoch: 182, train loss: 0.79102, val loss: 0.79433\n",
      "Interaction training epoch: 183, train loss: 0.79594, val loss: 0.79144\n",
      "Interaction training epoch: 184, train loss: 0.77676, val loss: 0.78075\n",
      "Interaction training epoch: 185, train loss: 0.78866, val loss: 0.79059\n",
      "Interaction training epoch: 186, train loss: 0.79317, val loss: 0.79753\n",
      "Interaction training epoch: 187, train loss: 0.78639, val loss: 0.78766\n",
      "Interaction training epoch: 188, train loss: 0.78085, val loss: 0.78431\n",
      "Interaction training epoch: 189, train loss: 0.78361, val loss: 0.78890\n",
      "Interaction training epoch: 190, train loss: 0.79842, val loss: 0.79830\n",
      "Interaction training epoch: 191, train loss: 0.82357, val loss: 0.82242\n",
      "Interaction training epoch: 192, train loss: 0.80006, val loss: 0.80592\n",
      "Interaction training epoch: 193, train loss: 0.77764, val loss: 0.77985\n",
      "Interaction training epoch: 194, train loss: 0.79993, val loss: 0.79344\n",
      "Interaction training epoch: 195, train loss: 0.79803, val loss: 0.80525\n",
      "Interaction training epoch: 196, train loss: 0.86620, val loss: 0.86794\n",
      "Interaction training epoch: 197, train loss: 0.79719, val loss: 0.79885\n",
      "Interaction training epoch: 198, train loss: 0.78008, val loss: 0.78422\n",
      "Interaction training epoch: 199, train loss: 0.80406, val loss: 0.80492\n",
      "Interaction training epoch: 200, train loss: 0.79272, val loss: 0.79979\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########1 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.79484, val loss: 0.79198\n",
      "Interaction tuning epoch: 2, train loss: 0.80300, val loss: 0.80775\n",
      "Interaction tuning epoch: 3, train loss: 0.81385, val loss: 0.81311\n",
      "Interaction tuning epoch: 4, train loss: 0.79136, val loss: 0.79652\n",
      "Interaction tuning epoch: 5, train loss: 0.79589, val loss: 0.80164\n",
      "Interaction tuning epoch: 6, train loss: 0.79359, val loss: 0.79275\n",
      "Interaction tuning epoch: 7, train loss: 0.78681, val loss: 0.78778\n",
      "Interaction tuning epoch: 8, train loss: 0.79479, val loss: 0.80098\n",
      "Interaction tuning epoch: 9, train loss: 0.79811, val loss: 0.80243\n",
      "Interaction tuning epoch: 10, train loss: 0.81729, val loss: 0.82045\n",
      "Interaction tuning epoch: 11, train loss: 0.80097, val loss: 0.80440\n",
      "Interaction tuning epoch: 12, train loss: 0.78386, val loss: 0.78395\n",
      "Interaction tuning epoch: 13, train loss: 0.77886, val loss: 0.78348\n",
      "Interaction tuning epoch: 14, train loss: 0.81033, val loss: 0.81444\n",
      "Interaction tuning epoch: 15, train loss: 0.80162, val loss: 0.80066\n",
      "Interaction tuning epoch: 16, train loss: 0.78309, val loss: 0.79371\n",
      "Interaction tuning epoch: 17, train loss: 0.79677, val loss: 0.79781\n",
      "Interaction tuning epoch: 18, train loss: 0.78056, val loss: 0.78429\n",
      "Interaction tuning epoch: 19, train loss: 0.81090, val loss: 0.81093\n",
      "Interaction tuning epoch: 20, train loss: 0.79834, val loss: 0.79891\n",
      "Interaction tuning epoch: 21, train loss: 0.77955, val loss: 0.78580\n",
      "Interaction tuning epoch: 22, train loss: 0.78006, val loss: 0.78170\n",
      "Interaction tuning epoch: 23, train loss: 0.80759, val loss: 0.80690\n",
      "Interaction tuning epoch: 24, train loss: 0.80078, val loss: 0.80411\n",
      "Interaction tuning epoch: 25, train loss: 0.80632, val loss: 0.80195\n",
      "Interaction tuning epoch: 26, train loss: 0.78592, val loss: 0.79834\n",
      "Interaction tuning epoch: 27, train loss: 0.78619, val loss: 0.78285\n",
      "Interaction tuning epoch: 28, train loss: 0.82386, val loss: 0.83187\n",
      "Interaction tuning epoch: 29, train loss: 0.77990, val loss: 0.77950\n",
      "Interaction tuning epoch: 30, train loss: 0.80973, val loss: 0.81641\n",
      "Interaction tuning epoch: 31, train loss: 0.77998, val loss: 0.77810\n",
      "Interaction tuning epoch: 32, train loss: 0.80216, val loss: 0.81033\n",
      "Interaction tuning epoch: 33, train loss: 0.80219, val loss: 0.80439\n",
      "Interaction tuning epoch: 34, train loss: 0.78232, val loss: 0.78488\n",
      "Interaction tuning epoch: 35, train loss: 0.79573, val loss: 0.79541\n",
      "Interaction tuning epoch: 36, train loss: 0.78595, val loss: 0.78581\n",
      "Interaction tuning epoch: 37, train loss: 0.77529, val loss: 0.77669\n",
      "Interaction tuning epoch: 38, train loss: 0.78982, val loss: 0.79014\n",
      "Interaction tuning epoch: 39, train loss: 0.78803, val loss: 0.78909\n",
      "Interaction tuning epoch: 40, train loss: 0.77593, val loss: 0.77690\n",
      "Interaction tuning epoch: 41, train loss: 0.77756, val loss: 0.77969\n",
      "Interaction tuning epoch: 42, train loss: 0.79983, val loss: 0.80053\n",
      "Interaction tuning epoch: 43, train loss: 0.80625, val loss: 0.80634\n",
      "Interaction tuning epoch: 44, train loss: 0.79053, val loss: 0.79440\n",
      "Interaction tuning epoch: 45, train loss: 0.80733, val loss: 0.81002\n",
      "Interaction tuning epoch: 46, train loss: 0.77972, val loss: 0.77617\n",
      "Interaction tuning epoch: 47, train loss: 0.80718, val loss: 0.81012\n",
      "Interaction tuning epoch: 48, train loss: 0.82560, val loss: 0.83277\n",
      "Interaction tuning epoch: 49, train loss: 0.78429, val loss: 0.78447\n",
      "Interaction tuning epoch: 50, train loss: 0.78669, val loss: 0.79192\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 38.73021197319031\n",
      "After the gam stage, training error is 0.78669 , validation error is 0.79192\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 19.287498\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.648169 validation MAE=0.749067,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.602056 validation MAE=0.730082,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 3: observed MAE=0.564010 validation MAE=0.712852,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.531884 validation MAE=0.697927,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.503814 validation MAE=0.684465,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.478962 validation MAE=0.672126,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.457223 validation MAE=0.660581,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.439872 validation MAE=0.650642,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.425897 validation MAE=0.642296,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.411792 validation MAE=0.634709,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.398606 validation MAE=0.626693,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.386081 validation MAE=0.619477,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.375145 validation MAE=0.613702,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.364352 validation MAE=0.607273,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.355269 validation MAE=0.601708,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.346480 validation MAE=0.596266,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.338237 validation MAE=0.591346,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.330989 validation MAE=0.587744,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.323767 validation MAE=0.583226,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.317063 validation MAE=0.580049,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.311747 validation MAE=0.576552,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.305933 validation MAE=0.573281,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.300975 validation MAE=0.570251,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.295924 validation MAE=0.567180,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.291625 validation MAE=0.564952,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.287120 validation MAE=0.561565,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.282833 validation MAE=0.558974,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.279330 validation MAE=0.556987,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.276214 validation MAE=0.554566,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.272294 validation MAE=0.552956,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.269119 validation MAE=0.550774,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.266018 validation MAE=0.548837,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.263643 validation MAE=0.547344,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.260418 validation MAE=0.545779,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.257931 validation MAE=0.544188,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.254983 validation MAE=0.542768,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.253166 validation MAE=0.541184,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.250900 validation MAE=0.539749,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.248594 validation MAE=0.538209,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.246293 validation MAE=0.536274,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.244421 validation MAE=0.536018,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.242541 validation MAE=0.534414,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.239901 validation MAE=0.533510,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.238756 validation MAE=0.532112,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.236981 validation MAE=0.530753,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.235920 validation MAE=0.530082,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.234126 validation MAE=0.528576,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.231629 validation MAE=0.527627,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.230157 validation MAE=0.526343,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.228558 validation MAE=0.525665,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.226949 validation MAE=0.524730,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.225931 validation MAE=0.524538,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.224101 validation MAE=0.523409,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.222871 validation MAE=0.522426,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.222003 validation MAE=0.521774,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.220934 validation MAE=0.521555,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.219401 validation MAE=0.519887,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.217804 validation MAE=0.519557,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.216442 validation MAE=0.518789,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.215933 validation MAE=0.518253,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.214897 validation MAE=0.517107,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.213323 validation MAE=0.516951,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.212172 validation MAE=0.515604,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.211178 validation MAE=0.515114,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.209983 validation MAE=0.514831,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.209976 validation MAE=0.514234,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.208870 validation MAE=0.513600,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.207189 validation MAE=0.512958,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.206177 validation MAE=0.511815,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.205521 validation MAE=0.511080,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.204809 validation MAE=0.511099,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.204231 validation MAE=0.510766,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.203010 validation MAE=0.509790,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.202096 validation MAE=0.509383,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.201547 validation MAE=0.508118,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.201117 validation MAE=0.508504,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.200010 validation MAE=0.507311,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.198928 validation MAE=0.506683,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.198369 validation MAE=0.506342,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.197480 validation MAE=0.505844,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.197097 validation MAE=0.504912,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.196451 validation MAE=0.505186,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.195706 validation MAE=0.504731,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.195471 validation MAE=0.503859,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.194654 validation MAE=0.503594,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.193277 validation MAE=0.502865,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.192855 validation MAE=0.501803,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.192480 validation MAE=0.501322,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.191930 validation MAE=0.501074,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.191899 validation MAE=0.500669,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.191145 validation MAE=0.499659,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.190281 validation MAE=0.499835,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.189915 validation MAE=0.499533,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.189220 validation MAE=0.498592,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.188592 validation MAE=0.498292,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.187431 validation MAE=0.498185,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.188125 validation MAE=0.497387,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.187020 validation MAE=0.497177,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.186415 validation MAE=0.496759,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.186234 validation MAE=0.496199,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.185388 validation MAE=0.495835,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.184763 validation MAE=0.495998,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.184200 validation MAE=0.496002,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.184164 validation MAE=0.496094,rank=5\n",
      "[SoftImpute] Stopped after iteration 104 for lambda=0.385750\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 3.28076171875\n",
      "After the matrix factor stage, training error is 0.18416, validation error is 0.49609\n",
      "4\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.06680, val loss: 4.20164\n",
      "Main effects training epoch: 2, train loss: 3.93080, val loss: 4.07448\n",
      "Main effects training epoch: 3, train loss: 3.74075, val loss: 3.89222\n",
      "Main effects training epoch: 4, train loss: 3.69760, val loss: 3.83732\n",
      "Main effects training epoch: 5, train loss: 3.46401, val loss: 3.56983\n",
      "Main effects training epoch: 6, train loss: 3.28808, val loss: 3.36736\n",
      "Main effects training epoch: 7, train loss: 3.23310, val loss: 3.31357\n",
      "Main effects training epoch: 8, train loss: 3.24031, val loss: 3.32522\n",
      "Main effects training epoch: 9, train loss: 3.24091, val loss: 3.33306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 10, train loss: 3.16690, val loss: 3.25794\n",
      "Main effects training epoch: 11, train loss: 3.12465, val loss: 3.21024\n",
      "Main effects training epoch: 12, train loss: 3.02258, val loss: 3.10545\n",
      "Main effects training epoch: 13, train loss: 3.01383, val loss: 3.10316\n",
      "Main effects training epoch: 14, train loss: 2.95645, val loss: 3.05318\n",
      "Main effects training epoch: 15, train loss: 2.90503, val loss: 2.99535\n",
      "Main effects training epoch: 16, train loss: 2.86024, val loss: 2.95006\n",
      "Main effects training epoch: 17, train loss: 2.81006, val loss: 2.89960\n",
      "Main effects training epoch: 18, train loss: 2.77449, val loss: 2.86602\n",
      "Main effects training epoch: 19, train loss: 2.69317, val loss: 2.78576\n",
      "Main effects training epoch: 20, train loss: 2.63628, val loss: 2.72741\n",
      "Main effects training epoch: 21, train loss: 2.57131, val loss: 2.66580\n",
      "Main effects training epoch: 22, train loss: 2.52903, val loss: 2.62536\n",
      "Main effects training epoch: 23, train loss: 2.45888, val loss: 2.54905\n",
      "Main effects training epoch: 24, train loss: 2.40624, val loss: 2.49603\n",
      "Main effects training epoch: 25, train loss: 2.37216, val loss: 2.45340\n",
      "Main effects training epoch: 26, train loss: 2.30469, val loss: 2.38256\n",
      "Main effects training epoch: 27, train loss: 2.28426, val loss: 2.35708\n",
      "Main effects training epoch: 28, train loss: 2.24121, val loss: 2.31604\n",
      "Main effects training epoch: 29, train loss: 2.22349, val loss: 2.30482\n",
      "Main effects training epoch: 30, train loss: 2.20247, val loss: 2.28138\n",
      "Main effects training epoch: 31, train loss: 2.17194, val loss: 2.25371\n",
      "Main effects training epoch: 32, train loss: 2.15609, val loss: 2.23852\n",
      "Main effects training epoch: 33, train loss: 2.11234, val loss: 2.18736\n",
      "Main effects training epoch: 34, train loss: 2.11890, val loss: 2.20559\n",
      "Main effects training epoch: 35, train loss: 2.07460, val loss: 2.15217\n",
      "Main effects training epoch: 36, train loss: 2.06756, val loss: 2.14275\n",
      "Main effects training epoch: 37, train loss: 2.03616, val loss: 2.11964\n",
      "Main effects training epoch: 38, train loss: 2.00906, val loss: 2.08613\n",
      "Main effects training epoch: 39, train loss: 2.01523, val loss: 2.08714\n",
      "Main effects training epoch: 40, train loss: 2.00932, val loss: 2.08913\n",
      "Main effects training epoch: 41, train loss: 1.97990, val loss: 2.05111\n",
      "Main effects training epoch: 42, train loss: 1.95610, val loss: 2.03029\n",
      "Main effects training epoch: 43, train loss: 1.95278, val loss: 2.02595\n",
      "Main effects training epoch: 44, train loss: 1.90841, val loss: 1.97820\n",
      "Main effects training epoch: 45, train loss: 1.92377, val loss: 1.99724\n",
      "Main effects training epoch: 46, train loss: 1.89995, val loss: 1.96687\n",
      "Main effects training epoch: 47, train loss: 1.88752, val loss: 1.96274\n",
      "Main effects training epoch: 48, train loss: 1.87637, val loss: 1.94493\n",
      "Main effects training epoch: 49, train loss: 1.86557, val loss: 1.93325\n",
      "Main effects training epoch: 50, train loss: 1.84842, val loss: 1.91845\n",
      "Main effects training epoch: 51, train loss: 1.84369, val loss: 1.90526\n",
      "Main effects training epoch: 52, train loss: 1.83295, val loss: 1.89869\n",
      "Main effects training epoch: 53, train loss: 1.83214, val loss: 1.89572\n",
      "Main effects training epoch: 54, train loss: 1.82643, val loss: 1.89382\n",
      "Main effects training epoch: 55, train loss: 1.80565, val loss: 1.87253\n",
      "Main effects training epoch: 56, train loss: 1.80548, val loss: 1.86689\n",
      "Main effects training epoch: 57, train loss: 1.78969, val loss: 1.85271\n",
      "Main effects training epoch: 58, train loss: 1.78388, val loss: 1.84780\n",
      "Main effects training epoch: 59, train loss: 1.78733, val loss: 1.85511\n",
      "Main effects training epoch: 60, train loss: 1.78396, val loss: 1.84056\n",
      "Main effects training epoch: 61, train loss: 1.76549, val loss: 1.82695\n",
      "Main effects training epoch: 62, train loss: 1.76732, val loss: 1.82506\n",
      "Main effects training epoch: 63, train loss: 1.76667, val loss: 1.82530\n",
      "Main effects training epoch: 64, train loss: 1.76171, val loss: 1.81646\n",
      "Main effects training epoch: 65, train loss: 1.75730, val loss: 1.81292\n",
      "Main effects training epoch: 66, train loss: 1.76083, val loss: 1.82039\n",
      "Main effects training epoch: 67, train loss: 1.74312, val loss: 1.79202\n",
      "Main effects training epoch: 68, train loss: 1.74930, val loss: 1.80727\n",
      "Main effects training epoch: 69, train loss: 1.73597, val loss: 1.78871\n",
      "Main effects training epoch: 70, train loss: 1.74614, val loss: 1.79932\n",
      "Main effects training epoch: 71, train loss: 1.73552, val loss: 1.78838\n",
      "Main effects training epoch: 72, train loss: 1.73118, val loss: 1.78356\n",
      "Main effects training epoch: 73, train loss: 1.72948, val loss: 1.78278\n",
      "Main effects training epoch: 74, train loss: 1.73004, val loss: 1.78017\n",
      "Main effects training epoch: 75, train loss: 1.72587, val loss: 1.78166\n",
      "Main effects training epoch: 76, train loss: 1.72994, val loss: 1.78207\n",
      "Main effects training epoch: 77, train loss: 1.72144, val loss: 1.77080\n",
      "Main effects training epoch: 78, train loss: 1.72129, val loss: 1.77145\n",
      "Main effects training epoch: 79, train loss: 1.71758, val loss: 1.77245\n",
      "Main effects training epoch: 80, train loss: 1.71542, val loss: 1.76721\n",
      "Main effects training epoch: 81, train loss: 1.71708, val loss: 1.77038\n",
      "Main effects training epoch: 82, train loss: 1.70823, val loss: 1.75433\n",
      "Main effects training epoch: 83, train loss: 1.70818, val loss: 1.76002\n",
      "Main effects training epoch: 84, train loss: 1.70672, val loss: 1.75743\n",
      "Main effects training epoch: 85, train loss: 1.70737, val loss: 1.75975\n",
      "Main effects training epoch: 86, train loss: 1.70690, val loss: 1.76225\n",
      "Main effects training epoch: 87, train loss: 1.70367, val loss: 1.75003\n",
      "Main effects training epoch: 88, train loss: 1.70574, val loss: 1.77065\n",
      "Main effects training epoch: 89, train loss: 1.69881, val loss: 1.74532\n",
      "Main effects training epoch: 90, train loss: 1.70003, val loss: 1.76253\n",
      "Main effects training epoch: 91, train loss: 1.69370, val loss: 1.74474\n",
      "Main effects training epoch: 92, train loss: 1.69256, val loss: 1.74962\n",
      "Main effects training epoch: 93, train loss: 1.68959, val loss: 1.73745\n",
      "Main effects training epoch: 94, train loss: 1.69048, val loss: 1.74998\n",
      "Main effects training epoch: 95, train loss: 1.68442, val loss: 1.73891\n",
      "Main effects training epoch: 96, train loss: 1.68244, val loss: 1.73544\n",
      "Main effects training epoch: 97, train loss: 1.68410, val loss: 1.74756\n",
      "Main effects training epoch: 98, train loss: 1.67409, val loss: 1.72389\n",
      "Main effects training epoch: 99, train loss: 1.67363, val loss: 1.72855\n",
      "Main effects training epoch: 100, train loss: 1.67744, val loss: 1.73980\n",
      "Main effects training epoch: 101, train loss: 1.66659, val loss: 1.72148\n",
      "Main effects training epoch: 102, train loss: 1.66758, val loss: 1.72206\n",
      "Main effects training epoch: 103, train loss: 1.66027, val loss: 1.71760\n",
      "Main effects training epoch: 104, train loss: 1.65101, val loss: 1.71108\n",
      "Main effects training epoch: 105, train loss: 1.65228, val loss: 1.71128\n",
      "Main effects training epoch: 106, train loss: 1.64528, val loss: 1.71485\n",
      "Main effects training epoch: 107, train loss: 1.64108, val loss: 1.70160\n",
      "Main effects training epoch: 108, train loss: 1.63046, val loss: 1.70215\n",
      "Main effects training epoch: 109, train loss: 1.62719, val loss: 1.69217\n",
      "Main effects training epoch: 110, train loss: 1.62222, val loss: 1.69176\n",
      "Main effects training epoch: 111, train loss: 1.64242, val loss: 1.71545\n",
      "Main effects training epoch: 112, train loss: 1.62778, val loss: 1.69352\n",
      "Main effects training epoch: 113, train loss: 1.62156, val loss: 1.69753\n",
      "Main effects training epoch: 114, train loss: 1.61718, val loss: 1.68450\n",
      "Main effects training epoch: 115, train loss: 1.61054, val loss: 1.68849\n",
      "Main effects training epoch: 116, train loss: 1.61316, val loss: 1.68425\n",
      "Main effects training epoch: 117, train loss: 1.60752, val loss: 1.69277\n",
      "Main effects training epoch: 118, train loss: 1.60696, val loss: 1.67507\n",
      "Main effects training epoch: 119, train loss: 1.59923, val loss: 1.68278\n",
      "Main effects training epoch: 120, train loss: 1.59434, val loss: 1.67012\n",
      "Main effects training epoch: 121, train loss: 1.59657, val loss: 1.67741\n",
      "Main effects training epoch: 122, train loss: 1.59713, val loss: 1.68293\n",
      "Main effects training epoch: 123, train loss: 1.59132, val loss: 1.66582\n",
      "Main effects training epoch: 124, train loss: 1.59126, val loss: 1.68452\n",
      "Main effects training epoch: 125, train loss: 1.59270, val loss: 1.66966\n",
      "Main effects training epoch: 126, train loss: 1.59117, val loss: 1.68447\n",
      "Main effects training epoch: 127, train loss: 1.59076, val loss: 1.66257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 128, train loss: 1.58665, val loss: 1.67147\n",
      "Main effects training epoch: 129, train loss: 1.58225, val loss: 1.66115\n",
      "Main effects training epoch: 130, train loss: 1.58095, val loss: 1.66400\n",
      "Main effects training epoch: 131, train loss: 1.58134, val loss: 1.66003\n",
      "Main effects training epoch: 132, train loss: 1.57814, val loss: 1.66383\n",
      "Main effects training epoch: 133, train loss: 1.58062, val loss: 1.66166\n",
      "Main effects training epoch: 134, train loss: 1.58034, val loss: 1.66530\n",
      "Main effects training epoch: 135, train loss: 1.58412, val loss: 1.66721\n",
      "Main effects training epoch: 136, train loss: 1.57679, val loss: 1.66130\n",
      "Main effects training epoch: 137, train loss: 1.57564, val loss: 1.66278\n",
      "Main effects training epoch: 138, train loss: 1.57811, val loss: 1.66114\n",
      "Main effects training epoch: 139, train loss: 1.58087, val loss: 1.66685\n",
      "Main effects training epoch: 140, train loss: 1.57420, val loss: 1.65711\n",
      "Main effects training epoch: 141, train loss: 1.58097, val loss: 1.66378\n",
      "Main effects training epoch: 142, train loss: 1.57293, val loss: 1.65921\n",
      "Main effects training epoch: 143, train loss: 1.57491, val loss: 1.65538\n",
      "Main effects training epoch: 144, train loss: 1.58238, val loss: 1.66613\n",
      "Main effects training epoch: 145, train loss: 1.57482, val loss: 1.66062\n",
      "Main effects training epoch: 146, train loss: 1.58287, val loss: 1.66299\n",
      "Main effects training epoch: 147, train loss: 1.57397, val loss: 1.65856\n",
      "Main effects training epoch: 148, train loss: 1.57630, val loss: 1.66433\n",
      "Main effects training epoch: 149, train loss: 1.56826, val loss: 1.64309\n",
      "Main effects training epoch: 150, train loss: 1.56617, val loss: 1.64983\n",
      "Main effects training epoch: 151, train loss: 1.56745, val loss: 1.64596\n",
      "Main effects training epoch: 152, train loss: 1.56873, val loss: 1.65185\n",
      "Main effects training epoch: 153, train loss: 1.56498, val loss: 1.64461\n",
      "Main effects training epoch: 154, train loss: 1.56689, val loss: 1.65426\n",
      "Main effects training epoch: 155, train loss: 1.57307, val loss: 1.64557\n",
      "Main effects training epoch: 156, train loss: 1.56731, val loss: 1.65074\n",
      "Main effects training epoch: 157, train loss: 1.56651, val loss: 1.63715\n",
      "Main effects training epoch: 158, train loss: 1.56424, val loss: 1.65166\n",
      "Main effects training epoch: 159, train loss: 1.55933, val loss: 1.64674\n",
      "Main effects training epoch: 160, train loss: 1.56484, val loss: 1.64140\n",
      "Main effects training epoch: 161, train loss: 1.55869, val loss: 1.64190\n",
      "Main effects training epoch: 162, train loss: 1.56010, val loss: 1.63494\n",
      "Main effects training epoch: 163, train loss: 1.55875, val loss: 1.64945\n",
      "Main effects training epoch: 164, train loss: 1.56017, val loss: 1.62652\n",
      "Main effects training epoch: 165, train loss: 1.55687, val loss: 1.65054\n",
      "Main effects training epoch: 166, train loss: 1.55824, val loss: 1.63689\n",
      "Main effects training epoch: 167, train loss: 1.55288, val loss: 1.63071\n",
      "Main effects training epoch: 168, train loss: 1.57395, val loss: 1.64467\n",
      "Main effects training epoch: 169, train loss: 1.55953, val loss: 1.63941\n",
      "Main effects training epoch: 170, train loss: 1.56055, val loss: 1.64201\n",
      "Main effects training epoch: 171, train loss: 1.55284, val loss: 1.62011\n",
      "Main effects training epoch: 172, train loss: 1.55646, val loss: 1.64794\n",
      "Main effects training epoch: 173, train loss: 1.55002, val loss: 1.61833\n",
      "Main effects training epoch: 174, train loss: 1.54811, val loss: 1.63436\n",
      "Main effects training epoch: 175, train loss: 1.55392, val loss: 1.62785\n",
      "Main effects training epoch: 176, train loss: 1.54753, val loss: 1.62696\n",
      "Main effects training epoch: 177, train loss: 1.55024, val loss: 1.63404\n",
      "Main effects training epoch: 178, train loss: 1.54905, val loss: 1.62490\n",
      "Main effects training epoch: 179, train loss: 1.54696, val loss: 1.62119\n",
      "Main effects training epoch: 180, train loss: 1.55349, val loss: 1.63279\n",
      "Main effects training epoch: 181, train loss: 1.54293, val loss: 1.61892\n",
      "Main effects training epoch: 182, train loss: 1.54544, val loss: 1.62199\n",
      "Main effects training epoch: 183, train loss: 1.54228, val loss: 1.61704\n",
      "Main effects training epoch: 184, train loss: 1.54044, val loss: 1.60739\n",
      "Main effects training epoch: 185, train loss: 1.54087, val loss: 1.62460\n",
      "Main effects training epoch: 186, train loss: 1.53728, val loss: 1.60874\n",
      "Main effects training epoch: 187, train loss: 1.53766, val loss: 1.62381\n",
      "Main effects training epoch: 188, train loss: 1.54333, val loss: 1.61908\n",
      "Main effects training epoch: 189, train loss: 1.53849, val loss: 1.60844\n",
      "Main effects training epoch: 190, train loss: 1.53714, val loss: 1.61316\n",
      "Main effects training epoch: 191, train loss: 1.53803, val loss: 1.61816\n",
      "Main effects training epoch: 192, train loss: 1.54008, val loss: 1.61236\n",
      "Main effects training epoch: 193, train loss: 1.53945, val loss: 1.62800\n",
      "Main effects training epoch: 194, train loss: 1.54394, val loss: 1.59910\n",
      "Main effects training epoch: 195, train loss: 1.54099, val loss: 1.62879\n",
      "Main effects training epoch: 196, train loss: 1.55065, val loss: 1.62264\n",
      "Main effects training epoch: 197, train loss: 1.55286, val loss: 1.62283\n",
      "Main effects training epoch: 198, train loss: 1.55517, val loss: 1.61551\n",
      "Main effects training epoch: 199, train loss: 1.53672, val loss: 1.60873\n",
      "Main effects training epoch: 200, train loss: 1.53435, val loss: 1.61124\n",
      "Main effects training epoch: 201, train loss: 1.54005, val loss: 1.60852\n",
      "Main effects training epoch: 202, train loss: 1.54050, val loss: 1.61467\n",
      "Main effects training epoch: 203, train loss: 1.53806, val loss: 1.61337\n",
      "Main effects training epoch: 204, train loss: 1.53242, val loss: 1.60933\n",
      "Main effects training epoch: 205, train loss: 1.53429, val loss: 1.60317\n",
      "Main effects training epoch: 206, train loss: 1.53655, val loss: 1.60741\n",
      "Main effects training epoch: 207, train loss: 1.53349, val loss: 1.60815\n",
      "Main effects training epoch: 208, train loss: 1.53402, val loss: 1.60336\n",
      "Main effects training epoch: 209, train loss: 1.54431, val loss: 1.62855\n",
      "Main effects training epoch: 210, train loss: 1.53383, val loss: 1.59773\n",
      "Main effects training epoch: 211, train loss: 1.53353, val loss: 1.61599\n",
      "Main effects training epoch: 212, train loss: 1.53135, val loss: 1.60496\n",
      "Main effects training epoch: 213, train loss: 1.53909, val loss: 1.60362\n",
      "Main effects training epoch: 214, train loss: 1.53562, val loss: 1.61107\n",
      "Main effects training epoch: 215, train loss: 1.53883, val loss: 1.61660\n",
      "Main effects training epoch: 216, train loss: 1.53483, val loss: 1.60713\n",
      "Main effects training epoch: 217, train loss: 1.53193, val loss: 1.60146\n",
      "Main effects training epoch: 218, train loss: 1.53158, val loss: 1.60463\n",
      "Main effects training epoch: 219, train loss: 1.52893, val loss: 1.60268\n",
      "Main effects training epoch: 220, train loss: 1.53949, val loss: 1.61555\n",
      "Main effects training epoch: 221, train loss: 1.53449, val loss: 1.60597\n",
      "Main effects training epoch: 222, train loss: 1.53221, val loss: 1.60131\n",
      "Main effects training epoch: 223, train loss: 1.52936, val loss: 1.59796\n",
      "Main effects training epoch: 224, train loss: 1.53475, val loss: 1.61383\n",
      "Main effects training epoch: 225, train loss: 1.52924, val loss: 1.59527\n",
      "Main effects training epoch: 226, train loss: 1.52479, val loss: 1.60220\n",
      "Main effects training epoch: 227, train loss: 1.52611, val loss: 1.59221\n",
      "Main effects training epoch: 228, train loss: 1.52875, val loss: 1.59828\n",
      "Main effects training epoch: 229, train loss: 1.52856, val loss: 1.60775\n",
      "Main effects training epoch: 230, train loss: 1.52373, val loss: 1.58684\n",
      "Main effects training epoch: 231, train loss: 1.52417, val loss: 1.59201\n",
      "Main effects training epoch: 232, train loss: 1.53523, val loss: 1.61018\n",
      "Main effects training epoch: 233, train loss: 1.52694, val loss: 1.59339\n",
      "Main effects training epoch: 234, train loss: 1.52752, val loss: 1.60506\n",
      "Main effects training epoch: 235, train loss: 1.52706, val loss: 1.59953\n",
      "Main effects training epoch: 236, train loss: 1.52193, val loss: 1.59137\n",
      "Main effects training epoch: 237, train loss: 1.52181, val loss: 1.59384\n",
      "Main effects training epoch: 238, train loss: 1.52517, val loss: 1.59994\n",
      "Main effects training epoch: 239, train loss: 1.51713, val loss: 1.58390\n",
      "Main effects training epoch: 240, train loss: 1.52300, val loss: 1.59302\n",
      "Main effects training epoch: 241, train loss: 1.52083, val loss: 1.60237\n",
      "Main effects training epoch: 242, train loss: 1.51754, val loss: 1.57950\n",
      "Main effects training epoch: 243, train loss: 1.51553, val loss: 1.58469\n",
      "Main effects training epoch: 244, train loss: 1.51152, val loss: 1.58478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 245, train loss: 1.51119, val loss: 1.58977\n",
      "Main effects training epoch: 246, train loss: 1.51268, val loss: 1.58644\n",
      "Main effects training epoch: 247, train loss: 1.50851, val loss: 1.57810\n",
      "Main effects training epoch: 248, train loss: 1.50475, val loss: 1.57346\n",
      "Main effects training epoch: 249, train loss: 1.50708, val loss: 1.58795\n",
      "Main effects training epoch: 250, train loss: 1.50235, val loss: 1.56329\n",
      "Main effects training epoch: 251, train loss: 1.50375, val loss: 1.56692\n",
      "Main effects training epoch: 252, train loss: 1.50156, val loss: 1.57071\n",
      "Main effects training epoch: 253, train loss: 1.50606, val loss: 1.58064\n",
      "Main effects training epoch: 254, train loss: 1.50227, val loss: 1.57147\n",
      "Main effects training epoch: 255, train loss: 1.50157, val loss: 1.56699\n",
      "Main effects training epoch: 256, train loss: 1.49727, val loss: 1.55798\n",
      "Main effects training epoch: 257, train loss: 1.49102, val loss: 1.56357\n",
      "Main effects training epoch: 258, train loss: 1.49328, val loss: 1.56866\n",
      "Main effects training epoch: 259, train loss: 1.49033, val loss: 1.55771\n",
      "Main effects training epoch: 260, train loss: 1.49099, val loss: 1.56461\n",
      "Main effects training epoch: 261, train loss: 1.48797, val loss: 1.55829\n",
      "Main effects training epoch: 262, train loss: 1.48513, val loss: 1.56245\n",
      "Main effects training epoch: 263, train loss: 1.48732, val loss: 1.55343\n",
      "Main effects training epoch: 264, train loss: 1.48759, val loss: 1.55942\n",
      "Main effects training epoch: 265, train loss: 1.48754, val loss: 1.55547\n",
      "Main effects training epoch: 266, train loss: 1.48333, val loss: 1.55552\n",
      "Main effects training epoch: 267, train loss: 1.48151, val loss: 1.55740\n",
      "Main effects training epoch: 268, train loss: 1.48485, val loss: 1.54415\n",
      "Main effects training epoch: 269, train loss: 1.48796, val loss: 1.56269\n",
      "Main effects training epoch: 270, train loss: 1.48853, val loss: 1.54827\n",
      "Main effects training epoch: 271, train loss: 1.49240, val loss: 1.55333\n",
      "Main effects training epoch: 272, train loss: 1.49335, val loss: 1.55321\n",
      "Main effects training epoch: 273, train loss: 1.48015, val loss: 1.55126\n",
      "Main effects training epoch: 274, train loss: 1.47496, val loss: 1.54581\n",
      "Main effects training epoch: 275, train loss: 1.47821, val loss: 1.54314\n",
      "Main effects training epoch: 276, train loss: 1.48003, val loss: 1.55256\n",
      "Main effects training epoch: 277, train loss: 1.48604, val loss: 1.54271\n",
      "Main effects training epoch: 278, train loss: 1.47571, val loss: 1.52932\n",
      "Main effects training epoch: 279, train loss: 1.48625, val loss: 1.56192\n",
      "Main effects training epoch: 280, train loss: 1.48386, val loss: 1.53366\n",
      "Main effects training epoch: 281, train loss: 1.48290, val loss: 1.54293\n",
      "Main effects training epoch: 282, train loss: 1.47268, val loss: 1.52979\n",
      "Main effects training epoch: 283, train loss: 1.47247, val loss: 1.53684\n",
      "Main effects training epoch: 284, train loss: 1.47618, val loss: 1.53794\n",
      "Main effects training epoch: 285, train loss: 1.47740, val loss: 1.52996\n",
      "Main effects training epoch: 286, train loss: 1.47922, val loss: 1.53749\n",
      "Main effects training epoch: 287, train loss: 1.46874, val loss: 1.52546\n",
      "Main effects training epoch: 288, train loss: 1.46895, val loss: 1.53249\n",
      "Main effects training epoch: 289, train loss: 1.46914, val loss: 1.52660\n",
      "Main effects training epoch: 290, train loss: 1.46519, val loss: 1.53070\n",
      "Main effects training epoch: 291, train loss: 1.46873, val loss: 1.52014\n",
      "Main effects training epoch: 292, train loss: 1.46470, val loss: 1.51975\n",
      "Main effects training epoch: 293, train loss: 1.46914, val loss: 1.53282\n",
      "Main effects training epoch: 294, train loss: 1.46030, val loss: 1.52605\n",
      "Main effects training epoch: 295, train loss: 1.46756, val loss: 1.51706\n",
      "Main effects training epoch: 296, train loss: 1.45796, val loss: 1.51060\n",
      "Main effects training epoch: 297, train loss: 1.46605, val loss: 1.53025\n",
      "Main effects training epoch: 298, train loss: 1.45392, val loss: 1.50637\n",
      "Main effects training epoch: 299, train loss: 1.45857, val loss: 1.51584\n",
      "Main effects training epoch: 300, train loss: 1.46302, val loss: 1.53093\n",
      "##########Stage 1: main effect training stop.##########\n",
      "3 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.48618, val loss: 1.51188\n",
      "Main effects tuning epoch: 2, train loss: 1.48381, val loss: 1.52934\n",
      "Main effects tuning epoch: 3, train loss: 1.47631, val loss: 1.51358\n",
      "Main effects tuning epoch: 4, train loss: 1.49098, val loss: 1.52052\n",
      "Main effects tuning epoch: 5, train loss: 1.47613, val loss: 1.51049\n",
      "Main effects tuning epoch: 6, train loss: 1.48957, val loss: 1.53120\n",
      "Main effects tuning epoch: 7, train loss: 1.47658, val loss: 1.51153\n",
      "Main effects tuning epoch: 8, train loss: 1.47899, val loss: 1.51856\n",
      "Main effects tuning epoch: 9, train loss: 1.47659, val loss: 1.51415\n",
      "Main effects tuning epoch: 10, train loss: 1.47241, val loss: 1.50790\n",
      "Main effects tuning epoch: 11, train loss: 1.47979, val loss: 1.52039\n",
      "Main effects tuning epoch: 12, train loss: 1.47664, val loss: 1.50713\n",
      "Main effects tuning epoch: 13, train loss: 1.48620, val loss: 1.53692\n",
      "Main effects tuning epoch: 14, train loss: 1.48808, val loss: 1.51501\n",
      "Main effects tuning epoch: 15, train loss: 1.47312, val loss: 1.52201\n",
      "Main effects tuning epoch: 16, train loss: 1.46794, val loss: 1.50331\n",
      "Main effects tuning epoch: 17, train loss: 1.47474, val loss: 1.51678\n",
      "Main effects tuning epoch: 18, train loss: 1.46616, val loss: 1.49913\n",
      "Main effects tuning epoch: 19, train loss: 1.46254, val loss: 1.49842\n",
      "Main effects tuning epoch: 20, train loss: 1.47298, val loss: 1.50096\n",
      "Main effects tuning epoch: 21, train loss: 1.46392, val loss: 1.50891\n",
      "Main effects tuning epoch: 22, train loss: 1.46986, val loss: 1.50850\n",
      "Main effects tuning epoch: 23, train loss: 1.46237, val loss: 1.50457\n",
      "Main effects tuning epoch: 24, train loss: 1.47206, val loss: 1.49871\n",
      "Main effects tuning epoch: 25, train loss: 1.46778, val loss: 1.50486\n",
      "Main effects tuning epoch: 26, train loss: 1.46955, val loss: 1.49842\n",
      "Main effects tuning epoch: 27, train loss: 1.46210, val loss: 1.50114\n",
      "Main effects tuning epoch: 28, train loss: 1.46229, val loss: 1.51342\n",
      "Main effects tuning epoch: 29, train loss: 1.46146, val loss: 1.48910\n",
      "Main effects tuning epoch: 30, train loss: 1.46499, val loss: 1.50173\n",
      "Main effects tuning epoch: 31, train loss: 1.47308, val loss: 1.51040\n",
      "Main effects tuning epoch: 32, train loss: 1.46457, val loss: 1.49352\n",
      "Main effects tuning epoch: 33, train loss: 1.49161, val loss: 1.52024\n",
      "Main effects tuning epoch: 34, train loss: 1.49418, val loss: 1.54026\n",
      "Main effects tuning epoch: 35, train loss: 1.46364, val loss: 1.49741\n",
      "Main effects tuning epoch: 36, train loss: 1.45525, val loss: 1.49440\n",
      "Main effects tuning epoch: 37, train loss: 1.45460, val loss: 1.49386\n",
      "Main effects tuning epoch: 38, train loss: 1.45395, val loss: 1.48827\n",
      "Main effects tuning epoch: 39, train loss: 1.45665, val loss: 1.48940\n",
      "Main effects tuning epoch: 40, train loss: 1.45614, val loss: 1.49135\n",
      "Main effects tuning epoch: 41, train loss: 1.45318, val loss: 1.49149\n",
      "Main effects tuning epoch: 42, train loss: 1.45748, val loss: 1.49395\n",
      "Main effects tuning epoch: 43, train loss: 1.45886, val loss: 1.49746\n",
      "Main effects tuning epoch: 44, train loss: 1.46298, val loss: 1.49714\n",
      "Main effects tuning epoch: 45, train loss: 1.45201, val loss: 1.48259\n",
      "Main effects tuning epoch: 46, train loss: 1.45425, val loss: 1.48623\n",
      "Main effects tuning epoch: 47, train loss: 1.45860, val loss: 1.49589\n",
      "Main effects tuning epoch: 48, train loss: 1.47414, val loss: 1.50144\n",
      "Main effects tuning epoch: 49, train loss: 1.45879, val loss: 1.49895\n",
      "Main effects tuning epoch: 50, train loss: 1.45418, val loss: 1.48377\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.46938, val loss: 1.49557\n",
      "Interaction training epoch: 2, train loss: 1.22464, val loss: 1.18558\n",
      "Interaction training epoch: 3, train loss: 1.14017, val loss: 1.14608\n",
      "Interaction training epoch: 4, train loss: 1.06885, val loss: 1.07150\n",
      "Interaction training epoch: 5, train loss: 1.03112, val loss: 1.01238\n",
      "Interaction training epoch: 6, train loss: 1.02071, val loss: 1.01588\n",
      "Interaction training epoch: 7, train loss: 1.02011, val loss: 1.02150\n",
      "Interaction training epoch: 8, train loss: 1.00544, val loss: 0.99518\n",
      "Interaction training epoch: 9, train loss: 0.98911, val loss: 0.98035\n",
      "Interaction training epoch: 10, train loss: 0.99496, val loss: 0.98963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 11, train loss: 1.00012, val loss: 0.99541\n",
      "Interaction training epoch: 12, train loss: 0.98310, val loss: 0.97099\n",
      "Interaction training epoch: 13, train loss: 0.98327, val loss: 0.96967\n",
      "Interaction training epoch: 14, train loss: 0.98267, val loss: 0.97757\n",
      "Interaction training epoch: 15, train loss: 0.97491, val loss: 0.97743\n",
      "Interaction training epoch: 16, train loss: 0.94797, val loss: 0.94595\n",
      "Interaction training epoch: 17, train loss: 0.94356, val loss: 0.94184\n",
      "Interaction training epoch: 18, train loss: 0.94736, val loss: 0.93860\n",
      "Interaction training epoch: 19, train loss: 0.95369, val loss: 0.95350\n",
      "Interaction training epoch: 20, train loss: 0.93758, val loss: 0.93649\n",
      "Interaction training epoch: 21, train loss: 0.95146, val loss: 0.95238\n",
      "Interaction training epoch: 22, train loss: 0.94002, val loss: 0.94402\n",
      "Interaction training epoch: 23, train loss: 0.93808, val loss: 0.93538\n",
      "Interaction training epoch: 24, train loss: 0.96148, val loss: 0.96227\n",
      "Interaction training epoch: 25, train loss: 0.93512, val loss: 0.93942\n",
      "Interaction training epoch: 26, train loss: 0.91858, val loss: 0.90958\n",
      "Interaction training epoch: 27, train loss: 0.92857, val loss: 0.92089\n",
      "Interaction training epoch: 28, train loss: 0.91130, val loss: 0.91526\n",
      "Interaction training epoch: 29, train loss: 0.91723, val loss: 0.91536\n",
      "Interaction training epoch: 30, train loss: 0.89939, val loss: 0.90174\n",
      "Interaction training epoch: 31, train loss: 0.89369, val loss: 0.89959\n",
      "Interaction training epoch: 32, train loss: 0.91812, val loss: 0.91877\n",
      "Interaction training epoch: 33, train loss: 0.90314, val loss: 0.89647\n",
      "Interaction training epoch: 34, train loss: 0.89346, val loss: 0.88999\n",
      "Interaction training epoch: 35, train loss: 0.89733, val loss: 0.89851\n",
      "Interaction training epoch: 36, train loss: 0.88521, val loss: 0.87949\n",
      "Interaction training epoch: 37, train loss: 0.89855, val loss: 0.89150\n",
      "Interaction training epoch: 38, train loss: 0.87952, val loss: 0.88060\n",
      "Interaction training epoch: 39, train loss: 0.90166, val loss: 0.88720\n",
      "Interaction training epoch: 40, train loss: 0.89495, val loss: 0.89246\n",
      "Interaction training epoch: 41, train loss: 0.88758, val loss: 0.88589\n",
      "Interaction training epoch: 42, train loss: 0.88125, val loss: 0.87745\n",
      "Interaction training epoch: 43, train loss: 0.87372, val loss: 0.87555\n",
      "Interaction training epoch: 44, train loss: 0.87044, val loss: 0.86025\n",
      "Interaction training epoch: 45, train loss: 0.86519, val loss: 0.86769\n",
      "Interaction training epoch: 46, train loss: 0.87113, val loss: 0.86575\n",
      "Interaction training epoch: 47, train loss: 0.86741, val loss: 0.86959\n",
      "Interaction training epoch: 48, train loss: 0.87336, val loss: 0.87446\n",
      "Interaction training epoch: 49, train loss: 0.86421, val loss: 0.85813\n",
      "Interaction training epoch: 50, train loss: 0.86373, val loss: 0.85709\n",
      "Interaction training epoch: 51, train loss: 0.87412, val loss: 0.88176\n",
      "Interaction training epoch: 52, train loss: 0.86147, val loss: 0.86159\n",
      "Interaction training epoch: 53, train loss: 0.86513, val loss: 0.85817\n",
      "Interaction training epoch: 54, train loss: 0.86132, val loss: 0.85422\n",
      "Interaction training epoch: 55, train loss: 0.88980, val loss: 0.88766\n",
      "Interaction training epoch: 56, train loss: 0.85467, val loss: 0.84937\n",
      "Interaction training epoch: 57, train loss: 0.87906, val loss: 0.86960\n",
      "Interaction training epoch: 58, train loss: 0.86503, val loss: 0.85289\n",
      "Interaction training epoch: 59, train loss: 0.87896, val loss: 0.87548\n",
      "Interaction training epoch: 60, train loss: 0.86117, val loss: 0.84865\n",
      "Interaction training epoch: 61, train loss: 0.85636, val loss: 0.85481\n",
      "Interaction training epoch: 62, train loss: 0.85703, val loss: 0.85623\n",
      "Interaction training epoch: 63, train loss: 0.85876, val loss: 0.85808\n",
      "Interaction training epoch: 64, train loss: 0.86306, val loss: 0.86072\n",
      "Interaction training epoch: 65, train loss: 0.88288, val loss: 0.88419\n",
      "Interaction training epoch: 66, train loss: 0.87522, val loss: 0.88429\n",
      "Interaction training epoch: 67, train loss: 0.86080, val loss: 0.85777\n",
      "Interaction training epoch: 68, train loss: 0.85545, val loss: 0.85623\n",
      "Interaction training epoch: 69, train loss: 0.84813, val loss: 0.84481\n",
      "Interaction training epoch: 70, train loss: 0.85171, val loss: 0.85155\n",
      "Interaction training epoch: 71, train loss: 0.85072, val loss: 0.84935\n",
      "Interaction training epoch: 72, train loss: 0.85515, val loss: 0.85010\n",
      "Interaction training epoch: 73, train loss: 0.84212, val loss: 0.84186\n",
      "Interaction training epoch: 74, train loss: 0.84764, val loss: 0.83953\n",
      "Interaction training epoch: 75, train loss: 0.84915, val loss: 0.85373\n",
      "Interaction training epoch: 76, train loss: 0.84401, val loss: 0.85536\n",
      "Interaction training epoch: 77, train loss: 0.85429, val loss: 0.84274\n",
      "Interaction training epoch: 78, train loss: 0.84091, val loss: 0.84399\n",
      "Interaction training epoch: 79, train loss: 0.84052, val loss: 0.84858\n",
      "Interaction training epoch: 80, train loss: 0.84880, val loss: 0.84155\n",
      "Interaction training epoch: 81, train loss: 0.86907, val loss: 0.86661\n",
      "Interaction training epoch: 82, train loss: 0.85847, val loss: 0.84690\n",
      "Interaction training epoch: 83, train loss: 0.84635, val loss: 0.84297\n",
      "Interaction training epoch: 84, train loss: 0.85620, val loss: 0.84851\n",
      "Interaction training epoch: 85, train loss: 0.84787, val loss: 0.85483\n",
      "Interaction training epoch: 86, train loss: 0.85147, val loss: 0.84822\n",
      "Interaction training epoch: 87, train loss: 0.84301, val loss: 0.84853\n",
      "Interaction training epoch: 88, train loss: 0.85106, val loss: 0.85273\n",
      "Interaction training epoch: 89, train loss: 0.83908, val loss: 0.83371\n",
      "Interaction training epoch: 90, train loss: 0.83994, val loss: 0.84356\n",
      "Interaction training epoch: 91, train loss: 0.83709, val loss: 0.83675\n",
      "Interaction training epoch: 92, train loss: 0.83838, val loss: 0.84261\n",
      "Interaction training epoch: 93, train loss: 0.84758, val loss: 0.85433\n",
      "Interaction training epoch: 94, train loss: 0.83567, val loss: 0.84333\n",
      "Interaction training epoch: 95, train loss: 0.84221, val loss: 0.84139\n",
      "Interaction training epoch: 96, train loss: 0.84626, val loss: 0.84002\n",
      "Interaction training epoch: 97, train loss: 0.84301, val loss: 0.85055\n",
      "Interaction training epoch: 98, train loss: 0.83598, val loss: 0.83461\n",
      "Interaction training epoch: 99, train loss: 0.84332, val loss: 0.84606\n",
      "Interaction training epoch: 100, train loss: 0.83433, val loss: 0.84495\n",
      "Interaction training epoch: 101, train loss: 0.83481, val loss: 0.83834\n",
      "Interaction training epoch: 102, train loss: 0.83306, val loss: 0.83442\n",
      "Interaction training epoch: 103, train loss: 0.83813, val loss: 0.84552\n",
      "Interaction training epoch: 104, train loss: 0.83353, val loss: 0.83806\n",
      "Interaction training epoch: 105, train loss: 0.84271, val loss: 0.83739\n",
      "Interaction training epoch: 106, train loss: 0.83233, val loss: 0.83326\n",
      "Interaction training epoch: 107, train loss: 0.84153, val loss: 0.84719\n",
      "Interaction training epoch: 108, train loss: 0.84383, val loss: 0.85564\n",
      "Interaction training epoch: 109, train loss: 0.84281, val loss: 0.84755\n",
      "Interaction training epoch: 110, train loss: 0.84589, val loss: 0.84481\n",
      "Interaction training epoch: 111, train loss: 0.83869, val loss: 0.83998\n",
      "Interaction training epoch: 112, train loss: 0.84036, val loss: 0.84500\n",
      "Interaction training epoch: 113, train loss: 0.82834, val loss: 0.83964\n",
      "Interaction training epoch: 114, train loss: 0.83580, val loss: 0.83470\n",
      "Interaction training epoch: 115, train loss: 0.84241, val loss: 0.84537\n",
      "Interaction training epoch: 116, train loss: 0.83167, val loss: 0.83927\n",
      "Interaction training epoch: 117, train loss: 0.83282, val loss: 0.84589\n",
      "Interaction training epoch: 118, train loss: 0.83148, val loss: 0.83142\n",
      "Interaction training epoch: 119, train loss: 0.82801, val loss: 0.83390\n",
      "Interaction training epoch: 120, train loss: 0.82981, val loss: 0.83550\n",
      "Interaction training epoch: 121, train loss: 0.82898, val loss: 0.83557\n",
      "Interaction training epoch: 122, train loss: 0.82919, val loss: 0.83826\n",
      "Interaction training epoch: 123, train loss: 0.83160, val loss: 0.82929\n",
      "Interaction training epoch: 124, train loss: 0.83254, val loss: 0.84019\n",
      "Interaction training epoch: 125, train loss: 0.83540, val loss: 0.84629\n",
      "Interaction training epoch: 126, train loss: 0.83137, val loss: 0.83337\n",
      "Interaction training epoch: 127, train loss: 0.82803, val loss: 0.83325\n",
      "Interaction training epoch: 128, train loss: 0.82668, val loss: 0.83433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 129, train loss: 0.83026, val loss: 0.83309\n",
      "Interaction training epoch: 130, train loss: 0.83507, val loss: 0.83384\n",
      "Interaction training epoch: 131, train loss: 0.82699, val loss: 0.83693\n",
      "Interaction training epoch: 132, train loss: 0.82952, val loss: 0.83377\n",
      "Interaction training epoch: 133, train loss: 0.82638, val loss: 0.82880\n",
      "Interaction training epoch: 134, train loss: 0.83320, val loss: 0.84619\n",
      "Interaction training epoch: 135, train loss: 0.82884, val loss: 0.82799\n",
      "Interaction training epoch: 136, train loss: 0.83308, val loss: 0.83919\n",
      "Interaction training epoch: 137, train loss: 0.82685, val loss: 0.83040\n",
      "Interaction training epoch: 138, train loss: 0.83775, val loss: 0.84972\n",
      "Interaction training epoch: 139, train loss: 0.83576, val loss: 0.83606\n",
      "Interaction training epoch: 140, train loss: 0.83015, val loss: 0.83859\n",
      "Interaction training epoch: 141, train loss: 0.83514, val loss: 0.84809\n",
      "Interaction training epoch: 142, train loss: 0.82882, val loss: 0.82881\n",
      "Interaction training epoch: 143, train loss: 0.82885, val loss: 0.82944\n",
      "Interaction training epoch: 144, train loss: 0.83280, val loss: 0.83780\n",
      "Interaction training epoch: 145, train loss: 0.83422, val loss: 0.83868\n",
      "Interaction training epoch: 146, train loss: 0.82976, val loss: 0.83811\n",
      "Interaction training epoch: 147, train loss: 0.82565, val loss: 0.83583\n",
      "Interaction training epoch: 148, train loss: 0.83132, val loss: 0.83143\n",
      "Interaction training epoch: 149, train loss: 0.83627, val loss: 0.84520\n",
      "Interaction training epoch: 150, train loss: 0.82822, val loss: 0.82606\n",
      "Interaction training epoch: 151, train loss: 0.83502, val loss: 0.84117\n",
      "Interaction training epoch: 152, train loss: 0.82468, val loss: 0.83218\n",
      "Interaction training epoch: 153, train loss: 0.83202, val loss: 0.84143\n",
      "Interaction training epoch: 154, train loss: 0.84085, val loss: 0.84536\n",
      "Interaction training epoch: 155, train loss: 0.82985, val loss: 0.83725\n",
      "Interaction training epoch: 156, train loss: 0.83108, val loss: 0.83562\n",
      "Interaction training epoch: 157, train loss: 0.83476, val loss: 0.84294\n",
      "Interaction training epoch: 158, train loss: 0.83576, val loss: 0.82957\n",
      "Interaction training epoch: 159, train loss: 0.83055, val loss: 0.83711\n",
      "Interaction training epoch: 160, train loss: 0.82637, val loss: 0.83117\n",
      "Interaction training epoch: 161, train loss: 0.82689, val loss: 0.83483\n",
      "Interaction training epoch: 162, train loss: 0.83126, val loss: 0.83592\n",
      "Interaction training epoch: 163, train loss: 0.83075, val loss: 0.83864\n",
      "Interaction training epoch: 164, train loss: 0.83637, val loss: 0.83227\n",
      "Interaction training epoch: 165, train loss: 0.82371, val loss: 0.83614\n",
      "Interaction training epoch: 166, train loss: 0.82773, val loss: 0.84136\n",
      "Interaction training epoch: 167, train loss: 0.83357, val loss: 0.83920\n",
      "Interaction training epoch: 168, train loss: 0.82967, val loss: 0.82866\n",
      "Interaction training epoch: 169, train loss: 0.83184, val loss: 0.83975\n",
      "Interaction training epoch: 170, train loss: 0.82696, val loss: 0.83301\n",
      "Interaction training epoch: 171, train loss: 0.82908, val loss: 0.84185\n",
      "Interaction training epoch: 172, train loss: 0.83422, val loss: 0.84429\n",
      "Interaction training epoch: 173, train loss: 0.83383, val loss: 0.83031\n",
      "Interaction training epoch: 174, train loss: 0.82451, val loss: 0.82907\n",
      "Interaction training epoch: 175, train loss: 0.82640, val loss: 0.84035\n",
      "Interaction training epoch: 176, train loss: 0.83271, val loss: 0.82802\n",
      "Interaction training epoch: 177, train loss: 0.83500, val loss: 0.83614\n",
      "Interaction training epoch: 178, train loss: 0.83008, val loss: 0.84346\n",
      "Interaction training epoch: 179, train loss: 0.82710, val loss: 0.83631\n",
      "Interaction training epoch: 180, train loss: 0.82496, val loss: 0.82911\n",
      "Interaction training epoch: 181, train loss: 0.82587, val loss: 0.83024\n",
      "Interaction training epoch: 182, train loss: 0.82003, val loss: 0.82584\n",
      "Interaction training epoch: 183, train loss: 0.82799, val loss: 0.83241\n",
      "Interaction training epoch: 184, train loss: 0.82813, val loss: 0.82918\n",
      "Interaction training epoch: 185, train loss: 0.83260, val loss: 0.84204\n",
      "Interaction training epoch: 186, train loss: 0.82428, val loss: 0.82974\n",
      "Interaction training epoch: 187, train loss: 0.83202, val loss: 0.82704\n",
      "Interaction training epoch: 188, train loss: 0.82519, val loss: 0.83502\n",
      "Interaction training epoch: 189, train loss: 0.82852, val loss: 0.84031\n",
      "Interaction training epoch: 190, train loss: 0.82719, val loss: 0.83589\n",
      "Interaction training epoch: 191, train loss: 0.82817, val loss: 0.82432\n",
      "Interaction training epoch: 192, train loss: 0.83134, val loss: 0.83996\n",
      "Interaction training epoch: 193, train loss: 0.82270, val loss: 0.83033\n",
      "Interaction training epoch: 194, train loss: 0.82941, val loss: 0.83506\n",
      "Interaction training epoch: 195, train loss: 0.82515, val loss: 0.82818\n",
      "Interaction training epoch: 196, train loss: 0.82555, val loss: 0.83494\n",
      "Interaction training epoch: 197, train loss: 0.82902, val loss: 0.83261\n",
      "Interaction training epoch: 198, train loss: 0.82219, val loss: 0.82795\n",
      "Interaction training epoch: 199, train loss: 0.82606, val loss: 0.83729\n",
      "Interaction training epoch: 200, train loss: 0.82286, val loss: 0.82730\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########1 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.82881, val loss: 0.83417\n",
      "Interaction tuning epoch: 2, train loss: 0.82697, val loss: 0.83366\n",
      "Interaction tuning epoch: 3, train loss: 0.82976, val loss: 0.83385\n",
      "Interaction tuning epoch: 4, train loss: 0.83078, val loss: 0.82868\n",
      "Interaction tuning epoch: 5, train loss: 0.82831, val loss: 0.82957\n",
      "Interaction tuning epoch: 6, train loss: 0.82365, val loss: 0.82830\n",
      "Interaction tuning epoch: 7, train loss: 0.82630, val loss: 0.83592\n",
      "Interaction tuning epoch: 8, train loss: 0.83153, val loss: 0.83820\n",
      "Interaction tuning epoch: 9, train loss: 0.82737, val loss: 0.82612\n",
      "Interaction tuning epoch: 10, train loss: 0.82729, val loss: 0.83492\n",
      "Interaction tuning epoch: 11, train loss: 0.82965, val loss: 0.83537\n",
      "Interaction tuning epoch: 12, train loss: 0.82513, val loss: 0.82338\n",
      "Interaction tuning epoch: 13, train loss: 0.83043, val loss: 0.83047\n",
      "Interaction tuning epoch: 14, train loss: 0.82306, val loss: 0.83068\n",
      "Interaction tuning epoch: 15, train loss: 0.82720, val loss: 0.83790\n",
      "Interaction tuning epoch: 16, train loss: 0.83419, val loss: 0.83673\n",
      "Interaction tuning epoch: 17, train loss: 0.82628, val loss: 0.82939\n",
      "Interaction tuning epoch: 18, train loss: 0.82549, val loss: 0.82675\n",
      "Interaction tuning epoch: 19, train loss: 0.82412, val loss: 0.83198\n",
      "Interaction tuning epoch: 20, train loss: 0.83917, val loss: 0.84761\n",
      "Interaction tuning epoch: 21, train loss: 0.82223, val loss: 0.82020\n",
      "Interaction tuning epoch: 22, train loss: 0.82210, val loss: 0.82982\n",
      "Interaction tuning epoch: 23, train loss: 0.82726, val loss: 0.82555\n",
      "Interaction tuning epoch: 24, train loss: 0.82889, val loss: 0.83942\n",
      "Interaction tuning epoch: 25, train loss: 0.82254, val loss: 0.82358\n",
      "Interaction tuning epoch: 26, train loss: 0.82640, val loss: 0.82870\n",
      "Interaction tuning epoch: 27, train loss: 0.82205, val loss: 0.82195\n",
      "Interaction tuning epoch: 28, train loss: 0.82325, val loss: 0.83162\n",
      "Interaction tuning epoch: 29, train loss: 0.82690, val loss: 0.83120\n",
      "Interaction tuning epoch: 30, train loss: 0.82323, val loss: 0.82703\n",
      "Interaction tuning epoch: 31, train loss: 0.82276, val loss: 0.82433\n",
      "Interaction tuning epoch: 32, train loss: 0.82314, val loss: 0.82286\n",
      "Interaction tuning epoch: 33, train loss: 0.82751, val loss: 0.83368\n",
      "Interaction tuning epoch: 34, train loss: 0.83350, val loss: 0.83617\n",
      "Interaction tuning epoch: 35, train loss: 0.82758, val loss: 0.83182\n",
      "Interaction tuning epoch: 36, train loss: 0.82320, val loss: 0.83185\n",
      "Interaction tuning epoch: 37, train loss: 0.82061, val loss: 0.81922\n",
      "Interaction tuning epoch: 38, train loss: 0.82174, val loss: 0.82799\n",
      "Interaction tuning epoch: 39, train loss: 0.83102, val loss: 0.84067\n",
      "Interaction tuning epoch: 40, train loss: 0.82378, val loss: 0.82636\n",
      "Interaction tuning epoch: 41, train loss: 0.82318, val loss: 0.82530\n",
      "Interaction tuning epoch: 42, train loss: 0.82340, val loss: 0.83091\n",
      "Interaction tuning epoch: 43, train loss: 0.82176, val loss: 0.82393\n",
      "Interaction tuning epoch: 44, train loss: 0.82324, val loss: 0.82866\n",
      "Interaction tuning epoch: 45, train loss: 0.82202, val loss: 0.82360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 46, train loss: 0.82566, val loss: 0.82865\n",
      "Interaction tuning epoch: 47, train loss: 0.82517, val loss: 0.83221\n",
      "Interaction tuning epoch: 48, train loss: 0.82308, val loss: 0.82212\n",
      "Interaction tuning epoch: 49, train loss: 0.82050, val loss: 0.82830\n",
      "Interaction tuning epoch: 50, train loss: 0.82735, val loss: 0.83815\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 39.83947467803955\n",
      "After the gam stage, training error is 0.82735 , validation error is 0.83815\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 20.939391\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.683277 validation MAE=0.791447,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.635670 validation MAE=0.770916,rank=5\n",
      "[SoftImpute] Iter 3: observed MAE=0.594202 validation MAE=0.752130,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.559786 validation MAE=0.734917,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.530498 validation MAE=0.719304,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.505208 validation MAE=0.705811,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.482858 validation MAE=0.692941,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.464463 validation MAE=0.682501,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.447900 validation MAE=0.672769,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.433584 validation MAE=0.663683,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.419737 validation MAE=0.655092,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.407234 validation MAE=0.647091,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.396704 validation MAE=0.640515,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.386493 validation MAE=0.633870,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.376944 validation MAE=0.627709,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.369890 validation MAE=0.622552,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.362898 validation MAE=0.617321,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.355870 validation MAE=0.612246,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.347184 validation MAE=0.607861,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.340597 validation MAE=0.603949,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.333776 validation MAE=0.600067,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.328002 validation MAE=0.596346,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.322573 validation MAE=0.593671,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.316690 validation MAE=0.590565,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.310606 validation MAE=0.588417,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.307421 validation MAE=0.585434,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.302064 validation MAE=0.583155,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.297415 validation MAE=0.581228,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.293771 validation MAE=0.578710,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.290355 validation MAE=0.576929,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.287122 validation MAE=0.575029,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.283438 validation MAE=0.573502,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.280468 validation MAE=0.571388,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.277666 validation MAE=0.569558,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.274653 validation MAE=0.568303,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.271788 validation MAE=0.566312,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.269719 validation MAE=0.565129,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.266705 validation MAE=0.564259,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.265004 validation MAE=0.562776,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.262355 validation MAE=0.561481,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.260108 validation MAE=0.560041,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.257552 validation MAE=0.559097,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.256711 validation MAE=0.558468,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.254695 validation MAE=0.557283,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.252565 validation MAE=0.556015,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.250446 validation MAE=0.555104,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.249088 validation MAE=0.554373,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.247770 validation MAE=0.553581,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.246127 validation MAE=0.552695,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.244858 validation MAE=0.551787,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.243264 validation MAE=0.551144,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.241800 validation MAE=0.550404,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.239683 validation MAE=0.549258,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.239651 validation MAE=0.549149,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.238374 validation MAE=0.547816,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.236349 validation MAE=0.546982,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.235383 validation MAE=0.546758,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.233923 validation MAE=0.546028,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.232296 validation MAE=0.545112,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.231816 validation MAE=0.545142,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.231466 validation MAE=0.544292,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.229745 validation MAE=0.543608,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.229124 validation MAE=0.543134,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.227439 validation MAE=0.542429,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.226629 validation MAE=0.541744,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.226456 validation MAE=0.541887,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.225218 validation MAE=0.541255,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.223847 validation MAE=0.540488,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.222990 validation MAE=0.540021,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.222589 validation MAE=0.540027,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.221225 validation MAE=0.539253,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.221095 validation MAE=0.538530,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.220451 validation MAE=0.538444,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.219568 validation MAE=0.538542,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.219032 validation MAE=0.537897,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.218253 validation MAE=0.537329,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.217223 validation MAE=0.537052,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.217434 validation MAE=0.536517,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.216289 validation MAE=0.536671,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.215632 validation MAE=0.536180,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.215025 validation MAE=0.535472,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.214046 validation MAE=0.535908,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.213334 validation MAE=0.535420,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.213115 validation MAE=0.534921,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.212306 validation MAE=0.534787,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.211938 validation MAE=0.534439,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.211667 validation MAE=0.533905,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.211042 validation MAE=0.534020,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.210706 validation MAE=0.534103,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.209890 validation MAE=0.533992,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.209989 validation MAE=0.533573,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.208578 validation MAE=0.532990,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.208597 validation MAE=0.532683,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.208294 validation MAE=0.532502,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.207223 validation MAE=0.532593,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.206838 validation MAE=0.532217,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.206844 validation MAE=0.531855,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.205901 validation MAE=0.531980,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.205285 validation MAE=0.531722,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.205360 validation MAE=0.531635,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.204448 validation MAE=0.530925,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.203604 validation MAE=0.530666,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.204039 validation MAE=0.530738,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.203933 validation MAE=0.530195,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.203438 validation MAE=0.530678,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.203304 validation MAE=0.530308,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.202216 validation MAE=0.529978,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.201880 validation MAE=0.529426,rank=5\n",
      "[SoftImpute] Iter 109: observed MAE=0.202008 validation MAE=0.529758,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.201516 validation MAE=0.529823,rank=5\n",
      "[SoftImpute] Iter 111: observed MAE=0.200697 validation MAE=0.529618,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.200823 validation MAE=0.529613,rank=5\n",
      "[SoftImpute] Iter 113: observed MAE=0.200690 validation MAE=0.529337,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 114: observed MAE=0.199620 validation MAE=0.528984,rank=5\n",
      "[SoftImpute] Iter 115: observed MAE=0.199302 validation MAE=0.529265,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.199204 validation MAE=0.528975,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.198347 validation MAE=0.528644,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.198871 validation MAE=0.528441,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.199022 validation MAE=0.528475,rank=5\n",
      "[SoftImpute] Iter 120: observed MAE=0.198930 validation MAE=0.528024,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.198496 validation MAE=0.527754,rank=5\n",
      "[SoftImpute] Iter 122: observed MAE=0.198088 validation MAE=0.527217,rank=5\n",
      "[SoftImpute] Iter 123: observed MAE=0.197399 validation MAE=0.527567,rank=5\n",
      "[SoftImpute] Iter 124: observed MAE=0.197176 validation MAE=0.527526,rank=5\n",
      "[SoftImpute] Iter 125: observed MAE=0.196509 validation MAE=0.527419,rank=5\n",
      "[SoftImpute] Iter 126: observed MAE=0.196435 validation MAE=0.527544,rank=5\n",
      "[SoftImpute] Iter 127: observed MAE=0.196178 validation MAE=0.527676,rank=5\n",
      "[SoftImpute] Iter 128: observed MAE=0.196162 validation MAE=0.527020,rank=5\n",
      "[SoftImpute] Iter 129: observed MAE=0.195780 validation MAE=0.527009,rank=5\n",
      "[SoftImpute] Iter 130: observed MAE=0.195599 validation MAE=0.527024,rank=5\n",
      "[SoftImpute] Iter 131: observed MAE=0.195254 validation MAE=0.527048,rank=5\n",
      "[SoftImpute] Iter 132: observed MAE=0.194618 validation MAE=0.527001,rank=5\n",
      "[SoftImpute] Iter 133: observed MAE=0.195040 validation MAE=0.527407,rank=5\n",
      "[SoftImpute] Iter 134: observed MAE=0.194275 validation MAE=0.526673,rank=5\n",
      "[SoftImpute] Iter 135: observed MAE=0.193721 validation MAE=0.526878,rank=5\n",
      "[SoftImpute] Iter 136: observed MAE=0.193808 validation MAE=0.527004,rank=5\n",
      "[SoftImpute] Iter 137: observed MAE=0.193692 validation MAE=0.526468,rank=5\n",
      "[SoftImpute] Iter 138: observed MAE=0.193237 validation MAE=0.526235,rank=5\n",
      "[SoftImpute] Iter 139: observed MAE=0.193791 validation MAE=0.526826,rank=5\n",
      "[SoftImpute] Iter 140: observed MAE=0.193421 validation MAE=0.526617,rank=5\n",
      "[SoftImpute] Iter 141: observed MAE=0.192777 validation MAE=0.526075,rank=5\n",
      "[SoftImpute] Iter 142: observed MAE=0.192973 validation MAE=0.526314,rank=5\n",
      "[SoftImpute] Iter 143: observed MAE=0.192158 validation MAE=0.525895,rank=5\n",
      "[SoftImpute] Iter 144: observed MAE=0.192162 validation MAE=0.525800,rank=5\n",
      "[SoftImpute] Iter 145: observed MAE=0.192175 validation MAE=0.525967,rank=5\n",
      "[SoftImpute] Iter 146: observed MAE=0.191385 validation MAE=0.525834,rank=5\n",
      "[SoftImpute] Iter 147: observed MAE=0.191585 validation MAE=0.526005,rank=5\n",
      "[SoftImpute] Iter 148: observed MAE=0.191908 validation MAE=0.525569,rank=5\n",
      "[SoftImpute] Iter 149: observed MAE=0.191522 validation MAE=0.525637,rank=5\n",
      "[SoftImpute] Iter 150: observed MAE=0.191639 validation MAE=0.525492,rank=5\n",
      "[SoftImpute] Iter 151: observed MAE=0.191229 validation MAE=0.525542,rank=5\n",
      "[SoftImpute] Iter 152: observed MAE=0.190812 validation MAE=0.524988,rank=5\n",
      "[SoftImpute] Iter 153: observed MAE=0.190152 validation MAE=0.524629,rank=5\n",
      "[SoftImpute] Iter 154: observed MAE=0.190347 validation MAE=0.525412,rank=5\n",
      "[SoftImpute] Iter 155: observed MAE=0.190062 validation MAE=0.525529,rank=5\n",
      "[SoftImpute] Iter 156: observed MAE=0.189889 validation MAE=0.525083,rank=5\n",
      "[SoftImpute] Iter 157: observed MAE=0.190237 validation MAE=0.525230,rank=5\n",
      "[SoftImpute] Iter 158: observed MAE=0.189462 validation MAE=0.524775,rank=5\n",
      "[SoftImpute] Iter 159: observed MAE=0.188701 validation MAE=0.524895,rank=5\n",
      "[SoftImpute] Iter 160: observed MAE=0.189183 validation MAE=0.524568,rank=5\n",
      "[SoftImpute] Iter 161: observed MAE=0.188783 validation MAE=0.524788,rank=5\n",
      "[SoftImpute] Iter 162: observed MAE=0.188327 validation MAE=0.524873,rank=5\n",
      "[SoftImpute] Iter 163: observed MAE=0.188051 validation MAE=0.524999,rank=5\n",
      "[SoftImpute] Stopped after iteration 163 for lambda=0.418788\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 5.044107437133789\n",
      "After the matrix factor stage, training error is 0.18805, validation error is 0.52500\n",
      "5\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.05710, val loss: 4.18755\n",
      "Main effects training epoch: 2, train loss: 3.92707, val loss: 4.07069\n",
      "Main effects training epoch: 3, train loss: 3.76768, val loss: 3.90911\n",
      "Main effects training epoch: 4, train loss: 3.50250, val loss: 3.64626\n",
      "Main effects training epoch: 5, train loss: 3.40024, val loss: 3.51334\n",
      "Main effects training epoch: 6, train loss: 3.29254, val loss: 3.36501\n",
      "Main effects training epoch: 7, train loss: 3.26227, val loss: 3.33327\n",
      "Main effects training epoch: 8, train loss: 3.27378, val loss: 3.36541\n",
      "Main effects training epoch: 9, train loss: 3.20641, val loss: 3.30237\n",
      "Main effects training epoch: 10, train loss: 3.18522, val loss: 3.27339\n",
      "Main effects training epoch: 11, train loss: 3.05619, val loss: 3.13718\n",
      "Main effects training epoch: 12, train loss: 3.04834, val loss: 3.14212\n",
      "Main effects training epoch: 13, train loss: 2.97396, val loss: 3.06152\n",
      "Main effects training epoch: 14, train loss: 2.97759, val loss: 3.07033\n",
      "Main effects training epoch: 15, train loss: 2.93633, val loss: 3.02826\n",
      "Main effects training epoch: 16, train loss: 2.87702, val loss: 2.97413\n",
      "Main effects training epoch: 17, train loss: 2.83196, val loss: 2.92631\n",
      "Main effects training epoch: 18, train loss: 2.78017, val loss: 2.87843\n",
      "Main effects training epoch: 19, train loss: 2.71702, val loss: 2.81418\n",
      "Main effects training epoch: 20, train loss: 2.71047, val loss: 2.81147\n",
      "Main effects training epoch: 21, train loss: 2.63816, val loss: 2.74423\n",
      "Main effects training epoch: 22, train loss: 2.57322, val loss: 2.67689\n",
      "Main effects training epoch: 23, train loss: 2.53302, val loss: 2.64859\n",
      "Main effects training epoch: 24, train loss: 2.47403, val loss: 2.59068\n",
      "Main effects training epoch: 25, train loss: 2.45648, val loss: 2.57184\n",
      "Main effects training epoch: 26, train loss: 2.40477, val loss: 2.51878\n",
      "Main effects training epoch: 27, train loss: 2.37876, val loss: 2.49509\n",
      "Main effects training epoch: 28, train loss: 2.32441, val loss: 2.44698\n",
      "Main effects training epoch: 29, train loss: 2.29649, val loss: 2.41378\n",
      "Main effects training epoch: 30, train loss: 2.27831, val loss: 2.39431\n",
      "Main effects training epoch: 31, train loss: 2.24647, val loss: 2.35997\n",
      "Main effects training epoch: 32, train loss: 2.23018, val loss: 2.34869\n",
      "Main effects training epoch: 33, train loss: 2.20211, val loss: 2.32035\n",
      "Main effects training epoch: 34, train loss: 2.17312, val loss: 2.29052\n",
      "Main effects training epoch: 35, train loss: 2.13601, val loss: 2.24620\n",
      "Main effects training epoch: 36, train loss: 2.11724, val loss: 2.23193\n",
      "Main effects training epoch: 37, train loss: 2.06714, val loss: 2.16614\n",
      "Main effects training epoch: 38, train loss: 2.02109, val loss: 2.11913\n",
      "Main effects training epoch: 39, train loss: 2.02850, val loss: 2.11507\n",
      "Main effects training epoch: 40, train loss: 1.97092, val loss: 2.05392\n",
      "Main effects training epoch: 41, train loss: 1.95160, val loss: 2.02347\n",
      "Main effects training epoch: 42, train loss: 1.94570, val loss: 2.01501\n",
      "Main effects training epoch: 43, train loss: 1.93383, val loss: 2.00885\n",
      "Main effects training epoch: 44, train loss: 1.91764, val loss: 2.00237\n",
      "Main effects training epoch: 45, train loss: 1.90924, val loss: 1.97639\n",
      "Main effects training epoch: 46, train loss: 1.89907, val loss: 1.97702\n",
      "Main effects training epoch: 47, train loss: 1.88470, val loss: 1.95250\n",
      "Main effects training epoch: 48, train loss: 1.87831, val loss: 1.94964\n",
      "Main effects training epoch: 49, train loss: 1.85647, val loss: 1.92063\n",
      "Main effects training epoch: 50, train loss: 1.85260, val loss: 1.91837\n",
      "Main effects training epoch: 51, train loss: 1.83567, val loss: 1.90244\n",
      "Main effects training epoch: 52, train loss: 1.83691, val loss: 1.90274\n",
      "Main effects training epoch: 53, train loss: 1.81280, val loss: 1.87424\n",
      "Main effects training epoch: 54, train loss: 1.81427, val loss: 1.87426\n",
      "Main effects training epoch: 55, train loss: 1.80387, val loss: 1.86822\n",
      "Main effects training epoch: 56, train loss: 1.80147, val loss: 1.86058\n",
      "Main effects training epoch: 57, train loss: 1.79206, val loss: 1.86027\n",
      "Main effects training epoch: 58, train loss: 1.77546, val loss: 1.82719\n",
      "Main effects training epoch: 59, train loss: 1.78815, val loss: 1.85453\n",
      "Main effects training epoch: 60, train loss: 1.77844, val loss: 1.83996\n",
      "Main effects training epoch: 61, train loss: 1.76170, val loss: 1.81744\n",
      "Main effects training epoch: 62, train loss: 1.75959, val loss: 1.82286\n",
      "Main effects training epoch: 63, train loss: 1.75750, val loss: 1.81371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 64, train loss: 1.75860, val loss: 1.82026\n",
      "Main effects training epoch: 65, train loss: 1.75056, val loss: 1.80610\n",
      "Main effects training epoch: 66, train loss: 1.74767, val loss: 1.80531\n",
      "Main effects training epoch: 67, train loss: 1.74060, val loss: 1.79794\n",
      "Main effects training epoch: 68, train loss: 1.74525, val loss: 1.79848\n",
      "Main effects training epoch: 69, train loss: 1.73679, val loss: 1.79222\n",
      "Main effects training epoch: 70, train loss: 1.74305, val loss: 1.79358\n",
      "Main effects training epoch: 71, train loss: 1.73393, val loss: 1.79120\n",
      "Main effects training epoch: 72, train loss: 1.72970, val loss: 1.78347\n",
      "Main effects training epoch: 73, train loss: 1.72728, val loss: 1.77467\n",
      "Main effects training epoch: 74, train loss: 1.73267, val loss: 1.78905\n",
      "Main effects training epoch: 75, train loss: 1.72245, val loss: 1.76778\n",
      "Main effects training epoch: 76, train loss: 1.72328, val loss: 1.77906\n",
      "Main effects training epoch: 77, train loss: 1.72055, val loss: 1.76825\n",
      "Main effects training epoch: 78, train loss: 1.72070, val loss: 1.77382\n",
      "Main effects training epoch: 79, train loss: 1.71676, val loss: 1.76619\n",
      "Main effects training epoch: 80, train loss: 1.71385, val loss: 1.76665\n",
      "Main effects training epoch: 81, train loss: 1.71213, val loss: 1.76439\n",
      "Main effects training epoch: 82, train loss: 1.71363, val loss: 1.76271\n",
      "Main effects training epoch: 83, train loss: 1.71206, val loss: 1.76507\n",
      "Main effects training epoch: 84, train loss: 1.70887, val loss: 1.75568\n",
      "Main effects training epoch: 85, train loss: 1.70782, val loss: 1.76169\n",
      "Main effects training epoch: 86, train loss: 1.70938, val loss: 1.75567\n",
      "Main effects training epoch: 87, train loss: 1.71151, val loss: 1.76680\n",
      "Main effects training epoch: 88, train loss: 1.70351, val loss: 1.74850\n",
      "Main effects training epoch: 89, train loss: 1.70232, val loss: 1.75537\n",
      "Main effects training epoch: 90, train loss: 1.70374, val loss: 1.75442\n",
      "Main effects training epoch: 91, train loss: 1.70308, val loss: 1.75007\n",
      "Main effects training epoch: 92, train loss: 1.70294, val loss: 1.75676\n",
      "Main effects training epoch: 93, train loss: 1.70063, val loss: 1.74989\n",
      "Main effects training epoch: 94, train loss: 1.69903, val loss: 1.75266\n",
      "Main effects training epoch: 95, train loss: 1.69475, val loss: 1.74409\n",
      "Main effects training epoch: 96, train loss: 1.69444, val loss: 1.74520\n",
      "Main effects training epoch: 97, train loss: 1.69258, val loss: 1.74521\n",
      "Main effects training epoch: 98, train loss: 1.69301, val loss: 1.74354\n",
      "Main effects training epoch: 99, train loss: 1.68626, val loss: 1.73941\n",
      "Main effects training epoch: 100, train loss: 1.68236, val loss: 1.72935\n",
      "Main effects training epoch: 101, train loss: 1.68294, val loss: 1.73840\n",
      "Main effects training epoch: 102, train loss: 1.67649, val loss: 1.72304\n",
      "Main effects training epoch: 103, train loss: 1.67555, val loss: 1.73371\n",
      "Main effects training epoch: 104, train loss: 1.66799, val loss: 1.71888\n",
      "Main effects training epoch: 105, train loss: 1.66659, val loss: 1.72733\n",
      "Main effects training epoch: 106, train loss: 1.65691, val loss: 1.70762\n",
      "Main effects training epoch: 107, train loss: 1.65777, val loss: 1.71814\n",
      "Main effects training epoch: 108, train loss: 1.65238, val loss: 1.71792\n",
      "Main effects training epoch: 109, train loss: 1.64222, val loss: 1.70436\n",
      "Main effects training epoch: 110, train loss: 1.63905, val loss: 1.70845\n",
      "Main effects training epoch: 111, train loss: 1.63743, val loss: 1.69729\n",
      "Main effects training epoch: 112, train loss: 1.63113, val loss: 1.70543\n",
      "Main effects training epoch: 113, train loss: 1.62880, val loss: 1.69922\n",
      "Main effects training epoch: 114, train loss: 1.62680, val loss: 1.70045\n",
      "Main effects training epoch: 115, train loss: 1.62836, val loss: 1.69526\n",
      "Main effects training epoch: 116, train loss: 1.62297, val loss: 1.68856\n",
      "Main effects training epoch: 117, train loss: 1.62301, val loss: 1.70098\n",
      "Main effects training epoch: 118, train loss: 1.62834, val loss: 1.69378\n",
      "Main effects training epoch: 119, train loss: 1.63193, val loss: 1.70481\n",
      "Main effects training epoch: 120, train loss: 1.62154, val loss: 1.69656\n",
      "Main effects training epoch: 121, train loss: 1.62442, val loss: 1.70362\n",
      "Main effects training epoch: 122, train loss: 1.62027, val loss: 1.69658\n",
      "Main effects training epoch: 123, train loss: 1.61625, val loss: 1.70113\n",
      "Main effects training epoch: 124, train loss: 1.61805, val loss: 1.69211\n",
      "Main effects training epoch: 125, train loss: 1.61200, val loss: 1.69097\n",
      "Main effects training epoch: 126, train loss: 1.61387, val loss: 1.70292\n",
      "Main effects training epoch: 127, train loss: 1.61836, val loss: 1.69611\n",
      "Main effects training epoch: 128, train loss: 1.61339, val loss: 1.69241\n",
      "Main effects training epoch: 129, train loss: 1.61077, val loss: 1.69666\n",
      "Main effects training epoch: 130, train loss: 1.61114, val loss: 1.69384\n",
      "Main effects training epoch: 131, train loss: 1.60802, val loss: 1.69228\n",
      "Main effects training epoch: 132, train loss: 1.62021, val loss: 1.69291\n",
      "Main effects training epoch: 133, train loss: 1.61234, val loss: 1.70381\n",
      "Main effects training epoch: 134, train loss: 1.61363, val loss: 1.69987\n",
      "Main effects training epoch: 135, train loss: 1.60673, val loss: 1.69494\n",
      "Main effects training epoch: 136, train loss: 1.60381, val loss: 1.69199\n",
      "Main effects training epoch: 137, train loss: 1.61049, val loss: 1.69428\n",
      "Main effects training epoch: 138, train loss: 1.60393, val loss: 1.69109\n",
      "Main effects training epoch: 139, train loss: 1.60262, val loss: 1.68695\n",
      "Main effects training epoch: 140, train loss: 1.60110, val loss: 1.68777\n",
      "Main effects training epoch: 141, train loss: 1.59972, val loss: 1.68200\n",
      "Main effects training epoch: 142, train loss: 1.60469, val loss: 1.68760\n",
      "Main effects training epoch: 143, train loss: 1.60330, val loss: 1.69541\n",
      "Main effects training epoch: 144, train loss: 1.60226, val loss: 1.68589\n",
      "Main effects training epoch: 145, train loss: 1.60316, val loss: 1.69319\n",
      "Main effects training epoch: 146, train loss: 1.60175, val loss: 1.68014\n",
      "Main effects training epoch: 147, train loss: 1.59556, val loss: 1.68876\n",
      "Main effects training epoch: 148, train loss: 1.59445, val loss: 1.68014\n",
      "Main effects training epoch: 149, train loss: 1.59294, val loss: 1.67697\n",
      "Main effects training epoch: 150, train loss: 1.60435, val loss: 1.68894\n",
      "Main effects training epoch: 151, train loss: 1.59260, val loss: 1.68437\n",
      "Main effects training epoch: 152, train loss: 1.59950, val loss: 1.68939\n",
      "Main effects training epoch: 153, train loss: 1.59251, val loss: 1.67455\n",
      "Main effects training epoch: 154, train loss: 1.59500, val loss: 1.68203\n",
      "Main effects training epoch: 155, train loss: 1.59102, val loss: 1.67551\n",
      "Main effects training epoch: 156, train loss: 1.59445, val loss: 1.68543\n",
      "Main effects training epoch: 157, train loss: 1.58741, val loss: 1.67243\n",
      "Main effects training epoch: 158, train loss: 1.58363, val loss: 1.67297\n",
      "Main effects training epoch: 159, train loss: 1.58496, val loss: 1.67288\n",
      "Main effects training epoch: 160, train loss: 1.58458, val loss: 1.67756\n",
      "Main effects training epoch: 161, train loss: 1.58619, val loss: 1.67475\n",
      "Main effects training epoch: 162, train loss: 1.58634, val loss: 1.67239\n",
      "Main effects training epoch: 163, train loss: 1.58061, val loss: 1.66992\n",
      "Main effects training epoch: 164, train loss: 1.58002, val loss: 1.66916\n",
      "Main effects training epoch: 165, train loss: 1.58086, val loss: 1.66099\n",
      "Main effects training epoch: 166, train loss: 1.57773, val loss: 1.67114\n",
      "Main effects training epoch: 167, train loss: 1.57591, val loss: 1.66321\n",
      "Main effects training epoch: 168, train loss: 1.58343, val loss: 1.67205\n",
      "Main effects training epoch: 169, train loss: 1.58627, val loss: 1.66799\n",
      "Main effects training epoch: 170, train loss: 1.57834, val loss: 1.66211\n",
      "Main effects training epoch: 171, train loss: 1.58075, val loss: 1.65932\n",
      "Main effects training epoch: 172, train loss: 1.58831, val loss: 1.68622\n",
      "Main effects training epoch: 173, train loss: 1.57398, val loss: 1.65452\n",
      "Main effects training epoch: 174, train loss: 1.57428, val loss: 1.66348\n",
      "Main effects training epoch: 175, train loss: 1.56893, val loss: 1.65514\n",
      "Main effects training epoch: 176, train loss: 1.57241, val loss: 1.66808\n",
      "Main effects training epoch: 177, train loss: 1.57174, val loss: 1.65373\n",
      "Main effects training epoch: 178, train loss: 1.56857, val loss: 1.65172\n",
      "Main effects training epoch: 179, train loss: 1.56726, val loss: 1.65857\n",
      "Main effects training epoch: 180, train loss: 1.56440, val loss: 1.65056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 181, train loss: 1.57073, val loss: 1.65133\n",
      "Main effects training epoch: 182, train loss: 1.57376, val loss: 1.66299\n",
      "Main effects training epoch: 183, train loss: 1.56749, val loss: 1.65098\n",
      "Main effects training epoch: 184, train loss: 1.56730, val loss: 1.64367\n",
      "Main effects training epoch: 185, train loss: 1.56349, val loss: 1.65272\n",
      "Main effects training epoch: 186, train loss: 1.55947, val loss: 1.64078\n",
      "Main effects training epoch: 187, train loss: 1.56212, val loss: 1.65277\n",
      "Main effects training epoch: 188, train loss: 1.56638, val loss: 1.64955\n",
      "Main effects training epoch: 189, train loss: 1.56501, val loss: 1.64640\n",
      "Main effects training epoch: 190, train loss: 1.56136, val loss: 1.65289\n",
      "Main effects training epoch: 191, train loss: 1.56197, val loss: 1.64449\n",
      "Main effects training epoch: 192, train loss: 1.57106, val loss: 1.65922\n",
      "Main effects training epoch: 193, train loss: 1.56347, val loss: 1.64005\n",
      "Main effects training epoch: 194, train loss: 1.55972, val loss: 1.64309\n",
      "Main effects training epoch: 195, train loss: 1.56068, val loss: 1.65590\n",
      "Main effects training epoch: 196, train loss: 1.55612, val loss: 1.63892\n",
      "Main effects training epoch: 197, train loss: 1.55883, val loss: 1.64737\n",
      "Main effects training epoch: 198, train loss: 1.55456, val loss: 1.63697\n",
      "Main effects training epoch: 199, train loss: 1.55593, val loss: 1.63759\n",
      "Main effects training epoch: 200, train loss: 1.55430, val loss: 1.64349\n",
      "Main effects training epoch: 201, train loss: 1.55509, val loss: 1.63072\n",
      "Main effects training epoch: 202, train loss: 1.55366, val loss: 1.64275\n",
      "Main effects training epoch: 203, train loss: 1.55535, val loss: 1.63405\n",
      "Main effects training epoch: 204, train loss: 1.55732, val loss: 1.64685\n",
      "Main effects training epoch: 205, train loss: 1.55606, val loss: 1.63311\n",
      "Main effects training epoch: 206, train loss: 1.55723, val loss: 1.64528\n",
      "Main effects training epoch: 207, train loss: 1.55986, val loss: 1.64355\n",
      "Main effects training epoch: 208, train loss: 1.55327, val loss: 1.63638\n",
      "Main effects training epoch: 209, train loss: 1.55113, val loss: 1.63602\n",
      "Main effects training epoch: 210, train loss: 1.55210, val loss: 1.62717\n",
      "Main effects training epoch: 211, train loss: 1.55164, val loss: 1.63904\n",
      "Main effects training epoch: 212, train loss: 1.55023, val loss: 1.63000\n",
      "Main effects training epoch: 213, train loss: 1.54781, val loss: 1.63088\n",
      "Main effects training epoch: 214, train loss: 1.55459, val loss: 1.64171\n",
      "Main effects training epoch: 215, train loss: 1.55526, val loss: 1.63675\n",
      "Main effects training epoch: 216, train loss: 1.54694, val loss: 1.62429\n",
      "Main effects training epoch: 217, train loss: 1.54509, val loss: 1.62833\n",
      "Main effects training epoch: 218, train loss: 1.54426, val loss: 1.62066\n",
      "Main effects training epoch: 219, train loss: 1.54355, val loss: 1.62328\n",
      "Main effects training epoch: 220, train loss: 1.54315, val loss: 1.62318\n",
      "Main effects training epoch: 221, train loss: 1.54414, val loss: 1.62803\n",
      "Main effects training epoch: 222, train loss: 1.54747, val loss: 1.63281\n",
      "Main effects training epoch: 223, train loss: 1.54431, val loss: 1.63088\n",
      "Main effects training epoch: 224, train loss: 1.54537, val loss: 1.61135\n",
      "Main effects training epoch: 225, train loss: 1.54328, val loss: 1.63289\n",
      "Main effects training epoch: 226, train loss: 1.54393, val loss: 1.61494\n",
      "Main effects training epoch: 227, train loss: 1.53988, val loss: 1.62527\n",
      "Main effects training epoch: 228, train loss: 1.54869, val loss: 1.61648\n",
      "Main effects training epoch: 229, train loss: 1.54732, val loss: 1.64098\n",
      "Main effects training epoch: 230, train loss: 1.54690, val loss: 1.62891\n",
      "Main effects training epoch: 231, train loss: 1.55060, val loss: 1.62663\n",
      "Main effects training epoch: 232, train loss: 1.54671, val loss: 1.62763\n",
      "Main effects training epoch: 233, train loss: 1.54787, val loss: 1.63089\n",
      "Main effects training epoch: 234, train loss: 1.54631, val loss: 1.62564\n",
      "Main effects training epoch: 235, train loss: 1.53642, val loss: 1.61341\n",
      "Main effects training epoch: 236, train loss: 1.53631, val loss: 1.61877\n",
      "Main effects training epoch: 237, train loss: 1.53795, val loss: 1.61683\n",
      "Main effects training epoch: 238, train loss: 1.54071, val loss: 1.62040\n",
      "Main effects training epoch: 239, train loss: 1.53417, val loss: 1.60996\n",
      "Main effects training epoch: 240, train loss: 1.54131, val loss: 1.60484\n",
      "Main effects training epoch: 241, train loss: 1.54657, val loss: 1.63429\n",
      "Main effects training epoch: 242, train loss: 1.55233, val loss: 1.61318\n",
      "Main effects training epoch: 243, train loss: 1.53885, val loss: 1.63119\n",
      "Main effects training epoch: 244, train loss: 1.53930, val loss: 1.61343\n",
      "Main effects training epoch: 245, train loss: 1.53652, val loss: 1.61762\n",
      "Main effects training epoch: 246, train loss: 1.53586, val loss: 1.61222\n",
      "Main effects training epoch: 247, train loss: 1.53350, val loss: 1.62193\n",
      "Main effects training epoch: 248, train loss: 1.53131, val loss: 1.60201\n",
      "Main effects training epoch: 249, train loss: 1.53984, val loss: 1.60901\n",
      "Main effects training epoch: 250, train loss: 1.53480, val loss: 1.62149\n",
      "Main effects training epoch: 251, train loss: 1.53534, val loss: 1.61309\n",
      "Main effects training epoch: 252, train loss: 1.54386, val loss: 1.62780\n",
      "Main effects training epoch: 253, train loss: 1.53627, val loss: 1.61491\n",
      "Main effects training epoch: 254, train loss: 1.54652, val loss: 1.62068\n",
      "Main effects training epoch: 255, train loss: 1.53605, val loss: 1.61552\n",
      "Main effects training epoch: 256, train loss: 1.53088, val loss: 1.61185\n",
      "Main effects training epoch: 257, train loss: 1.53316, val loss: 1.61228\n",
      "Main effects training epoch: 258, train loss: 1.53147, val loss: 1.59926\n",
      "Main effects training epoch: 259, train loss: 1.52949, val loss: 1.61390\n",
      "Main effects training epoch: 260, train loss: 1.52645, val loss: 1.59801\n",
      "Main effects training epoch: 261, train loss: 1.52514, val loss: 1.59654\n",
      "Main effects training epoch: 262, train loss: 1.52608, val loss: 1.60221\n",
      "Main effects training epoch: 263, train loss: 1.52751, val loss: 1.59507\n",
      "Main effects training epoch: 264, train loss: 1.52225, val loss: 1.60240\n",
      "Main effects training epoch: 265, train loss: 1.52016, val loss: 1.60048\n",
      "Main effects training epoch: 266, train loss: 1.52030, val loss: 1.59629\n",
      "Main effects training epoch: 267, train loss: 1.52776, val loss: 1.59700\n",
      "Main effects training epoch: 268, train loss: 1.53515, val loss: 1.59683\n",
      "Main effects training epoch: 269, train loss: 1.51975, val loss: 1.58832\n",
      "Main effects training epoch: 270, train loss: 1.53344, val loss: 1.62923\n",
      "Main effects training epoch: 271, train loss: 1.53175, val loss: 1.59757\n",
      "Main effects training epoch: 272, train loss: 1.52854, val loss: 1.61529\n",
      "Main effects training epoch: 273, train loss: 1.52678, val loss: 1.59074\n",
      "Main effects training epoch: 274, train loss: 1.52175, val loss: 1.60291\n",
      "Main effects training epoch: 275, train loss: 1.52424, val loss: 1.59232\n",
      "Main effects training epoch: 276, train loss: 1.51843, val loss: 1.59820\n",
      "Main effects training epoch: 277, train loss: 1.51707, val loss: 1.58782\n",
      "Main effects training epoch: 278, train loss: 1.51876, val loss: 1.59286\n",
      "Main effects training epoch: 279, train loss: 1.51422, val loss: 1.59174\n",
      "Main effects training epoch: 280, train loss: 1.51510, val loss: 1.58208\n",
      "Main effects training epoch: 281, train loss: 1.52386, val loss: 1.60844\n",
      "Main effects training epoch: 282, train loss: 1.51856, val loss: 1.58539\n",
      "Main effects training epoch: 283, train loss: 1.51715, val loss: 1.60290\n",
      "Main effects training epoch: 284, train loss: 1.52164, val loss: 1.58089\n",
      "Main effects training epoch: 285, train loss: 1.52716, val loss: 1.61627\n",
      "Main effects training epoch: 286, train loss: 1.51797, val loss: 1.58869\n",
      "Main effects training epoch: 287, train loss: 1.51573, val loss: 1.59141\n",
      "Main effects training epoch: 288, train loss: 1.51905, val loss: 1.59260\n",
      "Main effects training epoch: 289, train loss: 1.52332, val loss: 1.60838\n",
      "Main effects training epoch: 290, train loss: 1.51595, val loss: 1.58615\n",
      "Main effects training epoch: 291, train loss: 1.52305, val loss: 1.59030\n",
      "Main effects training epoch: 292, train loss: 1.52735, val loss: 1.59508\n",
      "Main effects training epoch: 293, train loss: 1.51267, val loss: 1.58405\n",
      "Main effects training epoch: 294, train loss: 1.51145, val loss: 1.57384\n",
      "Main effects training epoch: 295, train loss: 1.51097, val loss: 1.58160\n",
      "Main effects training epoch: 296, train loss: 1.51364, val loss: 1.60240\n",
      "Main effects training epoch: 297, train loss: 1.51450, val loss: 1.58627\n",
      "Main effects training epoch: 298, train loss: 1.50963, val loss: 1.58348\n",
      "Main effects training epoch: 299, train loss: 1.50775, val loss: 1.57467\n",
      "Main effects training epoch: 300, train loss: 1.50603, val loss: 1.56920\n",
      "##########Stage 1: main effect training stop.##########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.52664, val loss: 1.58586\n",
      "Main effects tuning epoch: 2, train loss: 1.52736, val loss: 1.58588\n",
      "Main effects tuning epoch: 3, train loss: 1.52426, val loss: 1.58439\n",
      "Main effects tuning epoch: 4, train loss: 1.52970, val loss: 1.58967\n",
      "Main effects tuning epoch: 5, train loss: 1.53919, val loss: 1.59692\n",
      "Main effects tuning epoch: 6, train loss: 1.53680, val loss: 1.60333\n",
      "Main effects tuning epoch: 7, train loss: 1.52492, val loss: 1.58169\n",
      "Main effects tuning epoch: 8, train loss: 1.52973, val loss: 1.59282\n",
      "Main effects tuning epoch: 9, train loss: 1.52948, val loss: 1.59111\n",
      "Main effects tuning epoch: 10, train loss: 1.52940, val loss: 1.59791\n",
      "Main effects tuning epoch: 11, train loss: 1.52353, val loss: 1.58829\n",
      "Main effects tuning epoch: 12, train loss: 1.52157, val loss: 1.57666\n",
      "Main effects tuning epoch: 13, train loss: 1.51779, val loss: 1.58040\n",
      "Main effects tuning epoch: 14, train loss: 1.53912, val loss: 1.61323\n",
      "Main effects tuning epoch: 15, train loss: 1.52431, val loss: 1.57922\n",
      "Main effects tuning epoch: 16, train loss: 1.52714, val loss: 1.58119\n",
      "Main effects tuning epoch: 17, train loss: 1.51930, val loss: 1.58442\n",
      "Main effects tuning epoch: 18, train loss: 1.51696, val loss: 1.57777\n",
      "Main effects tuning epoch: 19, train loss: 1.51703, val loss: 1.58279\n",
      "Main effects tuning epoch: 20, train loss: 1.51884, val loss: 1.57535\n",
      "Main effects tuning epoch: 21, train loss: 1.52350, val loss: 1.58670\n",
      "Main effects tuning epoch: 22, train loss: 1.51442, val loss: 1.56524\n",
      "Main effects tuning epoch: 23, train loss: 1.52024, val loss: 1.59029\n",
      "Main effects tuning epoch: 24, train loss: 1.52991, val loss: 1.59931\n",
      "Main effects tuning epoch: 25, train loss: 1.52456, val loss: 1.57417\n",
      "Main effects tuning epoch: 26, train loss: 1.53880, val loss: 1.57965\n",
      "Main effects tuning epoch: 27, train loss: 1.51409, val loss: 1.57541\n",
      "Main effects tuning epoch: 28, train loss: 1.52356, val loss: 1.59410\n",
      "Main effects tuning epoch: 29, train loss: 1.52356, val loss: 1.57132\n",
      "Main effects tuning epoch: 30, train loss: 1.51506, val loss: 1.56816\n",
      "Main effects tuning epoch: 31, train loss: 1.51346, val loss: 1.57494\n",
      "Main effects tuning epoch: 32, train loss: 1.51108, val loss: 1.57531\n",
      "Main effects tuning epoch: 33, train loss: 1.51096, val loss: 1.56776\n",
      "Main effects tuning epoch: 34, train loss: 1.52252, val loss: 1.57155\n",
      "Main effects tuning epoch: 35, train loss: 1.51386, val loss: 1.58034\n",
      "Main effects tuning epoch: 36, train loss: 1.51620, val loss: 1.57377\n",
      "Main effects tuning epoch: 37, train loss: 1.51298, val loss: 1.57481\n",
      "Main effects tuning epoch: 38, train loss: 1.51163, val loss: 1.58295\n",
      "Main effects tuning epoch: 39, train loss: 1.50821, val loss: 1.55790\n",
      "Main effects tuning epoch: 40, train loss: 1.50709, val loss: 1.57228\n",
      "Main effects tuning epoch: 41, train loss: 1.50921, val loss: 1.55676\n",
      "Main effects tuning epoch: 42, train loss: 1.50827, val loss: 1.57774\n",
      "Main effects tuning epoch: 43, train loss: 1.51360, val loss: 1.57885\n",
      "Main effects tuning epoch: 44, train loss: 1.51090, val loss: 1.56180\n",
      "Main effects tuning epoch: 45, train loss: 1.50737, val loss: 1.56380\n",
      "Main effects tuning epoch: 46, train loss: 1.51318, val loss: 1.57271\n",
      "Main effects tuning epoch: 47, train loss: 1.50781, val loss: 1.58030\n",
      "Main effects tuning epoch: 48, train loss: 1.50717, val loss: 1.56697\n",
      "Main effects tuning epoch: 49, train loss: 1.51250, val loss: 1.56836\n",
      "Main effects tuning epoch: 50, train loss: 1.50430, val loss: 1.56216\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.44438, val loss: 1.47901\n",
      "Interaction training epoch: 2, train loss: 1.19369, val loss: 1.19856\n",
      "Interaction training epoch: 3, train loss: 1.09632, val loss: 1.09062\n",
      "Interaction training epoch: 4, train loss: 1.06766, val loss: 1.04762\n",
      "Interaction training epoch: 5, train loss: 1.02914, val loss: 1.01305\n",
      "Interaction training epoch: 6, train loss: 1.01591, val loss: 1.00878\n",
      "Interaction training epoch: 7, train loss: 1.06358, val loss: 1.06191\n",
      "Interaction training epoch: 8, train loss: 1.03615, val loss: 1.03415\n",
      "Interaction training epoch: 9, train loss: 0.99194, val loss: 0.98152\n",
      "Interaction training epoch: 10, train loss: 0.98470, val loss: 0.98903\n",
      "Interaction training epoch: 11, train loss: 0.95819, val loss: 0.96008\n",
      "Interaction training epoch: 12, train loss: 0.97185, val loss: 0.96465\n",
      "Interaction training epoch: 13, train loss: 0.94185, val loss: 0.94742\n",
      "Interaction training epoch: 14, train loss: 0.93390, val loss: 0.93863\n",
      "Interaction training epoch: 15, train loss: 0.91949, val loss: 0.92695\n",
      "Interaction training epoch: 16, train loss: 0.91512, val loss: 0.92203\n",
      "Interaction training epoch: 17, train loss: 0.91363, val loss: 0.92388\n",
      "Interaction training epoch: 18, train loss: 0.91322, val loss: 0.92192\n",
      "Interaction training epoch: 19, train loss: 0.91889, val loss: 0.91689\n",
      "Interaction training epoch: 20, train loss: 0.91030, val loss: 0.90976\n",
      "Interaction training epoch: 21, train loss: 0.90212, val loss: 0.90105\n",
      "Interaction training epoch: 22, train loss: 0.91216, val loss: 0.91107\n",
      "Interaction training epoch: 23, train loss: 0.91788, val loss: 0.91065\n",
      "Interaction training epoch: 24, train loss: 0.90936, val loss: 0.90986\n",
      "Interaction training epoch: 25, train loss: 0.89969, val loss: 0.89557\n",
      "Interaction training epoch: 26, train loss: 0.91396, val loss: 0.91089\n",
      "Interaction training epoch: 27, train loss: 0.89939, val loss: 0.90426\n",
      "Interaction training epoch: 28, train loss: 0.90530, val loss: 0.89862\n",
      "Interaction training epoch: 29, train loss: 0.89282, val loss: 0.88714\n",
      "Interaction training epoch: 30, train loss: 0.90134, val loss: 0.89129\n",
      "Interaction training epoch: 31, train loss: 0.89143, val loss: 0.89373\n",
      "Interaction training epoch: 32, train loss: 0.88697, val loss: 0.88219\n",
      "Interaction training epoch: 33, train loss: 0.87656, val loss: 0.87256\n",
      "Interaction training epoch: 34, train loss: 0.88738, val loss: 0.88062\n",
      "Interaction training epoch: 35, train loss: 0.88361, val loss: 0.87946\n",
      "Interaction training epoch: 36, train loss: 0.88244, val loss: 0.87375\n",
      "Interaction training epoch: 37, train loss: 0.88481, val loss: 0.88786\n",
      "Interaction training epoch: 38, train loss: 0.86898, val loss: 0.86348\n",
      "Interaction training epoch: 39, train loss: 0.87102, val loss: 0.86146\n",
      "Interaction training epoch: 40, train loss: 0.86584, val loss: 0.85518\n",
      "Interaction training epoch: 41, train loss: 0.92499, val loss: 0.92809\n",
      "Interaction training epoch: 42, train loss: 0.86838, val loss: 0.85576\n",
      "Interaction training epoch: 43, train loss: 0.88460, val loss: 0.86580\n",
      "Interaction training epoch: 44, train loss: 0.88491, val loss: 0.88168\n",
      "Interaction training epoch: 45, train loss: 0.86966, val loss: 0.87189\n",
      "Interaction training epoch: 46, train loss: 0.85728, val loss: 0.84880\n",
      "Interaction training epoch: 47, train loss: 0.86010, val loss: 0.85503\n",
      "Interaction training epoch: 48, train loss: 0.86464, val loss: 0.86162\n",
      "Interaction training epoch: 49, train loss: 0.87873, val loss: 0.86415\n",
      "Interaction training epoch: 50, train loss: 0.87219, val loss: 0.86170\n",
      "Interaction training epoch: 51, train loss: 0.86383, val loss: 0.86244\n",
      "Interaction training epoch: 52, train loss: 0.84573, val loss: 0.84763\n",
      "Interaction training epoch: 53, train loss: 0.84697, val loss: 0.84169\n",
      "Interaction training epoch: 54, train loss: 0.84694, val loss: 0.84379\n",
      "Interaction training epoch: 55, train loss: 0.85308, val loss: 0.84095\n",
      "Interaction training epoch: 56, train loss: 0.84366, val loss: 0.84517\n",
      "Interaction training epoch: 57, train loss: 0.84581, val loss: 0.84451\n",
      "Interaction training epoch: 58, train loss: 0.84040, val loss: 0.83073\n",
      "Interaction training epoch: 59, train loss: 0.84330, val loss: 0.83043\n",
      "Interaction training epoch: 60, train loss: 0.84536, val loss: 0.84023\n",
      "Interaction training epoch: 61, train loss: 0.84098, val loss: 0.84568\n",
      "Interaction training epoch: 62, train loss: 0.84357, val loss: 0.83794\n",
      "Interaction training epoch: 63, train loss: 0.84226, val loss: 0.83503\n",
      "Interaction training epoch: 64, train loss: 0.84178, val loss: 0.84530\n",
      "Interaction training epoch: 65, train loss: 0.83526, val loss: 0.83150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 66, train loss: 0.84395, val loss: 0.83687\n",
      "Interaction training epoch: 67, train loss: 0.84613, val loss: 0.84470\n",
      "Interaction training epoch: 68, train loss: 0.83905, val loss: 0.83630\n",
      "Interaction training epoch: 69, train loss: 0.83339, val loss: 0.83048\n",
      "Interaction training epoch: 70, train loss: 0.84705, val loss: 0.85343\n",
      "Interaction training epoch: 71, train loss: 0.84214, val loss: 0.83718\n",
      "Interaction training epoch: 72, train loss: 0.83230, val loss: 0.83437\n",
      "Interaction training epoch: 73, train loss: 0.83875, val loss: 0.83393\n",
      "Interaction training epoch: 74, train loss: 0.83602, val loss: 0.83613\n",
      "Interaction training epoch: 75, train loss: 0.82823, val loss: 0.82581\n",
      "Interaction training epoch: 76, train loss: 0.82875, val loss: 0.82798\n",
      "Interaction training epoch: 77, train loss: 0.83669, val loss: 0.83998\n",
      "Interaction training epoch: 78, train loss: 0.83132, val loss: 0.83395\n",
      "Interaction training epoch: 79, train loss: 0.83032, val loss: 0.82510\n",
      "Interaction training epoch: 80, train loss: 0.83607, val loss: 0.83983\n",
      "Interaction training epoch: 81, train loss: 0.82574, val loss: 0.81968\n",
      "Interaction training epoch: 82, train loss: 0.84160, val loss: 0.83790\n",
      "Interaction training epoch: 83, train loss: 0.83571, val loss: 0.83894\n",
      "Interaction training epoch: 84, train loss: 0.82457, val loss: 0.82394\n",
      "Interaction training epoch: 85, train loss: 0.83506, val loss: 0.83366\n",
      "Interaction training epoch: 86, train loss: 0.83112, val loss: 0.83206\n",
      "Interaction training epoch: 87, train loss: 0.82897, val loss: 0.83805\n",
      "Interaction training epoch: 88, train loss: 0.82881, val loss: 0.82700\n",
      "Interaction training epoch: 89, train loss: 0.82242, val loss: 0.82033\n",
      "Interaction training epoch: 90, train loss: 0.82647, val loss: 0.82786\n",
      "Interaction training epoch: 91, train loss: 0.82379, val loss: 0.81565\n",
      "Interaction training epoch: 92, train loss: 0.82603, val loss: 0.82390\n",
      "Interaction training epoch: 93, train loss: 0.82282, val loss: 0.82088\n",
      "Interaction training epoch: 94, train loss: 0.82752, val loss: 0.82701\n",
      "Interaction training epoch: 95, train loss: 0.82206, val loss: 0.82698\n",
      "Interaction training epoch: 96, train loss: 0.82478, val loss: 0.83090\n",
      "Interaction training epoch: 97, train loss: 0.82240, val loss: 0.82491\n",
      "Interaction training epoch: 98, train loss: 0.82134, val loss: 0.82403\n",
      "Interaction training epoch: 99, train loss: 0.82218, val loss: 0.81842\n",
      "Interaction training epoch: 100, train loss: 0.82129, val loss: 0.82475\n",
      "Interaction training epoch: 101, train loss: 0.82926, val loss: 0.82863\n",
      "Interaction training epoch: 102, train loss: 0.82342, val loss: 0.81634\n",
      "Interaction training epoch: 103, train loss: 0.81906, val loss: 0.82755\n",
      "Interaction training epoch: 104, train loss: 0.82650, val loss: 0.83376\n",
      "Interaction training epoch: 105, train loss: 0.82231, val loss: 0.82285\n",
      "Interaction training epoch: 106, train loss: 0.81466, val loss: 0.81521\n",
      "Interaction training epoch: 107, train loss: 0.81372, val loss: 0.82139\n",
      "Interaction training epoch: 108, train loss: 0.81959, val loss: 0.82090\n",
      "Interaction training epoch: 109, train loss: 0.81562, val loss: 0.81227\n",
      "Interaction training epoch: 110, train loss: 0.81331, val loss: 0.81768\n",
      "Interaction training epoch: 111, train loss: 0.81783, val loss: 0.82425\n",
      "Interaction training epoch: 112, train loss: 0.81900, val loss: 0.81393\n",
      "Interaction training epoch: 113, train loss: 0.81605, val loss: 0.81996\n",
      "Interaction training epoch: 114, train loss: 0.82564, val loss: 0.82854\n",
      "Interaction training epoch: 115, train loss: 0.81695, val loss: 0.81721\n",
      "Interaction training epoch: 116, train loss: 0.81939, val loss: 0.81364\n",
      "Interaction training epoch: 117, train loss: 0.82301, val loss: 0.82736\n",
      "Interaction training epoch: 118, train loss: 0.81643, val loss: 0.81881\n",
      "Interaction training epoch: 119, train loss: 0.81692, val loss: 0.82008\n",
      "Interaction training epoch: 120, train loss: 0.81738, val loss: 0.82289\n",
      "Interaction training epoch: 121, train loss: 0.81710, val loss: 0.82163\n",
      "Interaction training epoch: 122, train loss: 0.81505, val loss: 0.82523\n",
      "Interaction training epoch: 123, train loss: 0.81464, val loss: 0.81410\n",
      "Interaction training epoch: 124, train loss: 0.82053, val loss: 0.82895\n",
      "Interaction training epoch: 125, train loss: 0.81899, val loss: 0.81830\n",
      "Interaction training epoch: 126, train loss: 0.82195, val loss: 0.82064\n",
      "Interaction training epoch: 127, train loss: 0.81603, val loss: 0.82306\n",
      "Interaction training epoch: 128, train loss: 0.81742, val loss: 0.81949\n",
      "Interaction training epoch: 129, train loss: 0.81473, val loss: 0.81521\n",
      "Interaction training epoch: 130, train loss: 0.81513, val loss: 0.82154\n",
      "Interaction training epoch: 131, train loss: 0.81182, val loss: 0.81499\n",
      "Interaction training epoch: 132, train loss: 0.81307, val loss: 0.81900\n",
      "Interaction training epoch: 133, train loss: 0.81672, val loss: 0.82037\n",
      "Interaction training epoch: 134, train loss: 0.81417, val loss: 0.81572\n",
      "Interaction training epoch: 135, train loss: 0.81367, val loss: 0.82144\n",
      "Interaction training epoch: 136, train loss: 0.81361, val loss: 0.81108\n",
      "Interaction training epoch: 137, train loss: 0.80932, val loss: 0.81995\n",
      "Interaction training epoch: 138, train loss: 0.81181, val loss: 0.81318\n",
      "Interaction training epoch: 139, train loss: 0.80853, val loss: 0.81289\n",
      "Interaction training epoch: 140, train loss: 0.81656, val loss: 0.82429\n",
      "Interaction training epoch: 141, train loss: 0.81985, val loss: 0.81969\n",
      "Interaction training epoch: 142, train loss: 0.82891, val loss: 0.83452\n",
      "Interaction training epoch: 143, train loss: 0.82113, val loss: 0.82668\n",
      "Interaction training epoch: 144, train loss: 0.81862, val loss: 0.81619\n",
      "Interaction training epoch: 145, train loss: 0.81979, val loss: 0.82493\n",
      "Interaction training epoch: 146, train loss: 0.80794, val loss: 0.81710\n",
      "Interaction training epoch: 147, train loss: 0.81291, val loss: 0.82201\n",
      "Interaction training epoch: 148, train loss: 0.81591, val loss: 0.82027\n",
      "Interaction training epoch: 149, train loss: 0.81403, val loss: 0.82098\n",
      "Interaction training epoch: 150, train loss: 0.80779, val loss: 0.81630\n",
      "Interaction training epoch: 151, train loss: 0.81152, val loss: 0.81222\n",
      "Interaction training epoch: 152, train loss: 0.81461, val loss: 0.82288\n",
      "Interaction training epoch: 153, train loss: 0.80903, val loss: 0.81454\n",
      "Interaction training epoch: 154, train loss: 0.81613, val loss: 0.81888\n",
      "Interaction training epoch: 155, train loss: 0.81388, val loss: 0.82147\n",
      "Interaction training epoch: 156, train loss: 0.80906, val loss: 0.81776\n",
      "Interaction training epoch: 157, train loss: 0.81889, val loss: 0.82356\n",
      "Interaction training epoch: 158, train loss: 0.80814, val loss: 0.81361\n",
      "Interaction training epoch: 159, train loss: 0.80897, val loss: 0.81078\n",
      "Interaction training epoch: 160, train loss: 0.81377, val loss: 0.82212\n",
      "Interaction training epoch: 161, train loss: 0.80932, val loss: 0.81388\n",
      "Interaction training epoch: 162, train loss: 0.81171, val loss: 0.82121\n",
      "Interaction training epoch: 163, train loss: 0.81287, val loss: 0.82121\n",
      "Interaction training epoch: 164, train loss: 0.81851, val loss: 0.82095\n",
      "Interaction training epoch: 165, train loss: 0.81255, val loss: 0.82528\n",
      "Interaction training epoch: 166, train loss: 0.81165, val loss: 0.81180\n",
      "Interaction training epoch: 167, train loss: 0.80829, val loss: 0.81279\n",
      "Interaction training epoch: 168, train loss: 0.81238, val loss: 0.82613\n",
      "Interaction training epoch: 169, train loss: 0.80394, val loss: 0.80636\n",
      "Interaction training epoch: 170, train loss: 0.81360, val loss: 0.81890\n",
      "Interaction training epoch: 171, train loss: 0.80650, val loss: 0.81267\n",
      "Interaction training epoch: 172, train loss: 0.80775, val loss: 0.81235\n",
      "Interaction training epoch: 173, train loss: 0.80773, val loss: 0.81532\n",
      "Interaction training epoch: 174, train loss: 0.81219, val loss: 0.81406\n",
      "Interaction training epoch: 175, train loss: 0.80747, val loss: 0.81996\n",
      "Interaction training epoch: 176, train loss: 0.80583, val loss: 0.81313\n",
      "Interaction training epoch: 177, train loss: 0.80975, val loss: 0.81412\n",
      "Interaction training epoch: 178, train loss: 0.81833, val loss: 0.82279\n",
      "Interaction training epoch: 179, train loss: 0.80748, val loss: 0.81166\n",
      "Interaction training epoch: 180, train loss: 0.81005, val loss: 0.81730\n",
      "Interaction training epoch: 181, train loss: 0.80734, val loss: 0.80926\n",
      "Interaction training epoch: 182, train loss: 0.80668, val loss: 0.81514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 183, train loss: 0.81271, val loss: 0.81974\n",
      "Interaction training epoch: 184, train loss: 0.81663, val loss: 0.81935\n",
      "Interaction training epoch: 185, train loss: 0.82000, val loss: 0.82545\n",
      "Interaction training epoch: 186, train loss: 0.81488, val loss: 0.82236\n",
      "Interaction training epoch: 187, train loss: 0.81687, val loss: 0.82708\n",
      "Interaction training epoch: 188, train loss: 0.81252, val loss: 0.82146\n",
      "Interaction training epoch: 189, train loss: 0.81613, val loss: 0.82250\n",
      "Interaction training epoch: 190, train loss: 0.80767, val loss: 0.81389\n",
      "Interaction training epoch: 191, train loss: 0.80966, val loss: 0.81236\n",
      "Interaction training epoch: 192, train loss: 0.81294, val loss: 0.82504\n",
      "Interaction training epoch: 193, train loss: 0.81119, val loss: 0.81753\n",
      "Interaction training epoch: 194, train loss: 0.80326, val loss: 0.80787\n",
      "Interaction training epoch: 195, train loss: 0.80621, val loss: 0.81181\n",
      "Interaction training epoch: 196, train loss: 0.81240, val loss: 0.81700\n",
      "Interaction training epoch: 197, train loss: 0.81029, val loss: 0.81844\n",
      "Interaction training epoch: 198, train loss: 0.80853, val loss: 0.82017\n",
      "Interaction training epoch: 199, train loss: 0.80597, val loss: 0.80853\n",
      "Interaction training epoch: 200, train loss: 0.80634, val loss: 0.81854\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########1 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.81804, val loss: 0.81888\n",
      "Interaction tuning epoch: 2, train loss: 0.80925, val loss: 0.81165\n",
      "Interaction tuning epoch: 3, train loss: 0.80584, val loss: 0.80653\n",
      "Interaction tuning epoch: 4, train loss: 0.81375, val loss: 0.81895\n",
      "Interaction tuning epoch: 5, train loss: 0.81393, val loss: 0.81698\n",
      "Interaction tuning epoch: 6, train loss: 0.80662, val loss: 0.81327\n",
      "Interaction tuning epoch: 7, train loss: 0.81151, val loss: 0.81622\n",
      "Interaction tuning epoch: 8, train loss: 0.80811, val loss: 0.81462\n",
      "Interaction tuning epoch: 9, train loss: 0.81615, val loss: 0.82075\n",
      "Interaction tuning epoch: 10, train loss: 0.80656, val loss: 0.81131\n",
      "Interaction tuning epoch: 11, train loss: 0.81029, val loss: 0.81714\n",
      "Interaction tuning epoch: 12, train loss: 0.81677, val loss: 0.81148\n",
      "Interaction tuning epoch: 13, train loss: 0.81157, val loss: 0.81497\n",
      "Interaction tuning epoch: 14, train loss: 0.80632, val loss: 0.81207\n",
      "Interaction tuning epoch: 15, train loss: 0.81666, val loss: 0.82041\n",
      "Interaction tuning epoch: 16, train loss: 0.81130, val loss: 0.82255\n",
      "Interaction tuning epoch: 17, train loss: 0.80603, val loss: 0.81079\n",
      "Interaction tuning epoch: 18, train loss: 0.81021, val loss: 0.81235\n",
      "Interaction tuning epoch: 19, train loss: 0.81521, val loss: 0.81904\n",
      "Interaction tuning epoch: 20, train loss: 0.80443, val loss: 0.81313\n",
      "Interaction tuning epoch: 21, train loss: 0.80655, val loss: 0.80518\n",
      "Interaction tuning epoch: 22, train loss: 0.80431, val loss: 0.80505\n",
      "Interaction tuning epoch: 23, train loss: 0.81297, val loss: 0.81931\n",
      "Interaction tuning epoch: 24, train loss: 0.81232, val loss: 0.81806\n",
      "Interaction tuning epoch: 25, train loss: 0.80727, val loss: 0.81133\n",
      "Interaction tuning epoch: 26, train loss: 0.81345, val loss: 0.81793\n",
      "Interaction tuning epoch: 27, train loss: 0.80278, val loss: 0.81305\n",
      "Interaction tuning epoch: 28, train loss: 0.81700, val loss: 0.81785\n",
      "Interaction tuning epoch: 29, train loss: 0.80730, val loss: 0.81358\n",
      "Interaction tuning epoch: 30, train loss: 0.80834, val loss: 0.81444\n",
      "Interaction tuning epoch: 31, train loss: 0.81063, val loss: 0.81611\n",
      "Interaction tuning epoch: 32, train loss: 0.80596, val loss: 0.81355\n",
      "Interaction tuning epoch: 33, train loss: 0.80751, val loss: 0.81448\n",
      "Interaction tuning epoch: 34, train loss: 0.80677, val loss: 0.81279\n",
      "Interaction tuning epoch: 35, train loss: 0.81001, val loss: 0.81343\n",
      "Interaction tuning epoch: 36, train loss: 0.80633, val loss: 0.81169\n",
      "Interaction tuning epoch: 37, train loss: 0.81535, val loss: 0.82359\n",
      "Interaction tuning epoch: 38, train loss: 0.81078, val loss: 0.81622\n",
      "Interaction tuning epoch: 39, train loss: 0.80363, val loss: 0.80845\n",
      "Interaction tuning epoch: 40, train loss: 0.80209, val loss: 0.80625\n",
      "Interaction tuning epoch: 41, train loss: 0.80848, val loss: 0.81421\n",
      "Interaction tuning epoch: 42, train loss: 0.80608, val loss: 0.81207\n",
      "Interaction tuning epoch: 43, train loss: 0.80741, val loss: 0.80908\n",
      "Interaction tuning epoch: 44, train loss: 0.80233, val loss: 0.80814\n",
      "Interaction tuning epoch: 45, train loss: 0.80654, val loss: 0.81195\n",
      "Interaction tuning epoch: 46, train loss: 0.80518, val loss: 0.80855\n",
      "Interaction tuning epoch: 47, train loss: 0.79724, val loss: 0.80195\n",
      "Interaction tuning epoch: 48, train loss: 0.81498, val loss: 0.82626\n",
      "Interaction tuning epoch: 49, train loss: 0.81333, val loss: 0.81645\n",
      "Interaction tuning epoch: 50, train loss: 0.80283, val loss: 0.80673\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 41.06839895248413\n",
      "After the gam stage, training error is 0.80283 , validation error is 0.80673\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 20.088928\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.663590 validation MAE=0.763374,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.617107 validation MAE=0.744439,rank=5\n",
      "[SoftImpute] Iter 3: observed MAE=0.577221 validation MAE=0.726847,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.543278 validation MAE=0.711695,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.514280 validation MAE=0.697332,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.488640 validation MAE=0.684544,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.464733 validation MAE=0.672773,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.446507 validation MAE=0.662812,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.430395 validation MAE=0.654411,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.416146 validation MAE=0.646234,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.401583 validation MAE=0.637847,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.389794 validation MAE=0.630254,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.377785 validation MAE=0.622973,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.367159 validation MAE=0.616781,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.357290 validation MAE=0.611259,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.348862 validation MAE=0.606245,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.341093 validation MAE=0.601620,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.333748 validation MAE=0.597478,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.326106 validation MAE=0.593325,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.319778 validation MAE=0.589849,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.314076 validation MAE=0.585936,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.308953 validation MAE=0.583237,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.303352 validation MAE=0.579671,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.299018 validation MAE=0.576638,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.294094 validation MAE=0.574051,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.289692 validation MAE=0.571286,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.286045 validation MAE=0.568754,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.282671 validation MAE=0.566440,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.278436 validation MAE=0.564248,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.275140 validation MAE=0.562286,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.272107 validation MAE=0.560282,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.268920 validation MAE=0.558414,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.265762 validation MAE=0.556562,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.263843 validation MAE=0.554981,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.261154 validation MAE=0.553404,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.258991 validation MAE=0.552334,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.256183 validation MAE=0.550244,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.253977 validation MAE=0.548977,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.251569 validation MAE=0.547810,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.249984 validation MAE=0.547316,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.248494 validation MAE=0.545924,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.246482 validation MAE=0.544969,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.244515 validation MAE=0.543389,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.242660 validation MAE=0.542627,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.242358 validation MAE=0.540067,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.243608 validation MAE=0.538470,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.245164 validation MAE=0.535814,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.246409 validation MAE=0.534924,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.246696 validation MAE=0.533068,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.245575 validation MAE=0.532357,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.245661 validation MAE=0.531188,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.244261 validation MAE=0.529670,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.243734 validation MAE=0.528332,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.242754 validation MAE=0.528023,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.241907 validation MAE=0.527332,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.239513 validation MAE=0.526985,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.239432 validation MAE=0.526085,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.238237 validation MAE=0.525356,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.236145 validation MAE=0.523994,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.235316 validation MAE=0.524136,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.234528 validation MAE=0.523845,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.233885 validation MAE=0.523262,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.232602 validation MAE=0.522592,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.229953 validation MAE=0.522360,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.229747 validation MAE=0.521538,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.228820 validation MAE=0.520406,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.228328 validation MAE=0.520264,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.227540 validation MAE=0.519990,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.226863 validation MAE=0.519561,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.225652 validation MAE=0.519403,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.224721 validation MAE=0.519183,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.222962 validation MAE=0.518516,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.222289 validation MAE=0.518041,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.221190 validation MAE=0.517901,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.220724 validation MAE=0.517316,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.219987 validation MAE=0.517029,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.219149 validation MAE=0.515940,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.218782 validation MAE=0.515534,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.217609 validation MAE=0.515020,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.217056 validation MAE=0.514887,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.216174 validation MAE=0.514794,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.214828 validation MAE=0.514766,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.214100 validation MAE=0.513838,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.213478 validation MAE=0.513701,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.212911 validation MAE=0.513579,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.212246 validation MAE=0.512749,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.211122 validation MAE=0.512316,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.209943 validation MAE=0.511876,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.209085 validation MAE=0.511862,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.209148 validation MAE=0.511599,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.208163 validation MAE=0.511395,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.207011 validation MAE=0.511040,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.206159 validation MAE=0.510912,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.206379 validation MAE=0.510191,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.206113 validation MAE=0.509879,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.205251 validation MAE=0.509847,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.204956 validation MAE=0.509339,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.204302 validation MAE=0.508796,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.202993 validation MAE=0.508681,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.203041 validation MAE=0.508566,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.202629 validation MAE=0.507546,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.201992 validation MAE=0.507250,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.201040 validation MAE=0.507058,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.200570 validation MAE=0.506583,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.199573 validation MAE=0.506157,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.199170 validation MAE=0.505952,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.197329 validation MAE=0.505231,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.197668 validation MAE=0.505258,rank=5\n",
      "[SoftImpute] Iter 109: observed MAE=0.197491 validation MAE=0.504741,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.197168 validation MAE=0.504485,rank=5\n",
      "[SoftImpute] Iter 111: observed MAE=0.196911 validation MAE=0.503922,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.196563 validation MAE=0.503589,rank=5\n",
      "[SoftImpute] Iter 113: observed MAE=0.196002 validation MAE=0.503255,rank=5\n",
      "[SoftImpute] Iter 114: observed MAE=0.195610 validation MAE=0.503027,rank=5\n",
      "[SoftImpute] Iter 115: observed MAE=0.193973 validation MAE=0.502502,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 116: observed MAE=0.193980 validation MAE=0.502460,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.192964 validation MAE=0.501913,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.193373 validation MAE=0.501783,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.192772 validation MAE=0.500974,rank=5\n",
      "[SoftImpute] Iter 120: observed MAE=0.193280 validation MAE=0.500500,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.191931 validation MAE=0.500134,rank=5\n",
      "[SoftImpute] Iter 122: observed MAE=0.191719 validation MAE=0.499922,rank=5\n",
      "[SoftImpute] Iter 123: observed MAE=0.190708 validation MAE=0.499392,rank=5\n",
      "[SoftImpute] Iter 124: observed MAE=0.189694 validation MAE=0.499219,rank=5\n",
      "[SoftImpute] Iter 125: observed MAE=0.189320 validation MAE=0.498411,rank=5\n",
      "[SoftImpute] Iter 126: observed MAE=0.189829 validation MAE=0.498846,rank=5\n",
      "[SoftImpute] Iter 127: observed MAE=0.188732 validation MAE=0.498241,rank=5\n",
      "[SoftImpute] Iter 128: observed MAE=0.189146 validation MAE=0.497822,rank=5\n",
      "[SoftImpute] Iter 129: observed MAE=0.188878 validation MAE=0.497540,rank=5\n",
      "[SoftImpute] Iter 130: observed MAE=0.188020 validation MAE=0.497517,rank=5\n",
      "[SoftImpute] Iter 131: observed MAE=0.187045 validation MAE=0.496928,rank=5\n",
      "[SoftImpute] Iter 132: observed MAE=0.186608 validation MAE=0.497199,rank=5\n",
      "[SoftImpute] Iter 133: observed MAE=0.185976 validation MAE=0.496769,rank=5\n",
      "[SoftImpute] Iter 134: observed MAE=0.186700 validation MAE=0.496049,rank=5\n",
      "[SoftImpute] Iter 135: observed MAE=0.185574 validation MAE=0.495481,rank=5\n",
      "[SoftImpute] Iter 136: observed MAE=0.186543 validation MAE=0.495554,rank=5\n",
      "[SoftImpute] Iter 137: observed MAE=0.185060 validation MAE=0.495048,rank=5\n",
      "[SoftImpute] Iter 138: observed MAE=0.184693 validation MAE=0.494461,rank=5\n",
      "[SoftImpute] Iter 139: observed MAE=0.183910 validation MAE=0.494132,rank=5\n",
      "[SoftImpute] Iter 140: observed MAE=0.184195 validation MAE=0.494430,rank=5\n",
      "[SoftImpute] Iter 141: observed MAE=0.183588 validation MAE=0.493775,rank=5\n",
      "[SoftImpute] Iter 142: observed MAE=0.183557 validation MAE=0.493495,rank=5\n",
      "[SoftImpute] Iter 143: observed MAE=0.182364 validation MAE=0.492928,rank=5\n",
      "[SoftImpute] Iter 144: observed MAE=0.182740 validation MAE=0.493380,rank=5\n",
      "[SoftImpute] Iter 145: observed MAE=0.182283 validation MAE=0.492772,rank=5\n",
      "[SoftImpute] Iter 146: observed MAE=0.181596 validation MAE=0.492462,rank=5\n",
      "[SoftImpute] Iter 147: observed MAE=0.181076 validation MAE=0.492213,rank=5\n",
      "[SoftImpute] Iter 148: observed MAE=0.181089 validation MAE=0.491853,rank=5\n",
      "[SoftImpute] Iter 149: observed MAE=0.180554 validation MAE=0.491490,rank=5\n",
      "[SoftImpute] Iter 150: observed MAE=0.180329 validation MAE=0.491027,rank=5\n",
      "[SoftImpute] Iter 151: observed MAE=0.179375 validation MAE=0.490403,rank=5\n",
      "[SoftImpute] Iter 152: observed MAE=0.179925 validation MAE=0.490529,rank=5\n",
      "[SoftImpute] Iter 153: observed MAE=0.179664 validation MAE=0.490062,rank=5\n",
      "[SoftImpute] Iter 154: observed MAE=0.179038 validation MAE=0.489604,rank=5\n",
      "[SoftImpute] Iter 155: observed MAE=0.179091 validation MAE=0.489151,rank=5\n",
      "[SoftImpute] Iter 156: observed MAE=0.178437 validation MAE=0.488926,rank=5\n",
      "[SoftImpute] Iter 157: observed MAE=0.178283 validation MAE=0.489014,rank=5\n",
      "[SoftImpute] Iter 158: observed MAE=0.178076 validation MAE=0.488466,rank=5\n",
      "[SoftImpute] Iter 159: observed MAE=0.177427 validation MAE=0.488189,rank=5\n",
      "[SoftImpute] Iter 160: observed MAE=0.177188 validation MAE=0.487710,rank=5\n",
      "[SoftImpute] Iter 161: observed MAE=0.177399 validation MAE=0.487342,rank=5\n",
      "[SoftImpute] Iter 162: observed MAE=0.176931 validation MAE=0.487176,rank=5\n",
      "[SoftImpute] Iter 163: observed MAE=0.176757 validation MAE=0.486759,rank=5\n",
      "[SoftImpute] Iter 164: observed MAE=0.176428 validation MAE=0.486321,rank=5\n",
      "[SoftImpute] Iter 165: observed MAE=0.175967 validation MAE=0.486112,rank=5\n",
      "[SoftImpute] Iter 166: observed MAE=0.175343 validation MAE=0.485748,rank=5\n",
      "[SoftImpute] Iter 167: observed MAE=0.175193 validation MAE=0.485098,rank=5\n",
      "[SoftImpute] Iter 168: observed MAE=0.174683 validation MAE=0.484956,rank=5\n",
      "[SoftImpute] Iter 169: observed MAE=0.174678 validation MAE=0.484864,rank=5\n",
      "[SoftImpute] Iter 170: observed MAE=0.174606 validation MAE=0.484470,rank=5\n",
      "[SoftImpute] Iter 171: observed MAE=0.174578 validation MAE=0.483999,rank=5\n",
      "[SoftImpute] Iter 172: observed MAE=0.174450 validation MAE=0.484000,rank=5\n",
      "[SoftImpute] Iter 173: observed MAE=0.174033 validation MAE=0.483692,rank=5\n",
      "[SoftImpute] Iter 174: observed MAE=0.173915 validation MAE=0.483390,rank=5\n",
      "[SoftImpute] Iter 175: observed MAE=0.173172 validation MAE=0.482875,rank=5\n",
      "[SoftImpute] Iter 176: observed MAE=0.173002 validation MAE=0.482856,rank=5\n",
      "[SoftImpute] Iter 177: observed MAE=0.172915 validation MAE=0.482472,rank=5\n",
      "[SoftImpute] Iter 178: observed MAE=0.172601 validation MAE=0.482125,rank=5\n",
      "[SoftImpute] Iter 179: observed MAE=0.172975 validation MAE=0.482035,rank=5\n",
      "[SoftImpute] Iter 180: observed MAE=0.171778 validation MAE=0.481428,rank=5\n",
      "[SoftImpute] Iter 181: observed MAE=0.171225 validation MAE=0.481318,rank=5\n",
      "[SoftImpute] Iter 182: observed MAE=0.171961 validation MAE=0.481303,rank=5\n",
      "[SoftImpute] Iter 183: observed MAE=0.171967 validation MAE=0.480511,rank=5\n",
      "[SoftImpute] Iter 184: observed MAE=0.171220 validation MAE=0.480092,rank=5\n",
      "[SoftImpute] Iter 185: observed MAE=0.171320 validation MAE=0.479990,rank=5\n",
      "[SoftImpute] Iter 186: observed MAE=0.171427 validation MAE=0.479855,rank=5\n",
      "[SoftImpute] Iter 187: observed MAE=0.171150 validation MAE=0.479332,rank=5\n",
      "[SoftImpute] Iter 188: observed MAE=0.170786 validation MAE=0.478902,rank=5\n",
      "[SoftImpute] Iter 189: observed MAE=0.170863 validation MAE=0.478576,rank=5\n",
      "[SoftImpute] Iter 190: observed MAE=0.170780 validation MAE=0.478380,rank=5\n",
      "[SoftImpute] Iter 191: observed MAE=0.170552 validation MAE=0.477783,rank=5\n",
      "[SoftImpute] Iter 192: observed MAE=0.169599 validation MAE=0.477600,rank=5\n",
      "[SoftImpute] Iter 193: observed MAE=0.169464 validation MAE=0.477526,rank=5\n",
      "[SoftImpute] Iter 194: observed MAE=0.169136 validation MAE=0.477149,rank=5\n",
      "[SoftImpute] Iter 195: observed MAE=0.168977 validation MAE=0.476625,rank=5\n",
      "[SoftImpute] Iter 196: observed MAE=0.168958 validation MAE=0.476604,rank=5\n",
      "[SoftImpute] Iter 197: observed MAE=0.169544 validation MAE=0.475988,rank=5\n",
      "[SoftImpute] Iter 198: observed MAE=0.169248 validation MAE=0.475578,rank=5\n",
      "[SoftImpute] Iter 199: observed MAE=0.168829 validation MAE=0.475737,rank=5\n",
      "[SoftImpute] Iter 200: observed MAE=0.168495 validation MAE=0.475408,rank=5\n",
      "[SoftImpute] Stopped after iteration 200 for lambda=0.401779\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 5.52797532081604\n",
      "After the matrix factor stage, training error is 0.16849, validation error is 0.47541\n",
      "6\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.06127, val loss: 4.18750\n",
      "Main effects training epoch: 2, train loss: 3.94372, val loss: 4.06998\n",
      "Main effects training epoch: 3, train loss: 3.73385, val loss: 3.85840\n",
      "Main effects training epoch: 4, train loss: 3.50173, val loss: 3.64644\n",
      "Main effects training epoch: 5, train loss: 3.30801, val loss: 3.42576\n",
      "Main effects training epoch: 6, train loss: 3.24069, val loss: 3.31197\n",
      "Main effects training epoch: 7, train loss: 3.26974, val loss: 3.34367\n",
      "Main effects training epoch: 8, train loss: 3.25481, val loss: 3.34357\n",
      "Main effects training epoch: 9, train loss: 3.24396, val loss: 3.33332\n",
      "Main effects training epoch: 10, train loss: 3.11985, val loss: 3.21158\n",
      "Main effects training epoch: 11, train loss: 3.06289, val loss: 3.14888\n",
      "Main effects training epoch: 12, train loss: 3.03354, val loss: 3.12163\n",
      "Main effects training epoch: 13, train loss: 3.00590, val loss: 3.09664\n",
      "Main effects training epoch: 14, train loss: 2.94311, val loss: 3.02991\n",
      "Main effects training epoch: 15, train loss: 2.87152, val loss: 2.96454\n",
      "Main effects training epoch: 16, train loss: 2.80454, val loss: 2.89249\n",
      "Main effects training epoch: 17, train loss: 2.75883, val loss: 2.84694\n",
      "Main effects training epoch: 18, train loss: 2.70602, val loss: 2.78567\n",
      "Main effects training epoch: 19, train loss: 2.65646, val loss: 2.74703\n",
      "Main effects training epoch: 20, train loss: 2.57439, val loss: 2.66271\n",
      "Main effects training epoch: 21, train loss: 2.53241, val loss: 2.61779\n",
      "Main effects training epoch: 22, train loss: 2.46536, val loss: 2.53903\n",
      "Main effects training epoch: 23, train loss: 2.42146, val loss: 2.49493\n",
      "Main effects training epoch: 24, train loss: 2.39033, val loss: 2.46035\n",
      "Main effects training epoch: 25, train loss: 2.35002, val loss: 2.42549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 26, train loss: 2.31734, val loss: 2.39614\n",
      "Main effects training epoch: 27, train loss: 2.28992, val loss: 2.36738\n",
      "Main effects training epoch: 28, train loss: 2.25937, val loss: 2.33715\n",
      "Main effects training epoch: 29, train loss: 2.22815, val loss: 2.31193\n",
      "Main effects training epoch: 30, train loss: 2.19672, val loss: 2.27397\n",
      "Main effects training epoch: 31, train loss: 2.18084, val loss: 2.26244\n",
      "Main effects training epoch: 32, train loss: 2.17676, val loss: 2.25526\n",
      "Main effects training epoch: 33, train loss: 2.11736, val loss: 2.19960\n",
      "Main effects training epoch: 34, train loss: 2.09021, val loss: 2.16812\n",
      "Main effects training epoch: 35, train loss: 2.08909, val loss: 2.16431\n",
      "Main effects training epoch: 36, train loss: 2.05781, val loss: 2.14102\n",
      "Main effects training epoch: 37, train loss: 2.01661, val loss: 2.09296\n",
      "Main effects training epoch: 38, train loss: 2.03895, val loss: 2.11742\n",
      "Main effects training epoch: 39, train loss: 2.00570, val loss: 2.08593\n",
      "Main effects training epoch: 40, train loss: 1.98853, val loss: 2.05987\n",
      "Main effects training epoch: 41, train loss: 1.96965, val loss: 2.04945\n",
      "Main effects training epoch: 42, train loss: 1.94716, val loss: 2.02087\n",
      "Main effects training epoch: 43, train loss: 1.97227, val loss: 2.04284\n",
      "Main effects training epoch: 44, train loss: 1.91054, val loss: 1.97838\n",
      "Main effects training epoch: 45, train loss: 1.89739, val loss: 1.97083\n",
      "Main effects training epoch: 46, train loss: 1.92470, val loss: 1.99509\n",
      "Main effects training epoch: 47, train loss: 1.88008, val loss: 1.94695\n",
      "Main effects training epoch: 48, train loss: 1.87298, val loss: 1.94842\n",
      "Main effects training epoch: 49, train loss: 1.88399, val loss: 1.95287\n",
      "Main effects training epoch: 50, train loss: 1.84662, val loss: 1.91116\n",
      "Main effects training epoch: 51, train loss: 1.83958, val loss: 1.90553\n",
      "Main effects training epoch: 52, train loss: 1.83802, val loss: 1.90608\n",
      "Main effects training epoch: 53, train loss: 1.82487, val loss: 1.89072\n",
      "Main effects training epoch: 54, train loss: 1.81213, val loss: 1.87326\n",
      "Main effects training epoch: 55, train loss: 1.81298, val loss: 1.88045\n",
      "Main effects training epoch: 56, train loss: 1.79967, val loss: 1.86275\n",
      "Main effects training epoch: 57, train loss: 1.79736, val loss: 1.86081\n",
      "Main effects training epoch: 58, train loss: 1.79215, val loss: 1.85879\n",
      "Main effects training epoch: 59, train loss: 1.77764, val loss: 1.84023\n",
      "Main effects training epoch: 60, train loss: 1.77848, val loss: 1.84209\n",
      "Main effects training epoch: 61, train loss: 1.77332, val loss: 1.84034\n",
      "Main effects training epoch: 62, train loss: 1.76471, val loss: 1.82407\n",
      "Main effects training epoch: 63, train loss: 1.76065, val loss: 1.82336\n",
      "Main effects training epoch: 64, train loss: 1.75630, val loss: 1.81499\n",
      "Main effects training epoch: 65, train loss: 1.74798, val loss: 1.80774\n",
      "Main effects training epoch: 66, train loss: 1.74856, val loss: 1.80859\n",
      "Main effects training epoch: 67, train loss: 1.74143, val loss: 1.79900\n",
      "Main effects training epoch: 68, train loss: 1.73362, val loss: 1.79091\n",
      "Main effects training epoch: 69, train loss: 1.73911, val loss: 1.79849\n",
      "Main effects training epoch: 70, train loss: 1.72520, val loss: 1.78539\n",
      "Main effects training epoch: 71, train loss: 1.72849, val loss: 1.78233\n",
      "Main effects training epoch: 72, train loss: 1.72473, val loss: 1.78799\n",
      "Main effects training epoch: 73, train loss: 1.72298, val loss: 1.77058\n",
      "Main effects training epoch: 74, train loss: 1.71202, val loss: 1.77062\n",
      "Main effects training epoch: 75, train loss: 1.72654, val loss: 1.78159\n",
      "Main effects training epoch: 76, train loss: 1.71560, val loss: 1.76835\n",
      "Main effects training epoch: 77, train loss: 1.71143, val loss: 1.76226\n",
      "Main effects training epoch: 78, train loss: 1.70797, val loss: 1.76406\n",
      "Main effects training epoch: 79, train loss: 1.70880, val loss: 1.76013\n",
      "Main effects training epoch: 80, train loss: 1.70127, val loss: 1.75448\n",
      "Main effects training epoch: 81, train loss: 1.70074, val loss: 1.75333\n",
      "Main effects training epoch: 82, train loss: 1.69782, val loss: 1.75625\n",
      "Main effects training epoch: 83, train loss: 1.70192, val loss: 1.75167\n",
      "Main effects training epoch: 84, train loss: 1.69596, val loss: 1.74962\n",
      "Main effects training epoch: 85, train loss: 1.68968, val loss: 1.74112\n",
      "Main effects training epoch: 86, train loss: 1.69279, val loss: 1.74699\n",
      "Main effects training epoch: 87, train loss: 1.69393, val loss: 1.75105\n",
      "Main effects training epoch: 88, train loss: 1.69086, val loss: 1.73692\n",
      "Main effects training epoch: 89, train loss: 1.68567, val loss: 1.74274\n",
      "Main effects training epoch: 90, train loss: 1.68247, val loss: 1.73429\n",
      "Main effects training epoch: 91, train loss: 1.67493, val loss: 1.72980\n",
      "Main effects training epoch: 92, train loss: 1.68052, val loss: 1.73880\n",
      "Main effects training epoch: 93, train loss: 1.66965, val loss: 1.71845\n",
      "Main effects training epoch: 94, train loss: 1.66726, val loss: 1.72760\n",
      "Main effects training epoch: 95, train loss: 1.66137, val loss: 1.71726\n",
      "Main effects training epoch: 96, train loss: 1.65476, val loss: 1.71428\n",
      "Main effects training epoch: 97, train loss: 1.64354, val loss: 1.70635\n",
      "Main effects training epoch: 98, train loss: 1.64341, val loss: 1.71263\n",
      "Main effects training epoch: 99, train loss: 1.64107, val loss: 1.70890\n",
      "Main effects training epoch: 100, train loss: 1.63587, val loss: 1.71051\n",
      "Main effects training epoch: 101, train loss: 1.62601, val loss: 1.69722\n",
      "Main effects training epoch: 102, train loss: 1.62524, val loss: 1.69425\n",
      "Main effects training epoch: 103, train loss: 1.63423, val loss: 1.70263\n",
      "Main effects training epoch: 104, train loss: 1.62083, val loss: 1.69510\n",
      "Main effects training epoch: 105, train loss: 1.62740, val loss: 1.69945\n",
      "Main effects training epoch: 106, train loss: 1.62820, val loss: 1.69663\n",
      "Main effects training epoch: 107, train loss: 1.62375, val loss: 1.70101\n",
      "Main effects training epoch: 108, train loss: 1.61729, val loss: 1.69291\n",
      "Main effects training epoch: 109, train loss: 1.61271, val loss: 1.68458\n",
      "Main effects training epoch: 110, train loss: 1.61465, val loss: 1.68778\n",
      "Main effects training epoch: 111, train loss: 1.61099, val loss: 1.68535\n",
      "Main effects training epoch: 112, train loss: 1.61246, val loss: 1.68637\n",
      "Main effects training epoch: 113, train loss: 1.61303, val loss: 1.68723\n",
      "Main effects training epoch: 114, train loss: 1.60829, val loss: 1.68124\n",
      "Main effects training epoch: 115, train loss: 1.61204, val loss: 1.69366\n",
      "Main effects training epoch: 116, train loss: 1.61357, val loss: 1.67498\n",
      "Main effects training epoch: 117, train loss: 1.60872, val loss: 1.69046\n",
      "Main effects training epoch: 118, train loss: 1.61545, val loss: 1.68665\n",
      "Main effects training epoch: 119, train loss: 1.60402, val loss: 1.67758\n",
      "Main effects training epoch: 120, train loss: 1.60771, val loss: 1.67789\n",
      "Main effects training epoch: 121, train loss: 1.60289, val loss: 1.67759\n",
      "Main effects training epoch: 122, train loss: 1.60329, val loss: 1.67604\n",
      "Main effects training epoch: 123, train loss: 1.60375, val loss: 1.67674\n",
      "Main effects training epoch: 124, train loss: 1.60251, val loss: 1.67944\n",
      "Main effects training epoch: 125, train loss: 1.60209, val loss: 1.66942\n",
      "Main effects training epoch: 126, train loss: 1.60022, val loss: 1.67702\n",
      "Main effects training epoch: 127, train loss: 1.60154, val loss: 1.67918\n",
      "Main effects training epoch: 128, train loss: 1.59849, val loss: 1.66996\n",
      "Main effects training epoch: 129, train loss: 1.59849, val loss: 1.67197\n",
      "Main effects training epoch: 130, train loss: 1.59925, val loss: 1.67638\n",
      "Main effects training epoch: 131, train loss: 1.60172, val loss: 1.67142\n",
      "Main effects training epoch: 132, train loss: 1.60205, val loss: 1.67755\n",
      "Main effects training epoch: 133, train loss: 1.61384, val loss: 1.68009\n",
      "Main effects training epoch: 134, train loss: 1.60747, val loss: 1.68191\n",
      "Main effects training epoch: 135, train loss: 1.59626, val loss: 1.66811\n",
      "Main effects training epoch: 136, train loss: 1.59704, val loss: 1.67125\n",
      "Main effects training epoch: 137, train loss: 1.59354, val loss: 1.66728\n",
      "Main effects training epoch: 138, train loss: 1.59614, val loss: 1.66837\n",
      "Main effects training epoch: 139, train loss: 1.60023, val loss: 1.66773\n",
      "Main effects training epoch: 140, train loss: 1.59437, val loss: 1.67661\n",
      "Main effects training epoch: 141, train loss: 1.59273, val loss: 1.66611\n",
      "Main effects training epoch: 142, train loss: 1.59281, val loss: 1.66671\n",
      "Main effects training epoch: 143, train loss: 1.59028, val loss: 1.66378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 144, train loss: 1.58915, val loss: 1.66548\n",
      "Main effects training epoch: 145, train loss: 1.58895, val loss: 1.66534\n",
      "Main effects training epoch: 146, train loss: 1.59217, val loss: 1.66893\n",
      "Main effects training epoch: 147, train loss: 1.59220, val loss: 1.66056\n",
      "Main effects training epoch: 148, train loss: 1.58885, val loss: 1.67141\n",
      "Main effects training epoch: 149, train loss: 1.59025, val loss: 1.65907\n",
      "Main effects training epoch: 150, train loss: 1.58671, val loss: 1.67221\n",
      "Main effects training epoch: 151, train loss: 1.58348, val loss: 1.66014\n",
      "Main effects training epoch: 152, train loss: 1.58409, val loss: 1.65472\n",
      "Main effects training epoch: 153, train loss: 1.58512, val loss: 1.66679\n",
      "Main effects training epoch: 154, train loss: 1.58560, val loss: 1.66331\n",
      "Main effects training epoch: 155, train loss: 1.58197, val loss: 1.65927\n",
      "Main effects training epoch: 156, train loss: 1.59129, val loss: 1.66635\n",
      "Main effects training epoch: 157, train loss: 1.58915, val loss: 1.66328\n",
      "Main effects training epoch: 158, train loss: 1.58246, val loss: 1.66379\n",
      "Main effects training epoch: 159, train loss: 1.58165, val loss: 1.66234\n",
      "Main effects training epoch: 160, train loss: 1.58394, val loss: 1.66665\n",
      "Main effects training epoch: 161, train loss: 1.59397, val loss: 1.67059\n",
      "Main effects training epoch: 162, train loss: 1.57824, val loss: 1.65615\n",
      "Main effects training epoch: 163, train loss: 1.58155, val loss: 1.67260\n",
      "Main effects training epoch: 164, train loss: 1.57631, val loss: 1.65650\n",
      "Main effects training epoch: 165, train loss: 1.57873, val loss: 1.65284\n",
      "Main effects training epoch: 166, train loss: 1.57939, val loss: 1.66703\n",
      "Main effects training epoch: 167, train loss: 1.58080, val loss: 1.65835\n",
      "Main effects training epoch: 168, train loss: 1.56912, val loss: 1.65550\n",
      "Main effects training epoch: 169, train loss: 1.57298, val loss: 1.65586\n",
      "Main effects training epoch: 170, train loss: 1.57307, val loss: 1.64656\n",
      "Main effects training epoch: 171, train loss: 1.56736, val loss: 1.66263\n",
      "Main effects training epoch: 172, train loss: 1.57177, val loss: 1.65181\n",
      "Main effects training epoch: 173, train loss: 1.56248, val loss: 1.64827\n",
      "Main effects training epoch: 174, train loss: 1.56551, val loss: 1.64093\n",
      "Main effects training epoch: 175, train loss: 1.55865, val loss: 1.63909\n",
      "Main effects training epoch: 176, train loss: 1.56529, val loss: 1.64795\n",
      "Main effects training epoch: 177, train loss: 1.55759, val loss: 1.63497\n",
      "Main effects training epoch: 178, train loss: 1.56142, val loss: 1.65084\n",
      "Main effects training epoch: 179, train loss: 1.55921, val loss: 1.64350\n",
      "Main effects training epoch: 180, train loss: 1.55240, val loss: 1.63291\n",
      "Main effects training epoch: 181, train loss: 1.55097, val loss: 1.62861\n",
      "Main effects training epoch: 182, train loss: 1.54926, val loss: 1.62669\n",
      "Main effects training epoch: 183, train loss: 1.55610, val loss: 1.64271\n",
      "Main effects training epoch: 184, train loss: 1.55246, val loss: 1.63195\n",
      "Main effects training epoch: 185, train loss: 1.54824, val loss: 1.63326\n",
      "Main effects training epoch: 186, train loss: 1.56004, val loss: 1.63316\n",
      "Main effects training epoch: 187, train loss: 1.55381, val loss: 1.63438\n",
      "Main effects training epoch: 188, train loss: 1.56255, val loss: 1.62429\n",
      "Main effects training epoch: 189, train loss: 1.55339, val loss: 1.62896\n",
      "Main effects training epoch: 190, train loss: 1.54633, val loss: 1.63940\n",
      "Main effects training epoch: 191, train loss: 1.55924, val loss: 1.63972\n",
      "Main effects training epoch: 192, train loss: 1.54491, val loss: 1.63396\n",
      "Main effects training epoch: 193, train loss: 1.54251, val loss: 1.61933\n",
      "Main effects training epoch: 194, train loss: 1.54740, val loss: 1.61324\n",
      "Main effects training epoch: 195, train loss: 1.54167, val loss: 1.62660\n",
      "Main effects training epoch: 196, train loss: 1.54385, val loss: 1.62804\n",
      "Main effects training epoch: 197, train loss: 1.54812, val loss: 1.62185\n",
      "Main effects training epoch: 198, train loss: 1.55514, val loss: 1.64292\n",
      "Main effects training epoch: 199, train loss: 1.54362, val loss: 1.62300\n",
      "Main effects training epoch: 200, train loss: 1.54086, val loss: 1.62181\n",
      "Main effects training epoch: 201, train loss: 1.54205, val loss: 1.62665\n",
      "Main effects training epoch: 202, train loss: 1.53938, val loss: 1.61419\n",
      "Main effects training epoch: 203, train loss: 1.53822, val loss: 1.61982\n",
      "Main effects training epoch: 204, train loss: 1.53552, val loss: 1.62238\n",
      "Main effects training epoch: 205, train loss: 1.54988, val loss: 1.63397\n",
      "Main effects training epoch: 206, train loss: 1.53933, val loss: 1.62622\n",
      "Main effects training epoch: 207, train loss: 1.53280, val loss: 1.61600\n",
      "Main effects training epoch: 208, train loss: 1.52970, val loss: 1.60759\n",
      "Main effects training epoch: 209, train loss: 1.53037, val loss: 1.61048\n",
      "Main effects training epoch: 210, train loss: 1.53219, val loss: 1.60778\n",
      "Main effects training epoch: 211, train loss: 1.52914, val loss: 1.61013\n",
      "Main effects training epoch: 212, train loss: 1.52946, val loss: 1.59967\n",
      "Main effects training epoch: 213, train loss: 1.53340, val loss: 1.61464\n",
      "Main effects training epoch: 214, train loss: 1.53958, val loss: 1.61630\n",
      "Main effects training epoch: 215, train loss: 1.53945, val loss: 1.63404\n",
      "Main effects training epoch: 216, train loss: 1.52760, val loss: 1.61544\n",
      "Main effects training epoch: 217, train loss: 1.53511, val loss: 1.60940\n",
      "Main effects training epoch: 218, train loss: 1.53579, val loss: 1.60756\n",
      "Main effects training epoch: 219, train loss: 1.52678, val loss: 1.60738\n",
      "Main effects training epoch: 220, train loss: 1.52859, val loss: 1.60741\n",
      "Main effects training epoch: 221, train loss: 1.52799, val loss: 1.60680\n",
      "Main effects training epoch: 222, train loss: 1.52904, val loss: 1.61247\n",
      "Main effects training epoch: 223, train loss: 1.53119, val loss: 1.60855\n",
      "Main effects training epoch: 224, train loss: 1.53165, val loss: 1.59753\n",
      "Main effects training epoch: 225, train loss: 1.52496, val loss: 1.61112\n",
      "Main effects training epoch: 226, train loss: 1.52548, val loss: 1.60883\n",
      "Main effects training epoch: 227, train loss: 1.53167, val loss: 1.62063\n",
      "Main effects training epoch: 228, train loss: 1.53368, val loss: 1.60157\n",
      "Main effects training epoch: 229, train loss: 1.52192, val loss: 1.59653\n",
      "Main effects training epoch: 230, train loss: 1.52174, val loss: 1.60876\n",
      "Main effects training epoch: 231, train loss: 1.51822, val loss: 1.59244\n",
      "Main effects training epoch: 232, train loss: 1.52019, val loss: 1.61059\n",
      "Main effects training epoch: 233, train loss: 1.52339, val loss: 1.59180\n",
      "Main effects training epoch: 234, train loss: 1.52052, val loss: 1.59686\n",
      "Main effects training epoch: 235, train loss: 1.52212, val loss: 1.59960\n",
      "Main effects training epoch: 236, train loss: 1.52399, val loss: 1.61243\n",
      "Main effects training epoch: 237, train loss: 1.51962, val loss: 1.59290\n",
      "Main effects training epoch: 238, train loss: 1.51770, val loss: 1.60073\n",
      "Main effects training epoch: 239, train loss: 1.51641, val loss: 1.59640\n",
      "Main effects training epoch: 240, train loss: 1.52326, val loss: 1.59562\n",
      "Main effects training epoch: 241, train loss: 1.52100, val loss: 1.58235\n",
      "Main effects training epoch: 242, train loss: 1.51718, val loss: 1.59731\n",
      "Main effects training epoch: 243, train loss: 1.51419, val loss: 1.59359\n",
      "Main effects training epoch: 244, train loss: 1.51163, val loss: 1.58890\n",
      "Main effects training epoch: 245, train loss: 1.51211, val loss: 1.59266\n",
      "Main effects training epoch: 246, train loss: 1.51921, val loss: 1.60692\n",
      "Main effects training epoch: 247, train loss: 1.51745, val loss: 1.60221\n",
      "Main effects training epoch: 248, train loss: 1.51960, val loss: 1.59891\n",
      "Main effects training epoch: 249, train loss: 1.52659, val loss: 1.60356\n",
      "Main effects training epoch: 250, train loss: 1.52106, val loss: 1.59905\n",
      "Main effects training epoch: 251, train loss: 1.51183, val loss: 1.60288\n",
      "Main effects training epoch: 252, train loss: 1.51604, val loss: 1.59182\n",
      "Main effects training epoch: 253, train loss: 1.51691, val loss: 1.58787\n",
      "Main effects training epoch: 254, train loss: 1.51537, val loss: 1.59050\n",
      "Main effects training epoch: 255, train loss: 1.51519, val loss: 1.60199\n",
      "Main effects training epoch: 256, train loss: 1.51385, val loss: 1.57830\n",
      "Main effects training epoch: 257, train loss: 1.55455, val loss: 1.62471\n",
      "Main effects training epoch: 258, train loss: 1.51261, val loss: 1.58695\n",
      "Main effects training epoch: 259, train loss: 1.50901, val loss: 1.58842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 260, train loss: 1.51819, val loss: 1.59645\n",
      "Main effects training epoch: 261, train loss: 1.52249, val loss: 1.60460\n",
      "Main effects training epoch: 262, train loss: 1.51222, val loss: 1.59456\n",
      "Main effects training epoch: 263, train loss: 1.50809, val loss: 1.59122\n",
      "Main effects training epoch: 264, train loss: 1.50764, val loss: 1.57608\n",
      "Main effects training epoch: 265, train loss: 1.51277, val loss: 1.59853\n",
      "Main effects training epoch: 266, train loss: 1.51178, val loss: 1.58530\n",
      "Main effects training epoch: 267, train loss: 1.51348, val loss: 1.58992\n",
      "Main effects training epoch: 268, train loss: 1.50642, val loss: 1.59113\n",
      "Main effects training epoch: 269, train loss: 1.50349, val loss: 1.57173\n",
      "Main effects training epoch: 270, train loss: 1.51501, val loss: 1.58123\n",
      "Main effects training epoch: 271, train loss: 1.52075, val loss: 1.58455\n",
      "Main effects training epoch: 272, train loss: 1.50516, val loss: 1.58423\n",
      "Main effects training epoch: 273, train loss: 1.51239, val loss: 1.59578\n",
      "Main effects training epoch: 274, train loss: 1.51247, val loss: 1.59159\n",
      "Main effects training epoch: 275, train loss: 1.50439, val loss: 1.58321\n",
      "Main effects training epoch: 276, train loss: 1.50293, val loss: 1.58240\n",
      "Main effects training epoch: 277, train loss: 1.50519, val loss: 1.57517\n",
      "Main effects training epoch: 278, train loss: 1.50519, val loss: 1.59454\n",
      "Main effects training epoch: 279, train loss: 1.50544, val loss: 1.56921\n",
      "Main effects training epoch: 280, train loss: 1.50081, val loss: 1.58215\n",
      "Main effects training epoch: 281, train loss: 1.50759, val loss: 1.59667\n",
      "Main effects training epoch: 282, train loss: 1.50367, val loss: 1.57629\n",
      "Main effects training epoch: 283, train loss: 1.50502, val loss: 1.59287\n",
      "Main effects training epoch: 284, train loss: 1.51222, val loss: 1.57465\n",
      "Main effects training epoch: 285, train loss: 1.50736, val loss: 1.58432\n",
      "Main effects training epoch: 286, train loss: 1.49892, val loss: 1.56898\n",
      "Main effects training epoch: 287, train loss: 1.50241, val loss: 1.58199\n",
      "Main effects training epoch: 288, train loss: 1.49695, val loss: 1.57269\n",
      "Main effects training epoch: 289, train loss: 1.49436, val loss: 1.56611\n",
      "Main effects training epoch: 290, train loss: 1.50664, val loss: 1.58591\n",
      "Main effects training epoch: 291, train loss: 1.50586, val loss: 1.58715\n",
      "Main effects training epoch: 292, train loss: 1.51494, val loss: 1.58830\n",
      "Main effects training epoch: 293, train loss: 1.50709, val loss: 1.58697\n",
      "Main effects training epoch: 294, train loss: 1.51325, val loss: 1.57977\n",
      "Main effects training epoch: 295, train loss: 1.51135, val loss: 1.57628\n",
      "Main effects training epoch: 296, train loss: 1.49420, val loss: 1.56582\n",
      "Main effects training epoch: 297, train loss: 1.50513, val loss: 1.56900\n",
      "Main effects training epoch: 298, train loss: 1.49463, val loss: 1.56271\n",
      "Main effects training epoch: 299, train loss: 1.51030, val loss: 1.60281\n",
      "Main effects training epoch: 300, train loss: 1.51242, val loss: 1.59064\n",
      "##########Stage 1: main effect training stop.##########\n",
      "3 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.51916, val loss: 1.58115\n",
      "Main effects tuning epoch: 2, train loss: 1.51716, val loss: 1.58629\n",
      "Main effects tuning epoch: 3, train loss: 1.52237, val loss: 1.58345\n",
      "Main effects tuning epoch: 4, train loss: 1.52530, val loss: 1.56763\n",
      "Main effects tuning epoch: 5, train loss: 1.52780, val loss: 1.58045\n",
      "Main effects tuning epoch: 6, train loss: 1.52363, val loss: 1.57730\n",
      "Main effects tuning epoch: 7, train loss: 1.51823, val loss: 1.57824\n",
      "Main effects tuning epoch: 8, train loss: 1.52181, val loss: 1.56849\n",
      "Main effects tuning epoch: 9, train loss: 1.52058, val loss: 1.57305\n",
      "Main effects tuning epoch: 10, train loss: 1.51199, val loss: 1.56406\n",
      "Main effects tuning epoch: 11, train loss: 1.51319, val loss: 1.56986\n",
      "Main effects tuning epoch: 12, train loss: 1.50988, val loss: 1.56614\n",
      "Main effects tuning epoch: 13, train loss: 1.51817, val loss: 1.57239\n",
      "Main effects tuning epoch: 14, train loss: 1.51475, val loss: 1.57909\n",
      "Main effects tuning epoch: 15, train loss: 1.51428, val loss: 1.56110\n",
      "Main effects tuning epoch: 16, train loss: 1.51181, val loss: 1.56498\n",
      "Main effects tuning epoch: 17, train loss: 1.51305, val loss: 1.56166\n",
      "Main effects tuning epoch: 18, train loss: 1.50845, val loss: 1.56363\n",
      "Main effects tuning epoch: 19, train loss: 1.51418, val loss: 1.57691\n",
      "Main effects tuning epoch: 20, train loss: 1.52258, val loss: 1.58071\n",
      "Main effects tuning epoch: 21, train loss: 1.51696, val loss: 1.57345\n",
      "Main effects tuning epoch: 22, train loss: 1.51919, val loss: 1.56665\n",
      "Main effects tuning epoch: 23, train loss: 1.51000, val loss: 1.57219\n",
      "Main effects tuning epoch: 24, train loss: 1.51595, val loss: 1.58089\n",
      "Main effects tuning epoch: 25, train loss: 1.51627, val loss: 1.56833\n",
      "Main effects tuning epoch: 26, train loss: 1.51359, val loss: 1.57507\n",
      "Main effects tuning epoch: 27, train loss: 1.50896, val loss: 1.55646\n",
      "Main effects tuning epoch: 28, train loss: 1.50899, val loss: 1.56328\n",
      "Main effects tuning epoch: 29, train loss: 1.52470, val loss: 1.57033\n",
      "Main effects tuning epoch: 30, train loss: 1.51458, val loss: 1.56305\n",
      "Main effects tuning epoch: 31, train loss: 1.51368, val loss: 1.57916\n",
      "Main effects tuning epoch: 32, train loss: 1.52479, val loss: 1.56499\n",
      "Main effects tuning epoch: 33, train loss: 1.50986, val loss: 1.58194\n",
      "Main effects tuning epoch: 34, train loss: 1.51597, val loss: 1.56815\n",
      "Main effects tuning epoch: 35, train loss: 1.52122, val loss: 1.58183\n",
      "Main effects tuning epoch: 36, train loss: 1.51486, val loss: 1.56553\n",
      "Main effects tuning epoch: 37, train loss: 1.51078, val loss: 1.55806\n",
      "Main effects tuning epoch: 38, train loss: 1.50750, val loss: 1.56260\n",
      "Main effects tuning epoch: 39, train loss: 1.51023, val loss: 1.57353\n",
      "Main effects tuning epoch: 40, train loss: 1.50538, val loss: 1.54659\n",
      "Main effects tuning epoch: 41, train loss: 1.50742, val loss: 1.56989\n",
      "Main effects tuning epoch: 42, train loss: 1.50751, val loss: 1.57024\n",
      "Main effects tuning epoch: 43, train loss: 1.50965, val loss: 1.56588\n",
      "Main effects tuning epoch: 44, train loss: 1.50569, val loss: 1.56855\n",
      "Main effects tuning epoch: 45, train loss: 1.50370, val loss: 1.56615\n",
      "Main effects tuning epoch: 46, train loss: 1.50684, val loss: 1.55539\n",
      "Main effects tuning epoch: 47, train loss: 1.50745, val loss: 1.56665\n",
      "Main effects tuning epoch: 48, train loss: 1.50780, val loss: 1.56301\n",
      "Main effects tuning epoch: 49, train loss: 1.50438, val loss: 1.55575\n",
      "Main effects tuning epoch: 50, train loss: 1.50466, val loss: 1.56869\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.38419, val loss: 1.42531\n",
      "Interaction training epoch: 2, train loss: 1.38921, val loss: 1.38145\n",
      "Interaction training epoch: 3, train loss: 1.07519, val loss: 1.06363\n",
      "Interaction training epoch: 4, train loss: 1.07548, val loss: 1.06523\n",
      "Interaction training epoch: 5, train loss: 1.05270, val loss: 1.06515\n",
      "Interaction training epoch: 6, train loss: 1.06395, val loss: 1.06469\n",
      "Interaction training epoch: 7, train loss: 1.02630, val loss: 1.00868\n",
      "Interaction training epoch: 8, train loss: 1.05409, val loss: 1.05996\n",
      "Interaction training epoch: 9, train loss: 1.02755, val loss: 1.01646\n",
      "Interaction training epoch: 10, train loss: 0.97995, val loss: 0.97999\n",
      "Interaction training epoch: 11, train loss: 0.98387, val loss: 0.99127\n",
      "Interaction training epoch: 12, train loss: 1.01649, val loss: 1.01133\n",
      "Interaction training epoch: 13, train loss: 0.96592, val loss: 0.95421\n",
      "Interaction training epoch: 14, train loss: 0.99509, val loss: 0.99202\n",
      "Interaction training epoch: 15, train loss: 0.92277, val loss: 0.93841\n",
      "Interaction training epoch: 16, train loss: 0.92396, val loss: 0.93260\n",
      "Interaction training epoch: 17, train loss: 0.94378, val loss: 0.93623\n",
      "Interaction training epoch: 18, train loss: 0.92454, val loss: 0.92999\n",
      "Interaction training epoch: 19, train loss: 0.92665, val loss: 0.94311\n",
      "Interaction training epoch: 20, train loss: 0.92726, val loss: 0.93349\n",
      "Interaction training epoch: 21, train loss: 0.91692, val loss: 0.93051\n",
      "Interaction training epoch: 22, train loss: 0.90584, val loss: 0.90440\n",
      "Interaction training epoch: 23, train loss: 0.90804, val loss: 0.89667\n",
      "Interaction training epoch: 24, train loss: 0.90118, val loss: 0.90716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 25, train loss: 0.90421, val loss: 0.89686\n",
      "Interaction training epoch: 26, train loss: 0.89706, val loss: 0.88644\n",
      "Interaction training epoch: 27, train loss: 0.90342, val loss: 0.90497\n",
      "Interaction training epoch: 28, train loss: 0.89618, val loss: 0.88752\n",
      "Interaction training epoch: 29, train loss: 0.88978, val loss: 0.89663\n",
      "Interaction training epoch: 30, train loss: 0.89665, val loss: 0.88834\n",
      "Interaction training epoch: 31, train loss: 0.89151, val loss: 0.88768\n",
      "Interaction training epoch: 32, train loss: 0.89509, val loss: 0.88512\n",
      "Interaction training epoch: 33, train loss: 0.88911, val loss: 0.88438\n",
      "Interaction training epoch: 34, train loss: 0.90517, val loss: 0.90084\n",
      "Interaction training epoch: 35, train loss: 0.89201, val loss: 0.90561\n",
      "Interaction training epoch: 36, train loss: 0.88964, val loss: 0.87570\n",
      "Interaction training epoch: 37, train loss: 0.88279, val loss: 0.88669\n",
      "Interaction training epoch: 38, train loss: 0.88121, val loss: 0.87657\n",
      "Interaction training epoch: 39, train loss: 0.88106, val loss: 0.88073\n",
      "Interaction training epoch: 40, train loss: 0.87271, val loss: 0.86344\n",
      "Interaction training epoch: 41, train loss: 0.86803, val loss: 0.87089\n",
      "Interaction training epoch: 42, train loss: 0.91041, val loss: 0.90955\n",
      "Interaction training epoch: 43, train loss: 0.87895, val loss: 0.87331\n",
      "Interaction training epoch: 44, train loss: 0.86692, val loss: 0.85804\n",
      "Interaction training epoch: 45, train loss: 0.85889, val loss: 0.86151\n",
      "Interaction training epoch: 46, train loss: 0.87112, val loss: 0.87115\n",
      "Interaction training epoch: 47, train loss: 0.87085, val loss: 0.87133\n",
      "Interaction training epoch: 48, train loss: 0.86347, val loss: 0.86517\n",
      "Interaction training epoch: 49, train loss: 0.86359, val loss: 0.85689\n",
      "Interaction training epoch: 50, train loss: 0.87612, val loss: 0.87718\n",
      "Interaction training epoch: 51, train loss: 0.86220, val loss: 0.86325\n",
      "Interaction training epoch: 52, train loss: 0.85672, val loss: 0.85517\n",
      "Interaction training epoch: 53, train loss: 0.86348, val loss: 0.85999\n",
      "Interaction training epoch: 54, train loss: 0.85953, val loss: 0.85956\n",
      "Interaction training epoch: 55, train loss: 0.85573, val loss: 0.85655\n",
      "Interaction training epoch: 56, train loss: 0.87794, val loss: 0.87348\n",
      "Interaction training epoch: 57, train loss: 0.86952, val loss: 0.87056\n",
      "Interaction training epoch: 58, train loss: 0.85352, val loss: 0.84879\n",
      "Interaction training epoch: 59, train loss: 0.85408, val loss: 0.84595\n",
      "Interaction training epoch: 60, train loss: 0.85057, val loss: 0.85308\n",
      "Interaction training epoch: 61, train loss: 0.85442, val loss: 0.85665\n",
      "Interaction training epoch: 62, train loss: 0.85364, val loss: 0.85012\n",
      "Interaction training epoch: 63, train loss: 0.85042, val loss: 0.85059\n",
      "Interaction training epoch: 64, train loss: 0.85713, val loss: 0.85907\n",
      "Interaction training epoch: 65, train loss: 0.85212, val loss: 0.84837\n",
      "Interaction training epoch: 66, train loss: 0.86305, val loss: 0.86643\n",
      "Interaction training epoch: 67, train loss: 0.85253, val loss: 0.84232\n",
      "Interaction training epoch: 68, train loss: 0.85390, val loss: 0.85794\n",
      "Interaction training epoch: 69, train loss: 0.84932, val loss: 0.85153\n",
      "Interaction training epoch: 70, train loss: 0.85064, val loss: 0.85536\n",
      "Interaction training epoch: 71, train loss: 0.86202, val loss: 0.86185\n",
      "Interaction training epoch: 72, train loss: 0.85025, val loss: 0.84662\n",
      "Interaction training epoch: 73, train loss: 0.85716, val loss: 0.84976\n",
      "Interaction training epoch: 74, train loss: 0.84035, val loss: 0.83992\n",
      "Interaction training epoch: 75, train loss: 0.85565, val loss: 0.86248\n",
      "Interaction training epoch: 76, train loss: 0.84931, val loss: 0.85383\n",
      "Interaction training epoch: 77, train loss: 0.84342, val loss: 0.84017\n",
      "Interaction training epoch: 78, train loss: 0.85172, val loss: 0.84982\n",
      "Interaction training epoch: 79, train loss: 0.84531, val loss: 0.85374\n",
      "Interaction training epoch: 80, train loss: 0.85383, val loss: 0.85742\n",
      "Interaction training epoch: 81, train loss: 0.85493, val loss: 0.84143\n",
      "Interaction training epoch: 82, train loss: 0.84798, val loss: 0.84171\n",
      "Interaction training epoch: 83, train loss: 0.83906, val loss: 0.84147\n",
      "Interaction training epoch: 84, train loss: 0.86029, val loss: 0.85699\n",
      "Interaction training epoch: 85, train loss: 0.84863, val loss: 0.85272\n",
      "Interaction training epoch: 86, train loss: 0.86463, val loss: 0.85009\n",
      "Interaction training epoch: 87, train loss: 0.84698, val loss: 0.84594\n",
      "Interaction training epoch: 88, train loss: 0.84725, val loss: 0.84545\n",
      "Interaction training epoch: 89, train loss: 0.84576, val loss: 0.83704\n",
      "Interaction training epoch: 90, train loss: 0.84120, val loss: 0.84579\n",
      "Interaction training epoch: 91, train loss: 0.84370, val loss: 0.83420\n",
      "Interaction training epoch: 92, train loss: 0.84067, val loss: 0.83231\n",
      "Interaction training epoch: 93, train loss: 0.84020, val loss: 0.83579\n",
      "Interaction training epoch: 94, train loss: 0.84501, val loss: 0.84274\n",
      "Interaction training epoch: 95, train loss: 0.83698, val loss: 0.83883\n",
      "Interaction training epoch: 96, train loss: 0.83650, val loss: 0.83688\n",
      "Interaction training epoch: 97, train loss: 0.84196, val loss: 0.84387\n",
      "Interaction training epoch: 98, train loss: 0.83455, val loss: 0.82822\n",
      "Interaction training epoch: 99, train loss: 0.84014, val loss: 0.84548\n",
      "Interaction training epoch: 100, train loss: 0.83711, val loss: 0.83329\n",
      "Interaction training epoch: 101, train loss: 0.84639, val loss: 0.84360\n",
      "Interaction training epoch: 102, train loss: 0.83152, val loss: 0.83521\n",
      "Interaction training epoch: 103, train loss: 0.84425, val loss: 0.84966\n",
      "Interaction training epoch: 104, train loss: 0.83620, val loss: 0.83342\n",
      "Interaction training epoch: 105, train loss: 0.83776, val loss: 0.83407\n",
      "Interaction training epoch: 106, train loss: 0.83400, val loss: 0.82633\n",
      "Interaction training epoch: 107, train loss: 0.83408, val loss: 0.82483\n",
      "Interaction training epoch: 108, train loss: 0.83355, val loss: 0.83454\n",
      "Interaction training epoch: 109, train loss: 0.83870, val loss: 0.82780\n",
      "Interaction training epoch: 110, train loss: 0.83538, val loss: 0.83501\n",
      "Interaction training epoch: 111, train loss: 0.83681, val loss: 0.83541\n",
      "Interaction training epoch: 112, train loss: 0.84302, val loss: 0.85155\n",
      "Interaction training epoch: 113, train loss: 0.83156, val loss: 0.83483\n",
      "Interaction training epoch: 114, train loss: 0.84083, val loss: 0.84850\n",
      "Interaction training epoch: 115, train loss: 0.83851, val loss: 0.83433\n",
      "Interaction training epoch: 116, train loss: 0.83319, val loss: 0.83669\n",
      "Interaction training epoch: 117, train loss: 0.84457, val loss: 0.83410\n",
      "Interaction training epoch: 118, train loss: 0.83629, val loss: 0.84222\n",
      "Interaction training epoch: 119, train loss: 0.83334, val loss: 0.83147\n",
      "Interaction training epoch: 120, train loss: 0.83701, val loss: 0.82365\n",
      "Interaction training epoch: 121, train loss: 0.84348, val loss: 0.84200\n",
      "Interaction training epoch: 122, train loss: 0.84037, val loss: 0.83802\n",
      "Interaction training epoch: 123, train loss: 0.83178, val loss: 0.83446\n",
      "Interaction training epoch: 124, train loss: 0.82621, val loss: 0.82358\n",
      "Interaction training epoch: 125, train loss: 0.83305, val loss: 0.82931\n",
      "Interaction training epoch: 126, train loss: 0.83001, val loss: 0.82584\n",
      "Interaction training epoch: 127, train loss: 0.82930, val loss: 0.82738\n",
      "Interaction training epoch: 128, train loss: 0.85138, val loss: 0.84930\n",
      "Interaction training epoch: 129, train loss: 0.83773, val loss: 0.83739\n",
      "Interaction training epoch: 130, train loss: 0.84724, val loss: 0.84746\n",
      "Interaction training epoch: 131, train loss: 0.82784, val loss: 0.83355\n",
      "Interaction training epoch: 132, train loss: 0.83635, val loss: 0.82840\n",
      "Interaction training epoch: 133, train loss: 0.84250, val loss: 0.83900\n",
      "Interaction training epoch: 134, train loss: 0.82444, val loss: 0.82276\n",
      "Interaction training epoch: 135, train loss: 0.84368, val loss: 0.84946\n",
      "Interaction training epoch: 136, train loss: 0.83654, val loss: 0.83164\n",
      "Interaction training epoch: 137, train loss: 0.83237, val loss: 0.82364\n",
      "Interaction training epoch: 138, train loss: 0.83030, val loss: 0.82912\n",
      "Interaction training epoch: 139, train loss: 0.83174, val loss: 0.83696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 140, train loss: 0.83451, val loss: 0.82703\n",
      "Interaction training epoch: 141, train loss: 0.83297, val loss: 0.82207\n",
      "Interaction training epoch: 142, train loss: 0.83164, val loss: 0.83785\n",
      "Interaction training epoch: 143, train loss: 0.82929, val loss: 0.82325\n",
      "Interaction training epoch: 144, train loss: 0.82868, val loss: 0.81810\n",
      "Interaction training epoch: 145, train loss: 0.83301, val loss: 0.83259\n",
      "Interaction training epoch: 146, train loss: 0.82437, val loss: 0.82251\n",
      "Interaction training epoch: 147, train loss: 0.83493, val loss: 0.82476\n",
      "Interaction training epoch: 148, train loss: 0.82870, val loss: 0.83168\n",
      "Interaction training epoch: 149, train loss: 0.82939, val loss: 0.83278\n",
      "Interaction training epoch: 150, train loss: 0.83993, val loss: 0.84097\n",
      "Interaction training epoch: 151, train loss: 0.82699, val loss: 0.81911\n",
      "Interaction training epoch: 152, train loss: 0.83672, val loss: 0.83587\n",
      "Interaction training epoch: 153, train loss: 0.83517, val loss: 0.82850\n",
      "Interaction training epoch: 154, train loss: 0.82761, val loss: 0.83041\n",
      "Interaction training epoch: 155, train loss: 0.82547, val loss: 0.82518\n",
      "Interaction training epoch: 156, train loss: 0.83271, val loss: 0.82541\n",
      "Interaction training epoch: 157, train loss: 0.82498, val loss: 0.82870\n",
      "Interaction training epoch: 158, train loss: 0.83510, val loss: 0.82566\n",
      "Interaction training epoch: 159, train loss: 0.83538, val loss: 0.82193\n",
      "Interaction training epoch: 160, train loss: 0.82813, val loss: 0.82789\n",
      "Interaction training epoch: 161, train loss: 0.83363, val loss: 0.83591\n",
      "Interaction training epoch: 162, train loss: 0.83400, val loss: 0.82610\n",
      "Interaction training epoch: 163, train loss: 0.83225, val loss: 0.83573\n",
      "Interaction training epoch: 164, train loss: 0.83690, val loss: 0.83646\n",
      "Interaction training epoch: 165, train loss: 0.83549, val loss: 0.83119\n",
      "Interaction training epoch: 166, train loss: 0.82898, val loss: 0.82732\n",
      "Interaction training epoch: 167, train loss: 0.83935, val loss: 0.83817\n",
      "Interaction training epoch: 168, train loss: 0.82740, val loss: 0.82413\n",
      "Interaction training epoch: 169, train loss: 0.83091, val loss: 0.82242\n",
      "Interaction training epoch: 170, train loss: 0.82093, val loss: 0.82143\n",
      "Interaction training epoch: 171, train loss: 0.83097, val loss: 0.82688\n",
      "Interaction training epoch: 172, train loss: 0.82920, val loss: 0.82592\n",
      "Interaction training epoch: 173, train loss: 0.82456, val loss: 0.82743\n",
      "Interaction training epoch: 174, train loss: 0.82763, val loss: 0.82253\n",
      "Interaction training epoch: 175, train loss: 0.82587, val loss: 0.82162\n",
      "Interaction training epoch: 176, train loss: 0.82407, val loss: 0.82615\n",
      "Interaction training epoch: 177, train loss: 0.82336, val loss: 0.82342\n",
      "Interaction training epoch: 178, train loss: 0.82301, val loss: 0.82463\n",
      "Interaction training epoch: 179, train loss: 0.82510, val loss: 0.82204\n",
      "Interaction training epoch: 180, train loss: 0.82700, val loss: 0.82540\n",
      "Interaction training epoch: 181, train loss: 0.83027, val loss: 0.81846\n",
      "Interaction training epoch: 182, train loss: 0.82680, val loss: 0.82973\n",
      "Interaction training epoch: 183, train loss: 0.83507, val loss: 0.83151\n",
      "Interaction training epoch: 184, train loss: 0.82488, val loss: 0.81990\n",
      "Interaction training epoch: 185, train loss: 0.83264, val loss: 0.83094\n",
      "Interaction training epoch: 186, train loss: 0.82440, val loss: 0.82071\n",
      "Interaction training epoch: 187, train loss: 0.82292, val loss: 0.82025\n",
      "Interaction training epoch: 188, train loss: 0.82477, val loss: 0.81875\n",
      "Interaction training epoch: 189, train loss: 0.82379, val loss: 0.81023\n",
      "Interaction training epoch: 190, train loss: 0.82926, val loss: 0.83036\n",
      "Interaction training epoch: 191, train loss: 0.81931, val loss: 0.81165\n",
      "Interaction training epoch: 192, train loss: 0.83252, val loss: 0.82635\n",
      "Interaction training epoch: 193, train loss: 0.82388, val loss: 0.82146\n",
      "Interaction training epoch: 194, train loss: 0.82796, val loss: 0.82162\n",
      "Interaction training epoch: 195, train loss: 0.82211, val loss: 0.81750\n",
      "Interaction training epoch: 196, train loss: 0.83229, val loss: 0.83695\n",
      "Interaction training epoch: 197, train loss: 0.82471, val loss: 0.82261\n",
      "Interaction training epoch: 198, train loss: 0.82366, val loss: 0.81792\n",
      "Interaction training epoch: 199, train loss: 0.82397, val loss: 0.81594\n",
      "Interaction training epoch: 200, train loss: 0.82445, val loss: 0.81740\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########1 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.82823, val loss: 0.82847\n",
      "Interaction tuning epoch: 2, train loss: 0.82624, val loss: 0.82478\n",
      "Interaction tuning epoch: 3, train loss: 0.82055, val loss: 0.82475\n",
      "Interaction tuning epoch: 4, train loss: 0.82577, val loss: 0.81595\n",
      "Interaction tuning epoch: 5, train loss: 0.82579, val loss: 0.82960\n",
      "Interaction tuning epoch: 6, train loss: 0.82605, val loss: 0.82253\n",
      "Interaction tuning epoch: 7, train loss: 0.83087, val loss: 0.83490\n",
      "Interaction tuning epoch: 8, train loss: 0.81953, val loss: 0.82363\n",
      "Interaction tuning epoch: 9, train loss: 0.83179, val loss: 0.82401\n",
      "Interaction tuning epoch: 10, train loss: 0.82916, val loss: 0.83002\n",
      "Interaction tuning epoch: 11, train loss: 0.83033, val loss: 0.82333\n",
      "Interaction tuning epoch: 12, train loss: 0.84005, val loss: 0.84769\n",
      "Interaction tuning epoch: 13, train loss: 0.83326, val loss: 0.82541\n",
      "Interaction tuning epoch: 14, train loss: 0.83025, val loss: 0.82759\n",
      "Interaction tuning epoch: 15, train loss: 0.85222, val loss: 0.84691\n",
      "Interaction tuning epoch: 16, train loss: 0.82412, val loss: 0.81565\n",
      "Interaction tuning epoch: 17, train loss: 0.82435, val loss: 0.82565\n",
      "Interaction tuning epoch: 18, train loss: 0.82878, val loss: 0.82402\n",
      "Interaction tuning epoch: 19, train loss: 0.82235, val loss: 0.82085\n",
      "Interaction tuning epoch: 20, train loss: 0.82450, val loss: 0.82115\n",
      "Interaction tuning epoch: 21, train loss: 0.82601, val loss: 0.82197\n",
      "Interaction tuning epoch: 22, train loss: 0.81968, val loss: 0.82159\n",
      "Interaction tuning epoch: 23, train loss: 0.82752, val loss: 0.82388\n",
      "Interaction tuning epoch: 24, train loss: 0.82642, val loss: 0.82904\n",
      "Interaction tuning epoch: 25, train loss: 0.82354, val loss: 0.82977\n",
      "Interaction tuning epoch: 26, train loss: 0.82821, val loss: 0.82360\n",
      "Interaction tuning epoch: 27, train loss: 0.82446, val loss: 0.82208\n",
      "Interaction tuning epoch: 28, train loss: 0.82573, val loss: 0.82325\n",
      "Interaction tuning epoch: 29, train loss: 0.82504, val loss: 0.82422\n",
      "Interaction tuning epoch: 30, train loss: 0.82584, val loss: 0.82595\n",
      "Interaction tuning epoch: 31, train loss: 0.82202, val loss: 0.81377\n",
      "Interaction tuning epoch: 32, train loss: 0.82944, val loss: 0.82835\n",
      "Interaction tuning epoch: 33, train loss: 0.83598, val loss: 0.82700\n",
      "Interaction tuning epoch: 34, train loss: 0.82502, val loss: 0.82130\n",
      "Interaction tuning epoch: 35, train loss: 0.82260, val loss: 0.82173\n",
      "Interaction tuning epoch: 36, train loss: 0.82351, val loss: 0.81735\n",
      "Interaction tuning epoch: 37, train loss: 0.82653, val loss: 0.82469\n",
      "Interaction tuning epoch: 38, train loss: 0.82489, val loss: 0.82369\n",
      "Interaction tuning epoch: 39, train loss: 0.82442, val loss: 0.82374\n",
      "Interaction tuning epoch: 40, train loss: 0.82017, val loss: 0.81793\n",
      "Interaction tuning epoch: 41, train loss: 0.82907, val loss: 0.82834\n",
      "Interaction tuning epoch: 42, train loss: 0.82046, val loss: 0.81732\n",
      "Interaction tuning epoch: 43, train loss: 0.82363, val loss: 0.82198\n",
      "Interaction tuning epoch: 44, train loss: 0.82655, val loss: 0.82206\n",
      "Interaction tuning epoch: 45, train loss: 0.82857, val loss: 0.82253\n",
      "Interaction tuning epoch: 46, train loss: 0.82151, val loss: 0.82046\n",
      "Interaction tuning epoch: 47, train loss: 0.82235, val loss: 0.82533\n",
      "Interaction tuning epoch: 48, train loss: 0.82286, val loss: 0.82052\n",
      "Interaction tuning epoch: 49, train loss: 0.82600, val loss: 0.82682\n",
      "Interaction tuning epoch: 50, train loss: 0.82595, val loss: 0.82062\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 42.07396697998047\n",
      "After the gam stage, training error is 0.82595 , validation error is 0.82062\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 20.889507\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.681937 validation MAE=0.776484,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.633563 validation MAE=0.756914,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 3: observed MAE=0.592941 validation MAE=0.739168,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.558101 validation MAE=0.723212,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.528491 validation MAE=0.708950,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.503023 validation MAE=0.695950,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.480026 validation MAE=0.684115,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.460036 validation MAE=0.672652,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.441378 validation MAE=0.662285,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.425255 validation MAE=0.652991,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.410334 validation MAE=0.644776,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.400163 validation MAE=0.638443,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.390581 validation MAE=0.631928,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.381571 validation MAE=0.625896,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.371887 validation MAE=0.620036,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.362843 validation MAE=0.614753,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.353774 validation MAE=0.610158,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.346910 validation MAE=0.605938,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.340324 validation MAE=0.601930,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.333618 validation MAE=0.598367,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.327288 validation MAE=0.594497,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.322499 validation MAE=0.590948,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.317649 validation MAE=0.588621,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.312837 validation MAE=0.585550,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.308639 validation MAE=0.583140,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.303948 validation MAE=0.579982,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.299829 validation MAE=0.577608,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.296037 validation MAE=0.575360,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.292425 validation MAE=0.573644,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.289857 validation MAE=0.572325,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.286280 validation MAE=0.570937,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.282777 validation MAE=0.569070,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.280199 validation MAE=0.567656,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.277936 validation MAE=0.565884,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.274857 validation MAE=0.564514,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.272605 validation MAE=0.563390,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.269752 validation MAE=0.562282,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.268198 validation MAE=0.561066,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.265736 validation MAE=0.560294,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.263759 validation MAE=0.559630,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.261565 validation MAE=0.558796,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.259453 validation MAE=0.557555,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.257526 validation MAE=0.556659,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.256181 validation MAE=0.556195,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.254615 validation MAE=0.555250,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.253120 validation MAE=0.554843,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.251102 validation MAE=0.553604,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.249540 validation MAE=0.553164,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.248246 validation MAE=0.552377,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.247391 validation MAE=0.551907,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.245796 validation MAE=0.551065,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.244591 validation MAE=0.550915,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.243093 validation MAE=0.550389,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.242121 validation MAE=0.550357,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.241011 validation MAE=0.548756,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.239791 validation MAE=0.548945,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.238836 validation MAE=0.548613,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.237746 validation MAE=0.547886,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.235985 validation MAE=0.546929,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.235251 validation MAE=0.546698,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.234267 validation MAE=0.546340,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.234200 validation MAE=0.546305,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.233070 validation MAE=0.545815,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.232048 validation MAE=0.545301,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.231494 validation MAE=0.544697,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.230357 validation MAE=0.544141,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.229507 validation MAE=0.543823,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.228854 validation MAE=0.543354,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.227608 validation MAE=0.543120,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.227402 validation MAE=0.543090,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.226210 validation MAE=0.542231,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.225819 validation MAE=0.541782,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.224321 validation MAE=0.541060,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.224203 validation MAE=0.541090,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.223625 validation MAE=0.540884,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.222559 validation MAE=0.540424,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.222537 validation MAE=0.539579,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.222028 validation MAE=0.539529,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.221127 validation MAE=0.539155,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.220545 validation MAE=0.539335,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.219741 validation MAE=0.538911,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.219220 validation MAE=0.537978,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.218600 validation MAE=0.538014,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.218161 validation MAE=0.537587,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.217315 validation MAE=0.536848,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.216986 validation MAE=0.536807,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.216541 validation MAE=0.536421,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.215673 validation MAE=0.536190,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.215540 validation MAE=0.536194,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.215014 validation MAE=0.535700,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.214494 validation MAE=0.534890,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.213971 validation MAE=0.534744,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.213832 validation MAE=0.534682,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.213024 validation MAE=0.534237,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.212715 validation MAE=0.534197,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.212129 validation MAE=0.533891,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.211958 validation MAE=0.533994,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.211452 validation MAE=0.533332,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.210811 validation MAE=0.532965,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.210373 validation MAE=0.532926,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.209805 validation MAE=0.532195,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.209599 validation MAE=0.532482,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.209034 validation MAE=0.531799,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.208601 validation MAE=0.531455,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.208329 validation MAE=0.531528,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.208223 validation MAE=0.530690,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.207842 validation MAE=0.530618,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.207475 validation MAE=0.530134,rank=5\n",
      "[SoftImpute] Iter 109: observed MAE=0.206904 validation MAE=0.529740,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.206466 validation MAE=0.529686,rank=5\n",
      "[SoftImpute] Iter 111: observed MAE=0.206345 validation MAE=0.529579,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.205745 validation MAE=0.529437,rank=5\n",
      "[SoftImpute] Iter 113: observed MAE=0.205221 validation MAE=0.528781,rank=5\n",
      "[SoftImpute] Iter 114: observed MAE=0.204596 validation MAE=0.528564,rank=5\n",
      "[SoftImpute] Iter 115: observed MAE=0.204589 validation MAE=0.527955,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.204665 validation MAE=0.528356,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.204562 validation MAE=0.528409,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.204003 validation MAE=0.527718,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 119: observed MAE=0.203372 validation MAE=0.526982,rank=5\n",
      "[SoftImpute] Iter 120: observed MAE=0.203316 validation MAE=0.527051,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.202672 validation MAE=0.526784,rank=5\n",
      "[SoftImpute] Iter 122: observed MAE=0.202612 validation MAE=0.526659,rank=5\n",
      "[SoftImpute] Iter 123: observed MAE=0.202148 validation MAE=0.526155,rank=5\n",
      "[SoftImpute] Iter 124: observed MAE=0.201795 validation MAE=0.526364,rank=5\n",
      "[SoftImpute] Iter 125: observed MAE=0.201603 validation MAE=0.525972,rank=5\n",
      "[SoftImpute] Iter 126: observed MAE=0.201208 validation MAE=0.525366,rank=5\n",
      "[SoftImpute] Iter 127: observed MAE=0.201130 validation MAE=0.525221,rank=5\n",
      "[SoftImpute] Iter 128: observed MAE=0.201127 validation MAE=0.524819,rank=5\n",
      "[SoftImpute] Iter 129: observed MAE=0.200608 validation MAE=0.524959,rank=5\n",
      "[SoftImpute] Iter 130: observed MAE=0.200395 validation MAE=0.524958,rank=5\n",
      "[SoftImpute] Iter 131: observed MAE=0.199969 validation MAE=0.524379,rank=5\n",
      "[SoftImpute] Iter 132: observed MAE=0.199881 validation MAE=0.524021,rank=5\n",
      "[SoftImpute] Iter 133: observed MAE=0.199244 validation MAE=0.523875,rank=5\n",
      "[SoftImpute] Iter 134: observed MAE=0.198970 validation MAE=0.523328,rank=5\n",
      "[SoftImpute] Iter 135: observed MAE=0.198865 validation MAE=0.523339,rank=5\n",
      "[SoftImpute] Iter 136: observed MAE=0.198600 validation MAE=0.523194,rank=5\n",
      "[SoftImpute] Iter 137: observed MAE=0.198384 validation MAE=0.522927,rank=5\n",
      "[SoftImpute] Iter 138: observed MAE=0.198277 validation MAE=0.522616,rank=5\n",
      "[SoftImpute] Iter 139: observed MAE=0.198246 validation MAE=0.522251,rank=5\n",
      "[SoftImpute] Iter 140: observed MAE=0.197905 validation MAE=0.521757,rank=5\n",
      "[SoftImpute] Iter 141: observed MAE=0.197904 validation MAE=0.521642,rank=5\n",
      "[SoftImpute] Iter 142: observed MAE=0.197485 validation MAE=0.522525,rank=5\n",
      "[SoftImpute] Iter 143: observed MAE=0.197207 validation MAE=0.521990,rank=5\n",
      "[SoftImpute] Iter 144: observed MAE=0.196730 validation MAE=0.521448,rank=5\n",
      "[SoftImpute] Iter 145: observed MAE=0.195944 validation MAE=0.520877,rank=5\n",
      "[SoftImpute] Iter 146: observed MAE=0.196057 validation MAE=0.520372,rank=5\n",
      "[SoftImpute] Iter 147: observed MAE=0.196204 validation MAE=0.520783,rank=5\n",
      "[SoftImpute] Iter 148: observed MAE=0.195859 validation MAE=0.520215,rank=5\n",
      "[SoftImpute] Iter 149: observed MAE=0.195810 validation MAE=0.519764,rank=5\n",
      "[SoftImpute] Iter 150: observed MAE=0.195473 validation MAE=0.519370,rank=5\n",
      "[SoftImpute] Iter 151: observed MAE=0.195341 validation MAE=0.518689,rank=5\n",
      "[SoftImpute] Iter 152: observed MAE=0.195041 validation MAE=0.519364,rank=5\n",
      "[SoftImpute] Iter 153: observed MAE=0.195080 validation MAE=0.519092,rank=5\n",
      "[SoftImpute] Iter 154: observed MAE=0.194713 validation MAE=0.518724,rank=5\n",
      "[SoftImpute] Iter 155: observed MAE=0.194937 validation MAE=0.518663,rank=5\n",
      "[SoftImpute] Iter 156: observed MAE=0.194605 validation MAE=0.518407,rank=5\n",
      "[SoftImpute] Iter 157: observed MAE=0.194143 validation MAE=0.518581,rank=5\n",
      "[SoftImpute] Iter 158: observed MAE=0.194136 validation MAE=0.518328,rank=5\n",
      "[SoftImpute] Iter 159: observed MAE=0.193636 validation MAE=0.518016,rank=5\n",
      "[SoftImpute] Iter 160: observed MAE=0.193405 validation MAE=0.517987,rank=5\n",
      "[SoftImpute] Iter 161: observed MAE=0.193541 validation MAE=0.517117,rank=5\n",
      "[SoftImpute] Iter 162: observed MAE=0.193466 validation MAE=0.517993,rank=5\n",
      "[SoftImpute] Iter 163: observed MAE=0.193233 validation MAE=0.517614,rank=5\n",
      "[SoftImpute] Iter 164: observed MAE=0.193151 validation MAE=0.516559,rank=5\n",
      "[SoftImpute] Iter 165: observed MAE=0.192810 validation MAE=0.516601,rank=5\n",
      "[SoftImpute] Iter 166: observed MAE=0.192692 validation MAE=0.516521,rank=5\n",
      "[SoftImpute] Iter 167: observed MAE=0.192408 validation MAE=0.516316,rank=5\n",
      "[SoftImpute] Iter 168: observed MAE=0.192444 validation MAE=0.516662,rank=5\n",
      "[SoftImpute] Iter 169: observed MAE=0.191919 validation MAE=0.515902,rank=5\n",
      "[SoftImpute] Iter 170: observed MAE=0.191911 validation MAE=0.515695,rank=5\n",
      "[SoftImpute] Iter 171: observed MAE=0.191511 validation MAE=0.515946,rank=5\n",
      "[SoftImpute] Iter 172: observed MAE=0.191289 validation MAE=0.516001,rank=5\n",
      "[SoftImpute] Iter 173: observed MAE=0.191526 validation MAE=0.515378,rank=5\n",
      "[SoftImpute] Iter 174: observed MAE=0.191277 validation MAE=0.514890,rank=5\n",
      "[SoftImpute] Iter 175: observed MAE=0.190747 validation MAE=0.515209,rank=5\n",
      "[SoftImpute] Iter 176: observed MAE=0.191350 validation MAE=0.515051,rank=5\n",
      "[SoftImpute] Iter 177: observed MAE=0.190795 validation MAE=0.514906,rank=5\n",
      "[SoftImpute] Iter 178: observed MAE=0.190676 validation MAE=0.514695,rank=5\n",
      "[SoftImpute] Iter 179: observed MAE=0.190646 validation MAE=0.514604,rank=5\n",
      "[SoftImpute] Iter 180: observed MAE=0.190140 validation MAE=0.514610,rank=5\n",
      "[SoftImpute] Iter 181: observed MAE=0.190024 validation MAE=0.514454,rank=5\n",
      "[SoftImpute] Iter 182: observed MAE=0.190161 validation MAE=0.513429,rank=5\n",
      "[SoftImpute] Iter 183: observed MAE=0.189837 validation MAE=0.513924,rank=5\n",
      "[SoftImpute] Iter 184: observed MAE=0.189525 validation MAE=0.514208,rank=5\n",
      "[SoftImpute] Iter 185: observed MAE=0.189252 validation MAE=0.514053,rank=5\n",
      "[SoftImpute] Iter 186: observed MAE=0.189366 validation MAE=0.513801,rank=5\n",
      "[SoftImpute] Iter 187: observed MAE=0.189278 validation MAE=0.513231,rank=5\n",
      "[SoftImpute] Iter 188: observed MAE=0.189160 validation MAE=0.513033,rank=5\n",
      "[SoftImpute] Iter 189: observed MAE=0.189091 validation MAE=0.513303,rank=5\n",
      "[SoftImpute] Iter 190: observed MAE=0.189245 validation MAE=0.513220,rank=5\n",
      "[SoftImpute] Iter 191: observed MAE=0.188855 validation MAE=0.512576,rank=5\n",
      "[SoftImpute] Iter 192: observed MAE=0.188778 validation MAE=0.512042,rank=5\n",
      "[SoftImpute] Iter 193: observed MAE=0.188624 validation MAE=0.511787,rank=5\n",
      "[SoftImpute] Iter 194: observed MAE=0.188152 validation MAE=0.511701,rank=5\n",
      "[SoftImpute] Iter 195: observed MAE=0.188338 validation MAE=0.512133,rank=5\n",
      "[SoftImpute] Iter 196: observed MAE=0.188392 validation MAE=0.511822,rank=5\n",
      "[SoftImpute] Iter 197: observed MAE=0.187848 validation MAE=0.511998,rank=5\n",
      "[SoftImpute] Iter 198: observed MAE=0.187897 validation MAE=0.511432,rank=5\n",
      "[SoftImpute] Iter 199: observed MAE=0.187686 validation MAE=0.511095,rank=5\n",
      "[SoftImpute] Iter 200: observed MAE=0.187530 validation MAE=0.511103,rank=5\n",
      "[SoftImpute] Stopped after iteration 200 for lambda=0.417790\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 5.6233439445495605\n",
      "After the matrix factor stage, training error is 0.18753, validation error is 0.51110\n",
      "7\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.06378, val loss: 4.19074\n",
      "Main effects training epoch: 2, train loss: 3.90191, val loss: 4.02538\n",
      "Main effects training epoch: 3, train loss: 3.67542, val loss: 3.81714\n",
      "Main effects training epoch: 4, train loss: 3.50578, val loss: 3.66860\n",
      "Main effects training epoch: 5, train loss: 3.45350, val loss: 3.60213\n",
      "Main effects training epoch: 6, train loss: 3.38836, val loss: 3.50593\n",
      "Main effects training epoch: 7, train loss: 3.17657, val loss: 3.25175\n",
      "Main effects training epoch: 8, train loss: 3.30225, val loss: 3.37744\n",
      "Main effects training epoch: 9, train loss: 3.24797, val loss: 3.32684\n",
      "Main effects training epoch: 10, train loss: 3.18280, val loss: 3.27015\n",
      "Main effects training epoch: 11, train loss: 3.11495, val loss: 3.21061\n",
      "Main effects training epoch: 12, train loss: 3.04873, val loss: 3.13932\n",
      "Main effects training epoch: 13, train loss: 3.00426, val loss: 3.08955\n",
      "Main effects training epoch: 14, train loss: 2.94394, val loss: 3.03347\n",
      "Main effects training epoch: 15, train loss: 2.87501, val loss: 2.95977\n",
      "Main effects training epoch: 16, train loss: 2.78482, val loss: 2.86683\n",
      "Main effects training epoch: 17, train loss: 2.73051, val loss: 2.79662\n",
      "Main effects training epoch: 18, train loss: 2.66125, val loss: 2.72023\n",
      "Main effects training epoch: 19, train loss: 2.62185, val loss: 2.67801\n",
      "Main effects training epoch: 20, train loss: 2.60435, val loss: 2.67076\n",
      "Main effects training epoch: 21, train loss: 2.51597, val loss: 2.58904\n",
      "Main effects training epoch: 22, train loss: 2.47380, val loss: 2.54489\n",
      "Main effects training epoch: 23, train loss: 2.43493, val loss: 2.50914\n",
      "Main effects training epoch: 24, train loss: 2.41444, val loss: 2.49707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 25, train loss: 2.34451, val loss: 2.42025\n",
      "Main effects training epoch: 26, train loss: 2.31292, val loss: 2.38216\n",
      "Main effects training epoch: 27, train loss: 2.29757, val loss: 2.37500\n",
      "Main effects training epoch: 28, train loss: 2.26204, val loss: 2.33722\n",
      "Main effects training epoch: 29, train loss: 2.21261, val loss: 2.29367\n",
      "Main effects training epoch: 30, train loss: 2.21180, val loss: 2.29162\n",
      "Main effects training epoch: 31, train loss: 2.15895, val loss: 2.24114\n",
      "Main effects training epoch: 32, train loss: 2.16646, val loss: 2.24916\n",
      "Main effects training epoch: 33, train loss: 2.12970, val loss: 2.20868\n",
      "Main effects training epoch: 34, train loss: 2.09335, val loss: 2.17294\n",
      "Main effects training epoch: 35, train loss: 2.09382, val loss: 2.17675\n",
      "Main effects training epoch: 36, train loss: 2.06738, val loss: 2.14112\n",
      "Main effects training epoch: 37, train loss: 2.01868, val loss: 2.09695\n",
      "Main effects training epoch: 38, train loss: 2.01391, val loss: 2.09813\n",
      "Main effects training epoch: 39, train loss: 1.99195, val loss: 2.06342\n",
      "Main effects training epoch: 40, train loss: 1.99457, val loss: 2.06918\n",
      "Main effects training epoch: 41, train loss: 1.95290, val loss: 2.03209\n",
      "Main effects training epoch: 42, train loss: 1.94533, val loss: 2.02084\n",
      "Main effects training epoch: 43, train loss: 1.95085, val loss: 2.02469\n",
      "Main effects training epoch: 44, train loss: 1.91994, val loss: 1.99295\n",
      "Main effects training epoch: 45, train loss: 1.89472, val loss: 1.96681\n",
      "Main effects training epoch: 46, train loss: 1.89734, val loss: 1.97052\n",
      "Main effects training epoch: 47, train loss: 1.90080, val loss: 1.97656\n",
      "Main effects training epoch: 48, train loss: 1.86906, val loss: 1.93907\n",
      "Main effects training epoch: 49, train loss: 1.86608, val loss: 1.93955\n",
      "Main effects training epoch: 50, train loss: 1.85954, val loss: 1.92990\n",
      "Main effects training epoch: 51, train loss: 1.85314, val loss: 1.92377\n",
      "Main effects training epoch: 52, train loss: 1.83561, val loss: 1.90956\n",
      "Main effects training epoch: 53, train loss: 1.82099, val loss: 1.88645\n",
      "Main effects training epoch: 54, train loss: 1.82205, val loss: 1.89192\n",
      "Main effects training epoch: 55, train loss: 1.81214, val loss: 1.88707\n",
      "Main effects training epoch: 56, train loss: 1.79571, val loss: 1.86296\n",
      "Main effects training epoch: 57, train loss: 1.78702, val loss: 1.85249\n",
      "Main effects training epoch: 58, train loss: 1.79353, val loss: 1.86052\n",
      "Main effects training epoch: 59, train loss: 1.77295, val loss: 1.83898\n",
      "Main effects training epoch: 60, train loss: 1.78185, val loss: 1.84749\n",
      "Main effects training epoch: 61, train loss: 1.77634, val loss: 1.84152\n",
      "Main effects training epoch: 62, train loss: 1.75440, val loss: 1.81893\n",
      "Main effects training epoch: 63, train loss: 1.76864, val loss: 1.83187\n",
      "Main effects training epoch: 64, train loss: 1.75153, val loss: 1.80992\n",
      "Main effects training epoch: 65, train loss: 1.75805, val loss: 1.82137\n",
      "Main effects training epoch: 66, train loss: 1.74828, val loss: 1.80811\n",
      "Main effects training epoch: 67, train loss: 1.75058, val loss: 1.81228\n",
      "Main effects training epoch: 68, train loss: 1.73969, val loss: 1.79817\n",
      "Main effects training epoch: 69, train loss: 1.73153, val loss: 1.79256\n",
      "Main effects training epoch: 70, train loss: 1.73465, val loss: 1.79701\n",
      "Main effects training epoch: 71, train loss: 1.72854, val loss: 1.78090\n",
      "Main effects training epoch: 72, train loss: 1.72913, val loss: 1.79103\n",
      "Main effects training epoch: 73, train loss: 1.72758, val loss: 1.78366\n",
      "Main effects training epoch: 74, train loss: 1.72449, val loss: 1.78122\n",
      "Main effects training epoch: 75, train loss: 1.71907, val loss: 1.78369\n",
      "Main effects training epoch: 76, train loss: 1.71983, val loss: 1.77132\n",
      "Main effects training epoch: 77, train loss: 1.71549, val loss: 1.77768\n",
      "Main effects training epoch: 78, train loss: 1.71558, val loss: 1.77664\n",
      "Main effects training epoch: 79, train loss: 1.71491, val loss: 1.77112\n",
      "Main effects training epoch: 80, train loss: 1.70636, val loss: 1.76402\n",
      "Main effects training epoch: 81, train loss: 1.70680, val loss: 1.76587\n",
      "Main effects training epoch: 82, train loss: 1.70402, val loss: 1.75948\n",
      "Main effects training epoch: 83, train loss: 1.70090, val loss: 1.76373\n",
      "Main effects training epoch: 84, train loss: 1.69622, val loss: 1.74957\n",
      "Main effects training epoch: 85, train loss: 1.69386, val loss: 1.75388\n",
      "Main effects training epoch: 86, train loss: 1.69243, val loss: 1.74943\n",
      "Main effects training epoch: 87, train loss: 1.69290, val loss: 1.74959\n",
      "Main effects training epoch: 88, train loss: 1.68823, val loss: 1.75087\n",
      "Main effects training epoch: 89, train loss: 1.68930, val loss: 1.74487\n",
      "Main effects training epoch: 90, train loss: 1.68462, val loss: 1.75396\n",
      "Main effects training epoch: 91, train loss: 1.67107, val loss: 1.72672\n",
      "Main effects training epoch: 92, train loss: 1.67301, val loss: 1.73993\n",
      "Main effects training epoch: 93, train loss: 1.66667, val loss: 1.72769\n",
      "Main effects training epoch: 94, train loss: 1.66474, val loss: 1.72848\n",
      "Main effects training epoch: 95, train loss: 1.65677, val loss: 1.72380\n",
      "Main effects training epoch: 96, train loss: 1.65191, val loss: 1.71595\n",
      "Main effects training epoch: 97, train loss: 1.64670, val loss: 1.72191\n",
      "Main effects training epoch: 98, train loss: 1.63889, val loss: 1.71528\n",
      "Main effects training epoch: 99, train loss: 1.63440, val loss: 1.70377\n",
      "Main effects training epoch: 100, train loss: 1.63745, val loss: 1.71779\n",
      "Main effects training epoch: 101, train loss: 1.63581, val loss: 1.71170\n",
      "Main effects training epoch: 102, train loss: 1.62747, val loss: 1.71276\n",
      "Main effects training epoch: 103, train loss: 1.62229, val loss: 1.69818\n",
      "Main effects training epoch: 104, train loss: 1.61937, val loss: 1.70621\n",
      "Main effects training epoch: 105, train loss: 1.61826, val loss: 1.70504\n",
      "Main effects training epoch: 106, train loss: 1.61633, val loss: 1.70517\n",
      "Main effects training epoch: 107, train loss: 1.61999, val loss: 1.70671\n",
      "Main effects training epoch: 108, train loss: 1.61554, val loss: 1.70333\n",
      "Main effects training epoch: 109, train loss: 1.61091, val loss: 1.70073\n",
      "Main effects training epoch: 110, train loss: 1.60950, val loss: 1.70262\n",
      "Main effects training epoch: 111, train loss: 1.60863, val loss: 1.70615\n",
      "Main effects training epoch: 112, train loss: 1.60803, val loss: 1.70016\n",
      "Main effects training epoch: 113, train loss: 1.60847, val loss: 1.69284\n",
      "Main effects training epoch: 114, train loss: 1.61347, val loss: 1.70269\n",
      "Main effects training epoch: 115, train loss: 1.60250, val loss: 1.69770\n",
      "Main effects training epoch: 116, train loss: 1.60484, val loss: 1.70143\n",
      "Main effects training epoch: 117, train loss: 1.60949, val loss: 1.69843\n",
      "Main effects training epoch: 118, train loss: 1.60107, val loss: 1.69123\n",
      "Main effects training epoch: 119, train loss: 1.60446, val loss: 1.69628\n",
      "Main effects training epoch: 120, train loss: 1.59927, val loss: 1.68903\n",
      "Main effects training epoch: 121, train loss: 1.59625, val loss: 1.69188\n",
      "Main effects training epoch: 122, train loss: 1.60099, val loss: 1.68805\n",
      "Main effects training epoch: 123, train loss: 1.59786, val loss: 1.69347\n",
      "Main effects training epoch: 124, train loss: 1.59340, val loss: 1.67732\n",
      "Main effects training epoch: 125, train loss: 1.59656, val loss: 1.69156\n",
      "Main effects training epoch: 126, train loss: 1.59198, val loss: 1.67780\n",
      "Main effects training epoch: 127, train loss: 1.59101, val loss: 1.68746\n",
      "Main effects training epoch: 128, train loss: 1.58858, val loss: 1.67521\n",
      "Main effects training epoch: 129, train loss: 1.59145, val loss: 1.68142\n",
      "Main effects training epoch: 130, train loss: 1.59443, val loss: 1.68435\n",
      "Main effects training epoch: 131, train loss: 1.58711, val loss: 1.68022\n",
      "Main effects training epoch: 132, train loss: 1.58925, val loss: 1.67216\n",
      "Main effects training epoch: 133, train loss: 1.58534, val loss: 1.66615\n",
      "Main effects training epoch: 134, train loss: 1.58518, val loss: 1.68569\n",
      "Main effects training epoch: 135, train loss: 1.59467, val loss: 1.66713\n",
      "Main effects training epoch: 136, train loss: 1.58201, val loss: 1.68022\n",
      "Main effects training epoch: 137, train loss: 1.58042, val loss: 1.66410\n",
      "Main effects training epoch: 138, train loss: 1.57833, val loss: 1.67043\n",
      "Main effects training epoch: 139, train loss: 1.57666, val loss: 1.66204\n",
      "Main effects training epoch: 140, train loss: 1.57938, val loss: 1.67195\n",
      "Main effects training epoch: 141, train loss: 1.57444, val loss: 1.66010\n",
      "Main effects training epoch: 142, train loss: 1.57400, val loss: 1.65761\n",
      "Main effects training epoch: 143, train loss: 1.57359, val loss: 1.65441\n",
      "Main effects training epoch: 144, train loss: 1.56951, val loss: 1.65466\n",
      "Main effects training epoch: 145, train loss: 1.56775, val loss: 1.64201\n",
      "Main effects training epoch: 146, train loss: 1.56915, val loss: 1.66288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 147, train loss: 1.56985, val loss: 1.64730\n",
      "Main effects training epoch: 148, train loss: 1.57402, val loss: 1.65423\n",
      "Main effects training epoch: 149, train loss: 1.56871, val loss: 1.64721\n",
      "Main effects training epoch: 150, train loss: 1.56564, val loss: 1.63986\n",
      "Main effects training epoch: 151, train loss: 1.56768, val loss: 1.65676\n",
      "Main effects training epoch: 152, train loss: 1.56880, val loss: 1.64287\n",
      "Main effects training epoch: 153, train loss: 1.56230, val loss: 1.64431\n",
      "Main effects training epoch: 154, train loss: 1.56604, val loss: 1.64118\n",
      "Main effects training epoch: 155, train loss: 1.56469, val loss: 1.64845\n",
      "Main effects training epoch: 156, train loss: 1.56219, val loss: 1.63677\n",
      "Main effects training epoch: 157, train loss: 1.56061, val loss: 1.64747\n",
      "Main effects training epoch: 158, train loss: 1.55994, val loss: 1.62465\n",
      "Main effects training epoch: 159, train loss: 1.55831, val loss: 1.65064\n",
      "Main effects training epoch: 160, train loss: 1.56412, val loss: 1.63726\n",
      "Main effects training epoch: 161, train loss: 1.55937, val loss: 1.64216\n",
      "Main effects training epoch: 162, train loss: 1.55929, val loss: 1.63603\n",
      "Main effects training epoch: 163, train loss: 1.56065, val loss: 1.64464\n",
      "Main effects training epoch: 164, train loss: 1.55345, val loss: 1.62750\n",
      "Main effects training epoch: 165, train loss: 1.55763, val loss: 1.63999\n",
      "Main effects training epoch: 166, train loss: 1.55081, val loss: 1.63147\n",
      "Main effects training epoch: 167, train loss: 1.56116, val loss: 1.63321\n",
      "Main effects training epoch: 168, train loss: 1.55618, val loss: 1.63496\n",
      "Main effects training epoch: 169, train loss: 1.55119, val loss: 1.63530\n",
      "Main effects training epoch: 170, train loss: 1.55224, val loss: 1.62772\n",
      "Main effects training epoch: 171, train loss: 1.54933, val loss: 1.62748\n",
      "Main effects training epoch: 172, train loss: 1.54929, val loss: 1.62531\n",
      "Main effects training epoch: 173, train loss: 1.54798, val loss: 1.62536\n",
      "Main effects training epoch: 174, train loss: 1.55183, val loss: 1.62806\n",
      "Main effects training epoch: 175, train loss: 1.55201, val loss: 1.62707\n",
      "Main effects training epoch: 176, train loss: 1.55153, val loss: 1.63199\n",
      "Main effects training epoch: 177, train loss: 1.54795, val loss: 1.63225\n",
      "Main effects training epoch: 178, train loss: 1.54742, val loss: 1.62148\n",
      "Main effects training epoch: 179, train loss: 1.54684, val loss: 1.63454\n",
      "Main effects training epoch: 180, train loss: 1.54599, val loss: 1.62549\n",
      "Main effects training epoch: 181, train loss: 1.55071, val loss: 1.62845\n",
      "Main effects training epoch: 182, train loss: 1.54716, val loss: 1.61493\n",
      "Main effects training epoch: 183, train loss: 1.55554, val loss: 1.63971\n",
      "Main effects training epoch: 184, train loss: 1.54519, val loss: 1.62132\n",
      "Main effects training epoch: 185, train loss: 1.55931, val loss: 1.63065\n",
      "Main effects training epoch: 186, train loss: 1.55107, val loss: 1.62411\n",
      "Main effects training epoch: 187, train loss: 1.54372, val loss: 1.63018\n",
      "Main effects training epoch: 188, train loss: 1.54205, val loss: 1.61647\n",
      "Main effects training epoch: 189, train loss: 1.54523, val loss: 1.62298\n",
      "Main effects training epoch: 190, train loss: 1.54188, val loss: 1.62148\n",
      "Main effects training epoch: 191, train loss: 1.54802, val loss: 1.62633\n",
      "Main effects training epoch: 192, train loss: 1.54515, val loss: 1.62456\n",
      "Main effects training epoch: 193, train loss: 1.54839, val loss: 1.61247\n",
      "Main effects training epoch: 194, train loss: 1.54602, val loss: 1.63807\n",
      "Main effects training epoch: 195, train loss: 1.54263, val loss: 1.61227\n",
      "Main effects training epoch: 196, train loss: 1.53867, val loss: 1.62103\n",
      "Main effects training epoch: 197, train loss: 1.54210, val loss: 1.61883\n",
      "Main effects training epoch: 198, train loss: 1.53663, val loss: 1.61126\n",
      "Main effects training epoch: 199, train loss: 1.54387, val loss: 1.62141\n",
      "Main effects training epoch: 200, train loss: 1.54144, val loss: 1.62307\n",
      "Main effects training epoch: 201, train loss: 1.54450, val loss: 1.61222\n",
      "Main effects training epoch: 202, train loss: 1.54193, val loss: 1.61854\n",
      "Main effects training epoch: 203, train loss: 1.54360, val loss: 1.62275\n",
      "Main effects training epoch: 204, train loss: 1.54118, val loss: 1.61952\n",
      "Main effects training epoch: 205, train loss: 1.53864, val loss: 1.60693\n",
      "Main effects training epoch: 206, train loss: 1.53967, val loss: 1.61228\n",
      "Main effects training epoch: 207, train loss: 1.53830, val loss: 1.62071\n",
      "Main effects training epoch: 208, train loss: 1.54415, val loss: 1.62287\n",
      "Main effects training epoch: 209, train loss: 1.54203, val loss: 1.60369\n",
      "Main effects training epoch: 210, train loss: 1.53577, val loss: 1.61807\n",
      "Main effects training epoch: 211, train loss: 1.53830, val loss: 1.61345\n",
      "Main effects training epoch: 212, train loss: 1.53579, val loss: 1.60368\n",
      "Main effects training epoch: 213, train loss: 1.54037, val loss: 1.62612\n",
      "Main effects training epoch: 214, train loss: 1.53439, val loss: 1.60570\n",
      "Main effects training epoch: 215, train loss: 1.53145, val loss: 1.60463\n",
      "Main effects training epoch: 216, train loss: 1.53169, val loss: 1.61035\n",
      "Main effects training epoch: 217, train loss: 1.53588, val loss: 1.60883\n",
      "Main effects training epoch: 218, train loss: 1.53707, val loss: 1.61932\n",
      "Main effects training epoch: 219, train loss: 1.52895, val loss: 1.59864\n",
      "Main effects training epoch: 220, train loss: 1.53005, val loss: 1.59752\n",
      "Main effects training epoch: 221, train loss: 1.53373, val loss: 1.59935\n",
      "Main effects training epoch: 222, train loss: 1.52895, val loss: 1.60395\n",
      "Main effects training epoch: 223, train loss: 1.53663, val loss: 1.60909\n",
      "Main effects training epoch: 224, train loss: 1.53529, val loss: 1.61333\n",
      "Main effects training epoch: 225, train loss: 1.53307, val loss: 1.59317\n",
      "Main effects training epoch: 226, train loss: 1.52908, val loss: 1.59237\n",
      "Main effects training epoch: 227, train loss: 1.52620, val loss: 1.60278\n",
      "Main effects training epoch: 228, train loss: 1.53444, val loss: 1.60920\n",
      "Main effects training epoch: 229, train loss: 1.52593, val loss: 1.59325\n",
      "Main effects training epoch: 230, train loss: 1.53628, val loss: 1.59949\n",
      "Main effects training epoch: 231, train loss: 1.52588, val loss: 1.59398\n",
      "Main effects training epoch: 232, train loss: 1.52975, val loss: 1.60993\n",
      "Main effects training epoch: 233, train loss: 1.52438, val loss: 1.59966\n",
      "Main effects training epoch: 234, train loss: 1.52898, val loss: 1.58823\n",
      "Main effects training epoch: 235, train loss: 1.53599, val loss: 1.60265\n",
      "Main effects training epoch: 236, train loss: 1.52879, val loss: 1.60168\n",
      "Main effects training epoch: 237, train loss: 1.52101, val loss: 1.59276\n",
      "Main effects training epoch: 238, train loss: 1.52013, val loss: 1.59004\n",
      "Main effects training epoch: 239, train loss: 1.52251, val loss: 1.58902\n",
      "Main effects training epoch: 240, train loss: 1.52220, val loss: 1.59172\n",
      "Main effects training epoch: 241, train loss: 1.52573, val loss: 1.59273\n",
      "Main effects training epoch: 242, train loss: 1.52741, val loss: 1.60485\n",
      "Main effects training epoch: 243, train loss: 1.51843, val loss: 1.59249\n",
      "Main effects training epoch: 244, train loss: 1.51958, val loss: 1.58716\n",
      "Main effects training epoch: 245, train loss: 1.52521, val loss: 1.59762\n",
      "Main effects training epoch: 246, train loss: 1.51839, val loss: 1.57602\n",
      "Main effects training epoch: 247, train loss: 1.51487, val loss: 1.58628\n",
      "Main effects training epoch: 248, train loss: 1.51806, val loss: 1.59221\n",
      "Main effects training epoch: 249, train loss: 1.51429, val loss: 1.58688\n",
      "Main effects training epoch: 250, train loss: 1.51584, val loss: 1.57706\n",
      "Main effects training epoch: 251, train loss: 1.51526, val loss: 1.58731\n",
      "Main effects training epoch: 252, train loss: 1.52155, val loss: 1.59006\n",
      "Main effects training epoch: 253, train loss: 1.51490, val loss: 1.59032\n",
      "Main effects training epoch: 254, train loss: 1.51077, val loss: 1.57841\n",
      "Main effects training epoch: 255, train loss: 1.51252, val loss: 1.57814\n",
      "Main effects training epoch: 256, train loss: 1.52393, val loss: 1.58697\n",
      "Main effects training epoch: 257, train loss: 1.51278, val loss: 1.58326\n",
      "Main effects training epoch: 258, train loss: 1.52116, val loss: 1.59611\n",
      "Main effects training epoch: 259, train loss: 1.51251, val loss: 1.58375\n",
      "Main effects training epoch: 260, train loss: 1.52234, val loss: 1.57684\n",
      "Main effects training epoch: 261, train loss: 1.50825, val loss: 1.58319\n",
      "Main effects training epoch: 262, train loss: 1.50884, val loss: 1.58657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 263, train loss: 1.50674, val loss: 1.56814\n",
      "Main effects training epoch: 264, train loss: 1.50794, val loss: 1.57054\n",
      "Main effects training epoch: 265, train loss: 1.50854, val loss: 1.57979\n",
      "Main effects training epoch: 266, train loss: 1.50596, val loss: 1.58461\n",
      "Main effects training epoch: 267, train loss: 1.50295, val loss: 1.57693\n",
      "Main effects training epoch: 268, train loss: 1.50384, val loss: 1.57183\n",
      "Main effects training epoch: 269, train loss: 1.50332, val loss: 1.57517\n",
      "Main effects training epoch: 270, train loss: 1.50089, val loss: 1.57636\n",
      "Main effects training epoch: 271, train loss: 1.50262, val loss: 1.57377\n",
      "Main effects training epoch: 272, train loss: 1.50279, val loss: 1.57101\n",
      "Main effects training epoch: 273, train loss: 1.50696, val loss: 1.59007\n",
      "Main effects training epoch: 274, train loss: 1.50030, val loss: 1.56474\n",
      "Main effects training epoch: 275, train loss: 1.50538, val loss: 1.57593\n",
      "Main effects training epoch: 276, train loss: 1.50298, val loss: 1.57434\n",
      "Main effects training epoch: 277, train loss: 1.49972, val loss: 1.55812\n",
      "Main effects training epoch: 278, train loss: 1.50226, val loss: 1.57121\n",
      "Main effects training epoch: 279, train loss: 1.49730, val loss: 1.56981\n",
      "Main effects training epoch: 280, train loss: 1.49690, val loss: 1.55827\n",
      "Main effects training epoch: 281, train loss: 1.50074, val loss: 1.57655\n",
      "Main effects training epoch: 282, train loss: 1.50070, val loss: 1.56263\n",
      "Main effects training epoch: 283, train loss: 1.50969, val loss: 1.58176\n",
      "Main effects training epoch: 284, train loss: 1.51066, val loss: 1.57430\n",
      "Main effects training epoch: 285, train loss: 1.50972, val loss: 1.57788\n",
      "Main effects training epoch: 286, train loss: 1.49277, val loss: 1.55721\n",
      "Main effects training epoch: 287, train loss: 1.49520, val loss: 1.55709\n",
      "Main effects training epoch: 288, train loss: 1.50080, val loss: 1.57896\n",
      "Main effects training epoch: 289, train loss: 1.49581, val loss: 1.55921\n",
      "Main effects training epoch: 290, train loss: 1.49936, val loss: 1.55625\n",
      "Main effects training epoch: 291, train loss: 1.49139, val loss: 1.56140\n",
      "Main effects training epoch: 292, train loss: 1.49416, val loss: 1.55454\n",
      "Main effects training epoch: 293, train loss: 1.49206, val loss: 1.55164\n",
      "Main effects training epoch: 294, train loss: 1.48907, val loss: 1.55601\n",
      "Main effects training epoch: 295, train loss: 1.48738, val loss: 1.55628\n",
      "Main effects training epoch: 296, train loss: 1.48761, val loss: 1.55904\n",
      "Main effects training epoch: 297, train loss: 1.48538, val loss: 1.55455\n",
      "Main effects training epoch: 298, train loss: 1.49405, val loss: 1.55449\n",
      "Main effects training epoch: 299, train loss: 1.49303, val loss: 1.56311\n",
      "Main effects training epoch: 300, train loss: 1.49744, val loss: 1.55955\n",
      "##########Stage 1: main effect training stop.##########\n",
      "3 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.51337, val loss: 1.56671\n",
      "Main effects tuning epoch: 2, train loss: 1.51992, val loss: 1.57466\n",
      "Main effects tuning epoch: 3, train loss: 1.50826, val loss: 1.55441\n",
      "Main effects tuning epoch: 4, train loss: 1.50810, val loss: 1.55595\n",
      "Main effects tuning epoch: 5, train loss: 1.50700, val loss: 1.54890\n",
      "Main effects tuning epoch: 6, train loss: 1.50376, val loss: 1.55555\n",
      "Main effects tuning epoch: 7, train loss: 1.50440, val loss: 1.56847\n",
      "Main effects tuning epoch: 8, train loss: 1.50871, val loss: 1.54252\n",
      "Main effects tuning epoch: 9, train loss: 1.50356, val loss: 1.56133\n",
      "Main effects tuning epoch: 10, train loss: 1.50833, val loss: 1.54997\n",
      "Main effects tuning epoch: 11, train loss: 1.51778, val loss: 1.57232\n",
      "Main effects tuning epoch: 12, train loss: 1.50160, val loss: 1.55587\n",
      "Main effects tuning epoch: 13, train loss: 1.50429, val loss: 1.55487\n",
      "Main effects tuning epoch: 14, train loss: 1.50006, val loss: 1.55487\n",
      "Main effects tuning epoch: 15, train loss: 1.50995, val loss: 1.56215\n",
      "Main effects tuning epoch: 16, train loss: 1.49558, val loss: 1.54760\n",
      "Main effects tuning epoch: 17, train loss: 1.50119, val loss: 1.55000\n",
      "Main effects tuning epoch: 18, train loss: 1.50913, val loss: 1.55032\n",
      "Main effects tuning epoch: 19, train loss: 1.50036, val loss: 1.56257\n",
      "Main effects tuning epoch: 20, train loss: 1.50422, val loss: 1.54286\n",
      "Main effects tuning epoch: 21, train loss: 1.49272, val loss: 1.55074\n",
      "Main effects tuning epoch: 22, train loss: 1.49344, val loss: 1.54349\n",
      "Main effects tuning epoch: 23, train loss: 1.48828, val loss: 1.53733\n",
      "Main effects tuning epoch: 24, train loss: 1.49604, val loss: 1.55229\n",
      "Main effects tuning epoch: 25, train loss: 1.49884, val loss: 1.54311\n",
      "Main effects tuning epoch: 26, train loss: 1.50240, val loss: 1.56756\n",
      "Main effects tuning epoch: 27, train loss: 1.49593, val loss: 1.53658\n",
      "Main effects tuning epoch: 28, train loss: 1.49549, val loss: 1.55337\n",
      "Main effects tuning epoch: 29, train loss: 1.49192, val loss: 1.55181\n",
      "Main effects tuning epoch: 30, train loss: 1.48772, val loss: 1.53104\n",
      "Main effects tuning epoch: 31, train loss: 1.48971, val loss: 1.54950\n",
      "Main effects tuning epoch: 32, train loss: 1.49407, val loss: 1.53978\n",
      "Main effects tuning epoch: 33, train loss: 1.49190, val loss: 1.54043\n",
      "Main effects tuning epoch: 34, train loss: 1.48759, val loss: 1.55408\n",
      "Main effects tuning epoch: 35, train loss: 1.48356, val loss: 1.53146\n",
      "Main effects tuning epoch: 36, train loss: 1.48440, val loss: 1.54120\n",
      "Main effects tuning epoch: 37, train loss: 1.48404, val loss: 1.52902\n",
      "Main effects tuning epoch: 38, train loss: 1.47819, val loss: 1.53223\n",
      "Main effects tuning epoch: 39, train loss: 1.47829, val loss: 1.53636\n",
      "Main effects tuning epoch: 40, train loss: 1.48516, val loss: 1.53372\n",
      "Main effects tuning epoch: 41, train loss: 1.48082, val loss: 1.53021\n",
      "Main effects tuning epoch: 42, train loss: 1.47956, val loss: 1.53598\n",
      "Main effects tuning epoch: 43, train loss: 1.47768, val loss: 1.51976\n",
      "Main effects tuning epoch: 44, train loss: 1.48577, val loss: 1.53696\n",
      "Main effects tuning epoch: 45, train loss: 1.47661, val loss: 1.53628\n",
      "Main effects tuning epoch: 46, train loss: 1.47978, val loss: 1.52675\n",
      "Main effects tuning epoch: 47, train loss: 1.47604, val loss: 1.53440\n",
      "Main effects tuning epoch: 48, train loss: 1.47834, val loss: 1.53936\n",
      "Main effects tuning epoch: 49, train loss: 1.47345, val loss: 1.52340\n",
      "Main effects tuning epoch: 50, train loss: 1.47211, val loss: 1.52105\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.30306, val loss: 1.30740\n",
      "Interaction training epoch: 2, train loss: 1.11724, val loss: 1.13596\n",
      "Interaction training epoch: 3, train loss: 1.26649, val loss: 1.23594\n",
      "Interaction training epoch: 4, train loss: 1.07155, val loss: 1.05513\n",
      "Interaction training epoch: 5, train loss: 1.05770, val loss: 1.04297\n",
      "Interaction training epoch: 6, train loss: 1.02441, val loss: 1.03001\n",
      "Interaction training epoch: 7, train loss: 0.99507, val loss: 0.98156\n",
      "Interaction training epoch: 8, train loss: 1.03012, val loss: 1.03034\n",
      "Interaction training epoch: 9, train loss: 0.99583, val loss: 0.99204\n",
      "Interaction training epoch: 10, train loss: 0.98163, val loss: 0.97215\n",
      "Interaction training epoch: 11, train loss: 1.00898, val loss: 1.01167\n",
      "Interaction training epoch: 12, train loss: 0.97748, val loss: 0.95575\n",
      "Interaction training epoch: 13, train loss: 0.98323, val loss: 0.98033\n",
      "Interaction training epoch: 14, train loss: 0.99172, val loss: 0.97958\n",
      "Interaction training epoch: 15, train loss: 0.94848, val loss: 0.94764\n",
      "Interaction training epoch: 16, train loss: 0.96693, val loss: 0.95217\n",
      "Interaction training epoch: 17, train loss: 0.96463, val loss: 0.96189\n",
      "Interaction training epoch: 18, train loss: 1.04986, val loss: 1.05859\n",
      "Interaction training epoch: 19, train loss: 0.96986, val loss: 0.96684\n",
      "Interaction training epoch: 20, train loss: 0.93525, val loss: 0.92906\n",
      "Interaction training epoch: 21, train loss: 0.92817, val loss: 0.92043\n",
      "Interaction training epoch: 22, train loss: 0.92044, val loss: 0.90494\n",
      "Interaction training epoch: 23, train loss: 0.91570, val loss: 0.90104\n",
      "Interaction training epoch: 24, train loss: 0.92633, val loss: 0.91305\n",
      "Interaction training epoch: 25, train loss: 0.93167, val loss: 0.92015\n",
      "Interaction training epoch: 26, train loss: 0.92534, val loss: 0.91049\n",
      "Interaction training epoch: 27, train loss: 0.90401, val loss: 0.89044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 28, train loss: 0.91556, val loss: 0.89337\n",
      "Interaction training epoch: 29, train loss: 0.93306, val loss: 0.91081\n",
      "Interaction training epoch: 30, train loss: 0.90143, val loss: 0.88047\n",
      "Interaction training epoch: 31, train loss: 0.92593, val loss: 0.92551\n",
      "Interaction training epoch: 32, train loss: 0.89909, val loss: 0.87629\n",
      "Interaction training epoch: 33, train loss: 0.90549, val loss: 0.90007\n",
      "Interaction training epoch: 34, train loss: 0.88969, val loss: 0.88936\n",
      "Interaction training epoch: 35, train loss: 0.88497, val loss: 0.86808\n",
      "Interaction training epoch: 36, train loss: 0.87737, val loss: 0.86555\n",
      "Interaction training epoch: 37, train loss: 0.87621, val loss: 0.85545\n",
      "Interaction training epoch: 38, train loss: 0.88214, val loss: 0.86240\n",
      "Interaction training epoch: 39, train loss: 0.87809, val loss: 0.86862\n",
      "Interaction training epoch: 40, train loss: 0.88267, val loss: 0.86910\n",
      "Interaction training epoch: 41, train loss: 0.87495, val loss: 0.86465\n",
      "Interaction training epoch: 42, train loss: 0.87817, val loss: 0.86499\n",
      "Interaction training epoch: 43, train loss: 0.92358, val loss: 0.88784\n",
      "Interaction training epoch: 44, train loss: 0.90722, val loss: 0.88857\n",
      "Interaction training epoch: 45, train loss: 0.86442, val loss: 0.84111\n",
      "Interaction training epoch: 46, train loss: 0.87406, val loss: 0.86356\n",
      "Interaction training epoch: 47, train loss: 0.86616, val loss: 0.83501\n",
      "Interaction training epoch: 48, train loss: 0.87017, val loss: 0.85325\n",
      "Interaction training epoch: 49, train loss: 0.88579, val loss: 0.86681\n",
      "Interaction training epoch: 50, train loss: 0.84966, val loss: 0.82979\n",
      "Interaction training epoch: 51, train loss: 0.83744, val loss: 0.82344\n",
      "Interaction training epoch: 52, train loss: 0.85579, val loss: 0.82446\n",
      "Interaction training epoch: 53, train loss: 0.87866, val loss: 0.85929\n",
      "Interaction training epoch: 54, train loss: 0.85887, val loss: 0.83870\n",
      "Interaction training epoch: 55, train loss: 0.84705, val loss: 0.83460\n",
      "Interaction training epoch: 56, train loss: 0.84548, val loss: 0.82253\n",
      "Interaction training epoch: 57, train loss: 0.87357, val loss: 0.84704\n",
      "Interaction training epoch: 58, train loss: 0.87543, val loss: 0.85090\n",
      "Interaction training epoch: 59, train loss: 0.86260, val loss: 0.83728\n",
      "Interaction training epoch: 60, train loss: 0.87225, val loss: 0.85268\n",
      "Interaction training epoch: 61, train loss: 0.83933, val loss: 0.81899\n",
      "Interaction training epoch: 62, train loss: 0.84807, val loss: 0.83162\n",
      "Interaction training epoch: 63, train loss: 0.85638, val loss: 0.82145\n",
      "Interaction training epoch: 64, train loss: 0.84931, val loss: 0.83469\n",
      "Interaction training epoch: 65, train loss: 0.84077, val loss: 0.81945\n",
      "Interaction training epoch: 66, train loss: 0.84013, val loss: 0.82978\n",
      "Interaction training epoch: 67, train loss: 0.85615, val loss: 0.84472\n",
      "Interaction training epoch: 68, train loss: 0.83737, val loss: 0.83333\n",
      "Interaction training epoch: 69, train loss: 0.82278, val loss: 0.80835\n",
      "Interaction training epoch: 70, train loss: 0.82593, val loss: 0.80800\n",
      "Interaction training epoch: 71, train loss: 0.83450, val loss: 0.82174\n",
      "Interaction training epoch: 72, train loss: 0.86302, val loss: 0.83353\n",
      "Interaction training epoch: 73, train loss: 0.83248, val loss: 0.81971\n",
      "Interaction training epoch: 74, train loss: 0.83128, val loss: 0.82546\n",
      "Interaction training epoch: 75, train loss: 0.84353, val loss: 0.82369\n",
      "Interaction training epoch: 76, train loss: 0.83247, val loss: 0.81747\n",
      "Interaction training epoch: 77, train loss: 0.82787, val loss: 0.81179\n",
      "Interaction training epoch: 78, train loss: 0.82180, val loss: 0.81238\n",
      "Interaction training epoch: 79, train loss: 0.83880, val loss: 0.81323\n",
      "Interaction training epoch: 80, train loss: 0.82001, val loss: 0.80358\n",
      "Interaction training epoch: 81, train loss: 0.80878, val loss: 0.79723\n",
      "Interaction training epoch: 82, train loss: 0.82140, val loss: 0.80052\n",
      "Interaction training epoch: 83, train loss: 0.83988, val loss: 0.82670\n",
      "Interaction training epoch: 84, train loss: 0.81905, val loss: 0.81423\n",
      "Interaction training epoch: 85, train loss: 0.84091, val loss: 0.81845\n",
      "Interaction training epoch: 86, train loss: 0.82733, val loss: 0.79864\n",
      "Interaction training epoch: 87, train loss: 0.82277, val loss: 0.82016\n",
      "Interaction training epoch: 88, train loss: 0.81184, val loss: 0.79442\n",
      "Interaction training epoch: 89, train loss: 0.81926, val loss: 0.81413\n",
      "Interaction training epoch: 90, train loss: 0.82319, val loss: 0.81008\n",
      "Interaction training epoch: 91, train loss: 0.83100, val loss: 0.80608\n",
      "Interaction training epoch: 92, train loss: 0.83728, val loss: 0.82680\n",
      "Interaction training epoch: 93, train loss: 0.84638, val loss: 0.82710\n",
      "Interaction training epoch: 94, train loss: 0.83443, val loss: 0.82460\n",
      "Interaction training epoch: 95, train loss: 0.83631, val loss: 0.82092\n",
      "Interaction training epoch: 96, train loss: 0.81027, val loss: 0.78599\n",
      "Interaction training epoch: 97, train loss: 0.81630, val loss: 0.79824\n",
      "Interaction training epoch: 98, train loss: 0.83758, val loss: 0.82463\n",
      "Interaction training epoch: 99, train loss: 0.82440, val loss: 0.80555\n",
      "Interaction training epoch: 100, train loss: 0.81208, val loss: 0.79845\n",
      "Interaction training epoch: 101, train loss: 0.80253, val loss: 0.79345\n",
      "Interaction training epoch: 102, train loss: 0.80335, val loss: 0.79265\n",
      "Interaction training epoch: 103, train loss: 0.82773, val loss: 0.80868\n",
      "Interaction training epoch: 104, train loss: 0.83089, val loss: 0.81983\n",
      "Interaction training epoch: 105, train loss: 0.82292, val loss: 0.80587\n",
      "Interaction training epoch: 106, train loss: 0.81271, val loss: 0.80288\n",
      "Interaction training epoch: 107, train loss: 0.83118, val loss: 0.81464\n",
      "Interaction training epoch: 108, train loss: 0.82845, val loss: 0.83079\n",
      "Interaction training epoch: 109, train loss: 0.82050, val loss: 0.82073\n",
      "Interaction training epoch: 110, train loss: 0.81058, val loss: 0.79167\n",
      "Interaction training epoch: 111, train loss: 0.82232, val loss: 0.81635\n",
      "Interaction training epoch: 112, train loss: 0.82097, val loss: 0.81462\n",
      "Interaction training epoch: 113, train loss: 0.81789, val loss: 0.79479\n",
      "Interaction training epoch: 114, train loss: 0.80272, val loss: 0.79406\n",
      "Interaction training epoch: 115, train loss: 0.80104, val loss: 0.79554\n",
      "Interaction training epoch: 116, train loss: 0.81790, val loss: 0.79657\n",
      "Interaction training epoch: 117, train loss: 0.81609, val loss: 0.80546\n",
      "Interaction training epoch: 118, train loss: 0.81118, val loss: 0.80564\n",
      "Interaction training epoch: 119, train loss: 0.81587, val loss: 0.80529\n",
      "Interaction training epoch: 120, train loss: 0.81920, val loss: 0.81559\n",
      "Interaction training epoch: 121, train loss: 0.79772, val loss: 0.79027\n",
      "Interaction training epoch: 122, train loss: 0.80611, val loss: 0.78593\n",
      "Interaction training epoch: 123, train loss: 0.80216, val loss: 0.79046\n",
      "Interaction training epoch: 124, train loss: 0.81816, val loss: 0.80718\n",
      "Interaction training epoch: 125, train loss: 0.81254, val loss: 0.80072\n",
      "Interaction training epoch: 126, train loss: 0.80488, val loss: 0.79132\n",
      "Interaction training epoch: 127, train loss: 0.80118, val loss: 0.79618\n",
      "Interaction training epoch: 128, train loss: 0.80490, val loss: 0.80129\n",
      "Interaction training epoch: 129, train loss: 0.81110, val loss: 0.79816\n",
      "Interaction training epoch: 130, train loss: 0.80773, val loss: 0.79514\n",
      "Interaction training epoch: 131, train loss: 0.80176, val loss: 0.79345\n",
      "Interaction training epoch: 132, train loss: 0.80367, val loss: 0.79783\n",
      "Interaction training epoch: 133, train loss: 0.79785, val loss: 0.78490\n",
      "Interaction training epoch: 134, train loss: 0.81183, val loss: 0.81525\n",
      "Interaction training epoch: 135, train loss: 0.80483, val loss: 0.79839\n",
      "Interaction training epoch: 136, train loss: 0.79881, val loss: 0.79330\n",
      "Interaction training epoch: 137, train loss: 0.80788, val loss: 0.80699\n",
      "Interaction training epoch: 138, train loss: 0.83095, val loss: 0.82215\n",
      "Interaction training epoch: 139, train loss: 0.82269, val loss: 0.82163\n",
      "Interaction training epoch: 140, train loss: 0.80203, val loss: 0.77967\n",
      "Interaction training epoch: 141, train loss: 0.81356, val loss: 0.80575\n",
      "Interaction training epoch: 142, train loss: 0.80976, val loss: 0.79958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 143, train loss: 0.80691, val loss: 0.80588\n",
      "Interaction training epoch: 144, train loss: 0.80965, val loss: 0.79814\n",
      "Interaction training epoch: 145, train loss: 0.79050, val loss: 0.78942\n",
      "Interaction training epoch: 146, train loss: 0.82190, val loss: 0.80936\n",
      "Interaction training epoch: 147, train loss: 0.81717, val loss: 0.80704\n",
      "Interaction training epoch: 148, train loss: 0.80005, val loss: 0.79274\n",
      "Interaction training epoch: 149, train loss: 0.79365, val loss: 0.79447\n",
      "Interaction training epoch: 150, train loss: 0.81421, val loss: 0.80437\n",
      "Interaction training epoch: 151, train loss: 0.79399, val loss: 0.79148\n",
      "Interaction training epoch: 152, train loss: 0.79612, val loss: 0.79510\n",
      "Interaction training epoch: 153, train loss: 0.79746, val loss: 0.79451\n",
      "Interaction training epoch: 154, train loss: 0.80502, val loss: 0.79835\n",
      "Interaction training epoch: 155, train loss: 0.79548, val loss: 0.79528\n",
      "Interaction training epoch: 156, train loss: 0.82982, val loss: 0.82527\n",
      "Interaction training epoch: 157, train loss: 0.80361, val loss: 0.78949\n",
      "Interaction training epoch: 158, train loss: 0.80514, val loss: 0.80820\n",
      "Interaction training epoch: 159, train loss: 0.79353, val loss: 0.78870\n",
      "Interaction training epoch: 160, train loss: 0.80087, val loss: 0.79232\n",
      "Interaction training epoch: 161, train loss: 0.79234, val loss: 0.79340\n",
      "Interaction training epoch: 162, train loss: 0.79691, val loss: 0.79706\n",
      "Interaction training epoch: 163, train loss: 0.80899, val loss: 0.80009\n",
      "Interaction training epoch: 164, train loss: 0.81505, val loss: 0.80792\n",
      "Interaction training epoch: 165, train loss: 0.80550, val loss: 0.79132\n",
      "Interaction training epoch: 166, train loss: 0.81953, val loss: 0.81867\n",
      "Interaction training epoch: 167, train loss: 0.79185, val loss: 0.79268\n",
      "Interaction training epoch: 168, train loss: 0.80271, val loss: 0.79611\n",
      "Interaction training epoch: 169, train loss: 0.80914, val loss: 0.79132\n",
      "Interaction training epoch: 170, train loss: 0.79476, val loss: 0.79421\n",
      "Interaction training epoch: 171, train loss: 0.79690, val loss: 0.78949\n",
      "Interaction training epoch: 172, train loss: 0.81498, val loss: 0.81136\n",
      "Interaction training epoch: 173, train loss: 0.81046, val loss: 0.80590\n",
      "Interaction training epoch: 174, train loss: 0.80975, val loss: 0.81154\n",
      "Interaction training epoch: 175, train loss: 0.80685, val loss: 0.80283\n",
      "Interaction training epoch: 176, train loss: 0.80997, val loss: 0.80575\n",
      "Interaction training epoch: 177, train loss: 0.80149, val loss: 0.79532\n",
      "Interaction training epoch: 178, train loss: 0.79595, val loss: 0.79921\n",
      "Interaction training epoch: 179, train loss: 0.79348, val loss: 0.79204\n",
      "Interaction training epoch: 180, train loss: 0.78988, val loss: 0.78805\n",
      "Interaction training epoch: 181, train loss: 0.80670, val loss: 0.80671\n",
      "Interaction training epoch: 182, train loss: 0.80836, val loss: 0.79956\n",
      "Interaction training epoch: 183, train loss: 0.80206, val loss: 0.80698\n",
      "Interaction training epoch: 184, train loss: 0.78813, val loss: 0.78557\n",
      "Interaction training epoch: 185, train loss: 0.79267, val loss: 0.78878\n",
      "Interaction training epoch: 186, train loss: 0.79782, val loss: 0.79886\n",
      "Interaction training epoch: 187, train loss: 0.79692, val loss: 0.78880\n",
      "Interaction training epoch: 188, train loss: 0.79158, val loss: 0.78813\n",
      "Interaction training epoch: 189, train loss: 0.79519, val loss: 0.79209\n",
      "Interaction training epoch: 190, train loss: 0.77964, val loss: 0.77955\n",
      "Interaction training epoch: 191, train loss: 0.80314, val loss: 0.79338\n",
      "Interaction training epoch: 192, train loss: 0.80291, val loss: 0.80363\n",
      "Interaction training epoch: 193, train loss: 0.79379, val loss: 0.78035\n",
      "Interaction training epoch: 194, train loss: 0.79681, val loss: 0.79677\n",
      "Interaction training epoch: 195, train loss: 0.79837, val loss: 0.79630\n",
      "Interaction training epoch: 196, train loss: 0.79306, val loss: 0.78251\n",
      "Interaction training epoch: 197, train loss: 0.79555, val loss: 0.79293\n",
      "Interaction training epoch: 198, train loss: 0.80876, val loss: 0.80431\n",
      "Interaction training epoch: 199, train loss: 0.79537, val loss: 0.79303\n",
      "Interaction training epoch: 200, train loss: 0.78261, val loss: 0.78319\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########2 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.79040, val loss: 0.78585\n",
      "Interaction tuning epoch: 2, train loss: 0.79949, val loss: 0.78630\n",
      "Interaction tuning epoch: 3, train loss: 0.78432, val loss: 0.78897\n",
      "Interaction tuning epoch: 4, train loss: 0.79378, val loss: 0.79521\n",
      "Interaction tuning epoch: 5, train loss: 0.80523, val loss: 0.79872\n",
      "Interaction tuning epoch: 6, train loss: 0.79820, val loss: 0.79902\n",
      "Interaction tuning epoch: 7, train loss: 0.78922, val loss: 0.79028\n",
      "Interaction tuning epoch: 8, train loss: 0.78391, val loss: 0.77869\n",
      "Interaction tuning epoch: 9, train loss: 0.79859, val loss: 0.79684\n",
      "Interaction tuning epoch: 10, train loss: 0.79789, val loss: 0.78950\n",
      "Interaction tuning epoch: 11, train loss: 0.78983, val loss: 0.78596\n",
      "Interaction tuning epoch: 12, train loss: 0.79796, val loss: 0.80208\n",
      "Interaction tuning epoch: 13, train loss: 0.81109, val loss: 0.80382\n",
      "Interaction tuning epoch: 14, train loss: 0.78396, val loss: 0.78463\n",
      "Interaction tuning epoch: 15, train loss: 0.79799, val loss: 0.79059\n",
      "Interaction tuning epoch: 16, train loss: 0.80728, val loss: 0.80397\n",
      "Interaction tuning epoch: 17, train loss: 0.78243, val loss: 0.77725\n",
      "Interaction tuning epoch: 18, train loss: 0.80801, val loss: 0.81267\n",
      "Interaction tuning epoch: 19, train loss: 0.80151, val loss: 0.79052\n",
      "Interaction tuning epoch: 20, train loss: 0.80104, val loss: 0.79707\n",
      "Interaction tuning epoch: 21, train loss: 0.78151, val loss: 0.77737\n",
      "Interaction tuning epoch: 22, train loss: 0.80451, val loss: 0.80298\n",
      "Interaction tuning epoch: 23, train loss: 0.79498, val loss: 0.79827\n",
      "Interaction tuning epoch: 24, train loss: 0.78402, val loss: 0.78199\n",
      "Interaction tuning epoch: 25, train loss: 0.81112, val loss: 0.80693\n",
      "Interaction tuning epoch: 26, train loss: 0.79103, val loss: 0.78959\n",
      "Interaction tuning epoch: 27, train loss: 0.78107, val loss: 0.78411\n",
      "Interaction tuning epoch: 28, train loss: 0.79973, val loss: 0.78975\n",
      "Interaction tuning epoch: 29, train loss: 0.80042, val loss: 0.80180\n",
      "Interaction tuning epoch: 30, train loss: 0.78306, val loss: 0.78070\n",
      "Interaction tuning epoch: 31, train loss: 0.79207, val loss: 0.79022\n",
      "Interaction tuning epoch: 32, train loss: 0.80494, val loss: 0.80112\n",
      "Interaction tuning epoch: 33, train loss: 0.79346, val loss: 0.78097\n",
      "Interaction tuning epoch: 34, train loss: 0.79516, val loss: 0.79329\n",
      "Interaction tuning epoch: 35, train loss: 0.79399, val loss: 0.79078\n",
      "Interaction tuning epoch: 36, train loss: 0.79537, val loss: 0.78106\n",
      "Interaction tuning epoch: 37, train loss: 0.78356, val loss: 0.78289\n",
      "Interaction tuning epoch: 38, train loss: 0.79436, val loss: 0.79288\n",
      "Interaction tuning epoch: 39, train loss: 0.78703, val loss: 0.78050\n",
      "Interaction tuning epoch: 40, train loss: 0.79659, val loss: 0.79476\n",
      "Interaction tuning epoch: 41, train loss: 0.79569, val loss: 0.79399\n",
      "Interaction tuning epoch: 42, train loss: 0.79207, val loss: 0.78893\n",
      "Interaction tuning epoch: 43, train loss: 0.79239, val loss: 0.78366\n",
      "Interaction tuning epoch: 44, train loss: 0.79169, val loss: 0.78867\n",
      "Interaction tuning epoch: 45, train loss: 0.77755, val loss: 0.77564\n",
      "Interaction tuning epoch: 46, train loss: 0.79564, val loss: 0.79726\n",
      "Interaction tuning epoch: 47, train loss: 0.79518, val loss: 0.78854\n",
      "Interaction tuning epoch: 48, train loss: 0.77880, val loss: 0.77850\n",
      "Interaction tuning epoch: 49, train loss: 0.78734, val loss: 0.78297\n",
      "Interaction tuning epoch: 50, train loss: 0.79480, val loss: 0.79236\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 42.28152632713318\n",
      "After the gam stage, training error is 0.79480 , validation error is 0.79236\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 19.530462\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.655145 validation MAE=0.747906,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.608901 validation MAE=0.729042,rank=5\n",
      "[SoftImpute] Iter 3: observed MAE=0.569951 validation MAE=0.712109,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.536733 validation MAE=0.696713,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.508566 validation MAE=0.682937,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.483683 validation MAE=0.670818,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.462134 validation MAE=0.659917,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.444682 validation MAE=0.650553,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.428708 validation MAE=0.642335,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.414115 validation MAE=0.633668,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.400940 validation MAE=0.626286,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.388783 validation MAE=0.619772,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.378156 validation MAE=0.613891,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.368512 validation MAE=0.608291,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.357782 validation MAE=0.602761,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.348637 validation MAE=0.597567,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.339718 validation MAE=0.592688,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.332307 validation MAE=0.587862,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.325745 validation MAE=0.584388,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.318274 validation MAE=0.580472,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.312238 validation MAE=0.576425,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.306642 validation MAE=0.573584,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.301301 validation MAE=0.570404,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.296918 validation MAE=0.567787,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.292750 validation MAE=0.565417,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.287858 validation MAE=0.562491,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.283257 validation MAE=0.560318,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.279242 validation MAE=0.557488,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.275524 validation MAE=0.555171,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.271986 validation MAE=0.553147,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.268984 validation MAE=0.551379,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.265937 validation MAE=0.549636,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.262885 validation MAE=0.547329,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.260192 validation MAE=0.545407,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.257543 validation MAE=0.544088,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.254783 validation MAE=0.542418,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.252334 validation MAE=0.540343,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.251182 validation MAE=0.539412,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.248416 validation MAE=0.537738,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.246088 validation MAE=0.536220,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.244101 validation MAE=0.534991,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.241758 validation MAE=0.533790,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.240015 validation MAE=0.532248,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.238695 validation MAE=0.530646,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.237289 validation MAE=0.530488,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.235250 validation MAE=0.529322,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.233337 validation MAE=0.528248,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.231607 validation MAE=0.527770,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.230438 validation MAE=0.527034,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.228771 validation MAE=0.525847,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.227292 validation MAE=0.525009,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.226441 validation MAE=0.524597,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.224841 validation MAE=0.523793,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.223588 validation MAE=0.523169,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.222466 validation MAE=0.523078,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.220909 validation MAE=0.522366,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.219795 validation MAE=0.521045,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.218460 validation MAE=0.520433,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.217297 validation MAE=0.520306,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.216755 validation MAE=0.520312,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.215536 validation MAE=0.519861,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.214348 validation MAE=0.519089,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.213252 validation MAE=0.518648,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.212345 validation MAE=0.518164,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.211726 validation MAE=0.517684,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.210724 validation MAE=0.517154,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.210293 validation MAE=0.517328,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.208976 validation MAE=0.516927,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.208014 validation MAE=0.516402,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.207370 validation MAE=0.516181,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.206213 validation MAE=0.515539,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.205464 validation MAE=0.515157,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.204676 validation MAE=0.514658,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.204058 validation MAE=0.513607,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.203387 validation MAE=0.513912,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.202691 validation MAE=0.513660,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.202218 validation MAE=0.513076,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.201613 validation MAE=0.513009,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.200605 validation MAE=0.512331,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.199714 validation MAE=0.512336,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.199807 validation MAE=0.512189,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.198690 validation MAE=0.511866,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.197736 validation MAE=0.511238,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.197394 validation MAE=0.510369,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.197002 validation MAE=0.510042,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.196381 validation MAE=0.509925,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.195655 validation MAE=0.510324,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.195162 validation MAE=0.509730,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.194276 validation MAE=0.509230,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.194153 validation MAE=0.509436,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.193426 validation MAE=0.509096,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.192978 validation MAE=0.508670,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.192241 validation MAE=0.508655,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.191542 validation MAE=0.508359,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.191713 validation MAE=0.507600,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.191156 validation MAE=0.507501,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.191082 validation MAE=0.507013,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.190362 validation MAE=0.506502,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.189590 validation MAE=0.506478,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.189198 validation MAE=0.506185,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.188635 validation MAE=0.506288,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.188668 validation MAE=0.506207,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.187928 validation MAE=0.505691,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.187211 validation MAE=0.505448,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.187127 validation MAE=0.504959,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.186712 validation MAE=0.505109,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.186492 validation MAE=0.504332,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.185892 validation MAE=0.504056,rank=5\n",
      "[SoftImpute] Iter 109: observed MAE=0.185615 validation MAE=0.503363,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.185335 validation MAE=0.503891,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 111: observed MAE=0.184409 validation MAE=0.503182,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.184180 validation MAE=0.502737,rank=5\n",
      "[SoftImpute] Iter 113: observed MAE=0.183892 validation MAE=0.502406,rank=5\n",
      "[SoftImpute] Iter 114: observed MAE=0.183314 validation MAE=0.502318,rank=5\n",
      "[SoftImpute] Iter 115: observed MAE=0.183140 validation MAE=0.502042,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.182748 validation MAE=0.501857,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.182218 validation MAE=0.501251,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.182055 validation MAE=0.501517,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.181952 validation MAE=0.501182,rank=5\n",
      "[SoftImpute] Iter 120: observed MAE=0.181649 validation MAE=0.500711,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.181446 validation MAE=0.499934,rank=5\n",
      "[SoftImpute] Iter 122: observed MAE=0.181519 validation MAE=0.500265,rank=5\n",
      "[SoftImpute] Iter 123: observed MAE=0.181129 validation MAE=0.499561,rank=5\n",
      "[SoftImpute] Iter 124: observed MAE=0.180611 validation MAE=0.499452,rank=5\n",
      "[SoftImpute] Iter 125: observed MAE=0.179927 validation MAE=0.499101,rank=5\n",
      "[SoftImpute] Iter 126: observed MAE=0.179402 validation MAE=0.499243,rank=5\n",
      "[SoftImpute] Iter 127: observed MAE=0.178873 validation MAE=0.498749,rank=5\n",
      "[SoftImpute] Iter 128: observed MAE=0.179007 validation MAE=0.498230,rank=5\n",
      "[SoftImpute] Iter 129: observed MAE=0.178957 validation MAE=0.497746,rank=5\n",
      "[SoftImpute] Iter 130: observed MAE=0.178749 validation MAE=0.498317,rank=5\n",
      "[SoftImpute] Iter 131: observed MAE=0.178427 validation MAE=0.497413,rank=5\n",
      "[SoftImpute] Iter 132: observed MAE=0.178090 validation MAE=0.497373,rank=5\n",
      "[SoftImpute] Iter 133: observed MAE=0.177691 validation MAE=0.496348,rank=5\n",
      "[SoftImpute] Iter 134: observed MAE=0.176969 validation MAE=0.496572,rank=5\n",
      "[SoftImpute] Iter 135: observed MAE=0.177038 validation MAE=0.496275,rank=5\n",
      "[SoftImpute] Iter 136: observed MAE=0.177378 validation MAE=0.495656,rank=5\n",
      "[SoftImpute] Iter 137: observed MAE=0.176856 validation MAE=0.495791,rank=5\n",
      "[SoftImpute] Iter 138: observed MAE=0.176297 validation MAE=0.495798,rank=5\n",
      "[SoftImpute] Iter 139: observed MAE=0.176046 validation MAE=0.494611,rank=5\n",
      "[SoftImpute] Iter 140: observed MAE=0.175963 validation MAE=0.494231,rank=5\n",
      "[SoftImpute] Iter 141: observed MAE=0.175742 validation MAE=0.493885,rank=5\n",
      "[SoftImpute] Iter 142: observed MAE=0.176084 validation MAE=0.494544,rank=5\n",
      "[SoftImpute] Iter 143: observed MAE=0.175421 validation MAE=0.494215,rank=5\n",
      "[SoftImpute] Iter 144: observed MAE=0.174912 validation MAE=0.493908,rank=5\n",
      "[SoftImpute] Iter 145: observed MAE=0.174792 validation MAE=0.493606,rank=5\n",
      "[SoftImpute] Iter 146: observed MAE=0.174442 validation MAE=0.493737,rank=5\n",
      "[SoftImpute] Iter 147: observed MAE=0.174317 validation MAE=0.493461,rank=5\n",
      "[SoftImpute] Iter 148: observed MAE=0.174118 validation MAE=0.492908,rank=5\n",
      "[SoftImpute] Iter 149: observed MAE=0.174119 validation MAE=0.492897,rank=5\n",
      "[SoftImpute] Iter 150: observed MAE=0.173556 validation MAE=0.492907,rank=5\n",
      "[SoftImpute] Iter 151: observed MAE=0.173557 validation MAE=0.492815,rank=5\n",
      "[SoftImpute] Iter 152: observed MAE=0.173003 validation MAE=0.492294,rank=5\n",
      "[SoftImpute] Iter 153: observed MAE=0.172769 validation MAE=0.492051,rank=5\n",
      "[SoftImpute] Iter 154: observed MAE=0.172808 validation MAE=0.491827,rank=5\n",
      "[SoftImpute] Iter 155: observed MAE=0.172912 validation MAE=0.491120,rank=5\n",
      "[SoftImpute] Iter 156: observed MAE=0.172833 validation MAE=0.491503,rank=5\n",
      "[SoftImpute] Iter 157: observed MAE=0.172602 validation MAE=0.490948,rank=5\n",
      "[SoftImpute] Iter 158: observed MAE=0.172046 validation MAE=0.490660,rank=5\n",
      "[SoftImpute] Iter 159: observed MAE=0.172134 validation MAE=0.490922,rank=5\n",
      "[SoftImpute] Iter 160: observed MAE=0.171873 validation MAE=0.490108,rank=5\n",
      "[SoftImpute] Iter 161: observed MAE=0.171374 validation MAE=0.490168,rank=5\n",
      "[SoftImpute] Iter 162: observed MAE=0.171489 validation MAE=0.490066,rank=5\n",
      "[SoftImpute] Iter 163: observed MAE=0.171290 validation MAE=0.489651,rank=5\n",
      "[SoftImpute] Iter 164: observed MAE=0.171110 validation MAE=0.489839,rank=5\n",
      "[SoftImpute] Iter 165: observed MAE=0.170513 validation MAE=0.489555,rank=5\n",
      "[SoftImpute] Iter 166: observed MAE=0.170278 validation MAE=0.489093,rank=5\n",
      "[SoftImpute] Iter 167: observed MAE=0.170377 validation MAE=0.488810,rank=5\n",
      "[SoftImpute] Iter 168: observed MAE=0.169960 validation MAE=0.488837,rank=5\n",
      "[SoftImpute] Iter 169: observed MAE=0.170427 validation MAE=0.489077,rank=5\n",
      "[SoftImpute] Iter 170: observed MAE=0.169881 validation MAE=0.488883,rank=5\n",
      "[SoftImpute] Iter 171: observed MAE=0.169655 validation MAE=0.488646,rank=5\n",
      "[SoftImpute] Iter 172: observed MAE=0.169478 validation MAE=0.488398,rank=5\n",
      "[SoftImpute] Iter 173: observed MAE=0.169233 validation MAE=0.487889,rank=5\n",
      "[SoftImpute] Iter 174: observed MAE=0.169162 validation MAE=0.487498,rank=5\n",
      "[SoftImpute] Iter 175: observed MAE=0.169499 validation MAE=0.487658,rank=5\n",
      "[SoftImpute] Iter 176: observed MAE=0.168975 validation MAE=0.487577,rank=5\n",
      "[SoftImpute] Iter 177: observed MAE=0.168499 validation MAE=0.487000,rank=5\n",
      "[SoftImpute] Iter 178: observed MAE=0.168561 validation MAE=0.487586,rank=5\n",
      "[SoftImpute] Iter 179: observed MAE=0.168660 validation MAE=0.487859,rank=5\n",
      "[SoftImpute] Iter 180: observed MAE=0.168407 validation MAE=0.487418,rank=5\n",
      "[SoftImpute] Iter 181: observed MAE=0.168075 validation MAE=0.487048,rank=5\n",
      "[SoftImpute] Iter 182: observed MAE=0.167603 validation MAE=0.486836,rank=5\n",
      "[SoftImpute] Iter 183: observed MAE=0.168537 validation MAE=0.486399,rank=5\n",
      "[SoftImpute] Iter 184: observed MAE=0.168135 validation MAE=0.486512,rank=5\n",
      "[SoftImpute] Iter 185: observed MAE=0.167743 validation MAE=0.486005,rank=5\n",
      "[SoftImpute] Iter 186: observed MAE=0.167235 validation MAE=0.485903,rank=5\n",
      "[SoftImpute] Iter 187: observed MAE=0.167187 validation MAE=0.485542,rank=5\n",
      "[SoftImpute] Iter 188: observed MAE=0.166444 validation MAE=0.485380,rank=5\n",
      "[SoftImpute] Iter 189: observed MAE=0.166702 validation MAE=0.485371,rank=5\n",
      "[SoftImpute] Iter 190: observed MAE=0.167277 validation MAE=0.485523,rank=5\n",
      "[SoftImpute] Iter 191: observed MAE=0.167025 validation MAE=0.485215,rank=5\n",
      "[SoftImpute] Iter 192: observed MAE=0.166372 validation MAE=0.485023,rank=5\n",
      "[SoftImpute] Iter 193: observed MAE=0.165947 validation MAE=0.484692,rank=5\n",
      "[SoftImpute] Iter 194: observed MAE=0.166349 validation MAE=0.484726,rank=5\n",
      "[SoftImpute] Iter 195: observed MAE=0.165850 validation MAE=0.484459,rank=5\n",
      "[SoftImpute] Iter 196: observed MAE=0.165555 validation MAE=0.484355,rank=5\n",
      "[SoftImpute] Iter 197: observed MAE=0.166018 validation MAE=0.484602,rank=5\n",
      "[SoftImpute] Iter 198: observed MAE=0.165504 validation MAE=0.483954,rank=5\n",
      "[SoftImpute] Iter 199: observed MAE=0.165616 validation MAE=0.484500,rank=5\n",
      "[SoftImpute] Iter 200: observed MAE=0.165325 validation MAE=0.484083,rank=5\n",
      "[SoftImpute] Stopped after iteration 200 for lambda=0.390609\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 5.978224515914917\n",
      "After the matrix factor stage, training error is 0.16533, validation error is 0.48408\n",
      "8\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.05618, val loss: 4.18318\n",
      "Main effects training epoch: 2, train loss: 3.88440, val loss: 4.01715\n",
      "Main effects training epoch: 3, train loss: 3.71347, val loss: 3.85862\n",
      "Main effects training epoch: 4, train loss: 3.50238, val loss: 3.64061\n",
      "Main effects training epoch: 5, train loss: 3.34978, val loss: 3.43911\n",
      "Main effects training epoch: 6, train loss: 3.32799, val loss: 3.39781\n",
      "Main effects training epoch: 7, train loss: 3.25270, val loss: 3.32902\n",
      "Main effects training epoch: 8, train loss: 3.24530, val loss: 3.33700\n",
      "Main effects training epoch: 9, train loss: 3.25031, val loss: 3.34456\n",
      "Main effects training epoch: 10, train loss: 3.13233, val loss: 3.21744\n",
      "Main effects training epoch: 11, train loss: 3.07253, val loss: 3.15687\n",
      "Main effects training epoch: 12, train loss: 3.02815, val loss: 3.11118\n",
      "Main effects training epoch: 13, train loss: 2.94428, val loss: 3.02646\n",
      "Main effects training epoch: 14, train loss: 2.91002, val loss: 2.98992\n",
      "Main effects training epoch: 15, train loss: 2.83901, val loss: 2.92378\n",
      "Main effects training epoch: 16, train loss: 2.75776, val loss: 2.82772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 17, train loss: 2.72361, val loss: 2.79304\n",
      "Main effects training epoch: 18, train loss: 2.66777, val loss: 2.73633\n",
      "Main effects training epoch: 19, train loss: 2.61003, val loss: 2.68021\n",
      "Main effects training epoch: 20, train loss: 2.52569, val loss: 2.59213\n",
      "Main effects training epoch: 21, train loss: 2.50834, val loss: 2.57475\n",
      "Main effects training epoch: 22, train loss: 2.44888, val loss: 2.52370\n",
      "Main effects training epoch: 23, train loss: 2.42073, val loss: 2.50013\n",
      "Main effects training epoch: 24, train loss: 2.38350, val loss: 2.45799\n",
      "Main effects training epoch: 25, train loss: 2.34544, val loss: 2.42053\n",
      "Main effects training epoch: 26, train loss: 2.31742, val loss: 2.39535\n",
      "Main effects training epoch: 27, train loss: 2.30274, val loss: 2.38088\n",
      "Main effects training epoch: 28, train loss: 2.24347, val loss: 2.31484\n",
      "Main effects training epoch: 29, train loss: 2.23474, val loss: 2.31769\n",
      "Main effects training epoch: 30, train loss: 2.20432, val loss: 2.28905\n",
      "Main effects training epoch: 31, train loss: 2.17839, val loss: 2.25947\n",
      "Main effects training epoch: 32, train loss: 2.15391, val loss: 2.23335\n",
      "Main effects training epoch: 33, train loss: 2.14073, val loss: 2.21946\n",
      "Main effects training epoch: 34, train loss: 2.10537, val loss: 2.18323\n",
      "Main effects training epoch: 35, train loss: 2.07343, val loss: 2.15084\n",
      "Main effects training epoch: 36, train loss: 2.06753, val loss: 2.14589\n",
      "Main effects training epoch: 37, train loss: 2.04768, val loss: 2.12287\n",
      "Main effects training epoch: 38, train loss: 2.03897, val loss: 2.12011\n",
      "Main effects training epoch: 39, train loss: 2.00191, val loss: 2.07669\n",
      "Main effects training epoch: 40, train loss: 1.99856, val loss: 2.06854\n",
      "Main effects training epoch: 41, train loss: 1.96233, val loss: 2.04296\n",
      "Main effects training epoch: 42, train loss: 1.95853, val loss: 2.02513\n",
      "Main effects training epoch: 43, train loss: 1.96130, val loss: 2.03697\n",
      "Main effects training epoch: 44, train loss: 1.92836, val loss: 2.00093\n",
      "Main effects training epoch: 45, train loss: 1.90544, val loss: 1.97813\n",
      "Main effects training epoch: 46, train loss: 1.91287, val loss: 1.98414\n",
      "Main effects training epoch: 47, train loss: 1.90526, val loss: 1.97824\n",
      "Main effects training epoch: 48, train loss: 1.87309, val loss: 1.94429\n",
      "Main effects training epoch: 49, train loss: 1.86805, val loss: 1.93808\n",
      "Main effects training epoch: 50, train loss: 1.85705, val loss: 1.92210\n",
      "Main effects training epoch: 51, train loss: 1.83842, val loss: 1.90954\n",
      "Main effects training epoch: 52, train loss: 1.83290, val loss: 1.89419\n",
      "Main effects training epoch: 53, train loss: 1.82565, val loss: 1.90205\n",
      "Main effects training epoch: 54, train loss: 1.82157, val loss: 1.88133\n",
      "Main effects training epoch: 55, train loss: 1.81462, val loss: 1.88181\n",
      "Main effects training epoch: 56, train loss: 1.80054, val loss: 1.86861\n",
      "Main effects training epoch: 57, train loss: 1.79913, val loss: 1.86240\n",
      "Main effects training epoch: 58, train loss: 1.78434, val loss: 1.84518\n",
      "Main effects training epoch: 59, train loss: 1.78675, val loss: 1.85330\n",
      "Main effects training epoch: 60, train loss: 1.77774, val loss: 1.84316\n",
      "Main effects training epoch: 61, train loss: 1.78020, val loss: 1.84054\n",
      "Main effects training epoch: 62, train loss: 1.76990, val loss: 1.83511\n",
      "Main effects training epoch: 63, train loss: 1.76057, val loss: 1.82476\n",
      "Main effects training epoch: 64, train loss: 1.75994, val loss: 1.81913\n",
      "Main effects training epoch: 65, train loss: 1.74323, val loss: 1.80324\n",
      "Main effects training epoch: 66, train loss: 1.76274, val loss: 1.82401\n",
      "Main effects training epoch: 67, train loss: 1.74195, val loss: 1.80078\n",
      "Main effects training epoch: 68, train loss: 1.74692, val loss: 1.79857\n",
      "Main effects training epoch: 69, train loss: 1.73942, val loss: 1.79909\n",
      "Main effects training epoch: 70, train loss: 1.73691, val loss: 1.79072\n",
      "Main effects training epoch: 71, train loss: 1.73386, val loss: 1.79288\n",
      "Main effects training epoch: 72, train loss: 1.72802, val loss: 1.78251\n",
      "Main effects training epoch: 73, train loss: 1.72607, val loss: 1.77985\n",
      "Main effects training epoch: 74, train loss: 1.72341, val loss: 1.78011\n",
      "Main effects training epoch: 75, train loss: 1.72110, val loss: 1.77630\n",
      "Main effects training epoch: 76, train loss: 1.71640, val loss: 1.77446\n",
      "Main effects training epoch: 77, train loss: 1.72105, val loss: 1.77812\n",
      "Main effects training epoch: 78, train loss: 1.71452, val loss: 1.76870\n",
      "Main effects training epoch: 79, train loss: 1.71543, val loss: 1.77038\n",
      "Main effects training epoch: 80, train loss: 1.71059, val loss: 1.76492\n",
      "Main effects training epoch: 81, train loss: 1.70754, val loss: 1.76246\n",
      "Main effects training epoch: 82, train loss: 1.71374, val loss: 1.77165\n",
      "Main effects training epoch: 83, train loss: 1.70468, val loss: 1.75706\n",
      "Main effects training epoch: 84, train loss: 1.70309, val loss: 1.75611\n",
      "Main effects training epoch: 85, train loss: 1.69683, val loss: 1.75347\n",
      "Main effects training epoch: 86, train loss: 1.69590, val loss: 1.75334\n",
      "Main effects training epoch: 87, train loss: 1.69894, val loss: 1.75685\n",
      "Main effects training epoch: 88, train loss: 1.69104, val loss: 1.74447\n",
      "Main effects training epoch: 89, train loss: 1.68752, val loss: 1.74506\n",
      "Main effects training epoch: 90, train loss: 1.68065, val loss: 1.73731\n",
      "Main effects training epoch: 91, train loss: 1.67923, val loss: 1.73956\n",
      "Main effects training epoch: 92, train loss: 1.67486, val loss: 1.73834\n",
      "Main effects training epoch: 93, train loss: 1.67068, val loss: 1.73057\n",
      "Main effects training epoch: 94, train loss: 1.66393, val loss: 1.73389\n",
      "Main effects training epoch: 95, train loss: 1.65851, val loss: 1.72342\n",
      "Main effects training epoch: 96, train loss: 1.65046, val loss: 1.72542\n",
      "Main effects training epoch: 97, train loss: 1.64575, val loss: 1.71873\n",
      "Main effects training epoch: 98, train loss: 1.64785, val loss: 1.72338\n",
      "Main effects training epoch: 99, train loss: 1.64802, val loss: 1.72916\n",
      "Main effects training epoch: 100, train loss: 1.64153, val loss: 1.71828\n",
      "Main effects training epoch: 101, train loss: 1.63378, val loss: 1.71592\n",
      "Main effects training epoch: 102, train loss: 1.63295, val loss: 1.70982\n",
      "Main effects training epoch: 103, train loss: 1.62986, val loss: 1.71349\n",
      "Main effects training epoch: 104, train loss: 1.64107, val loss: 1.72085\n",
      "Main effects training epoch: 105, train loss: 1.63504, val loss: 1.71835\n",
      "Main effects training epoch: 106, train loss: 1.62875, val loss: 1.71519\n",
      "Main effects training epoch: 107, train loss: 1.62790, val loss: 1.70404\n",
      "Main effects training epoch: 108, train loss: 1.62995, val loss: 1.71674\n",
      "Main effects training epoch: 109, train loss: 1.62463, val loss: 1.71062\n",
      "Main effects training epoch: 110, train loss: 1.62179, val loss: 1.70580\n",
      "Main effects training epoch: 111, train loss: 1.61658, val loss: 1.70578\n",
      "Main effects training epoch: 112, train loss: 1.61780, val loss: 1.70100\n",
      "Main effects training epoch: 113, train loss: 1.61715, val loss: 1.70060\n",
      "Main effects training epoch: 114, train loss: 1.61990, val loss: 1.70866\n",
      "Main effects training epoch: 115, train loss: 1.61354, val loss: 1.70548\n",
      "Main effects training epoch: 116, train loss: 1.61332, val loss: 1.69504\n",
      "Main effects training epoch: 117, train loss: 1.61228, val loss: 1.70391\n",
      "Main effects training epoch: 118, train loss: 1.60954, val loss: 1.70017\n",
      "Main effects training epoch: 119, train loss: 1.61228, val loss: 1.70299\n",
      "Main effects training epoch: 120, train loss: 1.60949, val loss: 1.69485\n",
      "Main effects training epoch: 121, train loss: 1.60750, val loss: 1.70083\n",
      "Main effects training epoch: 122, train loss: 1.60467, val loss: 1.69957\n",
      "Main effects training epoch: 123, train loss: 1.60538, val loss: 1.69609\n",
      "Main effects training epoch: 124, train loss: 1.60479, val loss: 1.69988\n",
      "Main effects training epoch: 125, train loss: 1.60225, val loss: 1.69088\n",
      "Main effects training epoch: 126, train loss: 1.60091, val loss: 1.69707\n",
      "Main effects training epoch: 127, train loss: 1.60342, val loss: 1.68861\n",
      "Main effects training epoch: 128, train loss: 1.59793, val loss: 1.69523\n",
      "Main effects training epoch: 129, train loss: 1.59670, val loss: 1.68703\n",
      "Main effects training epoch: 130, train loss: 1.59807, val loss: 1.69024\n",
      "Main effects training epoch: 131, train loss: 1.60828, val loss: 1.69720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 132, train loss: 1.60809, val loss: 1.70316\n",
      "Main effects training epoch: 133, train loss: 1.60840, val loss: 1.69187\n",
      "Main effects training epoch: 134, train loss: 1.60115, val loss: 1.69494\n",
      "Main effects training epoch: 135, train loss: 1.59173, val loss: 1.68242\n",
      "Main effects training epoch: 136, train loss: 1.59875, val loss: 1.68427\n",
      "Main effects training epoch: 137, train loss: 1.59706, val loss: 1.69372\n",
      "Main effects training epoch: 138, train loss: 1.60295, val loss: 1.69552\n",
      "Main effects training epoch: 139, train loss: 1.59225, val loss: 1.68627\n",
      "Main effects training epoch: 140, train loss: 1.59017, val loss: 1.68166\n",
      "Main effects training epoch: 141, train loss: 1.59034, val loss: 1.68154\n",
      "Main effects training epoch: 142, train loss: 1.58830, val loss: 1.68400\n",
      "Main effects training epoch: 143, train loss: 1.59072, val loss: 1.68858\n",
      "Main effects training epoch: 144, train loss: 1.58889, val loss: 1.68331\n",
      "Main effects training epoch: 145, train loss: 1.58573, val loss: 1.67466\n",
      "Main effects training epoch: 146, train loss: 1.58993, val loss: 1.68447\n",
      "Main effects training epoch: 147, train loss: 1.58745, val loss: 1.68092\n",
      "Main effects training epoch: 148, train loss: 1.58649, val loss: 1.68265\n",
      "Main effects training epoch: 149, train loss: 1.58401, val loss: 1.67501\n",
      "Main effects training epoch: 150, train loss: 1.58307, val loss: 1.67603\n",
      "Main effects training epoch: 151, train loss: 1.58255, val loss: 1.68423\n",
      "Main effects training epoch: 152, train loss: 1.58296, val loss: 1.67696\n",
      "Main effects training epoch: 153, train loss: 1.58440, val loss: 1.67551\n",
      "Main effects training epoch: 154, train loss: 1.58879, val loss: 1.68092\n",
      "Main effects training epoch: 155, train loss: 1.58435, val loss: 1.68342\n",
      "Main effects training epoch: 156, train loss: 1.58001, val loss: 1.67469\n",
      "Main effects training epoch: 157, train loss: 1.57932, val loss: 1.67656\n",
      "Main effects training epoch: 158, train loss: 1.58310, val loss: 1.67707\n",
      "Main effects training epoch: 159, train loss: 1.57961, val loss: 1.67348\n",
      "Main effects training epoch: 160, train loss: 1.57706, val loss: 1.67081\n",
      "Main effects training epoch: 161, train loss: 1.58157, val loss: 1.67622\n",
      "Main effects training epoch: 162, train loss: 1.57902, val loss: 1.67038\n",
      "Main effects training epoch: 163, train loss: 1.57752, val loss: 1.66497\n",
      "Main effects training epoch: 164, train loss: 1.58144, val loss: 1.68094\n",
      "Main effects training epoch: 165, train loss: 1.58025, val loss: 1.67022\n",
      "Main effects training epoch: 166, train loss: 1.58141, val loss: 1.67720\n",
      "Main effects training epoch: 167, train loss: 1.57589, val loss: 1.66253\n",
      "Main effects training epoch: 168, train loss: 1.57583, val loss: 1.67279\n",
      "Main effects training epoch: 169, train loss: 1.57548, val loss: 1.65967\n",
      "Main effects training epoch: 170, train loss: 1.57519, val loss: 1.66714\n",
      "Main effects training epoch: 171, train loss: 1.57821, val loss: 1.67307\n",
      "Main effects training epoch: 172, train loss: 1.57396, val loss: 1.66185\n",
      "Main effects training epoch: 173, train loss: 1.57179, val loss: 1.66142\n",
      "Main effects training epoch: 174, train loss: 1.57021, val loss: 1.66753\n",
      "Main effects training epoch: 175, train loss: 1.58292, val loss: 1.67279\n",
      "Main effects training epoch: 176, train loss: 1.57193, val loss: 1.66325\n",
      "Main effects training epoch: 177, train loss: 1.57032, val loss: 1.66749\n",
      "Main effects training epoch: 178, train loss: 1.56694, val loss: 1.65316\n",
      "Main effects training epoch: 179, train loss: 1.56630, val loss: 1.66215\n",
      "Main effects training epoch: 180, train loss: 1.56357, val loss: 1.65493\n",
      "Main effects training epoch: 181, train loss: 1.56926, val loss: 1.66301\n",
      "Main effects training epoch: 182, train loss: 1.56626, val loss: 1.66051\n",
      "Main effects training epoch: 183, train loss: 1.57017, val loss: 1.65986\n",
      "Main effects training epoch: 184, train loss: 1.57159, val loss: 1.65875\n",
      "Main effects training epoch: 185, train loss: 1.55689, val loss: 1.65109\n",
      "Main effects training epoch: 186, train loss: 1.55904, val loss: 1.65752\n",
      "Main effects training epoch: 187, train loss: 1.55988, val loss: 1.64751\n",
      "Main effects training epoch: 188, train loss: 1.55580, val loss: 1.65458\n",
      "Main effects training epoch: 189, train loss: 1.56134, val loss: 1.66187\n",
      "Main effects training epoch: 190, train loss: 1.56045, val loss: 1.64443\n",
      "Main effects training epoch: 191, train loss: 1.56027, val loss: 1.65431\n",
      "Main effects training epoch: 192, train loss: 1.55479, val loss: 1.65790\n",
      "Main effects training epoch: 193, train loss: 1.55675, val loss: 1.65236\n",
      "Main effects training epoch: 194, train loss: 1.56139, val loss: 1.64732\n",
      "Main effects training epoch: 195, train loss: 1.55718, val loss: 1.65686\n",
      "Main effects training epoch: 196, train loss: 1.54875, val loss: 1.64569\n",
      "Main effects training epoch: 197, train loss: 1.55209, val loss: 1.63787\n",
      "Main effects training epoch: 198, train loss: 1.55650, val loss: 1.66160\n",
      "Main effects training epoch: 199, train loss: 1.54824, val loss: 1.63290\n",
      "Main effects training epoch: 200, train loss: 1.55011, val loss: 1.63923\n",
      "Main effects training epoch: 201, train loss: 1.54893, val loss: 1.64583\n",
      "Main effects training epoch: 202, train loss: 1.54301, val loss: 1.63800\n",
      "Main effects training epoch: 203, train loss: 1.54697, val loss: 1.64393\n",
      "Main effects training epoch: 204, train loss: 1.54214, val loss: 1.63393\n",
      "Main effects training epoch: 205, train loss: 1.54078, val loss: 1.63423\n",
      "Main effects training epoch: 206, train loss: 1.53974, val loss: 1.63921\n",
      "Main effects training epoch: 207, train loss: 1.53534, val loss: 1.63254\n",
      "Main effects training epoch: 208, train loss: 1.53849, val loss: 1.62559\n",
      "Main effects training epoch: 209, train loss: 1.53384, val loss: 1.62428\n",
      "Main effects training epoch: 210, train loss: 1.54506, val loss: 1.63814\n",
      "Main effects training epoch: 211, train loss: 1.53850, val loss: 1.63812\n",
      "Main effects training epoch: 212, train loss: 1.52942, val loss: 1.62446\n",
      "Main effects training epoch: 213, train loss: 1.52697, val loss: 1.62625\n",
      "Main effects training epoch: 214, train loss: 1.53236, val loss: 1.62527\n",
      "Main effects training epoch: 215, train loss: 1.52361, val loss: 1.62250\n",
      "Main effects training epoch: 216, train loss: 1.52352, val loss: 1.61637\n",
      "Main effects training epoch: 217, train loss: 1.52324, val loss: 1.62149\n",
      "Main effects training epoch: 218, train loss: 1.52304, val loss: 1.62546\n",
      "Main effects training epoch: 219, train loss: 1.52727, val loss: 1.61459\n",
      "Main effects training epoch: 220, train loss: 1.52126, val loss: 1.63044\n",
      "Main effects training epoch: 221, train loss: 1.51651, val loss: 1.60531\n",
      "Main effects training epoch: 222, train loss: 1.52147, val loss: 1.62192\n",
      "Main effects training epoch: 223, train loss: 1.51880, val loss: 1.60560\n",
      "Main effects training epoch: 224, train loss: 1.51689, val loss: 1.61930\n",
      "Main effects training epoch: 225, train loss: 1.51974, val loss: 1.60970\n",
      "Main effects training epoch: 226, train loss: 1.51927, val loss: 1.61566\n",
      "Main effects training epoch: 227, train loss: 1.51114, val loss: 1.60399\n",
      "Main effects training epoch: 228, train loss: 1.51079, val loss: 1.61547\n",
      "Main effects training epoch: 229, train loss: 1.51245, val loss: 1.60764\n",
      "Main effects training epoch: 230, train loss: 1.50932, val loss: 1.60241\n",
      "Main effects training epoch: 231, train loss: 1.51103, val loss: 1.60500\n",
      "Main effects training epoch: 232, train loss: 1.51586, val loss: 1.59794\n",
      "Main effects training epoch: 233, train loss: 1.50527, val loss: 1.60692\n",
      "Main effects training epoch: 234, train loss: 1.50663, val loss: 1.59524\n",
      "Main effects training epoch: 235, train loss: 1.52030, val loss: 1.60660\n",
      "Main effects training epoch: 236, train loss: 1.52342, val loss: 1.61078\n",
      "Main effects training epoch: 237, train loss: 1.50817, val loss: 1.59478\n",
      "Main effects training epoch: 238, train loss: 1.50647, val loss: 1.59680\n",
      "Main effects training epoch: 239, train loss: 1.50759, val loss: 1.60011\n",
      "Main effects training epoch: 240, train loss: 1.51016, val loss: 1.59902\n",
      "Main effects training epoch: 241, train loss: 1.50451, val loss: 1.59889\n",
      "Main effects training epoch: 242, train loss: 1.49943, val loss: 1.58402\n",
      "Main effects training epoch: 243, train loss: 1.50066, val loss: 1.58982\n",
      "Main effects training epoch: 244, train loss: 1.50099, val loss: 1.59308\n",
      "Main effects training epoch: 245, train loss: 1.49747, val loss: 1.58192\n",
      "Main effects training epoch: 246, train loss: 1.49824, val loss: 1.57630\n",
      "Main effects training epoch: 247, train loss: 1.49976, val loss: 1.60126\n",
      "Main effects training epoch: 248, train loss: 1.50558, val loss: 1.59288\n",
      "Main effects training epoch: 249, train loss: 1.49278, val loss: 1.57526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 250, train loss: 1.50128, val loss: 1.59982\n",
      "Main effects training epoch: 251, train loss: 1.49728, val loss: 1.57613\n",
      "Main effects training epoch: 252, train loss: 1.49686, val loss: 1.58737\n",
      "Main effects training epoch: 253, train loss: 1.49603, val loss: 1.58308\n",
      "Main effects training epoch: 254, train loss: 1.49484, val loss: 1.57777\n",
      "Main effects training epoch: 255, train loss: 1.49430, val loss: 1.58261\n",
      "Main effects training epoch: 256, train loss: 1.49294, val loss: 1.57680\n",
      "Main effects training epoch: 257, train loss: 1.49396, val loss: 1.57272\n",
      "Main effects training epoch: 258, train loss: 1.49943, val loss: 1.57103\n",
      "Main effects training epoch: 259, train loss: 1.49705, val loss: 1.59147\n",
      "Main effects training epoch: 260, train loss: 1.49033, val loss: 1.57683\n",
      "Main effects training epoch: 261, train loss: 1.50046, val loss: 1.56570\n",
      "Main effects training epoch: 262, train loss: 1.49904, val loss: 1.59249\n",
      "Main effects training epoch: 263, train loss: 1.48742, val loss: 1.56410\n",
      "Main effects training epoch: 264, train loss: 1.49074, val loss: 1.56522\n",
      "Main effects training epoch: 265, train loss: 1.49643, val loss: 1.58300\n",
      "Main effects training epoch: 266, train loss: 1.49924, val loss: 1.57157\n",
      "Main effects training epoch: 267, train loss: 1.49029, val loss: 1.57616\n",
      "Main effects training epoch: 268, train loss: 1.51632, val loss: 1.60233\n",
      "Main effects training epoch: 269, train loss: 1.50137, val loss: 1.58321\n",
      "Main effects training epoch: 270, train loss: 1.49906, val loss: 1.58905\n",
      "Main effects training epoch: 271, train loss: 1.49648, val loss: 1.57003\n",
      "Main effects training epoch: 272, train loss: 1.49180, val loss: 1.58404\n",
      "Main effects training epoch: 273, train loss: 1.48968, val loss: 1.56873\n",
      "Main effects training epoch: 274, train loss: 1.49240, val loss: 1.58593\n",
      "Main effects training epoch: 275, train loss: 1.49157, val loss: 1.56603\n",
      "Main effects training epoch: 276, train loss: 1.50002, val loss: 1.58550\n",
      "Main effects training epoch: 277, train loss: 1.48857, val loss: 1.57710\n",
      "Main effects training epoch: 278, train loss: 1.48395, val loss: 1.56141\n",
      "Main effects training epoch: 279, train loss: 1.49617, val loss: 1.56996\n",
      "Main effects training epoch: 280, train loss: 1.48856, val loss: 1.57201\n",
      "Main effects training epoch: 281, train loss: 1.48871, val loss: 1.56275\n",
      "Main effects training epoch: 282, train loss: 1.48678, val loss: 1.56541\n",
      "Main effects training epoch: 283, train loss: 1.49207, val loss: 1.56533\n",
      "Main effects training epoch: 284, train loss: 1.48869, val loss: 1.56581\n",
      "Main effects training epoch: 285, train loss: 1.48382, val loss: 1.57153\n",
      "Main effects training epoch: 286, train loss: 1.48515, val loss: 1.55505\n",
      "Main effects training epoch: 287, train loss: 1.48009, val loss: 1.56152\n",
      "Main effects training epoch: 288, train loss: 1.49583, val loss: 1.56485\n",
      "Main effects training epoch: 289, train loss: 1.48390, val loss: 1.57673\n",
      "Main effects training epoch: 290, train loss: 1.47589, val loss: 1.55803\n",
      "Main effects training epoch: 291, train loss: 1.47857, val loss: 1.55122\n",
      "Main effects training epoch: 292, train loss: 1.48008, val loss: 1.55752\n",
      "Main effects training epoch: 293, train loss: 1.48031, val loss: 1.56612\n",
      "Main effects training epoch: 294, train loss: 1.48242, val loss: 1.56278\n",
      "Main effects training epoch: 295, train loss: 1.48056, val loss: 1.55843\n",
      "Main effects training epoch: 296, train loss: 1.47570, val loss: 1.55767\n",
      "Main effects training epoch: 297, train loss: 1.47008, val loss: 1.55776\n",
      "Main effects training epoch: 298, train loss: 1.47638, val loss: 1.56024\n",
      "Main effects training epoch: 299, train loss: 1.47231, val loss: 1.55068\n",
      "Main effects training epoch: 300, train loss: 1.47118, val loss: 1.54462\n",
      "##########Stage 1: main effect training stop.##########\n",
      "3 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.49496, val loss: 1.57848\n",
      "Main effects tuning epoch: 2, train loss: 1.49084, val loss: 1.55263\n",
      "Main effects tuning epoch: 3, train loss: 1.49367, val loss: 1.55704\n",
      "Main effects tuning epoch: 4, train loss: 1.49267, val loss: 1.55517\n",
      "Main effects tuning epoch: 5, train loss: 1.49067, val loss: 1.55695\n",
      "Main effects tuning epoch: 6, train loss: 1.48793, val loss: 1.54857\n",
      "Main effects tuning epoch: 7, train loss: 1.48720, val loss: 1.54732\n",
      "Main effects tuning epoch: 8, train loss: 1.48944, val loss: 1.55393\n",
      "Main effects tuning epoch: 9, train loss: 1.48572, val loss: 1.55330\n",
      "Main effects tuning epoch: 10, train loss: 1.49227, val loss: 1.55018\n",
      "Main effects tuning epoch: 11, train loss: 1.48947, val loss: 1.55693\n",
      "Main effects tuning epoch: 12, train loss: 1.48865, val loss: 1.54707\n",
      "Main effects tuning epoch: 13, train loss: 1.48824, val loss: 1.54415\n",
      "Main effects tuning epoch: 14, train loss: 1.49471, val loss: 1.56567\n",
      "Main effects tuning epoch: 15, train loss: 1.49109, val loss: 1.54648\n",
      "Main effects tuning epoch: 16, train loss: 1.48589, val loss: 1.55117\n",
      "Main effects tuning epoch: 17, train loss: 1.48328, val loss: 1.54487\n",
      "Main effects tuning epoch: 18, train loss: 1.48278, val loss: 1.54592\n",
      "Main effects tuning epoch: 19, train loss: 1.48366, val loss: 1.54743\n",
      "Main effects tuning epoch: 20, train loss: 1.47884, val loss: 1.53430\n",
      "Main effects tuning epoch: 21, train loss: 1.48179, val loss: 1.54470\n",
      "Main effects tuning epoch: 22, train loss: 1.49330, val loss: 1.56091\n",
      "Main effects tuning epoch: 23, train loss: 1.49654, val loss: 1.54692\n",
      "Main effects tuning epoch: 24, train loss: 1.49243, val loss: 1.55760\n",
      "Main effects tuning epoch: 25, train loss: 1.49745, val loss: 1.53089\n",
      "Main effects tuning epoch: 26, train loss: 1.49107, val loss: 1.56513\n",
      "Main effects tuning epoch: 27, train loss: 1.48274, val loss: 1.53561\n",
      "Main effects tuning epoch: 28, train loss: 1.48559, val loss: 1.55148\n",
      "Main effects tuning epoch: 29, train loss: 1.49780, val loss: 1.54154\n",
      "Main effects tuning epoch: 30, train loss: 1.48491, val loss: 1.55118\n",
      "Main effects tuning epoch: 31, train loss: 1.48644, val loss: 1.54800\n",
      "Main effects tuning epoch: 32, train loss: 1.49407, val loss: 1.55152\n",
      "Main effects tuning epoch: 33, train loss: 1.48741, val loss: 1.54718\n",
      "Main effects tuning epoch: 34, train loss: 1.48085, val loss: 1.53408\n",
      "Main effects tuning epoch: 35, train loss: 1.47808, val loss: 1.53757\n",
      "Main effects tuning epoch: 36, train loss: 1.48336, val loss: 1.53300\n",
      "Main effects tuning epoch: 37, train loss: 1.48224, val loss: 1.54887\n",
      "Main effects tuning epoch: 38, train loss: 1.48173, val loss: 1.53828\n",
      "Main effects tuning epoch: 39, train loss: 1.48015, val loss: 1.53073\n",
      "Main effects tuning epoch: 40, train loss: 1.48639, val loss: 1.56037\n",
      "Main effects tuning epoch: 41, train loss: 1.48039, val loss: 1.53972\n",
      "Main effects tuning epoch: 42, train loss: 1.48382, val loss: 1.54140\n",
      "Main effects tuning epoch: 43, train loss: 1.48975, val loss: 1.54257\n",
      "Main effects tuning epoch: 44, train loss: 1.48039, val loss: 1.53639\n",
      "Main effects tuning epoch: 45, train loss: 1.47674, val loss: 1.53272\n",
      "Main effects tuning epoch: 46, train loss: 1.47444, val loss: 1.53397\n",
      "Main effects tuning epoch: 47, train loss: 1.48402, val loss: 1.52891\n",
      "Main effects tuning epoch: 48, train loss: 1.47852, val loss: 1.54589\n",
      "Main effects tuning epoch: 49, train loss: 1.47353, val loss: 1.52720\n",
      "Main effects tuning epoch: 50, train loss: 1.47471, val loss: 1.53299\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.33605, val loss: 1.37756\n",
      "Interaction training epoch: 2, train loss: 1.09144, val loss: 1.05528\n",
      "Interaction training epoch: 3, train loss: 1.12873, val loss: 1.12865\n",
      "Interaction training epoch: 4, train loss: 1.14300, val loss: 1.16503\n",
      "Interaction training epoch: 5, train loss: 1.06492, val loss: 1.05601\n",
      "Interaction training epoch: 6, train loss: 1.02996, val loss: 1.03632\n",
      "Interaction training epoch: 7, train loss: 1.07491, val loss: 1.07102\n",
      "Interaction training epoch: 8, train loss: 1.03829, val loss: 1.04232\n",
      "Interaction training epoch: 9, train loss: 1.01642, val loss: 1.01922\n",
      "Interaction training epoch: 10, train loss: 1.04801, val loss: 1.05592\n",
      "Interaction training epoch: 11, train loss: 0.98181, val loss: 0.98651\n",
      "Interaction training epoch: 12, train loss: 0.98215, val loss: 0.98804\n",
      "Interaction training epoch: 13, train loss: 0.96974, val loss: 0.97725\n",
      "Interaction training epoch: 14, train loss: 0.97665, val loss: 0.97782\n",
      "Interaction training epoch: 15, train loss: 0.95666, val loss: 0.97041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 16, train loss: 0.94760, val loss: 0.94319\n",
      "Interaction training epoch: 17, train loss: 0.94279, val loss: 0.94638\n",
      "Interaction training epoch: 18, train loss: 0.95502, val loss: 0.94491\n",
      "Interaction training epoch: 19, train loss: 0.96867, val loss: 0.96748\n",
      "Interaction training epoch: 20, train loss: 0.94413, val loss: 0.93369\n",
      "Interaction training epoch: 21, train loss: 0.94237, val loss: 0.94139\n",
      "Interaction training epoch: 22, train loss: 0.93155, val loss: 0.91611\n",
      "Interaction training epoch: 23, train loss: 0.91859, val loss: 0.91239\n",
      "Interaction training epoch: 24, train loss: 0.91755, val loss: 0.90776\n",
      "Interaction training epoch: 25, train loss: 0.91650, val loss: 0.91150\n",
      "Interaction training epoch: 26, train loss: 0.91238, val loss: 0.91120\n",
      "Interaction training epoch: 27, train loss: 0.90805, val loss: 0.89745\n",
      "Interaction training epoch: 28, train loss: 0.90066, val loss: 0.89814\n",
      "Interaction training epoch: 29, train loss: 0.93822, val loss: 0.92244\n",
      "Interaction training epoch: 30, train loss: 0.90351, val loss: 0.90266\n",
      "Interaction training epoch: 31, train loss: 0.90109, val loss: 0.90095\n",
      "Interaction training epoch: 32, train loss: 0.90733, val loss: 0.89927\n",
      "Interaction training epoch: 33, train loss: 0.88334, val loss: 0.88656\n",
      "Interaction training epoch: 34, train loss: 0.89979, val loss: 0.89175\n",
      "Interaction training epoch: 35, train loss: 0.92916, val loss: 0.93035\n",
      "Interaction training epoch: 36, train loss: 0.89657, val loss: 0.89368\n",
      "Interaction training epoch: 37, train loss: 0.90960, val loss: 0.90858\n",
      "Interaction training epoch: 38, train loss: 0.93115, val loss: 0.93263\n",
      "Interaction training epoch: 39, train loss: 0.90383, val loss: 0.88957\n",
      "Interaction training epoch: 40, train loss: 0.88670, val loss: 0.88108\n",
      "Interaction training epoch: 41, train loss: 0.88476, val loss: 0.87546\n",
      "Interaction training epoch: 42, train loss: 0.89379, val loss: 0.89117\n",
      "Interaction training epoch: 43, train loss: 0.87679, val loss: 0.87890\n",
      "Interaction training epoch: 44, train loss: 0.87086, val loss: 0.85904\n",
      "Interaction training epoch: 45, train loss: 0.88952, val loss: 0.88399\n",
      "Interaction training epoch: 46, train loss: 0.87728, val loss: 0.86878\n",
      "Interaction training epoch: 47, train loss: 0.87296, val loss: 0.87552\n",
      "Interaction training epoch: 48, train loss: 0.88958, val loss: 0.87708\n",
      "Interaction training epoch: 49, train loss: 0.87199, val loss: 0.85918\n",
      "Interaction training epoch: 50, train loss: 0.87807, val loss: 0.86697\n",
      "Interaction training epoch: 51, train loss: 0.88310, val loss: 0.87609\n",
      "Interaction training epoch: 52, train loss: 0.87899, val loss: 0.86865\n",
      "Interaction training epoch: 53, train loss: 0.86698, val loss: 0.86272\n",
      "Interaction training epoch: 54, train loss: 0.87347, val loss: 0.86574\n",
      "Interaction training epoch: 55, train loss: 0.87620, val loss: 0.87794\n",
      "Interaction training epoch: 56, train loss: 0.86792, val loss: 0.85884\n",
      "Interaction training epoch: 57, train loss: 0.87605, val loss: 0.86355\n",
      "Interaction training epoch: 58, train loss: 0.86210, val loss: 0.85310\n",
      "Interaction training epoch: 59, train loss: 0.86682, val loss: 0.86218\n",
      "Interaction training epoch: 60, train loss: 0.85759, val loss: 0.84714\n",
      "Interaction training epoch: 61, train loss: 0.86620, val loss: 0.85511\n",
      "Interaction training epoch: 62, train loss: 0.87477, val loss: 0.86470\n",
      "Interaction training epoch: 63, train loss: 0.87618, val loss: 0.86265\n",
      "Interaction training epoch: 64, train loss: 0.86914, val loss: 0.85924\n",
      "Interaction training epoch: 65, train loss: 0.86283, val loss: 0.86355\n",
      "Interaction training epoch: 66, train loss: 0.87384, val loss: 0.85917\n",
      "Interaction training epoch: 67, train loss: 0.87224, val loss: 0.85709\n",
      "Interaction training epoch: 68, train loss: 0.85760, val loss: 0.85366\n",
      "Interaction training epoch: 69, train loss: 0.87202, val loss: 0.86889\n",
      "Interaction training epoch: 70, train loss: 0.85969, val loss: 0.85395\n",
      "Interaction training epoch: 71, train loss: 0.85512, val loss: 0.84838\n",
      "Interaction training epoch: 72, train loss: 0.87423, val loss: 0.86024\n",
      "Interaction training epoch: 73, train loss: 0.86425, val loss: 0.85952\n",
      "Interaction training epoch: 74, train loss: 0.88501, val loss: 0.87784\n",
      "Interaction training epoch: 75, train loss: 0.87062, val loss: 0.86379\n",
      "Interaction training epoch: 76, train loss: 0.85900, val loss: 0.85135\n",
      "Interaction training epoch: 77, train loss: 0.86041, val loss: 0.84809\n",
      "Interaction training epoch: 78, train loss: 0.84775, val loss: 0.83416\n",
      "Interaction training epoch: 79, train loss: 0.86338, val loss: 0.85890\n",
      "Interaction training epoch: 80, train loss: 0.86498, val loss: 0.85609\n",
      "Interaction training epoch: 81, train loss: 0.85230, val loss: 0.84909\n",
      "Interaction training epoch: 82, train loss: 0.87453, val loss: 0.86361\n",
      "Interaction training epoch: 83, train loss: 0.85839, val loss: 0.85128\n",
      "Interaction training epoch: 84, train loss: 0.86079, val loss: 0.84300\n",
      "Interaction training epoch: 85, train loss: 0.86270, val loss: 0.86108\n",
      "Interaction training epoch: 86, train loss: 0.85489, val loss: 0.84071\n",
      "Interaction training epoch: 87, train loss: 0.85418, val loss: 0.85113\n",
      "Interaction training epoch: 88, train loss: 0.86491, val loss: 0.85796\n",
      "Interaction training epoch: 89, train loss: 0.85240, val loss: 0.84050\n",
      "Interaction training epoch: 90, train loss: 0.86454, val loss: 0.85972\n",
      "Interaction training epoch: 91, train loss: 0.85663, val loss: 0.85728\n",
      "Interaction training epoch: 92, train loss: 0.84638, val loss: 0.83152\n",
      "Interaction training epoch: 93, train loss: 0.84503, val loss: 0.83700\n",
      "Interaction training epoch: 94, train loss: 0.88001, val loss: 0.87390\n",
      "Interaction training epoch: 95, train loss: 0.85271, val loss: 0.85175\n",
      "Interaction training epoch: 96, train loss: 0.86425, val loss: 0.84992\n",
      "Interaction training epoch: 97, train loss: 0.84829, val loss: 0.83424\n",
      "Interaction training epoch: 98, train loss: 0.84691, val loss: 0.83408\n",
      "Interaction training epoch: 99, train loss: 0.84596, val loss: 0.83395\n",
      "Interaction training epoch: 100, train loss: 0.83982, val loss: 0.83312\n",
      "Interaction training epoch: 101, train loss: 0.84962, val loss: 0.84637\n",
      "Interaction training epoch: 102, train loss: 0.85731, val loss: 0.84223\n",
      "Interaction training epoch: 103, train loss: 0.87100, val loss: 0.87563\n",
      "Interaction training epoch: 104, train loss: 0.84806, val loss: 0.83680\n",
      "Interaction training epoch: 105, train loss: 0.90837, val loss: 0.90949\n",
      "Interaction training epoch: 106, train loss: 0.83934, val loss: 0.83186\n",
      "Interaction training epoch: 107, train loss: 0.84369, val loss: 0.84352\n",
      "Interaction training epoch: 108, train loss: 0.83068, val loss: 0.82152\n",
      "Interaction training epoch: 109, train loss: 0.83357, val loss: 0.82422\n",
      "Interaction training epoch: 110, train loss: 0.83806, val loss: 0.83228\n",
      "Interaction training epoch: 111, train loss: 0.84627, val loss: 0.83196\n",
      "Interaction training epoch: 112, train loss: 0.85549, val loss: 0.85711\n",
      "Interaction training epoch: 113, train loss: 0.85033, val loss: 0.84237\n",
      "Interaction training epoch: 114, train loss: 0.84487, val loss: 0.84173\n",
      "Interaction training epoch: 115, train loss: 0.83802, val loss: 0.82818\n",
      "Interaction training epoch: 116, train loss: 0.86618, val loss: 0.86438\n",
      "Interaction training epoch: 117, train loss: 0.83563, val loss: 0.83220\n",
      "Interaction training epoch: 118, train loss: 0.85441, val loss: 0.84609\n",
      "Interaction training epoch: 119, train loss: 0.84593, val loss: 0.84319\n",
      "Interaction training epoch: 120, train loss: 0.85483, val loss: 0.85193\n",
      "Interaction training epoch: 121, train loss: 0.83801, val loss: 0.82853\n",
      "Interaction training epoch: 122, train loss: 0.84689, val loss: 0.84412\n",
      "Interaction training epoch: 123, train loss: 0.84807, val loss: 0.83972\n",
      "Interaction training epoch: 124, train loss: 0.84364, val loss: 0.83582\n",
      "Interaction training epoch: 125, train loss: 0.84354, val loss: 0.84153\n",
      "Interaction training epoch: 126, train loss: 0.83173, val loss: 0.82219\n",
      "Interaction training epoch: 127, train loss: 0.84656, val loss: 0.83962\n",
      "Interaction training epoch: 128, train loss: 0.82375, val loss: 0.82272\n",
      "Interaction training epoch: 129, train loss: 0.84117, val loss: 0.83281\n",
      "Interaction training epoch: 130, train loss: 0.83367, val loss: 0.83225\n",
      "Interaction training epoch: 131, train loss: 0.84121, val loss: 0.83777\n",
      "Interaction training epoch: 132, train loss: 0.83418, val loss: 0.82456\n",
      "Interaction training epoch: 133, train loss: 0.84447, val loss: 0.83469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 134, train loss: 0.83337, val loss: 0.82696\n",
      "Interaction training epoch: 135, train loss: 0.83402, val loss: 0.82200\n",
      "Interaction training epoch: 136, train loss: 0.82954, val loss: 0.82763\n",
      "Interaction training epoch: 137, train loss: 0.84278, val loss: 0.84058\n",
      "Interaction training epoch: 138, train loss: 0.82300, val loss: 0.82204\n",
      "Interaction training epoch: 139, train loss: 0.85163, val loss: 0.83990\n",
      "Interaction training epoch: 140, train loss: 0.83839, val loss: 0.84033\n",
      "Interaction training epoch: 141, train loss: 0.83792, val loss: 0.83268\n",
      "Interaction training epoch: 142, train loss: 0.81733, val loss: 0.81308\n",
      "Interaction training epoch: 143, train loss: 0.84970, val loss: 0.84877\n",
      "Interaction training epoch: 144, train loss: 0.83503, val loss: 0.82207\n",
      "Interaction training epoch: 145, train loss: 0.82618, val loss: 0.82058\n",
      "Interaction training epoch: 146, train loss: 0.82285, val loss: 0.82176\n",
      "Interaction training epoch: 147, train loss: 0.84545, val loss: 0.84373\n",
      "Interaction training epoch: 148, train loss: 0.83077, val loss: 0.82656\n",
      "Interaction training epoch: 149, train loss: 0.83728, val loss: 0.82896\n",
      "Interaction training epoch: 150, train loss: 0.83310, val loss: 0.83798\n",
      "Interaction training epoch: 151, train loss: 0.83797, val loss: 0.82678\n",
      "Interaction training epoch: 152, train loss: 0.82675, val loss: 0.82360\n",
      "Interaction training epoch: 153, train loss: 0.83216, val loss: 0.82799\n",
      "Interaction training epoch: 154, train loss: 0.83830, val loss: 0.82967\n",
      "Interaction training epoch: 155, train loss: 0.84253, val loss: 0.83432\n",
      "Interaction training epoch: 156, train loss: 0.83866, val loss: 0.84456\n",
      "Interaction training epoch: 157, train loss: 0.82436, val loss: 0.81688\n",
      "Interaction training epoch: 158, train loss: 0.83453, val loss: 0.82238\n",
      "Interaction training epoch: 159, train loss: 0.82244, val loss: 0.82247\n",
      "Interaction training epoch: 160, train loss: 0.83030, val loss: 0.82616\n",
      "Interaction training epoch: 161, train loss: 0.81760, val loss: 0.81596\n",
      "Interaction training epoch: 162, train loss: 0.82051, val loss: 0.81705\n",
      "Interaction training epoch: 163, train loss: 0.82413, val loss: 0.81928\n",
      "Interaction training epoch: 164, train loss: 0.83147, val loss: 0.82146\n",
      "Interaction training epoch: 165, train loss: 0.84211, val loss: 0.84626\n",
      "Interaction training epoch: 166, train loss: 0.82902, val loss: 0.82296\n",
      "Interaction training epoch: 167, train loss: 0.82806, val loss: 0.82407\n",
      "Interaction training epoch: 168, train loss: 0.82427, val loss: 0.82609\n",
      "Interaction training epoch: 169, train loss: 0.82949, val loss: 0.82907\n",
      "Interaction training epoch: 170, train loss: 0.84278, val loss: 0.82987\n",
      "Interaction training epoch: 171, train loss: 0.83096, val loss: 0.82850\n",
      "Interaction training epoch: 172, train loss: 0.81491, val loss: 0.81770\n",
      "Interaction training epoch: 173, train loss: 0.85508, val loss: 0.85400\n",
      "Interaction training epoch: 174, train loss: 0.82025, val loss: 0.82213\n",
      "Interaction training epoch: 175, train loss: 0.83233, val loss: 0.83247\n",
      "Interaction training epoch: 176, train loss: 0.81847, val loss: 0.82049\n",
      "Interaction training epoch: 177, train loss: 0.84722, val loss: 0.83859\n",
      "Interaction training epoch: 178, train loss: 0.82306, val loss: 0.82116\n",
      "Interaction training epoch: 179, train loss: 0.84135, val loss: 0.83521\n",
      "Interaction training epoch: 180, train loss: 0.82317, val loss: 0.83131\n",
      "Interaction training epoch: 181, train loss: 0.84640, val loss: 0.83681\n",
      "Interaction training epoch: 182, train loss: 0.83701, val loss: 0.83623\n",
      "Interaction training epoch: 183, train loss: 0.81262, val loss: 0.81241\n",
      "Interaction training epoch: 184, train loss: 0.82854, val loss: 0.82613\n",
      "Interaction training epoch: 185, train loss: 0.81687, val loss: 0.81840\n",
      "Interaction training epoch: 186, train loss: 0.83797, val loss: 0.83133\n",
      "Interaction training epoch: 187, train loss: 0.81999, val loss: 0.81977\n",
      "Interaction training epoch: 188, train loss: 0.82229, val loss: 0.81296\n",
      "Interaction training epoch: 189, train loss: 0.83646, val loss: 0.84091\n",
      "Interaction training epoch: 190, train loss: 0.81717, val loss: 0.82389\n",
      "Interaction training epoch: 191, train loss: 0.82686, val loss: 0.81571\n",
      "Interaction training epoch: 192, train loss: 0.82916, val loss: 0.83159\n",
      "Interaction training epoch: 193, train loss: 0.82244, val loss: 0.81822\n",
      "Interaction training epoch: 194, train loss: 0.82326, val loss: 0.82646\n",
      "Interaction training epoch: 195, train loss: 0.82466, val loss: 0.82212\n",
      "Interaction training epoch: 196, train loss: 0.84277, val loss: 0.83569\n",
      "Interaction training epoch: 197, train loss: 0.82338, val loss: 0.82568\n",
      "Interaction training epoch: 198, train loss: 0.81883, val loss: 0.81285\n",
      "Interaction training epoch: 199, train loss: 0.82228, val loss: 0.82647\n",
      "Interaction training epoch: 200, train loss: 0.86164, val loss: 0.86149\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########1 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.83900, val loss: 0.83770\n",
      "Interaction tuning epoch: 2, train loss: 0.81758, val loss: 0.81696\n",
      "Interaction tuning epoch: 3, train loss: 0.82925, val loss: 0.82329\n",
      "Interaction tuning epoch: 4, train loss: 0.82084, val loss: 0.81581\n",
      "Interaction tuning epoch: 5, train loss: 0.82135, val loss: 0.82400\n",
      "Interaction tuning epoch: 6, train loss: 0.82540, val loss: 0.81503\n",
      "Interaction tuning epoch: 7, train loss: 0.84184, val loss: 0.83665\n",
      "Interaction tuning epoch: 8, train loss: 0.82869, val loss: 0.83088\n",
      "Interaction tuning epoch: 9, train loss: 0.82465, val loss: 0.82236\n",
      "Interaction tuning epoch: 10, train loss: 0.82015, val loss: 0.81555\n",
      "Interaction tuning epoch: 11, train loss: 0.82312, val loss: 0.81877\n",
      "Interaction tuning epoch: 12, train loss: 0.86193, val loss: 0.85917\n",
      "Interaction tuning epoch: 13, train loss: 0.83710, val loss: 0.82621\n",
      "Interaction tuning epoch: 14, train loss: 0.83694, val loss: 0.83059\n",
      "Interaction tuning epoch: 15, train loss: 0.83761, val loss: 0.83834\n",
      "Interaction tuning epoch: 16, train loss: 0.82164, val loss: 0.81862\n",
      "Interaction tuning epoch: 17, train loss: 0.81670, val loss: 0.81300\n",
      "Interaction tuning epoch: 18, train loss: 0.84294, val loss: 0.83361\n",
      "Interaction tuning epoch: 19, train loss: 0.82127, val loss: 0.82922\n",
      "Interaction tuning epoch: 20, train loss: 0.81917, val loss: 0.80697\n",
      "Interaction tuning epoch: 21, train loss: 0.84189, val loss: 0.84329\n",
      "Interaction tuning epoch: 22, train loss: 0.82909, val loss: 0.82401\n",
      "Interaction tuning epoch: 23, train loss: 0.81170, val loss: 0.81466\n",
      "Interaction tuning epoch: 24, train loss: 0.82895, val loss: 0.82572\n",
      "Interaction tuning epoch: 25, train loss: 0.83233, val loss: 0.83080\n",
      "Interaction tuning epoch: 26, train loss: 0.85281, val loss: 0.85516\n",
      "Interaction tuning epoch: 27, train loss: 0.81813, val loss: 0.82068\n",
      "Interaction tuning epoch: 28, train loss: 0.82583, val loss: 0.81822\n",
      "Interaction tuning epoch: 29, train loss: 0.81332, val loss: 0.80607\n",
      "Interaction tuning epoch: 30, train loss: 0.83515, val loss: 0.83454\n",
      "Interaction tuning epoch: 31, train loss: 0.83486, val loss: 0.83647\n",
      "Interaction tuning epoch: 32, train loss: 0.81992, val loss: 0.81708\n",
      "Interaction tuning epoch: 33, train loss: 0.81843, val loss: 0.81113\n",
      "Interaction tuning epoch: 34, train loss: 0.81140, val loss: 0.81313\n",
      "Interaction tuning epoch: 35, train loss: 0.81686, val loss: 0.81564\n",
      "Interaction tuning epoch: 36, train loss: 0.81713, val loss: 0.81392\n",
      "Interaction tuning epoch: 37, train loss: 0.82920, val loss: 0.82429\n",
      "Interaction tuning epoch: 38, train loss: 0.82178, val loss: 0.82207\n",
      "Interaction tuning epoch: 39, train loss: 0.83839, val loss: 0.83143\n",
      "Interaction tuning epoch: 40, train loss: 0.85712, val loss: 0.85666\n",
      "Interaction tuning epoch: 41, train loss: 0.83866, val loss: 0.83663\n",
      "Interaction tuning epoch: 42, train loss: 0.81467, val loss: 0.81126\n",
      "Interaction tuning epoch: 43, train loss: 0.83017, val loss: 0.83064\n",
      "Interaction tuning epoch: 44, train loss: 0.83711, val loss: 0.83306\n",
      "Interaction tuning epoch: 45, train loss: 0.83723, val loss: 0.84097\n",
      "Interaction tuning epoch: 46, train loss: 0.85086, val loss: 0.84340\n",
      "Interaction tuning epoch: 47, train loss: 0.81803, val loss: 0.81819\n",
      "Interaction tuning epoch: 48, train loss: 0.82626, val loss: 0.81674\n",
      "Interaction tuning epoch: 49, train loss: 0.82904, val loss: 0.83436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 50, train loss: 0.81615, val loss: 0.81269\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 39.997408866882324\n",
      "After the gam stage, training error is 0.81615 , validation error is 0.81269\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 20.415733\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.672116 validation MAE=0.765481,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.623749 validation MAE=0.745185,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 3: observed MAE=0.582638 validation MAE=0.726563,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.547800 validation MAE=0.709861,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.518331 validation MAE=0.694930,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.493786 validation MAE=0.681418,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.472090 validation MAE=0.669053,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.454689 validation MAE=0.658126,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.439961 validation MAE=0.648858,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.428890 validation MAE=0.641079,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.417192 validation MAE=0.632665,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.405872 validation MAE=0.625232,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.396170 validation MAE=0.618600,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.386694 validation MAE=0.611469,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.377019 validation MAE=0.605133,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.368956 validation MAE=0.600138,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.360533 validation MAE=0.594682,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.353205 validation MAE=0.590041,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.346465 validation MAE=0.585473,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.340572 validation MAE=0.582578,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.334292 validation MAE=0.578023,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.328380 validation MAE=0.574752,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.323251 validation MAE=0.571400,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.319315 validation MAE=0.568563,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.313915 validation MAE=0.565348,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.310436 validation MAE=0.562836,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.306259 validation MAE=0.559920,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.302302 validation MAE=0.558262,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.298868 validation MAE=0.555064,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.295844 validation MAE=0.553280,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.292640 validation MAE=0.550847,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.289766 validation MAE=0.549362,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.287552 validation MAE=0.547419,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.284660 validation MAE=0.545782,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.281688 validation MAE=0.543943,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.279812 validation MAE=0.542921,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.277270 validation MAE=0.541735,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.275545 validation MAE=0.539928,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.273452 validation MAE=0.538407,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.271389 validation MAE=0.537596,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.269484 validation MAE=0.536716,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.268092 validation MAE=0.535307,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.266458 validation MAE=0.533959,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.263958 validation MAE=0.533273,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.263240 validation MAE=0.532239,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.262113 validation MAE=0.531826,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.260017 validation MAE=0.530106,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.259112 validation MAE=0.529367,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.256423 validation MAE=0.528607,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.255739 validation MAE=0.527911,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.255277 validation MAE=0.527097,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.254122 validation MAE=0.526740,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.252296 validation MAE=0.525625,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.251371 validation MAE=0.525168,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.249673 validation MAE=0.524516,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.248228 validation MAE=0.523841,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.247790 validation MAE=0.523684,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.247493 validation MAE=0.522885,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.245757 validation MAE=0.522137,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.245609 validation MAE=0.521993,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.243979 validation MAE=0.521108,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.242295 validation MAE=0.520335,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.242440 validation MAE=0.520592,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.241202 validation MAE=0.519963,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.239452 validation MAE=0.519495,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.239562 validation MAE=0.519381,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.239184 validation MAE=0.518936,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.237766 validation MAE=0.518356,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.236974 validation MAE=0.517883,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.236610 validation MAE=0.517699,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.234988 validation MAE=0.517423,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.233825 validation MAE=0.516747,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.232702 validation MAE=0.516245,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.233060 validation MAE=0.516266,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.232659 validation MAE=0.515785,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.231606 validation MAE=0.515130,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.230009 validation MAE=0.515247,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.229540 validation MAE=0.514748,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.229135 validation MAE=0.514629,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.227512 validation MAE=0.513646,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.227940 validation MAE=0.513133,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.227170 validation MAE=0.513178,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.225647 validation MAE=0.513100,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.225478 validation MAE=0.512742,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.226226 validation MAE=0.512994,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.225408 validation MAE=0.512598,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.224524 validation MAE=0.512070,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.223637 validation MAE=0.511279,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.223315 validation MAE=0.511140,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.222347 validation MAE=0.510964,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.220942 validation MAE=0.510864,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.221211 validation MAE=0.510110,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.221106 validation MAE=0.510202,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.220189 validation MAE=0.509825,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.220697 validation MAE=0.509826,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.219079 validation MAE=0.509200,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.218715 validation MAE=0.508832,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.218712 validation MAE=0.508464,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.218009 validation MAE=0.508740,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.216702 validation MAE=0.508209,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.216723 validation MAE=0.508296,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.216851 validation MAE=0.508209,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.216479 validation MAE=0.507570,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.215159 validation MAE=0.507344,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.214763 validation MAE=0.507329,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.214169 validation MAE=0.507323,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.213895 validation MAE=0.506688,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.212779 validation MAE=0.506584,rank=5\n",
      "[SoftImpute] Iter 109: observed MAE=0.213184 validation MAE=0.506806,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.213198 validation MAE=0.506198,rank=5\n",
      "[SoftImpute] Iter 111: observed MAE=0.211853 validation MAE=0.506311,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.211319 validation MAE=0.506088,rank=5\n",
      "[SoftImpute] Iter 113: observed MAE=0.211123 validation MAE=0.505369,rank=5\n",
      "[SoftImpute] Iter 114: observed MAE=0.210858 validation MAE=0.505183,rank=5\n",
      "[SoftImpute] Iter 115: observed MAE=0.210148 validation MAE=0.504810,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.210419 validation MAE=0.504824,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 117: observed MAE=0.210441 validation MAE=0.504803,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.209360 validation MAE=0.504062,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.209488 validation MAE=0.503975,rank=5\n",
      "[SoftImpute] Iter 120: observed MAE=0.208290 validation MAE=0.503976,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.208651 validation MAE=0.503667,rank=5\n",
      "[SoftImpute] Iter 122: observed MAE=0.207185 validation MAE=0.503404,rank=5\n",
      "[SoftImpute] Iter 123: observed MAE=0.206966 validation MAE=0.503295,rank=5\n",
      "[SoftImpute] Iter 124: observed MAE=0.207116 validation MAE=0.503177,rank=5\n",
      "[SoftImpute] Iter 125: observed MAE=0.206468 validation MAE=0.502985,rank=5\n",
      "[SoftImpute] Iter 126: observed MAE=0.206061 validation MAE=0.502570,rank=5\n",
      "[SoftImpute] Iter 127: observed MAE=0.206273 validation MAE=0.502462,rank=5\n",
      "[SoftImpute] Iter 128: observed MAE=0.204817 validation MAE=0.502233,rank=5\n",
      "[SoftImpute] Iter 129: observed MAE=0.205135 validation MAE=0.502099,rank=5\n",
      "[SoftImpute] Iter 130: observed MAE=0.204636 validation MAE=0.501775,rank=5\n",
      "[SoftImpute] Iter 131: observed MAE=0.205095 validation MAE=0.501543,rank=5\n",
      "[SoftImpute] Iter 132: observed MAE=0.204191 validation MAE=0.501274,rank=5\n",
      "[SoftImpute] Iter 133: observed MAE=0.204212 validation MAE=0.501206,rank=5\n",
      "[SoftImpute] Iter 134: observed MAE=0.203135 validation MAE=0.500970,rank=5\n",
      "[SoftImpute] Iter 135: observed MAE=0.202905 validation MAE=0.501317,rank=5\n",
      "[SoftImpute] Iter 136: observed MAE=0.202913 validation MAE=0.500677,rank=5\n",
      "[SoftImpute] Iter 137: observed MAE=0.202908 validation MAE=0.500744,rank=5\n",
      "[SoftImpute] Iter 138: observed MAE=0.201928 validation MAE=0.500337,rank=5\n",
      "[SoftImpute] Iter 139: observed MAE=0.202651 validation MAE=0.499872,rank=5\n",
      "[SoftImpute] Iter 140: observed MAE=0.202335 validation MAE=0.499760,rank=5\n",
      "[SoftImpute] Iter 141: observed MAE=0.201575 validation MAE=0.499772,rank=5\n",
      "[SoftImpute] Iter 142: observed MAE=0.201593 validation MAE=0.499341,rank=5\n",
      "[SoftImpute] Iter 143: observed MAE=0.201724 validation MAE=0.499437,rank=5\n",
      "[SoftImpute] Iter 144: observed MAE=0.200456 validation MAE=0.499593,rank=5\n",
      "[SoftImpute] Iter 145: observed MAE=0.201058 validation MAE=0.499100,rank=5\n",
      "[SoftImpute] Iter 146: observed MAE=0.200565 validation MAE=0.498969,rank=5\n",
      "[SoftImpute] Iter 147: observed MAE=0.199964 validation MAE=0.498944,rank=5\n",
      "[SoftImpute] Iter 148: observed MAE=0.199698 validation MAE=0.498412,rank=5\n",
      "[SoftImpute] Iter 149: observed MAE=0.199941 validation MAE=0.498501,rank=5\n",
      "[SoftImpute] Iter 150: observed MAE=0.198927 validation MAE=0.498343,rank=5\n",
      "[SoftImpute] Iter 151: observed MAE=0.199881 validation MAE=0.498086,rank=5\n",
      "[SoftImpute] Iter 152: observed MAE=0.199817 validation MAE=0.498089,rank=5\n",
      "[SoftImpute] Iter 153: observed MAE=0.199600 validation MAE=0.497648,rank=5\n",
      "[SoftImpute] Iter 154: observed MAE=0.198637 validation MAE=0.497954,rank=5\n",
      "[SoftImpute] Iter 155: observed MAE=0.198852 validation MAE=0.497704,rank=5\n",
      "[SoftImpute] Iter 156: observed MAE=0.199076 validation MAE=0.497629,rank=5\n",
      "[SoftImpute] Iter 157: observed MAE=0.197251 validation MAE=0.497639,rank=5\n",
      "[SoftImpute] Iter 158: observed MAE=0.197486 validation MAE=0.497488,rank=5\n",
      "[SoftImpute] Iter 159: observed MAE=0.197705 validation MAE=0.497666,rank=5\n",
      "[SoftImpute] Iter 160: observed MAE=0.196723 validation MAE=0.497364,rank=5\n",
      "[SoftImpute] Iter 161: observed MAE=0.197324 validation MAE=0.497387,rank=5\n",
      "[SoftImpute] Iter 162: observed MAE=0.197622 validation MAE=0.497036,rank=5\n",
      "[SoftImpute] Iter 163: observed MAE=0.196653 validation MAE=0.496788,rank=5\n",
      "[SoftImpute] Iter 164: observed MAE=0.196143 validation MAE=0.496814,rank=5\n",
      "[SoftImpute] Iter 165: observed MAE=0.196709 validation MAE=0.496480,rank=5\n",
      "[SoftImpute] Iter 166: observed MAE=0.195920 validation MAE=0.496626,rank=5\n",
      "[SoftImpute] Iter 167: observed MAE=0.195193 validation MAE=0.496690,rank=5\n",
      "[SoftImpute] Iter 168: observed MAE=0.194807 validation MAE=0.496673,rank=5\n",
      "[SoftImpute] Iter 169: observed MAE=0.195172 validation MAE=0.496648,rank=5\n",
      "[SoftImpute] Iter 170: observed MAE=0.195670 validation MAE=0.496632,rank=5\n",
      "[SoftImpute] Iter 171: observed MAE=0.195540 validation MAE=0.496533,rank=5\n",
      "[SoftImpute] Iter 172: observed MAE=0.194321 validation MAE=0.496108,rank=5\n",
      "[SoftImpute] Iter 173: observed MAE=0.195180 validation MAE=0.495918,rank=5\n",
      "[SoftImpute] Iter 174: observed MAE=0.194473 validation MAE=0.495786,rank=5\n",
      "[SoftImpute] Iter 175: observed MAE=0.194769 validation MAE=0.495672,rank=5\n",
      "[SoftImpute] Iter 176: observed MAE=0.193523 validation MAE=0.495650,rank=5\n",
      "[SoftImpute] Iter 177: observed MAE=0.193492 validation MAE=0.495514,rank=5\n",
      "[SoftImpute] Iter 178: observed MAE=0.193643 validation MAE=0.495394,rank=5\n",
      "[SoftImpute] Iter 179: observed MAE=0.193844 validation MAE=0.495584,rank=5\n",
      "[SoftImpute] Iter 180: observed MAE=0.193699 validation MAE=0.495452,rank=5\n",
      "[SoftImpute] Iter 181: observed MAE=0.193140 validation MAE=0.495467,rank=5\n",
      "[SoftImpute] Iter 182: observed MAE=0.192874 validation MAE=0.494962,rank=5\n",
      "[SoftImpute] Iter 183: observed MAE=0.192690 validation MAE=0.495050,rank=5\n",
      "[SoftImpute] Iter 184: observed MAE=0.192986 validation MAE=0.494934,rank=5\n",
      "[SoftImpute] Iter 185: observed MAE=0.191651 validation MAE=0.494606,rank=5\n",
      "[SoftImpute] Iter 186: observed MAE=0.192314 validation MAE=0.494390,rank=5\n",
      "[SoftImpute] Iter 187: observed MAE=0.192465 validation MAE=0.494238,rank=5\n",
      "[SoftImpute] Iter 188: observed MAE=0.192258 validation MAE=0.494785,rank=5\n",
      "[SoftImpute] Iter 189: observed MAE=0.191595 validation MAE=0.494288,rank=5\n",
      "[SoftImpute] Iter 190: observed MAE=0.191706 validation MAE=0.493956,rank=5\n",
      "[SoftImpute] Iter 191: observed MAE=0.191384 validation MAE=0.493880,rank=5\n",
      "[SoftImpute] Iter 192: observed MAE=0.191238 validation MAE=0.493865,rank=5\n",
      "[SoftImpute] Iter 193: observed MAE=0.191316 validation MAE=0.493499,rank=5\n",
      "[SoftImpute] Iter 194: observed MAE=0.191161 validation MAE=0.493885,rank=5\n",
      "[SoftImpute] Iter 195: observed MAE=0.190390 validation MAE=0.493711,rank=5\n",
      "[SoftImpute] Iter 196: observed MAE=0.190509 validation MAE=0.493347,rank=5\n",
      "[SoftImpute] Iter 197: observed MAE=0.190133 validation MAE=0.493365,rank=5\n",
      "[SoftImpute] Iter 198: observed MAE=0.190264 validation MAE=0.493100,rank=5\n",
      "[SoftImpute] Iter 199: observed MAE=0.189916 validation MAE=0.492751,rank=5\n",
      "[SoftImpute] Iter 200: observed MAE=0.189883 validation MAE=0.492769,rank=5\n",
      "[SoftImpute] Stopped after iteration 200 for lambda=0.408315\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 5.834384441375732\n",
      "After the matrix factor stage, training error is 0.18988, validation error is 0.49277\n",
      "9\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.04054, val loss: 4.17642\n",
      "Main effects training epoch: 2, train loss: 3.83978, val loss: 3.98160\n",
      "Main effects training epoch: 3, train loss: 3.63593, val loss: 3.77780\n",
      "Main effects training epoch: 4, train loss: 3.45334, val loss: 3.58672\n",
      "Main effects training epoch: 5, train loss: 3.43677, val loss: 3.53877\n",
      "Main effects training epoch: 6, train loss: 3.29006, val loss: 3.36685\n",
      "Main effects training epoch: 7, train loss: 3.25791, val loss: 3.33149\n",
      "Main effects training epoch: 8, train loss: 3.30670, val loss: 3.40234\n",
      "Main effects training epoch: 9, train loss: 3.19313, val loss: 3.29150\n",
      "Main effects training epoch: 10, train loss: 3.14736, val loss: 3.23704\n",
      "Main effects training epoch: 11, train loss: 3.13481, val loss: 3.22074\n",
      "Main effects training epoch: 12, train loss: 3.08458, val loss: 3.17727\n",
      "Main effects training epoch: 13, train loss: 2.97288, val loss: 3.06046\n",
      "Main effects training epoch: 14, train loss: 2.96188, val loss: 3.05278\n",
      "Main effects training epoch: 15, train loss: 2.87923, val loss: 2.96951\n",
      "Main effects training epoch: 16, train loss: 2.80451, val loss: 2.88979\n",
      "Main effects training epoch: 17, train loss: 2.74464, val loss: 2.83293\n",
      "Main effects training epoch: 18, train loss: 2.69655, val loss: 2.78016\n",
      "Main effects training epoch: 19, train loss: 2.62865, val loss: 2.71495\n",
      "Main effects training epoch: 20, train loss: 2.56729, val loss: 2.64770\n",
      "Main effects training epoch: 21, train loss: 2.53391, val loss: 2.60548\n",
      "Main effects training epoch: 22, train loss: 2.44579, val loss: 2.51655\n",
      "Main effects training epoch: 23, train loss: 2.43399, val loss: 2.50180\n",
      "Main effects training epoch: 24, train loss: 2.37110, val loss: 2.44494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 25, train loss: 2.33821, val loss: 2.41624\n",
      "Main effects training epoch: 26, train loss: 2.30939, val loss: 2.38706\n",
      "Main effects training epoch: 27, train loss: 2.28377, val loss: 2.36142\n",
      "Main effects training epoch: 28, train loss: 2.25496, val loss: 2.33129\n",
      "Main effects training epoch: 29, train loss: 2.22501, val loss: 2.30349\n",
      "Main effects training epoch: 30, train loss: 2.20637, val loss: 2.28959\n",
      "Main effects training epoch: 31, train loss: 2.18614, val loss: 2.27490\n",
      "Main effects training epoch: 32, train loss: 2.13531, val loss: 2.21596\n",
      "Main effects training epoch: 33, train loss: 2.13496, val loss: 2.21395\n",
      "Main effects training epoch: 34, train loss: 2.11050, val loss: 2.19239\n",
      "Main effects training epoch: 35, train loss: 2.06667, val loss: 2.15022\n",
      "Main effects training epoch: 36, train loss: 2.07002, val loss: 2.14888\n",
      "Main effects training epoch: 37, train loss: 2.03589, val loss: 2.12398\n",
      "Main effects training epoch: 38, train loss: 2.02183, val loss: 2.10678\n",
      "Main effects training epoch: 39, train loss: 2.00129, val loss: 2.07696\n",
      "Main effects training epoch: 40, train loss: 1.99990, val loss: 2.08408\n",
      "Main effects training epoch: 41, train loss: 1.96668, val loss: 2.04538\n",
      "Main effects training epoch: 42, train loss: 1.94603, val loss: 2.02671\n",
      "Main effects training epoch: 43, train loss: 1.95107, val loss: 2.02491\n",
      "Main effects training epoch: 44, train loss: 1.92145, val loss: 1.99792\n",
      "Main effects training epoch: 45, train loss: 1.90420, val loss: 1.98325\n",
      "Main effects training epoch: 46, train loss: 1.90742, val loss: 1.97810\n",
      "Main effects training epoch: 47, train loss: 1.88371, val loss: 1.97085\n",
      "Main effects training epoch: 48, train loss: 1.86837, val loss: 1.94115\n",
      "Main effects training epoch: 49, train loss: 1.86788, val loss: 1.94107\n",
      "Main effects training epoch: 50, train loss: 1.84089, val loss: 1.91192\n",
      "Main effects training epoch: 51, train loss: 1.83780, val loss: 1.90855\n",
      "Main effects training epoch: 52, train loss: 1.83954, val loss: 1.91282\n",
      "Main effects training epoch: 53, train loss: 1.82514, val loss: 1.89584\n",
      "Main effects training epoch: 54, train loss: 1.80669, val loss: 1.87556\n",
      "Main effects training epoch: 55, train loss: 1.80307, val loss: 1.86996\n",
      "Main effects training epoch: 56, train loss: 1.80675, val loss: 1.88038\n",
      "Main effects training epoch: 57, train loss: 1.79083, val loss: 1.85939\n",
      "Main effects training epoch: 58, train loss: 1.79766, val loss: 1.86330\n",
      "Main effects training epoch: 59, train loss: 1.77883, val loss: 1.84555\n",
      "Main effects training epoch: 60, train loss: 1.77797, val loss: 1.84699\n",
      "Main effects training epoch: 61, train loss: 1.77174, val loss: 1.83087\n",
      "Main effects training epoch: 62, train loss: 1.76318, val loss: 1.82991\n",
      "Main effects training epoch: 63, train loss: 1.76301, val loss: 1.82497\n",
      "Main effects training epoch: 64, train loss: 1.75537, val loss: 1.81784\n",
      "Main effects training epoch: 65, train loss: 1.74875, val loss: 1.80924\n",
      "Main effects training epoch: 66, train loss: 1.75059, val loss: 1.81190\n",
      "Main effects training epoch: 67, train loss: 1.74922, val loss: 1.80946\n",
      "Main effects training epoch: 68, train loss: 1.74061, val loss: 1.79859\n",
      "Main effects training epoch: 69, train loss: 1.74184, val loss: 1.80099\n",
      "Main effects training epoch: 70, train loss: 1.73818, val loss: 1.79458\n",
      "Main effects training epoch: 71, train loss: 1.72887, val loss: 1.78399\n",
      "Main effects training epoch: 72, train loss: 1.73612, val loss: 1.79513\n",
      "Main effects training epoch: 73, train loss: 1.72610, val loss: 1.77995\n",
      "Main effects training epoch: 74, train loss: 1.72688, val loss: 1.77784\n",
      "Main effects training epoch: 75, train loss: 1.72638, val loss: 1.78121\n",
      "Main effects training epoch: 76, train loss: 1.72605, val loss: 1.77647\n",
      "Main effects training epoch: 77, train loss: 1.72194, val loss: 1.77400\n",
      "Main effects training epoch: 78, train loss: 1.71697, val loss: 1.77189\n",
      "Main effects training epoch: 79, train loss: 1.71774, val loss: 1.77009\n",
      "Main effects training epoch: 80, train loss: 1.71533, val loss: 1.76760\n",
      "Main effects training epoch: 81, train loss: 1.71379, val loss: 1.76313\n",
      "Main effects training epoch: 82, train loss: 1.71539, val loss: 1.76771\n",
      "Main effects training epoch: 83, train loss: 1.70979, val loss: 1.75626\n",
      "Main effects training epoch: 84, train loss: 1.70653, val loss: 1.75702\n",
      "Main effects training epoch: 85, train loss: 1.70916, val loss: 1.76105\n",
      "Main effects training epoch: 86, train loss: 1.70808, val loss: 1.75797\n",
      "Main effects training epoch: 87, train loss: 1.70469, val loss: 1.75298\n",
      "Main effects training epoch: 88, train loss: 1.70032, val loss: 1.74959\n",
      "Main effects training epoch: 89, train loss: 1.69733, val loss: 1.74981\n",
      "Main effects training epoch: 90, train loss: 1.69550, val loss: 1.74440\n",
      "Main effects training epoch: 91, train loss: 1.69440, val loss: 1.74699\n",
      "Main effects training epoch: 92, train loss: 1.69232, val loss: 1.74489\n",
      "Main effects training epoch: 93, train loss: 1.68867, val loss: 1.73850\n",
      "Main effects training epoch: 94, train loss: 1.68692, val loss: 1.73997\n",
      "Main effects training epoch: 95, train loss: 1.68149, val loss: 1.73649\n",
      "Main effects training epoch: 96, train loss: 1.67816, val loss: 1.72984\n",
      "Main effects training epoch: 97, train loss: 1.67287, val loss: 1.73222\n",
      "Main effects training epoch: 98, train loss: 1.66515, val loss: 1.72014\n",
      "Main effects training epoch: 99, train loss: 1.66100, val loss: 1.72772\n",
      "Main effects training epoch: 100, train loss: 1.65680, val loss: 1.71607\n",
      "Main effects training epoch: 101, train loss: 1.65296, val loss: 1.72419\n",
      "Main effects training epoch: 102, train loss: 1.64588, val loss: 1.70730\n",
      "Main effects training epoch: 103, train loss: 1.63789, val loss: 1.71259\n",
      "Main effects training epoch: 104, train loss: 1.63730, val loss: 1.70564\n",
      "Main effects training epoch: 105, train loss: 1.63124, val loss: 1.69789\n",
      "Main effects training epoch: 106, train loss: 1.63615, val loss: 1.70591\n",
      "Main effects training epoch: 107, train loss: 1.62711, val loss: 1.70392\n",
      "Main effects training epoch: 108, train loss: 1.62499, val loss: 1.69506\n",
      "Main effects training epoch: 109, train loss: 1.62852, val loss: 1.69472\n",
      "Main effects training epoch: 110, train loss: 1.62952, val loss: 1.70363\n",
      "Main effects training epoch: 111, train loss: 1.61890, val loss: 1.69226\n",
      "Main effects training epoch: 112, train loss: 1.61571, val loss: 1.69116\n",
      "Main effects training epoch: 113, train loss: 1.61778, val loss: 1.68051\n",
      "Main effects training epoch: 114, train loss: 1.62227, val loss: 1.70399\n",
      "Main effects training epoch: 115, train loss: 1.61969, val loss: 1.68188\n",
      "Main effects training epoch: 116, train loss: 1.61368, val loss: 1.69684\n",
      "Main effects training epoch: 117, train loss: 1.60978, val loss: 1.67813\n",
      "Main effects training epoch: 118, train loss: 1.61960, val loss: 1.68544\n",
      "Main effects training epoch: 119, train loss: 1.61533, val loss: 1.69068\n",
      "Main effects training epoch: 120, train loss: 1.61183, val loss: 1.67684\n",
      "Main effects training epoch: 121, train loss: 1.61693, val loss: 1.68878\n",
      "Main effects training epoch: 122, train loss: 1.60977, val loss: 1.68881\n",
      "Main effects training epoch: 123, train loss: 1.61000, val loss: 1.68155\n",
      "Main effects training epoch: 124, train loss: 1.60564, val loss: 1.67841\n",
      "Main effects training epoch: 125, train loss: 1.60619, val loss: 1.67314\n",
      "Main effects training epoch: 126, train loss: 1.60494, val loss: 1.68736\n",
      "Main effects training epoch: 127, train loss: 1.61232, val loss: 1.68036\n",
      "Main effects training epoch: 128, train loss: 1.60441, val loss: 1.67985\n",
      "Main effects training epoch: 129, train loss: 1.60663, val loss: 1.68617\n",
      "Main effects training epoch: 130, train loss: 1.60364, val loss: 1.67674\n",
      "Main effects training epoch: 131, train loss: 1.60371, val loss: 1.67212\n",
      "Main effects training epoch: 132, train loss: 1.60037, val loss: 1.67841\n",
      "Main effects training epoch: 133, train loss: 1.59959, val loss: 1.68002\n",
      "Main effects training epoch: 134, train loss: 1.60204, val loss: 1.67260\n",
      "Main effects training epoch: 135, train loss: 1.59865, val loss: 1.67963\n",
      "Main effects training epoch: 136, train loss: 1.60504, val loss: 1.68138\n",
      "Main effects training epoch: 137, train loss: 1.60681, val loss: 1.68456\n",
      "Main effects training epoch: 138, train loss: 1.60081, val loss: 1.68245\n",
      "Main effects training epoch: 139, train loss: 1.59745, val loss: 1.67379\n",
      "Main effects training epoch: 140, train loss: 1.59809, val loss: 1.67481\n",
      "Main effects training epoch: 141, train loss: 1.59462, val loss: 1.67326\n",
      "Main effects training epoch: 142, train loss: 1.59441, val loss: 1.68221\n",
      "Main effects training epoch: 143, train loss: 1.59593, val loss: 1.67560\n",
      "Main effects training epoch: 144, train loss: 1.59492, val loss: 1.67555\n",
      "Main effects training epoch: 145, train loss: 1.59592, val loss: 1.68777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 146, train loss: 1.59379, val loss: 1.67523\n",
      "Main effects training epoch: 147, train loss: 1.59068, val loss: 1.67440\n",
      "Main effects training epoch: 148, train loss: 1.59253, val loss: 1.67441\n",
      "Main effects training epoch: 149, train loss: 1.59019, val loss: 1.68342\n",
      "Main effects training epoch: 150, train loss: 1.58957, val loss: 1.67371\n",
      "Main effects training epoch: 151, train loss: 1.59217, val loss: 1.67824\n",
      "Main effects training epoch: 152, train loss: 1.58920, val loss: 1.67851\n",
      "Main effects training epoch: 153, train loss: 1.58683, val loss: 1.67082\n",
      "Main effects training epoch: 154, train loss: 1.58834, val loss: 1.67945\n",
      "Main effects training epoch: 155, train loss: 1.58615, val loss: 1.66991\n",
      "Main effects training epoch: 156, train loss: 1.58629, val loss: 1.67836\n",
      "Main effects training epoch: 157, train loss: 1.58733, val loss: 1.67168\n",
      "Main effects training epoch: 158, train loss: 1.58653, val loss: 1.68086\n",
      "Main effects training epoch: 159, train loss: 1.58303, val loss: 1.67098\n",
      "Main effects training epoch: 160, train loss: 1.59100, val loss: 1.67731\n",
      "Main effects training epoch: 161, train loss: 1.58322, val loss: 1.67050\n",
      "Main effects training epoch: 162, train loss: 1.58293, val loss: 1.67288\n",
      "Main effects training epoch: 163, train loss: 1.58107, val loss: 1.66795\n",
      "Main effects training epoch: 164, train loss: 1.58258, val loss: 1.67858\n",
      "Main effects training epoch: 165, train loss: 1.58188, val loss: 1.66798\n",
      "Main effects training epoch: 166, train loss: 1.58215, val loss: 1.67074\n",
      "Main effects training epoch: 167, train loss: 1.57800, val loss: 1.66980\n",
      "Main effects training epoch: 168, train loss: 1.58168, val loss: 1.67611\n",
      "Main effects training epoch: 169, train loss: 1.57717, val loss: 1.66393\n",
      "Main effects training epoch: 170, train loss: 1.57544, val loss: 1.66806\n",
      "Main effects training epoch: 171, train loss: 1.58146, val loss: 1.67078\n",
      "Main effects training epoch: 172, train loss: 1.57646, val loss: 1.66314\n",
      "Main effects training epoch: 173, train loss: 1.57535, val loss: 1.66626\n",
      "Main effects training epoch: 174, train loss: 1.57717, val loss: 1.66375\n",
      "Main effects training epoch: 175, train loss: 1.57259, val loss: 1.66159\n",
      "Main effects training epoch: 176, train loss: 1.57729, val loss: 1.67606\n",
      "Main effects training epoch: 177, train loss: 1.57823, val loss: 1.66316\n",
      "Main effects training epoch: 178, train loss: 1.57659, val loss: 1.66413\n",
      "Main effects training epoch: 179, train loss: 1.57044, val loss: 1.66542\n",
      "Main effects training epoch: 180, train loss: 1.56719, val loss: 1.65176\n",
      "Main effects training epoch: 181, train loss: 1.56723, val loss: 1.65970\n",
      "Main effects training epoch: 182, train loss: 1.56463, val loss: 1.64989\n",
      "Main effects training epoch: 183, train loss: 1.56509, val loss: 1.65265\n",
      "Main effects training epoch: 184, train loss: 1.56483, val loss: 1.65452\n",
      "Main effects training epoch: 185, train loss: 1.56428, val loss: 1.64651\n",
      "Main effects training epoch: 186, train loss: 1.56569, val loss: 1.65310\n",
      "Main effects training epoch: 187, train loss: 1.56113, val loss: 1.64099\n",
      "Main effects training epoch: 188, train loss: 1.55777, val loss: 1.65320\n",
      "Main effects training epoch: 189, train loss: 1.55946, val loss: 1.63377\n",
      "Main effects training epoch: 190, train loss: 1.55647, val loss: 1.64973\n",
      "Main effects training epoch: 191, train loss: 1.55451, val loss: 1.63016\n",
      "Main effects training epoch: 192, train loss: 1.55425, val loss: 1.64659\n",
      "Main effects training epoch: 193, train loss: 1.55600, val loss: 1.64258\n",
      "Main effects training epoch: 194, train loss: 1.54931, val loss: 1.62644\n",
      "Main effects training epoch: 195, train loss: 1.55096, val loss: 1.64168\n",
      "Main effects training epoch: 196, train loss: 1.55229, val loss: 1.63790\n",
      "Main effects training epoch: 197, train loss: 1.55352, val loss: 1.63588\n",
      "Main effects training epoch: 198, train loss: 1.55769, val loss: 1.65537\n",
      "Main effects training epoch: 199, train loss: 1.55265, val loss: 1.63458\n",
      "Main effects training epoch: 200, train loss: 1.55287, val loss: 1.63704\n",
      "Main effects training epoch: 201, train loss: 1.55554, val loss: 1.63409\n",
      "Main effects training epoch: 202, train loss: 1.55994, val loss: 1.64852\n",
      "Main effects training epoch: 203, train loss: 1.55162, val loss: 1.63202\n",
      "Main effects training epoch: 204, train loss: 1.56254, val loss: 1.65096\n",
      "Main effects training epoch: 205, train loss: 1.54767, val loss: 1.62527\n",
      "Main effects training epoch: 206, train loss: 1.55160, val loss: 1.62955\n",
      "Main effects training epoch: 207, train loss: 1.54867, val loss: 1.63530\n",
      "Main effects training epoch: 208, train loss: 1.54791, val loss: 1.62861\n",
      "Main effects training epoch: 209, train loss: 1.56241, val loss: 1.64481\n",
      "Main effects training epoch: 210, train loss: 1.54597, val loss: 1.62807\n",
      "Main effects training epoch: 211, train loss: 1.54342, val loss: 1.62995\n",
      "Main effects training epoch: 212, train loss: 1.54781, val loss: 1.63228\n",
      "Main effects training epoch: 213, train loss: 1.54560, val loss: 1.63152\n",
      "Main effects training epoch: 214, train loss: 1.54688, val loss: 1.63594\n",
      "Main effects training epoch: 215, train loss: 1.55627, val loss: 1.63939\n",
      "Main effects training epoch: 216, train loss: 1.54471, val loss: 1.63048\n",
      "Main effects training epoch: 217, train loss: 1.54636, val loss: 1.62574\n",
      "Main effects training epoch: 218, train loss: 1.54388, val loss: 1.63335\n",
      "Main effects training epoch: 219, train loss: 1.54978, val loss: 1.62647\n",
      "Main effects training epoch: 220, train loss: 1.55583, val loss: 1.64542\n",
      "Main effects training epoch: 221, train loss: 1.55033, val loss: 1.62413\n",
      "Main effects training epoch: 222, train loss: 1.55185, val loss: 1.63329\n",
      "Main effects training epoch: 223, train loss: 1.55388, val loss: 1.63464\n",
      "Main effects training epoch: 224, train loss: 1.54620, val loss: 1.63427\n",
      "Main effects training epoch: 225, train loss: 1.54302, val loss: 1.62239\n",
      "Main effects training epoch: 226, train loss: 1.54851, val loss: 1.63117\n",
      "Main effects training epoch: 227, train loss: 1.54605, val loss: 1.63473\n",
      "Main effects training epoch: 228, train loss: 1.54577, val loss: 1.62878\n",
      "Main effects training epoch: 229, train loss: 1.54873, val loss: 1.63108\n",
      "Main effects training epoch: 230, train loss: 1.53996, val loss: 1.63127\n",
      "Main effects training epoch: 231, train loss: 1.54279, val loss: 1.61758\n",
      "Main effects training epoch: 232, train loss: 1.54250, val loss: 1.63247\n",
      "Main effects training epoch: 233, train loss: 1.53998, val loss: 1.62461\n",
      "Main effects training epoch: 234, train loss: 1.54448, val loss: 1.62538\n",
      "Main effects training epoch: 235, train loss: 1.54599, val loss: 1.63329\n",
      "Main effects training epoch: 236, train loss: 1.54134, val loss: 1.62376\n",
      "Main effects training epoch: 237, train loss: 1.54013, val loss: 1.62374\n",
      "Main effects training epoch: 238, train loss: 1.53803, val loss: 1.62560\n",
      "Main effects training epoch: 239, train loss: 1.54169, val loss: 1.61453\n",
      "Main effects training epoch: 240, train loss: 1.54062, val loss: 1.62828\n",
      "Main effects training epoch: 241, train loss: 1.54389, val loss: 1.62723\n",
      "Main effects training epoch: 242, train loss: 1.54914, val loss: 1.62306\n",
      "Main effects training epoch: 243, train loss: 1.54533, val loss: 1.63141\n",
      "Main effects training epoch: 244, train loss: 1.53943, val loss: 1.62837\n",
      "Main effects training epoch: 245, train loss: 1.54035, val loss: 1.61745\n",
      "Main effects training epoch: 246, train loss: 1.53856, val loss: 1.62285\n",
      "Main effects training epoch: 247, train loss: 1.53855, val loss: 1.61933\n",
      "Main effects training epoch: 248, train loss: 1.53701, val loss: 1.62055\n",
      "Main effects training epoch: 249, train loss: 1.53838, val loss: 1.62482\n",
      "Main effects training epoch: 250, train loss: 1.54493, val loss: 1.62438\n",
      "Main effects training epoch: 251, train loss: 1.54209, val loss: 1.62644\n",
      "Main effects training epoch: 252, train loss: 1.53960, val loss: 1.61897\n",
      "Main effects training epoch: 253, train loss: 1.53973, val loss: 1.62681\n",
      "Main effects training epoch: 254, train loss: 1.53718, val loss: 1.61543\n",
      "Main effects training epoch: 255, train loss: 1.54106, val loss: 1.62922\n",
      "Main effects training epoch: 256, train loss: 1.53718, val loss: 1.62823\n",
      "Main effects training epoch: 257, train loss: 1.53639, val loss: 1.61538\n",
      "Main effects training epoch: 258, train loss: 1.53367, val loss: 1.62572\n",
      "Main effects training epoch: 259, train loss: 1.53908, val loss: 1.62350\n",
      "Main effects training epoch: 260, train loss: 1.53773, val loss: 1.61755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 261, train loss: 1.54120, val loss: 1.63445\n",
      "Main effects training epoch: 262, train loss: 1.53887, val loss: 1.61798\n",
      "Main effects training epoch: 263, train loss: 1.53939, val loss: 1.62914\n",
      "Main effects training epoch: 264, train loss: 1.53378, val loss: 1.61284\n",
      "Main effects training epoch: 265, train loss: 1.53557, val loss: 1.61541\n",
      "Main effects training epoch: 266, train loss: 1.53215, val loss: 1.61924\n",
      "Main effects training epoch: 267, train loss: 1.53351, val loss: 1.62173\n",
      "Main effects training epoch: 268, train loss: 1.53473, val loss: 1.61716\n",
      "Main effects training epoch: 269, train loss: 1.53571, val loss: 1.61589\n",
      "Main effects training epoch: 270, train loss: 1.53266, val loss: 1.62267\n",
      "Main effects training epoch: 271, train loss: 1.53514, val loss: 1.61643\n",
      "Main effects training epoch: 272, train loss: 1.53182, val loss: 1.61207\n",
      "Main effects training epoch: 273, train loss: 1.53149, val loss: 1.61824\n",
      "Main effects training epoch: 274, train loss: 1.53253, val loss: 1.61316\n",
      "Main effects training epoch: 275, train loss: 1.53173, val loss: 1.62278\n",
      "Main effects training epoch: 276, train loss: 1.52974, val loss: 1.60738\n",
      "Main effects training epoch: 277, train loss: 1.52867, val loss: 1.62064\n",
      "Main effects training epoch: 278, train loss: 1.52911, val loss: 1.61377\n",
      "Main effects training epoch: 279, train loss: 1.53233, val loss: 1.62018\n",
      "Main effects training epoch: 280, train loss: 1.53000, val loss: 1.61611\n",
      "Main effects training epoch: 281, train loss: 1.52913, val loss: 1.62132\n",
      "Main effects training epoch: 282, train loss: 1.52654, val loss: 1.61359\n",
      "Main effects training epoch: 283, train loss: 1.52897, val loss: 1.60902\n",
      "Main effects training epoch: 284, train loss: 1.53055, val loss: 1.61546\n",
      "Main effects training epoch: 285, train loss: 1.52796, val loss: 1.61332\n",
      "Main effects training epoch: 286, train loss: 1.52856, val loss: 1.61688\n",
      "Main effects training epoch: 287, train loss: 1.52802, val loss: 1.61643\n",
      "Main effects training epoch: 288, train loss: 1.52438, val loss: 1.61194\n",
      "Main effects training epoch: 289, train loss: 1.52517, val loss: 1.60571\n",
      "Main effects training epoch: 290, train loss: 1.52450, val loss: 1.62052\n",
      "Main effects training epoch: 291, train loss: 1.52432, val loss: 1.60212\n",
      "Main effects training epoch: 292, train loss: 1.52290, val loss: 1.61192\n",
      "Main effects training epoch: 293, train loss: 1.52215, val loss: 1.61200\n",
      "Main effects training epoch: 294, train loss: 1.52485, val loss: 1.61279\n",
      "Main effects training epoch: 295, train loss: 1.52842, val loss: 1.61037\n",
      "Main effects training epoch: 296, train loss: 1.52384, val loss: 1.61213\n",
      "Main effects training epoch: 297, train loss: 1.52285, val loss: 1.60044\n",
      "Main effects training epoch: 298, train loss: 1.52209, val loss: 1.61239\n",
      "Main effects training epoch: 299, train loss: 1.51911, val loss: 1.59778\n",
      "Main effects training epoch: 300, train loss: 1.52033, val loss: 1.61658\n",
      "##########Stage 1: main effect training stop.##########\n",
      "3 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.53865, val loss: 1.61406\n",
      "Main effects tuning epoch: 2, train loss: 1.53849, val loss: 1.60571\n",
      "Main effects tuning epoch: 3, train loss: 1.53688, val loss: 1.62233\n",
      "Main effects tuning epoch: 4, train loss: 1.53899, val loss: 1.60364\n",
      "Main effects tuning epoch: 5, train loss: 1.54103, val loss: 1.61761\n",
      "Main effects tuning epoch: 6, train loss: 1.53771, val loss: 1.60657\n",
      "Main effects tuning epoch: 7, train loss: 1.53498, val loss: 1.60664\n",
      "Main effects tuning epoch: 8, train loss: 1.53345, val loss: 1.61303\n",
      "Main effects tuning epoch: 9, train loss: 1.53282, val loss: 1.60313\n",
      "Main effects tuning epoch: 10, train loss: 1.53465, val loss: 1.61397\n",
      "Main effects tuning epoch: 11, train loss: 1.53150, val loss: 1.60975\n",
      "Main effects tuning epoch: 12, train loss: 1.52904, val loss: 1.60162\n",
      "Main effects tuning epoch: 13, train loss: 1.53135, val loss: 1.61338\n",
      "Main effects tuning epoch: 14, train loss: 1.52973, val loss: 1.60078\n",
      "Main effects tuning epoch: 15, train loss: 1.52993, val loss: 1.61283\n",
      "Main effects tuning epoch: 16, train loss: 1.52929, val loss: 1.59992\n",
      "Main effects tuning epoch: 17, train loss: 1.52867, val loss: 1.61062\n",
      "Main effects tuning epoch: 18, train loss: 1.52676, val loss: 1.60160\n",
      "Main effects tuning epoch: 19, train loss: 1.52924, val loss: 1.60805\n",
      "Main effects tuning epoch: 20, train loss: 1.52848, val loss: 1.60089\n",
      "Main effects tuning epoch: 21, train loss: 1.52436, val loss: 1.59810\n",
      "Main effects tuning epoch: 22, train loss: 1.52447, val loss: 1.60481\n",
      "Main effects tuning epoch: 23, train loss: 1.52461, val loss: 1.58827\n",
      "Main effects tuning epoch: 24, train loss: 1.52291, val loss: 1.60513\n",
      "Main effects tuning epoch: 25, train loss: 1.52311, val loss: 1.59759\n",
      "Main effects tuning epoch: 26, train loss: 1.52130, val loss: 1.59632\n",
      "Main effects tuning epoch: 27, train loss: 1.52032, val loss: 1.59793\n",
      "Main effects tuning epoch: 28, train loss: 1.52186, val loss: 1.59740\n",
      "Main effects tuning epoch: 29, train loss: 1.51961, val loss: 1.59923\n",
      "Main effects tuning epoch: 30, train loss: 1.51630, val loss: 1.59467\n",
      "Main effects tuning epoch: 31, train loss: 1.52053, val loss: 1.60179\n",
      "Main effects tuning epoch: 32, train loss: 1.52460, val loss: 1.59829\n",
      "Main effects tuning epoch: 33, train loss: 1.52498, val loss: 1.59948\n",
      "Main effects tuning epoch: 34, train loss: 1.51899, val loss: 1.59354\n",
      "Main effects tuning epoch: 35, train loss: 1.51621, val loss: 1.59225\n",
      "Main effects tuning epoch: 36, train loss: 1.51795, val loss: 1.59464\n",
      "Main effects tuning epoch: 37, train loss: 1.51946, val loss: 1.59450\n",
      "Main effects tuning epoch: 38, train loss: 1.51890, val loss: 1.60675\n",
      "Main effects tuning epoch: 39, train loss: 1.51255, val loss: 1.58260\n",
      "Main effects tuning epoch: 40, train loss: 1.51404, val loss: 1.58905\n",
      "Main effects tuning epoch: 41, train loss: 1.51262, val loss: 1.60083\n",
      "Main effects tuning epoch: 42, train loss: 1.51062, val loss: 1.58500\n",
      "Main effects tuning epoch: 43, train loss: 1.50937, val loss: 1.58895\n",
      "Main effects tuning epoch: 44, train loss: 1.51122, val loss: 1.58985\n",
      "Main effects tuning epoch: 45, train loss: 1.51222, val loss: 1.58374\n",
      "Main effects tuning epoch: 46, train loss: 1.51011, val loss: 1.58518\n",
      "Main effects tuning epoch: 47, train loss: 1.50931, val loss: 1.58288\n",
      "Main effects tuning epoch: 48, train loss: 1.51077, val loss: 1.59571\n",
      "Main effects tuning epoch: 49, train loss: 1.51200, val loss: 1.58837\n",
      "Main effects tuning epoch: 50, train loss: 1.50768, val loss: 1.58089\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.35734, val loss: 1.41400\n",
      "Interaction training epoch: 2, train loss: 1.12146, val loss: 1.16408\n",
      "Interaction training epoch: 3, train loss: 1.17362, val loss: 1.22888\n",
      "Interaction training epoch: 4, train loss: 1.05229, val loss: 1.05964\n",
      "Interaction training epoch: 5, train loss: 1.05475, val loss: 1.07088\n",
      "Interaction training epoch: 6, train loss: 1.05345, val loss: 1.05921\n",
      "Interaction training epoch: 7, train loss: 1.04506, val loss: 1.05739\n",
      "Interaction training epoch: 8, train loss: 1.00731, val loss: 1.01201\n",
      "Interaction training epoch: 9, train loss: 1.00095, val loss: 0.99310\n",
      "Interaction training epoch: 10, train loss: 0.99423, val loss: 1.00937\n",
      "Interaction training epoch: 11, train loss: 0.99451, val loss: 1.00156\n",
      "Interaction training epoch: 12, train loss: 0.98322, val loss: 0.99148\n",
      "Interaction training epoch: 13, train loss: 0.96633, val loss: 0.97128\n",
      "Interaction training epoch: 14, train loss: 0.95572, val loss: 0.95859\n",
      "Interaction training epoch: 15, train loss: 0.95832, val loss: 0.96506\n",
      "Interaction training epoch: 16, train loss: 0.97791, val loss: 0.98225\n",
      "Interaction training epoch: 17, train loss: 0.95337, val loss: 0.95659\n",
      "Interaction training epoch: 18, train loss: 0.96425, val loss: 0.97979\n",
      "Interaction training epoch: 19, train loss: 0.92693, val loss: 0.91500\n",
      "Interaction training epoch: 20, train loss: 0.91472, val loss: 0.89845\n",
      "Interaction training epoch: 21, train loss: 0.91224, val loss: 0.91026\n",
      "Interaction training epoch: 22, train loss: 0.90202, val loss: 0.89802\n",
      "Interaction training epoch: 23, train loss: 0.88764, val loss: 0.88436\n",
      "Interaction training epoch: 24, train loss: 0.88107, val loss: 0.88314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 25, train loss: 0.88298, val loss: 0.88122\n",
      "Interaction training epoch: 26, train loss: 0.88110, val loss: 0.86951\n",
      "Interaction training epoch: 27, train loss: 0.90524, val loss: 0.91036\n",
      "Interaction training epoch: 28, train loss: 0.86282, val loss: 0.86086\n",
      "Interaction training epoch: 29, train loss: 0.86108, val loss: 0.84775\n",
      "Interaction training epoch: 30, train loss: 0.87296, val loss: 0.86722\n",
      "Interaction training epoch: 31, train loss: 0.86033, val loss: 0.85419\n",
      "Interaction training epoch: 32, train loss: 0.88130, val loss: 0.87686\n",
      "Interaction training epoch: 33, train loss: 0.85284, val loss: 0.84320\n",
      "Interaction training epoch: 34, train loss: 0.86346, val loss: 0.86202\n",
      "Interaction training epoch: 35, train loss: 0.85911, val loss: 0.85974\n",
      "Interaction training epoch: 36, train loss: 0.84567, val loss: 0.83681\n",
      "Interaction training epoch: 37, train loss: 0.84399, val loss: 0.83352\n",
      "Interaction training epoch: 38, train loss: 0.86158, val loss: 0.85996\n",
      "Interaction training epoch: 39, train loss: 0.84253, val loss: 0.83827\n",
      "Interaction training epoch: 40, train loss: 0.84776, val loss: 0.84761\n",
      "Interaction training epoch: 41, train loss: 0.84114, val loss: 0.82967\n",
      "Interaction training epoch: 42, train loss: 0.85133, val loss: 0.85663\n",
      "Interaction training epoch: 43, train loss: 0.84678, val loss: 0.84520\n",
      "Interaction training epoch: 44, train loss: 0.84428, val loss: 0.83953\n",
      "Interaction training epoch: 45, train loss: 0.84896, val loss: 0.84527\n",
      "Interaction training epoch: 46, train loss: 0.84414, val loss: 0.83898\n",
      "Interaction training epoch: 47, train loss: 0.85239, val loss: 0.85421\n",
      "Interaction training epoch: 48, train loss: 0.84084, val loss: 0.83443\n",
      "Interaction training epoch: 49, train loss: 0.85461, val loss: 0.85066\n",
      "Interaction training epoch: 50, train loss: 0.85550, val loss: 0.85556\n",
      "Interaction training epoch: 51, train loss: 0.87948, val loss: 0.87895\n",
      "Interaction training epoch: 52, train loss: 0.83903, val loss: 0.83386\n",
      "Interaction training epoch: 53, train loss: 0.85089, val loss: 0.83745\n",
      "Interaction training epoch: 54, train loss: 0.85133, val loss: 0.85036\n",
      "Interaction training epoch: 55, train loss: 0.85976, val loss: 0.84200\n",
      "Interaction training epoch: 56, train loss: 0.86136, val loss: 0.85514\n",
      "Interaction training epoch: 57, train loss: 0.83517, val loss: 0.83210\n",
      "Interaction training epoch: 58, train loss: 0.83232, val loss: 0.83059\n",
      "Interaction training epoch: 59, train loss: 0.83648, val loss: 0.84215\n",
      "Interaction training epoch: 60, train loss: 0.82719, val loss: 0.82008\n",
      "Interaction training epoch: 61, train loss: 0.83365, val loss: 0.82833\n",
      "Interaction training epoch: 62, train loss: 0.82422, val loss: 0.81731\n",
      "Interaction training epoch: 63, train loss: 0.83261, val loss: 0.82801\n",
      "Interaction training epoch: 64, train loss: 0.83212, val loss: 0.82798\n",
      "Interaction training epoch: 65, train loss: 0.84198, val loss: 0.83779\n",
      "Interaction training epoch: 66, train loss: 0.82500, val loss: 0.82702\n",
      "Interaction training epoch: 67, train loss: 0.84200, val loss: 0.83886\n",
      "Interaction training epoch: 68, train loss: 0.82619, val loss: 0.81824\n",
      "Interaction training epoch: 69, train loss: 0.83933, val loss: 0.83456\n",
      "Interaction training epoch: 70, train loss: 0.83990, val loss: 0.83664\n",
      "Interaction training epoch: 71, train loss: 0.83929, val loss: 0.83813\n",
      "Interaction training epoch: 72, train loss: 0.82749, val loss: 0.81704\n",
      "Interaction training epoch: 73, train loss: 0.83009, val loss: 0.83150\n",
      "Interaction training epoch: 74, train loss: 0.83621, val loss: 0.83606\n",
      "Interaction training epoch: 75, train loss: 0.82799, val loss: 0.82674\n",
      "Interaction training epoch: 76, train loss: 0.82683, val loss: 0.81872\n",
      "Interaction training epoch: 77, train loss: 0.83387, val loss: 0.83543\n",
      "Interaction training epoch: 78, train loss: 0.82576, val loss: 0.82091\n",
      "Interaction training epoch: 79, train loss: 0.82433, val loss: 0.82229\n",
      "Interaction training epoch: 80, train loss: 0.82323, val loss: 0.81854\n",
      "Interaction training epoch: 81, train loss: 0.82600, val loss: 0.82911\n",
      "Interaction training epoch: 82, train loss: 0.83587, val loss: 0.82935\n",
      "Interaction training epoch: 83, train loss: 0.82520, val loss: 0.82927\n",
      "Interaction training epoch: 84, train loss: 0.82270, val loss: 0.81737\n",
      "Interaction training epoch: 85, train loss: 0.82120, val loss: 0.81986\n",
      "Interaction training epoch: 86, train loss: 0.82714, val loss: 0.82776\n",
      "Interaction training epoch: 87, train loss: 0.82529, val loss: 0.82049\n",
      "Interaction training epoch: 88, train loss: 0.82646, val loss: 0.83177\n",
      "Interaction training epoch: 89, train loss: 0.82704, val loss: 0.82427\n",
      "Interaction training epoch: 90, train loss: 0.83458, val loss: 0.83116\n",
      "Interaction training epoch: 91, train loss: 0.82096, val loss: 0.81450\n",
      "Interaction training epoch: 92, train loss: 0.81861, val loss: 0.82111\n",
      "Interaction training epoch: 93, train loss: 0.81838, val loss: 0.81265\n",
      "Interaction training epoch: 94, train loss: 0.82061, val loss: 0.82416\n",
      "Interaction training epoch: 95, train loss: 0.82497, val loss: 0.82606\n",
      "Interaction training epoch: 96, train loss: 0.82057, val loss: 0.81579\n",
      "Interaction training epoch: 97, train loss: 0.82356, val loss: 0.81814\n",
      "Interaction training epoch: 98, train loss: 0.81833, val loss: 0.81906\n",
      "Interaction training epoch: 99, train loss: 0.82391, val loss: 0.81752\n",
      "Interaction training epoch: 100, train loss: 0.82651, val loss: 0.82993\n",
      "Interaction training epoch: 101, train loss: 0.81869, val loss: 0.81073\n",
      "Interaction training epoch: 102, train loss: 0.82402, val loss: 0.82946\n",
      "Interaction training epoch: 103, train loss: 0.82587, val loss: 0.82786\n",
      "Interaction training epoch: 104, train loss: 0.82198, val loss: 0.81857\n",
      "Interaction training epoch: 105, train loss: 0.82269, val loss: 0.82102\n",
      "Interaction training epoch: 106, train loss: 0.82258, val loss: 0.82261\n",
      "Interaction training epoch: 107, train loss: 0.81711, val loss: 0.81261\n",
      "Interaction training epoch: 108, train loss: 0.82600, val loss: 0.82548\n",
      "Interaction training epoch: 109, train loss: 0.82556, val loss: 0.81882\n",
      "Interaction training epoch: 110, train loss: 0.82491, val loss: 0.83359\n",
      "Interaction training epoch: 111, train loss: 0.82143, val loss: 0.81607\n",
      "Interaction training epoch: 112, train loss: 0.82062, val loss: 0.82914\n",
      "Interaction training epoch: 113, train loss: 0.82007, val loss: 0.82666\n",
      "Interaction training epoch: 114, train loss: 0.82355, val loss: 0.81472\n",
      "Interaction training epoch: 115, train loss: 0.82651, val loss: 0.82478\n",
      "Interaction training epoch: 116, train loss: 0.81983, val loss: 0.82021\n",
      "Interaction training epoch: 117, train loss: 0.81303, val loss: 0.81161\n",
      "Interaction training epoch: 118, train loss: 0.81787, val loss: 0.82011\n",
      "Interaction training epoch: 119, train loss: 0.82477, val loss: 0.82443\n",
      "Interaction training epoch: 120, train loss: 0.81147, val loss: 0.81384\n",
      "Interaction training epoch: 121, train loss: 0.81837, val loss: 0.82301\n",
      "Interaction training epoch: 122, train loss: 0.81418, val loss: 0.81481\n",
      "Interaction training epoch: 123, train loss: 0.81166, val loss: 0.81267\n",
      "Interaction training epoch: 124, train loss: 0.81759, val loss: 0.82088\n",
      "Interaction training epoch: 125, train loss: 0.81221, val loss: 0.81109\n",
      "Interaction training epoch: 126, train loss: 0.82507, val loss: 0.82635\n",
      "Interaction training epoch: 127, train loss: 0.81597, val loss: 0.82193\n",
      "Interaction training epoch: 128, train loss: 0.81864, val loss: 0.81683\n",
      "Interaction training epoch: 129, train loss: 0.81893, val loss: 0.81176\n",
      "Interaction training epoch: 130, train loss: 0.81697, val loss: 0.82383\n",
      "Interaction training epoch: 131, train loss: 0.82130, val loss: 0.81769\n",
      "Interaction training epoch: 132, train loss: 0.82042, val loss: 0.81853\n",
      "Interaction training epoch: 133, train loss: 0.81547, val loss: 0.81800\n",
      "Interaction training epoch: 134, train loss: 0.81077, val loss: 0.81510\n",
      "Interaction training epoch: 135, train loss: 0.81555, val loss: 0.81821\n",
      "Interaction training epoch: 136, train loss: 0.82170, val loss: 0.82040\n",
      "Interaction training epoch: 137, train loss: 0.81229, val loss: 0.81634\n",
      "Interaction training epoch: 138, train loss: 0.80926, val loss: 0.81066\n",
      "Interaction training epoch: 139, train loss: 0.81460, val loss: 0.81658\n",
      "Interaction training epoch: 140, train loss: 0.81447, val loss: 0.81516\n",
      "Interaction training epoch: 141, train loss: 0.81569, val loss: 0.82223\n",
      "Interaction training epoch: 142, train loss: 0.81636, val loss: 0.81501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 143, train loss: 0.82134, val loss: 0.82350\n",
      "Interaction training epoch: 144, train loss: 0.81314, val loss: 0.81914\n",
      "Interaction training epoch: 145, train loss: 0.81073, val loss: 0.80741\n",
      "Interaction training epoch: 146, train loss: 0.81273, val loss: 0.81480\n",
      "Interaction training epoch: 147, train loss: 0.81333, val loss: 0.81621\n",
      "Interaction training epoch: 148, train loss: 0.81614, val loss: 0.81114\n",
      "Interaction training epoch: 149, train loss: 0.81234, val loss: 0.81355\n",
      "Interaction training epoch: 150, train loss: 0.81128, val loss: 0.82195\n",
      "Interaction training epoch: 151, train loss: 0.81147, val loss: 0.81008\n",
      "Interaction training epoch: 152, train loss: 0.81225, val loss: 0.80995\n",
      "Interaction training epoch: 153, train loss: 0.82020, val loss: 0.82229\n",
      "Interaction training epoch: 154, train loss: 0.81380, val loss: 0.81341\n",
      "Interaction training epoch: 155, train loss: 0.81242, val loss: 0.81672\n",
      "Interaction training epoch: 156, train loss: 0.81871, val loss: 0.81361\n",
      "Interaction training epoch: 157, train loss: 0.81607, val loss: 0.81856\n",
      "Interaction training epoch: 158, train loss: 0.81207, val loss: 0.81070\n",
      "Interaction training epoch: 159, train loss: 0.80967, val loss: 0.80906\n",
      "Interaction training epoch: 160, train loss: 0.81195, val loss: 0.81898\n",
      "Interaction training epoch: 161, train loss: 0.81790, val loss: 0.81478\n",
      "Interaction training epoch: 162, train loss: 0.81646, val loss: 0.81976\n",
      "Interaction training epoch: 163, train loss: 0.80938, val loss: 0.81554\n",
      "Interaction training epoch: 164, train loss: 0.81535, val loss: 0.81779\n",
      "Interaction training epoch: 165, train loss: 0.80910, val loss: 0.80916\n",
      "Interaction training epoch: 166, train loss: 0.81218, val loss: 0.81727\n",
      "Interaction training epoch: 167, train loss: 0.81601, val loss: 0.82010\n",
      "Interaction training epoch: 168, train loss: 0.81210, val loss: 0.80927\n",
      "Interaction training epoch: 169, train loss: 0.80848, val loss: 0.81184\n",
      "Interaction training epoch: 170, train loss: 0.81393, val loss: 0.81355\n",
      "Interaction training epoch: 171, train loss: 0.81494, val loss: 0.81747\n",
      "Interaction training epoch: 172, train loss: 0.80964, val loss: 0.81010\n",
      "Interaction training epoch: 173, train loss: 0.81274, val loss: 0.81131\n",
      "Interaction training epoch: 174, train loss: 0.81239, val loss: 0.81433\n",
      "Interaction training epoch: 175, train loss: 0.81496, val loss: 0.82502\n",
      "Interaction training epoch: 176, train loss: 0.81328, val loss: 0.81120\n",
      "Interaction training epoch: 177, train loss: 0.81117, val loss: 0.81481\n",
      "Interaction training epoch: 178, train loss: 0.80834, val loss: 0.81262\n",
      "Interaction training epoch: 179, train loss: 0.81583, val loss: 0.81297\n",
      "Interaction training epoch: 180, train loss: 0.80844, val loss: 0.81018\n",
      "Interaction training epoch: 181, train loss: 0.80900, val loss: 0.81699\n",
      "Interaction training epoch: 182, train loss: 0.81027, val loss: 0.80549\n",
      "Interaction training epoch: 183, train loss: 0.80652, val loss: 0.81540\n",
      "Interaction training epoch: 184, train loss: 0.80735, val loss: 0.81091\n",
      "Interaction training epoch: 185, train loss: 0.80845, val loss: 0.81008\n",
      "Interaction training epoch: 186, train loss: 0.81206, val loss: 0.81180\n",
      "Interaction training epoch: 187, train loss: 0.81222, val loss: 0.81878\n",
      "Interaction training epoch: 188, train loss: 0.81638, val loss: 0.82215\n",
      "Interaction training epoch: 189, train loss: 0.82079, val loss: 0.81842\n",
      "Interaction training epoch: 190, train loss: 0.80805, val loss: 0.81136\n",
      "Interaction training epoch: 191, train loss: 0.81160, val loss: 0.81919\n",
      "Interaction training epoch: 192, train loss: 0.80952, val loss: 0.81035\n",
      "Interaction training epoch: 193, train loss: 0.80542, val loss: 0.80554\n",
      "Interaction training epoch: 194, train loss: 0.81476, val loss: 0.81852\n",
      "Interaction training epoch: 195, train loss: 0.80966, val loss: 0.81906\n",
      "Interaction training epoch: 196, train loss: 0.80682, val loss: 0.80521\n",
      "Interaction training epoch: 197, train loss: 0.80812, val loss: 0.81546\n",
      "Interaction training epoch: 198, train loss: 0.81025, val loss: 0.80924\n",
      "Interaction training epoch: 199, train loss: 0.80546, val loss: 0.81242\n",
      "Interaction training epoch: 200, train loss: 0.81064, val loss: 0.81554\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########1 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.81459, val loss: 0.82754\n",
      "Interaction tuning epoch: 2, train loss: 0.81694, val loss: 0.82504\n",
      "Interaction tuning epoch: 3, train loss: 0.81103, val loss: 0.81059\n",
      "Interaction tuning epoch: 4, train loss: 0.81112, val loss: 0.81450\n",
      "Interaction tuning epoch: 5, train loss: 0.81764, val loss: 0.82652\n",
      "Interaction tuning epoch: 6, train loss: 0.81412, val loss: 0.81552\n",
      "Interaction tuning epoch: 7, train loss: 0.81007, val loss: 0.81958\n",
      "Interaction tuning epoch: 8, train loss: 0.81388, val loss: 0.81520\n",
      "Interaction tuning epoch: 9, train loss: 0.80793, val loss: 0.81612\n",
      "Interaction tuning epoch: 10, train loss: 0.81653, val loss: 0.81628\n",
      "Interaction tuning epoch: 11, train loss: 0.81142, val loss: 0.81758\n",
      "Interaction tuning epoch: 12, train loss: 0.80759, val loss: 0.81183\n",
      "Interaction tuning epoch: 13, train loss: 0.80822, val loss: 0.81334\n",
      "Interaction tuning epoch: 14, train loss: 0.80968, val loss: 0.81462\n",
      "Interaction tuning epoch: 15, train loss: 0.81348, val loss: 0.81512\n",
      "Interaction tuning epoch: 16, train loss: 0.81050, val loss: 0.81199\n",
      "Interaction tuning epoch: 17, train loss: 0.81448, val loss: 0.82002\n",
      "Interaction tuning epoch: 18, train loss: 0.81267, val loss: 0.81656\n",
      "Interaction tuning epoch: 19, train loss: 0.80709, val loss: 0.81172\n",
      "Interaction tuning epoch: 20, train loss: 0.80915, val loss: 0.81463\n",
      "Interaction tuning epoch: 21, train loss: 0.80884, val loss: 0.80770\n",
      "Interaction tuning epoch: 22, train loss: 0.80556, val loss: 0.80928\n",
      "Interaction tuning epoch: 23, train loss: 0.81786, val loss: 0.82338\n",
      "Interaction tuning epoch: 24, train loss: 0.80592, val loss: 0.81088\n",
      "Interaction tuning epoch: 25, train loss: 0.80823, val loss: 0.81511\n",
      "Interaction tuning epoch: 26, train loss: 0.80907, val loss: 0.81654\n",
      "Interaction tuning epoch: 27, train loss: 0.80887, val loss: 0.80885\n",
      "Interaction tuning epoch: 28, train loss: 0.80989, val loss: 0.81550\n",
      "Interaction tuning epoch: 29, train loss: 0.80894, val loss: 0.81350\n",
      "Interaction tuning epoch: 30, train loss: 0.81046, val loss: 0.81695\n",
      "Interaction tuning epoch: 31, train loss: 0.80807, val loss: 0.80937\n",
      "Interaction tuning epoch: 32, train loss: 0.80822, val loss: 0.81894\n",
      "Interaction tuning epoch: 33, train loss: 0.81229, val loss: 0.81859\n",
      "Interaction tuning epoch: 34, train loss: 0.80871, val loss: 0.81090\n",
      "Interaction tuning epoch: 35, train loss: 0.81224, val loss: 0.81983\n",
      "Interaction tuning epoch: 36, train loss: 0.80610, val loss: 0.80811\n",
      "Interaction tuning epoch: 37, train loss: 0.80875, val loss: 0.81136\n",
      "Interaction tuning epoch: 38, train loss: 0.80910, val loss: 0.81573\n",
      "Interaction tuning epoch: 39, train loss: 0.80895, val loss: 0.81354\n",
      "Interaction tuning epoch: 40, train loss: 0.80653, val loss: 0.81197\n",
      "Interaction tuning epoch: 41, train loss: 0.80413, val loss: 0.81096\n",
      "Interaction tuning epoch: 42, train loss: 0.80706, val loss: 0.81037\n",
      "Interaction tuning epoch: 43, train loss: 0.80977, val loss: 0.81361\n",
      "Interaction tuning epoch: 44, train loss: 0.80373, val loss: 0.81036\n",
      "Interaction tuning epoch: 45, train loss: 0.81053, val loss: 0.81174\n",
      "Interaction tuning epoch: 46, train loss: 0.81147, val loss: 0.82107\n",
      "Interaction tuning epoch: 47, train loss: 0.81005, val loss: 0.81286\n",
      "Interaction tuning epoch: 48, train loss: 0.81197, val loss: 0.81525\n",
      "Interaction tuning epoch: 49, train loss: 0.81053, val loss: 0.81920\n",
      "Interaction tuning epoch: 50, train loss: 0.80858, val loss: 0.81306\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 42.15354371070862\n",
      "After the gam stage, training error is 0.80858 , validation error is 0.81306\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 20.584015\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.668862 validation MAE=0.770521,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.622646 validation MAE=0.751582,rank=5\n",
      "[SoftImpute] Iter 3: observed MAE=0.582886 validation MAE=0.734112,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.549562 validation MAE=0.718536,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.521175 validation MAE=0.703689,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.497297 validation MAE=0.690544,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.477148 validation MAE=0.678963,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.459270 validation MAE=0.667826,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.442365 validation MAE=0.657461,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.428160 validation MAE=0.648098,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.414134 validation MAE=0.639678,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.401982 validation MAE=0.631459,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.390238 validation MAE=0.624338,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.379512 validation MAE=0.617722,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.369950 validation MAE=0.611091,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.361104 validation MAE=0.605620,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.354331 validation MAE=0.601451,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.350562 validation MAE=0.598206,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.350789 validation MAE=0.596789,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.347782 validation MAE=0.594387,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.346330 validation MAE=0.593197,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.344494 validation MAE=0.591493,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.342666 validation MAE=0.590062,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.340958 validation MAE=0.588262,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.339575 validation MAE=0.586781,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.338763 validation MAE=0.585012,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.336757 validation MAE=0.583214,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.333296 validation MAE=0.581343,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.330992 validation MAE=0.579390,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.329020 validation MAE=0.578809,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.326798 validation MAE=0.576947,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.323443 validation MAE=0.575236,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.320873 validation MAE=0.573948,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.319177 validation MAE=0.572787,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.316757 validation MAE=0.570747,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.314569 validation MAE=0.570036,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.312241 validation MAE=0.568083,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.310025 validation MAE=0.567193,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.308499 validation MAE=0.566682,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.306611 validation MAE=0.565250,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.304608 validation MAE=0.564132,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.303112 validation MAE=0.563456,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.300809 validation MAE=0.562542,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.299758 validation MAE=0.561705,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.298633 validation MAE=0.560985,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.296459 validation MAE=0.560376,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.295095 validation MAE=0.558913,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.294365 validation MAE=0.558867,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.291901 validation MAE=0.557963,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.290800 validation MAE=0.556927,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.290190 validation MAE=0.556575,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.288845 validation MAE=0.555571,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.287641 validation MAE=0.555277,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.286971 validation MAE=0.554995,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.285113 validation MAE=0.553461,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.283794 validation MAE=0.552870,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.283349 validation MAE=0.552911,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.281556 validation MAE=0.551465,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.281126 validation MAE=0.550922,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.280364 validation MAE=0.550322,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.279178 validation MAE=0.550115,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.277695 validation MAE=0.549289,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.277062 validation MAE=0.548671,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.276592 validation MAE=0.548076,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.276051 validation MAE=0.548357,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.274444 validation MAE=0.547636,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.273680 validation MAE=0.546699,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.273299 validation MAE=0.546374,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.272803 validation MAE=0.546137,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.271979 validation MAE=0.545455,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.270874 validation MAE=0.544879,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.270078 validation MAE=0.544870,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.269632 validation MAE=0.544205,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.268768 validation MAE=0.543835,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.268953 validation MAE=0.543274,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.267604 validation MAE=0.543025,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.266843 validation MAE=0.542315,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.265878 validation MAE=0.542092,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.265615 validation MAE=0.542437,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.264879 validation MAE=0.541180,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.265033 validation MAE=0.540570,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.263961 validation MAE=0.540648,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.263244 validation MAE=0.540199,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.262835 validation MAE=0.540334,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.262169 validation MAE=0.539755,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.261465 validation MAE=0.538936,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.260626 validation MAE=0.539127,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.261248 validation MAE=0.538865,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.260389 validation MAE=0.539054,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.260057 validation MAE=0.538362,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.259694 validation MAE=0.537934,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.258720 validation MAE=0.537553,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.258498 validation MAE=0.537017,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.257503 validation MAE=0.536772,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.256744 validation MAE=0.536169,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.256504 validation MAE=0.536578,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.256363 validation MAE=0.536091,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.256412 validation MAE=0.535509,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.255653 validation MAE=0.535265,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.255126 validation MAE=0.534629,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.253967 validation MAE=0.534298,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.254260 validation MAE=0.534235,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.253975 validation MAE=0.534265,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.253493 validation MAE=0.533687,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.253001 validation MAE=0.532570,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.252702 validation MAE=0.532559,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.253272 validation MAE=0.533107,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.252905 validation MAE=0.532391,rank=5\n",
      "[SoftImpute] Iter 109: observed MAE=0.252096 validation MAE=0.532262,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.251928 validation MAE=0.531600,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 111: observed MAE=0.251245 validation MAE=0.531174,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.250992 validation MAE=0.530807,rank=5\n",
      "[SoftImpute] Iter 113: observed MAE=0.250578 validation MAE=0.530182,rank=5\n",
      "[SoftImpute] Iter 114: observed MAE=0.249622 validation MAE=0.530217,rank=5\n",
      "[SoftImpute] Iter 115: observed MAE=0.249534 validation MAE=0.530131,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.248923 validation MAE=0.529793,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.248792 validation MAE=0.529116,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.248235 validation MAE=0.529319,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.247686 validation MAE=0.528641,rank=5\n",
      "[SoftImpute] Iter 120: observed MAE=0.247631 validation MAE=0.528557,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.247012 validation MAE=0.527747,rank=5\n",
      "[SoftImpute] Iter 122: observed MAE=0.246144 validation MAE=0.527554,rank=5\n",
      "[SoftImpute] Iter 123: observed MAE=0.246995 validation MAE=0.527335,rank=5\n",
      "[SoftImpute] Iter 124: observed MAE=0.247070 validation MAE=0.526804,rank=5\n",
      "[SoftImpute] Iter 125: observed MAE=0.246245 validation MAE=0.526296,rank=5\n",
      "[SoftImpute] Iter 126: observed MAE=0.246051 validation MAE=0.526075,rank=5\n",
      "[SoftImpute] Iter 127: observed MAE=0.245185 validation MAE=0.526218,rank=5\n",
      "[SoftImpute] Iter 128: observed MAE=0.244323 validation MAE=0.526501,rank=5\n",
      "[SoftImpute] Iter 129: observed MAE=0.244273 validation MAE=0.525169,rank=5\n",
      "[SoftImpute] Iter 130: observed MAE=0.243647 validation MAE=0.524825,rank=5\n",
      "[SoftImpute] Iter 131: observed MAE=0.243791 validation MAE=0.524881,rank=5\n",
      "[SoftImpute] Iter 132: observed MAE=0.243098 validation MAE=0.524411,rank=5\n",
      "[SoftImpute] Iter 133: observed MAE=0.242434 validation MAE=0.524134,rank=5\n",
      "[SoftImpute] Iter 134: observed MAE=0.242554 validation MAE=0.524801,rank=5\n",
      "[SoftImpute] Iter 135: observed MAE=0.242909 validation MAE=0.524519,rank=5\n",
      "[SoftImpute] Iter 136: observed MAE=0.242210 validation MAE=0.523981,rank=5\n",
      "[SoftImpute] Iter 137: observed MAE=0.242088 validation MAE=0.524154,rank=5\n",
      "[SoftImpute] Iter 138: observed MAE=0.241605 validation MAE=0.523187,rank=5\n",
      "[SoftImpute] Iter 139: observed MAE=0.240495 validation MAE=0.522788,rank=5\n",
      "[SoftImpute] Iter 140: observed MAE=0.240330 validation MAE=0.522716,rank=5\n",
      "[SoftImpute] Iter 141: observed MAE=0.240326 validation MAE=0.521850,rank=5\n",
      "[SoftImpute] Iter 142: observed MAE=0.239476 validation MAE=0.522353,rank=5\n",
      "[SoftImpute] Iter 143: observed MAE=0.239118 validation MAE=0.522157,rank=5\n",
      "[SoftImpute] Iter 144: observed MAE=0.238545 validation MAE=0.522316,rank=5\n",
      "[SoftImpute] Iter 145: observed MAE=0.238462 validation MAE=0.521212,rank=5\n",
      "[SoftImpute] Iter 146: observed MAE=0.238104 validation MAE=0.521738,rank=5\n",
      "[SoftImpute] Iter 147: observed MAE=0.237981 validation MAE=0.520376,rank=5\n",
      "[SoftImpute] Iter 148: observed MAE=0.237769 validation MAE=0.520771,rank=5\n",
      "[SoftImpute] Iter 149: observed MAE=0.237322 validation MAE=0.520740,rank=5\n",
      "[SoftImpute] Iter 150: observed MAE=0.237527 validation MAE=0.520501,rank=5\n",
      "[SoftImpute] Iter 151: observed MAE=0.237066 validation MAE=0.519579,rank=5\n",
      "[SoftImpute] Iter 152: observed MAE=0.236488 validation MAE=0.519562,rank=5\n",
      "[SoftImpute] Iter 153: observed MAE=0.236502 validation MAE=0.519073,rank=5\n",
      "[SoftImpute] Iter 154: observed MAE=0.236213 validation MAE=0.518879,rank=5\n",
      "[SoftImpute] Iter 155: observed MAE=0.236022 validation MAE=0.519189,rank=5\n",
      "[SoftImpute] Iter 156: observed MAE=0.236337 validation MAE=0.519552,rank=5\n",
      "[SoftImpute] Iter 157: observed MAE=0.234316 validation MAE=0.518825,rank=5\n",
      "[SoftImpute] Iter 158: observed MAE=0.234089 validation MAE=0.518637,rank=5\n",
      "[SoftImpute] Iter 159: observed MAE=0.234534 validation MAE=0.517860,rank=5\n",
      "[SoftImpute] Iter 160: observed MAE=0.234200 validation MAE=0.517552,rank=5\n",
      "[SoftImpute] Iter 161: observed MAE=0.232504 validation MAE=0.517210,rank=5\n",
      "[SoftImpute] Iter 162: observed MAE=0.232102 validation MAE=0.517322,rank=5\n",
      "[SoftImpute] Iter 163: observed MAE=0.232455 validation MAE=0.516815,rank=5\n",
      "[SoftImpute] Iter 164: observed MAE=0.232755 validation MAE=0.517819,rank=5\n",
      "[SoftImpute] Iter 165: observed MAE=0.232859 validation MAE=0.516830,rank=5\n",
      "[SoftImpute] Iter 166: observed MAE=0.232793 validation MAE=0.516676,rank=5\n",
      "[SoftImpute] Iter 167: observed MAE=0.232351 validation MAE=0.516161,rank=5\n",
      "[SoftImpute] Iter 168: observed MAE=0.231983 validation MAE=0.516112,rank=5\n",
      "[SoftImpute] Iter 169: observed MAE=0.231612 validation MAE=0.515485,rank=5\n",
      "[SoftImpute] Iter 170: observed MAE=0.232237 validation MAE=0.516300,rank=5\n",
      "[SoftImpute] Iter 171: observed MAE=0.230885 validation MAE=0.515392,rank=5\n",
      "[SoftImpute] Iter 172: observed MAE=0.231124 validation MAE=0.515274,rank=5\n",
      "[SoftImpute] Iter 173: observed MAE=0.230438 validation MAE=0.514831,rank=5\n",
      "[SoftImpute] Iter 174: observed MAE=0.230120 validation MAE=0.515233,rank=5\n",
      "[SoftImpute] Iter 175: observed MAE=0.229558 validation MAE=0.514737,rank=5\n",
      "[SoftImpute] Iter 176: observed MAE=0.230025 validation MAE=0.514671,rank=5\n",
      "[SoftImpute] Iter 177: observed MAE=0.228853 validation MAE=0.514031,rank=5\n",
      "[SoftImpute] Iter 178: observed MAE=0.229724 validation MAE=0.513938,rank=5\n",
      "[SoftImpute] Iter 179: observed MAE=0.229865 validation MAE=0.513668,rank=5\n",
      "[SoftImpute] Iter 180: observed MAE=0.229380 validation MAE=0.514282,rank=5\n",
      "[SoftImpute] Iter 181: observed MAE=0.229028 validation MAE=0.513964,rank=5\n",
      "[SoftImpute] Iter 182: observed MAE=0.228905 validation MAE=0.513963,rank=5\n",
      "[SoftImpute] Iter 183: observed MAE=0.227652 validation MAE=0.513291,rank=5\n",
      "[SoftImpute] Iter 184: observed MAE=0.228631 validation MAE=0.513429,rank=5\n",
      "[SoftImpute] Iter 185: observed MAE=0.228232 validation MAE=0.513070,rank=5\n",
      "[SoftImpute] Iter 186: observed MAE=0.227746 validation MAE=0.513509,rank=5\n",
      "[SoftImpute] Iter 187: observed MAE=0.227955 validation MAE=0.512868,rank=5\n",
      "[SoftImpute] Iter 188: observed MAE=0.227580 validation MAE=0.513226,rank=5\n",
      "[SoftImpute] Iter 189: observed MAE=0.226728 validation MAE=0.512967,rank=5\n",
      "[SoftImpute] Iter 190: observed MAE=0.227255 validation MAE=0.512375,rank=5\n",
      "[SoftImpute] Iter 191: observed MAE=0.226814 validation MAE=0.512435,rank=5\n",
      "[SoftImpute] Iter 192: observed MAE=0.226459 validation MAE=0.513251,rank=5\n",
      "[SoftImpute] Iter 193: observed MAE=0.226400 validation MAE=0.511975,rank=5\n",
      "[SoftImpute] Iter 194: observed MAE=0.226181 validation MAE=0.511918,rank=5\n",
      "[SoftImpute] Iter 195: observed MAE=0.225074 validation MAE=0.511341,rank=5\n",
      "[SoftImpute] Iter 196: observed MAE=0.225835 validation MAE=0.511464,rank=5\n",
      "[SoftImpute] Iter 197: observed MAE=0.225680 validation MAE=0.511675,rank=5\n",
      "[SoftImpute] Iter 198: observed MAE=0.225615 validation MAE=0.511902,rank=5\n",
      "[SoftImpute] Stopped after iteration 198 for lambda=0.411680\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 6.148122310638428\n",
      "After the matrix factor stage, training error is 0.22562, validation error is 0.51190\n"
     ]
    }
   ],
   "source": [
    "result_lvxnn = lvxnn('warm',tr_x, val_x, te_x, tr_y, val_y, te_y,tr_Xi, val_Xi, te_Xi, tr_idx, val_idx, meta_info, model_info, task_type , random_state=0, params=lx_params)\n",
    "result_svd = svd('warm',tr_x, val_x, te_x, tr_y, val_y, te_y,tr_Xi, val_Xi, te_Xi, tr_idx, val_idx, meta_info, model_info, task_type , random_state=0)\n",
    "result_deepfm, result_fm = deepfm_fm('warm',train,test,tr_x, val_x, te_x, tr_y, val_y, te_y,tr_Xi, val_Xi, te_Xi, tr_idx, val_idx, meta_info, model_info, task_type , random_state=0, epochs=300)\n",
    "result_xgb = xgb('warm',tr_x, val_x, te_x, tr_y, val_y, te_y,tr_Xi, val_Xi, te_Xi, tr_idx, val_idx, meta_info, model_info, task_type , random_state=0)\n",
    "\n",
    "result_sim_re = pd.concat([result_lvxnn,result_svd,result_xgb,result_deepfm,result_fm],0)\n",
    "\n",
    "result_sim_re.to_csv('simulation_regression_result.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error,roc_auc_score,mean_absolute_error,log_loss\n",
    "import sys\n",
    "sys.path.append('../benchmark/')\n",
    "from lvxnn_test import lvxnn\n",
    "from xgb_test import xgb\n",
    "from svd_test import svd\n",
    "from deepfm_fm_test import deepfm_fm\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from lvxnn.LVXNN import LV_XNN\n",
    "from lvxnn.DataReader import data_initialize\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "data= pd.read_csv('../simulation/data/sim_binary_0.9.csv')\n",
    "train , test = train_test_split(data,test_size=0.2,random_state=0)\n",
    "task_type = \"Classification\"\n",
    "\n",
    "meta_info = OrderedDict()\n",
    "\n",
    "meta_info['uf_1']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_2']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_3']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_4']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_5']={'type': 'continues','source':'user'}\n",
    "meta_info['if_1']={'type': 'continues','source':'item'}\n",
    "meta_info['if_2']={'type': 'continues','source':'item'}\n",
    "meta_info['if_3']={'type': 'continues','source':'item'}\n",
    "meta_info['if_4']={'type': 'continues','source':'item'}\n",
    "meta_info['if_5']={'type': 'continues','source':'item'}\n",
    "meta_info['user_id']={\"type\":\"id\",'source':'user'}\n",
    "meta_info['item_id']={\"type\":\"id\",'source':'item'}\n",
    "meta_info['target']={\"type\":\"target\",'source':''}\n",
    "\n",
    "lx_params = {\n",
    "        \"main_effect_epochs\":300,\n",
    "        \"interaction_epochs\" : 200 ,\n",
    "        \"tuning_epochs\" : 50 , \n",
    "        \"mf_training_iters\": 100,\n",
    "        \"u_group_num\":30,\n",
    "        \"i_group_num\":50\n",
    "    }\n",
    "\n",
    "tr_x, tr_Xi, tr_y, tr_idx, te_x, te_Xi, te_y, val_x, val_Xi, val_y, val_idx, meta_info, model_info = data_initialize(train,test,meta_info,task_type ,'warm', random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68318, val loss: 0.68192\n",
      "Main effects training epoch: 2, train loss: 0.67724, val loss: 0.67807\n",
      "Main effects training epoch: 3, train loss: 0.67035, val loss: 0.67239\n",
      "Main effects training epoch: 4, train loss: 0.66391, val loss: 0.66654\n",
      "Main effects training epoch: 5, train loss: 0.65041, val loss: 0.65054\n",
      "Main effects training epoch: 6, train loss: 0.62260, val loss: 0.61929\n",
      "Main effects training epoch: 7, train loss: 0.58024, val loss: 0.57432\n",
      "Main effects training epoch: 8, train loss: 0.54720, val loss: 0.53681\n",
      "Main effects training epoch: 9, train loss: 0.53313, val loss: 0.51521\n",
      "Main effects training epoch: 10, train loss: 0.53085, val loss: 0.50585\n",
      "Main effects training epoch: 11, train loss: 0.52583, val loss: 0.50411\n",
      "Main effects training epoch: 12, train loss: 0.52419, val loss: 0.50436\n",
      "Main effects training epoch: 13, train loss: 0.52555, val loss: 0.50572\n",
      "Main effects training epoch: 14, train loss: 0.52540, val loss: 0.50457\n",
      "Main effects training epoch: 15, train loss: 0.52453, val loss: 0.50567\n",
      "Main effects training epoch: 16, train loss: 0.52467, val loss: 0.50372\n",
      "Main effects training epoch: 17, train loss: 0.52324, val loss: 0.50326\n",
      "Main effects training epoch: 18, train loss: 0.52333, val loss: 0.50195\n",
      "Main effects training epoch: 19, train loss: 0.52447, val loss: 0.50358\n",
      "Main effects training epoch: 20, train loss: 0.52310, val loss: 0.50254\n",
      "Main effects training epoch: 21, train loss: 0.52305, val loss: 0.50199\n",
      "Main effects training epoch: 22, train loss: 0.52218, val loss: 0.50175\n",
      "Main effects training epoch: 23, train loss: 0.52230, val loss: 0.50203\n",
      "Main effects training epoch: 24, train loss: 0.52207, val loss: 0.50145\n",
      "Main effects training epoch: 25, train loss: 0.52221, val loss: 0.50219\n",
      "Main effects training epoch: 26, train loss: 0.52249, val loss: 0.50205\n",
      "Main effects training epoch: 27, train loss: 0.52240, val loss: 0.50186\n",
      "Main effects training epoch: 28, train loss: 0.52215, val loss: 0.50118\n",
      "Main effects training epoch: 29, train loss: 0.52167, val loss: 0.50131\n",
      "Main effects training epoch: 30, train loss: 0.52172, val loss: 0.50206\n",
      "Main effects training epoch: 31, train loss: 0.52143, val loss: 0.50126\n",
      "Main effects training epoch: 32, train loss: 0.52173, val loss: 0.50185\n",
      "Main effects training epoch: 33, train loss: 0.52157, val loss: 0.50118\n",
      "Main effects training epoch: 34, train loss: 0.52134, val loss: 0.50190\n",
      "Main effects training epoch: 35, train loss: 0.52160, val loss: 0.50114\n",
      "Main effects training epoch: 36, train loss: 0.52124, val loss: 0.50122\n",
      "Main effects training epoch: 37, train loss: 0.52159, val loss: 0.50182\n",
      "Main effects training epoch: 38, train loss: 0.52178, val loss: 0.50166\n",
      "Main effects training epoch: 39, train loss: 0.52107, val loss: 0.50101\n",
      "Main effects training epoch: 40, train loss: 0.52120, val loss: 0.50207\n",
      "Main effects training epoch: 41, train loss: 0.52137, val loss: 0.50102\n",
      "Main effects training epoch: 42, train loss: 0.52109, val loss: 0.50159\n",
      "Main effects training epoch: 43, train loss: 0.52137, val loss: 0.50169\n",
      "Main effects training epoch: 44, train loss: 0.52140, val loss: 0.50110\n",
      "Main effects training epoch: 45, train loss: 0.52299, val loss: 0.50459\n",
      "Main effects training epoch: 46, train loss: 0.52226, val loss: 0.50272\n",
      "Main effects training epoch: 47, train loss: 0.52101, val loss: 0.50147\n",
      "Main effects training epoch: 48, train loss: 0.52109, val loss: 0.50075\n",
      "Main effects training epoch: 49, train loss: 0.52069, val loss: 0.50215\n",
      "Main effects training epoch: 50, train loss: 0.52045, val loss: 0.50063\n",
      "Main effects training epoch: 51, train loss: 0.52108, val loss: 0.50126\n",
      "Main effects training epoch: 52, train loss: 0.52199, val loss: 0.50324\n",
      "Main effects training epoch: 53, train loss: 0.52131, val loss: 0.50122\n",
      "Main effects training epoch: 54, train loss: 0.52256, val loss: 0.50297\n",
      "Main effects training epoch: 55, train loss: 0.52142, val loss: 0.50258\n",
      "Main effects training epoch: 56, train loss: 0.52045, val loss: 0.50089\n",
      "Main effects training epoch: 57, train loss: 0.52043, val loss: 0.50180\n",
      "Main effects training epoch: 58, train loss: 0.52018, val loss: 0.50121\n",
      "Main effects training epoch: 59, train loss: 0.51996, val loss: 0.50068\n",
      "Main effects training epoch: 60, train loss: 0.52012, val loss: 0.50109\n",
      "Main effects training epoch: 61, train loss: 0.52012, val loss: 0.50024\n",
      "Main effects training epoch: 62, train loss: 0.52057, val loss: 0.50418\n",
      "Main effects training epoch: 63, train loss: 0.52043, val loss: 0.50038\n",
      "Main effects training epoch: 64, train loss: 0.52095, val loss: 0.50364\n",
      "Main effects training epoch: 65, train loss: 0.52038, val loss: 0.50085\n",
      "Main effects training epoch: 66, train loss: 0.52047, val loss: 0.50241\n",
      "Main effects training epoch: 67, train loss: 0.52117, val loss: 0.50295\n",
      "Main effects training epoch: 68, train loss: 0.52073, val loss: 0.50183\n",
      "Main effects training epoch: 69, train loss: 0.52087, val loss: 0.50244\n",
      "Main effects training epoch: 70, train loss: 0.52010, val loss: 0.50147\n",
      "Main effects training epoch: 71, train loss: 0.51994, val loss: 0.50147\n",
      "Main effects training epoch: 72, train loss: 0.51942, val loss: 0.50086\n",
      "Main effects training epoch: 73, train loss: 0.51949, val loss: 0.50028\n",
      "Main effects training epoch: 74, train loss: 0.52005, val loss: 0.50237\n",
      "Main effects training epoch: 75, train loss: 0.51990, val loss: 0.50184\n",
      "Main effects training epoch: 76, train loss: 0.51923, val loss: 0.50236\n",
      "Main effects training epoch: 77, train loss: 0.51910, val loss: 0.50038\n",
      "Main effects training epoch: 78, train loss: 0.51884, val loss: 0.50095\n",
      "Main effects training epoch: 79, train loss: 0.51884, val loss: 0.49983\n",
      "Main effects training epoch: 80, train loss: 0.51866, val loss: 0.50191\n",
      "Main effects training epoch: 81, train loss: 0.51939, val loss: 0.50027\n",
      "Main effects training epoch: 82, train loss: 0.51929, val loss: 0.50319\n",
      "Main effects training epoch: 83, train loss: 0.51868, val loss: 0.50126\n",
      "Main effects training epoch: 84, train loss: 0.51858, val loss: 0.50088\n",
      "Main effects training epoch: 85, train loss: 0.51835, val loss: 0.50068\n",
      "Main effects training epoch: 86, train loss: 0.51834, val loss: 0.50062\n",
      "Main effects training epoch: 87, train loss: 0.51844, val loss: 0.50224\n",
      "Main effects training epoch: 88, train loss: 0.51878, val loss: 0.50098\n",
      "Main effects training epoch: 89, train loss: 0.51903, val loss: 0.50192\n",
      "Main effects training epoch: 90, train loss: 0.51939, val loss: 0.50209\n",
      "Main effects training epoch: 91, train loss: 0.51881, val loss: 0.50201\n",
      "Main effects training epoch: 92, train loss: 0.51849, val loss: 0.50011\n",
      "Main effects training epoch: 93, train loss: 0.51893, val loss: 0.50406\n",
      "Main effects training epoch: 94, train loss: 0.51813, val loss: 0.49975\n",
      "Main effects training epoch: 95, train loss: 0.51804, val loss: 0.50212\n",
      "Main effects training epoch: 96, train loss: 0.51773, val loss: 0.50088\n",
      "Main effects training epoch: 97, train loss: 0.51767, val loss: 0.50090\n",
      "Main effects training epoch: 98, train loss: 0.51759, val loss: 0.50188\n",
      "Main effects training epoch: 99, train loss: 0.51749, val loss: 0.50231\n",
      "Main effects training epoch: 100, train loss: 0.51739, val loss: 0.50044\n",
      "Main effects training epoch: 101, train loss: 0.51733, val loss: 0.50052\n",
      "Main effects training epoch: 102, train loss: 0.51797, val loss: 0.50373\n",
      "Main effects training epoch: 103, train loss: 0.51813, val loss: 0.50085\n",
      "Main effects training epoch: 104, train loss: 0.51776, val loss: 0.50234\n",
      "Main effects training epoch: 105, train loss: 0.51726, val loss: 0.50227\n",
      "Main effects training epoch: 106, train loss: 0.51709, val loss: 0.50129\n",
      "Main effects training epoch: 107, train loss: 0.51781, val loss: 0.50127\n",
      "Main effects training epoch: 108, train loss: 0.51764, val loss: 0.50367\n",
      "Main effects training epoch: 109, train loss: 0.51682, val loss: 0.50096\n",
      "Main effects training epoch: 110, train loss: 0.51693, val loss: 0.50167\n",
      "Main effects training epoch: 111, train loss: 0.51671, val loss: 0.50182\n",
      "Main effects training epoch: 112, train loss: 0.51688, val loss: 0.50161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 113, train loss: 0.51698, val loss: 0.50128\n",
      "Main effects training epoch: 114, train loss: 0.51709, val loss: 0.50115\n",
      "Main effects training epoch: 115, train loss: 0.51748, val loss: 0.50244\n",
      "Main effects training epoch: 116, train loss: 0.51737, val loss: 0.50229\n",
      "Main effects training epoch: 117, train loss: 0.51675, val loss: 0.50288\n",
      "Main effects training epoch: 118, train loss: 0.51650, val loss: 0.50089\n",
      "Main effects training epoch: 119, train loss: 0.51679, val loss: 0.50204\n",
      "Main effects training epoch: 120, train loss: 0.51664, val loss: 0.50211\n",
      "Main effects training epoch: 121, train loss: 0.51670, val loss: 0.50227\n",
      "Main effects training epoch: 122, train loss: 0.51695, val loss: 0.50339\n",
      "Main effects training epoch: 123, train loss: 0.51659, val loss: 0.50129\n",
      "Main effects training epoch: 124, train loss: 0.51633, val loss: 0.50324\n",
      "Main effects training epoch: 125, train loss: 0.51613, val loss: 0.50129\n",
      "Main effects training epoch: 126, train loss: 0.51658, val loss: 0.50223\n",
      "Main effects training epoch: 127, train loss: 0.51636, val loss: 0.50314\n",
      "Main effects training epoch: 128, train loss: 0.51642, val loss: 0.50299\n",
      "Main effects training epoch: 129, train loss: 0.51689, val loss: 0.50193\n",
      "Main effects training epoch: 130, train loss: 0.51717, val loss: 0.50329\n",
      "Main effects training epoch: 131, train loss: 0.51619, val loss: 0.50278\n",
      "Main effects training epoch: 132, train loss: 0.51595, val loss: 0.50213\n",
      "Main effects training epoch: 133, train loss: 0.51591, val loss: 0.50190\n",
      "Main effects training epoch: 134, train loss: 0.51614, val loss: 0.50354\n",
      "Main effects training epoch: 135, train loss: 0.51593, val loss: 0.50167\n",
      "Main effects training epoch: 136, train loss: 0.51683, val loss: 0.50322\n",
      "Main effects training epoch: 137, train loss: 0.51631, val loss: 0.50292\n",
      "Main effects training epoch: 138, train loss: 0.51597, val loss: 0.50196\n",
      "Main effects training epoch: 139, train loss: 0.51594, val loss: 0.50118\n",
      "Main effects training epoch: 140, train loss: 0.51575, val loss: 0.50247\n",
      "Main effects training epoch: 141, train loss: 0.51562, val loss: 0.50226\n",
      "Main effects training epoch: 142, train loss: 0.51583, val loss: 0.50147\n",
      "Main effects training epoch: 143, train loss: 0.51602, val loss: 0.50303\n",
      "Main effects training epoch: 144, train loss: 0.51584, val loss: 0.50264\n",
      "Main effects training epoch: 145, train loss: 0.51575, val loss: 0.50308\n",
      "Main effects training epoch: 146, train loss: 0.51556, val loss: 0.50241\n",
      "Main effects training epoch: 147, train loss: 0.51596, val loss: 0.50159\n",
      "Main effects training epoch: 148, train loss: 0.51555, val loss: 0.50244\n",
      "Main effects training epoch: 149, train loss: 0.51552, val loss: 0.50203\n",
      "Main effects training epoch: 150, train loss: 0.51570, val loss: 0.50205\n",
      "Main effects training epoch: 151, train loss: 0.51556, val loss: 0.50363\n",
      "Main effects training epoch: 152, train loss: 0.51593, val loss: 0.50189\n",
      "Main effects training epoch: 153, train loss: 0.51639, val loss: 0.50286\n",
      "Main effects training epoch: 154, train loss: 0.51624, val loss: 0.50523\n",
      "Main effects training epoch: 155, train loss: 0.51611, val loss: 0.50246\n",
      "Main effects training epoch: 156, train loss: 0.51643, val loss: 0.50459\n",
      "Main effects training epoch: 157, train loss: 0.51592, val loss: 0.50249\n",
      "Main effects training epoch: 158, train loss: 0.51552, val loss: 0.50365\n",
      "Main effects training epoch: 159, train loss: 0.51556, val loss: 0.50177\n",
      "Main effects training epoch: 160, train loss: 0.51540, val loss: 0.50281\n",
      "Main effects training epoch: 161, train loss: 0.51544, val loss: 0.50300\n",
      "Main effects training epoch: 162, train loss: 0.51528, val loss: 0.50196\n",
      "Main effects training epoch: 163, train loss: 0.51547, val loss: 0.50171\n",
      "Main effects training epoch: 164, train loss: 0.51538, val loss: 0.50210\n",
      "Main effects training epoch: 165, train loss: 0.51565, val loss: 0.50469\n",
      "Main effects training epoch: 166, train loss: 0.51521, val loss: 0.50062\n",
      "Main effects training epoch: 167, train loss: 0.51535, val loss: 0.50326\n",
      "Main effects training epoch: 168, train loss: 0.51525, val loss: 0.50222\n",
      "Main effects training epoch: 169, train loss: 0.51514, val loss: 0.50331\n",
      "Main effects training epoch: 170, train loss: 0.51509, val loss: 0.50200\n",
      "Main effects training epoch: 171, train loss: 0.51490, val loss: 0.50324\n",
      "Main effects training epoch: 172, train loss: 0.51508, val loss: 0.50247\n",
      "Main effects training epoch: 173, train loss: 0.51493, val loss: 0.50182\n",
      "Main effects training epoch: 174, train loss: 0.51522, val loss: 0.50212\n",
      "Main effects training epoch: 175, train loss: 0.51508, val loss: 0.50334\n",
      "Main effects training epoch: 176, train loss: 0.51483, val loss: 0.50241\n",
      "Main effects training epoch: 177, train loss: 0.51504, val loss: 0.50039\n",
      "Main effects training epoch: 178, train loss: 0.51532, val loss: 0.50473\n",
      "Main effects training epoch: 179, train loss: 0.51527, val loss: 0.50142\n",
      "Main effects training epoch: 180, train loss: 0.51478, val loss: 0.50288\n",
      "Main effects training epoch: 181, train loss: 0.51468, val loss: 0.50177\n",
      "Main effects training epoch: 182, train loss: 0.51487, val loss: 0.50212\n",
      "Main effects training epoch: 183, train loss: 0.51473, val loss: 0.50130\n",
      "Main effects training epoch: 184, train loss: 0.51443, val loss: 0.50233\n",
      "Main effects training epoch: 185, train loss: 0.51443, val loss: 0.50209\n",
      "Main effects training epoch: 186, train loss: 0.51449, val loss: 0.50105\n",
      "Main effects training epoch: 187, train loss: 0.51500, val loss: 0.50328\n",
      "Main effects training epoch: 188, train loss: 0.51584, val loss: 0.50243\n",
      "Main effects training epoch: 189, train loss: 0.51498, val loss: 0.50282\n",
      "Main effects training epoch: 190, train loss: 0.51476, val loss: 0.50202\n",
      "Main effects training epoch: 191, train loss: 0.51499, val loss: 0.50370\n",
      "Main effects training epoch: 192, train loss: 0.51451, val loss: 0.50061\n",
      "Main effects training epoch: 193, train loss: 0.51427, val loss: 0.50163\n",
      "Main effects training epoch: 194, train loss: 0.51468, val loss: 0.50289\n",
      "Main effects training epoch: 195, train loss: 0.51485, val loss: 0.50152\n",
      "Early stop at epoch 195, with validation loss: 0.50152\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51652, val loss: 0.50334\n",
      "Main effects tuning epoch: 2, train loss: 0.51620, val loss: 0.50298\n",
      "Main effects tuning epoch: 3, train loss: 0.51591, val loss: 0.50243\n",
      "Main effects tuning epoch: 4, train loss: 0.51583, val loss: 0.50178\n",
      "Main effects tuning epoch: 5, train loss: 0.51587, val loss: 0.50310\n",
      "Main effects tuning epoch: 6, train loss: 0.51589, val loss: 0.50141\n",
      "Main effects tuning epoch: 7, train loss: 0.51573, val loss: 0.50312\n",
      "Main effects tuning epoch: 8, train loss: 0.51616, val loss: 0.50270\n",
      "Main effects tuning epoch: 9, train loss: 0.51592, val loss: 0.50204\n",
      "Main effects tuning epoch: 10, train loss: 0.51562, val loss: 0.50183\n",
      "Main effects tuning epoch: 11, train loss: 0.51574, val loss: 0.50319\n",
      "Main effects tuning epoch: 12, train loss: 0.51563, val loss: 0.50186\n",
      "Main effects tuning epoch: 13, train loss: 0.51576, val loss: 0.50178\n",
      "Main effects tuning epoch: 14, train loss: 0.51619, val loss: 0.50417\n",
      "Main effects tuning epoch: 15, train loss: 0.51630, val loss: 0.50174\n",
      "Main effects tuning epoch: 16, train loss: 0.51617, val loss: 0.50504\n",
      "Main effects tuning epoch: 17, train loss: 0.51578, val loss: 0.50114\n",
      "Main effects tuning epoch: 18, train loss: 0.51571, val loss: 0.50339\n",
      "Main effects tuning epoch: 19, train loss: 0.51541, val loss: 0.50297\n",
      "Main effects tuning epoch: 20, train loss: 0.51567, val loss: 0.50233\n",
      "Main effects tuning epoch: 21, train loss: 0.51545, val loss: 0.50194\n",
      "Main effects tuning epoch: 22, train loss: 0.51550, val loss: 0.50202\n",
      "Main effects tuning epoch: 23, train loss: 0.51527, val loss: 0.50183\n",
      "Main effects tuning epoch: 24, train loss: 0.51583, val loss: 0.50227\n",
      "Main effects tuning epoch: 25, train loss: 0.51612, val loss: 0.50429\n",
      "Main effects tuning epoch: 26, train loss: 0.51526, val loss: 0.50222\n",
      "Main effects tuning epoch: 27, train loss: 0.51592, val loss: 0.50259\n",
      "Main effects tuning epoch: 28, train loss: 0.51541, val loss: 0.50208\n",
      "Main effects tuning epoch: 29, train loss: 0.51598, val loss: 0.50367\n",
      "Main effects tuning epoch: 30, train loss: 0.51600, val loss: 0.50229\n",
      "Main effects tuning epoch: 31, train loss: 0.51550, val loss: 0.50323\n",
      "Main effects tuning epoch: 32, train loss: 0.51546, val loss: 0.50104\n",
      "Main effects tuning epoch: 33, train loss: 0.51535, val loss: 0.50236\n",
      "Main effects tuning epoch: 34, train loss: 0.51519, val loss: 0.50118\n",
      "Main effects tuning epoch: 35, train loss: 0.51491, val loss: 0.50257\n",
      "Main effects tuning epoch: 36, train loss: 0.51491, val loss: 0.50182\n",
      "Main effects tuning epoch: 37, train loss: 0.51516, val loss: 0.50071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects tuning epoch: 38, train loss: 0.51511, val loss: 0.50194\n",
      "Main effects tuning epoch: 39, train loss: 0.51511, val loss: 0.50230\n",
      "Main effects tuning epoch: 40, train loss: 0.51584, val loss: 0.50414\n",
      "Main effects tuning epoch: 41, train loss: 0.51531, val loss: 0.50065\n",
      "Main effects tuning epoch: 42, train loss: 0.51515, val loss: 0.50337\n",
      "Main effects tuning epoch: 43, train loss: 0.51471, val loss: 0.50169\n",
      "Main effects tuning epoch: 44, train loss: 0.51484, val loss: 0.50097\n",
      "Main effects tuning epoch: 45, train loss: 0.51488, val loss: 0.50235\n",
      "Main effects tuning epoch: 46, train loss: 0.51479, val loss: 0.50102\n",
      "Main effects tuning epoch: 47, train loss: 0.51487, val loss: 0.50196\n",
      "Main effects tuning epoch: 48, train loss: 0.51463, val loss: 0.50183\n",
      "Main effects tuning epoch: 49, train loss: 0.51469, val loss: 0.50103\n",
      "Main effects tuning epoch: 50, train loss: 0.51472, val loss: 0.50134\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.49630, val loss: 0.48952\n",
      "Interaction training epoch: 2, train loss: 0.33769, val loss: 0.34284\n",
      "Interaction training epoch: 3, train loss: 0.35128, val loss: 0.36496\n",
      "Interaction training epoch: 4, train loss: 0.31659, val loss: 0.31842\n",
      "Interaction training epoch: 5, train loss: 0.30166, val loss: 0.30688\n",
      "Interaction training epoch: 6, train loss: 0.29518, val loss: 0.29846\n",
      "Interaction training epoch: 7, train loss: 0.28798, val loss: 0.28992\n",
      "Interaction training epoch: 8, train loss: 0.28937, val loss: 0.29121\n",
      "Interaction training epoch: 9, train loss: 0.29272, val loss: 0.29817\n",
      "Interaction training epoch: 10, train loss: 0.29134, val loss: 0.29788\n",
      "Interaction training epoch: 11, train loss: 0.28068, val loss: 0.28755\n",
      "Interaction training epoch: 12, train loss: 0.28154, val loss: 0.28695\n",
      "Interaction training epoch: 13, train loss: 0.27857, val loss: 0.28611\n",
      "Interaction training epoch: 14, train loss: 0.28018, val loss: 0.28605\n",
      "Interaction training epoch: 15, train loss: 0.27761, val loss: 0.28852\n",
      "Interaction training epoch: 16, train loss: 0.27799, val loss: 0.28495\n",
      "Interaction training epoch: 17, train loss: 0.27600, val loss: 0.28684\n",
      "Interaction training epoch: 18, train loss: 0.27797, val loss: 0.28446\n",
      "Interaction training epoch: 19, train loss: 0.27427, val loss: 0.28254\n",
      "Interaction training epoch: 20, train loss: 0.27444, val loss: 0.28269\n",
      "Interaction training epoch: 21, train loss: 0.27197, val loss: 0.28017\n",
      "Interaction training epoch: 22, train loss: 0.28111, val loss: 0.29098\n",
      "Interaction training epoch: 23, train loss: 0.27471, val loss: 0.28461\n",
      "Interaction training epoch: 24, train loss: 0.27383, val loss: 0.28217\n",
      "Interaction training epoch: 25, train loss: 0.27561, val loss: 0.28093\n",
      "Interaction training epoch: 26, train loss: 0.27286, val loss: 0.28056\n",
      "Interaction training epoch: 27, train loss: 0.27501, val loss: 0.28302\n",
      "Interaction training epoch: 28, train loss: 0.27465, val loss: 0.28525\n",
      "Interaction training epoch: 29, train loss: 0.27005, val loss: 0.27613\n",
      "Interaction training epoch: 30, train loss: 0.27126, val loss: 0.27893\n",
      "Interaction training epoch: 31, train loss: 0.27173, val loss: 0.28095\n",
      "Interaction training epoch: 32, train loss: 0.27099, val loss: 0.28101\n",
      "Interaction training epoch: 33, train loss: 0.26687, val loss: 0.27472\n",
      "Interaction training epoch: 34, train loss: 0.26881, val loss: 0.27762\n",
      "Interaction training epoch: 35, train loss: 0.26911, val loss: 0.28276\n",
      "Interaction training epoch: 36, train loss: 0.27233, val loss: 0.28279\n",
      "Interaction training epoch: 37, train loss: 0.27192, val loss: 0.28092\n",
      "Interaction training epoch: 38, train loss: 0.26677, val loss: 0.27617\n",
      "Interaction training epoch: 39, train loss: 0.26946, val loss: 0.27759\n",
      "Interaction training epoch: 40, train loss: 0.26705, val loss: 0.28085\n",
      "Interaction training epoch: 41, train loss: 0.26524, val loss: 0.27444\n",
      "Interaction training epoch: 42, train loss: 0.27260, val loss: 0.28364\n",
      "Interaction training epoch: 43, train loss: 0.26587, val loss: 0.27973\n",
      "Interaction training epoch: 44, train loss: 0.26654, val loss: 0.27690\n",
      "Interaction training epoch: 45, train loss: 0.26566, val loss: 0.28007\n",
      "Interaction training epoch: 46, train loss: 0.26457, val loss: 0.27688\n",
      "Interaction training epoch: 47, train loss: 0.26778, val loss: 0.28093\n",
      "Interaction training epoch: 48, train loss: 0.26713, val loss: 0.28041\n",
      "Interaction training epoch: 49, train loss: 0.26385, val loss: 0.27635\n",
      "Interaction training epoch: 50, train loss: 0.26436, val loss: 0.27912\n",
      "Interaction training epoch: 51, train loss: 0.26506, val loss: 0.27701\n",
      "Interaction training epoch: 52, train loss: 0.26332, val loss: 0.27839\n",
      "Interaction training epoch: 53, train loss: 0.26575, val loss: 0.27820\n",
      "Interaction training epoch: 54, train loss: 0.26444, val loss: 0.27399\n",
      "Interaction training epoch: 55, train loss: 0.26446, val loss: 0.28050\n",
      "Interaction training epoch: 56, train loss: 0.26212, val loss: 0.27389\n",
      "Interaction training epoch: 57, train loss: 0.26537, val loss: 0.28100\n",
      "Interaction training epoch: 58, train loss: 0.26231, val loss: 0.27333\n",
      "Interaction training epoch: 59, train loss: 0.26531, val loss: 0.28369\n",
      "Interaction training epoch: 60, train loss: 0.26223, val loss: 0.27409\n",
      "Interaction training epoch: 61, train loss: 0.26070, val loss: 0.27585\n",
      "Interaction training epoch: 62, train loss: 0.26415, val loss: 0.28179\n",
      "Interaction training epoch: 63, train loss: 0.26012, val loss: 0.27531\n",
      "Interaction training epoch: 64, train loss: 0.26214, val loss: 0.27596\n",
      "Interaction training epoch: 65, train loss: 0.26123, val loss: 0.27634\n",
      "Interaction training epoch: 66, train loss: 0.26041, val loss: 0.27562\n",
      "Interaction training epoch: 67, train loss: 0.26073, val loss: 0.27566\n",
      "Interaction training epoch: 68, train loss: 0.26038, val loss: 0.27691\n",
      "Interaction training epoch: 69, train loss: 0.25848, val loss: 0.27646\n",
      "Interaction training epoch: 70, train loss: 0.26296, val loss: 0.27763\n",
      "Interaction training epoch: 71, train loss: 0.26102, val loss: 0.27864\n",
      "Interaction training epoch: 72, train loss: 0.26107, val loss: 0.27708\n",
      "Interaction training epoch: 73, train loss: 0.26414, val loss: 0.27619\n",
      "Interaction training epoch: 74, train loss: 0.25864, val loss: 0.27725\n",
      "Interaction training epoch: 75, train loss: 0.25926, val loss: 0.27525\n",
      "Interaction training epoch: 76, train loss: 0.25686, val loss: 0.27432\n",
      "Interaction training epoch: 77, train loss: 0.25954, val loss: 0.27993\n",
      "Interaction training epoch: 78, train loss: 0.26144, val loss: 0.27545\n",
      "Interaction training epoch: 79, train loss: 0.25751, val loss: 0.27693\n",
      "Interaction training epoch: 80, train loss: 0.25861, val loss: 0.27448\n",
      "Interaction training epoch: 81, train loss: 0.25719, val loss: 0.27631\n",
      "Interaction training epoch: 82, train loss: 0.26048, val loss: 0.27937\n",
      "Interaction training epoch: 83, train loss: 0.25757, val loss: 0.27424\n",
      "Interaction training epoch: 84, train loss: 0.25932, val loss: 0.27815\n",
      "Interaction training epoch: 85, train loss: 0.26140, val loss: 0.27858\n",
      "Interaction training epoch: 86, train loss: 0.25378, val loss: 0.27194\n",
      "Interaction training epoch: 87, train loss: 0.25811, val loss: 0.27779\n",
      "Interaction training epoch: 88, train loss: 0.25630, val loss: 0.27527\n",
      "Interaction training epoch: 89, train loss: 0.25527, val loss: 0.27723\n",
      "Interaction training epoch: 90, train loss: 0.25759, val loss: 0.27676\n",
      "Interaction training epoch: 91, train loss: 0.25457, val loss: 0.27467\n",
      "Interaction training epoch: 92, train loss: 0.25542, val loss: 0.27403\n",
      "Interaction training epoch: 93, train loss: 0.25535, val loss: 0.27589\n",
      "Interaction training epoch: 94, train loss: 0.25407, val loss: 0.27538\n",
      "Interaction training epoch: 95, train loss: 0.25767, val loss: 0.27769\n",
      "Interaction training epoch: 96, train loss: 0.25220, val loss: 0.27444\n",
      "Interaction training epoch: 97, train loss: 0.25584, val loss: 0.27263\n",
      "Interaction training epoch: 98, train loss: 0.25244, val loss: 0.27405\n",
      "Interaction training epoch: 99, train loss: 0.25535, val loss: 0.27697\n",
      "Interaction training epoch: 100, train loss: 0.25545, val loss: 0.27934\n",
      "Interaction training epoch: 101, train loss: 0.25633, val loss: 0.27754\n",
      "Interaction training epoch: 102, train loss: 0.25267, val loss: 0.27028\n",
      "Interaction training epoch: 103, train loss: 0.25493, val loss: 0.27683\n",
      "Interaction training epoch: 104, train loss: 0.25262, val loss: 0.27466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 105, train loss: 0.25030, val loss: 0.27110\n",
      "Interaction training epoch: 106, train loss: 0.25358, val loss: 0.27660\n",
      "Interaction training epoch: 107, train loss: 0.25603, val loss: 0.27754\n",
      "Interaction training epoch: 108, train loss: 0.24978, val loss: 0.27290\n",
      "Interaction training epoch: 109, train loss: 0.25282, val loss: 0.27685\n",
      "Interaction training epoch: 110, train loss: 0.25069, val loss: 0.27038\n",
      "Interaction training epoch: 111, train loss: 0.25313, val loss: 0.27830\n",
      "Interaction training epoch: 112, train loss: 0.25039, val loss: 0.27405\n",
      "Interaction training epoch: 113, train loss: 0.25058, val loss: 0.27310\n",
      "Interaction training epoch: 114, train loss: 0.25279, val loss: 0.27999\n",
      "Interaction training epoch: 115, train loss: 0.24922, val loss: 0.27298\n",
      "Interaction training epoch: 116, train loss: 0.25238, val loss: 0.28017\n",
      "Interaction training epoch: 117, train loss: 0.24869, val loss: 0.27323\n",
      "Interaction training epoch: 118, train loss: 0.24943, val loss: 0.27615\n",
      "Interaction training epoch: 119, train loss: 0.24886, val loss: 0.27248\n",
      "Interaction training epoch: 120, train loss: 0.24706, val loss: 0.27312\n",
      "Interaction training epoch: 121, train loss: 0.24960, val loss: 0.27178\n",
      "Interaction training epoch: 122, train loss: 0.24969, val loss: 0.27520\n",
      "Interaction training epoch: 123, train loss: 0.24595, val loss: 0.27154\n",
      "Interaction training epoch: 124, train loss: 0.24900, val loss: 0.27416\n",
      "Interaction training epoch: 125, train loss: 0.24952, val loss: 0.27728\n",
      "Interaction training epoch: 126, train loss: 0.24514, val loss: 0.27059\n",
      "Interaction training epoch: 127, train loss: 0.24724, val loss: 0.27257\n",
      "Interaction training epoch: 128, train loss: 0.24934, val loss: 0.27673\n",
      "Interaction training epoch: 129, train loss: 0.25057, val loss: 0.27925\n",
      "Interaction training epoch: 130, train loss: 0.24874, val loss: 0.27362\n",
      "Interaction training epoch: 131, train loss: 0.24820, val loss: 0.27789\n",
      "Interaction training epoch: 132, train loss: 0.24936, val loss: 0.27270\n",
      "Interaction training epoch: 133, train loss: 0.24469, val loss: 0.27186\n",
      "Interaction training epoch: 134, train loss: 0.24571, val loss: 0.27370\n",
      "Interaction training epoch: 135, train loss: 0.24431, val loss: 0.27322\n",
      "Interaction training epoch: 136, train loss: 0.24633, val loss: 0.27261\n",
      "Interaction training epoch: 137, train loss: 0.24825, val loss: 0.27969\n",
      "Interaction training epoch: 138, train loss: 0.24335, val loss: 0.27059\n",
      "Interaction training epoch: 139, train loss: 0.24851, val loss: 0.27798\n",
      "Interaction training epoch: 140, train loss: 0.24583, val loss: 0.27031\n",
      "Interaction training epoch: 141, train loss: 0.24941, val loss: 0.28354\n",
      "Interaction training epoch: 142, train loss: 0.24411, val loss: 0.27392\n",
      "Interaction training epoch: 143, train loss: 0.24626, val loss: 0.27780\n",
      "Interaction training epoch: 144, train loss: 0.24308, val loss: 0.27648\n",
      "Interaction training epoch: 145, train loss: 0.24347, val loss: 0.27154\n",
      "Interaction training epoch: 146, train loss: 0.24461, val loss: 0.27730\n",
      "Interaction training epoch: 147, train loss: 0.24531, val loss: 0.27648\n",
      "Interaction training epoch: 148, train loss: 0.24362, val loss: 0.27834\n",
      "Interaction training epoch: 149, train loss: 0.24601, val loss: 0.27461\n",
      "Interaction training epoch: 150, train loss: 0.24428, val loss: 0.27347\n",
      "Interaction training epoch: 151, train loss: 0.24732, val loss: 0.28053\n",
      "Interaction training epoch: 152, train loss: 0.24191, val loss: 0.27360\n",
      "Interaction training epoch: 153, train loss: 0.24221, val loss: 0.27440\n",
      "Interaction training epoch: 154, train loss: 0.24280, val loss: 0.27499\n",
      "Interaction training epoch: 155, train loss: 0.24201, val loss: 0.27586\n",
      "Interaction training epoch: 156, train loss: 0.24477, val loss: 0.27752\n",
      "Interaction training epoch: 157, train loss: 0.24507, val loss: 0.27833\n",
      "Interaction training epoch: 158, train loss: 0.24068, val loss: 0.27333\n",
      "Interaction training epoch: 159, train loss: 0.24391, val loss: 0.27683\n",
      "Interaction training epoch: 160, train loss: 0.24175, val loss: 0.27674\n",
      "Interaction training epoch: 161, train loss: 0.23881, val loss: 0.26956\n",
      "Interaction training epoch: 162, train loss: 0.24359, val loss: 0.27676\n",
      "Interaction training epoch: 163, train loss: 0.23898, val loss: 0.27401\n",
      "Interaction training epoch: 164, train loss: 0.23981, val loss: 0.27359\n",
      "Interaction training epoch: 165, train loss: 0.24112, val loss: 0.27625\n",
      "Interaction training epoch: 166, train loss: 0.23941, val loss: 0.27442\n",
      "Interaction training epoch: 167, train loss: 0.24063, val loss: 0.27651\n",
      "Interaction training epoch: 168, train loss: 0.24010, val loss: 0.27885\n",
      "Interaction training epoch: 169, train loss: 0.24168, val loss: 0.27591\n",
      "Interaction training epoch: 170, train loss: 0.24327, val loss: 0.28124\n",
      "Interaction training epoch: 171, train loss: 0.23941, val loss: 0.27438\n",
      "Interaction training epoch: 172, train loss: 0.24055, val loss: 0.27595\n",
      "Interaction training epoch: 173, train loss: 0.23916, val loss: 0.27534\n",
      "Interaction training epoch: 174, train loss: 0.24062, val loss: 0.28139\n",
      "Interaction training epoch: 175, train loss: 0.23799, val loss: 0.27390\n",
      "Interaction training epoch: 176, train loss: 0.23947, val loss: 0.27684\n",
      "Interaction training epoch: 177, train loss: 0.23886, val loss: 0.27735\n",
      "Interaction training epoch: 178, train loss: 0.23895, val loss: 0.27472\n",
      "Interaction training epoch: 179, train loss: 0.23942, val loss: 0.27651\n",
      "Interaction training epoch: 180, train loss: 0.24049, val loss: 0.28036\n",
      "Interaction training epoch: 181, train loss: 0.23897, val loss: 0.27791\n",
      "Interaction training epoch: 182, train loss: 0.23646, val loss: 0.27363\n",
      "Interaction training epoch: 183, train loss: 0.23810, val loss: 0.27910\n",
      "Interaction training epoch: 184, train loss: 0.23821, val loss: 0.27539\n",
      "Interaction training epoch: 185, train loss: 0.23642, val loss: 0.27411\n",
      "Interaction training epoch: 186, train loss: 0.23809, val loss: 0.27799\n",
      "Interaction training epoch: 187, train loss: 0.23796, val loss: 0.27475\n",
      "Interaction training epoch: 188, train loss: 0.23539, val loss: 0.27566\n",
      "Interaction training epoch: 189, train loss: 0.23735, val loss: 0.27996\n",
      "Interaction training epoch: 190, train loss: 0.23828, val loss: 0.27894\n",
      "Interaction training epoch: 191, train loss: 0.23465, val loss: 0.27510\n",
      "Interaction training epoch: 192, train loss: 0.23838, val loss: 0.27980\n",
      "Interaction training epoch: 193, train loss: 0.23741, val loss: 0.27565\n",
      "Interaction training epoch: 194, train loss: 0.23572, val loss: 0.27452\n",
      "Interaction training epoch: 195, train loss: 0.23802, val loss: 0.28027\n",
      "Interaction training epoch: 196, train loss: 0.23539, val loss: 0.27689\n",
      "Interaction training epoch: 197, train loss: 0.23723, val loss: 0.27746\n",
      "Interaction training epoch: 198, train loss: 0.23395, val loss: 0.27462\n",
      "Interaction training epoch: 199, train loss: 0.23661, val loss: 0.28002\n",
      "Interaction training epoch: 200, train loss: 0.23513, val loss: 0.27434\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########3 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.24968, val loss: 0.27903\n",
      "Interaction tuning epoch: 2, train loss: 0.24776, val loss: 0.28053\n",
      "Interaction tuning epoch: 3, train loss: 0.24547, val loss: 0.27095\n",
      "Interaction tuning epoch: 4, train loss: 0.24990, val loss: 0.28140\n",
      "Interaction tuning epoch: 5, train loss: 0.24715, val loss: 0.27506\n",
      "Interaction tuning epoch: 6, train loss: 0.24635, val loss: 0.27332\n",
      "Interaction tuning epoch: 7, train loss: 0.24591, val loss: 0.27750\n",
      "Interaction tuning epoch: 8, train loss: 0.24374, val loss: 0.27154\n",
      "Interaction tuning epoch: 9, train loss: 0.24867, val loss: 0.27508\n",
      "Interaction tuning epoch: 10, train loss: 0.24609, val loss: 0.27569\n",
      "Interaction tuning epoch: 11, train loss: 0.24703, val loss: 0.27608\n",
      "Interaction tuning epoch: 12, train loss: 0.24513, val loss: 0.27373\n",
      "Interaction tuning epoch: 13, train loss: 0.24529, val loss: 0.27530\n",
      "Interaction tuning epoch: 14, train loss: 0.24515, val loss: 0.27229\n",
      "Interaction tuning epoch: 15, train loss: 0.24666, val loss: 0.27752\n",
      "Interaction tuning epoch: 16, train loss: 0.24396, val loss: 0.27377\n",
      "Interaction tuning epoch: 17, train loss: 0.24588, val loss: 0.27324\n",
      "Interaction tuning epoch: 18, train loss: 0.24525, val loss: 0.27665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 19, train loss: 0.24285, val loss: 0.27234\n",
      "Interaction tuning epoch: 20, train loss: 0.24430, val loss: 0.27507\n",
      "Interaction tuning epoch: 21, train loss: 0.24167, val loss: 0.27076\n",
      "Interaction tuning epoch: 22, train loss: 0.24412, val loss: 0.27322\n",
      "Interaction tuning epoch: 23, train loss: 0.24488, val loss: 0.27656\n",
      "Interaction tuning epoch: 24, train loss: 0.24215, val loss: 0.27113\n",
      "Interaction tuning epoch: 25, train loss: 0.24397, val loss: 0.27656\n",
      "Interaction tuning epoch: 26, train loss: 0.24370, val loss: 0.27526\n",
      "Interaction tuning epoch: 27, train loss: 0.24330, val loss: 0.27308\n",
      "Interaction tuning epoch: 28, train loss: 0.24366, val loss: 0.27189\n",
      "Interaction tuning epoch: 29, train loss: 0.24133, val loss: 0.27163\n",
      "Interaction tuning epoch: 30, train loss: 0.24330, val loss: 0.27379\n",
      "Interaction tuning epoch: 31, train loss: 0.24268, val loss: 0.27507\n",
      "Interaction tuning epoch: 32, train loss: 0.24475, val loss: 0.27470\n",
      "Interaction tuning epoch: 33, train loss: 0.24434, val loss: 0.27429\n",
      "Interaction tuning epoch: 34, train loss: 0.24265, val loss: 0.27108\n",
      "Interaction tuning epoch: 35, train loss: 0.24270, val loss: 0.27413\n",
      "Interaction tuning epoch: 36, train loss: 0.24404, val loss: 0.27374\n",
      "Interaction tuning epoch: 37, train loss: 0.24276, val loss: 0.27324\n",
      "Interaction tuning epoch: 38, train loss: 0.24406, val loss: 0.27388\n",
      "Interaction tuning epoch: 39, train loss: 0.24261, val loss: 0.27577\n",
      "Interaction tuning epoch: 40, train loss: 0.24088, val loss: 0.27160\n",
      "Interaction tuning epoch: 41, train loss: 0.24276, val loss: 0.27220\n",
      "Interaction tuning epoch: 42, train loss: 0.24198, val loss: 0.27404\n",
      "Interaction tuning epoch: 43, train loss: 0.24426, val loss: 0.27793\n",
      "Interaction tuning epoch: 44, train loss: 0.24072, val loss: 0.27060\n",
      "Interaction tuning epoch: 45, train loss: 0.24021, val loss: 0.27217\n",
      "Interaction tuning epoch: 46, train loss: 0.24042, val loss: 0.27170\n",
      "Interaction tuning epoch: 47, train loss: 0.24126, val loss: 0.27376\n",
      "Interaction tuning epoch: 48, train loss: 0.24628, val loss: 0.27823\n",
      "Interaction tuning epoch: 49, train loss: 0.24267, val loss: 0.26952\n",
      "Interaction tuning epoch: 50, train loss: 0.24486, val loss: 0.27854\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 35.130433559417725\n",
      "After the gam stage, training error is 0.24486 , validation error is 0.27854\n",
      "missing value counts: 99148\n",
      "#####start auto_tuning#####\n",
      "the best shrinkage is 0.793600\n",
      "the best combination is 0.828062\n",
      "[SoftImpute] Max Singular Value of X_init = 3.614024\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.210223 validation BCE=0.302001,rank=5\n",
      "[SoftImpute] Iter 2: observed BCE=0.205954 validation BCE=0.311485,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.203334 validation BCE=0.311179,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.201262 validation BCE=0.311024,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.199578 validation BCE=0.295228,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.198305 validation BCE=0.322317,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.197369 validation BCE=0.311858,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.196545 validation BCE=0.300987,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.196233 validation BCE=0.308098,rank=5\n",
      "[SoftImpute] Iter 10: observed BCE=0.195690 validation BCE=0.307743,rank=5\n",
      "[SoftImpute] Iter 11: observed BCE=0.195249 validation BCE=0.306858,rank=5\n",
      "[SoftImpute] Iter 12: observed BCE=0.194848 validation BCE=0.305982,rank=5\n",
      "[SoftImpute] Iter 13: observed BCE=0.194206 validation BCE=0.305673,rank=5\n",
      "[SoftImpute] Iter 14: observed BCE=0.194243 validation BCE=0.305196,rank=5\n",
      "[SoftImpute] Iter 15: observed BCE=0.193768 validation BCE=0.305220,rank=5\n",
      "[SoftImpute] Iter 16: observed BCE=0.194005 validation BCE=0.304775,rank=5\n",
      "[SoftImpute] Iter 17: observed BCE=0.193539 validation BCE=0.304213,rank=5\n",
      "[SoftImpute] Iter 18: observed BCE=0.193023 validation BCE=0.304713,rank=5\n",
      "[SoftImpute] Iter 19: observed BCE=0.192590 validation BCE=0.303756,rank=5\n",
      "[SoftImpute] Iter 20: observed BCE=0.192547 validation BCE=0.304226,rank=5\n",
      "[SoftImpute] Iter 21: observed BCE=0.192367 validation BCE=0.303739,rank=5\n",
      "[SoftImpute] Iter 22: observed BCE=0.192544 validation BCE=0.293034,rank=5\n",
      "[SoftImpute] Iter 23: observed BCE=0.192264 validation BCE=0.292581,rank=5\n",
      "[SoftImpute] Iter 24: observed BCE=0.191847 validation BCE=0.291544,rank=5\n",
      "[SoftImpute] Iter 25: observed BCE=0.192086 validation BCE=0.291198,rank=5\n",
      "[SoftImpute] Iter 26: observed BCE=0.191725 validation BCE=0.291109,rank=5\n",
      "[SoftImpute] Iter 27: observed BCE=0.191269 validation BCE=0.291007,rank=5\n",
      "[SoftImpute] Iter 28: observed BCE=0.191605 validation BCE=0.291220,rank=5\n",
      "[SoftImpute] Iter 29: observed BCE=0.191331 validation BCE=0.290503,rank=5\n",
      "[SoftImpute] Iter 30: observed BCE=0.191158 validation BCE=0.290367,rank=5\n",
      "[SoftImpute] Iter 31: observed BCE=0.191544 validation BCE=0.290682,rank=5\n",
      "[SoftImpute] Iter 32: observed BCE=0.191516 validation BCE=0.289794,rank=5\n",
      "[SoftImpute] Iter 33: observed BCE=0.191111 validation BCE=0.290006,rank=5\n",
      "[SoftImpute] Iter 34: observed BCE=0.191081 validation BCE=0.289764,rank=5\n",
      "[SoftImpute] Iter 35: observed BCE=0.191116 validation BCE=0.289875,rank=5\n",
      "[SoftImpute] Iter 36: observed BCE=0.191080 validation BCE=0.289606,rank=5\n",
      "[SoftImpute] Iter 37: observed BCE=0.191509 validation BCE=0.289096,rank=5\n",
      "[SoftImpute] Iter 38: observed BCE=0.191282 validation BCE=0.289146,rank=5\n",
      "[SoftImpute] Iter 39: observed BCE=0.190939 validation BCE=0.288624,rank=5\n",
      "[SoftImpute] Iter 40: observed BCE=0.191055 validation BCE=0.289599,rank=5\n",
      "[SoftImpute] Iter 41: observed BCE=0.191111 validation BCE=0.289234,rank=5\n",
      "[SoftImpute] Iter 42: observed BCE=0.190971 validation BCE=0.288806,rank=5\n",
      "[SoftImpute] Iter 43: observed BCE=0.190708 validation BCE=0.288285,rank=5\n",
      "[SoftImpute] Iter 44: observed BCE=0.190928 validation BCE=0.288597,rank=5\n",
      "[SoftImpute] Iter 45: observed BCE=0.190795 validation BCE=0.288765,rank=5\n",
      "[SoftImpute] Iter 46: observed BCE=0.190900 validation BCE=0.287882,rank=5\n",
      "[SoftImpute] Iter 47: observed BCE=0.190767 validation BCE=0.288413,rank=5\n",
      "[SoftImpute] Iter 48: observed BCE=0.190769 validation BCE=0.277446,rank=5\n",
      "[SoftImpute] Iter 49: observed BCE=0.190373 validation BCE=0.278182,rank=5\n",
      "[SoftImpute] Iter 50: observed BCE=0.190845 validation BCE=0.278781,rank=5\n",
      "[SoftImpute] Iter 51: observed BCE=0.190776 validation BCE=0.277839,rank=5\n",
      "[SoftImpute] Iter 52: observed BCE=0.190754 validation BCE=0.277774,rank=5\n",
      "[SoftImpute] Iter 53: observed BCE=0.190906 validation BCE=0.277719,rank=5\n",
      "[SoftImpute] Iter 54: observed BCE=0.190650 validation BCE=0.277612,rank=5\n",
      "[SoftImpute] Iter 55: observed BCE=0.190479 validation BCE=0.277129,rank=5\n",
      "[SoftImpute] Iter 56: observed BCE=0.190766 validation BCE=0.277699,rank=5\n",
      "[SoftImpute] Iter 57: observed BCE=0.190427 validation BCE=0.277755,rank=5\n",
      "[SoftImpute] Iter 58: observed BCE=0.190524 validation BCE=0.276163,rank=5\n",
      "[SoftImpute] Iter 59: observed BCE=0.190243 validation BCE=0.276702,rank=5\n",
      "[SoftImpute] Iter 60: observed BCE=0.190478 validation BCE=0.276102,rank=5\n",
      "[SoftImpute] Iter 61: observed BCE=0.190734 validation BCE=0.275688,rank=5\n",
      "[SoftImpute] Iter 62: observed BCE=0.190719 validation BCE=0.275907,rank=5\n",
      "[SoftImpute] Iter 63: observed BCE=0.190321 validation BCE=0.276249,rank=5\n",
      "[SoftImpute] Iter 64: observed BCE=0.190791 validation BCE=0.275631,rank=5\n",
      "[SoftImpute] Iter 65: observed BCE=0.190693 validation BCE=0.276627,rank=5\n",
      "[SoftImpute] Iter 66: observed BCE=0.190658 validation BCE=0.275780,rank=5\n",
      "[SoftImpute] Iter 67: observed BCE=0.190280 validation BCE=0.276119,rank=5\n",
      "[SoftImpute] Iter 68: observed BCE=0.190557 validation BCE=0.275250,rank=5\n",
      "[SoftImpute] Iter 69: observed BCE=0.190407 validation BCE=0.275458,rank=5\n",
      "[SoftImpute] Iter 70: observed BCE=0.190417 validation BCE=0.275529,rank=5\n",
      "[SoftImpute] Iter 71: observed BCE=0.190585 validation BCE=0.275379,rank=5\n",
      "[SoftImpute] Iter 72: observed BCE=0.190580 validation BCE=0.275706,rank=5\n",
      "[SoftImpute] Iter 73: observed BCE=0.190507 validation BCE=0.275459,rank=5\n",
      "[SoftImpute] Iter 74: observed BCE=0.190276 validation BCE=0.275702,rank=5\n",
      "[SoftImpute] Iter 75: observed BCE=0.190370 validation BCE=0.275498,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 76: observed BCE=0.190409 validation BCE=0.276027,rank=5\n",
      "[SoftImpute] Iter 77: observed BCE=0.190424 validation BCE=0.275367,rank=5\n",
      "[SoftImpute] Iter 78: observed BCE=0.190334 validation BCE=0.275245,rank=5\n",
      "[SoftImpute] Iter 79: observed BCE=0.190571 validation BCE=0.275344,rank=5\n",
      "[SoftImpute] Iter 80: observed BCE=0.190451 validation BCE=0.274979,rank=5\n",
      "[SoftImpute] Iter 81: observed BCE=0.190378 validation BCE=0.275090,rank=5\n",
      "[SoftImpute] Iter 82: observed BCE=0.190490 validation BCE=0.275822,rank=5\n",
      "[SoftImpute] Iter 83: observed BCE=0.190518 validation BCE=0.275236,rank=5\n",
      "[SoftImpute] Iter 84: observed BCE=0.190407 validation BCE=0.275596,rank=5\n",
      "[SoftImpute] Iter 85: observed BCE=0.190548 validation BCE=0.275354,rank=5\n",
      "[SoftImpute] Iter 86: observed BCE=0.190594 validation BCE=0.275564,rank=5\n",
      "[SoftImpute] Iter 87: observed BCE=0.190307 validation BCE=0.275463,rank=5\n",
      "[SoftImpute] Iter 88: observed BCE=0.190281 validation BCE=0.275197,rank=5\n",
      "[SoftImpute] Iter 89: observed BCE=0.190190 validation BCE=0.275785,rank=5\n",
      "[SoftImpute] Iter 90: observed BCE=0.190067 validation BCE=0.275360,rank=5\n",
      "[SoftImpute] Iter 91: observed BCE=0.190165 validation BCE=0.275388,rank=5\n",
      "[SoftImpute] Iter 92: observed BCE=0.190414 validation BCE=0.275448,rank=5\n",
      "[SoftImpute] Iter 93: observed BCE=0.190278 validation BCE=0.275398,rank=5\n",
      "[SoftImpute] Iter 94: observed BCE=0.190302 validation BCE=0.275117,rank=5\n",
      "[SoftImpute] Iter 95: observed BCE=0.190507 validation BCE=0.275575,rank=5\n",
      "[SoftImpute] Iter 96: observed BCE=0.190485 validation BCE=0.275071,rank=5\n",
      "[SoftImpute] Iter 97: observed BCE=0.190490 validation BCE=0.275627,rank=5\n",
      "[SoftImpute] Iter 98: observed BCE=0.190235 validation BCE=0.275298,rank=5\n",
      "[SoftImpute] Iter 99: observed BCE=0.190086 validation BCE=0.275211,rank=5\n",
      "[SoftImpute] Iter 100: observed BCE=0.190076 validation BCE=0.275895,rank=5\n",
      "[SoftImpute] Stopped after iteration 100 for lambda=0.072280\n",
      "final num of user group: 30\n",
      "final num of item group: 48\n",
      "change mode state : True\n",
      "time cost: 264.89658641815186\n",
      "After the matrix factor stage, training error is 0.19008, validation error is 0.27590\n",
      "0\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68318, val loss: 0.68192\n",
      "Main effects training epoch: 2, train loss: 0.67724, val loss: 0.67807\n",
      "Main effects training epoch: 3, train loss: 0.67035, val loss: 0.67239\n",
      "Main effects training epoch: 4, train loss: 0.66391, val loss: 0.66654\n",
      "Main effects training epoch: 5, train loss: 0.65041, val loss: 0.65054\n",
      "Main effects training epoch: 6, train loss: 0.62260, val loss: 0.61929\n",
      "Main effects training epoch: 7, train loss: 0.58024, val loss: 0.57432\n",
      "Main effects training epoch: 8, train loss: 0.54720, val loss: 0.53681\n",
      "Main effects training epoch: 9, train loss: 0.53313, val loss: 0.51521\n",
      "Main effects training epoch: 10, train loss: 0.53085, val loss: 0.50585\n",
      "Main effects training epoch: 11, train loss: 0.52583, val loss: 0.50411\n",
      "Main effects training epoch: 12, train loss: 0.52419, val loss: 0.50436\n",
      "Main effects training epoch: 13, train loss: 0.52555, val loss: 0.50572\n",
      "Main effects training epoch: 14, train loss: 0.52540, val loss: 0.50457\n",
      "Main effects training epoch: 15, train loss: 0.52453, val loss: 0.50567\n",
      "Main effects training epoch: 16, train loss: 0.52467, val loss: 0.50372\n",
      "Main effects training epoch: 17, train loss: 0.52324, val loss: 0.50326\n",
      "Main effects training epoch: 18, train loss: 0.52333, val loss: 0.50195\n",
      "Main effects training epoch: 19, train loss: 0.52447, val loss: 0.50358\n",
      "Main effects training epoch: 20, train loss: 0.52310, val loss: 0.50254\n",
      "Main effects training epoch: 21, train loss: 0.52305, val loss: 0.50199\n",
      "Main effects training epoch: 22, train loss: 0.52218, val loss: 0.50175\n",
      "Main effects training epoch: 23, train loss: 0.52230, val loss: 0.50203\n",
      "Main effects training epoch: 24, train loss: 0.52207, val loss: 0.50145\n",
      "Main effects training epoch: 25, train loss: 0.52221, val loss: 0.50219\n",
      "Main effects training epoch: 26, train loss: 0.52249, val loss: 0.50205\n",
      "Main effects training epoch: 27, train loss: 0.52240, val loss: 0.50186\n",
      "Main effects training epoch: 28, train loss: 0.52215, val loss: 0.50118\n",
      "Main effects training epoch: 29, train loss: 0.52167, val loss: 0.50131\n",
      "Main effects training epoch: 30, train loss: 0.52172, val loss: 0.50206\n",
      "Main effects training epoch: 31, train loss: 0.52143, val loss: 0.50126\n",
      "Main effects training epoch: 32, train loss: 0.52173, val loss: 0.50185\n",
      "Main effects training epoch: 33, train loss: 0.52157, val loss: 0.50118\n",
      "Main effects training epoch: 34, train loss: 0.52134, val loss: 0.50190\n",
      "Main effects training epoch: 35, train loss: 0.52160, val loss: 0.50114\n",
      "Main effects training epoch: 36, train loss: 0.52124, val loss: 0.50122\n",
      "Main effects training epoch: 37, train loss: 0.52159, val loss: 0.50182\n",
      "Main effects training epoch: 38, train loss: 0.52178, val loss: 0.50166\n",
      "Main effects training epoch: 39, train loss: 0.52107, val loss: 0.50101\n",
      "Main effects training epoch: 40, train loss: 0.52120, val loss: 0.50207\n",
      "Main effects training epoch: 41, train loss: 0.52137, val loss: 0.50102\n",
      "Main effects training epoch: 42, train loss: 0.52109, val loss: 0.50159\n",
      "Main effects training epoch: 43, train loss: 0.52137, val loss: 0.50169\n",
      "Main effects training epoch: 44, train loss: 0.52140, val loss: 0.50110\n",
      "Main effects training epoch: 45, train loss: 0.52299, val loss: 0.50459\n",
      "Main effects training epoch: 46, train loss: 0.52226, val loss: 0.50272\n",
      "Main effects training epoch: 47, train loss: 0.52101, val loss: 0.50147\n",
      "Main effects training epoch: 48, train loss: 0.52109, val loss: 0.50075\n",
      "Main effects training epoch: 49, train loss: 0.52069, val loss: 0.50215\n",
      "Main effects training epoch: 50, train loss: 0.52045, val loss: 0.50063\n",
      "Main effects training epoch: 51, train loss: 0.52108, val loss: 0.50126\n",
      "Main effects training epoch: 52, train loss: 0.52199, val loss: 0.50324\n",
      "Main effects training epoch: 53, train loss: 0.52131, val loss: 0.50122\n",
      "Main effects training epoch: 54, train loss: 0.52256, val loss: 0.50297\n",
      "Main effects training epoch: 55, train loss: 0.52142, val loss: 0.50258\n",
      "Main effects training epoch: 56, train loss: 0.52045, val loss: 0.50089\n",
      "Main effects training epoch: 57, train loss: 0.52043, val loss: 0.50180\n",
      "Main effects training epoch: 58, train loss: 0.52018, val loss: 0.50121\n",
      "Main effects training epoch: 59, train loss: 0.51996, val loss: 0.50068\n",
      "Main effects training epoch: 60, train loss: 0.52012, val loss: 0.50109\n",
      "Main effects training epoch: 61, train loss: 0.52012, val loss: 0.50024\n",
      "Main effects training epoch: 62, train loss: 0.52057, val loss: 0.50418\n",
      "Main effects training epoch: 63, train loss: 0.52043, val loss: 0.50038\n",
      "Main effects training epoch: 64, train loss: 0.52095, val loss: 0.50364\n",
      "Main effects training epoch: 65, train loss: 0.52038, val loss: 0.50085\n",
      "Main effects training epoch: 66, train loss: 0.52047, val loss: 0.50241\n",
      "Main effects training epoch: 67, train loss: 0.52117, val loss: 0.50295\n",
      "Main effects training epoch: 68, train loss: 0.52073, val loss: 0.50183\n",
      "Main effects training epoch: 69, train loss: 0.52087, val loss: 0.50244\n",
      "Main effects training epoch: 70, train loss: 0.52010, val loss: 0.50147\n",
      "Main effects training epoch: 71, train loss: 0.51994, val loss: 0.50147\n",
      "Main effects training epoch: 72, train loss: 0.51942, val loss: 0.50086\n",
      "Main effects training epoch: 73, train loss: 0.51949, val loss: 0.50028\n",
      "Main effects training epoch: 74, train loss: 0.52005, val loss: 0.50237\n",
      "Main effects training epoch: 75, train loss: 0.51990, val loss: 0.50184\n",
      "Main effects training epoch: 76, train loss: 0.51923, val loss: 0.50236\n",
      "Main effects training epoch: 77, train loss: 0.51910, val loss: 0.50038\n",
      "Main effects training epoch: 78, train loss: 0.51884, val loss: 0.50095\n",
      "Main effects training epoch: 79, train loss: 0.51884, val loss: 0.49983\n",
      "Main effects training epoch: 80, train loss: 0.51866, val loss: 0.50191\n",
      "Main effects training epoch: 81, train loss: 0.51939, val loss: 0.50027\n",
      "Main effects training epoch: 82, train loss: 0.51929, val loss: 0.50319\n",
      "Main effects training epoch: 83, train loss: 0.51868, val loss: 0.50126\n",
      "Main effects training epoch: 84, train loss: 0.51858, val loss: 0.50088\n",
      "Main effects training epoch: 85, train loss: 0.51835, val loss: 0.50068\n",
      "Main effects training epoch: 86, train loss: 0.51834, val loss: 0.50062\n",
      "Main effects training epoch: 87, train loss: 0.51844, val loss: 0.50224\n",
      "Main effects training epoch: 88, train loss: 0.51878, val loss: 0.50098\n",
      "Main effects training epoch: 89, train loss: 0.51903, val loss: 0.50192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 90, train loss: 0.51939, val loss: 0.50209\n",
      "Main effects training epoch: 91, train loss: 0.51881, val loss: 0.50201\n",
      "Main effects training epoch: 92, train loss: 0.51849, val loss: 0.50011\n",
      "Main effects training epoch: 93, train loss: 0.51893, val loss: 0.50406\n",
      "Main effects training epoch: 94, train loss: 0.51813, val loss: 0.49975\n",
      "Main effects training epoch: 95, train loss: 0.51804, val loss: 0.50212\n",
      "Main effects training epoch: 96, train loss: 0.51773, val loss: 0.50088\n",
      "Main effects training epoch: 97, train loss: 0.51767, val loss: 0.50090\n",
      "Main effects training epoch: 98, train loss: 0.51759, val loss: 0.50188\n",
      "Main effects training epoch: 99, train loss: 0.51749, val loss: 0.50231\n",
      "Main effects training epoch: 100, train loss: 0.51739, val loss: 0.50044\n",
      "Main effects training epoch: 101, train loss: 0.51733, val loss: 0.50052\n",
      "Main effects training epoch: 102, train loss: 0.51797, val loss: 0.50373\n",
      "Main effects training epoch: 103, train loss: 0.51813, val loss: 0.50085\n",
      "Main effects training epoch: 104, train loss: 0.51776, val loss: 0.50234\n",
      "Main effects training epoch: 105, train loss: 0.51726, val loss: 0.50227\n",
      "Main effects training epoch: 106, train loss: 0.51709, val loss: 0.50129\n",
      "Main effects training epoch: 107, train loss: 0.51781, val loss: 0.50127\n",
      "Main effects training epoch: 108, train loss: 0.51764, val loss: 0.50367\n",
      "Main effects training epoch: 109, train loss: 0.51682, val loss: 0.50096\n",
      "Main effects training epoch: 110, train loss: 0.51693, val loss: 0.50167\n",
      "Main effects training epoch: 111, train loss: 0.51671, val loss: 0.50182\n",
      "Main effects training epoch: 112, train loss: 0.51688, val loss: 0.50161\n",
      "Main effects training epoch: 113, train loss: 0.51698, val loss: 0.50128\n",
      "Main effects training epoch: 114, train loss: 0.51709, val loss: 0.50115\n",
      "Main effects training epoch: 115, train loss: 0.51748, val loss: 0.50244\n",
      "Main effects training epoch: 116, train loss: 0.51737, val loss: 0.50229\n",
      "Main effects training epoch: 117, train loss: 0.51675, val loss: 0.50288\n",
      "Main effects training epoch: 118, train loss: 0.51650, val loss: 0.50089\n",
      "Main effects training epoch: 119, train loss: 0.51679, val loss: 0.50204\n",
      "Main effects training epoch: 120, train loss: 0.51664, val loss: 0.50211\n",
      "Main effects training epoch: 121, train loss: 0.51670, val loss: 0.50227\n",
      "Main effects training epoch: 122, train loss: 0.51695, val loss: 0.50339\n",
      "Main effects training epoch: 123, train loss: 0.51659, val loss: 0.50129\n",
      "Main effects training epoch: 124, train loss: 0.51633, val loss: 0.50324\n",
      "Main effects training epoch: 125, train loss: 0.51613, val loss: 0.50129\n",
      "Main effects training epoch: 126, train loss: 0.51658, val loss: 0.50223\n",
      "Main effects training epoch: 127, train loss: 0.51636, val loss: 0.50314\n",
      "Main effects training epoch: 128, train loss: 0.51642, val loss: 0.50299\n",
      "Main effects training epoch: 129, train loss: 0.51689, val loss: 0.50193\n",
      "Main effects training epoch: 130, train loss: 0.51717, val loss: 0.50329\n",
      "Main effects training epoch: 131, train loss: 0.51619, val loss: 0.50278\n",
      "Main effects training epoch: 132, train loss: 0.51595, val loss: 0.50213\n",
      "Main effects training epoch: 133, train loss: 0.51591, val loss: 0.50190\n",
      "Main effects training epoch: 134, train loss: 0.51614, val loss: 0.50354\n",
      "Main effects training epoch: 135, train loss: 0.51593, val loss: 0.50167\n",
      "Main effects training epoch: 136, train loss: 0.51683, val loss: 0.50322\n",
      "Main effects training epoch: 137, train loss: 0.51631, val loss: 0.50292\n",
      "Main effects training epoch: 138, train loss: 0.51597, val loss: 0.50196\n",
      "Main effects training epoch: 139, train loss: 0.51594, val loss: 0.50118\n",
      "Main effects training epoch: 140, train loss: 0.51575, val loss: 0.50247\n",
      "Main effects training epoch: 141, train loss: 0.51562, val loss: 0.50226\n",
      "Main effects training epoch: 142, train loss: 0.51583, val loss: 0.50147\n",
      "Main effects training epoch: 143, train loss: 0.51602, val loss: 0.50303\n",
      "Main effects training epoch: 144, train loss: 0.51584, val loss: 0.50264\n",
      "Main effects training epoch: 145, train loss: 0.51575, val loss: 0.50308\n",
      "Main effects training epoch: 146, train loss: 0.51556, val loss: 0.50241\n",
      "Main effects training epoch: 147, train loss: 0.51596, val loss: 0.50159\n",
      "Main effects training epoch: 148, train loss: 0.51555, val loss: 0.50244\n",
      "Main effects training epoch: 149, train loss: 0.51552, val loss: 0.50203\n",
      "Main effects training epoch: 150, train loss: 0.51570, val loss: 0.50205\n",
      "Main effects training epoch: 151, train loss: 0.51556, val loss: 0.50363\n",
      "Main effects training epoch: 152, train loss: 0.51593, val loss: 0.50189\n",
      "Main effects training epoch: 153, train loss: 0.51639, val loss: 0.50286\n",
      "Main effects training epoch: 154, train loss: 0.51624, val loss: 0.50523\n",
      "Main effects training epoch: 155, train loss: 0.51611, val loss: 0.50246\n",
      "Main effects training epoch: 156, train loss: 0.51643, val loss: 0.50459\n",
      "Main effects training epoch: 157, train loss: 0.51592, val loss: 0.50249\n",
      "Main effects training epoch: 158, train loss: 0.51552, val loss: 0.50365\n",
      "Main effects training epoch: 159, train loss: 0.51556, val loss: 0.50177\n",
      "Main effects training epoch: 160, train loss: 0.51540, val loss: 0.50281\n",
      "Main effects training epoch: 161, train loss: 0.51544, val loss: 0.50300\n",
      "Main effects training epoch: 162, train loss: 0.51528, val loss: 0.50196\n",
      "Main effects training epoch: 163, train loss: 0.51547, val loss: 0.50171\n",
      "Main effects training epoch: 164, train loss: 0.51538, val loss: 0.50210\n",
      "Main effects training epoch: 165, train loss: 0.51565, val loss: 0.50469\n",
      "Main effects training epoch: 166, train loss: 0.51521, val loss: 0.50062\n",
      "Main effects training epoch: 167, train loss: 0.51535, val loss: 0.50326\n",
      "Main effects training epoch: 168, train loss: 0.51525, val loss: 0.50222\n",
      "Main effects training epoch: 169, train loss: 0.51514, val loss: 0.50331\n",
      "Main effects training epoch: 170, train loss: 0.51509, val loss: 0.50200\n",
      "Main effects training epoch: 171, train loss: 0.51490, val loss: 0.50324\n",
      "Main effects training epoch: 172, train loss: 0.51508, val loss: 0.50247\n",
      "Main effects training epoch: 173, train loss: 0.51493, val loss: 0.50182\n",
      "Main effects training epoch: 174, train loss: 0.51522, val loss: 0.50212\n",
      "Main effects training epoch: 175, train loss: 0.51508, val loss: 0.50334\n",
      "Main effects training epoch: 176, train loss: 0.51483, val loss: 0.50241\n",
      "Main effects training epoch: 177, train loss: 0.51504, val loss: 0.50039\n",
      "Main effects training epoch: 178, train loss: 0.51532, val loss: 0.50473\n",
      "Main effects training epoch: 179, train loss: 0.51527, val loss: 0.50142\n",
      "Main effects training epoch: 180, train loss: 0.51478, val loss: 0.50288\n",
      "Main effects training epoch: 181, train loss: 0.51468, val loss: 0.50177\n",
      "Main effects training epoch: 182, train loss: 0.51487, val loss: 0.50212\n",
      "Main effects training epoch: 183, train loss: 0.51473, val loss: 0.50130\n",
      "Main effects training epoch: 184, train loss: 0.51443, val loss: 0.50233\n",
      "Main effects training epoch: 185, train loss: 0.51443, val loss: 0.50209\n",
      "Main effects training epoch: 186, train loss: 0.51449, val loss: 0.50105\n",
      "Main effects training epoch: 187, train loss: 0.51500, val loss: 0.50328\n",
      "Main effects training epoch: 188, train loss: 0.51584, val loss: 0.50243\n",
      "Main effects training epoch: 189, train loss: 0.51498, val loss: 0.50282\n",
      "Main effects training epoch: 190, train loss: 0.51476, val loss: 0.50202\n",
      "Main effects training epoch: 191, train loss: 0.51499, val loss: 0.50370\n",
      "Main effects training epoch: 192, train loss: 0.51451, val loss: 0.50061\n",
      "Main effects training epoch: 193, train loss: 0.51427, val loss: 0.50163\n",
      "Main effects training epoch: 194, train loss: 0.51468, val loss: 0.50289\n",
      "Main effects training epoch: 195, train loss: 0.51485, val loss: 0.50152\n",
      "Early stop at epoch 195, with validation loss: 0.50152\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51652, val loss: 0.50334\n",
      "Main effects tuning epoch: 2, train loss: 0.51620, val loss: 0.50298\n",
      "Main effects tuning epoch: 3, train loss: 0.51591, val loss: 0.50243\n",
      "Main effects tuning epoch: 4, train loss: 0.51583, val loss: 0.50178\n",
      "Main effects tuning epoch: 5, train loss: 0.51587, val loss: 0.50310\n",
      "Main effects tuning epoch: 6, train loss: 0.51589, val loss: 0.50141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects tuning epoch: 7, train loss: 0.51573, val loss: 0.50312\n",
      "Main effects tuning epoch: 8, train loss: 0.51616, val loss: 0.50270\n",
      "Main effects tuning epoch: 9, train loss: 0.51592, val loss: 0.50204\n",
      "Main effects tuning epoch: 10, train loss: 0.51562, val loss: 0.50183\n",
      "Main effects tuning epoch: 11, train loss: 0.51574, val loss: 0.50319\n",
      "Main effects tuning epoch: 12, train loss: 0.51563, val loss: 0.50186\n",
      "Main effects tuning epoch: 13, train loss: 0.51576, val loss: 0.50178\n",
      "Main effects tuning epoch: 14, train loss: 0.51619, val loss: 0.50417\n",
      "Main effects tuning epoch: 15, train loss: 0.51630, val loss: 0.50174\n",
      "Main effects tuning epoch: 16, train loss: 0.51617, val loss: 0.50504\n",
      "Main effects tuning epoch: 17, train loss: 0.51578, val loss: 0.50114\n",
      "Main effects tuning epoch: 18, train loss: 0.51571, val loss: 0.50339\n",
      "Main effects tuning epoch: 19, train loss: 0.51541, val loss: 0.50297\n",
      "Main effects tuning epoch: 20, train loss: 0.51567, val loss: 0.50233\n",
      "Main effects tuning epoch: 21, train loss: 0.51545, val loss: 0.50194\n",
      "Main effects tuning epoch: 22, train loss: 0.51550, val loss: 0.50202\n",
      "Main effects tuning epoch: 23, train loss: 0.51527, val loss: 0.50183\n",
      "Main effects tuning epoch: 24, train loss: 0.51583, val loss: 0.50227\n",
      "Main effects tuning epoch: 25, train loss: 0.51612, val loss: 0.50429\n",
      "Main effects tuning epoch: 26, train loss: 0.51526, val loss: 0.50222\n",
      "Main effects tuning epoch: 27, train loss: 0.51592, val loss: 0.50259\n",
      "Main effects tuning epoch: 28, train loss: 0.51541, val loss: 0.50208\n",
      "Main effects tuning epoch: 29, train loss: 0.51598, val loss: 0.50367\n",
      "Main effects tuning epoch: 30, train loss: 0.51600, val loss: 0.50229\n",
      "Main effects tuning epoch: 31, train loss: 0.51550, val loss: 0.50323\n",
      "Main effects tuning epoch: 32, train loss: 0.51546, val loss: 0.50104\n",
      "Main effects tuning epoch: 33, train loss: 0.51535, val loss: 0.50236\n",
      "Main effects tuning epoch: 34, train loss: 0.51519, val loss: 0.50118\n",
      "Main effects tuning epoch: 35, train loss: 0.51491, val loss: 0.50257\n",
      "Main effects tuning epoch: 36, train loss: 0.51491, val loss: 0.50182\n",
      "Main effects tuning epoch: 37, train loss: 0.51516, val loss: 0.50071\n",
      "Main effects tuning epoch: 38, train loss: 0.51511, val loss: 0.50194\n",
      "Main effects tuning epoch: 39, train loss: 0.51511, val loss: 0.50230\n",
      "Main effects tuning epoch: 40, train loss: 0.51584, val loss: 0.50414\n",
      "Main effects tuning epoch: 41, train loss: 0.51531, val loss: 0.50065\n",
      "Main effects tuning epoch: 42, train loss: 0.51515, val loss: 0.50337\n",
      "Main effects tuning epoch: 43, train loss: 0.51471, val loss: 0.50169\n",
      "Main effects tuning epoch: 44, train loss: 0.51484, val loss: 0.50097\n",
      "Main effects tuning epoch: 45, train loss: 0.51488, val loss: 0.50235\n",
      "Main effects tuning epoch: 46, train loss: 0.51479, val loss: 0.50102\n",
      "Main effects tuning epoch: 47, train loss: 0.51487, val loss: 0.50196\n",
      "Main effects tuning epoch: 48, train loss: 0.51463, val loss: 0.50183\n",
      "Main effects tuning epoch: 49, train loss: 0.51469, val loss: 0.50103\n",
      "Main effects tuning epoch: 50, train loss: 0.51472, val loss: 0.50134\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.49630, val loss: 0.48952\n",
      "Interaction training epoch: 2, train loss: 0.33769, val loss: 0.34284\n",
      "Interaction training epoch: 3, train loss: 0.35128, val loss: 0.36496\n",
      "Interaction training epoch: 4, train loss: 0.31659, val loss: 0.31842\n",
      "Interaction training epoch: 5, train loss: 0.30166, val loss: 0.30688\n",
      "Interaction training epoch: 6, train loss: 0.29518, val loss: 0.29846\n",
      "Interaction training epoch: 7, train loss: 0.28798, val loss: 0.28992\n",
      "Interaction training epoch: 8, train loss: 0.28937, val loss: 0.29121\n",
      "Interaction training epoch: 9, train loss: 0.29272, val loss: 0.29817\n",
      "Interaction training epoch: 10, train loss: 0.29134, val loss: 0.29788\n",
      "Interaction training epoch: 11, train loss: 0.28068, val loss: 0.28755\n",
      "Interaction training epoch: 12, train loss: 0.28154, val loss: 0.28695\n",
      "Interaction training epoch: 13, train loss: 0.27857, val loss: 0.28611\n",
      "Interaction training epoch: 14, train loss: 0.28018, val loss: 0.28605\n",
      "Interaction training epoch: 15, train loss: 0.27761, val loss: 0.28852\n",
      "Interaction training epoch: 16, train loss: 0.27799, val loss: 0.28495\n",
      "Interaction training epoch: 17, train loss: 0.27600, val loss: 0.28684\n",
      "Interaction training epoch: 18, train loss: 0.27797, val loss: 0.28446\n",
      "Interaction training epoch: 19, train loss: 0.27427, val loss: 0.28254\n",
      "Interaction training epoch: 20, train loss: 0.27444, val loss: 0.28269\n",
      "Interaction training epoch: 21, train loss: 0.27197, val loss: 0.28017\n",
      "Interaction training epoch: 22, train loss: 0.28111, val loss: 0.29098\n",
      "Interaction training epoch: 23, train loss: 0.27471, val loss: 0.28461\n",
      "Interaction training epoch: 24, train loss: 0.27383, val loss: 0.28217\n",
      "Interaction training epoch: 25, train loss: 0.27561, val loss: 0.28093\n",
      "Interaction training epoch: 26, train loss: 0.27286, val loss: 0.28056\n",
      "Interaction training epoch: 27, train loss: 0.27501, val loss: 0.28302\n",
      "Interaction training epoch: 28, train loss: 0.27465, val loss: 0.28525\n",
      "Interaction training epoch: 29, train loss: 0.27005, val loss: 0.27613\n",
      "Interaction training epoch: 30, train loss: 0.27126, val loss: 0.27893\n",
      "Interaction training epoch: 31, train loss: 0.27173, val loss: 0.28095\n",
      "Interaction training epoch: 32, train loss: 0.27099, val loss: 0.28101\n",
      "Interaction training epoch: 33, train loss: 0.26687, val loss: 0.27472\n",
      "Interaction training epoch: 34, train loss: 0.26881, val loss: 0.27762\n",
      "Interaction training epoch: 35, train loss: 0.26911, val loss: 0.28276\n",
      "Interaction training epoch: 36, train loss: 0.27233, val loss: 0.28279\n",
      "Interaction training epoch: 37, train loss: 0.27192, val loss: 0.28092\n",
      "Interaction training epoch: 38, train loss: 0.26677, val loss: 0.27617\n",
      "Interaction training epoch: 39, train loss: 0.26946, val loss: 0.27759\n",
      "Interaction training epoch: 40, train loss: 0.26705, val loss: 0.28085\n",
      "Interaction training epoch: 41, train loss: 0.26524, val loss: 0.27444\n",
      "Interaction training epoch: 42, train loss: 0.27260, val loss: 0.28364\n",
      "Interaction training epoch: 43, train loss: 0.26587, val loss: 0.27973\n",
      "Interaction training epoch: 44, train loss: 0.26654, val loss: 0.27690\n",
      "Interaction training epoch: 45, train loss: 0.26566, val loss: 0.28007\n",
      "Interaction training epoch: 46, train loss: 0.26457, val loss: 0.27688\n",
      "Interaction training epoch: 47, train loss: 0.26778, val loss: 0.28093\n",
      "Interaction training epoch: 48, train loss: 0.26713, val loss: 0.28041\n",
      "Interaction training epoch: 49, train loss: 0.26385, val loss: 0.27635\n",
      "Interaction training epoch: 50, train loss: 0.26436, val loss: 0.27912\n",
      "Interaction training epoch: 51, train loss: 0.26506, val loss: 0.27701\n",
      "Interaction training epoch: 52, train loss: 0.26332, val loss: 0.27839\n",
      "Interaction training epoch: 53, train loss: 0.26575, val loss: 0.27820\n",
      "Interaction training epoch: 54, train loss: 0.26444, val loss: 0.27399\n",
      "Interaction training epoch: 55, train loss: 0.26446, val loss: 0.28050\n",
      "Interaction training epoch: 56, train loss: 0.26212, val loss: 0.27389\n",
      "Interaction training epoch: 57, train loss: 0.26537, val loss: 0.28100\n",
      "Interaction training epoch: 58, train loss: 0.26231, val loss: 0.27333\n",
      "Interaction training epoch: 59, train loss: 0.26531, val loss: 0.28369\n",
      "Interaction training epoch: 60, train loss: 0.26223, val loss: 0.27409\n",
      "Interaction training epoch: 61, train loss: 0.26070, val loss: 0.27585\n",
      "Interaction training epoch: 62, train loss: 0.26415, val loss: 0.28179\n",
      "Interaction training epoch: 63, train loss: 0.26012, val loss: 0.27531\n",
      "Interaction training epoch: 64, train loss: 0.26214, val loss: 0.27596\n",
      "Interaction training epoch: 65, train loss: 0.26123, val loss: 0.27634\n",
      "Interaction training epoch: 66, train loss: 0.26041, val loss: 0.27562\n",
      "Interaction training epoch: 67, train loss: 0.26073, val loss: 0.27566\n",
      "Interaction training epoch: 68, train loss: 0.26038, val loss: 0.27691\n",
      "Interaction training epoch: 69, train loss: 0.25848, val loss: 0.27646\n",
      "Interaction training epoch: 70, train loss: 0.26296, val loss: 0.27763\n",
      "Interaction training epoch: 71, train loss: 0.26102, val loss: 0.27864\n",
      "Interaction training epoch: 72, train loss: 0.26107, val loss: 0.27708\n",
      "Interaction training epoch: 73, train loss: 0.26414, val loss: 0.27619\n",
      "Interaction training epoch: 74, train loss: 0.25864, val loss: 0.27725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 75, train loss: 0.25926, val loss: 0.27525\n",
      "Interaction training epoch: 76, train loss: 0.25686, val loss: 0.27432\n",
      "Interaction training epoch: 77, train loss: 0.25954, val loss: 0.27993\n",
      "Interaction training epoch: 78, train loss: 0.26144, val loss: 0.27545\n",
      "Interaction training epoch: 79, train loss: 0.25751, val loss: 0.27693\n",
      "Interaction training epoch: 80, train loss: 0.25861, val loss: 0.27448\n",
      "Interaction training epoch: 81, train loss: 0.25719, val loss: 0.27631\n",
      "Interaction training epoch: 82, train loss: 0.26048, val loss: 0.27937\n",
      "Interaction training epoch: 83, train loss: 0.25757, val loss: 0.27424\n",
      "Interaction training epoch: 84, train loss: 0.25932, val loss: 0.27815\n",
      "Interaction training epoch: 85, train loss: 0.26140, val loss: 0.27858\n",
      "Interaction training epoch: 86, train loss: 0.25378, val loss: 0.27194\n",
      "Interaction training epoch: 87, train loss: 0.25811, val loss: 0.27779\n",
      "Interaction training epoch: 88, train loss: 0.25630, val loss: 0.27527\n",
      "Interaction training epoch: 89, train loss: 0.25527, val loss: 0.27723\n",
      "Interaction training epoch: 90, train loss: 0.25759, val loss: 0.27676\n",
      "Interaction training epoch: 91, train loss: 0.25457, val loss: 0.27467\n",
      "Interaction training epoch: 92, train loss: 0.25542, val loss: 0.27403\n",
      "Interaction training epoch: 93, train loss: 0.25535, val loss: 0.27589\n",
      "Interaction training epoch: 94, train loss: 0.25407, val loss: 0.27538\n",
      "Interaction training epoch: 95, train loss: 0.25767, val loss: 0.27769\n",
      "Interaction training epoch: 96, train loss: 0.25220, val loss: 0.27444\n",
      "Interaction training epoch: 97, train loss: 0.25584, val loss: 0.27263\n",
      "Interaction training epoch: 98, train loss: 0.25244, val loss: 0.27405\n",
      "Interaction training epoch: 99, train loss: 0.25535, val loss: 0.27697\n",
      "Interaction training epoch: 100, train loss: 0.25545, val loss: 0.27934\n",
      "Interaction training epoch: 101, train loss: 0.25633, val loss: 0.27754\n",
      "Interaction training epoch: 102, train loss: 0.25267, val loss: 0.27028\n",
      "Interaction training epoch: 103, train loss: 0.25493, val loss: 0.27683\n",
      "Interaction training epoch: 104, train loss: 0.25262, val loss: 0.27466\n",
      "Interaction training epoch: 105, train loss: 0.25030, val loss: 0.27110\n",
      "Interaction training epoch: 106, train loss: 0.25358, val loss: 0.27660\n",
      "Interaction training epoch: 107, train loss: 0.25603, val loss: 0.27754\n",
      "Interaction training epoch: 108, train loss: 0.24978, val loss: 0.27290\n",
      "Interaction training epoch: 109, train loss: 0.25282, val loss: 0.27685\n",
      "Interaction training epoch: 110, train loss: 0.25069, val loss: 0.27038\n",
      "Interaction training epoch: 111, train loss: 0.25313, val loss: 0.27830\n",
      "Interaction training epoch: 112, train loss: 0.25039, val loss: 0.27405\n",
      "Interaction training epoch: 113, train loss: 0.25058, val loss: 0.27310\n",
      "Interaction training epoch: 114, train loss: 0.25279, val loss: 0.27999\n",
      "Interaction training epoch: 115, train loss: 0.24922, val loss: 0.27298\n",
      "Interaction training epoch: 116, train loss: 0.25238, val loss: 0.28017\n",
      "Interaction training epoch: 117, train loss: 0.24869, val loss: 0.27323\n",
      "Interaction training epoch: 118, train loss: 0.24943, val loss: 0.27615\n",
      "Interaction training epoch: 119, train loss: 0.24886, val loss: 0.27248\n",
      "Interaction training epoch: 120, train loss: 0.24706, val loss: 0.27312\n",
      "Interaction training epoch: 121, train loss: 0.24960, val loss: 0.27178\n",
      "Interaction training epoch: 122, train loss: 0.24969, val loss: 0.27520\n",
      "Interaction training epoch: 123, train loss: 0.24595, val loss: 0.27154\n",
      "Interaction training epoch: 124, train loss: 0.24900, val loss: 0.27416\n",
      "Interaction training epoch: 125, train loss: 0.24952, val loss: 0.27728\n",
      "Interaction training epoch: 126, train loss: 0.24514, val loss: 0.27059\n",
      "Interaction training epoch: 127, train loss: 0.24724, val loss: 0.27257\n",
      "Interaction training epoch: 128, train loss: 0.24934, val loss: 0.27673\n",
      "Interaction training epoch: 129, train loss: 0.25057, val loss: 0.27925\n",
      "Interaction training epoch: 130, train loss: 0.24874, val loss: 0.27362\n",
      "Interaction training epoch: 131, train loss: 0.24820, val loss: 0.27789\n",
      "Interaction training epoch: 132, train loss: 0.24936, val loss: 0.27270\n",
      "Interaction training epoch: 133, train loss: 0.24469, val loss: 0.27186\n",
      "Interaction training epoch: 134, train loss: 0.24571, val loss: 0.27370\n",
      "Interaction training epoch: 135, train loss: 0.24431, val loss: 0.27322\n",
      "Interaction training epoch: 136, train loss: 0.24633, val loss: 0.27261\n",
      "Interaction training epoch: 137, train loss: 0.24825, val loss: 0.27969\n",
      "Interaction training epoch: 138, train loss: 0.24335, val loss: 0.27059\n",
      "Interaction training epoch: 139, train loss: 0.24851, val loss: 0.27798\n",
      "Interaction training epoch: 140, train loss: 0.24583, val loss: 0.27031\n",
      "Interaction training epoch: 141, train loss: 0.24941, val loss: 0.28354\n",
      "Interaction training epoch: 142, train loss: 0.24411, val loss: 0.27392\n",
      "Interaction training epoch: 143, train loss: 0.24626, val loss: 0.27780\n",
      "Interaction training epoch: 144, train loss: 0.24308, val loss: 0.27648\n",
      "Interaction training epoch: 145, train loss: 0.24347, val loss: 0.27154\n",
      "Interaction training epoch: 146, train loss: 0.24461, val loss: 0.27730\n",
      "Interaction training epoch: 147, train loss: 0.24531, val loss: 0.27648\n",
      "Interaction training epoch: 148, train loss: 0.24362, val loss: 0.27834\n",
      "Interaction training epoch: 149, train loss: 0.24601, val loss: 0.27461\n",
      "Interaction training epoch: 150, train loss: 0.24428, val loss: 0.27347\n",
      "Interaction training epoch: 151, train loss: 0.24732, val loss: 0.28053\n",
      "Interaction training epoch: 152, train loss: 0.24191, val loss: 0.27360\n",
      "Interaction training epoch: 153, train loss: 0.24221, val loss: 0.27440\n",
      "Interaction training epoch: 154, train loss: 0.24280, val loss: 0.27499\n",
      "Interaction training epoch: 155, train loss: 0.24201, val loss: 0.27586\n",
      "Interaction training epoch: 156, train loss: 0.24477, val loss: 0.27752\n",
      "Interaction training epoch: 157, train loss: 0.24507, val loss: 0.27833\n",
      "Interaction training epoch: 158, train loss: 0.24068, val loss: 0.27333\n",
      "Interaction training epoch: 159, train loss: 0.24391, val loss: 0.27683\n",
      "Interaction training epoch: 160, train loss: 0.24175, val loss: 0.27674\n",
      "Interaction training epoch: 161, train loss: 0.23881, val loss: 0.26956\n",
      "Interaction training epoch: 162, train loss: 0.24359, val loss: 0.27676\n",
      "Interaction training epoch: 163, train loss: 0.23898, val loss: 0.27401\n",
      "Interaction training epoch: 164, train loss: 0.23981, val loss: 0.27359\n",
      "Interaction training epoch: 165, train loss: 0.24112, val loss: 0.27625\n",
      "Interaction training epoch: 166, train loss: 0.23941, val loss: 0.27442\n",
      "Interaction training epoch: 167, train loss: 0.24063, val loss: 0.27651\n",
      "Interaction training epoch: 168, train loss: 0.24010, val loss: 0.27885\n",
      "Interaction training epoch: 169, train loss: 0.24168, val loss: 0.27591\n",
      "Interaction training epoch: 170, train loss: 0.24327, val loss: 0.28124\n",
      "Interaction training epoch: 171, train loss: 0.23941, val loss: 0.27438\n",
      "Interaction training epoch: 172, train loss: 0.24055, val loss: 0.27595\n",
      "Interaction training epoch: 173, train loss: 0.23916, val loss: 0.27534\n",
      "Interaction training epoch: 174, train loss: 0.24062, val loss: 0.28139\n",
      "Interaction training epoch: 175, train loss: 0.23799, val loss: 0.27390\n",
      "Interaction training epoch: 176, train loss: 0.23947, val loss: 0.27684\n",
      "Interaction training epoch: 177, train loss: 0.23886, val loss: 0.27735\n",
      "Interaction training epoch: 178, train loss: 0.23895, val loss: 0.27472\n",
      "Interaction training epoch: 179, train loss: 0.23942, val loss: 0.27651\n",
      "Interaction training epoch: 180, train loss: 0.24049, val loss: 0.28036\n",
      "Interaction training epoch: 181, train loss: 0.23897, val loss: 0.27791\n",
      "Interaction training epoch: 182, train loss: 0.23646, val loss: 0.27363\n",
      "Interaction training epoch: 183, train loss: 0.23810, val loss: 0.27910\n",
      "Interaction training epoch: 184, train loss: 0.23821, val loss: 0.27539\n",
      "Interaction training epoch: 185, train loss: 0.23642, val loss: 0.27411\n",
      "Interaction training epoch: 186, train loss: 0.23809, val loss: 0.27799\n",
      "Interaction training epoch: 187, train loss: 0.23796, val loss: 0.27475\n",
      "Interaction training epoch: 188, train loss: 0.23539, val loss: 0.27566\n",
      "Interaction training epoch: 189, train loss: 0.23735, val loss: 0.27996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 190, train loss: 0.23828, val loss: 0.27894\n",
      "Interaction training epoch: 191, train loss: 0.23465, val loss: 0.27510\n",
      "Interaction training epoch: 192, train loss: 0.23838, val loss: 0.27980\n",
      "Interaction training epoch: 193, train loss: 0.23741, val loss: 0.27565\n",
      "Interaction training epoch: 194, train loss: 0.23572, val loss: 0.27452\n",
      "Interaction training epoch: 195, train loss: 0.23802, val loss: 0.28027\n",
      "Interaction training epoch: 196, train loss: 0.23539, val loss: 0.27689\n",
      "Interaction training epoch: 197, train loss: 0.23723, val loss: 0.27746\n",
      "Interaction training epoch: 198, train loss: 0.23395, val loss: 0.27462\n",
      "Interaction training epoch: 199, train loss: 0.23661, val loss: 0.28002\n",
      "Interaction training epoch: 200, train loss: 0.23513, val loss: 0.27434\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########3 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.24968, val loss: 0.27903\n",
      "Interaction tuning epoch: 2, train loss: 0.24776, val loss: 0.28053\n",
      "Interaction tuning epoch: 3, train loss: 0.24547, val loss: 0.27095\n",
      "Interaction tuning epoch: 4, train loss: 0.24990, val loss: 0.28140\n",
      "Interaction tuning epoch: 5, train loss: 0.24715, val loss: 0.27506\n",
      "Interaction tuning epoch: 6, train loss: 0.24635, val loss: 0.27332\n",
      "Interaction tuning epoch: 7, train loss: 0.24591, val loss: 0.27750\n",
      "Interaction tuning epoch: 8, train loss: 0.24374, val loss: 0.27154\n",
      "Interaction tuning epoch: 9, train loss: 0.24867, val loss: 0.27508\n",
      "Interaction tuning epoch: 10, train loss: 0.24609, val loss: 0.27569\n",
      "Interaction tuning epoch: 11, train loss: 0.24703, val loss: 0.27608\n",
      "Interaction tuning epoch: 12, train loss: 0.24513, val loss: 0.27373\n",
      "Interaction tuning epoch: 13, train loss: 0.24529, val loss: 0.27530\n",
      "Interaction tuning epoch: 14, train loss: 0.24515, val loss: 0.27229\n",
      "Interaction tuning epoch: 15, train loss: 0.24666, val loss: 0.27752\n",
      "Interaction tuning epoch: 16, train loss: 0.24396, val loss: 0.27377\n",
      "Interaction tuning epoch: 17, train loss: 0.24588, val loss: 0.27324\n",
      "Interaction tuning epoch: 18, train loss: 0.24525, val loss: 0.27665\n",
      "Interaction tuning epoch: 19, train loss: 0.24285, val loss: 0.27234\n",
      "Interaction tuning epoch: 20, train loss: 0.24430, val loss: 0.27507\n",
      "Interaction tuning epoch: 21, train loss: 0.24167, val loss: 0.27076\n",
      "Interaction tuning epoch: 22, train loss: 0.24412, val loss: 0.27322\n",
      "Interaction tuning epoch: 23, train loss: 0.24488, val loss: 0.27656\n",
      "Interaction tuning epoch: 24, train loss: 0.24215, val loss: 0.27113\n",
      "Interaction tuning epoch: 25, train loss: 0.24397, val loss: 0.27656\n",
      "Interaction tuning epoch: 26, train loss: 0.24370, val loss: 0.27526\n",
      "Interaction tuning epoch: 27, train loss: 0.24330, val loss: 0.27308\n",
      "Interaction tuning epoch: 28, train loss: 0.24366, val loss: 0.27189\n",
      "Interaction tuning epoch: 29, train loss: 0.24133, val loss: 0.27163\n",
      "Interaction tuning epoch: 30, train loss: 0.24330, val loss: 0.27379\n",
      "Interaction tuning epoch: 31, train loss: 0.24268, val loss: 0.27507\n",
      "Interaction tuning epoch: 32, train loss: 0.24475, val loss: 0.27470\n",
      "Interaction tuning epoch: 33, train loss: 0.24434, val loss: 0.27429\n",
      "Interaction tuning epoch: 34, train loss: 0.24265, val loss: 0.27108\n",
      "Interaction tuning epoch: 35, train loss: 0.24270, val loss: 0.27413\n",
      "Interaction tuning epoch: 36, train loss: 0.24404, val loss: 0.27374\n",
      "Interaction tuning epoch: 37, train loss: 0.24276, val loss: 0.27324\n",
      "Interaction tuning epoch: 38, train loss: 0.24406, val loss: 0.27388\n",
      "Interaction tuning epoch: 39, train loss: 0.24261, val loss: 0.27577\n",
      "Interaction tuning epoch: 40, train loss: 0.24088, val loss: 0.27160\n",
      "Interaction tuning epoch: 41, train loss: 0.24276, val loss: 0.27220\n",
      "Interaction tuning epoch: 42, train loss: 0.24198, val loss: 0.27404\n",
      "Interaction tuning epoch: 43, train loss: 0.24426, val loss: 0.27793\n",
      "Interaction tuning epoch: 44, train loss: 0.24072, val loss: 0.27060\n",
      "Interaction tuning epoch: 45, train loss: 0.24021, val loss: 0.27217\n",
      "Interaction tuning epoch: 46, train loss: 0.24042, val loss: 0.27170\n",
      "Interaction tuning epoch: 47, train loss: 0.24126, val loss: 0.27376\n",
      "Interaction tuning epoch: 48, train loss: 0.24628, val loss: 0.27823\n",
      "Interaction tuning epoch: 49, train loss: 0.24267, val loss: 0.26952\n",
      "Interaction tuning epoch: 50, train loss: 0.24486, val loss: 0.27854\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 38.63367819786072\n",
      "After the gam stage, training error is 0.24486 , validation error is 0.27854\n",
      "missing value counts: 99148\n",
      "[SoftImpute] Max Singular Value of X_init = 3.614024\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed BCE=0.210378 validation BCE=0.301129,rank=5\n",
      "[SoftImpute] Iter 2: observed BCE=0.206759 validation BCE=0.300241,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 3: observed BCE=0.204796 validation BCE=0.298731,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.203020 validation BCE=0.298448,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.201839 validation BCE=0.287451,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.200906 validation BCE=0.287448,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.200322 validation BCE=0.284549,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.199860 validation BCE=0.286653,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.199396 validation BCE=0.282869,rank=5\n",
      "[SoftImpute] Iter 10: observed BCE=0.198967 validation BCE=0.285297,rank=5\n",
      "[SoftImpute] Iter 11: observed BCE=0.198917 validation BCE=0.281769,rank=5\n",
      "[SoftImpute] Iter 12: observed BCE=0.198694 validation BCE=0.281508,rank=5\n",
      "[SoftImpute] Iter 13: observed BCE=0.198459 validation BCE=0.281685,rank=5\n",
      "[SoftImpute] Iter 14: observed BCE=0.199110 validation BCE=0.280893,rank=5\n",
      "[SoftImpute] Iter 15: observed BCE=0.199809 validation BCE=0.280142,rank=5\n",
      "[SoftImpute] Iter 16: observed BCE=0.200263 validation BCE=0.280590,rank=5\n",
      "[SoftImpute] Iter 17: observed BCE=0.200475 validation BCE=0.279902,rank=5\n",
      "[SoftImpute] Iter 18: observed BCE=0.199939 validation BCE=0.279429,rank=5\n",
      "[SoftImpute] Iter 19: observed BCE=0.200246 validation BCE=0.279864,rank=5\n",
      "[SoftImpute] Iter 20: observed BCE=0.200123 validation BCE=0.279228,rank=5\n",
      "[SoftImpute] Iter 21: observed BCE=0.200459 validation BCE=0.278858,rank=5\n",
      "[SoftImpute] Iter 22: observed BCE=0.200964 validation BCE=0.279182,rank=5\n",
      "[SoftImpute] Iter 23: observed BCE=0.201314 validation BCE=0.278525,rank=5\n",
      "[SoftImpute] Iter 24: observed BCE=0.201496 validation BCE=0.278243,rank=5\n",
      "[SoftImpute] Iter 25: observed BCE=0.201735 validation BCE=0.278712,rank=5\n",
      "[SoftImpute] Iter 26: observed BCE=0.201411 validation BCE=0.277947,rank=5\n",
      "[SoftImpute] Iter 27: observed BCE=0.201526 validation BCE=0.277716,rank=5\n",
      "[SoftImpute] Iter 28: observed BCE=0.201614 validation BCE=0.278250,rank=5\n",
      "[SoftImpute] Iter 29: observed BCE=0.201528 validation BCE=0.277580,rank=5\n",
      "[SoftImpute] Iter 30: observed BCE=0.201514 validation BCE=0.277288,rank=5\n",
      "[SoftImpute] Iter 31: observed BCE=0.201369 validation BCE=0.277313,rank=5\n",
      "[SoftImpute] Iter 32: observed BCE=0.201377 validation BCE=0.277092,rank=5\n",
      "[SoftImpute] Iter 33: observed BCE=0.201410 validation BCE=0.277143,rank=5\n",
      "[SoftImpute] Iter 34: observed BCE=0.201501 validation BCE=0.276923,rank=5\n",
      "[SoftImpute] Iter 35: observed BCE=0.201256 validation BCE=0.276790,rank=5\n",
      "[SoftImpute] Iter 36: observed BCE=0.201230 validation BCE=0.276696,rank=5\n",
      "[SoftImpute] Iter 37: observed BCE=0.201161 validation BCE=0.276516,rank=5\n",
      "[SoftImpute] Iter 38: observed BCE=0.201151 validation BCE=0.276627,rank=5\n",
      "[SoftImpute] Iter 39: observed BCE=0.201017 validation BCE=0.276479,rank=5\n",
      "[SoftImpute] Iter 40: observed BCE=0.201274 validation BCE=0.276531,rank=5\n",
      "[SoftImpute] Iter 41: observed BCE=0.201091 validation BCE=0.276624,rank=5\n",
      "[SoftImpute] Iter 42: observed BCE=0.201116 validation BCE=0.276186,rank=5\n",
      "[SoftImpute] Iter 43: observed BCE=0.201131 validation BCE=0.276378,rank=5\n",
      "[SoftImpute] Iter 44: observed BCE=0.200860 validation BCE=0.276196,rank=5\n",
      "[SoftImpute] Iter 45: observed BCE=0.200794 validation BCE=0.276130,rank=5\n",
      "[SoftImpute] Iter 46: observed BCE=0.201302 validation BCE=0.275975,rank=5\n",
      "[SoftImpute] Iter 47: observed BCE=0.201134 validation BCE=0.276209,rank=5\n",
      "[SoftImpute] Iter 48: observed BCE=0.200975 validation BCE=0.276073,rank=5\n",
      "[SoftImpute] Iter 49: observed BCE=0.200995 validation BCE=0.276468,rank=5\n",
      "[SoftImpute] Iter 50: observed BCE=0.201036 validation BCE=0.276093,rank=5\n",
      "[SoftImpute] Iter 51: observed BCE=0.200916 validation BCE=0.276176,rank=5\n",
      "[SoftImpute] Iter 52: observed BCE=0.201044 validation BCE=0.275939,rank=5\n",
      "[SoftImpute] Iter 53: observed BCE=0.201029 validation BCE=0.275972,rank=5\n",
      "[SoftImpute] Iter 54: observed BCE=0.201120 validation BCE=0.276238,rank=5\n",
      "[SoftImpute] Iter 55: observed BCE=0.201048 validation BCE=0.276244,rank=5\n",
      "[SoftImpute] Stopped after iteration 55 for lambda=0.072280\n",
      "final num of user group: 6\n",
      "final num of item group: 6\n",
      "change mode state : True\n",
      "time cost: 3.4654479026794434\n",
      "After the matrix factor stage, training error is 0.20105, validation error is 0.27624\n",
      "1\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68114, val loss: 0.68341\n",
      "Main effects training epoch: 2, train loss: 0.67669, val loss: 0.68117\n",
      "Main effects training epoch: 3, train loss: 0.67284, val loss: 0.67625\n",
      "Main effects training epoch: 4, train loss: 0.66760, val loss: 0.66910\n",
      "Main effects training epoch: 5, train loss: 0.66018, val loss: 0.66127\n",
      "Main effects training epoch: 6, train loss: 0.64498, val loss: 0.64545\n",
      "Main effects training epoch: 7, train loss: 0.61063, val loss: 0.60630\n",
      "Main effects training epoch: 8, train loss: 0.55974, val loss: 0.54802\n",
      "Main effects training epoch: 9, train loss: 0.52862, val loss: 0.50747\n",
      "Main effects training epoch: 10, train loss: 0.52943, val loss: 0.50795\n",
      "Main effects training epoch: 11, train loss: 0.52601, val loss: 0.50426\n",
      "Main effects training epoch: 12, train loss: 0.52343, val loss: 0.50388\n",
      "Main effects training epoch: 13, train loss: 0.52320, val loss: 0.50493\n",
      "Main effects training epoch: 14, train loss: 0.52318, val loss: 0.50353\n",
      "Main effects training epoch: 15, train loss: 0.52411, val loss: 0.50476\n",
      "Main effects training epoch: 16, train loss: 0.52283, val loss: 0.50148\n",
      "Main effects training epoch: 17, train loss: 0.52240, val loss: 0.50251\n",
      "Main effects training epoch: 18, train loss: 0.52247, val loss: 0.50230\n",
      "Main effects training epoch: 19, train loss: 0.52266, val loss: 0.50339\n",
      "Main effects training epoch: 20, train loss: 0.52292, val loss: 0.50205\n",
      "Main effects training epoch: 21, train loss: 0.52326, val loss: 0.50466\n",
      "Main effects training epoch: 22, train loss: 0.52324, val loss: 0.50277\n",
      "Main effects training epoch: 23, train loss: 0.52236, val loss: 0.50176\n",
      "Main effects training epoch: 24, train loss: 0.52228, val loss: 0.50140\n",
      "Main effects training epoch: 25, train loss: 0.52186, val loss: 0.50202\n",
      "Main effects training epoch: 26, train loss: 0.52177, val loss: 0.50129\n",
      "Main effects training epoch: 27, train loss: 0.52184, val loss: 0.50241\n",
      "Main effects training epoch: 28, train loss: 0.52180, val loss: 0.50198\n",
      "Main effects training epoch: 29, train loss: 0.52217, val loss: 0.50221\n",
      "Main effects training epoch: 30, train loss: 0.52186, val loss: 0.50169\n",
      "Main effects training epoch: 31, train loss: 0.52238, val loss: 0.50373\n",
      "Main effects training epoch: 32, train loss: 0.52156, val loss: 0.50193\n",
      "Main effects training epoch: 33, train loss: 0.52160, val loss: 0.50105\n",
      "Main effects training epoch: 34, train loss: 0.52215, val loss: 0.50291\n",
      "Main effects training epoch: 35, train loss: 0.52201, val loss: 0.50317\n",
      "Main effects training epoch: 36, train loss: 0.52171, val loss: 0.50161\n",
      "Main effects training epoch: 37, train loss: 0.52121, val loss: 0.50251\n",
      "Main effects training epoch: 38, train loss: 0.52118, val loss: 0.50188\n",
      "Main effects training epoch: 39, train loss: 0.52151, val loss: 0.50244\n",
      "Main effects training epoch: 40, train loss: 0.52164, val loss: 0.50348\n",
      "Main effects training epoch: 41, train loss: 0.52157, val loss: 0.50287\n",
      "Main effects training epoch: 42, train loss: 0.52140, val loss: 0.50129\n",
      "Main effects training epoch: 43, train loss: 0.52090, val loss: 0.50180\n",
      "Main effects training epoch: 44, train loss: 0.52133, val loss: 0.50263\n",
      "Main effects training epoch: 45, train loss: 0.52140, val loss: 0.50305\n",
      "Main effects training epoch: 46, train loss: 0.52136, val loss: 0.50207\n",
      "Main effects training epoch: 47, train loss: 0.52078, val loss: 0.50198\n",
      "Main effects training epoch: 48, train loss: 0.52098, val loss: 0.50195\n",
      "Main effects training epoch: 49, train loss: 0.52122, val loss: 0.50260\n",
      "Main effects training epoch: 50, train loss: 0.52100, val loss: 0.50226\n",
      "Main effects training epoch: 51, train loss: 0.52072, val loss: 0.50166\n",
      "Main effects training epoch: 52, train loss: 0.52076, val loss: 0.50272\n",
      "Main effects training epoch: 53, train loss: 0.52098, val loss: 0.50256\n",
      "Main effects training epoch: 54, train loss: 0.52124, val loss: 0.50189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 55, train loss: 0.52107, val loss: 0.50265\n",
      "Main effects training epoch: 56, train loss: 0.52128, val loss: 0.50357\n",
      "Main effects training epoch: 57, train loss: 0.52037, val loss: 0.50124\n",
      "Main effects training epoch: 58, train loss: 0.52039, val loss: 0.50179\n",
      "Main effects training epoch: 59, train loss: 0.52067, val loss: 0.50169\n",
      "Main effects training epoch: 60, train loss: 0.52055, val loss: 0.50253\n",
      "Main effects training epoch: 61, train loss: 0.52035, val loss: 0.50178\n",
      "Main effects training epoch: 62, train loss: 0.52045, val loss: 0.50238\n",
      "Main effects training epoch: 63, train loss: 0.52096, val loss: 0.50167\n",
      "Main effects training epoch: 64, train loss: 0.52237, val loss: 0.50556\n",
      "Main effects training epoch: 65, train loss: 0.52123, val loss: 0.50283\n",
      "Main effects training epoch: 66, train loss: 0.52058, val loss: 0.50341\n",
      "Main effects training epoch: 67, train loss: 0.52020, val loss: 0.50140\n",
      "Main effects training epoch: 68, train loss: 0.52010, val loss: 0.50159\n",
      "Main effects training epoch: 69, train loss: 0.52095, val loss: 0.50306\n",
      "Main effects training epoch: 70, train loss: 0.52017, val loss: 0.50164\n",
      "Main effects training epoch: 71, train loss: 0.51995, val loss: 0.50263\n",
      "Main effects training epoch: 72, train loss: 0.52008, val loss: 0.50033\n",
      "Main effects training epoch: 73, train loss: 0.51996, val loss: 0.50241\n",
      "Main effects training epoch: 74, train loss: 0.52001, val loss: 0.50140\n",
      "Main effects training epoch: 75, train loss: 0.51977, val loss: 0.50198\n",
      "Main effects training epoch: 76, train loss: 0.51960, val loss: 0.50061\n",
      "Main effects training epoch: 77, train loss: 0.51966, val loss: 0.50179\n",
      "Main effects training epoch: 78, train loss: 0.51957, val loss: 0.50043\n",
      "Main effects training epoch: 79, train loss: 0.51961, val loss: 0.50252\n",
      "Main effects training epoch: 80, train loss: 0.51965, val loss: 0.50120\n",
      "Main effects training epoch: 81, train loss: 0.51977, val loss: 0.50108\n",
      "Main effects training epoch: 82, train loss: 0.51934, val loss: 0.50186\n",
      "Main effects training epoch: 83, train loss: 0.51926, val loss: 0.50084\n",
      "Main effects training epoch: 84, train loss: 0.51930, val loss: 0.50066\n",
      "Main effects training epoch: 85, train loss: 0.51938, val loss: 0.50147\n",
      "Main effects training epoch: 86, train loss: 0.51917, val loss: 0.50169\n",
      "Main effects training epoch: 87, train loss: 0.51907, val loss: 0.50095\n",
      "Main effects training epoch: 88, train loss: 0.51900, val loss: 0.50127\n",
      "Main effects training epoch: 89, train loss: 0.51900, val loss: 0.50155\n",
      "Main effects training epoch: 90, train loss: 0.51880, val loss: 0.50049\n",
      "Main effects training epoch: 91, train loss: 0.51864, val loss: 0.50113\n",
      "Main effects training epoch: 92, train loss: 0.51887, val loss: 0.50240\n",
      "Main effects training epoch: 93, train loss: 0.51900, val loss: 0.50123\n",
      "Main effects training epoch: 94, train loss: 0.51868, val loss: 0.50055\n",
      "Main effects training epoch: 95, train loss: 0.51880, val loss: 0.50086\n",
      "Main effects training epoch: 96, train loss: 0.51891, val loss: 0.50235\n",
      "Main effects training epoch: 97, train loss: 0.51852, val loss: 0.50098\n",
      "Main effects training epoch: 98, train loss: 0.51842, val loss: 0.50091\n",
      "Main effects training epoch: 99, train loss: 0.51922, val loss: 0.50286\n",
      "Main effects training epoch: 100, train loss: 0.52056, val loss: 0.50286\n",
      "Main effects training epoch: 101, train loss: 0.52011, val loss: 0.50351\n",
      "Main effects training epoch: 102, train loss: 0.51915, val loss: 0.50229\n",
      "Main effects training epoch: 103, train loss: 0.51831, val loss: 0.50268\n",
      "Main effects training epoch: 104, train loss: 0.51800, val loss: 0.50025\n",
      "Main effects training epoch: 105, train loss: 0.51768, val loss: 0.50015\n",
      "Main effects training epoch: 106, train loss: 0.51766, val loss: 0.50112\n",
      "Main effects training epoch: 107, train loss: 0.51861, val loss: 0.50248\n",
      "Main effects training epoch: 108, train loss: 0.51796, val loss: 0.50131\n",
      "Main effects training epoch: 109, train loss: 0.51740, val loss: 0.50099\n",
      "Main effects training epoch: 110, train loss: 0.51727, val loss: 0.50086\n",
      "Main effects training epoch: 111, train loss: 0.51729, val loss: 0.50220\n",
      "Main effects training epoch: 112, train loss: 0.51726, val loss: 0.50057\n",
      "Main effects training epoch: 113, train loss: 0.51696, val loss: 0.50040\n",
      "Main effects training epoch: 114, train loss: 0.51686, val loss: 0.50066\n",
      "Main effects training epoch: 115, train loss: 0.51707, val loss: 0.50053\n",
      "Main effects training epoch: 116, train loss: 0.51700, val loss: 0.50124\n",
      "Main effects training epoch: 117, train loss: 0.51734, val loss: 0.50104\n",
      "Main effects training epoch: 118, train loss: 0.51791, val loss: 0.50221\n",
      "Main effects training epoch: 119, train loss: 0.51737, val loss: 0.50390\n",
      "Main effects training epoch: 120, train loss: 0.51679, val loss: 0.50022\n",
      "Main effects training epoch: 121, train loss: 0.51639, val loss: 0.50055\n",
      "Main effects training epoch: 122, train loss: 0.51617, val loss: 0.50053\n",
      "Main effects training epoch: 123, train loss: 0.51604, val loss: 0.50053\n",
      "Main effects training epoch: 124, train loss: 0.51614, val loss: 0.49975\n",
      "Main effects training epoch: 125, train loss: 0.51626, val loss: 0.50206\n",
      "Main effects training epoch: 126, train loss: 0.51667, val loss: 0.50019\n",
      "Main effects training epoch: 127, train loss: 0.51640, val loss: 0.50234\n",
      "Main effects training epoch: 128, train loss: 0.51656, val loss: 0.49966\n",
      "Main effects training epoch: 129, train loss: 0.51594, val loss: 0.50223\n",
      "Main effects training epoch: 130, train loss: 0.51577, val loss: 0.49997\n",
      "Main effects training epoch: 131, train loss: 0.51580, val loss: 0.50155\n",
      "Main effects training epoch: 132, train loss: 0.51569, val loss: 0.50016\n",
      "Main effects training epoch: 133, train loss: 0.51569, val loss: 0.50010\n",
      "Main effects training epoch: 134, train loss: 0.51553, val loss: 0.50116\n",
      "Main effects training epoch: 135, train loss: 0.51517, val loss: 0.50059\n",
      "Main effects training epoch: 136, train loss: 0.51532, val loss: 0.50048\n",
      "Main effects training epoch: 137, train loss: 0.51535, val loss: 0.50049\n",
      "Main effects training epoch: 138, train loss: 0.51539, val loss: 0.50123\n",
      "Main effects training epoch: 139, train loss: 0.51526, val loss: 0.49985\n",
      "Main effects training epoch: 140, train loss: 0.51494, val loss: 0.50060\n",
      "Main effects training epoch: 141, train loss: 0.51472, val loss: 0.49890\n",
      "Main effects training epoch: 142, train loss: 0.51506, val loss: 0.50197\n",
      "Main effects training epoch: 143, train loss: 0.51526, val loss: 0.50139\n",
      "Main effects training epoch: 144, train loss: 0.51502, val loss: 0.49907\n",
      "Main effects training epoch: 145, train loss: 0.51502, val loss: 0.49943\n",
      "Main effects training epoch: 146, train loss: 0.51451, val loss: 0.50117\n",
      "Main effects training epoch: 147, train loss: 0.51439, val loss: 0.49877\n",
      "Main effects training epoch: 148, train loss: 0.51463, val loss: 0.50091\n",
      "Main effects training epoch: 149, train loss: 0.51493, val loss: 0.50012\n",
      "Main effects training epoch: 150, train loss: 0.51477, val loss: 0.50099\n",
      "Main effects training epoch: 151, train loss: 0.51416, val loss: 0.49898\n",
      "Main effects training epoch: 152, train loss: 0.51402, val loss: 0.49875\n",
      "Main effects training epoch: 153, train loss: 0.51384, val loss: 0.49939\n",
      "Main effects training epoch: 154, train loss: 0.51371, val loss: 0.49833\n",
      "Main effects training epoch: 155, train loss: 0.51414, val loss: 0.50037\n",
      "Main effects training epoch: 156, train loss: 0.51373, val loss: 0.49918\n",
      "Main effects training epoch: 157, train loss: 0.51467, val loss: 0.50077\n",
      "Main effects training epoch: 158, train loss: 0.51382, val loss: 0.49865\n",
      "Main effects training epoch: 159, train loss: 0.51383, val loss: 0.49978\n",
      "Main effects training epoch: 160, train loss: 0.51337, val loss: 0.49946\n",
      "Main effects training epoch: 161, train loss: 0.51338, val loss: 0.49789\n",
      "Main effects training epoch: 162, train loss: 0.51328, val loss: 0.49831\n",
      "Main effects training epoch: 163, train loss: 0.51361, val loss: 0.49867\n",
      "Main effects training epoch: 164, train loss: 0.51470, val loss: 0.50105\n",
      "Main effects training epoch: 165, train loss: 0.51343, val loss: 0.49970\n",
      "Main effects training epoch: 166, train loss: 0.51328, val loss: 0.49892\n",
      "Main effects training epoch: 167, train loss: 0.51337, val loss: 0.49793\n",
      "Main effects training epoch: 168, train loss: 0.51311, val loss: 0.49864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 169, train loss: 0.51276, val loss: 0.49869\n",
      "Main effects training epoch: 170, train loss: 0.51274, val loss: 0.49903\n",
      "Main effects training epoch: 171, train loss: 0.51304, val loss: 0.49830\n",
      "Main effects training epoch: 172, train loss: 0.51368, val loss: 0.50010\n",
      "Main effects training epoch: 173, train loss: 0.51332, val loss: 0.49957\n",
      "Main effects training epoch: 174, train loss: 0.51488, val loss: 0.50066\n",
      "Main effects training epoch: 175, train loss: 0.51353, val loss: 0.49797\n",
      "Main effects training epoch: 176, train loss: 0.51301, val loss: 0.49896\n",
      "Main effects training epoch: 177, train loss: 0.51326, val loss: 0.49800\n",
      "Main effects training epoch: 178, train loss: 0.51343, val loss: 0.49951\n",
      "Main effects training epoch: 179, train loss: 0.51294, val loss: 0.49906\n",
      "Main effects training epoch: 180, train loss: 0.51266, val loss: 0.49752\n",
      "Main effects training epoch: 181, train loss: 0.51222, val loss: 0.49779\n",
      "Main effects training epoch: 182, train loss: 0.51255, val loss: 0.49644\n",
      "Main effects training epoch: 183, train loss: 0.51249, val loss: 0.49740\n",
      "Main effects training epoch: 184, train loss: 0.51249, val loss: 0.49841\n",
      "Main effects training epoch: 185, train loss: 0.51241, val loss: 0.49837\n",
      "Main effects training epoch: 186, train loss: 0.51255, val loss: 0.49849\n",
      "Main effects training epoch: 187, train loss: 0.51315, val loss: 0.49795\n",
      "Main effects training epoch: 188, train loss: 0.51336, val loss: 0.49924\n",
      "Main effects training epoch: 189, train loss: 0.51444, val loss: 0.49879\n",
      "Main effects training epoch: 190, train loss: 0.51387, val loss: 0.50072\n",
      "Main effects training epoch: 191, train loss: 0.51395, val loss: 0.49939\n",
      "Main effects training epoch: 192, train loss: 0.51299, val loss: 0.49865\n",
      "Main effects training epoch: 193, train loss: 0.51325, val loss: 0.49734\n",
      "Main effects training epoch: 194, train loss: 0.51295, val loss: 0.50001\n",
      "Main effects training epoch: 195, train loss: 0.51272, val loss: 0.49674\n",
      "Main effects training epoch: 196, train loss: 0.51489, val loss: 0.50255\n",
      "Main effects training epoch: 197, train loss: 0.51378, val loss: 0.49859\n",
      "Main effects training epoch: 198, train loss: 0.51234, val loss: 0.49700\n",
      "Main effects training epoch: 199, train loss: 0.51187, val loss: 0.49750\n",
      "Main effects training epoch: 200, train loss: 0.51248, val loss: 0.49789\n",
      "Main effects training epoch: 201, train loss: 0.51205, val loss: 0.49721\n",
      "Main effects training epoch: 202, train loss: 0.51212, val loss: 0.49690\n",
      "Main effects training epoch: 203, train loss: 0.51264, val loss: 0.49948\n",
      "Main effects training epoch: 204, train loss: 0.51189, val loss: 0.49750\n",
      "Main effects training epoch: 205, train loss: 0.51190, val loss: 0.49694\n",
      "Main effects training epoch: 206, train loss: 0.51195, val loss: 0.49689\n",
      "Main effects training epoch: 207, train loss: 0.51168, val loss: 0.49692\n",
      "Main effects training epoch: 208, train loss: 0.51176, val loss: 0.49648\n",
      "Main effects training epoch: 209, train loss: 0.51183, val loss: 0.49848\n",
      "Main effects training epoch: 210, train loss: 0.51201, val loss: 0.49706\n",
      "Main effects training epoch: 211, train loss: 0.51230, val loss: 0.49686\n",
      "Main effects training epoch: 212, train loss: 0.51205, val loss: 0.49769\n",
      "Main effects training epoch: 213, train loss: 0.51179, val loss: 0.49770\n",
      "Main effects training epoch: 214, train loss: 0.51179, val loss: 0.49710\n",
      "Main effects training epoch: 215, train loss: 0.51168, val loss: 0.49684\n",
      "Main effects training epoch: 216, train loss: 0.51184, val loss: 0.49801\n",
      "Main effects training epoch: 217, train loss: 0.51229, val loss: 0.49886\n",
      "Main effects training epoch: 218, train loss: 0.51201, val loss: 0.49714\n",
      "Main effects training epoch: 219, train loss: 0.51172, val loss: 0.49673\n",
      "Main effects training epoch: 220, train loss: 0.51173, val loss: 0.49719\n",
      "Main effects training epoch: 221, train loss: 0.51164, val loss: 0.49768\n",
      "Main effects training epoch: 222, train loss: 0.51190, val loss: 0.49588\n",
      "Main effects training epoch: 223, train loss: 0.51224, val loss: 0.49871\n",
      "Main effects training epoch: 224, train loss: 0.51176, val loss: 0.49553\n",
      "Main effects training epoch: 225, train loss: 0.51152, val loss: 0.49716\n",
      "Main effects training epoch: 226, train loss: 0.51177, val loss: 0.49861\n",
      "Main effects training epoch: 227, train loss: 0.51190, val loss: 0.49496\n",
      "Main effects training epoch: 228, train loss: 0.51226, val loss: 0.49835\n",
      "Main effects training epoch: 229, train loss: 0.51131, val loss: 0.49703\n",
      "Main effects training epoch: 230, train loss: 0.51162, val loss: 0.49702\n",
      "Main effects training epoch: 231, train loss: 0.51136, val loss: 0.49747\n",
      "Main effects training epoch: 232, train loss: 0.51194, val loss: 0.49593\n",
      "Main effects training epoch: 233, train loss: 0.51194, val loss: 0.49867\n",
      "Main effects training epoch: 234, train loss: 0.51209, val loss: 0.49581\n",
      "Main effects training epoch: 235, train loss: 0.51174, val loss: 0.49855\n",
      "Main effects training epoch: 236, train loss: 0.51176, val loss: 0.49671\n",
      "Main effects training epoch: 237, train loss: 0.51154, val loss: 0.49525\n",
      "Main effects training epoch: 238, train loss: 0.51127, val loss: 0.49617\n",
      "Main effects training epoch: 239, train loss: 0.51179, val loss: 0.49737\n",
      "Main effects training epoch: 240, train loss: 0.51130, val loss: 0.49672\n",
      "Main effects training epoch: 241, train loss: 0.51151, val loss: 0.49544\n",
      "Main effects training epoch: 242, train loss: 0.51135, val loss: 0.49646\n",
      "Main effects training epoch: 243, train loss: 0.51146, val loss: 0.49669\n",
      "Main effects training epoch: 244, train loss: 0.51163, val loss: 0.49707\n",
      "Main effects training epoch: 245, train loss: 0.51245, val loss: 0.49610\n",
      "Main effects training epoch: 246, train loss: 0.51200, val loss: 0.49810\n",
      "Main effects training epoch: 247, train loss: 0.51211, val loss: 0.49531\n",
      "Main effects training epoch: 248, train loss: 0.51136, val loss: 0.49738\n",
      "Main effects training epoch: 249, train loss: 0.51126, val loss: 0.49633\n",
      "Main effects training epoch: 250, train loss: 0.51136, val loss: 0.49688\n",
      "Main effects training epoch: 251, train loss: 0.51161, val loss: 0.49600\n",
      "Main effects training epoch: 252, train loss: 0.51159, val loss: 0.49765\n",
      "Main effects training epoch: 253, train loss: 0.51198, val loss: 0.49572\n",
      "Main effects training epoch: 254, train loss: 0.51206, val loss: 0.49878\n",
      "Main effects training epoch: 255, train loss: 0.51137, val loss: 0.49531\n",
      "Main effects training epoch: 256, train loss: 0.51189, val loss: 0.49797\n",
      "Main effects training epoch: 257, train loss: 0.51190, val loss: 0.49512\n",
      "Main effects training epoch: 258, train loss: 0.51162, val loss: 0.49794\n",
      "Main effects training epoch: 259, train loss: 0.51134, val loss: 0.49641\n",
      "Main effects training epoch: 260, train loss: 0.51136, val loss: 0.49786\n",
      "Main effects training epoch: 261, train loss: 0.51129, val loss: 0.49509\n",
      "Main effects training epoch: 262, train loss: 0.51136, val loss: 0.49774\n",
      "Main effects training epoch: 263, train loss: 0.51116, val loss: 0.49535\n",
      "Main effects training epoch: 264, train loss: 0.51158, val loss: 0.49815\n",
      "Main effects training epoch: 265, train loss: 0.51116, val loss: 0.49542\n",
      "Main effects training epoch: 266, train loss: 0.51112, val loss: 0.49659\n",
      "Main effects training epoch: 267, train loss: 0.51131, val loss: 0.49545\n",
      "Main effects training epoch: 268, train loss: 0.51133, val loss: 0.49729\n",
      "Main effects training epoch: 269, train loss: 0.51126, val loss: 0.49635\n",
      "Main effects training epoch: 270, train loss: 0.51116, val loss: 0.49600\n",
      "Main effects training epoch: 271, train loss: 0.51169, val loss: 0.49565\n",
      "Main effects training epoch: 272, train loss: 0.51177, val loss: 0.49757\n",
      "Main effects training epoch: 273, train loss: 0.51115, val loss: 0.49566\n",
      "Main effects training epoch: 274, train loss: 0.51131, val loss: 0.49570\n",
      "Main effects training epoch: 275, train loss: 0.51130, val loss: 0.49804\n",
      "Main effects training epoch: 276, train loss: 0.51111, val loss: 0.49718\n",
      "Main effects training epoch: 277, train loss: 0.51133, val loss: 0.49477\n",
      "Main effects training epoch: 278, train loss: 0.51103, val loss: 0.49665\n",
      "Main effects training epoch: 279, train loss: 0.51117, val loss: 0.49622\n",
      "Main effects training epoch: 280, train loss: 0.51115, val loss: 0.49629\n",
      "Main effects training epoch: 281, train loss: 0.51115, val loss: 0.49560\n",
      "Main effects training epoch: 282, train loss: 0.51114, val loss: 0.49540\n",
      "Main effects training epoch: 283, train loss: 0.51118, val loss: 0.49603\n",
      "Main effects training epoch: 284, train loss: 0.51158, val loss: 0.49720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 285, train loss: 0.51112, val loss: 0.49681\n",
      "Main effects training epoch: 286, train loss: 0.51133, val loss: 0.49533\n",
      "Main effects training epoch: 287, train loss: 0.51107, val loss: 0.49644\n",
      "Main effects training epoch: 288, train loss: 0.51139, val loss: 0.49576\n",
      "Main effects training epoch: 289, train loss: 0.51102, val loss: 0.49703\n",
      "Main effects training epoch: 290, train loss: 0.51123, val loss: 0.49526\n",
      "Main effects training epoch: 291, train loss: 0.51165, val loss: 0.49727\n",
      "Main effects training epoch: 292, train loss: 0.51098, val loss: 0.49621\n",
      "Main effects training epoch: 293, train loss: 0.51102, val loss: 0.49652\n",
      "Main effects training epoch: 294, train loss: 0.51098, val loss: 0.49557\n",
      "Main effects training epoch: 295, train loss: 0.51113, val loss: 0.49687\n",
      "Main effects training epoch: 296, train loss: 0.51085, val loss: 0.49668\n",
      "Main effects training epoch: 297, train loss: 0.51199, val loss: 0.49408\n",
      "Main effects training epoch: 298, train loss: 0.51197, val loss: 0.49855\n",
      "Main effects training epoch: 299, train loss: 0.51125, val loss: 0.49452\n",
      "Main effects training epoch: 300, train loss: 0.51101, val loss: 0.49705\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51260, val loss: 0.49643\n",
      "Main effects tuning epoch: 2, train loss: 0.51261, val loss: 0.49647\n",
      "Main effects tuning epoch: 3, train loss: 0.51291, val loss: 0.49753\n",
      "Main effects tuning epoch: 4, train loss: 0.51386, val loss: 0.49761\n",
      "Main effects tuning epoch: 5, train loss: 0.51280, val loss: 0.49768\n",
      "Main effects tuning epoch: 6, train loss: 0.51252, val loss: 0.49637\n",
      "Main effects tuning epoch: 7, train loss: 0.51240, val loss: 0.49646\n",
      "Main effects tuning epoch: 8, train loss: 0.51240, val loss: 0.49758\n",
      "Main effects tuning epoch: 9, train loss: 0.51237, val loss: 0.49633\n",
      "Main effects tuning epoch: 10, train loss: 0.51314, val loss: 0.49944\n",
      "Main effects tuning epoch: 11, train loss: 0.51310, val loss: 0.49629\n",
      "Main effects tuning epoch: 12, train loss: 0.51250, val loss: 0.49705\n",
      "Main effects tuning epoch: 13, train loss: 0.51253, val loss: 0.49831\n",
      "Main effects tuning epoch: 14, train loss: 0.51261, val loss: 0.49729\n",
      "Main effects tuning epoch: 15, train loss: 0.51285, val loss: 0.49803\n",
      "Main effects tuning epoch: 16, train loss: 0.51316, val loss: 0.49671\n",
      "Main effects tuning epoch: 17, train loss: 0.51257, val loss: 0.49884\n",
      "Main effects tuning epoch: 18, train loss: 0.51232, val loss: 0.49686\n",
      "Main effects tuning epoch: 19, train loss: 0.51280, val loss: 0.49818\n",
      "Main effects tuning epoch: 20, train loss: 0.51248, val loss: 0.49612\n",
      "Main effects tuning epoch: 21, train loss: 0.51240, val loss: 0.49860\n",
      "Main effects tuning epoch: 22, train loss: 0.51269, val loss: 0.49584\n",
      "Main effects tuning epoch: 23, train loss: 0.51295, val loss: 0.49947\n",
      "Main effects tuning epoch: 24, train loss: 0.51264, val loss: 0.49551\n",
      "Main effects tuning epoch: 25, train loss: 0.51254, val loss: 0.49716\n",
      "Main effects tuning epoch: 26, train loss: 0.51228, val loss: 0.49761\n",
      "Main effects tuning epoch: 27, train loss: 0.51205, val loss: 0.49662\n",
      "Main effects tuning epoch: 28, train loss: 0.51206, val loss: 0.49619\n",
      "Main effects tuning epoch: 29, train loss: 0.51217, val loss: 0.49733\n",
      "Main effects tuning epoch: 30, train loss: 0.51219, val loss: 0.49792\n",
      "Main effects tuning epoch: 31, train loss: 0.51217, val loss: 0.49723\n",
      "Main effects tuning epoch: 32, train loss: 0.51286, val loss: 0.49788\n",
      "Main effects tuning epoch: 33, train loss: 0.51229, val loss: 0.49727\n",
      "Main effects tuning epoch: 34, train loss: 0.51224, val loss: 0.49696\n",
      "Main effects tuning epoch: 35, train loss: 0.51240, val loss: 0.49634\n",
      "Main effects tuning epoch: 36, train loss: 0.51216, val loss: 0.49790\n",
      "Main effects tuning epoch: 37, train loss: 0.51212, val loss: 0.49768\n",
      "Main effects tuning epoch: 38, train loss: 0.51261, val loss: 0.49581\n",
      "Main effects tuning epoch: 39, train loss: 0.51316, val loss: 0.49776\n",
      "Main effects tuning epoch: 40, train loss: 0.51220, val loss: 0.49708\n",
      "Main effects tuning epoch: 41, train loss: 0.51203, val loss: 0.49613\n",
      "Main effects tuning epoch: 42, train loss: 0.51177, val loss: 0.49664\n",
      "Main effects tuning epoch: 43, train loss: 0.51193, val loss: 0.49672\n",
      "Main effects tuning epoch: 44, train loss: 0.51227, val loss: 0.49707\n",
      "Main effects tuning epoch: 45, train loss: 0.51217, val loss: 0.49539\n",
      "Main effects tuning epoch: 46, train loss: 0.51237, val loss: 0.50007\n",
      "Main effects tuning epoch: 47, train loss: 0.51199, val loss: 0.49696\n",
      "Main effects tuning epoch: 48, train loss: 0.51239, val loss: 0.49682\n",
      "Main effects tuning epoch: 49, train loss: 0.51246, val loss: 0.49562\n",
      "Main effects tuning epoch: 50, train loss: 0.51228, val loss: 0.49824\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.51483, val loss: 0.51079\n",
      "Interaction training epoch: 2, train loss: 0.33305, val loss: 0.32839\n",
      "Interaction training epoch: 3, train loss: 0.30622, val loss: 0.30403\n",
      "Interaction training epoch: 4, train loss: 0.31387, val loss: 0.31310\n",
      "Interaction training epoch: 5, train loss: 0.29362, val loss: 0.30121\n",
      "Interaction training epoch: 6, train loss: 0.29371, val loss: 0.29628\n",
      "Interaction training epoch: 7, train loss: 0.28380, val loss: 0.28659\n",
      "Interaction training epoch: 8, train loss: 0.28409, val loss: 0.28529\n",
      "Interaction training epoch: 9, train loss: 0.28059, val loss: 0.28500\n",
      "Interaction training epoch: 10, train loss: 0.28350, val loss: 0.28610\n",
      "Interaction training epoch: 11, train loss: 0.28181, val loss: 0.28829\n",
      "Interaction training epoch: 12, train loss: 0.28775, val loss: 0.28962\n",
      "Interaction training epoch: 13, train loss: 0.27865, val loss: 0.28421\n",
      "Interaction training epoch: 14, train loss: 0.28046, val loss: 0.28453\n",
      "Interaction training epoch: 15, train loss: 0.28053, val loss: 0.28671\n",
      "Interaction training epoch: 16, train loss: 0.27929, val loss: 0.28417\n",
      "Interaction training epoch: 17, train loss: 0.27751, val loss: 0.28494\n",
      "Interaction training epoch: 18, train loss: 0.27636, val loss: 0.28144\n",
      "Interaction training epoch: 19, train loss: 0.27819, val loss: 0.28439\n",
      "Interaction training epoch: 20, train loss: 0.28067, val loss: 0.28767\n",
      "Interaction training epoch: 21, train loss: 0.28455, val loss: 0.28967\n",
      "Interaction training epoch: 22, train loss: 0.28986, val loss: 0.29704\n",
      "Interaction training epoch: 23, train loss: 0.28127, val loss: 0.28819\n",
      "Interaction training epoch: 24, train loss: 0.27300, val loss: 0.28246\n",
      "Interaction training epoch: 25, train loss: 0.27394, val loss: 0.28282\n",
      "Interaction training epoch: 26, train loss: 0.27191, val loss: 0.28225\n",
      "Interaction training epoch: 27, train loss: 0.27529, val loss: 0.28427\n",
      "Interaction training epoch: 28, train loss: 0.27715, val loss: 0.28819\n",
      "Interaction training epoch: 29, train loss: 0.27208, val loss: 0.28418\n",
      "Interaction training epoch: 30, train loss: 0.27425, val loss: 0.28535\n",
      "Interaction training epoch: 31, train loss: 0.27290, val loss: 0.28185\n",
      "Interaction training epoch: 32, train loss: 0.27616, val loss: 0.28824\n",
      "Interaction training epoch: 33, train loss: 0.27508, val loss: 0.28552\n",
      "Interaction training epoch: 34, train loss: 0.27260, val loss: 0.28291\n",
      "Interaction training epoch: 35, train loss: 0.27143, val loss: 0.28271\n",
      "Interaction training epoch: 36, train loss: 0.27382, val loss: 0.28677\n",
      "Interaction training epoch: 37, train loss: 0.27202, val loss: 0.28373\n",
      "Interaction training epoch: 38, train loss: 0.27185, val loss: 0.28439\n",
      "Interaction training epoch: 39, train loss: 0.27310, val loss: 0.28769\n",
      "Interaction training epoch: 40, train loss: 0.27361, val loss: 0.28706\n",
      "Interaction training epoch: 41, train loss: 0.27107, val loss: 0.28237\n",
      "Interaction training epoch: 42, train loss: 0.27523, val loss: 0.28812\n",
      "Interaction training epoch: 43, train loss: 0.26949, val loss: 0.28315\n",
      "Interaction training epoch: 44, train loss: 0.26964, val loss: 0.28254\n",
      "Interaction training epoch: 45, train loss: 0.26893, val loss: 0.28563\n",
      "Interaction training epoch: 46, train loss: 0.26944, val loss: 0.28463\n",
      "Interaction training epoch: 47, train loss: 0.27056, val loss: 0.28435\n",
      "Interaction training epoch: 48, train loss: 0.27214, val loss: 0.28547\n",
      "Interaction training epoch: 49, train loss: 0.26995, val loss: 0.28971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 50, train loss: 0.26880, val loss: 0.28320\n",
      "Interaction training epoch: 51, train loss: 0.26811, val loss: 0.28338\n",
      "Interaction training epoch: 52, train loss: 0.26932, val loss: 0.28448\n",
      "Interaction training epoch: 53, train loss: 0.26922, val loss: 0.28487\n",
      "Interaction training epoch: 54, train loss: 0.26932, val loss: 0.28829\n",
      "Interaction training epoch: 55, train loss: 0.26774, val loss: 0.28317\n",
      "Interaction training epoch: 56, train loss: 0.26753, val loss: 0.28342\n",
      "Interaction training epoch: 57, train loss: 0.26970, val loss: 0.28730\n",
      "Interaction training epoch: 58, train loss: 0.26511, val loss: 0.28086\n",
      "Interaction training epoch: 59, train loss: 0.26706, val loss: 0.28362\n",
      "Interaction training epoch: 60, train loss: 0.27070, val loss: 0.28598\n",
      "Interaction training epoch: 61, train loss: 0.26798, val loss: 0.28939\n",
      "Interaction training epoch: 62, train loss: 0.26775, val loss: 0.28441\n",
      "Interaction training epoch: 63, train loss: 0.26605, val loss: 0.28317\n",
      "Interaction training epoch: 64, train loss: 0.26851, val loss: 0.28371\n",
      "Interaction training epoch: 65, train loss: 0.26378, val loss: 0.28111\n",
      "Interaction training epoch: 66, train loss: 0.26469, val loss: 0.28233\n",
      "Interaction training epoch: 67, train loss: 0.26372, val loss: 0.28149\n",
      "Interaction training epoch: 68, train loss: 0.26801, val loss: 0.28347\n",
      "Interaction training epoch: 69, train loss: 0.26662, val loss: 0.28565\n",
      "Interaction training epoch: 70, train loss: 0.26417, val loss: 0.27951\n",
      "Interaction training epoch: 71, train loss: 0.26361, val loss: 0.28437\n",
      "Interaction training epoch: 72, train loss: 0.26225, val loss: 0.28275\n",
      "Interaction training epoch: 73, train loss: 0.26459, val loss: 0.28555\n",
      "Interaction training epoch: 74, train loss: 0.26367, val loss: 0.27863\n",
      "Interaction training epoch: 75, train loss: 0.26319, val loss: 0.28099\n",
      "Interaction training epoch: 76, train loss: 0.26262, val loss: 0.28367\n",
      "Interaction training epoch: 77, train loss: 0.26297, val loss: 0.28107\n",
      "Interaction training epoch: 78, train loss: 0.26179, val loss: 0.27967\n",
      "Interaction training epoch: 79, train loss: 0.26191, val loss: 0.28076\n",
      "Interaction training epoch: 80, train loss: 0.26673, val loss: 0.28636\n",
      "Interaction training epoch: 81, train loss: 0.26253, val loss: 0.28046\n",
      "Interaction training epoch: 82, train loss: 0.26101, val loss: 0.27954\n",
      "Interaction training epoch: 83, train loss: 0.26252, val loss: 0.27906\n",
      "Interaction training epoch: 84, train loss: 0.26105, val loss: 0.27932\n",
      "Interaction training epoch: 85, train loss: 0.26208, val loss: 0.28217\n",
      "Interaction training epoch: 86, train loss: 0.26082, val loss: 0.27690\n",
      "Interaction training epoch: 87, train loss: 0.26199, val loss: 0.27993\n",
      "Interaction training epoch: 88, train loss: 0.26114, val loss: 0.28134\n",
      "Interaction training epoch: 89, train loss: 0.26578, val loss: 0.28745\n",
      "Interaction training epoch: 90, train loss: 0.26028, val loss: 0.27852\n",
      "Interaction training epoch: 91, train loss: 0.26060, val loss: 0.27977\n",
      "Interaction training epoch: 92, train loss: 0.26054, val loss: 0.27862\n",
      "Interaction training epoch: 93, train loss: 0.26267, val loss: 0.28199\n",
      "Interaction training epoch: 94, train loss: 0.25965, val loss: 0.28033\n",
      "Interaction training epoch: 95, train loss: 0.25836, val loss: 0.27837\n",
      "Interaction training epoch: 96, train loss: 0.26103, val loss: 0.27816\n",
      "Interaction training epoch: 97, train loss: 0.25799, val loss: 0.27508\n",
      "Interaction training epoch: 98, train loss: 0.25839, val loss: 0.27840\n",
      "Interaction training epoch: 99, train loss: 0.26145, val loss: 0.27822\n",
      "Interaction training epoch: 100, train loss: 0.25780, val loss: 0.27839\n",
      "Interaction training epoch: 101, train loss: 0.25920, val loss: 0.27591\n",
      "Interaction training epoch: 102, train loss: 0.25987, val loss: 0.27898\n",
      "Interaction training epoch: 103, train loss: 0.25877, val loss: 0.27614\n",
      "Interaction training epoch: 104, train loss: 0.26038, val loss: 0.27963\n",
      "Interaction training epoch: 105, train loss: 0.25748, val loss: 0.27091\n",
      "Interaction training epoch: 106, train loss: 0.25858, val loss: 0.27841\n",
      "Interaction training epoch: 107, train loss: 0.25982, val loss: 0.28079\n",
      "Interaction training epoch: 108, train loss: 0.25909, val loss: 0.27704\n",
      "Interaction training epoch: 109, train loss: 0.26019, val loss: 0.27921\n",
      "Interaction training epoch: 110, train loss: 0.26022, val loss: 0.27732\n",
      "Interaction training epoch: 111, train loss: 0.25869, val loss: 0.27915\n",
      "Interaction training epoch: 112, train loss: 0.25903, val loss: 0.27637\n",
      "Interaction training epoch: 113, train loss: 0.25759, val loss: 0.27749\n",
      "Interaction training epoch: 114, train loss: 0.25817, val loss: 0.27334\n",
      "Interaction training epoch: 115, train loss: 0.25756, val loss: 0.27609\n",
      "Interaction training epoch: 116, train loss: 0.25609, val loss: 0.27733\n",
      "Interaction training epoch: 117, train loss: 0.25819, val loss: 0.27608\n",
      "Interaction training epoch: 118, train loss: 0.25578, val loss: 0.27802\n",
      "Interaction training epoch: 119, train loss: 0.25785, val loss: 0.27538\n",
      "Interaction training epoch: 120, train loss: 0.25619, val loss: 0.27589\n",
      "Interaction training epoch: 121, train loss: 0.25428, val loss: 0.27171\n",
      "Interaction training epoch: 122, train loss: 0.25499, val loss: 0.27431\n",
      "Interaction training epoch: 123, train loss: 0.25573, val loss: 0.27333\n",
      "Interaction training epoch: 124, train loss: 0.25580, val loss: 0.27569\n",
      "Interaction training epoch: 125, train loss: 0.25514, val loss: 0.27372\n",
      "Interaction training epoch: 126, train loss: 0.25549, val loss: 0.27802\n",
      "Interaction training epoch: 127, train loss: 0.25764, val loss: 0.27786\n",
      "Interaction training epoch: 128, train loss: 0.25599, val loss: 0.27326\n",
      "Interaction training epoch: 129, train loss: 0.25706, val loss: 0.28046\n",
      "Interaction training epoch: 130, train loss: 0.25730, val loss: 0.27469\n",
      "Interaction training epoch: 131, train loss: 0.25433, val loss: 0.27646\n",
      "Interaction training epoch: 132, train loss: 0.25356, val loss: 0.27139\n",
      "Interaction training epoch: 133, train loss: 0.25488, val loss: 0.27713\n",
      "Interaction training epoch: 134, train loss: 0.25170, val loss: 0.27166\n",
      "Interaction training epoch: 135, train loss: 0.25382, val loss: 0.27513\n",
      "Interaction training epoch: 136, train loss: 0.25408, val loss: 0.27222\n",
      "Interaction training epoch: 137, train loss: 0.25278, val loss: 0.27477\n",
      "Interaction training epoch: 138, train loss: 0.25150, val loss: 0.27380\n",
      "Interaction training epoch: 139, train loss: 0.25366, val loss: 0.27802\n",
      "Interaction training epoch: 140, train loss: 0.25175, val loss: 0.27210\n",
      "Interaction training epoch: 141, train loss: 0.25476, val loss: 0.27779\n",
      "Interaction training epoch: 142, train loss: 0.25285, val loss: 0.27316\n",
      "Interaction training epoch: 143, train loss: 0.25132, val loss: 0.27768\n",
      "Interaction training epoch: 144, train loss: 0.25232, val loss: 0.27127\n",
      "Interaction training epoch: 145, train loss: 0.25301, val loss: 0.27426\n",
      "Interaction training epoch: 146, train loss: 0.25108, val loss: 0.27392\n",
      "Interaction training epoch: 147, train loss: 0.24970, val loss: 0.27462\n",
      "Interaction training epoch: 148, train loss: 0.25072, val loss: 0.27348\n",
      "Interaction training epoch: 149, train loss: 0.25154, val loss: 0.27467\n",
      "Interaction training epoch: 150, train loss: 0.25123, val loss: 0.27449\n",
      "Interaction training epoch: 151, train loss: 0.24910, val loss: 0.27403\n",
      "Interaction training epoch: 152, train loss: 0.25147, val loss: 0.28137\n",
      "Interaction training epoch: 153, train loss: 0.24822, val loss: 0.27273\n",
      "Interaction training epoch: 154, train loss: 0.24967, val loss: 0.27815\n",
      "Interaction training epoch: 155, train loss: 0.25014, val loss: 0.27541\n",
      "Interaction training epoch: 156, train loss: 0.25134, val loss: 0.27783\n",
      "Interaction training epoch: 157, train loss: 0.24806, val loss: 0.27428\n",
      "Interaction training epoch: 158, train loss: 0.24902, val loss: 0.27676\n",
      "Interaction training epoch: 159, train loss: 0.24957, val loss: 0.27494\n",
      "Interaction training epoch: 160, train loss: 0.24769, val loss: 0.27150\n",
      "Interaction training epoch: 161, train loss: 0.25085, val loss: 0.28218\n",
      "Interaction training epoch: 162, train loss: 0.24921, val loss: 0.27359\n",
      "Interaction training epoch: 163, train loss: 0.25360, val loss: 0.28233\n",
      "Interaction training epoch: 164, train loss: 0.24817, val loss: 0.27502\n",
      "Interaction training epoch: 165, train loss: 0.24566, val loss: 0.27363\n",
      "Interaction training epoch: 166, train loss: 0.24753, val loss: 0.27421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 167, train loss: 0.24763, val loss: 0.27706\n",
      "Interaction training epoch: 168, train loss: 0.24814, val loss: 0.27252\n",
      "Interaction training epoch: 169, train loss: 0.24935, val loss: 0.28180\n",
      "Interaction training epoch: 170, train loss: 0.24724, val loss: 0.27601\n",
      "Interaction training epoch: 171, train loss: 0.24630, val loss: 0.27415\n",
      "Interaction training epoch: 172, train loss: 0.24773, val loss: 0.27334\n",
      "Interaction training epoch: 173, train loss: 0.24741, val loss: 0.27387\n",
      "Interaction training epoch: 174, train loss: 0.24887, val loss: 0.27679\n",
      "Interaction training epoch: 175, train loss: 0.24581, val loss: 0.27637\n",
      "Interaction training epoch: 176, train loss: 0.24761, val loss: 0.27438\n",
      "Interaction training epoch: 177, train loss: 0.24300, val loss: 0.26960\n",
      "Interaction training epoch: 178, train loss: 0.24905, val loss: 0.28007\n",
      "Interaction training epoch: 179, train loss: 0.24621, val loss: 0.27499\n",
      "Interaction training epoch: 180, train loss: 0.24779, val loss: 0.27499\n",
      "Interaction training epoch: 181, train loss: 0.24545, val loss: 0.27370\n",
      "Interaction training epoch: 182, train loss: 0.24569, val loss: 0.27588\n",
      "Interaction training epoch: 183, train loss: 0.24466, val loss: 0.27559\n",
      "Interaction training epoch: 184, train loss: 0.24460, val loss: 0.27484\n",
      "Interaction training epoch: 185, train loss: 0.24540, val loss: 0.27407\n",
      "Interaction training epoch: 186, train loss: 0.24292, val loss: 0.27315\n",
      "Interaction training epoch: 187, train loss: 0.24388, val loss: 0.27579\n",
      "Interaction training epoch: 188, train loss: 0.24488, val loss: 0.27577\n",
      "Interaction training epoch: 189, train loss: 0.24720, val loss: 0.28383\n",
      "Interaction training epoch: 190, train loss: 0.24406, val loss: 0.27186\n",
      "Interaction training epoch: 191, train loss: 0.24334, val loss: 0.27728\n",
      "Interaction training epoch: 192, train loss: 0.24333, val loss: 0.27284\n",
      "Interaction training epoch: 193, train loss: 0.24705, val loss: 0.28059\n",
      "Interaction training epoch: 194, train loss: 0.24475, val loss: 0.27217\n",
      "Interaction training epoch: 195, train loss: 0.24741, val loss: 0.28068\n",
      "Interaction training epoch: 196, train loss: 0.24544, val loss: 0.27803\n",
      "Interaction training epoch: 197, train loss: 0.24198, val loss: 0.26964\n",
      "Interaction training epoch: 198, train loss: 0.24385, val loss: 0.27801\n",
      "Interaction training epoch: 199, train loss: 0.24415, val loss: 0.27512\n",
      "Interaction training epoch: 200, train loss: 0.24143, val loss: 0.27610\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########5 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.25375, val loss: 0.26928\n",
      "Interaction tuning epoch: 2, train loss: 0.25395, val loss: 0.26881\n",
      "Interaction tuning epoch: 3, train loss: 0.25655, val loss: 0.27626\n",
      "Interaction tuning epoch: 4, train loss: 0.25358, val loss: 0.26991\n",
      "Interaction tuning epoch: 5, train loss: 0.25457, val loss: 0.26903\n",
      "Interaction tuning epoch: 6, train loss: 0.25365, val loss: 0.27309\n",
      "Interaction tuning epoch: 7, train loss: 0.25444, val loss: 0.26746\n",
      "Interaction tuning epoch: 8, train loss: 0.25716, val loss: 0.27370\n",
      "Interaction tuning epoch: 9, train loss: 0.25436, val loss: 0.26671\n",
      "Interaction tuning epoch: 10, train loss: 0.25303, val loss: 0.27150\n",
      "Interaction tuning epoch: 11, train loss: 0.25498, val loss: 0.27214\n",
      "Interaction tuning epoch: 12, train loss: 0.25129, val loss: 0.26341\n",
      "Interaction tuning epoch: 13, train loss: 0.25398, val loss: 0.27211\n",
      "Interaction tuning epoch: 14, train loss: 0.25084, val loss: 0.26743\n",
      "Interaction tuning epoch: 15, train loss: 0.25494, val loss: 0.27019\n",
      "Interaction tuning epoch: 16, train loss: 0.25254, val loss: 0.26792\n",
      "Interaction tuning epoch: 17, train loss: 0.25622, val loss: 0.27793\n",
      "Interaction tuning epoch: 18, train loss: 0.25461, val loss: 0.26619\n",
      "Interaction tuning epoch: 19, train loss: 0.25340, val loss: 0.27326\n",
      "Interaction tuning epoch: 20, train loss: 0.25265, val loss: 0.27007\n",
      "Interaction tuning epoch: 21, train loss: 0.25103, val loss: 0.26878\n",
      "Interaction tuning epoch: 22, train loss: 0.25300, val loss: 0.26614\n",
      "Interaction tuning epoch: 23, train loss: 0.25094, val loss: 0.26928\n",
      "Interaction tuning epoch: 24, train loss: 0.25058, val loss: 0.26463\n",
      "Interaction tuning epoch: 25, train loss: 0.25064, val loss: 0.26988\n",
      "Interaction tuning epoch: 26, train loss: 0.25323, val loss: 0.26776\n",
      "Interaction tuning epoch: 27, train loss: 0.25092, val loss: 0.26530\n",
      "Interaction tuning epoch: 28, train loss: 0.25256, val loss: 0.27087\n",
      "Interaction tuning epoch: 29, train loss: 0.25148, val loss: 0.26922\n",
      "Interaction tuning epoch: 30, train loss: 0.25058, val loss: 0.26965\n",
      "Interaction tuning epoch: 31, train loss: 0.24993, val loss: 0.26758\n",
      "Interaction tuning epoch: 32, train loss: 0.25153, val loss: 0.27028\n",
      "Interaction tuning epoch: 33, train loss: 0.25045, val loss: 0.26469\n",
      "Interaction tuning epoch: 34, train loss: 0.25191, val loss: 0.27148\n",
      "Interaction tuning epoch: 35, train loss: 0.25120, val loss: 0.26618\n",
      "Interaction tuning epoch: 36, train loss: 0.24859, val loss: 0.26580\n",
      "Interaction tuning epoch: 37, train loss: 0.25360, val loss: 0.27050\n",
      "Interaction tuning epoch: 38, train loss: 0.25126, val loss: 0.26726\n",
      "Interaction tuning epoch: 39, train loss: 0.25021, val loss: 0.26750\n",
      "Interaction tuning epoch: 40, train loss: 0.25145, val loss: 0.27379\n",
      "Interaction tuning epoch: 41, train loss: 0.24880, val loss: 0.26662\n",
      "Interaction tuning epoch: 42, train loss: 0.25263, val loss: 0.26663\n",
      "Interaction tuning epoch: 43, train loss: 0.25277, val loss: 0.27081\n",
      "Interaction tuning epoch: 44, train loss: 0.24883, val loss: 0.26612\n",
      "Interaction tuning epoch: 45, train loss: 0.25295, val loss: 0.27238\n",
      "Interaction tuning epoch: 46, train loss: 0.25085, val loss: 0.27010\n",
      "Interaction tuning epoch: 47, train loss: 0.24910, val loss: 0.26498\n",
      "Interaction tuning epoch: 48, train loss: 0.25022, val loss: 0.26880\n",
      "Interaction tuning epoch: 49, train loss: 0.25160, val loss: 0.27104\n",
      "Interaction tuning epoch: 50, train loss: 0.24884, val loss: 0.26439\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 42.89450287818909\n",
      "After the gam stage, training error is 0.24884 , validation error is 0.26439\n",
      "missing value counts: 99129\n",
      "[SoftImpute] Max Singular Value of X_init = 3.792838\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed BCE=0.209949 validation BCE=0.298824,rank=5\n",
      "[SoftImpute] Iter 2: observed BCE=0.205861 validation BCE=0.277804,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.203129 validation BCE=0.274911,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.201427 validation BCE=0.274029,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.199936 validation BCE=0.273404,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.199365 validation BCE=0.272968,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.198789 validation BCE=0.273859,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.198689 validation BCE=0.282027,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.198865 validation BCE=0.281767,rank=5\n",
      "[SoftImpute] Iter 10: observed BCE=0.199639 validation BCE=0.281269,rank=5\n",
      "[SoftImpute] Iter 11: observed BCE=0.200171 validation BCE=0.280892,rank=5\n",
      "[SoftImpute] Iter 12: observed BCE=0.200247 validation BCE=0.280235,rank=5\n",
      "[SoftImpute] Iter 13: observed BCE=0.200320 validation BCE=0.280404,rank=5\n",
      "[SoftImpute] Iter 14: observed BCE=0.200230 validation BCE=0.279898,rank=5\n",
      "[SoftImpute] Iter 15: observed BCE=0.200188 validation BCE=0.279413,rank=5\n",
      "[SoftImpute] Iter 16: observed BCE=0.200412 validation BCE=0.279429,rank=5\n",
      "[SoftImpute] Iter 17: observed BCE=0.200173 validation BCE=0.279297,rank=5\n",
      "[SoftImpute] Iter 18: observed BCE=0.200211 validation BCE=0.269119,rank=5\n",
      "[SoftImpute] Iter 19: observed BCE=0.200223 validation BCE=0.267998,rank=5\n",
      "[SoftImpute] Iter 20: observed BCE=0.200319 validation BCE=0.267703,rank=5\n",
      "[SoftImpute] Iter 21: observed BCE=0.200010 validation BCE=0.267271,rank=5\n",
      "[SoftImpute] Iter 22: observed BCE=0.200071 validation BCE=0.267134,rank=5\n",
      "[SoftImpute] Iter 23: observed BCE=0.199963 validation BCE=0.266477,rank=5\n",
      "[SoftImpute] Iter 24: observed BCE=0.200002 validation BCE=0.266275,rank=5\n",
      "[SoftImpute] Iter 25: observed BCE=0.199841 validation BCE=0.266487,rank=5\n",
      "[SoftImpute] Iter 26: observed BCE=0.199990 validation BCE=0.266345,rank=5\n",
      "[SoftImpute] Iter 27: observed BCE=0.199886 validation BCE=0.266216,rank=5\n",
      "[SoftImpute] Iter 28: observed BCE=0.199624 validation BCE=0.266144,rank=5\n",
      "[SoftImpute] Iter 29: observed BCE=0.199714 validation BCE=0.266451,rank=5\n",
      "[SoftImpute] Iter 30: observed BCE=0.199648 validation BCE=0.266573,rank=5\n",
      "[SoftImpute] Iter 31: observed BCE=0.199687 validation BCE=0.266114,rank=5\n",
      "[SoftImpute] Iter 32: observed BCE=0.199723 validation BCE=0.266176,rank=5\n",
      "[SoftImpute] Iter 33: observed BCE=0.199642 validation BCE=0.266489,rank=5\n",
      "[SoftImpute] Iter 34: observed BCE=0.199213 validation BCE=0.266187,rank=5\n",
      "[SoftImpute] Iter 35: observed BCE=0.199422 validation BCE=0.266281,rank=5\n",
      "[SoftImpute] Iter 36: observed BCE=0.199510 validation BCE=0.266131,rank=5\n",
      "[SoftImpute] Iter 37: observed BCE=0.199454 validation BCE=0.266081,rank=5\n",
      "[SoftImpute] Iter 38: observed BCE=0.199292 validation BCE=0.266087,rank=5\n",
      "[SoftImpute] Iter 39: observed BCE=0.199415 validation BCE=0.266396,rank=5\n",
      "[SoftImpute] Iter 40: observed BCE=0.199225 validation BCE=0.265791,rank=5\n",
      "[SoftImpute] Iter 41: observed BCE=0.199381 validation BCE=0.266548,rank=5\n",
      "[SoftImpute] Iter 42: observed BCE=0.199331 validation BCE=0.266094,rank=5\n",
      "[SoftImpute] Iter 43: observed BCE=0.199347 validation BCE=0.266410,rank=5\n",
      "[SoftImpute] Iter 44: observed BCE=0.199158 validation BCE=0.265702,rank=5\n",
      "[SoftImpute] Iter 45: observed BCE=0.199099 validation BCE=0.266252,rank=5\n",
      "[SoftImpute] Iter 46: observed BCE=0.199237 validation BCE=0.266024,rank=5\n",
      "[SoftImpute] Iter 47: observed BCE=0.199382 validation BCE=0.266062,rank=5\n",
      "[SoftImpute] Iter 48: observed BCE=0.199309 validation BCE=0.266103,rank=5\n",
      "[SoftImpute] Iter 49: observed BCE=0.199330 validation BCE=0.266371,rank=5\n",
      "[SoftImpute] Stopped after iteration 49 for lambda=0.075857\n",
      "final num of user group: 8\n",
      "final num of item group: 10\n",
      "change mode state : True\n",
      "time cost: 4.7734527587890625\n",
      "After the matrix factor stage, training error is 0.19933, validation error is 0.26637\n",
      "2\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68277, val loss: 0.68235\n",
      "Main effects training epoch: 2, train loss: 0.67533, val loss: 0.67571\n",
      "Main effects training epoch: 3, train loss: 0.67089, val loss: 0.67353\n",
      "Main effects training epoch: 4, train loss: 0.66391, val loss: 0.66537\n",
      "Main effects training epoch: 5, train loss: 0.65196, val loss: 0.65059\n",
      "Main effects training epoch: 6, train loss: 0.62593, val loss: 0.62012\n",
      "Main effects training epoch: 7, train loss: 0.58484, val loss: 0.57247\n",
      "Main effects training epoch: 8, train loss: 0.54444, val loss: 0.52468\n",
      "Main effects training epoch: 9, train loss: 0.53045, val loss: 0.50819\n",
      "Main effects training epoch: 10, train loss: 0.52728, val loss: 0.50453\n",
      "Main effects training epoch: 11, train loss: 0.52410, val loss: 0.50010\n",
      "Main effects training epoch: 12, train loss: 0.52312, val loss: 0.50078\n",
      "Main effects training epoch: 13, train loss: 0.52343, val loss: 0.50079\n",
      "Main effects training epoch: 14, train loss: 0.52437, val loss: 0.50380\n",
      "Main effects training epoch: 15, train loss: 0.52425, val loss: 0.49965\n",
      "Main effects training epoch: 16, train loss: 0.52316, val loss: 0.49965\n",
      "Main effects training epoch: 17, train loss: 0.52256, val loss: 0.50066\n",
      "Main effects training epoch: 18, train loss: 0.52212, val loss: 0.49895\n",
      "Main effects training epoch: 19, train loss: 0.52197, val loss: 0.49936\n",
      "Main effects training epoch: 20, train loss: 0.52211, val loss: 0.50046\n",
      "Main effects training epoch: 21, train loss: 0.52210, val loss: 0.49898\n",
      "Main effects training epoch: 22, train loss: 0.52183, val loss: 0.49919\n",
      "Main effects training epoch: 23, train loss: 0.52220, val loss: 0.50078\n",
      "Main effects training epoch: 24, train loss: 0.52204, val loss: 0.50031\n",
      "Main effects training epoch: 25, train loss: 0.52258, val loss: 0.50000\n",
      "Main effects training epoch: 26, train loss: 0.52342, val loss: 0.50371\n",
      "Main effects training epoch: 27, train loss: 0.52241, val loss: 0.49927\n",
      "Main effects training epoch: 28, train loss: 0.52207, val loss: 0.50082\n",
      "Main effects training epoch: 29, train loss: 0.52186, val loss: 0.49959\n",
      "Main effects training epoch: 30, train loss: 0.52156, val loss: 0.49963\n",
      "Main effects training epoch: 31, train loss: 0.52149, val loss: 0.49951\n",
      "Main effects training epoch: 32, train loss: 0.52138, val loss: 0.49985\n",
      "Main effects training epoch: 33, train loss: 0.52150, val loss: 0.50033\n",
      "Main effects training epoch: 34, train loss: 0.52158, val loss: 0.49918\n",
      "Main effects training epoch: 35, train loss: 0.52130, val loss: 0.49992\n",
      "Main effects training epoch: 36, train loss: 0.52123, val loss: 0.49851\n",
      "Main effects training epoch: 37, train loss: 0.52121, val loss: 0.49995\n",
      "Main effects training epoch: 38, train loss: 0.52127, val loss: 0.50030\n",
      "Main effects training epoch: 39, train loss: 0.52146, val loss: 0.49911\n",
      "Main effects training epoch: 40, train loss: 0.52136, val loss: 0.50034\n",
      "Main effects training epoch: 41, train loss: 0.52126, val loss: 0.49914\n",
      "Main effects training epoch: 42, train loss: 0.52124, val loss: 0.50039\n",
      "Main effects training epoch: 43, train loss: 0.52181, val loss: 0.49940\n",
      "Main effects training epoch: 44, train loss: 0.52122, val loss: 0.50018\n",
      "Main effects training epoch: 45, train loss: 0.52132, val loss: 0.49945\n",
      "Main effects training epoch: 46, train loss: 0.52118, val loss: 0.49925\n",
      "Main effects training epoch: 47, train loss: 0.52081, val loss: 0.49991\n",
      "Main effects training epoch: 48, train loss: 0.52088, val loss: 0.49983\n",
      "Main effects training epoch: 49, train loss: 0.52169, val loss: 0.49966\n",
      "Main effects training epoch: 50, train loss: 0.52118, val loss: 0.50049\n",
      "Main effects training epoch: 51, train loss: 0.52072, val loss: 0.49904\n",
      "Main effects training epoch: 52, train loss: 0.52081, val loss: 0.49948\n",
      "Main effects training epoch: 53, train loss: 0.52103, val loss: 0.50039\n",
      "Main effects training epoch: 54, train loss: 0.52056, val loss: 0.49981\n",
      "Main effects training epoch: 55, train loss: 0.52070, val loss: 0.49898\n",
      "Main effects training epoch: 56, train loss: 0.52071, val loss: 0.50017\n",
      "Main effects training epoch: 57, train loss: 0.52048, val loss: 0.49934\n",
      "Main effects training epoch: 58, train loss: 0.52085, val loss: 0.50038\n",
      "Main effects training epoch: 59, train loss: 0.52039, val loss: 0.50034\n",
      "Main effects training epoch: 60, train loss: 0.52029, val loss: 0.49979\n",
      "Main effects training epoch: 61, train loss: 0.52027, val loss: 0.49946\n",
      "Main effects training epoch: 62, train loss: 0.52038, val loss: 0.49935\n",
      "Main effects training epoch: 63, train loss: 0.52062, val loss: 0.50017\n",
      "Main effects training epoch: 64, train loss: 0.52013, val loss: 0.49970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 65, train loss: 0.52022, val loss: 0.49941\n",
      "Main effects training epoch: 66, train loss: 0.52058, val loss: 0.50221\n",
      "Main effects training epoch: 67, train loss: 0.52030, val loss: 0.50119\n",
      "Main effects training epoch: 68, train loss: 0.52001, val loss: 0.49995\n",
      "Main effects training epoch: 69, train loss: 0.52054, val loss: 0.50132\n",
      "Main effects training epoch: 70, train loss: 0.51998, val loss: 0.50139\n",
      "Main effects training epoch: 71, train loss: 0.51990, val loss: 0.50069\n",
      "Main effects training epoch: 72, train loss: 0.52036, val loss: 0.50089\n",
      "Main effects training epoch: 73, train loss: 0.51992, val loss: 0.50162\n",
      "Main effects training epoch: 74, train loss: 0.51990, val loss: 0.49946\n",
      "Main effects training epoch: 75, train loss: 0.51956, val loss: 0.50061\n",
      "Main effects training epoch: 76, train loss: 0.51963, val loss: 0.50092\n",
      "Main effects training epoch: 77, train loss: 0.51963, val loss: 0.50089\n",
      "Main effects training epoch: 78, train loss: 0.51933, val loss: 0.50053\n",
      "Main effects training epoch: 79, train loss: 0.51996, val loss: 0.50063\n",
      "Main effects training epoch: 80, train loss: 0.51968, val loss: 0.50130\n",
      "Main effects training epoch: 81, train loss: 0.51964, val loss: 0.50128\n",
      "Main effects training epoch: 82, train loss: 0.51966, val loss: 0.50042\n",
      "Main effects training epoch: 83, train loss: 0.51974, val loss: 0.50236\n",
      "Main effects training epoch: 84, train loss: 0.51963, val loss: 0.49934\n",
      "Main effects training epoch: 85, train loss: 0.51909, val loss: 0.50025\n",
      "Main effects training epoch: 86, train loss: 0.51904, val loss: 0.50076\n",
      "Main effects training epoch: 87, train loss: 0.51925, val loss: 0.50040\n",
      "Main effects training epoch: 88, train loss: 0.51897, val loss: 0.50031\n",
      "Main effects training epoch: 89, train loss: 0.51942, val loss: 0.50261\n",
      "Main effects training epoch: 90, train loss: 0.51911, val loss: 0.49982\n",
      "Main effects training epoch: 91, train loss: 0.51891, val loss: 0.50017\n",
      "Main effects training epoch: 92, train loss: 0.51891, val loss: 0.50141\n",
      "Main effects training epoch: 93, train loss: 0.51887, val loss: 0.50055\n",
      "Main effects training epoch: 94, train loss: 0.51907, val loss: 0.50027\n",
      "Main effects training epoch: 95, train loss: 0.51941, val loss: 0.50231\n",
      "Main effects training epoch: 96, train loss: 0.51846, val loss: 0.50176\n",
      "Main effects training epoch: 97, train loss: 0.51843, val loss: 0.50006\n",
      "Main effects training epoch: 98, train loss: 0.51838, val loss: 0.49999\n",
      "Main effects training epoch: 99, train loss: 0.51834, val loss: 0.50142\n",
      "Main effects training epoch: 100, train loss: 0.51847, val loss: 0.50053\n",
      "Main effects training epoch: 101, train loss: 0.51838, val loss: 0.50079\n",
      "Main effects training epoch: 102, train loss: 0.51852, val loss: 0.50204\n",
      "Main effects training epoch: 103, train loss: 0.51836, val loss: 0.50051\n",
      "Main effects training epoch: 104, train loss: 0.51827, val loss: 0.50023\n",
      "Main effects training epoch: 105, train loss: 0.51786, val loss: 0.50119\n",
      "Main effects training epoch: 106, train loss: 0.51799, val loss: 0.50043\n",
      "Main effects training epoch: 107, train loss: 0.51798, val loss: 0.50145\n",
      "Main effects training epoch: 108, train loss: 0.51819, val loss: 0.50105\n",
      "Main effects training epoch: 109, train loss: 0.51795, val loss: 0.50161\n",
      "Main effects training epoch: 110, train loss: 0.51783, val loss: 0.50083\n",
      "Main effects training epoch: 111, train loss: 0.51823, val loss: 0.50241\n",
      "Main effects training epoch: 112, train loss: 0.51762, val loss: 0.50108\n",
      "Main effects training epoch: 113, train loss: 0.51789, val loss: 0.50186\n",
      "Main effects training epoch: 114, train loss: 0.51803, val loss: 0.49895\n",
      "Main effects training epoch: 115, train loss: 0.51791, val loss: 0.50238\n",
      "Main effects training epoch: 116, train loss: 0.51751, val loss: 0.50067\n",
      "Main effects training epoch: 117, train loss: 0.51755, val loss: 0.50215\n",
      "Main effects training epoch: 118, train loss: 0.51756, val loss: 0.50126\n",
      "Main effects training epoch: 119, train loss: 0.51704, val loss: 0.50035\n",
      "Main effects training epoch: 120, train loss: 0.51692, val loss: 0.50155\n",
      "Main effects training epoch: 121, train loss: 0.51705, val loss: 0.50065\n",
      "Main effects training epoch: 122, train loss: 0.51756, val loss: 0.50268\n",
      "Main effects training epoch: 123, train loss: 0.51722, val loss: 0.50122\n",
      "Main effects training epoch: 124, train loss: 0.51688, val loss: 0.50206\n",
      "Main effects training epoch: 125, train loss: 0.51743, val loss: 0.49980\n",
      "Main effects training epoch: 126, train loss: 0.51813, val loss: 0.50439\n",
      "Main effects training epoch: 127, train loss: 0.51708, val loss: 0.50060\n",
      "Main effects training epoch: 128, train loss: 0.51787, val loss: 0.50087\n",
      "Main effects training epoch: 129, train loss: 0.51755, val loss: 0.50203\n",
      "Main effects training epoch: 130, train loss: 0.51720, val loss: 0.50299\n",
      "Main effects training epoch: 131, train loss: 0.51707, val loss: 0.49969\n",
      "Main effects training epoch: 132, train loss: 0.51715, val loss: 0.50283\n",
      "Main effects training epoch: 133, train loss: 0.51718, val loss: 0.50028\n",
      "Main effects training epoch: 134, train loss: 0.51725, val loss: 0.50297\n",
      "Main effects training epoch: 135, train loss: 0.51725, val loss: 0.50038\n",
      "Main effects training epoch: 136, train loss: 0.51648, val loss: 0.50180\n",
      "Main effects training epoch: 137, train loss: 0.51635, val loss: 0.50066\n",
      "Early stop at epoch 137, with validation loss: 0.50066\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51855, val loss: 0.50166\n",
      "Main effects tuning epoch: 2, train loss: 0.51845, val loss: 0.50307\n",
      "Main effects tuning epoch: 3, train loss: 0.51843, val loss: 0.50305\n",
      "Main effects tuning epoch: 4, train loss: 0.51832, val loss: 0.50174\n",
      "Main effects tuning epoch: 5, train loss: 0.51837, val loss: 0.50295\n",
      "Main effects tuning epoch: 6, train loss: 0.51816, val loss: 0.50161\n",
      "Main effects tuning epoch: 7, train loss: 0.51813, val loss: 0.50283\n",
      "Main effects tuning epoch: 8, train loss: 0.51888, val loss: 0.50520\n",
      "Main effects tuning epoch: 9, train loss: 0.51856, val loss: 0.50131\n",
      "Main effects tuning epoch: 10, train loss: 0.51828, val loss: 0.50403\n",
      "Main effects tuning epoch: 11, train loss: 0.51847, val loss: 0.50273\n",
      "Main effects tuning epoch: 12, train loss: 0.51889, val loss: 0.50386\n",
      "Main effects tuning epoch: 13, train loss: 0.51843, val loss: 0.50336\n",
      "Main effects tuning epoch: 14, train loss: 0.51866, val loss: 0.50253\n",
      "Main effects tuning epoch: 15, train loss: 0.51789, val loss: 0.50254\n",
      "Main effects tuning epoch: 16, train loss: 0.51799, val loss: 0.50185\n",
      "Main effects tuning epoch: 17, train loss: 0.51793, val loss: 0.50397\n",
      "Main effects tuning epoch: 18, train loss: 0.51855, val loss: 0.50105\n",
      "Main effects tuning epoch: 19, train loss: 0.51784, val loss: 0.50292\n",
      "Main effects tuning epoch: 20, train loss: 0.51777, val loss: 0.50253\n",
      "Main effects tuning epoch: 21, train loss: 0.51790, val loss: 0.50236\n",
      "Main effects tuning epoch: 22, train loss: 0.51816, val loss: 0.50376\n",
      "Main effects tuning epoch: 23, train loss: 0.51790, val loss: 0.50156\n",
      "Main effects tuning epoch: 24, train loss: 0.51778, val loss: 0.50280\n",
      "Main effects tuning epoch: 25, train loss: 0.51833, val loss: 0.50231\n",
      "Main effects tuning epoch: 26, train loss: 0.51843, val loss: 0.50547\n",
      "Main effects tuning epoch: 27, train loss: 0.51854, val loss: 0.50169\n",
      "Main effects tuning epoch: 28, train loss: 0.51782, val loss: 0.50333\n",
      "Main effects tuning epoch: 29, train loss: 0.51764, val loss: 0.50356\n",
      "Main effects tuning epoch: 30, train loss: 0.51748, val loss: 0.50225\n",
      "Main effects tuning epoch: 31, train loss: 0.51750, val loss: 0.50196\n",
      "Main effects tuning epoch: 32, train loss: 0.51758, val loss: 0.50222\n",
      "Main effects tuning epoch: 33, train loss: 0.51749, val loss: 0.50339\n",
      "Main effects tuning epoch: 34, train loss: 0.51767, val loss: 0.50322\n",
      "Main effects tuning epoch: 35, train loss: 0.51799, val loss: 0.50256\n",
      "Main effects tuning epoch: 36, train loss: 0.51786, val loss: 0.50265\n",
      "Main effects tuning epoch: 37, train loss: 0.51757, val loss: 0.50389\n",
      "Main effects tuning epoch: 38, train loss: 0.51728, val loss: 0.50189\n",
      "Main effects tuning epoch: 39, train loss: 0.51737, val loss: 0.50152\n",
      "Main effects tuning epoch: 40, train loss: 0.51744, val loss: 0.50396\n",
      "Main effects tuning epoch: 41, train loss: 0.51714, val loss: 0.50191\n",
      "Main effects tuning epoch: 42, train loss: 0.51758, val loss: 0.50251\n",
      "Main effects tuning epoch: 43, train loss: 0.51728, val loss: 0.50302\n",
      "Main effects tuning epoch: 44, train loss: 0.51804, val loss: 0.50429\n",
      "Main effects tuning epoch: 45, train loss: 0.51803, val loss: 0.50137\n",
      "Main effects tuning epoch: 46, train loss: 0.51747, val loss: 0.50403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects tuning epoch: 47, train loss: 0.51755, val loss: 0.50142\n",
      "Main effects tuning epoch: 48, train loss: 0.51772, val loss: 0.50453\n",
      "Main effects tuning epoch: 49, train loss: 0.51715, val loss: 0.50252\n",
      "Main effects tuning epoch: 50, train loss: 0.51729, val loss: 0.50187\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.51469, val loss: 0.50133\n",
      "Interaction training epoch: 2, train loss: 0.48068, val loss: 0.47493\n",
      "Interaction training epoch: 3, train loss: 0.32842, val loss: 0.33273\n",
      "Interaction training epoch: 4, train loss: 0.32971, val loss: 0.33957\n",
      "Interaction training epoch: 5, train loss: 0.30206, val loss: 0.30869\n",
      "Interaction training epoch: 6, train loss: 0.29557, val loss: 0.29638\n",
      "Interaction training epoch: 7, train loss: 0.30085, val loss: 0.30032\n",
      "Interaction training epoch: 8, train loss: 0.29227, val loss: 0.29250\n",
      "Interaction training epoch: 9, train loss: 0.28816, val loss: 0.28743\n",
      "Interaction training epoch: 10, train loss: 0.28743, val loss: 0.28644\n",
      "Interaction training epoch: 11, train loss: 0.28682, val loss: 0.28598\n",
      "Interaction training epoch: 12, train loss: 0.28163, val loss: 0.28431\n",
      "Interaction training epoch: 13, train loss: 0.28974, val loss: 0.29020\n",
      "Interaction training epoch: 14, train loss: 0.28207, val loss: 0.28547\n",
      "Interaction training epoch: 15, train loss: 0.27930, val loss: 0.28344\n",
      "Interaction training epoch: 16, train loss: 0.27951, val loss: 0.28370\n",
      "Interaction training epoch: 17, train loss: 0.27897, val loss: 0.28139\n",
      "Interaction training epoch: 18, train loss: 0.27628, val loss: 0.28156\n",
      "Interaction training epoch: 19, train loss: 0.28772, val loss: 0.29425\n",
      "Interaction training epoch: 20, train loss: 0.27491, val loss: 0.28094\n",
      "Interaction training epoch: 21, train loss: 0.28124, val loss: 0.28641\n",
      "Interaction training epoch: 22, train loss: 0.27776, val loss: 0.28491\n",
      "Interaction training epoch: 23, train loss: 0.27793, val loss: 0.28287\n",
      "Interaction training epoch: 24, train loss: 0.27250, val loss: 0.27862\n",
      "Interaction training epoch: 25, train loss: 0.27429, val loss: 0.28100\n",
      "Interaction training epoch: 26, train loss: 0.27761, val loss: 0.28084\n",
      "Interaction training epoch: 27, train loss: 0.27852, val loss: 0.28739\n",
      "Interaction training epoch: 28, train loss: 0.27356, val loss: 0.28105\n",
      "Interaction training epoch: 29, train loss: 0.27301, val loss: 0.27930\n",
      "Interaction training epoch: 30, train loss: 0.27478, val loss: 0.28167\n",
      "Interaction training epoch: 31, train loss: 0.27237, val loss: 0.27645\n",
      "Interaction training epoch: 32, train loss: 0.27675, val loss: 0.28478\n",
      "Interaction training epoch: 33, train loss: 0.26937, val loss: 0.28073\n",
      "Interaction training epoch: 34, train loss: 0.26922, val loss: 0.27384\n",
      "Interaction training epoch: 35, train loss: 0.27181, val loss: 0.28105\n",
      "Interaction training epoch: 36, train loss: 0.27096, val loss: 0.28347\n",
      "Interaction training epoch: 37, train loss: 0.27593, val loss: 0.28398\n",
      "Interaction training epoch: 38, train loss: 0.27012, val loss: 0.27951\n",
      "Interaction training epoch: 39, train loss: 0.27130, val loss: 0.27964\n",
      "Interaction training epoch: 40, train loss: 0.27263, val loss: 0.28043\n",
      "Interaction training epoch: 41, train loss: 0.26542, val loss: 0.27197\n",
      "Interaction training epoch: 42, train loss: 0.26977, val loss: 0.27977\n",
      "Interaction training epoch: 43, train loss: 0.26936, val loss: 0.27655\n",
      "Interaction training epoch: 44, train loss: 0.26747, val loss: 0.27844\n",
      "Interaction training epoch: 45, train loss: 0.26699, val loss: 0.27682\n",
      "Interaction training epoch: 46, train loss: 0.26656, val loss: 0.27790\n",
      "Interaction training epoch: 47, train loss: 0.26459, val loss: 0.27409\n",
      "Interaction training epoch: 48, train loss: 0.26754, val loss: 0.27834\n",
      "Interaction training epoch: 49, train loss: 0.26535, val loss: 0.27655\n",
      "Interaction training epoch: 50, train loss: 0.26314, val loss: 0.27637\n",
      "Interaction training epoch: 51, train loss: 0.26315, val loss: 0.27500\n",
      "Interaction training epoch: 52, train loss: 0.26645, val loss: 0.27819\n",
      "Interaction training epoch: 53, train loss: 0.26483, val loss: 0.27923\n",
      "Interaction training epoch: 54, train loss: 0.26349, val loss: 0.27477\n",
      "Interaction training epoch: 55, train loss: 0.26753, val loss: 0.27803\n",
      "Interaction training epoch: 56, train loss: 0.26200, val loss: 0.27533\n",
      "Interaction training epoch: 57, train loss: 0.26476, val loss: 0.28103\n",
      "Interaction training epoch: 58, train loss: 0.26396, val loss: 0.27873\n",
      "Interaction training epoch: 59, train loss: 0.26175, val loss: 0.27314\n",
      "Interaction training epoch: 60, train loss: 0.26011, val loss: 0.27245\n",
      "Interaction training epoch: 61, train loss: 0.26029, val loss: 0.27751\n",
      "Interaction training epoch: 62, train loss: 0.26185, val loss: 0.27689\n",
      "Interaction training epoch: 63, train loss: 0.25880, val loss: 0.27342\n",
      "Interaction training epoch: 64, train loss: 0.25975, val loss: 0.27600\n",
      "Interaction training epoch: 65, train loss: 0.26059, val loss: 0.27560\n",
      "Interaction training epoch: 66, train loss: 0.25900, val loss: 0.27392\n",
      "Interaction training epoch: 67, train loss: 0.26015, val loss: 0.27986\n",
      "Interaction training epoch: 68, train loss: 0.25713, val loss: 0.27150\n",
      "Interaction training epoch: 69, train loss: 0.25689, val loss: 0.27098\n",
      "Interaction training epoch: 70, train loss: 0.25826, val loss: 0.27461\n",
      "Interaction training epoch: 71, train loss: 0.25849, val loss: 0.27781\n",
      "Interaction training epoch: 72, train loss: 0.26118, val loss: 0.27369\n",
      "Interaction training epoch: 73, train loss: 0.25637, val loss: 0.27696\n",
      "Interaction training epoch: 74, train loss: 0.25497, val loss: 0.27042\n",
      "Interaction training epoch: 75, train loss: 0.25689, val loss: 0.27519\n",
      "Interaction training epoch: 76, train loss: 0.25579, val loss: 0.27010\n",
      "Interaction training epoch: 77, train loss: 0.25566, val loss: 0.27372\n",
      "Interaction training epoch: 78, train loss: 0.25827, val loss: 0.27336\n",
      "Interaction training epoch: 79, train loss: 0.25612, val loss: 0.27411\n",
      "Interaction training epoch: 80, train loss: 0.25683, val loss: 0.27374\n",
      "Interaction training epoch: 81, train loss: 0.25346, val loss: 0.27227\n",
      "Interaction training epoch: 82, train loss: 0.25224, val loss: 0.27231\n",
      "Interaction training epoch: 83, train loss: 0.25625, val loss: 0.27285\n",
      "Interaction training epoch: 84, train loss: 0.25464, val loss: 0.27260\n",
      "Interaction training epoch: 85, train loss: 0.25179, val loss: 0.26866\n",
      "Interaction training epoch: 86, train loss: 0.25572, val loss: 0.27726\n",
      "Interaction training epoch: 87, train loss: 0.25181, val loss: 0.26699\n",
      "Interaction training epoch: 88, train loss: 0.25012, val loss: 0.26785\n",
      "Interaction training epoch: 89, train loss: 0.25107, val loss: 0.26633\n",
      "Interaction training epoch: 90, train loss: 0.25127, val loss: 0.27077\n",
      "Interaction training epoch: 91, train loss: 0.25191, val loss: 0.26989\n",
      "Interaction training epoch: 92, train loss: 0.25485, val loss: 0.27602\n",
      "Interaction training epoch: 93, train loss: 0.25032, val loss: 0.27023\n",
      "Interaction training epoch: 94, train loss: 0.25077, val loss: 0.27053\n",
      "Interaction training epoch: 95, train loss: 0.25265, val loss: 0.27262\n",
      "Interaction training epoch: 96, train loss: 0.24933, val loss: 0.26910\n",
      "Interaction training epoch: 97, train loss: 0.24999, val loss: 0.26944\n",
      "Interaction training epoch: 98, train loss: 0.24929, val loss: 0.27193\n",
      "Interaction training epoch: 99, train loss: 0.25401, val loss: 0.27672\n",
      "Interaction training epoch: 100, train loss: 0.24754, val loss: 0.27230\n",
      "Interaction training epoch: 101, train loss: 0.25154, val loss: 0.27402\n",
      "Interaction training epoch: 102, train loss: 0.25076, val loss: 0.27289\n",
      "Interaction training epoch: 103, train loss: 0.24768, val loss: 0.27136\n",
      "Interaction training epoch: 104, train loss: 0.24698, val loss: 0.26900\n",
      "Interaction training epoch: 105, train loss: 0.24938, val loss: 0.27299\n",
      "Interaction training epoch: 106, train loss: 0.24677, val loss: 0.27000\n",
      "Interaction training epoch: 107, train loss: 0.25296, val loss: 0.27476\n",
      "Interaction training epoch: 108, train loss: 0.24738, val loss: 0.27547\n",
      "Interaction training epoch: 109, train loss: 0.24933, val loss: 0.27092\n",
      "Interaction training epoch: 110, train loss: 0.24850, val loss: 0.27049\n",
      "Interaction training epoch: 111, train loss: 0.24518, val loss: 0.27207\n",
      "Interaction training epoch: 112, train loss: 0.24556, val loss: 0.26842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 113, train loss: 0.24408, val loss: 0.26809\n",
      "Interaction training epoch: 114, train loss: 0.24449, val loss: 0.26810\n",
      "Interaction training epoch: 115, train loss: 0.24380, val loss: 0.26777\n",
      "Interaction training epoch: 116, train loss: 0.24233, val loss: 0.26786\n",
      "Interaction training epoch: 117, train loss: 0.24695, val loss: 0.27148\n",
      "Interaction training epoch: 118, train loss: 0.24528, val loss: 0.27116\n",
      "Interaction training epoch: 119, train loss: 0.24336, val loss: 0.27055\n",
      "Interaction training epoch: 120, train loss: 0.24698, val loss: 0.27363\n",
      "Interaction training epoch: 121, train loss: 0.24423, val loss: 0.26998\n",
      "Interaction training epoch: 122, train loss: 0.24514, val loss: 0.26914\n",
      "Interaction training epoch: 123, train loss: 0.24836, val loss: 0.28154\n",
      "Interaction training epoch: 124, train loss: 0.24534, val loss: 0.27182\n",
      "Interaction training epoch: 125, train loss: 0.24446, val loss: 0.26797\n",
      "Interaction training epoch: 126, train loss: 0.24334, val loss: 0.27445\n",
      "Interaction training epoch: 127, train loss: 0.24326, val loss: 0.26665\n",
      "Interaction training epoch: 128, train loss: 0.24399, val loss: 0.27269\n",
      "Interaction training epoch: 129, train loss: 0.24432, val loss: 0.26911\n",
      "Interaction training epoch: 130, train loss: 0.24043, val loss: 0.26859\n",
      "Interaction training epoch: 131, train loss: 0.24051, val loss: 0.27185\n",
      "Interaction training epoch: 132, train loss: 0.24256, val loss: 0.27034\n",
      "Interaction training epoch: 133, train loss: 0.24454, val loss: 0.27768\n",
      "Interaction training epoch: 134, train loss: 0.24234, val loss: 0.26804\n",
      "Interaction training epoch: 135, train loss: 0.24341, val loss: 0.27871\n",
      "Interaction training epoch: 136, train loss: 0.24203, val loss: 0.26874\n",
      "Interaction training epoch: 137, train loss: 0.23998, val loss: 0.27129\n",
      "Interaction training epoch: 138, train loss: 0.24112, val loss: 0.27023\n",
      "Interaction training epoch: 139, train loss: 0.23912, val loss: 0.26943\n",
      "Interaction training epoch: 140, train loss: 0.23951, val loss: 0.26812\n",
      "Interaction training epoch: 141, train loss: 0.24356, val loss: 0.27270\n",
      "Interaction training epoch: 142, train loss: 0.23765, val loss: 0.26829\n",
      "Interaction training epoch: 143, train loss: 0.24032, val loss: 0.26800\n",
      "Interaction training epoch: 144, train loss: 0.23998, val loss: 0.27671\n",
      "Interaction training epoch: 145, train loss: 0.24146, val loss: 0.26878\n",
      "Interaction training epoch: 146, train loss: 0.24048, val loss: 0.27276\n",
      "Interaction training epoch: 147, train loss: 0.23809, val loss: 0.26341\n",
      "Interaction training epoch: 148, train loss: 0.23911, val loss: 0.27322\n",
      "Interaction training epoch: 149, train loss: 0.23517, val loss: 0.26123\n",
      "Interaction training epoch: 150, train loss: 0.24080, val loss: 0.27309\n",
      "Interaction training epoch: 151, train loss: 0.23847, val loss: 0.26822\n",
      "Interaction training epoch: 152, train loss: 0.23690, val loss: 0.26680\n",
      "Interaction training epoch: 153, train loss: 0.23950, val loss: 0.26998\n",
      "Interaction training epoch: 154, train loss: 0.23468, val loss: 0.26626\n",
      "Interaction training epoch: 155, train loss: 0.23742, val loss: 0.26611\n",
      "Interaction training epoch: 156, train loss: 0.23712, val loss: 0.27043\n",
      "Interaction training epoch: 157, train loss: 0.23612, val loss: 0.27034\n",
      "Interaction training epoch: 158, train loss: 0.23435, val loss: 0.26144\n",
      "Interaction training epoch: 159, train loss: 0.23870, val loss: 0.27342\n",
      "Interaction training epoch: 160, train loss: 0.23900, val loss: 0.27348\n",
      "Interaction training epoch: 161, train loss: 0.23610, val loss: 0.26839\n",
      "Interaction training epoch: 162, train loss: 0.23371, val loss: 0.26686\n",
      "Interaction training epoch: 163, train loss: 0.23633, val loss: 0.26591\n",
      "Interaction training epoch: 164, train loss: 0.23597, val loss: 0.27261\n",
      "Interaction training epoch: 165, train loss: 0.23545, val loss: 0.26814\n",
      "Interaction training epoch: 166, train loss: 0.23607, val loss: 0.26925\n",
      "Interaction training epoch: 167, train loss: 0.23377, val loss: 0.26665\n",
      "Interaction training epoch: 168, train loss: 0.23393, val loss: 0.26651\n",
      "Interaction training epoch: 169, train loss: 0.23131, val loss: 0.27100\n",
      "Interaction training epoch: 170, train loss: 0.23564, val loss: 0.26627\n",
      "Interaction training epoch: 171, train loss: 0.23470, val loss: 0.27160\n",
      "Interaction training epoch: 172, train loss: 0.23212, val loss: 0.26766\n",
      "Interaction training epoch: 173, train loss: 0.23537, val loss: 0.26788\n",
      "Interaction training epoch: 174, train loss: 0.23284, val loss: 0.26880\n",
      "Interaction training epoch: 175, train loss: 0.23351, val loss: 0.26690\n",
      "Interaction training epoch: 176, train loss: 0.23164, val loss: 0.26647\n",
      "Interaction training epoch: 177, train loss: 0.23091, val loss: 0.26460\n",
      "Interaction training epoch: 178, train loss: 0.23556, val loss: 0.26920\n",
      "Interaction training epoch: 179, train loss: 0.23060, val loss: 0.26705\n",
      "Interaction training epoch: 180, train loss: 0.23653, val loss: 0.27203\n",
      "Interaction training epoch: 181, train loss: 0.23132, val loss: 0.26459\n",
      "Interaction training epoch: 182, train loss: 0.23295, val loss: 0.27122\n",
      "Interaction training epoch: 183, train loss: 0.23079, val loss: 0.26490\n",
      "Interaction training epoch: 184, train loss: 0.23083, val loss: 0.27005\n",
      "Interaction training epoch: 185, train loss: 0.23404, val loss: 0.26816\n",
      "Interaction training epoch: 186, train loss: 0.23155, val loss: 0.26731\n",
      "Interaction training epoch: 187, train loss: 0.23724, val loss: 0.27022\n",
      "Interaction training epoch: 188, train loss: 0.23371, val loss: 0.27300\n",
      "Interaction training epoch: 189, train loss: 0.23038, val loss: 0.26660\n",
      "Interaction training epoch: 190, train loss: 0.23505, val loss: 0.27235\n",
      "Interaction training epoch: 191, train loss: 0.23218, val loss: 0.26357\n",
      "Interaction training epoch: 192, train loss: 0.23313, val loss: 0.26740\n",
      "Interaction training epoch: 193, train loss: 0.23404, val loss: 0.27109\n",
      "Interaction training epoch: 194, train loss: 0.23090, val loss: 0.26485\n",
      "Interaction training epoch: 195, train loss: 0.22882, val loss: 0.26437\n",
      "Interaction training epoch: 196, train loss: 0.23089, val loss: 0.27193\n",
      "Interaction training epoch: 197, train loss: 0.23023, val loss: 0.26383\n",
      "Interaction training epoch: 198, train loss: 0.22812, val loss: 0.26655\n",
      "Interaction training epoch: 199, train loss: 0.22879, val loss: 0.26474\n",
      "Interaction training epoch: 200, train loss: 0.22784, val loss: 0.26738\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########2 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.23184, val loss: 0.26165\n",
      "Interaction tuning epoch: 2, train loss: 0.23090, val loss: 0.26900\n",
      "Interaction tuning epoch: 3, train loss: 0.23137, val loss: 0.26622\n",
      "Interaction tuning epoch: 4, train loss: 0.23625, val loss: 0.27299\n",
      "Interaction tuning epoch: 5, train loss: 0.23109, val loss: 0.26953\n",
      "Interaction tuning epoch: 6, train loss: 0.23020, val loss: 0.25812\n",
      "Interaction tuning epoch: 7, train loss: 0.23606, val loss: 0.27894\n",
      "Interaction tuning epoch: 8, train loss: 0.23437, val loss: 0.26633\n",
      "Interaction tuning epoch: 9, train loss: 0.23450, val loss: 0.26824\n",
      "Interaction tuning epoch: 10, train loss: 0.23388, val loss: 0.26498\n",
      "Interaction tuning epoch: 11, train loss: 0.23289, val loss: 0.27114\n",
      "Interaction tuning epoch: 12, train loss: 0.23269, val loss: 0.26160\n",
      "Interaction tuning epoch: 13, train loss: 0.22949, val loss: 0.26529\n",
      "Interaction tuning epoch: 14, train loss: 0.23093, val loss: 0.26783\n",
      "Interaction tuning epoch: 15, train loss: 0.22826, val loss: 0.26382\n",
      "Interaction tuning epoch: 16, train loss: 0.23212, val loss: 0.26901\n",
      "Interaction tuning epoch: 17, train loss: 0.22744, val loss: 0.26112\n",
      "Interaction tuning epoch: 18, train loss: 0.23370, val loss: 0.26871\n",
      "Interaction tuning epoch: 19, train loss: 0.22930, val loss: 0.26431\n",
      "Interaction tuning epoch: 20, train loss: 0.22917, val loss: 0.26676\n",
      "Interaction tuning epoch: 21, train loss: 0.22991, val loss: 0.26557\n",
      "Interaction tuning epoch: 22, train loss: 0.23025, val loss: 0.26762\n",
      "Interaction tuning epoch: 23, train loss: 0.23072, val loss: 0.27030\n",
      "Interaction tuning epoch: 24, train loss: 0.23207, val loss: 0.26917\n",
      "Interaction tuning epoch: 25, train loss: 0.22900, val loss: 0.26533\n",
      "Interaction tuning epoch: 26, train loss: 0.22691, val loss: 0.26923\n",
      "Interaction tuning epoch: 27, train loss: 0.23313, val loss: 0.26838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 28, train loss: 0.22994, val loss: 0.26491\n",
      "Interaction tuning epoch: 29, train loss: 0.22943, val loss: 0.26662\n",
      "Interaction tuning epoch: 30, train loss: 0.23047, val loss: 0.26579\n",
      "Interaction tuning epoch: 31, train loss: 0.23160, val loss: 0.27323\n",
      "Interaction tuning epoch: 32, train loss: 0.22782, val loss: 0.26329\n",
      "Interaction tuning epoch: 33, train loss: 0.22939, val loss: 0.26797\n",
      "Interaction tuning epoch: 34, train loss: 0.22993, val loss: 0.26637\n",
      "Interaction tuning epoch: 35, train loss: 0.22897, val loss: 0.26534\n",
      "Interaction tuning epoch: 36, train loss: 0.23418, val loss: 0.27751\n",
      "Interaction tuning epoch: 37, train loss: 0.22993, val loss: 0.26602\n",
      "Interaction tuning epoch: 38, train loss: 0.22818, val loss: 0.26832\n",
      "Interaction tuning epoch: 39, train loss: 0.22958, val loss: 0.26312\n",
      "Interaction tuning epoch: 40, train loss: 0.22967, val loss: 0.26939\n",
      "Interaction tuning epoch: 41, train loss: 0.22760, val loss: 0.26746\n",
      "Interaction tuning epoch: 42, train loss: 0.22656, val loss: 0.26815\n",
      "Interaction tuning epoch: 43, train loss: 0.22521, val loss: 0.25891\n",
      "Interaction tuning epoch: 44, train loss: 0.22819, val loss: 0.27215\n",
      "Interaction tuning epoch: 45, train loss: 0.22520, val loss: 0.26283\n",
      "Interaction tuning epoch: 46, train loss: 0.22741, val loss: 0.26464\n",
      "Interaction tuning epoch: 47, train loss: 0.22647, val loss: 0.27053\n",
      "Interaction tuning epoch: 48, train loss: 0.22620, val loss: 0.26175\n",
      "Interaction tuning epoch: 49, train loss: 0.23034, val loss: 0.27254\n",
      "Interaction tuning epoch: 50, train loss: 0.22535, val loss: 0.26682\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 52.66181969642639\n",
      "After the gam stage, training error is 0.22535 , validation error is 0.26682\n",
      "missing value counts: 99158\n",
      "[SoftImpute] Max Singular Value of X_init = 3.606640\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed BCE=0.187533 validation BCE=0.286980,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 2: observed BCE=0.183949 validation BCE=0.295659,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.181930 validation BCE=0.294369,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.180519 validation BCE=0.285884,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.179886 validation BCE=0.283332,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.179375 validation BCE=0.281051,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.178869 validation BCE=0.269090,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.178491 validation BCE=0.268355,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.178222 validation BCE=0.266847,rank=5\n",
      "[SoftImpute] Iter 10: observed BCE=0.178215 validation BCE=0.265997,rank=5\n",
      "[SoftImpute] Iter 11: observed BCE=0.177737 validation BCE=0.265122,rank=5\n",
      "[SoftImpute] Iter 12: observed BCE=0.177622 validation BCE=0.264261,rank=5\n",
      "[SoftImpute] Iter 13: observed BCE=0.177460 validation BCE=0.264001,rank=5\n",
      "[SoftImpute] Iter 14: observed BCE=0.177289 validation BCE=0.263196,rank=5\n",
      "[SoftImpute] Iter 15: observed BCE=0.177117 validation BCE=0.263281,rank=5\n",
      "[SoftImpute] Iter 16: observed BCE=0.177054 validation BCE=0.262165,rank=5\n",
      "[SoftImpute] Iter 17: observed BCE=0.176870 validation BCE=0.262403,rank=5\n",
      "[SoftImpute] Iter 18: observed BCE=0.176953 validation BCE=0.261676,rank=5\n",
      "[SoftImpute] Iter 19: observed BCE=0.176959 validation BCE=0.261698,rank=5\n",
      "[SoftImpute] Iter 20: observed BCE=0.176551 validation BCE=0.261367,rank=5\n",
      "[SoftImpute] Iter 21: observed BCE=0.176581 validation BCE=0.261071,rank=5\n",
      "[SoftImpute] Iter 22: observed BCE=0.177293 validation BCE=0.260893,rank=5\n",
      "[SoftImpute] Iter 23: observed BCE=0.178262 validation BCE=0.260840,rank=5\n",
      "[SoftImpute] Iter 24: observed BCE=0.178565 validation BCE=0.260482,rank=5\n",
      "[SoftImpute] Iter 25: observed BCE=0.178588 validation BCE=0.260255,rank=5\n",
      "[SoftImpute] Iter 26: observed BCE=0.178550 validation BCE=0.260090,rank=5\n",
      "[SoftImpute] Iter 27: observed BCE=0.178576 validation BCE=0.260552,rank=5\n",
      "[SoftImpute] Iter 28: observed BCE=0.178719 validation BCE=0.259680,rank=5\n",
      "[SoftImpute] Iter 29: observed BCE=0.178639 validation BCE=0.259916,rank=5\n",
      "[SoftImpute] Iter 30: observed BCE=0.178566 validation BCE=0.259360,rank=5\n",
      "[SoftImpute] Iter 31: observed BCE=0.178366 validation BCE=0.259950,rank=5\n",
      "[SoftImpute] Iter 32: observed BCE=0.178489 validation BCE=0.259095,rank=5\n",
      "[SoftImpute] Iter 33: observed BCE=0.178634 validation BCE=0.259708,rank=5\n",
      "[SoftImpute] Iter 34: observed BCE=0.178767 validation BCE=0.259553,rank=5\n",
      "[SoftImpute] Iter 35: observed BCE=0.178449 validation BCE=0.259130,rank=5\n",
      "[SoftImpute] Iter 36: observed BCE=0.178350 validation BCE=0.259173,rank=5\n",
      "[SoftImpute] Iter 37: observed BCE=0.178433 validation BCE=0.259254,rank=5\n",
      "[SoftImpute] Iter 38: observed BCE=0.178362 validation BCE=0.259244,rank=5\n",
      "[SoftImpute] Iter 39: observed BCE=0.178873 validation BCE=0.259701,rank=5\n",
      "[SoftImpute] Iter 40: observed BCE=0.178765 validation BCE=0.259644,rank=5\n",
      "[SoftImpute] Iter 41: observed BCE=0.178590 validation BCE=0.259621,rank=5\n",
      "[SoftImpute] Iter 42: observed BCE=0.178765 validation BCE=0.259733,rank=5\n",
      "[SoftImpute] Iter 43: observed BCE=0.178551 validation BCE=0.260004,rank=5\n",
      "[SoftImpute] Iter 44: observed BCE=0.178655 validation BCE=0.259851,rank=5\n",
      "[SoftImpute] Iter 45: observed BCE=0.178441 validation BCE=0.259959,rank=5\n",
      "[SoftImpute] Iter 46: observed BCE=0.178578 validation BCE=0.259964,rank=5\n",
      "[SoftImpute] Iter 47: observed BCE=0.178410 validation BCE=0.259795,rank=5\n",
      "[SoftImpute] Iter 48: observed BCE=0.178461 validation BCE=0.260013,rank=5\n",
      "[SoftImpute] Iter 49: observed BCE=0.178438 validation BCE=0.260045,rank=5\n",
      "[SoftImpute] Iter 50: observed BCE=0.178469 validation BCE=0.260035,rank=5\n",
      "[SoftImpute] Iter 51: observed BCE=0.178514 validation BCE=0.260193,rank=5\n",
      "[SoftImpute] Iter 52: observed BCE=0.178649 validation BCE=0.260369,rank=5\n",
      "[SoftImpute] Iter 53: observed BCE=0.178419 validation BCE=0.260363,rank=5\n",
      "[SoftImpute] Iter 54: observed BCE=0.178394 validation BCE=0.260362,rank=5\n",
      "[SoftImpute] Iter 55: observed BCE=0.178401 validation BCE=0.260289,rank=5\n",
      "[SoftImpute] Iter 56: observed BCE=0.178258 validation BCE=0.260242,rank=5\n",
      "[SoftImpute] Iter 57: observed BCE=0.178196 validation BCE=0.260475,rank=5\n",
      "[SoftImpute] Iter 58: observed BCE=0.178382 validation BCE=0.260337,rank=5\n",
      "[SoftImpute] Iter 59: observed BCE=0.178306 validation BCE=0.260291,rank=5\n",
      "[SoftImpute] Iter 60: observed BCE=0.179075 validation BCE=0.260543,rank=5\n",
      "[SoftImpute] Iter 61: observed BCE=0.179502 validation BCE=0.260329,rank=5\n",
      "[SoftImpute] Iter 62: observed BCE=0.179758 validation BCE=0.260726,rank=5\n",
      "[SoftImpute] Iter 63: observed BCE=0.179630 validation BCE=0.260594,rank=5\n",
      "[SoftImpute] Iter 64: observed BCE=0.179577 validation BCE=0.260799,rank=5\n",
      "[SoftImpute] Iter 65: observed BCE=0.179512 validation BCE=0.260826,rank=5\n",
      "[SoftImpute] Iter 66: observed BCE=0.179616 validation BCE=0.260825,rank=5\n",
      "[SoftImpute] Iter 67: observed BCE=0.179427 validation BCE=0.260687,rank=5\n",
      "[SoftImpute] Iter 68: observed BCE=0.179423 validation BCE=0.260804,rank=5\n",
      "[SoftImpute] Iter 69: observed BCE=0.179385 validation BCE=0.260290,rank=5\n",
      "[SoftImpute] Iter 70: observed BCE=0.179656 validation BCE=0.260532,rank=5\n",
      "[SoftImpute] Iter 71: observed BCE=0.179310 validation BCE=0.260766,rank=5\n",
      "[SoftImpute] Iter 72: observed BCE=0.179379 validation BCE=0.260729,rank=5\n",
      "[SoftImpute] Iter 73: observed BCE=0.179333 validation BCE=0.260633,rank=5\n",
      "[SoftImpute] Iter 74: observed BCE=0.179570 validation BCE=0.260476,rank=5\n",
      "[SoftImpute] Iter 75: observed BCE=0.179620 validation BCE=0.260513,rank=5\n",
      "[SoftImpute] Iter 76: observed BCE=0.179604 validation BCE=0.260949,rank=5\n",
      "[SoftImpute] Iter 77: observed BCE=0.179354 validation BCE=0.260773,rank=5\n",
      "[SoftImpute] Iter 78: observed BCE=0.179686 validation BCE=0.260571,rank=5\n",
      "[SoftImpute] Iter 79: observed BCE=0.179422 validation BCE=0.260559,rank=5\n",
      "[SoftImpute] Iter 80: observed BCE=0.179624 validation BCE=0.260525,rank=5\n",
      "[SoftImpute] Iter 81: observed BCE=0.179496 validation BCE=0.260670,rank=5\n",
      "[SoftImpute] Iter 82: observed BCE=0.179675 validation BCE=0.260622,rank=5\n",
      "[SoftImpute] Iter 83: observed BCE=0.179293 validation BCE=0.260373,rank=5\n",
      "[SoftImpute] Iter 84: observed BCE=0.179270 validation BCE=0.260658,rank=5\n",
      "[SoftImpute] Iter 85: observed BCE=0.179154 validation BCE=0.260400,rank=5\n",
      "[SoftImpute] Iter 86: observed BCE=0.179627 validation BCE=0.260774,rank=5\n",
      "[SoftImpute] Iter 87: observed BCE=0.179661 validation BCE=0.260205,rank=5\n",
      "[SoftImpute] Iter 88: observed BCE=0.179554 validation BCE=0.260513,rank=5\n",
      "[SoftImpute] Iter 89: observed BCE=0.179280 validation BCE=0.260548,rank=5\n",
      "[SoftImpute] Iter 90: observed BCE=0.179609 validation BCE=0.260344,rank=5\n",
      "[SoftImpute] Iter 91: observed BCE=0.179420 validation BCE=0.260639,rank=5\n",
      "[SoftImpute] Iter 92: observed BCE=0.179182 validation BCE=0.260060,rank=5\n",
      "[SoftImpute] Iter 93: observed BCE=0.179274 validation BCE=0.260556,rank=5\n",
      "[SoftImpute] Iter 94: observed BCE=0.179525 validation BCE=0.260478,rank=5\n",
      "[SoftImpute] Iter 95: observed BCE=0.179379 validation BCE=0.260582,rank=5\n",
      "[SoftImpute] Iter 96: observed BCE=0.179412 validation BCE=0.260600,rank=5\n",
      "[SoftImpute] Iter 97: observed BCE=0.179376 validation BCE=0.260692,rank=5\n",
      "[SoftImpute] Stopped after iteration 97 for lambda=0.072133\n",
      "final num of user group: 5\n",
      "final num of item group: 9\n",
      "change mode state : True\n",
      "time cost: 7.181362867355347\n",
      "After the matrix factor stage, training error is 0.17938, validation error is 0.26069\n",
      "3\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68388, val loss: 0.68292\n",
      "Main effects training epoch: 2, train loss: 0.67731, val loss: 0.67630\n",
      "Main effects training epoch: 3, train loss: 0.67199, val loss: 0.67326\n",
      "Main effects training epoch: 4, train loss: 0.66734, val loss: 0.66904\n",
      "Main effects training epoch: 5, train loss: 0.65726, val loss: 0.65872\n",
      "Main effects training epoch: 6, train loss: 0.63659, val loss: 0.63784\n",
      "Main effects training epoch: 7, train loss: 0.60391, val loss: 0.60566\n",
      "Main effects training epoch: 8, train loss: 0.57596, val loss: 0.57251\n",
      "Main effects training epoch: 9, train loss: 0.54631, val loss: 0.53357\n",
      "Main effects training epoch: 10, train loss: 0.53272, val loss: 0.50907\n",
      "Main effects training epoch: 11, train loss: 0.52736, val loss: 0.50138\n",
      "Main effects training epoch: 12, train loss: 0.52610, val loss: 0.50368\n",
      "Main effects training epoch: 13, train loss: 0.52367, val loss: 0.50261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 14, train loss: 0.52399, val loss: 0.50130\n",
      "Main effects training epoch: 15, train loss: 0.52287, val loss: 0.50005\n",
      "Main effects training epoch: 16, train loss: 0.52235, val loss: 0.49976\n",
      "Main effects training epoch: 17, train loss: 0.52426, val loss: 0.50207\n",
      "Main effects training epoch: 18, train loss: 0.52414, val loss: 0.50170\n",
      "Main effects training epoch: 19, train loss: 0.52361, val loss: 0.50042\n",
      "Main effects training epoch: 20, train loss: 0.52229, val loss: 0.49973\n",
      "Main effects training epoch: 21, train loss: 0.52211, val loss: 0.50016\n",
      "Main effects training epoch: 22, train loss: 0.52197, val loss: 0.49827\n",
      "Main effects training epoch: 23, train loss: 0.52204, val loss: 0.50030\n",
      "Main effects training epoch: 24, train loss: 0.52223, val loss: 0.49963\n",
      "Main effects training epoch: 25, train loss: 0.52179, val loss: 0.49936\n",
      "Main effects training epoch: 26, train loss: 0.52175, val loss: 0.49923\n",
      "Main effects training epoch: 27, train loss: 0.52251, val loss: 0.50153\n",
      "Main effects training epoch: 28, train loss: 0.52194, val loss: 0.49970\n",
      "Main effects training epoch: 29, train loss: 0.52231, val loss: 0.50054\n",
      "Main effects training epoch: 30, train loss: 0.52304, val loss: 0.50070\n",
      "Main effects training epoch: 31, train loss: 0.52299, val loss: 0.50217\n",
      "Main effects training epoch: 32, train loss: 0.52307, val loss: 0.50133\n",
      "Main effects training epoch: 33, train loss: 0.52341, val loss: 0.50170\n",
      "Main effects training epoch: 34, train loss: 0.52159, val loss: 0.50059\n",
      "Main effects training epoch: 35, train loss: 0.52187, val loss: 0.49898\n",
      "Main effects training epoch: 36, train loss: 0.52187, val loss: 0.50127\n",
      "Main effects training epoch: 37, train loss: 0.52146, val loss: 0.49984\n",
      "Main effects training epoch: 38, train loss: 0.52178, val loss: 0.50123\n",
      "Main effects training epoch: 39, train loss: 0.52218, val loss: 0.50008\n",
      "Main effects training epoch: 40, train loss: 0.52136, val loss: 0.50050\n",
      "Main effects training epoch: 41, train loss: 0.52140, val loss: 0.49970\n",
      "Main effects training epoch: 42, train loss: 0.52141, val loss: 0.50077\n",
      "Main effects training epoch: 43, train loss: 0.52134, val loss: 0.49972\n",
      "Main effects training epoch: 44, train loss: 0.52195, val loss: 0.50113\n",
      "Main effects training epoch: 45, train loss: 0.52153, val loss: 0.50184\n",
      "Main effects training epoch: 46, train loss: 0.52224, val loss: 0.50062\n",
      "Main effects training epoch: 47, train loss: 0.52114, val loss: 0.50072\n",
      "Main effects training epoch: 48, train loss: 0.52086, val loss: 0.50044\n",
      "Main effects training epoch: 49, train loss: 0.52074, val loss: 0.50032\n",
      "Main effects training epoch: 50, train loss: 0.52081, val loss: 0.50071\n",
      "Main effects training epoch: 51, train loss: 0.52083, val loss: 0.50117\n",
      "Main effects training epoch: 52, train loss: 0.52142, val loss: 0.50103\n",
      "Main effects training epoch: 53, train loss: 0.52197, val loss: 0.50179\n",
      "Main effects training epoch: 54, train loss: 0.52071, val loss: 0.50072\n",
      "Main effects training epoch: 55, train loss: 0.52073, val loss: 0.50142\n",
      "Main effects training epoch: 56, train loss: 0.52090, val loss: 0.50017\n",
      "Main effects training epoch: 57, train loss: 0.52132, val loss: 0.50228\n",
      "Main effects training epoch: 58, train loss: 0.52071, val loss: 0.50096\n",
      "Main effects training epoch: 59, train loss: 0.52041, val loss: 0.50036\n",
      "Main effects training epoch: 60, train loss: 0.52059, val loss: 0.50111\n",
      "Main effects training epoch: 61, train loss: 0.52045, val loss: 0.50071\n",
      "Main effects training epoch: 62, train loss: 0.52057, val loss: 0.50053\n",
      "Main effects training epoch: 63, train loss: 0.52037, val loss: 0.50150\n",
      "Main effects training epoch: 64, train loss: 0.52136, val loss: 0.50140\n",
      "Main effects training epoch: 65, train loss: 0.52104, val loss: 0.50222\n",
      "Main effects training epoch: 66, train loss: 0.52093, val loss: 0.49986\n",
      "Main effects training epoch: 67, train loss: 0.52029, val loss: 0.50180\n",
      "Main effects training epoch: 68, train loss: 0.52047, val loss: 0.50009\n",
      "Main effects training epoch: 69, train loss: 0.52055, val loss: 0.50117\n",
      "Main effects training epoch: 70, train loss: 0.52145, val loss: 0.50194\n",
      "Main effects training epoch: 71, train loss: 0.52083, val loss: 0.50068\n",
      "Main effects training epoch: 72, train loss: 0.52018, val loss: 0.50106\n",
      "Main effects training epoch: 73, train loss: 0.52033, val loss: 0.50042\n",
      "Main effects training epoch: 74, train loss: 0.52096, val loss: 0.50014\n",
      "Main effects training epoch: 75, train loss: 0.52052, val loss: 0.50168\n",
      "Main effects training epoch: 76, train loss: 0.52048, val loss: 0.50069\n",
      "Main effects training epoch: 77, train loss: 0.52020, val loss: 0.50144\n",
      "Main effects training epoch: 78, train loss: 0.51990, val loss: 0.50030\n",
      "Main effects training epoch: 79, train loss: 0.51996, val loss: 0.50083\n",
      "Main effects training epoch: 80, train loss: 0.52026, val loss: 0.50104\n",
      "Main effects training epoch: 81, train loss: 0.52027, val loss: 0.50169\n",
      "Main effects training epoch: 82, train loss: 0.52016, val loss: 0.50067\n",
      "Main effects training epoch: 83, train loss: 0.51992, val loss: 0.50124\n",
      "Main effects training epoch: 84, train loss: 0.51993, val loss: 0.50021\n",
      "Main effects training epoch: 85, train loss: 0.52005, val loss: 0.50079\n",
      "Main effects training epoch: 86, train loss: 0.51976, val loss: 0.50109\n",
      "Main effects training epoch: 87, train loss: 0.52040, val loss: 0.50076\n",
      "Main effects training epoch: 88, train loss: 0.52125, val loss: 0.50265\n",
      "Main effects training epoch: 89, train loss: 0.52053, val loss: 0.50244\n",
      "Main effects training epoch: 90, train loss: 0.51987, val loss: 0.50177\n",
      "Main effects training epoch: 91, train loss: 0.52023, val loss: 0.50068\n",
      "Main effects training epoch: 92, train loss: 0.51989, val loss: 0.50051\n",
      "Main effects training epoch: 93, train loss: 0.51951, val loss: 0.50125\n",
      "Main effects training epoch: 94, train loss: 0.51946, val loss: 0.49956\n",
      "Main effects training epoch: 95, train loss: 0.51953, val loss: 0.50147\n",
      "Main effects training epoch: 96, train loss: 0.51958, val loss: 0.50028\n",
      "Main effects training epoch: 97, train loss: 0.51920, val loss: 0.50061\n",
      "Main effects training epoch: 98, train loss: 0.51935, val loss: 0.50044\n",
      "Main effects training epoch: 99, train loss: 0.51967, val loss: 0.50013\n",
      "Main effects training epoch: 100, train loss: 0.51984, val loss: 0.50170\n",
      "Main effects training epoch: 101, train loss: 0.51973, val loss: 0.50036\n",
      "Main effects training epoch: 102, train loss: 0.51906, val loss: 0.50074\n",
      "Main effects training epoch: 103, train loss: 0.51895, val loss: 0.50008\n",
      "Main effects training epoch: 104, train loss: 0.51892, val loss: 0.49957\n",
      "Main effects training epoch: 105, train loss: 0.51890, val loss: 0.50098\n",
      "Main effects training epoch: 106, train loss: 0.51901, val loss: 0.50045\n",
      "Main effects training epoch: 107, train loss: 0.51946, val loss: 0.50076\n",
      "Main effects training epoch: 108, train loss: 0.51913, val loss: 0.50130\n",
      "Main effects training epoch: 109, train loss: 0.51937, val loss: 0.50011\n",
      "Main effects training epoch: 110, train loss: 0.51971, val loss: 0.50257\n",
      "Main effects training epoch: 111, train loss: 0.51876, val loss: 0.49936\n",
      "Main effects training epoch: 112, train loss: 0.51879, val loss: 0.50107\n",
      "Main effects training epoch: 113, train loss: 0.51899, val loss: 0.49940\n",
      "Main effects training epoch: 114, train loss: 0.51954, val loss: 0.50242\n",
      "Main effects training epoch: 115, train loss: 0.51909, val loss: 0.49914\n",
      "Main effects training epoch: 116, train loss: 0.51964, val loss: 0.50230\n",
      "Main effects training epoch: 117, train loss: 0.51858, val loss: 0.49938\n",
      "Main effects training epoch: 118, train loss: 0.51857, val loss: 0.50039\n",
      "Main effects training epoch: 119, train loss: 0.51898, val loss: 0.50098\n",
      "Main effects training epoch: 120, train loss: 0.51865, val loss: 0.49967\n",
      "Main effects training epoch: 121, train loss: 0.51801, val loss: 0.49979\n",
      "Main effects training epoch: 122, train loss: 0.51758, val loss: 0.49858\n",
      "Main effects training epoch: 123, train loss: 0.51755, val loss: 0.49979\n",
      "Early stop at epoch 123, with validation loss: 0.49979\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.52027, val loss: 0.50157\n",
      "Main effects tuning epoch: 2, train loss: 0.52033, val loss: 0.50097\n",
      "Main effects tuning epoch: 3, train loss: 0.52035, val loss: 0.50109\n",
      "Main effects tuning epoch: 4, train loss: 0.52025, val loss: 0.50290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects tuning epoch: 5, train loss: 0.52028, val loss: 0.50041\n",
      "Main effects tuning epoch: 6, train loss: 0.52115, val loss: 0.50457\n",
      "Main effects tuning epoch: 7, train loss: 0.52062, val loss: 0.50113\n",
      "Main effects tuning epoch: 8, train loss: 0.52005, val loss: 0.50119\n",
      "Main effects tuning epoch: 9, train loss: 0.52086, val loss: 0.50246\n",
      "Main effects tuning epoch: 10, train loss: 0.52138, val loss: 0.50303\n",
      "Main effects tuning epoch: 11, train loss: 0.52015, val loss: 0.50289\n",
      "Main effects tuning epoch: 12, train loss: 0.51991, val loss: 0.50241\n",
      "Main effects tuning epoch: 13, train loss: 0.52008, val loss: 0.50099\n",
      "Main effects tuning epoch: 14, train loss: 0.52018, val loss: 0.50317\n",
      "Main effects tuning epoch: 15, train loss: 0.52078, val loss: 0.50370\n",
      "Main effects tuning epoch: 16, train loss: 0.52007, val loss: 0.50240\n",
      "Main effects tuning epoch: 17, train loss: 0.51998, val loss: 0.50273\n",
      "Main effects tuning epoch: 18, train loss: 0.51966, val loss: 0.50285\n",
      "Main effects tuning epoch: 19, train loss: 0.51938, val loss: 0.50221\n",
      "Main effects tuning epoch: 20, train loss: 0.51996, val loss: 0.50285\n",
      "Main effects tuning epoch: 21, train loss: 0.52018, val loss: 0.50200\n",
      "Main effects tuning epoch: 22, train loss: 0.52033, val loss: 0.50421\n",
      "Main effects tuning epoch: 23, train loss: 0.52070, val loss: 0.50264\n",
      "Main effects tuning epoch: 24, train loss: 0.52023, val loss: 0.50558\n",
      "Main effects tuning epoch: 25, train loss: 0.51927, val loss: 0.50135\n",
      "Main effects tuning epoch: 26, train loss: 0.51928, val loss: 0.50319\n",
      "Main effects tuning epoch: 27, train loss: 0.51915, val loss: 0.50199\n",
      "Main effects tuning epoch: 28, train loss: 0.51908, val loss: 0.50258\n",
      "Main effects tuning epoch: 29, train loss: 0.51917, val loss: 0.50384\n",
      "Main effects tuning epoch: 30, train loss: 0.51933, val loss: 0.50206\n",
      "Main effects tuning epoch: 31, train loss: 0.51939, val loss: 0.50194\n",
      "Main effects tuning epoch: 32, train loss: 0.51963, val loss: 0.50461\n",
      "Main effects tuning epoch: 33, train loss: 0.51928, val loss: 0.50280\n",
      "Main effects tuning epoch: 34, train loss: 0.51892, val loss: 0.50317\n",
      "Main effects tuning epoch: 35, train loss: 0.51917, val loss: 0.50189\n",
      "Main effects tuning epoch: 36, train loss: 0.51889, val loss: 0.50272\n",
      "Main effects tuning epoch: 37, train loss: 0.51879, val loss: 0.50292\n",
      "Main effects tuning epoch: 38, train loss: 0.51889, val loss: 0.50235\n",
      "Main effects tuning epoch: 39, train loss: 0.51877, val loss: 0.50292\n",
      "Main effects tuning epoch: 40, train loss: 0.51888, val loss: 0.50180\n",
      "Main effects tuning epoch: 41, train loss: 0.51872, val loss: 0.50326\n",
      "Main effects tuning epoch: 42, train loss: 0.51867, val loss: 0.50254\n",
      "Main effects tuning epoch: 43, train loss: 0.51868, val loss: 0.50333\n",
      "Main effects tuning epoch: 44, train loss: 0.51910, val loss: 0.50335\n",
      "Main effects tuning epoch: 45, train loss: 0.51899, val loss: 0.50287\n",
      "Main effects tuning epoch: 46, train loss: 0.51982, val loss: 0.50623\n",
      "Main effects tuning epoch: 47, train loss: 0.51924, val loss: 0.50127\n",
      "Main effects tuning epoch: 48, train loss: 0.51937, val loss: 0.50551\n",
      "Main effects tuning epoch: 49, train loss: 0.51872, val loss: 0.50307\n",
      "Main effects tuning epoch: 50, train loss: 0.51875, val loss: 0.50279\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.49081, val loss: 0.47509\n",
      "Interaction training epoch: 2, train loss: 0.36304, val loss: 0.36799\n",
      "Interaction training epoch: 3, train loss: 0.31448, val loss: 0.32380\n",
      "Interaction training epoch: 4, train loss: 0.29928, val loss: 0.29848\n",
      "Interaction training epoch: 5, train loss: 0.31435, val loss: 0.31682\n",
      "Interaction training epoch: 6, train loss: 0.30047, val loss: 0.29758\n",
      "Interaction training epoch: 7, train loss: 0.29241, val loss: 0.29693\n",
      "Interaction training epoch: 8, train loss: 0.28846, val loss: 0.28822\n",
      "Interaction training epoch: 9, train loss: 0.28655, val loss: 0.29014\n",
      "Interaction training epoch: 10, train loss: 0.28523, val loss: 0.28765\n",
      "Interaction training epoch: 11, train loss: 0.28433, val loss: 0.28758\n",
      "Interaction training epoch: 12, train loss: 0.28922, val loss: 0.29481\n",
      "Interaction training epoch: 13, train loss: 0.28049, val loss: 0.28424\n",
      "Interaction training epoch: 14, train loss: 0.27736, val loss: 0.28182\n",
      "Interaction training epoch: 15, train loss: 0.28535, val loss: 0.28961\n",
      "Interaction training epoch: 16, train loss: 0.27740, val loss: 0.28022\n",
      "Interaction training epoch: 17, train loss: 0.27899, val loss: 0.28432\n",
      "Interaction training epoch: 18, train loss: 0.28023, val loss: 0.28510\n",
      "Interaction training epoch: 19, train loss: 0.27595, val loss: 0.27983\n",
      "Interaction training epoch: 20, train loss: 0.27778, val loss: 0.28127\n",
      "Interaction training epoch: 21, train loss: 0.27909, val loss: 0.28422\n",
      "Interaction training epoch: 22, train loss: 0.28000, val loss: 0.28470\n",
      "Interaction training epoch: 23, train loss: 0.27459, val loss: 0.28257\n",
      "Interaction training epoch: 24, train loss: 0.27530, val loss: 0.28529\n",
      "Interaction training epoch: 25, train loss: 0.27215, val loss: 0.27789\n",
      "Interaction training epoch: 26, train loss: 0.27463, val loss: 0.28136\n",
      "Interaction training epoch: 27, train loss: 0.27101, val loss: 0.27756\n",
      "Interaction training epoch: 28, train loss: 0.27776, val loss: 0.28583\n",
      "Interaction training epoch: 29, train loss: 0.27903, val loss: 0.28504\n",
      "Interaction training epoch: 30, train loss: 0.27655, val loss: 0.28684\n",
      "Interaction training epoch: 31, train loss: 0.27173, val loss: 0.28065\n",
      "Interaction training epoch: 32, train loss: 0.27098, val loss: 0.27765\n",
      "Interaction training epoch: 33, train loss: 0.27345, val loss: 0.28333\n",
      "Interaction training epoch: 34, train loss: 0.27201, val loss: 0.28252\n",
      "Interaction training epoch: 35, train loss: 0.27134, val loss: 0.28223\n",
      "Interaction training epoch: 36, train loss: 0.27318, val loss: 0.28172\n",
      "Interaction training epoch: 37, train loss: 0.27355, val loss: 0.28170\n",
      "Interaction training epoch: 38, train loss: 0.27208, val loss: 0.28412\n",
      "Interaction training epoch: 39, train loss: 0.27030, val loss: 0.28108\n",
      "Interaction training epoch: 40, train loss: 0.27392, val loss: 0.28304\n",
      "Interaction training epoch: 41, train loss: 0.27115, val loss: 0.28345\n",
      "Interaction training epoch: 42, train loss: 0.27035, val loss: 0.28191\n",
      "Interaction training epoch: 43, train loss: 0.27121, val loss: 0.28113\n",
      "Interaction training epoch: 44, train loss: 0.26922, val loss: 0.28351\n",
      "Interaction training epoch: 45, train loss: 0.27374, val loss: 0.28404\n",
      "Interaction training epoch: 46, train loss: 0.27213, val loss: 0.28237\n",
      "Interaction training epoch: 47, train loss: 0.26741, val loss: 0.27824\n",
      "Interaction training epoch: 48, train loss: 0.26991, val loss: 0.28339\n",
      "Interaction training epoch: 49, train loss: 0.26632, val loss: 0.27613\n",
      "Interaction training epoch: 50, train loss: 0.26981, val loss: 0.28149\n",
      "Interaction training epoch: 51, train loss: 0.26622, val loss: 0.27896\n",
      "Interaction training epoch: 52, train loss: 0.27185, val loss: 0.28391\n",
      "Interaction training epoch: 53, train loss: 0.26909, val loss: 0.28082\n",
      "Interaction training epoch: 54, train loss: 0.26695, val loss: 0.27716\n",
      "Interaction training epoch: 55, train loss: 0.26711, val loss: 0.28098\n",
      "Interaction training epoch: 56, train loss: 0.26462, val loss: 0.27894\n",
      "Interaction training epoch: 57, train loss: 0.26626, val loss: 0.28026\n",
      "Interaction training epoch: 58, train loss: 0.26494, val loss: 0.27992\n",
      "Interaction training epoch: 59, train loss: 0.26366, val loss: 0.27900\n",
      "Interaction training epoch: 60, train loss: 0.26598, val loss: 0.27938\n",
      "Interaction training epoch: 61, train loss: 0.27694, val loss: 0.29206\n",
      "Interaction training epoch: 62, train loss: 0.26577, val loss: 0.28178\n",
      "Interaction training epoch: 63, train loss: 0.25993, val loss: 0.27526\n",
      "Interaction training epoch: 64, train loss: 0.27278, val loss: 0.28839\n",
      "Interaction training epoch: 65, train loss: 0.26280, val loss: 0.28103\n",
      "Interaction training epoch: 66, train loss: 0.26342, val loss: 0.27886\n",
      "Interaction training epoch: 67, train loss: 0.27280, val loss: 0.28818\n",
      "Interaction training epoch: 68, train loss: 0.26058, val loss: 0.27625\n",
      "Interaction training epoch: 69, train loss: 0.26310, val loss: 0.28135\n",
      "Interaction training epoch: 70, train loss: 0.26128, val loss: 0.27882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 71, train loss: 0.26105, val loss: 0.27931\n",
      "Interaction training epoch: 72, train loss: 0.26324, val loss: 0.28068\n",
      "Interaction training epoch: 73, train loss: 0.26327, val loss: 0.28170\n",
      "Interaction training epoch: 74, train loss: 0.26418, val loss: 0.27915\n",
      "Interaction training epoch: 75, train loss: 0.26107, val loss: 0.28065\n",
      "Interaction training epoch: 76, train loss: 0.25805, val loss: 0.27828\n",
      "Interaction training epoch: 77, train loss: 0.25902, val loss: 0.27645\n",
      "Interaction training epoch: 78, train loss: 0.26382, val loss: 0.28445\n",
      "Interaction training epoch: 79, train loss: 0.25767, val loss: 0.27501\n",
      "Interaction training epoch: 80, train loss: 0.25700, val loss: 0.27717\n",
      "Interaction training epoch: 81, train loss: 0.25763, val loss: 0.27958\n",
      "Interaction training epoch: 82, train loss: 0.26035, val loss: 0.28124\n",
      "Interaction training epoch: 83, train loss: 0.26238, val loss: 0.28138\n",
      "Interaction training epoch: 84, train loss: 0.25764, val loss: 0.27922\n",
      "Interaction training epoch: 85, train loss: 0.25800, val loss: 0.27739\n",
      "Interaction training epoch: 86, train loss: 0.25696, val loss: 0.27749\n",
      "Interaction training epoch: 87, train loss: 0.25790, val loss: 0.27722\n",
      "Interaction training epoch: 88, train loss: 0.25352, val loss: 0.27485\n",
      "Interaction training epoch: 89, train loss: 0.25637, val loss: 0.27852\n",
      "Interaction training epoch: 90, train loss: 0.25865, val loss: 0.27892\n",
      "Interaction training epoch: 91, train loss: 0.25635, val loss: 0.27803\n",
      "Interaction training epoch: 92, train loss: 0.25520, val loss: 0.27639\n",
      "Interaction training epoch: 93, train loss: 0.25346, val loss: 0.27808\n",
      "Interaction training epoch: 94, train loss: 0.25488, val loss: 0.27501\n",
      "Interaction training epoch: 95, train loss: 0.25455, val loss: 0.27578\n",
      "Interaction training epoch: 96, train loss: 0.25510, val loss: 0.27676\n",
      "Interaction training epoch: 97, train loss: 0.25396, val loss: 0.27564\n",
      "Interaction training epoch: 98, train loss: 0.25337, val loss: 0.27356\n",
      "Interaction training epoch: 99, train loss: 0.25228, val loss: 0.27716\n",
      "Interaction training epoch: 100, train loss: 0.25817, val loss: 0.27621\n",
      "Interaction training epoch: 101, train loss: 0.25602, val loss: 0.27942\n",
      "Interaction training epoch: 102, train loss: 0.25557, val loss: 0.27943\n",
      "Interaction training epoch: 103, train loss: 0.25655, val loss: 0.28035\n",
      "Interaction training epoch: 104, train loss: 0.25631, val loss: 0.27986\n",
      "Interaction training epoch: 105, train loss: 0.25325, val loss: 0.27502\n",
      "Interaction training epoch: 106, train loss: 0.25280, val loss: 0.27613\n",
      "Interaction training epoch: 107, train loss: 0.25297, val loss: 0.27950\n",
      "Interaction training epoch: 108, train loss: 0.24986, val loss: 0.27528\n",
      "Interaction training epoch: 109, train loss: 0.25273, val loss: 0.27593\n",
      "Interaction training epoch: 110, train loss: 0.25340, val loss: 0.27786\n",
      "Interaction training epoch: 111, train loss: 0.25263, val loss: 0.27657\n",
      "Interaction training epoch: 112, train loss: 0.25172, val loss: 0.27754\n",
      "Interaction training epoch: 113, train loss: 0.25043, val loss: 0.27552\n",
      "Interaction training epoch: 114, train loss: 0.25107, val loss: 0.27610\n",
      "Interaction training epoch: 115, train loss: 0.24953, val loss: 0.27515\n",
      "Interaction training epoch: 116, train loss: 0.24921, val loss: 0.27678\n",
      "Interaction training epoch: 117, train loss: 0.25001, val loss: 0.27707\n",
      "Interaction training epoch: 118, train loss: 0.24894, val loss: 0.27599\n",
      "Interaction training epoch: 119, train loss: 0.24727, val loss: 0.27445\n",
      "Interaction training epoch: 120, train loss: 0.24965, val loss: 0.27955\n",
      "Interaction training epoch: 121, train loss: 0.24868, val loss: 0.27485\n",
      "Interaction training epoch: 122, train loss: 0.24753, val loss: 0.27805\n",
      "Interaction training epoch: 123, train loss: 0.24816, val loss: 0.27513\n",
      "Interaction training epoch: 124, train loss: 0.24824, val loss: 0.27532\n",
      "Interaction training epoch: 125, train loss: 0.25110, val loss: 0.28218\n",
      "Interaction training epoch: 126, train loss: 0.24936, val loss: 0.27844\n",
      "Interaction training epoch: 127, train loss: 0.24840, val loss: 0.27918\n",
      "Interaction training epoch: 128, train loss: 0.24958, val loss: 0.27896\n",
      "Interaction training epoch: 129, train loss: 0.24901, val loss: 0.27837\n",
      "Interaction training epoch: 130, train loss: 0.24578, val loss: 0.28070\n",
      "Interaction training epoch: 131, train loss: 0.24530, val loss: 0.27557\n",
      "Interaction training epoch: 132, train loss: 0.24811, val loss: 0.27954\n",
      "Interaction training epoch: 133, train loss: 0.24959, val loss: 0.27979\n",
      "Interaction training epoch: 134, train loss: 0.24512, val loss: 0.27820\n",
      "Interaction training epoch: 135, train loss: 0.24612, val loss: 0.27997\n",
      "Interaction training epoch: 136, train loss: 0.24736, val loss: 0.28153\n",
      "Interaction training epoch: 137, train loss: 0.25079, val loss: 0.28383\n",
      "Interaction training epoch: 138, train loss: 0.24676, val loss: 0.28141\n",
      "Interaction training epoch: 139, train loss: 0.24434, val loss: 0.27635\n",
      "Interaction training epoch: 140, train loss: 0.24880, val loss: 0.28129\n",
      "Interaction training epoch: 141, train loss: 0.24539, val loss: 0.28046\n",
      "Interaction training epoch: 142, train loss: 0.24572, val loss: 0.27941\n",
      "Interaction training epoch: 143, train loss: 0.24338, val loss: 0.27904\n",
      "Interaction training epoch: 144, train loss: 0.24330, val loss: 0.27713\n",
      "Interaction training epoch: 145, train loss: 0.24118, val loss: 0.27699\n",
      "Interaction training epoch: 146, train loss: 0.24466, val loss: 0.28023\n",
      "Interaction training epoch: 147, train loss: 0.24327, val loss: 0.27578\n",
      "Interaction training epoch: 148, train loss: 0.24332, val loss: 0.28130\n",
      "Interaction training epoch: 149, train loss: 0.24522, val loss: 0.28186\n",
      "Interaction training epoch: 150, train loss: 0.24574, val loss: 0.27915\n",
      "Interaction training epoch: 151, train loss: 0.24221, val loss: 0.28056\n",
      "Interaction training epoch: 152, train loss: 0.24357, val loss: 0.27951\n",
      "Interaction training epoch: 153, train loss: 0.23959, val loss: 0.27655\n",
      "Interaction training epoch: 154, train loss: 0.24189, val loss: 0.27620\n",
      "Interaction training epoch: 155, train loss: 0.24176, val loss: 0.28170\n",
      "Interaction training epoch: 156, train loss: 0.24224, val loss: 0.28101\n",
      "Interaction training epoch: 157, train loss: 0.24817, val loss: 0.28131\n",
      "Interaction training epoch: 158, train loss: 0.24751, val loss: 0.28743\n",
      "Interaction training epoch: 159, train loss: 0.24466, val loss: 0.28232\n",
      "Interaction training epoch: 160, train loss: 0.23974, val loss: 0.27653\n",
      "Interaction training epoch: 161, train loss: 0.24232, val loss: 0.27931\n",
      "Interaction training epoch: 162, train loss: 0.23934, val loss: 0.27543\n",
      "Interaction training epoch: 163, train loss: 0.24543, val loss: 0.28100\n",
      "Interaction training epoch: 164, train loss: 0.24100, val loss: 0.28351\n",
      "Interaction training epoch: 165, train loss: 0.24161, val loss: 0.27760\n",
      "Interaction training epoch: 166, train loss: 0.24155, val loss: 0.28302\n",
      "Interaction training epoch: 167, train loss: 0.23804, val loss: 0.27445\n",
      "Interaction training epoch: 168, train loss: 0.23855, val loss: 0.27525\n",
      "Interaction training epoch: 169, train loss: 0.24108, val loss: 0.27742\n",
      "Interaction training epoch: 170, train loss: 0.24204, val loss: 0.28227\n",
      "Interaction training epoch: 171, train loss: 0.23664, val loss: 0.27560\n",
      "Interaction training epoch: 172, train loss: 0.24006, val loss: 0.27642\n",
      "Interaction training epoch: 173, train loss: 0.24098, val loss: 0.27589\n",
      "Interaction training epoch: 174, train loss: 0.23712, val loss: 0.27476\n",
      "Interaction training epoch: 175, train loss: 0.24234, val loss: 0.28175\n",
      "Interaction training epoch: 176, train loss: 0.24072, val loss: 0.27665\n",
      "Interaction training epoch: 177, train loss: 0.23835, val loss: 0.27701\n",
      "Interaction training epoch: 178, train loss: 0.23703, val loss: 0.27568\n",
      "Interaction training epoch: 179, train loss: 0.24062, val loss: 0.27816\n",
      "Interaction training epoch: 180, train loss: 0.23885, val loss: 0.27481\n",
      "Interaction training epoch: 181, train loss: 0.23843, val loss: 0.27833\n",
      "Interaction training epoch: 182, train loss: 0.23951, val loss: 0.27730\n",
      "Interaction training epoch: 183, train loss: 0.23823, val loss: 0.27267\n",
      "Interaction training epoch: 184, train loss: 0.23673, val loss: 0.27993\n",
      "Interaction training epoch: 185, train loss: 0.23912, val loss: 0.27198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 186, train loss: 0.24119, val loss: 0.28448\n",
      "Interaction training epoch: 187, train loss: 0.23616, val loss: 0.27349\n",
      "Interaction training epoch: 188, train loss: 0.23572, val loss: 0.27419\n",
      "Interaction training epoch: 189, train loss: 0.23874, val loss: 0.28055\n",
      "Interaction training epoch: 190, train loss: 0.23666, val loss: 0.27543\n",
      "Interaction training epoch: 191, train loss: 0.23524, val loss: 0.27288\n",
      "Interaction training epoch: 192, train loss: 0.23894, val loss: 0.27909\n",
      "Interaction training epoch: 193, train loss: 0.23498, val loss: 0.27593\n",
      "Interaction training epoch: 194, train loss: 0.23837, val loss: 0.27777\n",
      "Interaction training epoch: 195, train loss: 0.23832, val loss: 0.27334\n",
      "Interaction training epoch: 196, train loss: 0.23585, val loss: 0.27809\n",
      "Interaction training epoch: 197, train loss: 0.23627, val loss: 0.27344\n",
      "Interaction training epoch: 198, train loss: 0.23693, val loss: 0.27926\n",
      "Interaction training epoch: 199, train loss: 0.23654, val loss: 0.27476\n",
      "Interaction training epoch: 200, train loss: 0.24641, val loss: 0.28791\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########1 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.23682, val loss: 0.27124\n",
      "Interaction tuning epoch: 2, train loss: 0.23781, val loss: 0.27708\n",
      "Interaction tuning epoch: 3, train loss: 0.23605, val loss: 0.27318\n",
      "Interaction tuning epoch: 4, train loss: 0.23623, val loss: 0.27528\n",
      "Interaction tuning epoch: 5, train loss: 0.24063, val loss: 0.27809\n",
      "Interaction tuning epoch: 6, train loss: 0.23558, val loss: 0.27106\n",
      "Interaction tuning epoch: 7, train loss: 0.23537, val loss: 0.27463\n",
      "Interaction tuning epoch: 8, train loss: 0.23571, val loss: 0.27360\n",
      "Interaction tuning epoch: 9, train loss: 0.23593, val loss: 0.27377\n",
      "Interaction tuning epoch: 10, train loss: 0.23653, val loss: 0.27679\n",
      "Interaction tuning epoch: 11, train loss: 0.23737, val loss: 0.27402\n",
      "Interaction tuning epoch: 12, train loss: 0.23646, val loss: 0.27448\n",
      "Interaction tuning epoch: 13, train loss: 0.23372, val loss: 0.27110\n",
      "Interaction tuning epoch: 14, train loss: 0.23560, val loss: 0.27608\n",
      "Interaction tuning epoch: 15, train loss: 0.23349, val loss: 0.27034\n",
      "Interaction tuning epoch: 16, train loss: 0.23804, val loss: 0.27789\n",
      "Interaction tuning epoch: 17, train loss: 0.23319, val loss: 0.27038\n",
      "Interaction tuning epoch: 18, train loss: 0.23486, val loss: 0.27520\n",
      "Interaction tuning epoch: 19, train loss: 0.23659, val loss: 0.27473\n",
      "Interaction tuning epoch: 20, train loss: 0.23273, val loss: 0.27584\n",
      "Interaction tuning epoch: 21, train loss: 0.23264, val loss: 0.27037\n",
      "Interaction tuning epoch: 22, train loss: 0.23584, val loss: 0.27552\n",
      "Interaction tuning epoch: 23, train loss: 0.23430, val loss: 0.27119\n",
      "Interaction tuning epoch: 24, train loss: 0.23343, val loss: 0.27547\n",
      "Interaction tuning epoch: 25, train loss: 0.23740, val loss: 0.27430\n",
      "Interaction tuning epoch: 26, train loss: 0.23377, val loss: 0.27225\n",
      "Interaction tuning epoch: 27, train loss: 0.23422, val loss: 0.27742\n",
      "Interaction tuning epoch: 28, train loss: 0.23255, val loss: 0.27221\n",
      "Interaction tuning epoch: 29, train loss: 0.23384, val loss: 0.27290\n",
      "Interaction tuning epoch: 30, train loss: 0.23418, val loss: 0.27313\n",
      "Interaction tuning epoch: 31, train loss: 0.23255, val loss: 0.27374\n",
      "Interaction tuning epoch: 32, train loss: 0.23742, val loss: 0.27320\n",
      "Interaction tuning epoch: 33, train loss: 0.23183, val loss: 0.27165\n",
      "Interaction tuning epoch: 34, train loss: 0.23497, val loss: 0.27336\n",
      "Interaction tuning epoch: 35, train loss: 0.23365, val loss: 0.27652\n",
      "Interaction tuning epoch: 36, train loss: 0.23644, val loss: 0.27729\n",
      "Interaction tuning epoch: 37, train loss: 0.23320, val loss: 0.27413\n",
      "Interaction tuning epoch: 38, train loss: 0.23026, val loss: 0.27041\n",
      "Interaction tuning epoch: 39, train loss: 0.23413, val loss: 0.26991\n",
      "Interaction tuning epoch: 40, train loss: 0.22874, val loss: 0.27050\n",
      "Interaction tuning epoch: 41, train loss: 0.23533, val loss: 0.27898\n",
      "Interaction tuning epoch: 42, train loss: 0.23097, val loss: 0.26973\n",
      "Interaction tuning epoch: 43, train loss: 0.23394, val loss: 0.27624\n",
      "Interaction tuning epoch: 44, train loss: 0.23440, val loss: 0.27518\n",
      "Interaction tuning epoch: 45, train loss: 0.23429, val loss: 0.27157\n",
      "Interaction tuning epoch: 46, train loss: 0.23363, val loss: 0.27766\n",
      "Interaction tuning epoch: 47, train loss: 0.23061, val loss: 0.27182\n",
      "Interaction tuning epoch: 48, train loss: 0.23232, val loss: 0.27377\n",
      "Interaction tuning epoch: 49, train loss: 0.23108, val loss: 0.26934\n",
      "Interaction tuning epoch: 50, train loss: 0.23329, val loss: 0.27675\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 46.55953025817871\n",
      "After the gam stage, training error is 0.23329 , validation error is 0.27675\n",
      "missing value counts: 99166\n",
      "[SoftImpute] Max Singular Value of X_init = 3.512894\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed BCE=0.200026 validation BCE=0.296890,rank=5\n",
      "[SoftImpute] Iter 2: observed BCE=0.196726 validation BCE=0.296876,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.194933 validation BCE=0.293766,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.193681 validation BCE=0.292597,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.192865 validation BCE=0.292096,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.192503 validation BCE=0.291149,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.192405 validation BCE=0.290942,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.192456 validation BCE=0.290048,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.192343 validation BCE=0.283117,rank=5\n",
      "[SoftImpute] Iter 10: observed BCE=0.192397 validation BCE=0.278835,rank=5\n",
      "[SoftImpute] Iter 11: observed BCE=0.192296 validation BCE=0.277777,rank=5\n",
      "[SoftImpute] Iter 12: observed BCE=0.192054 validation BCE=0.277135,rank=5\n",
      "[SoftImpute] Iter 13: observed BCE=0.192133 validation BCE=0.276803,rank=5\n",
      "[SoftImpute] Iter 14: observed BCE=0.192014 validation BCE=0.276340,rank=5\n",
      "[SoftImpute] Iter 15: observed BCE=0.191757 validation BCE=0.275975,rank=5\n",
      "[SoftImpute] Iter 16: observed BCE=0.191941 validation BCE=0.275910,rank=5\n",
      "[SoftImpute] Iter 17: observed BCE=0.192019 validation BCE=0.275298,rank=5\n",
      "[SoftImpute] Iter 18: observed BCE=0.191875 validation BCE=0.275161,rank=5\n",
      "[SoftImpute] Iter 19: observed BCE=0.191768 validation BCE=0.275215,rank=5\n",
      "[SoftImpute] Iter 20: observed BCE=0.191700 validation BCE=0.274575,rank=5\n",
      "[SoftImpute] Iter 21: observed BCE=0.191671 validation BCE=0.274789,rank=5\n",
      "[SoftImpute] Iter 22: observed BCE=0.191385 validation BCE=0.274636,rank=5\n",
      "[SoftImpute] Iter 23: observed BCE=0.191224 validation BCE=0.274754,rank=5\n",
      "[SoftImpute] Iter 24: observed BCE=0.191225 validation BCE=0.274287,rank=5\n",
      "[SoftImpute] Iter 25: observed BCE=0.191320 validation BCE=0.274209,rank=5\n",
      "[SoftImpute] Iter 26: observed BCE=0.191416 validation BCE=0.274100,rank=5\n",
      "[SoftImpute] Iter 27: observed BCE=0.191112 validation BCE=0.274180,rank=5\n",
      "[SoftImpute] Iter 28: observed BCE=0.191179 validation BCE=0.263897,rank=5\n",
      "[SoftImpute] Iter 29: observed BCE=0.191112 validation BCE=0.274002,rank=5\n",
      "[SoftImpute] Iter 30: observed BCE=0.190874 validation BCE=0.263544,rank=5\n",
      "[SoftImpute] Iter 31: observed BCE=0.190956 validation BCE=0.265616,rank=5\n",
      "[SoftImpute] Iter 32: observed BCE=0.190833 validation BCE=0.263074,rank=5\n",
      "[SoftImpute] Iter 33: observed BCE=0.190751 validation BCE=0.263427,rank=5\n",
      "[SoftImpute] Iter 34: observed BCE=0.191042 validation BCE=0.263241,rank=5\n",
      "[SoftImpute] Iter 35: observed BCE=0.191210 validation BCE=0.263430,rank=5\n",
      "[SoftImpute] Iter 36: observed BCE=0.190988 validation BCE=0.262492,rank=5\n",
      "[SoftImpute] Iter 37: observed BCE=0.190878 validation BCE=0.262615,rank=5\n",
      "[SoftImpute] Iter 38: observed BCE=0.190860 validation BCE=0.262728,rank=5\n",
      "[SoftImpute] Iter 39: observed BCE=0.190814 validation BCE=0.262878,rank=5\n",
      "[SoftImpute] Stopped after iteration 39 for lambda=0.070258\n",
      "final num of user group: 9\n",
      "final num of item group: 9\n",
      "change mode state : True\n",
      "time cost: 3.849759817123413\n",
      "After the matrix factor stage, training error is 0.19081, validation error is 0.26288\n",
      "4\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.67939, val loss: 0.68210\n",
      "Main effects training epoch: 2, train loss: 0.67526, val loss: 0.67936\n",
      "Main effects training epoch: 3, train loss: 0.66951, val loss: 0.67185\n",
      "Main effects training epoch: 4, train loss: 0.66127, val loss: 0.66258\n",
      "Main effects training epoch: 5, train loss: 0.64321, val loss: 0.64272\n",
      "Main effects training epoch: 6, train loss: 0.60761, val loss: 0.60349\n",
      "Main effects training epoch: 7, train loss: 0.56190, val loss: 0.55268\n",
      "Main effects training epoch: 8, train loss: 0.53473, val loss: 0.52018\n",
      "Main effects training epoch: 9, train loss: 0.52747, val loss: 0.50766\n",
      "Main effects training epoch: 10, train loss: 0.52777, val loss: 0.50397\n",
      "Main effects training epoch: 11, train loss: 0.52424, val loss: 0.50262\n",
      "Main effects training epoch: 12, train loss: 0.52482, val loss: 0.50697\n",
      "Main effects training epoch: 13, train loss: 0.52378, val loss: 0.50510\n",
      "Main effects training epoch: 14, train loss: 0.52309, val loss: 0.50295\n",
      "Main effects training epoch: 15, train loss: 0.52301, val loss: 0.50242\n",
      "Main effects training epoch: 16, train loss: 0.52313, val loss: 0.50219\n",
      "Main effects training epoch: 17, train loss: 0.52320, val loss: 0.50207\n",
      "Main effects training epoch: 18, train loss: 0.52337, val loss: 0.50327\n",
      "Main effects training epoch: 19, train loss: 0.52318, val loss: 0.50175\n",
      "Main effects training epoch: 20, train loss: 0.52255, val loss: 0.50219\n",
      "Main effects training epoch: 21, train loss: 0.52315, val loss: 0.50214\n",
      "Main effects training epoch: 22, train loss: 0.52458, val loss: 0.50450\n",
      "Main effects training epoch: 23, train loss: 0.52338, val loss: 0.50316\n",
      "Main effects training epoch: 24, train loss: 0.52271, val loss: 0.50135\n",
      "Main effects training epoch: 25, train loss: 0.52241, val loss: 0.50178\n",
      "Main effects training epoch: 26, train loss: 0.52259, val loss: 0.50163\n",
      "Main effects training epoch: 27, train loss: 0.52278, val loss: 0.50357\n",
      "Main effects training epoch: 28, train loss: 0.52323, val loss: 0.50168\n",
      "Main effects training epoch: 29, train loss: 0.52280, val loss: 0.50152\n",
      "Main effects training epoch: 30, train loss: 0.52293, val loss: 0.50315\n",
      "Main effects training epoch: 31, train loss: 0.52197, val loss: 0.50090\n",
      "Main effects training epoch: 32, train loss: 0.52177, val loss: 0.50165\n",
      "Main effects training epoch: 33, train loss: 0.52185, val loss: 0.50214\n",
      "Main effects training epoch: 34, train loss: 0.52188, val loss: 0.50149\n",
      "Main effects training epoch: 35, train loss: 0.52170, val loss: 0.50091\n",
      "Main effects training epoch: 36, train loss: 0.52171, val loss: 0.50199\n",
      "Main effects training epoch: 37, train loss: 0.52221, val loss: 0.50161\n",
      "Main effects training epoch: 38, train loss: 0.52162, val loss: 0.50055\n",
      "Main effects training epoch: 39, train loss: 0.52153, val loss: 0.50225\n",
      "Main effects training epoch: 40, train loss: 0.52183, val loss: 0.50177\n",
      "Main effects training epoch: 41, train loss: 0.52207, val loss: 0.50152\n",
      "Main effects training epoch: 42, train loss: 0.52192, val loss: 0.50215\n",
      "Main effects training epoch: 43, train loss: 0.52133, val loss: 0.50118\n",
      "Main effects training epoch: 44, train loss: 0.52146, val loss: 0.50119\n",
      "Main effects training epoch: 45, train loss: 0.52176, val loss: 0.50223\n",
      "Main effects training epoch: 46, train loss: 0.52134, val loss: 0.50124\n",
      "Main effects training epoch: 47, train loss: 0.52117, val loss: 0.50091\n",
      "Main effects training epoch: 48, train loss: 0.52124, val loss: 0.50152\n",
      "Main effects training epoch: 49, train loss: 0.52143, val loss: 0.50242\n",
      "Main effects training epoch: 50, train loss: 0.52118, val loss: 0.50084\n",
      "Main effects training epoch: 51, train loss: 0.52151, val loss: 0.50229\n",
      "Main effects training epoch: 52, train loss: 0.52147, val loss: 0.50199\n",
      "Main effects training epoch: 53, train loss: 0.52111, val loss: 0.50172\n",
      "Main effects training epoch: 54, train loss: 0.52297, val loss: 0.50251\n",
      "Main effects training epoch: 55, train loss: 0.52271, val loss: 0.50429\n",
      "Main effects training epoch: 56, train loss: 0.52129, val loss: 0.50153\n",
      "Main effects training epoch: 57, train loss: 0.52086, val loss: 0.50115\n",
      "Main effects training epoch: 58, train loss: 0.52123, val loss: 0.50120\n",
      "Main effects training epoch: 59, train loss: 0.52096, val loss: 0.50239\n",
      "Main effects training epoch: 60, train loss: 0.52113, val loss: 0.50121\n",
      "Main effects training epoch: 61, train loss: 0.52079, val loss: 0.50243\n",
      "Main effects training epoch: 62, train loss: 0.52075, val loss: 0.50176\n",
      "Main effects training epoch: 63, train loss: 0.52074, val loss: 0.50106\n",
      "Main effects training epoch: 64, train loss: 0.52051, val loss: 0.50187\n",
      "Main effects training epoch: 65, train loss: 0.52053, val loss: 0.50107\n",
      "Main effects training epoch: 66, train loss: 0.52078, val loss: 0.50296\n",
      "Main effects training epoch: 67, train loss: 0.52080, val loss: 0.50155\n",
      "Main effects training epoch: 68, train loss: 0.52074, val loss: 0.50332\n",
      "Main effects training epoch: 69, train loss: 0.52058, val loss: 0.50207\n",
      "Main effects training epoch: 70, train loss: 0.52100, val loss: 0.50289\n",
      "Main effects training epoch: 71, train loss: 0.52084, val loss: 0.50107\n",
      "Main effects training epoch: 72, train loss: 0.52052, val loss: 0.50239\n",
      "Main effects training epoch: 73, train loss: 0.52049, val loss: 0.50280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 74, train loss: 0.52072, val loss: 0.50211\n",
      "Main effects training epoch: 75, train loss: 0.52031, val loss: 0.50266\n",
      "Main effects training epoch: 76, train loss: 0.52009, val loss: 0.50255\n",
      "Main effects training epoch: 77, train loss: 0.51982, val loss: 0.50233\n",
      "Main effects training epoch: 78, train loss: 0.51990, val loss: 0.50221\n",
      "Main effects training epoch: 79, train loss: 0.52003, val loss: 0.50231\n",
      "Main effects training epoch: 80, train loss: 0.51966, val loss: 0.50158\n",
      "Main effects training epoch: 81, train loss: 0.51962, val loss: 0.50172\n",
      "Main effects training epoch: 82, train loss: 0.51977, val loss: 0.50204\n",
      "Main effects training epoch: 83, train loss: 0.51970, val loss: 0.50243\n",
      "Main effects training epoch: 84, train loss: 0.52010, val loss: 0.50254\n",
      "Main effects training epoch: 85, train loss: 0.52012, val loss: 0.50243\n",
      "Main effects training epoch: 86, train loss: 0.52025, val loss: 0.50389\n",
      "Main effects training epoch: 87, train loss: 0.51975, val loss: 0.50221\n",
      "Main effects training epoch: 88, train loss: 0.51948, val loss: 0.50177\n",
      "Main effects training epoch: 89, train loss: 0.51974, val loss: 0.50217\n",
      "Main effects training epoch: 90, train loss: 0.51957, val loss: 0.50243\n",
      "Main effects training epoch: 91, train loss: 0.51985, val loss: 0.50097\n",
      "Main effects training epoch: 92, train loss: 0.51965, val loss: 0.50348\n",
      "Main effects training epoch: 93, train loss: 0.51978, val loss: 0.50174\n",
      "Main effects training epoch: 94, train loss: 0.51932, val loss: 0.50295\n",
      "Main effects training epoch: 95, train loss: 0.51935, val loss: 0.50138\n",
      "Main effects training epoch: 96, train loss: 0.51949, val loss: 0.50392\n",
      "Main effects training epoch: 97, train loss: 0.51915, val loss: 0.50161\n",
      "Main effects training epoch: 98, train loss: 0.51923, val loss: 0.50095\n",
      "Main effects training epoch: 99, train loss: 0.51922, val loss: 0.50305\n",
      "Main effects training epoch: 100, train loss: 0.51948, val loss: 0.50268\n",
      "Main effects training epoch: 101, train loss: 0.51912, val loss: 0.50133\n",
      "Main effects training epoch: 102, train loss: 0.51931, val loss: 0.50346\n",
      "Main effects training epoch: 103, train loss: 0.51999, val loss: 0.50123\n",
      "Main effects training epoch: 104, train loss: 0.51928, val loss: 0.50337\n",
      "Main effects training epoch: 105, train loss: 0.51928, val loss: 0.50126\n",
      "Main effects training epoch: 106, train loss: 0.51934, val loss: 0.50360\n",
      "Main effects training epoch: 107, train loss: 0.51956, val loss: 0.50223\n",
      "Main effects training epoch: 108, train loss: 0.51972, val loss: 0.50530\n",
      "Main effects training epoch: 109, train loss: 0.51893, val loss: 0.50097\n",
      "Main effects training epoch: 110, train loss: 0.51888, val loss: 0.50336\n",
      "Main effects training epoch: 111, train loss: 0.51898, val loss: 0.50180\n",
      "Main effects training epoch: 112, train loss: 0.51873, val loss: 0.50281\n",
      "Main effects training epoch: 113, train loss: 0.51866, val loss: 0.50194\n",
      "Main effects training epoch: 114, train loss: 0.51865, val loss: 0.50372\n",
      "Main effects training epoch: 115, train loss: 0.51883, val loss: 0.50057\n",
      "Main effects training epoch: 116, train loss: 0.51886, val loss: 0.50390\n",
      "Main effects training epoch: 117, train loss: 0.51909, val loss: 0.50200\n",
      "Main effects training epoch: 118, train loss: 0.51859, val loss: 0.50242\n",
      "Main effects training epoch: 119, train loss: 0.51848, val loss: 0.50153\n",
      "Main effects training epoch: 120, train loss: 0.51829, val loss: 0.50220\n",
      "Main effects training epoch: 121, train loss: 0.51830, val loss: 0.50150\n",
      "Main effects training epoch: 122, train loss: 0.51832, val loss: 0.50186\n",
      "Main effects training epoch: 123, train loss: 0.51819, val loss: 0.50220\n",
      "Main effects training epoch: 124, train loss: 0.51819, val loss: 0.50269\n",
      "Main effects training epoch: 125, train loss: 0.51823, val loss: 0.50164\n",
      "Main effects training epoch: 126, train loss: 0.51812, val loss: 0.50172\n",
      "Main effects training epoch: 127, train loss: 0.51800, val loss: 0.50252\n",
      "Main effects training epoch: 128, train loss: 0.51795, val loss: 0.50140\n",
      "Main effects training epoch: 129, train loss: 0.51811, val loss: 0.50249\n",
      "Main effects training epoch: 130, train loss: 0.51779, val loss: 0.50141\n",
      "Main effects training epoch: 131, train loss: 0.51790, val loss: 0.50192\n",
      "Main effects training epoch: 132, train loss: 0.51789, val loss: 0.50234\n",
      "Main effects training epoch: 133, train loss: 0.51780, val loss: 0.50131\n",
      "Main effects training epoch: 134, train loss: 0.51778, val loss: 0.50251\n",
      "Main effects training epoch: 135, train loss: 0.51774, val loss: 0.50196\n",
      "Main effects training epoch: 136, train loss: 0.51771, val loss: 0.50157\n",
      "Main effects training epoch: 137, train loss: 0.51778, val loss: 0.50100\n",
      "Main effects training epoch: 138, train loss: 0.51756, val loss: 0.50261\n",
      "Main effects training epoch: 139, train loss: 0.51741, val loss: 0.50260\n",
      "Early stop at epoch 139, with validation loss: 0.50260\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51956, val loss: 0.50114\n",
      "Main effects tuning epoch: 2, train loss: 0.51970, val loss: 0.50435\n",
      "Main effects tuning epoch: 3, train loss: 0.51923, val loss: 0.50177\n",
      "Main effects tuning epoch: 4, train loss: 0.51920, val loss: 0.50144\n",
      "Main effects tuning epoch: 5, train loss: 0.51909, val loss: 0.50295\n",
      "Main effects tuning epoch: 6, train loss: 0.51891, val loss: 0.50146\n",
      "Main effects tuning epoch: 7, train loss: 0.51921, val loss: 0.50273\n",
      "Main effects tuning epoch: 8, train loss: 0.51907, val loss: 0.50237\n",
      "Main effects tuning epoch: 9, train loss: 0.51889, val loss: 0.50269\n",
      "Main effects tuning epoch: 10, train loss: 0.51862, val loss: 0.50243\n",
      "Main effects tuning epoch: 11, train loss: 0.51853, val loss: 0.50196\n",
      "Main effects tuning epoch: 12, train loss: 0.51865, val loss: 0.50385\n",
      "Main effects tuning epoch: 13, train loss: 0.51894, val loss: 0.50120\n",
      "Main effects tuning epoch: 14, train loss: 0.51870, val loss: 0.50393\n",
      "Main effects tuning epoch: 15, train loss: 0.51833, val loss: 0.50178\n",
      "Main effects tuning epoch: 16, train loss: 0.51847, val loss: 0.50190\n",
      "Main effects tuning epoch: 17, train loss: 0.51845, val loss: 0.50352\n",
      "Main effects tuning epoch: 18, train loss: 0.51867, val loss: 0.50196\n",
      "Main effects tuning epoch: 19, train loss: 0.51838, val loss: 0.50328\n",
      "Main effects tuning epoch: 20, train loss: 0.51812, val loss: 0.50264\n",
      "Main effects tuning epoch: 21, train loss: 0.51818, val loss: 0.50092\n",
      "Main effects tuning epoch: 22, train loss: 0.51781, val loss: 0.50262\n",
      "Main effects tuning epoch: 23, train loss: 0.51776, val loss: 0.50241\n",
      "Main effects tuning epoch: 24, train loss: 0.51782, val loss: 0.50283\n",
      "Main effects tuning epoch: 25, train loss: 0.51767, val loss: 0.50184\n",
      "Main effects tuning epoch: 26, train loss: 0.51752, val loss: 0.50206\n",
      "Main effects tuning epoch: 27, train loss: 0.51771, val loss: 0.50227\n",
      "Main effects tuning epoch: 28, train loss: 0.51748, val loss: 0.50279\n",
      "Main effects tuning epoch: 29, train loss: 0.51746, val loss: 0.50340\n",
      "Main effects tuning epoch: 30, train loss: 0.51742, val loss: 0.50172\n",
      "Main effects tuning epoch: 31, train loss: 0.51730, val loss: 0.50241\n",
      "Main effects tuning epoch: 32, train loss: 0.51729, val loss: 0.50319\n",
      "Main effects tuning epoch: 33, train loss: 0.51719, val loss: 0.50127\n",
      "Main effects tuning epoch: 34, train loss: 0.51708, val loss: 0.50268\n",
      "Main effects tuning epoch: 35, train loss: 0.51729, val loss: 0.50195\n",
      "Main effects tuning epoch: 36, train loss: 0.51753, val loss: 0.50408\n",
      "Main effects tuning epoch: 37, train loss: 0.51763, val loss: 0.50098\n",
      "Main effects tuning epoch: 38, train loss: 0.51767, val loss: 0.50370\n",
      "Main effects tuning epoch: 39, train loss: 0.51786, val loss: 0.50380\n",
      "Main effects tuning epoch: 40, train loss: 0.51843, val loss: 0.50448\n",
      "Main effects tuning epoch: 41, train loss: 0.51793, val loss: 0.50288\n",
      "Main effects tuning epoch: 42, train loss: 0.51746, val loss: 0.50296\n",
      "Main effects tuning epoch: 43, train loss: 0.51743, val loss: 0.50104\n",
      "Main effects tuning epoch: 44, train loss: 0.51707, val loss: 0.50372\n",
      "Main effects tuning epoch: 45, train loss: 0.51687, val loss: 0.50201\n",
      "Main effects tuning epoch: 46, train loss: 0.51675, val loss: 0.50257\n",
      "Main effects tuning epoch: 47, train loss: 0.51694, val loss: 0.50252\n",
      "Main effects tuning epoch: 48, train loss: 0.51671, val loss: 0.50284\n",
      "Main effects tuning epoch: 49, train loss: 0.51664, val loss: 0.50305\n",
      "Main effects tuning epoch: 50, train loss: 0.51673, val loss: 0.50158\n",
      "##########Stage 2: interaction training start.##########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 1, train loss: 0.50006, val loss: 0.49012\n",
      "Interaction training epoch: 2, train loss: 0.32289, val loss: 0.31695\n",
      "Interaction training epoch: 3, train loss: 0.33398, val loss: 0.34102\n",
      "Interaction training epoch: 4, train loss: 0.31133, val loss: 0.30965\n",
      "Interaction training epoch: 5, train loss: 0.29896, val loss: 0.29848\n",
      "Interaction training epoch: 6, train loss: 0.29156, val loss: 0.29220\n",
      "Interaction training epoch: 7, train loss: 0.28999, val loss: 0.28974\n",
      "Interaction training epoch: 8, train loss: 0.30000, val loss: 0.30214\n",
      "Interaction training epoch: 9, train loss: 0.29344, val loss: 0.29353\n",
      "Interaction training epoch: 10, train loss: 0.28927, val loss: 0.29279\n",
      "Interaction training epoch: 11, train loss: 0.27846, val loss: 0.28344\n",
      "Interaction training epoch: 12, train loss: 0.29432, val loss: 0.30085\n",
      "Interaction training epoch: 13, train loss: 0.28526, val loss: 0.29080\n",
      "Interaction training epoch: 14, train loss: 0.28944, val loss: 0.29511\n",
      "Interaction training epoch: 15, train loss: 0.28142, val loss: 0.28418\n",
      "Interaction training epoch: 16, train loss: 0.28027, val loss: 0.28614\n",
      "Interaction training epoch: 17, train loss: 0.28630, val loss: 0.29100\n",
      "Interaction training epoch: 18, train loss: 0.27881, val loss: 0.28470\n",
      "Interaction training epoch: 19, train loss: 0.28051, val loss: 0.28649\n",
      "Interaction training epoch: 20, train loss: 0.27850, val loss: 0.28615\n",
      "Interaction training epoch: 21, train loss: 0.27799, val loss: 0.28560\n",
      "Interaction training epoch: 22, train loss: 0.27811, val loss: 0.28801\n",
      "Interaction training epoch: 23, train loss: 0.27701, val loss: 0.28510\n",
      "Interaction training epoch: 24, train loss: 0.27477, val loss: 0.28389\n",
      "Interaction training epoch: 25, train loss: 0.27677, val loss: 0.28542\n",
      "Interaction training epoch: 26, train loss: 0.27308, val loss: 0.28165\n",
      "Interaction training epoch: 27, train loss: 0.27589, val loss: 0.28540\n",
      "Interaction training epoch: 28, train loss: 0.27264, val loss: 0.28262\n",
      "Interaction training epoch: 29, train loss: 0.27776, val loss: 0.28809\n",
      "Interaction training epoch: 30, train loss: 0.27495, val loss: 0.28585\n",
      "Interaction training epoch: 31, train loss: 0.27419, val loss: 0.28460\n",
      "Interaction training epoch: 32, train loss: 0.27221, val loss: 0.28330\n",
      "Interaction training epoch: 33, train loss: 0.27363, val loss: 0.28664\n",
      "Interaction training epoch: 34, train loss: 0.27081, val loss: 0.28350\n",
      "Interaction training epoch: 35, train loss: 0.27026, val loss: 0.28318\n",
      "Interaction training epoch: 36, train loss: 0.27086, val loss: 0.28363\n",
      "Interaction training epoch: 37, train loss: 0.27114, val loss: 0.28460\n",
      "Interaction training epoch: 38, train loss: 0.27037, val loss: 0.28524\n",
      "Interaction training epoch: 39, train loss: 0.27155, val loss: 0.28280\n",
      "Interaction training epoch: 40, train loss: 0.26992, val loss: 0.28358\n",
      "Interaction training epoch: 41, train loss: 0.26928, val loss: 0.28097\n",
      "Interaction training epoch: 42, train loss: 0.26922, val loss: 0.28432\n",
      "Interaction training epoch: 43, train loss: 0.27095, val loss: 0.28126\n",
      "Interaction training epoch: 44, train loss: 0.26877, val loss: 0.28019\n",
      "Interaction training epoch: 45, train loss: 0.26985, val loss: 0.28636\n",
      "Interaction training epoch: 46, train loss: 0.26746, val loss: 0.28027\n",
      "Interaction training epoch: 47, train loss: 0.26969, val loss: 0.28149\n",
      "Interaction training epoch: 48, train loss: 0.26691, val loss: 0.28152\n",
      "Interaction training epoch: 49, train loss: 0.26923, val loss: 0.28244\n",
      "Interaction training epoch: 50, train loss: 0.27113, val loss: 0.28621\n",
      "Interaction training epoch: 51, train loss: 0.26794, val loss: 0.28101\n",
      "Interaction training epoch: 52, train loss: 0.26614, val loss: 0.28198\n",
      "Interaction training epoch: 53, train loss: 0.26950, val loss: 0.28310\n",
      "Interaction training epoch: 54, train loss: 0.26469, val loss: 0.27929\n",
      "Interaction training epoch: 55, train loss: 0.26653, val loss: 0.28226\n",
      "Interaction training epoch: 56, train loss: 0.26685, val loss: 0.27931\n",
      "Interaction training epoch: 57, train loss: 0.26688, val loss: 0.27615\n",
      "Interaction training epoch: 58, train loss: 0.26578, val loss: 0.27991\n",
      "Interaction training epoch: 59, train loss: 0.26539, val loss: 0.28192\n",
      "Interaction training epoch: 60, train loss: 0.26604, val loss: 0.27834\n",
      "Interaction training epoch: 61, train loss: 0.26589, val loss: 0.27898\n",
      "Interaction training epoch: 62, train loss: 0.26240, val loss: 0.27908\n",
      "Interaction training epoch: 63, train loss: 0.26513, val loss: 0.27917\n",
      "Interaction training epoch: 64, train loss: 0.26550, val loss: 0.27863\n",
      "Interaction training epoch: 65, train loss: 0.26210, val loss: 0.27708\n",
      "Interaction training epoch: 66, train loss: 0.26172, val loss: 0.27401\n",
      "Interaction training epoch: 67, train loss: 0.26725, val loss: 0.28327\n",
      "Interaction training epoch: 68, train loss: 0.26378, val loss: 0.27421\n",
      "Interaction training epoch: 69, train loss: 0.26296, val loss: 0.27645\n",
      "Interaction training epoch: 70, train loss: 0.26576, val loss: 0.28096\n",
      "Interaction training epoch: 71, train loss: 0.26226, val loss: 0.27510\n",
      "Interaction training epoch: 72, train loss: 0.26170, val loss: 0.27534\n",
      "Interaction training epoch: 73, train loss: 0.26513, val loss: 0.27854\n",
      "Interaction training epoch: 74, train loss: 0.26133, val loss: 0.27416\n",
      "Interaction training epoch: 75, train loss: 0.26307, val loss: 0.27747\n",
      "Interaction training epoch: 76, train loss: 0.26226, val loss: 0.27580\n",
      "Interaction training epoch: 77, train loss: 0.25955, val loss: 0.27200\n",
      "Interaction training epoch: 78, train loss: 0.26104, val loss: 0.27352\n",
      "Interaction training epoch: 79, train loss: 0.26316, val loss: 0.27659\n",
      "Interaction training epoch: 80, train loss: 0.26178, val loss: 0.27571\n",
      "Interaction training epoch: 81, train loss: 0.26172, val loss: 0.27376\n",
      "Interaction training epoch: 82, train loss: 0.26104, val loss: 0.27269\n",
      "Interaction training epoch: 83, train loss: 0.25905, val loss: 0.26913\n",
      "Interaction training epoch: 84, train loss: 0.26172, val loss: 0.27441\n",
      "Interaction training epoch: 85, train loss: 0.26074, val loss: 0.27302\n",
      "Interaction training epoch: 86, train loss: 0.25408, val loss: 0.26732\n",
      "Interaction training epoch: 87, train loss: 0.25348, val loss: 0.26732\n",
      "Interaction training epoch: 88, train loss: 0.25348, val loss: 0.27117\n",
      "Interaction training epoch: 89, train loss: 0.25286, val loss: 0.26945\n",
      "Interaction training epoch: 90, train loss: 0.25530, val loss: 0.27247\n",
      "Interaction training epoch: 91, train loss: 0.25533, val loss: 0.27148\n",
      "Interaction training epoch: 92, train loss: 0.25223, val loss: 0.27457\n",
      "Interaction training epoch: 93, train loss: 0.25267, val loss: 0.26665\n",
      "Interaction training epoch: 94, train loss: 0.24954, val loss: 0.26673\n",
      "Interaction training epoch: 95, train loss: 0.24816, val loss: 0.26639\n",
      "Interaction training epoch: 96, train loss: 0.25777, val loss: 0.26900\n",
      "Interaction training epoch: 97, train loss: 0.25111, val loss: 0.26569\n",
      "Interaction training epoch: 98, train loss: 0.25030, val loss: 0.26211\n",
      "Interaction training epoch: 99, train loss: 0.24730, val loss: 0.26249\n",
      "Interaction training epoch: 100, train loss: 0.24643, val loss: 0.26327\n",
      "Interaction training epoch: 101, train loss: 0.24600, val loss: 0.25954\n",
      "Interaction training epoch: 102, train loss: 0.24447, val loss: 0.26206\n",
      "Interaction training epoch: 103, train loss: 0.24655, val loss: 0.26486\n",
      "Interaction training epoch: 104, train loss: 0.24663, val loss: 0.26146\n",
      "Interaction training epoch: 105, train loss: 0.24688, val loss: 0.25803\n",
      "Interaction training epoch: 106, train loss: 0.24535, val loss: 0.25798\n",
      "Interaction training epoch: 107, train loss: 0.24173, val loss: 0.25761\n",
      "Interaction training epoch: 108, train loss: 0.25039, val loss: 0.26707\n",
      "Interaction training epoch: 109, train loss: 0.24546, val loss: 0.26089\n",
      "Interaction training epoch: 110, train loss: 0.24412, val loss: 0.25713\n",
      "Interaction training epoch: 111, train loss: 0.24272, val loss: 0.25624\n",
      "Interaction training epoch: 112, train loss: 0.24168, val loss: 0.25617\n",
      "Interaction training epoch: 113, train loss: 0.23982, val loss: 0.25402\n",
      "Interaction training epoch: 114, train loss: 0.23992, val loss: 0.25658\n",
      "Interaction training epoch: 115, train loss: 0.24013, val loss: 0.25751\n",
      "Interaction training epoch: 116, train loss: 0.24004, val loss: 0.25330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 117, train loss: 0.23905, val loss: 0.25493\n",
      "Interaction training epoch: 118, train loss: 0.25253, val loss: 0.26492\n",
      "Interaction training epoch: 119, train loss: 0.24950, val loss: 0.26299\n",
      "Interaction training epoch: 120, train loss: 0.24026, val loss: 0.25234\n",
      "Interaction training epoch: 121, train loss: 0.23845, val loss: 0.25403\n",
      "Interaction training epoch: 122, train loss: 0.24298, val loss: 0.25685\n",
      "Interaction training epoch: 123, train loss: 0.23814, val loss: 0.25448\n",
      "Interaction training epoch: 124, train loss: 0.23937, val loss: 0.25549\n",
      "Interaction training epoch: 125, train loss: 0.23820, val loss: 0.25419\n",
      "Interaction training epoch: 126, train loss: 0.24226, val loss: 0.25595\n",
      "Interaction training epoch: 127, train loss: 0.24617, val loss: 0.25947\n",
      "Interaction training epoch: 128, train loss: 0.23921, val loss: 0.25649\n",
      "Interaction training epoch: 129, train loss: 0.24296, val loss: 0.25800\n",
      "Interaction training epoch: 130, train loss: 0.24009, val loss: 0.25783\n",
      "Interaction training epoch: 131, train loss: 0.24101, val loss: 0.25677\n",
      "Interaction training epoch: 132, train loss: 0.24103, val loss: 0.25852\n",
      "Interaction training epoch: 133, train loss: 0.24262, val loss: 0.25590\n",
      "Interaction training epoch: 134, train loss: 0.23805, val loss: 0.25543\n",
      "Interaction training epoch: 135, train loss: 0.23595, val loss: 0.25028\n",
      "Interaction training epoch: 136, train loss: 0.23584, val loss: 0.25103\n",
      "Interaction training epoch: 137, train loss: 0.23923, val loss: 0.25538\n",
      "Interaction training epoch: 138, train loss: 0.23776, val loss: 0.25250\n",
      "Interaction training epoch: 139, train loss: 0.23506, val loss: 0.25320\n",
      "Interaction training epoch: 140, train loss: 0.23880, val loss: 0.25437\n",
      "Interaction training epoch: 141, train loss: 0.23784, val loss: 0.25482\n",
      "Interaction training epoch: 142, train loss: 0.23641, val loss: 0.24919\n",
      "Interaction training epoch: 143, train loss: 0.24041, val loss: 0.25629\n",
      "Interaction training epoch: 144, train loss: 0.23381, val loss: 0.25107\n",
      "Interaction training epoch: 145, train loss: 0.23471, val loss: 0.25214\n",
      "Interaction training epoch: 146, train loss: 0.23456, val loss: 0.25179\n",
      "Interaction training epoch: 147, train loss: 0.23452, val loss: 0.25065\n",
      "Interaction training epoch: 148, train loss: 0.23382, val loss: 0.25175\n",
      "Interaction training epoch: 149, train loss: 0.23703, val loss: 0.25585\n",
      "Interaction training epoch: 150, train loss: 0.23638, val loss: 0.25269\n",
      "Interaction training epoch: 151, train loss: 0.23352, val loss: 0.24767\n",
      "Interaction training epoch: 152, train loss: 0.23410, val loss: 0.25396\n",
      "Interaction training epoch: 153, train loss: 0.24404, val loss: 0.26168\n",
      "Interaction training epoch: 154, train loss: 0.23876, val loss: 0.25505\n",
      "Interaction training epoch: 155, train loss: 0.23383, val loss: 0.25325\n",
      "Interaction training epoch: 156, train loss: 0.23850, val loss: 0.25576\n",
      "Interaction training epoch: 157, train loss: 0.23436, val loss: 0.25418\n",
      "Interaction training epoch: 158, train loss: 0.23390, val loss: 0.25328\n",
      "Interaction training epoch: 159, train loss: 0.23508, val loss: 0.25245\n",
      "Interaction training epoch: 160, train loss: 0.23312, val loss: 0.25299\n",
      "Interaction training epoch: 161, train loss: 0.23593, val loss: 0.25342\n",
      "Interaction training epoch: 162, train loss: 0.23263, val loss: 0.25368\n",
      "Interaction training epoch: 163, train loss: 0.23183, val loss: 0.25193\n",
      "Interaction training epoch: 164, train loss: 0.23286, val loss: 0.25268\n",
      "Interaction training epoch: 165, train loss: 0.23174, val loss: 0.25207\n",
      "Interaction training epoch: 166, train loss: 0.23444, val loss: 0.25454\n",
      "Interaction training epoch: 167, train loss: 0.23374, val loss: 0.25435\n",
      "Interaction training epoch: 168, train loss: 0.23441, val loss: 0.25331\n",
      "Interaction training epoch: 169, train loss: 0.23381, val loss: 0.25381\n",
      "Interaction training epoch: 170, train loss: 0.23174, val loss: 0.25257\n",
      "Interaction training epoch: 171, train loss: 0.23720, val loss: 0.25831\n",
      "Interaction training epoch: 172, train loss: 0.23770, val loss: 0.25895\n",
      "Interaction training epoch: 173, train loss: 0.23216, val loss: 0.25440\n",
      "Interaction training epoch: 174, train loss: 0.24177, val loss: 0.26184\n",
      "Interaction training epoch: 175, train loss: 0.23378, val loss: 0.25369\n",
      "Interaction training epoch: 176, train loss: 0.23418, val loss: 0.25376\n",
      "Interaction training epoch: 177, train loss: 0.23321, val loss: 0.25676\n",
      "Interaction training epoch: 178, train loss: 0.24257, val loss: 0.25945\n",
      "Interaction training epoch: 179, train loss: 0.23323, val loss: 0.25345\n",
      "Interaction training epoch: 180, train loss: 0.23036, val loss: 0.25279\n",
      "Interaction training epoch: 181, train loss: 0.22901, val loss: 0.24922\n",
      "Interaction training epoch: 182, train loss: 0.23049, val loss: 0.25241\n",
      "Interaction training epoch: 183, train loss: 0.22944, val loss: 0.25203\n",
      "Interaction training epoch: 184, train loss: 0.23012, val loss: 0.25174\n",
      "Interaction training epoch: 185, train loss: 0.23050, val loss: 0.25345\n",
      "Interaction training epoch: 186, train loss: 0.23429, val loss: 0.25394\n",
      "Interaction training epoch: 187, train loss: 0.22846, val loss: 0.24897\n",
      "Interaction training epoch: 188, train loss: 0.22983, val loss: 0.25208\n",
      "Interaction training epoch: 189, train loss: 0.22957, val loss: 0.25138\n",
      "Interaction training epoch: 190, train loss: 0.23068, val loss: 0.25320\n",
      "Interaction training epoch: 191, train loss: 0.22943, val loss: 0.25190\n",
      "Interaction training epoch: 192, train loss: 0.22993, val loss: 0.25440\n",
      "Interaction training epoch: 193, train loss: 0.23591, val loss: 0.25783\n",
      "Interaction training epoch: 194, train loss: 0.22643, val loss: 0.24973\n",
      "Interaction training epoch: 195, train loss: 0.23276, val loss: 0.25681\n",
      "Interaction training epoch: 196, train loss: 0.22697, val loss: 0.25120\n",
      "Interaction training epoch: 197, train loss: 0.23208, val loss: 0.25490\n",
      "Interaction training epoch: 198, train loss: 0.22720, val loss: 0.24910\n",
      "Interaction training epoch: 199, train loss: 0.22809, val loss: 0.25102\n",
      "Interaction training epoch: 200, train loss: 0.22953, val loss: 0.25009\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########2 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.23169, val loss: 0.25107\n",
      "Interaction tuning epoch: 2, train loss: 0.23246, val loss: 0.25222\n",
      "Interaction tuning epoch: 3, train loss: 0.23256, val loss: 0.25247\n",
      "Interaction tuning epoch: 4, train loss: 0.24179, val loss: 0.25720\n",
      "Interaction tuning epoch: 5, train loss: 0.23272, val loss: 0.25207\n",
      "Interaction tuning epoch: 6, train loss: 0.23154, val loss: 0.24909\n",
      "Interaction tuning epoch: 7, train loss: 0.23104, val loss: 0.24884\n",
      "Interaction tuning epoch: 8, train loss: 0.22926, val loss: 0.24718\n",
      "Interaction tuning epoch: 9, train loss: 0.23344, val loss: 0.25137\n",
      "Interaction tuning epoch: 10, train loss: 0.23580, val loss: 0.25384\n",
      "Interaction tuning epoch: 11, train loss: 0.23423, val loss: 0.25306\n",
      "Interaction tuning epoch: 12, train loss: 0.23367, val loss: 0.25164\n",
      "Interaction tuning epoch: 13, train loss: 0.23308, val loss: 0.25039\n",
      "Interaction tuning epoch: 14, train loss: 0.23067, val loss: 0.24946\n",
      "Interaction tuning epoch: 15, train loss: 0.23174, val loss: 0.24846\n",
      "Interaction tuning epoch: 16, train loss: 0.23214, val loss: 0.25224\n",
      "Interaction tuning epoch: 17, train loss: 0.22916, val loss: 0.24842\n",
      "Interaction tuning epoch: 18, train loss: 0.23061, val loss: 0.24868\n",
      "Interaction tuning epoch: 19, train loss: 0.23024, val loss: 0.25053\n",
      "Interaction tuning epoch: 20, train loss: 0.23207, val loss: 0.25083\n",
      "Interaction tuning epoch: 21, train loss: 0.22847, val loss: 0.24570\n",
      "Interaction tuning epoch: 22, train loss: 0.23006, val loss: 0.25044\n",
      "Interaction tuning epoch: 23, train loss: 0.23161, val loss: 0.25078\n",
      "Interaction tuning epoch: 24, train loss: 0.22790, val loss: 0.24719\n",
      "Interaction tuning epoch: 25, train loss: 0.23048, val loss: 0.24777\n",
      "Interaction tuning epoch: 26, train loss: 0.22980, val loss: 0.24864\n",
      "Interaction tuning epoch: 27, train loss: 0.23027, val loss: 0.24907\n",
      "Interaction tuning epoch: 28, train loss: 0.22888, val loss: 0.24926\n",
      "Interaction tuning epoch: 29, train loss: 0.23078, val loss: 0.24832\n",
      "Interaction tuning epoch: 30, train loss: 0.23192, val loss: 0.25118\n",
      "Interaction tuning epoch: 31, train loss: 0.23290, val loss: 0.25128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 32, train loss: 0.23060, val loss: 0.25058\n",
      "Interaction tuning epoch: 33, train loss: 0.22868, val loss: 0.24958\n",
      "Interaction tuning epoch: 34, train loss: 0.23212, val loss: 0.25311\n",
      "Interaction tuning epoch: 35, train loss: 0.23255, val loss: 0.24984\n",
      "Interaction tuning epoch: 36, train loss: 0.22962, val loss: 0.24885\n",
      "Interaction tuning epoch: 37, train loss: 0.23045, val loss: 0.25088\n",
      "Interaction tuning epoch: 38, train loss: 0.22777, val loss: 0.24864\n",
      "Interaction tuning epoch: 39, train loss: 0.22849, val loss: 0.24765\n",
      "Interaction tuning epoch: 40, train loss: 0.22657, val loss: 0.24651\n",
      "Interaction tuning epoch: 41, train loss: 0.23080, val loss: 0.24961\n",
      "Interaction tuning epoch: 42, train loss: 0.23013, val loss: 0.24901\n",
      "Interaction tuning epoch: 43, train loss: 0.22857, val loss: 0.24981\n",
      "Interaction tuning epoch: 44, train loss: 0.23070, val loss: 0.24959\n",
      "Interaction tuning epoch: 45, train loss: 0.22899, val loss: 0.25023\n",
      "Interaction tuning epoch: 46, train loss: 0.22791, val loss: 0.24652\n",
      "Interaction tuning epoch: 47, train loss: 0.22767, val loss: 0.24850\n",
      "Interaction tuning epoch: 48, train loss: 0.22772, val loss: 0.24888\n",
      "Interaction tuning epoch: 49, train loss: 0.23138, val loss: 0.24999\n",
      "Interaction tuning epoch: 50, train loss: 0.22718, val loss: 0.24966\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 45.13923740386963\n",
      "After the gam stage, training error is 0.22718 , validation error is 0.24966\n",
      "missing value counts: 99166\n",
      "[SoftImpute] Max Singular Value of X_init = 3.488044\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed BCE=0.192175 validation BCE=0.273489,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 2: observed BCE=0.188464 validation BCE=0.283570,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.186165 validation BCE=0.272105,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.185466 validation BCE=0.271584,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.184674 validation BCE=0.269804,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.184056 validation BCE=0.269157,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.184590 validation BCE=0.268471,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.184913 validation BCE=0.268305,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.184925 validation BCE=0.267718,rank=5\n",
      "[SoftImpute] Iter 10: observed BCE=0.185081 validation BCE=0.267330,rank=5\n",
      "[SoftImpute] Iter 11: observed BCE=0.185228 validation BCE=0.267099,rank=5\n",
      "[SoftImpute] Iter 12: observed BCE=0.185193 validation BCE=0.257425,rank=5\n",
      "[SoftImpute] Iter 13: observed BCE=0.185212 validation BCE=0.255351,rank=5\n",
      "[SoftImpute] Iter 14: observed BCE=0.185147 validation BCE=0.254954,rank=5\n",
      "[SoftImpute] Iter 15: observed BCE=0.184956 validation BCE=0.254394,rank=5\n",
      "[SoftImpute] Iter 16: observed BCE=0.185060 validation BCE=0.254002,rank=5\n",
      "[SoftImpute] Iter 17: observed BCE=0.184985 validation BCE=0.253901,rank=5\n",
      "[SoftImpute] Iter 18: observed BCE=0.184934 validation BCE=0.253419,rank=5\n",
      "[SoftImpute] Iter 19: observed BCE=0.184776 validation BCE=0.253396,rank=5\n",
      "[SoftImpute] Iter 20: observed BCE=0.184828 validation BCE=0.253324,rank=5\n",
      "[SoftImpute] Iter 21: observed BCE=0.184892 validation BCE=0.253143,rank=5\n",
      "[SoftImpute] Iter 22: observed BCE=0.184785 validation BCE=0.253188,rank=5\n",
      "[SoftImpute] Iter 23: observed BCE=0.184672 validation BCE=0.252888,rank=5\n",
      "[SoftImpute] Iter 24: observed BCE=0.184758 validation BCE=0.252824,rank=5\n",
      "[SoftImpute] Iter 25: observed BCE=0.184596 validation BCE=0.252605,rank=5\n",
      "[SoftImpute] Iter 26: observed BCE=0.184409 validation BCE=0.252911,rank=5\n",
      "[SoftImpute] Iter 27: observed BCE=0.184569 validation BCE=0.252741,rank=5\n",
      "[SoftImpute] Iter 28: observed BCE=0.184579 validation BCE=0.252702,rank=5\n",
      "[SoftImpute] Iter 29: observed BCE=0.184313 validation BCE=0.252425,rank=5\n",
      "[SoftImpute] Iter 30: observed BCE=0.184642 validation BCE=0.252521,rank=5\n",
      "[SoftImpute] Iter 31: observed BCE=0.184098 validation BCE=0.252404,rank=5\n",
      "[SoftImpute] Iter 32: observed BCE=0.184148 validation BCE=0.252477,rank=5\n",
      "[SoftImpute] Iter 33: observed BCE=0.184278 validation BCE=0.252445,rank=5\n",
      "[SoftImpute] Iter 34: observed BCE=0.184583 validation BCE=0.252407,rank=5\n",
      "[SoftImpute] Iter 35: observed BCE=0.184171 validation BCE=0.252262,rank=5\n",
      "[SoftImpute] Iter 36: observed BCE=0.184302 validation BCE=0.252430,rank=5\n",
      "[SoftImpute] Iter 37: observed BCE=0.184263 validation BCE=0.252073,rank=5\n",
      "[SoftImpute] Iter 38: observed BCE=0.184140 validation BCE=0.252182,rank=5\n",
      "[SoftImpute] Iter 39: observed BCE=0.184074 validation BCE=0.252260,rank=5\n",
      "[SoftImpute] Iter 40: observed BCE=0.184055 validation BCE=0.252226,rank=5\n",
      "[SoftImpute] Iter 41: observed BCE=0.184070 validation BCE=0.252263,rank=5\n",
      "[SoftImpute] Iter 42: observed BCE=0.184039 validation BCE=0.252082,rank=5\n",
      "[SoftImpute] Iter 43: observed BCE=0.184086 validation BCE=0.252321,rank=5\n",
      "[SoftImpute] Iter 44: observed BCE=0.183774 validation BCE=0.251810,rank=5\n",
      "[SoftImpute] Iter 45: observed BCE=0.183961 validation BCE=0.252073,rank=5\n",
      "[SoftImpute] Iter 46: observed BCE=0.183800 validation BCE=0.251904,rank=5\n",
      "[SoftImpute] Iter 47: observed BCE=0.183722 validation BCE=0.252053,rank=5\n",
      "[SoftImpute] Iter 48: observed BCE=0.184063 validation BCE=0.252134,rank=5\n",
      "[SoftImpute] Iter 49: observed BCE=0.183935 validation BCE=0.252069,rank=5\n",
      "[SoftImpute] Iter 50: observed BCE=0.183668 validation BCE=0.251993,rank=5\n",
      "[SoftImpute] Iter 51: observed BCE=0.183912 validation BCE=0.251758,rank=5\n",
      "[SoftImpute] Iter 52: observed BCE=0.183632 validation BCE=0.251963,rank=5\n",
      "[SoftImpute] Iter 53: observed BCE=0.183743 validation BCE=0.252020,rank=5\n",
      "[SoftImpute] Iter 54: observed BCE=0.183822 validation BCE=0.252057,rank=5\n",
      "[SoftImpute] Stopped after iteration 54 for lambda=0.069761\n",
      "final num of user group: 7\n",
      "final num of item group: 10\n",
      "change mode state : True\n",
      "time cost: 4.706407785415649\n",
      "After the matrix factor stage, training error is 0.18382, validation error is 0.25206\n",
      "5\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68033, val loss: 0.68187\n",
      "Main effects training epoch: 2, train loss: 0.67441, val loss: 0.67840\n",
      "Main effects training epoch: 3, train loss: 0.67105, val loss: 0.67483\n",
      "Main effects training epoch: 4, train loss: 0.66495, val loss: 0.66689\n",
      "Main effects training epoch: 5, train loss: 0.65405, val loss: 0.65484\n",
      "Main effects training epoch: 6, train loss: 0.63164, val loss: 0.63056\n",
      "Main effects training epoch: 7, train loss: 0.59204, val loss: 0.58594\n",
      "Main effects training epoch: 8, train loss: 0.54720, val loss: 0.53209\n",
      "Main effects training epoch: 9, train loss: 0.52865, val loss: 0.50550\n",
      "Main effects training epoch: 10, train loss: 0.52704, val loss: 0.50504\n",
      "Main effects training epoch: 11, train loss: 0.52385, val loss: 0.50157\n",
      "Main effects training epoch: 12, train loss: 0.52281, val loss: 0.50161\n",
      "Main effects training epoch: 13, train loss: 0.52297, val loss: 0.50350\n",
      "Main effects training epoch: 14, train loss: 0.52280, val loss: 0.50283\n",
      "Main effects training epoch: 15, train loss: 0.52249, val loss: 0.50191\n",
      "Main effects training epoch: 16, train loss: 0.52275, val loss: 0.50282\n",
      "Main effects training epoch: 17, train loss: 0.52304, val loss: 0.50136\n",
      "Main effects training epoch: 18, train loss: 0.52376, val loss: 0.50392\n",
      "Main effects training epoch: 19, train loss: 0.52286, val loss: 0.50126\n",
      "Main effects training epoch: 20, train loss: 0.52253, val loss: 0.50101\n",
      "Main effects training epoch: 21, train loss: 0.52319, val loss: 0.50429\n",
      "Main effects training epoch: 22, train loss: 0.52431, val loss: 0.50247\n",
      "Main effects training epoch: 23, train loss: 0.52202, val loss: 0.50271\n",
      "Main effects training epoch: 24, train loss: 0.52192, val loss: 0.50082\n",
      "Main effects training epoch: 25, train loss: 0.52194, val loss: 0.50052\n",
      "Main effects training epoch: 26, train loss: 0.52172, val loss: 0.50058\n",
      "Main effects training epoch: 27, train loss: 0.52162, val loss: 0.50084\n",
      "Main effects training epoch: 28, train loss: 0.52175, val loss: 0.50134\n",
      "Main effects training epoch: 29, train loss: 0.52161, val loss: 0.50086\n",
      "Main effects training epoch: 30, train loss: 0.52215, val loss: 0.50126\n",
      "Main effects training epoch: 31, train loss: 0.52175, val loss: 0.50109\n",
      "Main effects training epoch: 32, train loss: 0.52145, val loss: 0.50138\n",
      "Main effects training epoch: 33, train loss: 0.52148, val loss: 0.50081\n",
      "Main effects training epoch: 34, train loss: 0.52211, val loss: 0.50184\n",
      "Main effects training epoch: 35, train loss: 0.52152, val loss: 0.50083\n",
      "Main effects training epoch: 36, train loss: 0.52181, val loss: 0.50085\n",
      "Main effects training epoch: 37, train loss: 0.52200, val loss: 0.50233\n",
      "Main effects training epoch: 38, train loss: 0.52185, val loss: 0.50038\n",
      "Main effects training epoch: 39, train loss: 0.52217, val loss: 0.50301\n",
      "Main effects training epoch: 40, train loss: 0.52159, val loss: 0.50034\n",
      "Main effects training epoch: 41, train loss: 0.52132, val loss: 0.50066\n",
      "Main effects training epoch: 42, train loss: 0.52152, val loss: 0.50192\n",
      "Main effects training epoch: 43, train loss: 0.52213, val loss: 0.50145\n",
      "Main effects training epoch: 44, train loss: 0.52194, val loss: 0.50178\n",
      "Main effects training epoch: 45, train loss: 0.52158, val loss: 0.50030\n",
      "Main effects training epoch: 46, train loss: 0.52132, val loss: 0.50169\n",
      "Main effects training epoch: 47, train loss: 0.52219, val loss: 0.50163\n",
      "Main effects training epoch: 48, train loss: 0.52149, val loss: 0.50059\n",
      "Main effects training epoch: 49, train loss: 0.52154, val loss: 0.50095\n",
      "Main effects training epoch: 50, train loss: 0.52085, val loss: 0.50039\n",
      "Main effects training epoch: 51, train loss: 0.52101, val loss: 0.50208\n",
      "Main effects training epoch: 52, train loss: 0.52066, val loss: 0.50011\n",
      "Main effects training epoch: 53, train loss: 0.52091, val loss: 0.50100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 54, train loss: 0.52062, val loss: 0.50121\n",
      "Main effects training epoch: 55, train loss: 0.52080, val loss: 0.49942\n",
      "Main effects training epoch: 56, train loss: 0.52077, val loss: 0.50077\n",
      "Main effects training epoch: 57, train loss: 0.52076, val loss: 0.50146\n",
      "Main effects training epoch: 58, train loss: 0.52077, val loss: 0.50016\n",
      "Main effects training epoch: 59, train loss: 0.52046, val loss: 0.50089\n",
      "Main effects training epoch: 60, train loss: 0.52052, val loss: 0.50069\n",
      "Main effects training epoch: 61, train loss: 0.52070, val loss: 0.50145\n",
      "Main effects training epoch: 62, train loss: 0.52155, val loss: 0.50151\n",
      "Main effects training epoch: 63, train loss: 0.52068, val loss: 0.50147\n",
      "Main effects training epoch: 64, train loss: 0.52016, val loss: 0.50042\n",
      "Main effects training epoch: 65, train loss: 0.52007, val loss: 0.49996\n",
      "Main effects training epoch: 66, train loss: 0.52027, val loss: 0.50129\n",
      "Main effects training epoch: 67, train loss: 0.52039, val loss: 0.49998\n",
      "Main effects training epoch: 68, train loss: 0.52058, val loss: 0.50269\n",
      "Main effects training epoch: 69, train loss: 0.52012, val loss: 0.50100\n",
      "Main effects training epoch: 70, train loss: 0.52003, val loss: 0.50118\n",
      "Main effects training epoch: 71, train loss: 0.52103, val loss: 0.50434\n",
      "Main effects training epoch: 72, train loss: 0.52106, val loss: 0.50075\n",
      "Main effects training epoch: 73, train loss: 0.52022, val loss: 0.50319\n",
      "Main effects training epoch: 74, train loss: 0.52041, val loss: 0.50141\n",
      "Main effects training epoch: 75, train loss: 0.52068, val loss: 0.50336\n",
      "Main effects training epoch: 76, train loss: 0.51989, val loss: 0.50043\n",
      "Main effects training epoch: 77, train loss: 0.51987, val loss: 0.50223\n",
      "Main effects training epoch: 78, train loss: 0.51963, val loss: 0.50153\n",
      "Main effects training epoch: 79, train loss: 0.52017, val loss: 0.50182\n",
      "Main effects training epoch: 80, train loss: 0.52001, val loss: 0.50192\n",
      "Main effects training epoch: 81, train loss: 0.51962, val loss: 0.50184\n",
      "Main effects training epoch: 82, train loss: 0.51957, val loss: 0.50232\n",
      "Main effects training epoch: 83, train loss: 0.51936, val loss: 0.50150\n",
      "Main effects training epoch: 84, train loss: 0.51939, val loss: 0.50151\n",
      "Main effects training epoch: 85, train loss: 0.52008, val loss: 0.50072\n",
      "Main effects training epoch: 86, train loss: 0.51965, val loss: 0.50325\n",
      "Main effects training epoch: 87, train loss: 0.51955, val loss: 0.50134\n",
      "Main effects training epoch: 88, train loss: 0.51952, val loss: 0.50120\n",
      "Main effects training epoch: 89, train loss: 0.52051, val loss: 0.50360\n",
      "Main effects training epoch: 90, train loss: 0.51993, val loss: 0.50134\n",
      "Main effects training epoch: 91, train loss: 0.51975, val loss: 0.50348\n",
      "Main effects training epoch: 92, train loss: 0.51928, val loss: 0.50089\n",
      "Main effects training epoch: 93, train loss: 0.51913, val loss: 0.50241\n",
      "Main effects training epoch: 94, train loss: 0.51904, val loss: 0.50038\n",
      "Main effects training epoch: 95, train loss: 0.51899, val loss: 0.50043\n",
      "Main effects training epoch: 96, train loss: 0.51930, val loss: 0.50379\n",
      "Main effects training epoch: 97, train loss: 0.51952, val loss: 0.49995\n",
      "Main effects training epoch: 98, train loss: 0.51925, val loss: 0.50319\n",
      "Main effects training epoch: 99, train loss: 0.51878, val loss: 0.50031\n",
      "Main effects training epoch: 100, train loss: 0.52008, val loss: 0.50372\n",
      "Main effects training epoch: 101, train loss: 0.52005, val loss: 0.50130\n",
      "Main effects training epoch: 102, train loss: 0.52002, val loss: 0.50452\n",
      "Main effects training epoch: 103, train loss: 0.51918, val loss: 0.50110\n",
      "Main effects training epoch: 104, train loss: 0.51840, val loss: 0.50148\n",
      "Main effects training epoch: 105, train loss: 0.51887, val loss: 0.50314\n",
      "Main effects training epoch: 106, train loss: 0.51904, val loss: 0.50150\n",
      "Main effects training epoch: 107, train loss: 0.51838, val loss: 0.50307\n",
      "Main effects training epoch: 108, train loss: 0.51858, val loss: 0.50017\n",
      "Main effects training epoch: 109, train loss: 0.51820, val loss: 0.50232\n",
      "Main effects training epoch: 110, train loss: 0.51910, val loss: 0.50217\n",
      "Main effects training epoch: 111, train loss: 0.51919, val loss: 0.50402\n",
      "Main effects training epoch: 112, train loss: 0.51794, val loss: 0.50240\n",
      "Main effects training epoch: 113, train loss: 0.51777, val loss: 0.50176\n",
      "Main effects training epoch: 114, train loss: 0.51745, val loss: 0.50104\n",
      "Main effects training epoch: 115, train loss: 0.51746, val loss: 0.50174\n",
      "Main effects training epoch: 116, train loss: 0.51764, val loss: 0.50078\n",
      "Main effects training epoch: 117, train loss: 0.51750, val loss: 0.50273\n",
      "Main effects training epoch: 118, train loss: 0.51758, val loss: 0.50195\n",
      "Main effects training epoch: 119, train loss: 0.51750, val loss: 0.50087\n",
      "Main effects training epoch: 120, train loss: 0.51760, val loss: 0.50324\n",
      "Main effects training epoch: 121, train loss: 0.51724, val loss: 0.50181\n",
      "Main effects training epoch: 122, train loss: 0.51711, val loss: 0.50060\n",
      "Main effects training epoch: 123, train loss: 0.51720, val loss: 0.50281\n",
      "Main effects training epoch: 124, train loss: 0.51745, val loss: 0.50268\n",
      "Main effects training epoch: 125, train loss: 0.51744, val loss: 0.50125\n",
      "Main effects training epoch: 126, train loss: 0.51725, val loss: 0.50247\n",
      "Main effects training epoch: 127, train loss: 0.51690, val loss: 0.50232\n",
      "Main effects training epoch: 128, train loss: 0.51709, val loss: 0.50084\n",
      "Main effects training epoch: 129, train loss: 0.51732, val loss: 0.50303\n",
      "Main effects training epoch: 130, train loss: 0.51699, val loss: 0.50079\n",
      "Main effects training epoch: 131, train loss: 0.51687, val loss: 0.50195\n",
      "Main effects training epoch: 132, train loss: 0.51754, val loss: 0.50198\n",
      "Main effects training epoch: 133, train loss: 0.51701, val loss: 0.50202\n",
      "Main effects training epoch: 134, train loss: 0.51675, val loss: 0.50117\n",
      "Main effects training epoch: 135, train loss: 0.51680, val loss: 0.50204\n",
      "Main effects training epoch: 136, train loss: 0.51695, val loss: 0.50222\n",
      "Main effects training epoch: 137, train loss: 0.51666, val loss: 0.50223\n",
      "Main effects training epoch: 138, train loss: 0.51689, val loss: 0.50050\n",
      "Main effects training epoch: 139, train loss: 0.51670, val loss: 0.50293\n",
      "Main effects training epoch: 140, train loss: 0.51698, val loss: 0.50139\n",
      "Main effects training epoch: 141, train loss: 0.51712, val loss: 0.50315\n",
      "Main effects training epoch: 142, train loss: 0.51683, val loss: 0.50220\n",
      "Main effects training epoch: 143, train loss: 0.51675, val loss: 0.50191\n",
      "Main effects training epoch: 144, train loss: 0.51702, val loss: 0.50103\n",
      "Main effects training epoch: 145, train loss: 0.51666, val loss: 0.50346\n",
      "Main effects training epoch: 146, train loss: 0.51713, val loss: 0.50059\n",
      "Main effects training epoch: 147, train loss: 0.51703, val loss: 0.50308\n",
      "Main effects training epoch: 148, train loss: 0.51663, val loss: 0.50102\n",
      "Main effects training epoch: 149, train loss: 0.51697, val loss: 0.50345\n",
      "Main effects training epoch: 150, train loss: 0.51684, val loss: 0.50174\n",
      "Main effects training epoch: 151, train loss: 0.51678, val loss: 0.50258\n",
      "Main effects training epoch: 152, train loss: 0.51618, val loss: 0.50098\n",
      "Main effects training epoch: 153, train loss: 0.51613, val loss: 0.50301\n",
      "Main effects training epoch: 154, train loss: 0.51604, val loss: 0.50106\n",
      "Main effects training epoch: 155, train loss: 0.51638, val loss: 0.49991\n",
      "Main effects training epoch: 156, train loss: 0.51665, val loss: 0.50450\n",
      "Early stop at epoch 156, with validation loss: 0.50450\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51801, val loss: 0.50133\n",
      "Main effects tuning epoch: 2, train loss: 0.51813, val loss: 0.50409\n",
      "Main effects tuning epoch: 3, train loss: 0.51856, val loss: 0.50014\n",
      "Main effects tuning epoch: 4, train loss: 0.51893, val loss: 0.50583\n",
      "Main effects tuning epoch: 5, train loss: 0.51804, val loss: 0.50174\n",
      "Main effects tuning epoch: 6, train loss: 0.51758, val loss: 0.50167\n",
      "Main effects tuning epoch: 7, train loss: 0.51742, val loss: 0.50200\n",
      "Main effects tuning epoch: 8, train loss: 0.51756, val loss: 0.50166\n",
      "Main effects tuning epoch: 9, train loss: 0.51743, val loss: 0.50133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects tuning epoch: 10, train loss: 0.51771, val loss: 0.50286\n",
      "Main effects tuning epoch: 11, train loss: 0.51796, val loss: 0.50161\n",
      "Main effects tuning epoch: 12, train loss: 0.51781, val loss: 0.50352\n",
      "Main effects tuning epoch: 13, train loss: 0.51740, val loss: 0.50058\n",
      "Main effects tuning epoch: 14, train loss: 0.51718, val loss: 0.50218\n",
      "Main effects tuning epoch: 15, train loss: 0.51714, val loss: 0.50147\n",
      "Main effects tuning epoch: 16, train loss: 0.51724, val loss: 0.50037\n",
      "Main effects tuning epoch: 17, train loss: 0.51738, val loss: 0.50364\n",
      "Main effects tuning epoch: 18, train loss: 0.51776, val loss: 0.50151\n",
      "Main effects tuning epoch: 19, train loss: 0.51717, val loss: 0.50280\n",
      "Main effects tuning epoch: 20, train loss: 0.51721, val loss: 0.50104\n",
      "Main effects tuning epoch: 21, train loss: 0.51716, val loss: 0.50181\n",
      "Main effects tuning epoch: 22, train loss: 0.51708, val loss: 0.50206\n",
      "Main effects tuning epoch: 23, train loss: 0.51713, val loss: 0.50235\n",
      "Main effects tuning epoch: 24, train loss: 0.51709, val loss: 0.50141\n",
      "Main effects tuning epoch: 25, train loss: 0.51678, val loss: 0.50044\n",
      "Main effects tuning epoch: 26, train loss: 0.51692, val loss: 0.50254\n",
      "Main effects tuning epoch: 27, train loss: 0.51666, val loss: 0.50131\n",
      "Main effects tuning epoch: 28, train loss: 0.51684, val loss: 0.50215\n",
      "Main effects tuning epoch: 29, train loss: 0.51668, val loss: 0.50150\n",
      "Main effects tuning epoch: 30, train loss: 0.51663, val loss: 0.50108\n",
      "Main effects tuning epoch: 31, train loss: 0.51644, val loss: 0.50225\n",
      "Main effects tuning epoch: 32, train loss: 0.51652, val loss: 0.50122\n",
      "Main effects tuning epoch: 33, train loss: 0.51668, val loss: 0.50114\n",
      "Main effects tuning epoch: 34, train loss: 0.51720, val loss: 0.50367\n",
      "Main effects tuning epoch: 35, train loss: 0.51722, val loss: 0.50092\n",
      "Main effects tuning epoch: 36, train loss: 0.51752, val loss: 0.50471\n",
      "Main effects tuning epoch: 37, train loss: 0.51712, val loss: 0.50022\n",
      "Main effects tuning epoch: 38, train loss: 0.51618, val loss: 0.50147\n",
      "Main effects tuning epoch: 39, train loss: 0.51628, val loss: 0.50204\n",
      "Main effects tuning epoch: 40, train loss: 0.51630, val loss: 0.50062\n",
      "Main effects tuning epoch: 41, train loss: 0.51613, val loss: 0.50052\n",
      "Main effects tuning epoch: 42, train loss: 0.51635, val loss: 0.50269\n",
      "Main effects tuning epoch: 43, train loss: 0.51617, val loss: 0.50161\n",
      "Main effects tuning epoch: 44, train loss: 0.51631, val loss: 0.49980\n",
      "Main effects tuning epoch: 45, train loss: 0.51626, val loss: 0.50284\n",
      "Main effects tuning epoch: 46, train loss: 0.51631, val loss: 0.50131\n",
      "Main effects tuning epoch: 47, train loss: 0.51614, val loss: 0.50136\n",
      "Main effects tuning epoch: 48, train loss: 0.51587, val loss: 0.50042\n",
      "Main effects tuning epoch: 49, train loss: 0.51571, val loss: 0.50126\n",
      "Main effects tuning epoch: 50, train loss: 0.51582, val loss: 0.50093\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.50961, val loss: 0.50125\n",
      "Interaction training epoch: 2, train loss: 0.34740, val loss: 0.34630\n",
      "Interaction training epoch: 3, train loss: 0.32593, val loss: 0.32969\n",
      "Interaction training epoch: 4, train loss: 0.32200, val loss: 0.32154\n",
      "Interaction training epoch: 5, train loss: 0.30711, val loss: 0.30768\n",
      "Interaction training epoch: 6, train loss: 0.29850, val loss: 0.30075\n",
      "Interaction training epoch: 7, train loss: 0.28794, val loss: 0.29080\n",
      "Interaction training epoch: 8, train loss: 0.29007, val loss: 0.29346\n",
      "Interaction training epoch: 9, train loss: 0.29207, val loss: 0.29506\n",
      "Interaction training epoch: 10, train loss: 0.28790, val loss: 0.29602\n",
      "Interaction training epoch: 11, train loss: 0.28431, val loss: 0.29251\n",
      "Interaction training epoch: 12, train loss: 0.28257, val loss: 0.28599\n",
      "Interaction training epoch: 13, train loss: 0.28422, val loss: 0.28847\n",
      "Interaction training epoch: 14, train loss: 0.27947, val loss: 0.28843\n",
      "Interaction training epoch: 15, train loss: 0.28232, val loss: 0.28849\n",
      "Interaction training epoch: 16, train loss: 0.27961, val loss: 0.28634\n",
      "Interaction training epoch: 17, train loss: 0.27629, val loss: 0.28330\n",
      "Interaction training epoch: 18, train loss: 0.27890, val loss: 0.28995\n",
      "Interaction training epoch: 19, train loss: 0.28029, val loss: 0.28960\n",
      "Interaction training epoch: 20, train loss: 0.27851, val loss: 0.28651\n",
      "Interaction training epoch: 21, train loss: 0.28050, val loss: 0.29277\n",
      "Interaction training epoch: 22, train loss: 0.27657, val loss: 0.28604\n",
      "Interaction training epoch: 23, train loss: 0.27554, val loss: 0.28671\n",
      "Interaction training epoch: 24, train loss: 0.27327, val loss: 0.28163\n",
      "Interaction training epoch: 25, train loss: 0.27293, val loss: 0.28330\n",
      "Interaction training epoch: 26, train loss: 0.27003, val loss: 0.27749\n",
      "Interaction training epoch: 27, train loss: 0.26862, val loss: 0.27454\n",
      "Interaction training epoch: 28, train loss: 0.27095, val loss: 0.28319\n",
      "Interaction training epoch: 29, train loss: 0.26278, val loss: 0.27025\n",
      "Interaction training epoch: 30, train loss: 0.26267, val loss: 0.27266\n",
      "Interaction training epoch: 31, train loss: 0.25868, val loss: 0.26924\n",
      "Interaction training epoch: 32, train loss: 0.25721, val loss: 0.26554\n",
      "Interaction training epoch: 33, train loss: 0.25856, val loss: 0.26873\n",
      "Interaction training epoch: 34, train loss: 0.25604, val loss: 0.26685\n",
      "Interaction training epoch: 35, train loss: 0.25731, val loss: 0.26644\n",
      "Interaction training epoch: 36, train loss: 0.25240, val loss: 0.26058\n",
      "Interaction training epoch: 37, train loss: 0.25372, val loss: 0.26205\n",
      "Interaction training epoch: 38, train loss: 0.25210, val loss: 0.25851\n",
      "Interaction training epoch: 39, train loss: 0.25261, val loss: 0.26177\n",
      "Interaction training epoch: 40, train loss: 0.26088, val loss: 0.26714\n",
      "Interaction training epoch: 41, train loss: 0.25552, val loss: 0.26262\n",
      "Interaction training epoch: 42, train loss: 0.25811, val loss: 0.26492\n",
      "Interaction training epoch: 43, train loss: 0.25451, val loss: 0.26223\n",
      "Interaction training epoch: 44, train loss: 0.25291, val loss: 0.25698\n",
      "Interaction training epoch: 45, train loss: 0.25314, val loss: 0.26201\n",
      "Interaction training epoch: 46, train loss: 0.25592, val loss: 0.26205\n",
      "Interaction training epoch: 47, train loss: 0.25399, val loss: 0.26010\n",
      "Interaction training epoch: 48, train loss: 0.25179, val loss: 0.25773\n",
      "Interaction training epoch: 49, train loss: 0.25460, val loss: 0.26013\n",
      "Interaction training epoch: 50, train loss: 0.25130, val loss: 0.25470\n",
      "Interaction training epoch: 51, train loss: 0.25228, val loss: 0.26073\n",
      "Interaction training epoch: 52, train loss: 0.25259, val loss: 0.25822\n",
      "Interaction training epoch: 53, train loss: 0.25270, val loss: 0.25953\n",
      "Interaction training epoch: 54, train loss: 0.25031, val loss: 0.25616\n",
      "Interaction training epoch: 55, train loss: 0.25363, val loss: 0.25771\n",
      "Interaction training epoch: 56, train loss: 0.25102, val loss: 0.25782\n",
      "Interaction training epoch: 57, train loss: 0.25050, val loss: 0.25531\n",
      "Interaction training epoch: 58, train loss: 0.24871, val loss: 0.25567\n",
      "Interaction training epoch: 59, train loss: 0.25372, val loss: 0.26200\n",
      "Interaction training epoch: 60, train loss: 0.25847, val loss: 0.26407\n",
      "Interaction training epoch: 61, train loss: 0.25169, val loss: 0.25844\n",
      "Interaction training epoch: 62, train loss: 0.25991, val loss: 0.26730\n",
      "Interaction training epoch: 63, train loss: 0.24857, val loss: 0.25721\n",
      "Interaction training epoch: 64, train loss: 0.26115, val loss: 0.26534\n",
      "Interaction training epoch: 65, train loss: 0.25271, val loss: 0.26069\n",
      "Interaction training epoch: 66, train loss: 0.25362, val loss: 0.25874\n",
      "Interaction training epoch: 67, train loss: 0.24911, val loss: 0.25914\n",
      "Interaction training epoch: 68, train loss: 0.24867, val loss: 0.25686\n",
      "Interaction training epoch: 69, train loss: 0.24956, val loss: 0.25622\n",
      "Interaction training epoch: 70, train loss: 0.24841, val loss: 0.25541\n",
      "Interaction training epoch: 71, train loss: 0.24674, val loss: 0.25700\n",
      "Interaction training epoch: 72, train loss: 0.24829, val loss: 0.25826\n",
      "Interaction training epoch: 73, train loss: 0.24592, val loss: 0.25189\n",
      "Interaction training epoch: 74, train loss: 0.24464, val loss: 0.25362\n",
      "Interaction training epoch: 75, train loss: 0.24713, val loss: 0.25903\n",
      "Interaction training epoch: 76, train loss: 0.24412, val loss: 0.25171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 77, train loss: 0.24908, val loss: 0.25911\n",
      "Interaction training epoch: 78, train loss: 0.24610, val loss: 0.25597\n",
      "Interaction training epoch: 79, train loss: 0.24562, val loss: 0.25454\n",
      "Interaction training epoch: 80, train loss: 0.24440, val loss: 0.25401\n",
      "Interaction training epoch: 81, train loss: 0.24729, val loss: 0.25675\n",
      "Interaction training epoch: 82, train loss: 0.25445, val loss: 0.26392\n",
      "Interaction training epoch: 83, train loss: 0.24642, val loss: 0.25859\n",
      "Interaction training epoch: 84, train loss: 0.24879, val loss: 0.25818\n",
      "Interaction training epoch: 85, train loss: 0.24680, val loss: 0.25405\n",
      "Interaction training epoch: 86, train loss: 0.24412, val loss: 0.25440\n",
      "Interaction training epoch: 87, train loss: 0.24881, val loss: 0.25955\n",
      "Interaction training epoch: 88, train loss: 0.24678, val loss: 0.25747\n",
      "Interaction training epoch: 89, train loss: 0.24355, val loss: 0.25275\n",
      "Interaction training epoch: 90, train loss: 0.24274, val loss: 0.25230\n",
      "Interaction training epoch: 91, train loss: 0.24527, val loss: 0.25797\n",
      "Interaction training epoch: 92, train loss: 0.24241, val loss: 0.25285\n",
      "Interaction training epoch: 93, train loss: 0.24499, val loss: 0.25908\n",
      "Interaction training epoch: 94, train loss: 0.24510, val loss: 0.25919\n",
      "Interaction training epoch: 95, train loss: 0.24398, val loss: 0.25518\n",
      "Interaction training epoch: 96, train loss: 0.24400, val loss: 0.25501\n",
      "Interaction training epoch: 97, train loss: 0.24183, val loss: 0.25262\n",
      "Interaction training epoch: 98, train loss: 0.24297, val loss: 0.25550\n",
      "Interaction training epoch: 99, train loss: 0.24275, val loss: 0.25405\n",
      "Interaction training epoch: 100, train loss: 0.24694, val loss: 0.26296\n",
      "Interaction training epoch: 101, train loss: 0.24338, val loss: 0.25467\n",
      "Interaction training epoch: 102, train loss: 0.24046, val loss: 0.25430\n",
      "Interaction training epoch: 103, train loss: 0.24025, val loss: 0.25260\n",
      "Interaction training epoch: 104, train loss: 0.24332, val loss: 0.25768\n",
      "Interaction training epoch: 105, train loss: 0.24113, val loss: 0.25548\n",
      "Interaction training epoch: 106, train loss: 0.24217, val loss: 0.25505\n",
      "Interaction training epoch: 107, train loss: 0.23957, val loss: 0.25207\n",
      "Interaction training epoch: 108, train loss: 0.24293, val loss: 0.26072\n",
      "Interaction training epoch: 109, train loss: 0.24112, val loss: 0.25483\n",
      "Interaction training epoch: 110, train loss: 0.24135, val loss: 0.25548\n",
      "Interaction training epoch: 111, train loss: 0.24370, val loss: 0.25611\n",
      "Interaction training epoch: 112, train loss: 0.24269, val loss: 0.26006\n",
      "Interaction training epoch: 113, train loss: 0.23853, val loss: 0.25198\n",
      "Interaction training epoch: 114, train loss: 0.24361, val loss: 0.25995\n",
      "Interaction training epoch: 115, train loss: 0.24410, val loss: 0.25828\n",
      "Interaction training epoch: 116, train loss: 0.23789, val loss: 0.25322\n",
      "Interaction training epoch: 117, train loss: 0.23754, val loss: 0.25406\n",
      "Interaction training epoch: 118, train loss: 0.23837, val loss: 0.25235\n",
      "Interaction training epoch: 119, train loss: 0.24127, val loss: 0.25903\n",
      "Interaction training epoch: 120, train loss: 0.24162, val loss: 0.25689\n",
      "Interaction training epoch: 121, train loss: 0.24074, val loss: 0.25982\n",
      "Interaction training epoch: 122, train loss: 0.23786, val loss: 0.25291\n",
      "Interaction training epoch: 123, train loss: 0.23826, val loss: 0.25680\n",
      "Interaction training epoch: 124, train loss: 0.24003, val loss: 0.26130\n",
      "Interaction training epoch: 125, train loss: 0.23787, val loss: 0.25388\n",
      "Interaction training epoch: 126, train loss: 0.24242, val loss: 0.26218\n",
      "Interaction training epoch: 127, train loss: 0.23972, val loss: 0.25441\n",
      "Interaction training epoch: 128, train loss: 0.23970, val loss: 0.25814\n",
      "Interaction training epoch: 129, train loss: 0.23751, val loss: 0.25374\n",
      "Interaction training epoch: 130, train loss: 0.23867, val loss: 0.25818\n",
      "Interaction training epoch: 131, train loss: 0.24016, val loss: 0.26035\n",
      "Interaction training epoch: 132, train loss: 0.23760, val loss: 0.25551\n",
      "Interaction training epoch: 133, train loss: 0.23922, val loss: 0.25630\n",
      "Interaction training epoch: 134, train loss: 0.23835, val loss: 0.25781\n",
      "Interaction training epoch: 135, train loss: 0.23701, val loss: 0.25365\n",
      "Interaction training epoch: 136, train loss: 0.23811, val loss: 0.25824\n",
      "Interaction training epoch: 137, train loss: 0.23915, val loss: 0.25396\n",
      "Interaction training epoch: 138, train loss: 0.23828, val loss: 0.25359\n",
      "Interaction training epoch: 139, train loss: 0.23593, val loss: 0.25353\n",
      "Interaction training epoch: 140, train loss: 0.23725, val loss: 0.25546\n",
      "Interaction training epoch: 141, train loss: 0.23656, val loss: 0.25484\n",
      "Interaction training epoch: 142, train loss: 0.23652, val loss: 0.25356\n",
      "Interaction training epoch: 143, train loss: 0.23680, val loss: 0.25066\n",
      "Interaction training epoch: 144, train loss: 0.24037, val loss: 0.26370\n",
      "Interaction training epoch: 145, train loss: 0.23718, val loss: 0.25048\n",
      "Interaction training epoch: 146, train loss: 0.23870, val loss: 0.25925\n",
      "Interaction training epoch: 147, train loss: 0.23863, val loss: 0.25931\n",
      "Interaction training epoch: 148, train loss: 0.23460, val loss: 0.25025\n",
      "Interaction training epoch: 149, train loss: 0.24130, val loss: 0.25849\n",
      "Interaction training epoch: 150, train loss: 0.23580, val loss: 0.25408\n",
      "Interaction training epoch: 151, train loss: 0.23453, val loss: 0.24926\n",
      "Interaction training epoch: 152, train loss: 0.23510, val loss: 0.25400\n",
      "Interaction training epoch: 153, train loss: 0.23822, val loss: 0.25782\n",
      "Interaction training epoch: 154, train loss: 0.23360, val loss: 0.24936\n",
      "Interaction training epoch: 155, train loss: 0.23327, val loss: 0.25022\n",
      "Interaction training epoch: 156, train loss: 0.23651, val loss: 0.25422\n",
      "Interaction training epoch: 157, train loss: 0.23383, val loss: 0.25403\n",
      "Interaction training epoch: 158, train loss: 0.23391, val loss: 0.24943\n",
      "Interaction training epoch: 159, train loss: 0.23477, val loss: 0.25518\n",
      "Interaction training epoch: 160, train loss: 0.23381, val loss: 0.24917\n",
      "Interaction training epoch: 161, train loss: 0.23327, val loss: 0.25444\n",
      "Interaction training epoch: 162, train loss: 0.23413, val loss: 0.25231\n",
      "Interaction training epoch: 163, train loss: 0.23299, val loss: 0.25355\n",
      "Interaction training epoch: 164, train loss: 0.23464, val loss: 0.25659\n",
      "Interaction training epoch: 165, train loss: 0.23426, val loss: 0.25049\n",
      "Interaction training epoch: 166, train loss: 0.23396, val loss: 0.25580\n",
      "Interaction training epoch: 167, train loss: 0.23287, val loss: 0.25229\n",
      "Interaction training epoch: 168, train loss: 0.23630, val loss: 0.25414\n",
      "Interaction training epoch: 169, train loss: 0.23337, val loss: 0.25341\n",
      "Interaction training epoch: 170, train loss: 0.23341, val loss: 0.25269\n",
      "Interaction training epoch: 171, train loss: 0.23328, val loss: 0.25040\n",
      "Interaction training epoch: 172, train loss: 0.23239, val loss: 0.25188\n",
      "Interaction training epoch: 173, train loss: 0.23339, val loss: 0.25383\n",
      "Interaction training epoch: 174, train loss: 0.23131, val loss: 0.25037\n",
      "Interaction training epoch: 175, train loss: 0.23169, val loss: 0.25045\n",
      "Interaction training epoch: 176, train loss: 0.23050, val loss: 0.24920\n",
      "Interaction training epoch: 177, train loss: 0.23132, val loss: 0.25112\n",
      "Interaction training epoch: 178, train loss: 0.23022, val loss: 0.25208\n",
      "Interaction training epoch: 179, train loss: 0.23341, val loss: 0.25551\n",
      "Interaction training epoch: 180, train loss: 0.23137, val loss: 0.25246\n",
      "Interaction training epoch: 181, train loss: 0.23106, val loss: 0.25276\n",
      "Interaction training epoch: 182, train loss: 0.23284, val loss: 0.25202\n",
      "Interaction training epoch: 183, train loss: 0.23232, val loss: 0.25538\n",
      "Interaction training epoch: 184, train loss: 0.22949, val loss: 0.24883\n",
      "Interaction training epoch: 185, train loss: 0.22989, val loss: 0.25198\n",
      "Interaction training epoch: 186, train loss: 0.23096, val loss: 0.25232\n",
      "Interaction training epoch: 187, train loss: 0.23112, val loss: 0.25251\n",
      "Interaction training epoch: 188, train loss: 0.23177, val loss: 0.25259\n",
      "Interaction training epoch: 189, train loss: 0.23290, val loss: 0.25494\n",
      "Interaction training epoch: 190, train loss: 0.23126, val loss: 0.25345\n",
      "Interaction training epoch: 191, train loss: 0.23106, val loss: 0.25156\n",
      "Interaction training epoch: 192, train loss: 0.23187, val loss: 0.25534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 193, train loss: 0.22950, val loss: 0.24833\n",
      "Interaction training epoch: 194, train loss: 0.23064, val loss: 0.25307\n",
      "Interaction training epoch: 195, train loss: 0.22859, val loss: 0.24897\n",
      "Interaction training epoch: 196, train loss: 0.22872, val loss: 0.25210\n",
      "Interaction training epoch: 197, train loss: 0.23134, val loss: 0.25260\n",
      "Interaction training epoch: 198, train loss: 0.22993, val loss: 0.25462\n",
      "Interaction training epoch: 199, train loss: 0.23014, val loss: 0.25088\n",
      "Interaction training epoch: 200, train loss: 0.22969, val loss: 0.25177\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########3 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.23521, val loss: 0.25089\n",
      "Interaction tuning epoch: 2, train loss: 0.23449, val loss: 0.24985\n",
      "Interaction tuning epoch: 3, train loss: 0.23557, val loss: 0.25256\n",
      "Interaction tuning epoch: 4, train loss: 0.23293, val loss: 0.24695\n",
      "Interaction tuning epoch: 5, train loss: 0.23284, val loss: 0.24999\n",
      "Interaction tuning epoch: 6, train loss: 0.23572, val loss: 0.25367\n",
      "Interaction tuning epoch: 7, train loss: 0.23672, val loss: 0.25494\n",
      "Interaction tuning epoch: 8, train loss: 0.24056, val loss: 0.25538\n",
      "Interaction tuning epoch: 9, train loss: 0.23276, val loss: 0.24887\n",
      "Interaction tuning epoch: 10, train loss: 0.23379, val loss: 0.25159\n",
      "Interaction tuning epoch: 11, train loss: 0.23359, val loss: 0.25107\n",
      "Interaction tuning epoch: 12, train loss: 0.23330, val loss: 0.24736\n",
      "Interaction tuning epoch: 13, train loss: 0.23587, val loss: 0.25639\n",
      "Interaction tuning epoch: 14, train loss: 0.23465, val loss: 0.25306\n",
      "Interaction tuning epoch: 15, train loss: 0.23579, val loss: 0.25197\n",
      "Interaction tuning epoch: 16, train loss: 0.23193, val loss: 0.25089\n",
      "Interaction tuning epoch: 17, train loss: 0.23335, val loss: 0.25110\n",
      "Interaction tuning epoch: 18, train loss: 0.23405, val loss: 0.25028\n",
      "Interaction tuning epoch: 19, train loss: 0.23136, val loss: 0.24781\n",
      "Interaction tuning epoch: 20, train loss: 0.23489, val loss: 0.25158\n",
      "Interaction tuning epoch: 21, train loss: 0.23332, val loss: 0.25128\n",
      "Interaction tuning epoch: 22, train loss: 0.23328, val loss: 0.25018\n",
      "Interaction tuning epoch: 23, train loss: 0.23355, val loss: 0.25268\n",
      "Interaction tuning epoch: 24, train loss: 0.23301, val loss: 0.24768\n",
      "Interaction tuning epoch: 25, train loss: 0.23378, val loss: 0.25398\n",
      "Interaction tuning epoch: 26, train loss: 0.23103, val loss: 0.24726\n",
      "Interaction tuning epoch: 27, train loss: 0.23038, val loss: 0.25034\n",
      "Interaction tuning epoch: 28, train loss: 0.23707, val loss: 0.25579\n",
      "Interaction tuning epoch: 29, train loss: 0.23276, val loss: 0.25126\n",
      "Interaction tuning epoch: 30, train loss: 0.23322, val loss: 0.25334\n",
      "Interaction tuning epoch: 31, train loss: 0.23194, val loss: 0.24805\n",
      "Interaction tuning epoch: 32, train loss: 0.23158, val loss: 0.25111\n",
      "Interaction tuning epoch: 33, train loss: 0.23285, val loss: 0.25237\n",
      "Interaction tuning epoch: 34, train loss: 0.23306, val loss: 0.25013\n",
      "Interaction tuning epoch: 35, train loss: 0.23195, val loss: 0.25446\n",
      "Interaction tuning epoch: 36, train loss: 0.23362, val loss: 0.25201\n",
      "Interaction tuning epoch: 37, train loss: 0.23095, val loss: 0.25038\n",
      "Interaction tuning epoch: 38, train loss: 0.23681, val loss: 0.25820\n",
      "Interaction tuning epoch: 39, train loss: 0.23213, val loss: 0.24940\n",
      "Interaction tuning epoch: 40, train loss: 0.23303, val loss: 0.25203\n",
      "Interaction tuning epoch: 41, train loss: 0.23007, val loss: 0.24779\n",
      "Interaction tuning epoch: 42, train loss: 0.23028, val loss: 0.25149\n",
      "Interaction tuning epoch: 43, train loss: 0.23095, val loss: 0.25271\n",
      "Interaction tuning epoch: 44, train loss: 0.23111, val loss: 0.24817\n",
      "Interaction tuning epoch: 45, train loss: 0.23356, val loss: 0.25784\n",
      "Interaction tuning epoch: 46, train loss: 0.23053, val loss: 0.24738\n",
      "Interaction tuning epoch: 47, train loss: 0.23151, val loss: 0.25036\n",
      "Interaction tuning epoch: 48, train loss: 0.23008, val loss: 0.25000\n",
      "Interaction tuning epoch: 49, train loss: 0.23056, val loss: 0.25256\n",
      "Interaction tuning epoch: 50, train loss: 0.23245, val loss: 0.25210\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 52.06194543838501\n",
      "After the gam stage, training error is 0.23245 , validation error is 0.25210\n",
      "missing value counts: 99146\n",
      "[SoftImpute] Max Singular Value of X_init = 3.766741\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed BCE=0.193042 validation BCE=0.286812,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 2: observed BCE=0.189663 validation BCE=0.287211,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.187720 validation BCE=0.297231,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.186927 validation BCE=0.296578,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.186325 validation BCE=0.287308,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.186085 validation BCE=0.285611,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.186134 validation BCE=0.295091,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.186234 validation BCE=0.294873,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.186137 validation BCE=0.287658,rank=5\n",
      "[SoftImpute] Iter 10: observed BCE=0.186131 validation BCE=0.274241,rank=5\n",
      "[SoftImpute] Iter 11: observed BCE=0.185712 validation BCE=0.272871,rank=5\n",
      "[SoftImpute] Iter 12: observed BCE=0.186019 validation BCE=0.271891,rank=5\n",
      "[SoftImpute] Iter 13: observed BCE=0.185916 validation BCE=0.271142,rank=5\n",
      "[SoftImpute] Iter 14: observed BCE=0.185863 validation BCE=0.270335,rank=5\n",
      "[SoftImpute] Iter 15: observed BCE=0.185947 validation BCE=0.260416,rank=5\n",
      "[SoftImpute] Iter 16: observed BCE=0.185679 validation BCE=0.259143,rank=5\n",
      "[SoftImpute] Iter 17: observed BCE=0.185642 validation BCE=0.259240,rank=5\n",
      "[SoftImpute] Iter 18: observed BCE=0.186050 validation BCE=0.258515,rank=5\n",
      "[SoftImpute] Iter 19: observed BCE=0.186299 validation BCE=0.258522,rank=5\n",
      "[SoftImpute] Iter 20: observed BCE=0.186535 validation BCE=0.258181,rank=5\n",
      "[SoftImpute] Iter 21: observed BCE=0.186282 validation BCE=0.257760,rank=5\n",
      "[SoftImpute] Iter 22: observed BCE=0.186099 validation BCE=0.257169,rank=5\n",
      "[SoftImpute] Iter 23: observed BCE=0.186020 validation BCE=0.257409,rank=5\n",
      "[SoftImpute] Iter 24: observed BCE=0.186350 validation BCE=0.257229,rank=5\n",
      "[SoftImpute] Iter 25: observed BCE=0.186127 validation BCE=0.257053,rank=5\n",
      "[SoftImpute] Iter 26: observed BCE=0.186064 validation BCE=0.256953,rank=5\n",
      "[SoftImpute] Iter 27: observed BCE=0.185944 validation BCE=0.256877,rank=5\n",
      "[SoftImpute] Iter 28: observed BCE=0.185887 validation BCE=0.256302,rank=5\n",
      "[SoftImpute] Iter 29: observed BCE=0.186068 validation BCE=0.256753,rank=5\n",
      "[SoftImpute] Iter 30: observed BCE=0.186135 validation BCE=0.256644,rank=5\n",
      "[SoftImpute] Iter 31: observed BCE=0.185989 validation BCE=0.256268,rank=5\n",
      "[SoftImpute] Iter 32: observed BCE=0.185915 validation BCE=0.256407,rank=5\n",
      "[SoftImpute] Iter 33: observed BCE=0.185763 validation BCE=0.256338,rank=5\n",
      "[SoftImpute] Iter 34: observed BCE=0.186035 validation BCE=0.256165,rank=5\n",
      "[SoftImpute] Iter 35: observed BCE=0.186007 validation BCE=0.256312,rank=5\n",
      "[SoftImpute] Iter 36: observed BCE=0.185702 validation BCE=0.255763,rank=5\n",
      "[SoftImpute] Iter 37: observed BCE=0.185602 validation BCE=0.256454,rank=5\n",
      "[SoftImpute] Iter 38: observed BCE=0.185916 validation BCE=0.256135,rank=5\n",
      "[SoftImpute] Iter 39: observed BCE=0.185621 validation BCE=0.255906,rank=5\n",
      "[SoftImpute] Iter 40: observed BCE=0.185599 validation BCE=0.255777,rank=5\n",
      "[SoftImpute] Iter 41: observed BCE=0.185550 validation BCE=0.255875,rank=5\n",
      "[SoftImpute] Iter 42: observed BCE=0.185695 validation BCE=0.256020,rank=5\n",
      "[SoftImpute] Iter 43: observed BCE=0.185818 validation BCE=0.255708,rank=5\n",
      "[SoftImpute] Iter 44: observed BCE=0.185610 validation BCE=0.255573,rank=5\n",
      "[SoftImpute] Iter 45: observed BCE=0.185705 validation BCE=0.256010,rank=5\n",
      "[SoftImpute] Iter 46: observed BCE=0.185626 validation BCE=0.255477,rank=5\n",
      "[SoftImpute] Iter 47: observed BCE=0.185831 validation BCE=0.255835,rank=5\n",
      "[SoftImpute] Iter 48: observed BCE=0.185770 validation BCE=0.255560,rank=5\n",
      "[SoftImpute] Iter 49: observed BCE=0.185700 validation BCE=0.255761,rank=5\n",
      "[SoftImpute] Iter 50: observed BCE=0.185730 validation BCE=0.255784,rank=5\n",
      "[SoftImpute] Iter 51: observed BCE=0.185615 validation BCE=0.255540,rank=5\n",
      "[SoftImpute] Iter 52: observed BCE=0.185281 validation BCE=0.255318,rank=5\n",
      "[SoftImpute] Iter 53: observed BCE=0.185353 validation BCE=0.255864,rank=5\n",
      "[SoftImpute] Iter 54: observed BCE=0.185478 validation BCE=0.255337,rank=5\n",
      "[SoftImpute] Iter 55: observed BCE=0.185404 validation BCE=0.255455,rank=5\n",
      "[SoftImpute] Iter 56: observed BCE=0.185510 validation BCE=0.255418,rank=5\n",
      "[SoftImpute] Iter 57: observed BCE=0.185405 validation BCE=0.255552,rank=5\n",
      "[SoftImpute] Iter 58: observed BCE=0.185433 validation BCE=0.255475,rank=5\n",
      "[SoftImpute] Iter 59: observed BCE=0.185818 validation BCE=0.255524,rank=5\n",
      "[SoftImpute] Iter 60: observed BCE=0.185585 validation BCE=0.255255,rank=5\n",
      "[SoftImpute] Iter 61: observed BCE=0.185102 validation BCE=0.255551,rank=5\n",
      "[SoftImpute] Iter 62: observed BCE=0.185345 validation BCE=0.255451,rank=5\n",
      "[SoftImpute] Iter 63: observed BCE=0.185135 validation BCE=0.255150,rank=5\n",
      "[SoftImpute] Iter 64: observed BCE=0.185388 validation BCE=0.255439,rank=5\n",
      "[SoftImpute] Iter 65: observed BCE=0.185520 validation BCE=0.255272,rank=5\n",
      "[SoftImpute] Iter 66: observed BCE=0.185370 validation BCE=0.255428,rank=5\n",
      "[SoftImpute] Iter 67: observed BCE=0.185359 validation BCE=0.255521,rank=5\n",
      "[SoftImpute] Iter 68: observed BCE=0.185246 validation BCE=0.255394,rank=5\n",
      "[SoftImpute] Iter 69: observed BCE=0.185218 validation BCE=0.255205,rank=5\n",
      "[SoftImpute] Iter 70: observed BCE=0.185350 validation BCE=0.255199,rank=5\n",
      "[SoftImpute] Iter 71: observed BCE=0.185474 validation BCE=0.255316,rank=5\n",
      "[SoftImpute] Iter 72: observed BCE=0.185289 validation BCE=0.255364,rank=5\n",
      "[SoftImpute] Iter 73: observed BCE=0.185262 validation BCE=0.255146,rank=5\n",
      "[SoftImpute] Iter 74: observed BCE=0.185373 validation BCE=0.255488,rank=5\n",
      "[SoftImpute] Iter 75: observed BCE=0.185159 validation BCE=0.255164,rank=5\n",
      "[SoftImpute] Iter 76: observed BCE=0.185340 validation BCE=0.255400,rank=5\n",
      "[SoftImpute] Iter 77: observed BCE=0.185108 validation BCE=0.255261,rank=5\n",
      "[SoftImpute] Iter 78: observed BCE=0.185144 validation BCE=0.255096,rank=5\n",
      "[SoftImpute] Iter 79: observed BCE=0.185235 validation BCE=0.255174,rank=5\n",
      "[SoftImpute] Iter 80: observed BCE=0.185501 validation BCE=0.255323,rank=5\n",
      "[SoftImpute] Iter 81: observed BCE=0.185249 validation BCE=0.255368,rank=5\n",
      "[SoftImpute] Stopped after iteration 81 for lambda=0.075335\n",
      "final num of user group: 5\n",
      "final num of item group: 8\n",
      "change mode state : True\n",
      "time cost: 4.816640138626099\n",
      "After the matrix factor stage, training error is 0.18525, validation error is 0.25537\n",
      "6\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68032, val loss: 0.68483\n",
      "Main effects training epoch: 2, train loss: 0.67590, val loss: 0.68028\n",
      "Main effects training epoch: 3, train loss: 0.67051, val loss: 0.67303\n",
      "Main effects training epoch: 4, train loss: 0.66371, val loss: 0.66492\n",
      "Main effects training epoch: 5, train loss: 0.65184, val loss: 0.65216\n",
      "Main effects training epoch: 6, train loss: 0.62939, val loss: 0.62665\n",
      "Main effects training epoch: 7, train loss: 0.59759, val loss: 0.58975\n",
      "Main effects training epoch: 8, train loss: 0.55768, val loss: 0.54556\n",
      "Main effects training epoch: 9, train loss: 0.53311, val loss: 0.51539\n",
      "Main effects training epoch: 10, train loss: 0.53333, val loss: 0.50951\n",
      "Main effects training epoch: 11, train loss: 0.52744, val loss: 0.50462\n",
      "Main effects training epoch: 12, train loss: 0.52419, val loss: 0.50058\n",
      "Main effects training epoch: 13, train loss: 0.52380, val loss: 0.50171\n",
      "Main effects training epoch: 14, train loss: 0.52430, val loss: 0.50254\n",
      "Main effects training epoch: 15, train loss: 0.52404, val loss: 0.50008\n",
      "Main effects training epoch: 16, train loss: 0.52291, val loss: 0.50020\n",
      "Main effects training epoch: 17, train loss: 0.52319, val loss: 0.50150\n",
      "Main effects training epoch: 18, train loss: 0.52350, val loss: 0.49899\n",
      "Main effects training epoch: 19, train loss: 0.52529, val loss: 0.50469\n",
      "Main effects training epoch: 20, train loss: 0.52344, val loss: 0.49948\n",
      "Main effects training epoch: 21, train loss: 0.52258, val loss: 0.50053\n",
      "Main effects training epoch: 22, train loss: 0.52256, val loss: 0.50054\n",
      "Main effects training epoch: 23, train loss: 0.52295, val loss: 0.49839\n",
      "Main effects training epoch: 24, train loss: 0.52237, val loss: 0.50063\n",
      "Main effects training epoch: 25, train loss: 0.52327, val loss: 0.49954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 26, train loss: 0.52269, val loss: 0.50079\n",
      "Main effects training epoch: 27, train loss: 0.52283, val loss: 0.50028\n",
      "Main effects training epoch: 28, train loss: 0.52300, val loss: 0.50073\n",
      "Main effects training epoch: 29, train loss: 0.52251, val loss: 0.50006\n",
      "Main effects training epoch: 30, train loss: 0.52285, val loss: 0.50128\n",
      "Main effects training epoch: 31, train loss: 0.52220, val loss: 0.49995\n",
      "Main effects training epoch: 32, train loss: 0.52182, val loss: 0.49998\n",
      "Main effects training epoch: 33, train loss: 0.52192, val loss: 0.49934\n",
      "Main effects training epoch: 34, train loss: 0.52136, val loss: 0.49938\n",
      "Main effects training epoch: 35, train loss: 0.52131, val loss: 0.49944\n",
      "Main effects training epoch: 36, train loss: 0.52135, val loss: 0.49948\n",
      "Main effects training epoch: 37, train loss: 0.52112, val loss: 0.49933\n",
      "Main effects training epoch: 38, train loss: 0.52140, val loss: 0.49988\n",
      "Main effects training epoch: 39, train loss: 0.52166, val loss: 0.50022\n",
      "Main effects training epoch: 40, train loss: 0.52116, val loss: 0.49834\n",
      "Main effects training epoch: 41, train loss: 0.52197, val loss: 0.50153\n",
      "Main effects training epoch: 42, train loss: 0.52161, val loss: 0.49920\n",
      "Main effects training epoch: 43, train loss: 0.52155, val loss: 0.50166\n",
      "Main effects training epoch: 44, train loss: 0.52132, val loss: 0.49914\n",
      "Main effects training epoch: 45, train loss: 0.52229, val loss: 0.50040\n",
      "Main effects training epoch: 46, train loss: 0.52104, val loss: 0.50009\n",
      "Main effects training epoch: 47, train loss: 0.52120, val loss: 0.50034\n",
      "Main effects training epoch: 48, train loss: 0.52091, val loss: 0.49979\n",
      "Main effects training epoch: 49, train loss: 0.52061, val loss: 0.49932\n",
      "Main effects training epoch: 50, train loss: 0.52082, val loss: 0.49930\n",
      "Main effects training epoch: 51, train loss: 0.52080, val loss: 0.49967\n",
      "Main effects training epoch: 52, train loss: 0.52125, val loss: 0.50129\n",
      "Main effects training epoch: 53, train loss: 0.52146, val loss: 0.49877\n",
      "Main effects training epoch: 54, train loss: 0.52059, val loss: 0.49998\n",
      "Main effects training epoch: 55, train loss: 0.52056, val loss: 0.49928\n",
      "Main effects training epoch: 56, train loss: 0.52052, val loss: 0.49942\n",
      "Main effects training epoch: 57, train loss: 0.52040, val loss: 0.50013\n",
      "Main effects training epoch: 58, train loss: 0.52033, val loss: 0.49916\n",
      "Main effects training epoch: 59, train loss: 0.52019, val loss: 0.49952\n",
      "Main effects training epoch: 60, train loss: 0.52010, val loss: 0.49971\n",
      "Main effects training epoch: 61, train loss: 0.52045, val loss: 0.49855\n",
      "Main effects training epoch: 62, train loss: 0.52006, val loss: 0.50061\n",
      "Main effects training epoch: 63, train loss: 0.52034, val loss: 0.49949\n",
      "Main effects training epoch: 64, train loss: 0.52105, val loss: 0.50132\n",
      "Main effects training epoch: 65, train loss: 0.52068, val loss: 0.50056\n",
      "Main effects training epoch: 66, train loss: 0.51992, val loss: 0.49917\n",
      "Main effects training epoch: 67, train loss: 0.52005, val loss: 0.49962\n",
      "Main effects training epoch: 68, train loss: 0.52025, val loss: 0.50204\n",
      "Main effects training epoch: 69, train loss: 0.52062, val loss: 0.50031\n",
      "Main effects training epoch: 70, train loss: 0.52058, val loss: 0.50258\n",
      "Main effects training epoch: 71, train loss: 0.52004, val loss: 0.49993\n",
      "Main effects training epoch: 72, train loss: 0.52182, val loss: 0.50545\n",
      "Main effects training epoch: 73, train loss: 0.52130, val loss: 0.50086\n",
      "Main effects training epoch: 74, train loss: 0.51979, val loss: 0.50184\n",
      "Main effects training epoch: 75, train loss: 0.51937, val loss: 0.50008\n",
      "Main effects training epoch: 76, train loss: 0.51954, val loss: 0.50202\n",
      "Main effects training epoch: 77, train loss: 0.52013, val loss: 0.50104\n",
      "Main effects training epoch: 78, train loss: 0.51941, val loss: 0.50079\n",
      "Main effects training epoch: 79, train loss: 0.51978, val loss: 0.50040\n",
      "Main effects training epoch: 80, train loss: 0.51944, val loss: 0.50171\n",
      "Main effects training epoch: 81, train loss: 0.51898, val loss: 0.49984\n",
      "Main effects training epoch: 82, train loss: 0.51887, val loss: 0.50095\n",
      "Main effects training epoch: 83, train loss: 0.51906, val loss: 0.50057\n",
      "Main effects training epoch: 84, train loss: 0.51928, val loss: 0.50055\n",
      "Main effects training epoch: 85, train loss: 0.51893, val loss: 0.50079\n",
      "Main effects training epoch: 86, train loss: 0.51906, val loss: 0.49976\n",
      "Main effects training epoch: 87, train loss: 0.51883, val loss: 0.50300\n",
      "Main effects training epoch: 88, train loss: 0.51859, val loss: 0.50052\n",
      "Main effects training epoch: 89, train loss: 0.51908, val loss: 0.49998\n",
      "Main effects training epoch: 90, train loss: 0.51933, val loss: 0.50233\n",
      "Main effects training epoch: 91, train loss: 0.51946, val loss: 0.50172\n",
      "Main effects training epoch: 92, train loss: 0.51931, val loss: 0.50296\n",
      "Main effects training epoch: 93, train loss: 0.52034, val loss: 0.50175\n",
      "Main effects training epoch: 94, train loss: 0.51925, val loss: 0.50181\n",
      "Main effects training epoch: 95, train loss: 0.51846, val loss: 0.50213\n",
      "Main effects training epoch: 96, train loss: 0.51868, val loss: 0.50024\n",
      "Main effects training epoch: 97, train loss: 0.51849, val loss: 0.50120\n",
      "Main effects training epoch: 98, train loss: 0.51916, val loss: 0.50052\n",
      "Main effects training epoch: 99, train loss: 0.51871, val loss: 0.50184\n",
      "Main effects training epoch: 100, train loss: 0.51829, val loss: 0.50106\n",
      "Main effects training epoch: 101, train loss: 0.51820, val loss: 0.50068\n",
      "Main effects training epoch: 102, train loss: 0.51829, val loss: 0.50225\n",
      "Main effects training epoch: 103, train loss: 0.51817, val loss: 0.50063\n",
      "Main effects training epoch: 104, train loss: 0.51823, val loss: 0.50030\n",
      "Main effects training epoch: 105, train loss: 0.51870, val loss: 0.50219\n",
      "Main effects training epoch: 106, train loss: 0.51821, val loss: 0.50154\n",
      "Main effects training epoch: 107, train loss: 0.51816, val loss: 0.50158\n",
      "Main effects training epoch: 108, train loss: 0.51816, val loss: 0.50035\n",
      "Main effects training epoch: 109, train loss: 0.51794, val loss: 0.50139\n",
      "Main effects training epoch: 110, train loss: 0.51782, val loss: 0.50036\n",
      "Main effects training epoch: 111, train loss: 0.51787, val loss: 0.50113\n",
      "Main effects training epoch: 112, train loss: 0.51788, val loss: 0.50238\n",
      "Main effects training epoch: 113, train loss: 0.51838, val loss: 0.50068\n",
      "Main effects training epoch: 114, train loss: 0.51844, val loss: 0.50246\n",
      "Main effects training epoch: 115, train loss: 0.51849, val loss: 0.49988\n",
      "Main effects training epoch: 116, train loss: 0.51862, val loss: 0.50317\n",
      "Main effects training epoch: 117, train loss: 0.51776, val loss: 0.50127\n",
      "Main effects training epoch: 118, train loss: 0.51747, val loss: 0.50162\n",
      "Main effects training epoch: 119, train loss: 0.51770, val loss: 0.50032\n",
      "Main effects training epoch: 120, train loss: 0.51755, val loss: 0.50184\n",
      "Main effects training epoch: 121, train loss: 0.51770, val loss: 0.49969\n",
      "Main effects training epoch: 122, train loss: 0.51814, val loss: 0.50370\n",
      "Main effects training epoch: 123, train loss: 0.51768, val loss: 0.49960\n",
      "Main effects training epoch: 124, train loss: 0.51738, val loss: 0.50230\n",
      "Main effects training epoch: 125, train loss: 0.51732, val loss: 0.50105\n",
      "Main effects training epoch: 126, train loss: 0.51767, val loss: 0.50162\n",
      "Main effects training epoch: 127, train loss: 0.51750, val loss: 0.50128\n",
      "Main effects training epoch: 128, train loss: 0.51770, val loss: 0.50167\n",
      "Main effects training epoch: 129, train loss: 0.51735, val loss: 0.50046\n",
      "Main effects training epoch: 130, train loss: 0.51700, val loss: 0.50186\n",
      "Main effects training epoch: 131, train loss: 0.51687, val loss: 0.50026\n",
      "Main effects training epoch: 132, train loss: 0.51790, val loss: 0.50090\n",
      "Main effects training epoch: 133, train loss: 0.51759, val loss: 0.50207\n",
      "Main effects training epoch: 134, train loss: 0.51738, val loss: 0.50134\n",
      "Main effects training epoch: 135, train loss: 0.51736, val loss: 0.50127\n",
      "Main effects training epoch: 136, train loss: 0.51752, val loss: 0.50082\n",
      "Main effects training epoch: 137, train loss: 0.51673, val loss: 0.50003\n",
      "Main effects training epoch: 138, train loss: 0.51653, val loss: 0.50057\n",
      "Main effects training epoch: 139, train loss: 0.51705, val loss: 0.50259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 140, train loss: 0.51696, val loss: 0.49990\n",
      "Main effects training epoch: 141, train loss: 0.51701, val loss: 0.50288\n",
      "Early stop at epoch 141, with validation loss: 0.50288\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51888, val loss: 0.50034\n",
      "Main effects tuning epoch: 2, train loss: 0.51830, val loss: 0.50139\n",
      "Main effects tuning epoch: 3, train loss: 0.51825, val loss: 0.50173\n",
      "Main effects tuning epoch: 4, train loss: 0.51820, val loss: 0.50137\n",
      "Main effects tuning epoch: 5, train loss: 0.51810, val loss: 0.50043\n",
      "Main effects tuning epoch: 6, train loss: 0.51874, val loss: 0.50223\n",
      "Main effects tuning epoch: 7, train loss: 0.51867, val loss: 0.50190\n",
      "Main effects tuning epoch: 8, train loss: 0.51823, val loss: 0.50144\n",
      "Main effects tuning epoch: 9, train loss: 0.51810, val loss: 0.50211\n",
      "Main effects tuning epoch: 10, train loss: 0.51807, val loss: 0.50120\n",
      "Main effects tuning epoch: 11, train loss: 0.51767, val loss: 0.50118\n",
      "Main effects tuning epoch: 12, train loss: 0.51783, val loss: 0.50284\n",
      "Main effects tuning epoch: 13, train loss: 0.51772, val loss: 0.50013\n",
      "Main effects tuning epoch: 14, train loss: 0.51745, val loss: 0.50135\n",
      "Main effects tuning epoch: 15, train loss: 0.51849, val loss: 0.50226\n",
      "Main effects tuning epoch: 16, train loss: 0.51898, val loss: 0.50405\n",
      "Main effects tuning epoch: 17, train loss: 0.51782, val loss: 0.49962\n",
      "Main effects tuning epoch: 18, train loss: 0.51748, val loss: 0.50150\n",
      "Main effects tuning epoch: 19, train loss: 0.51870, val loss: 0.50308\n",
      "Main effects tuning epoch: 20, train loss: 0.51769, val loss: 0.50133\n",
      "Main effects tuning epoch: 21, train loss: 0.51757, val loss: 0.50152\n",
      "Main effects tuning epoch: 22, train loss: 0.51725, val loss: 0.50117\n",
      "Main effects tuning epoch: 23, train loss: 0.51684, val loss: 0.50109\n",
      "Main effects tuning epoch: 24, train loss: 0.51689, val loss: 0.50151\n",
      "Main effects tuning epoch: 25, train loss: 0.51691, val loss: 0.49952\n",
      "Main effects tuning epoch: 26, train loss: 0.51660, val loss: 0.50027\n",
      "Main effects tuning epoch: 27, train loss: 0.51701, val loss: 0.50035\n",
      "Main effects tuning epoch: 28, train loss: 0.51746, val loss: 0.50290\n",
      "Main effects tuning epoch: 29, train loss: 0.51702, val loss: 0.50051\n",
      "Main effects tuning epoch: 30, train loss: 0.51701, val loss: 0.49947\n",
      "Main effects tuning epoch: 31, train loss: 0.51657, val loss: 0.50138\n",
      "Main effects tuning epoch: 32, train loss: 0.51682, val loss: 0.50182\n",
      "Main effects tuning epoch: 33, train loss: 0.51692, val loss: 0.49967\n",
      "Main effects tuning epoch: 34, train loss: 0.51690, val loss: 0.50361\n",
      "Main effects tuning epoch: 35, train loss: 0.51742, val loss: 0.49999\n",
      "Main effects tuning epoch: 36, train loss: 0.51603, val loss: 0.50145\n",
      "Main effects tuning epoch: 37, train loss: 0.51595, val loss: 0.50000\n",
      "Main effects tuning epoch: 38, train loss: 0.51641, val loss: 0.50102\n",
      "Main effects tuning epoch: 39, train loss: 0.51609, val loss: 0.50095\n",
      "Main effects tuning epoch: 40, train loss: 0.51589, val loss: 0.50043\n",
      "Main effects tuning epoch: 41, train loss: 0.51562, val loss: 0.49988\n",
      "Main effects tuning epoch: 42, train loss: 0.51654, val loss: 0.50148\n",
      "Main effects tuning epoch: 43, train loss: 0.51657, val loss: 0.49952\n",
      "Main effects tuning epoch: 44, train loss: 0.51639, val loss: 0.50272\n",
      "Main effects tuning epoch: 45, train loss: 0.51635, val loss: 0.50111\n",
      "Main effects tuning epoch: 46, train loss: 0.51589, val loss: 0.50069\n",
      "Main effects tuning epoch: 47, train loss: 0.51566, val loss: 0.50011\n",
      "Main effects tuning epoch: 48, train loss: 0.51567, val loss: 0.50036\n",
      "Main effects tuning epoch: 49, train loss: 0.51537, val loss: 0.50064\n",
      "Main effects tuning epoch: 50, train loss: 0.51572, val loss: 0.50161\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.47346, val loss: 0.46454\n",
      "Interaction training epoch: 2, train loss: 0.36463, val loss: 0.36528\n",
      "Interaction training epoch: 3, train loss: 0.30562, val loss: 0.30969\n",
      "Interaction training epoch: 4, train loss: 0.30416, val loss: 0.30352\n",
      "Interaction training epoch: 5, train loss: 0.30880, val loss: 0.31189\n",
      "Interaction training epoch: 6, train loss: 0.29446, val loss: 0.29924\n",
      "Interaction training epoch: 7, train loss: 0.28707, val loss: 0.29237\n",
      "Interaction training epoch: 8, train loss: 0.28395, val loss: 0.29409\n",
      "Interaction training epoch: 9, train loss: 0.28284, val loss: 0.29181\n",
      "Interaction training epoch: 10, train loss: 0.27961, val loss: 0.28715\n",
      "Interaction training epoch: 11, train loss: 0.28168, val loss: 0.29300\n",
      "Interaction training epoch: 12, train loss: 0.27493, val loss: 0.28360\n",
      "Interaction training epoch: 13, train loss: 0.28302, val loss: 0.29305\n",
      "Interaction training epoch: 14, train loss: 0.28471, val loss: 0.29356\n",
      "Interaction training epoch: 15, train loss: 0.28640, val loss: 0.29424\n",
      "Interaction training epoch: 16, train loss: 0.27896, val loss: 0.28725\n",
      "Interaction training epoch: 17, train loss: 0.28507, val loss: 0.29541\n",
      "Interaction training epoch: 18, train loss: 0.28580, val loss: 0.29734\n",
      "Interaction training epoch: 19, train loss: 0.27836, val loss: 0.29515\n",
      "Interaction training epoch: 20, train loss: 0.27889, val loss: 0.28965\n",
      "Interaction training epoch: 21, train loss: 0.28084, val loss: 0.29465\n",
      "Interaction training epoch: 22, train loss: 0.27885, val loss: 0.29104\n",
      "Interaction training epoch: 23, train loss: 0.27543, val loss: 0.28599\n",
      "Interaction training epoch: 24, train loss: 0.27632, val loss: 0.28794\n",
      "Interaction training epoch: 25, train loss: 0.27641, val loss: 0.29070\n",
      "Interaction training epoch: 26, train loss: 0.27249, val loss: 0.28752\n",
      "Interaction training epoch: 27, train loss: 0.27309, val loss: 0.28509\n",
      "Interaction training epoch: 28, train loss: 0.27180, val loss: 0.28849\n",
      "Interaction training epoch: 29, train loss: 0.27262, val loss: 0.28583\n",
      "Interaction training epoch: 30, train loss: 0.27810, val loss: 0.28697\n",
      "Interaction training epoch: 31, train loss: 0.28104, val loss: 0.29311\n",
      "Interaction training epoch: 32, train loss: 0.27323, val loss: 0.28492\n",
      "Interaction training epoch: 33, train loss: 0.27056, val loss: 0.28057\n",
      "Interaction training epoch: 34, train loss: 0.26963, val loss: 0.28814\n",
      "Interaction training epoch: 35, train loss: 0.27169, val loss: 0.28344\n",
      "Interaction training epoch: 36, train loss: 0.26974, val loss: 0.28076\n",
      "Interaction training epoch: 37, train loss: 0.26905, val loss: 0.28830\n",
      "Interaction training epoch: 38, train loss: 0.27444, val loss: 0.28896\n",
      "Interaction training epoch: 39, train loss: 0.26716, val loss: 0.28044\n",
      "Interaction training epoch: 40, train loss: 0.26844, val loss: 0.28558\n",
      "Interaction training epoch: 41, train loss: 0.27417, val loss: 0.28698\n",
      "Interaction training epoch: 42, train loss: 0.26780, val loss: 0.28555\n",
      "Interaction training epoch: 43, train loss: 0.26868, val loss: 0.28833\n",
      "Interaction training epoch: 44, train loss: 0.27016, val loss: 0.28194\n",
      "Interaction training epoch: 45, train loss: 0.27345, val loss: 0.29410\n",
      "Interaction training epoch: 46, train loss: 0.26323, val loss: 0.28344\n",
      "Interaction training epoch: 47, train loss: 0.27234, val loss: 0.28496\n",
      "Interaction training epoch: 48, train loss: 0.26689, val loss: 0.28903\n",
      "Interaction training epoch: 49, train loss: 0.26570, val loss: 0.28442\n",
      "Interaction training epoch: 50, train loss: 0.26630, val loss: 0.28560\n",
      "Interaction training epoch: 51, train loss: 0.26564, val loss: 0.28441\n",
      "Interaction training epoch: 52, train loss: 0.26812, val loss: 0.29405\n",
      "Interaction training epoch: 53, train loss: 0.26253, val loss: 0.28256\n",
      "Interaction training epoch: 54, train loss: 0.27269, val loss: 0.29208\n",
      "Interaction training epoch: 55, train loss: 0.26714, val loss: 0.28576\n",
      "Interaction training epoch: 56, train loss: 0.26979, val loss: 0.28955\n",
      "Interaction training epoch: 57, train loss: 0.26692, val loss: 0.29256\n",
      "Interaction training epoch: 58, train loss: 0.26395, val loss: 0.28112\n",
      "Interaction training epoch: 59, train loss: 0.26691, val loss: 0.29124\n",
      "Interaction training epoch: 60, train loss: 0.26130, val loss: 0.28448\n",
      "Interaction training epoch: 61, train loss: 0.26426, val loss: 0.28689\n",
      "Interaction training epoch: 62, train loss: 0.26225, val loss: 0.28457\n",
      "Interaction training epoch: 63, train loss: 0.26016, val loss: 0.28344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 64, train loss: 0.26656, val loss: 0.29000\n",
      "Interaction training epoch: 65, train loss: 0.26104, val loss: 0.28801\n",
      "Interaction training epoch: 66, train loss: 0.26158, val loss: 0.28413\n",
      "Interaction training epoch: 67, train loss: 0.26160, val loss: 0.28669\n",
      "Interaction training epoch: 68, train loss: 0.25968, val loss: 0.28226\n",
      "Interaction training epoch: 69, train loss: 0.26280, val loss: 0.28724\n",
      "Interaction training epoch: 70, train loss: 0.25878, val loss: 0.28698\n",
      "Interaction training epoch: 71, train loss: 0.26304, val loss: 0.28551\n",
      "Interaction training epoch: 72, train loss: 0.26190, val loss: 0.28833\n",
      "Interaction training epoch: 73, train loss: 0.25801, val loss: 0.28336\n",
      "Interaction training epoch: 74, train loss: 0.25991, val loss: 0.28593\n",
      "Interaction training epoch: 75, train loss: 0.25750, val loss: 0.28384\n",
      "Interaction training epoch: 76, train loss: 0.26111, val loss: 0.28396\n",
      "Interaction training epoch: 77, train loss: 0.25874, val loss: 0.28792\n",
      "Interaction training epoch: 78, train loss: 0.25977, val loss: 0.28563\n",
      "Interaction training epoch: 79, train loss: 0.25761, val loss: 0.28129\n",
      "Interaction training epoch: 80, train loss: 0.25785, val loss: 0.28626\n",
      "Interaction training epoch: 81, train loss: 0.25830, val loss: 0.28394\n",
      "Interaction training epoch: 82, train loss: 0.25752, val loss: 0.28555\n",
      "Interaction training epoch: 83, train loss: 0.25508, val loss: 0.28119\n",
      "Interaction training epoch: 84, train loss: 0.25617, val loss: 0.28547\n",
      "Interaction training epoch: 85, train loss: 0.25758, val loss: 0.28367\n",
      "Interaction training epoch: 86, train loss: 0.25447, val loss: 0.28256\n",
      "Interaction training epoch: 87, train loss: 0.25785, val loss: 0.28382\n",
      "Interaction training epoch: 88, train loss: 0.25818, val loss: 0.28553\n",
      "Interaction training epoch: 89, train loss: 0.25683, val loss: 0.28800\n",
      "Interaction training epoch: 90, train loss: 0.25760, val loss: 0.28288\n",
      "Interaction training epoch: 91, train loss: 0.25648, val loss: 0.28413\n",
      "Interaction training epoch: 92, train loss: 0.25477, val loss: 0.28303\n",
      "Interaction training epoch: 93, train loss: 0.25622, val loss: 0.28190\n",
      "Interaction training epoch: 94, train loss: 0.25598, val loss: 0.28494\n",
      "Interaction training epoch: 95, train loss: 0.25487, val loss: 0.28428\n",
      "Interaction training epoch: 96, train loss: 0.25387, val loss: 0.28105\n",
      "Interaction training epoch: 97, train loss: 0.25428, val loss: 0.28643\n",
      "Interaction training epoch: 98, train loss: 0.25359, val loss: 0.28071\n",
      "Interaction training epoch: 99, train loss: 0.25642, val loss: 0.28633\n",
      "Interaction training epoch: 100, train loss: 0.25679, val loss: 0.28615\n",
      "Interaction training epoch: 101, train loss: 0.25183, val loss: 0.28091\n",
      "Interaction training epoch: 102, train loss: 0.25421, val loss: 0.28198\n",
      "Interaction training epoch: 103, train loss: 0.25297, val loss: 0.28560\n",
      "Interaction training epoch: 104, train loss: 0.25678, val loss: 0.28420\n",
      "Interaction training epoch: 105, train loss: 0.25008, val loss: 0.27864\n",
      "Interaction training epoch: 106, train loss: 0.25566, val loss: 0.28531\n",
      "Interaction training epoch: 107, train loss: 0.25246, val loss: 0.28397\n",
      "Interaction training epoch: 108, train loss: 0.25281, val loss: 0.28117\n",
      "Interaction training epoch: 109, train loss: 0.25309, val loss: 0.28350\n",
      "Interaction training epoch: 110, train loss: 0.25188, val loss: 0.28323\n",
      "Interaction training epoch: 111, train loss: 0.25463, val loss: 0.28619\n",
      "Interaction training epoch: 112, train loss: 0.25084, val loss: 0.28036\n",
      "Interaction training epoch: 113, train loss: 0.25239, val loss: 0.28154\n",
      "Interaction training epoch: 114, train loss: 0.25342, val loss: 0.28466\n",
      "Interaction training epoch: 115, train loss: 0.25108, val loss: 0.27970\n",
      "Interaction training epoch: 116, train loss: 0.25307, val loss: 0.28410\n",
      "Interaction training epoch: 117, train loss: 0.25246, val loss: 0.28457\n",
      "Interaction training epoch: 118, train loss: 0.25081, val loss: 0.27889\n",
      "Interaction training epoch: 119, train loss: 0.25009, val loss: 0.28091\n",
      "Interaction training epoch: 120, train loss: 0.25113, val loss: 0.28137\n",
      "Interaction training epoch: 121, train loss: 0.24818, val loss: 0.27818\n",
      "Interaction training epoch: 122, train loss: 0.25036, val loss: 0.27783\n",
      "Interaction training epoch: 123, train loss: 0.25137, val loss: 0.28027\n",
      "Interaction training epoch: 124, train loss: 0.25397, val loss: 0.28838\n",
      "Interaction training epoch: 125, train loss: 0.24968, val loss: 0.27546\n",
      "Interaction training epoch: 126, train loss: 0.25170, val loss: 0.28313\n",
      "Interaction training epoch: 127, train loss: 0.25103, val loss: 0.28371\n",
      "Interaction training epoch: 128, train loss: 0.24969, val loss: 0.27777\n",
      "Interaction training epoch: 129, train loss: 0.24908, val loss: 0.28128\n",
      "Interaction training epoch: 130, train loss: 0.25024, val loss: 0.28059\n",
      "Interaction training epoch: 131, train loss: 0.24659, val loss: 0.27693\n",
      "Interaction training epoch: 132, train loss: 0.25004, val loss: 0.27778\n",
      "Interaction training epoch: 133, train loss: 0.24959, val loss: 0.27914\n",
      "Interaction training epoch: 134, train loss: 0.24758, val loss: 0.28387\n",
      "Interaction training epoch: 135, train loss: 0.24924, val loss: 0.28007\n",
      "Interaction training epoch: 136, train loss: 0.24517, val loss: 0.27592\n",
      "Interaction training epoch: 137, train loss: 0.24996, val loss: 0.27776\n",
      "Interaction training epoch: 138, train loss: 0.24960, val loss: 0.28120\n",
      "Interaction training epoch: 139, train loss: 0.24482, val loss: 0.27700\n",
      "Interaction training epoch: 140, train loss: 0.24772, val loss: 0.27976\n",
      "Interaction training epoch: 141, train loss: 0.24654, val loss: 0.27857\n",
      "Interaction training epoch: 142, train loss: 0.24531, val loss: 0.27667\n",
      "Interaction training epoch: 143, train loss: 0.24533, val loss: 0.27452\n",
      "Interaction training epoch: 144, train loss: 0.24453, val loss: 0.27717\n",
      "Interaction training epoch: 145, train loss: 0.24371, val loss: 0.27703\n",
      "Interaction training epoch: 146, train loss: 0.25114, val loss: 0.28177\n",
      "Interaction training epoch: 147, train loss: 0.24554, val loss: 0.27912\n",
      "Interaction training epoch: 148, train loss: 0.24343, val loss: 0.27433\n",
      "Interaction training epoch: 149, train loss: 0.24489, val loss: 0.27988\n",
      "Interaction training epoch: 150, train loss: 0.24471, val loss: 0.27837\n",
      "Interaction training epoch: 151, train loss: 0.24682, val loss: 0.27918\n",
      "Interaction training epoch: 152, train loss: 0.24384, val loss: 0.27837\n",
      "Interaction training epoch: 153, train loss: 0.24440, val loss: 0.27802\n",
      "Interaction training epoch: 154, train loss: 0.24307, val loss: 0.27338\n",
      "Interaction training epoch: 155, train loss: 0.24370, val loss: 0.27744\n",
      "Interaction training epoch: 156, train loss: 0.24109, val loss: 0.27475\n",
      "Interaction training epoch: 157, train loss: 0.24446, val loss: 0.27581\n",
      "Interaction training epoch: 158, train loss: 0.24529, val loss: 0.27799\n",
      "Interaction training epoch: 159, train loss: 0.24314, val loss: 0.27675\n",
      "Interaction training epoch: 160, train loss: 0.24454, val loss: 0.27759\n",
      "Interaction training epoch: 161, train loss: 0.24342, val loss: 0.27564\n",
      "Interaction training epoch: 162, train loss: 0.24557, val loss: 0.27953\n",
      "Interaction training epoch: 163, train loss: 0.23991, val loss: 0.27212\n",
      "Interaction training epoch: 164, train loss: 0.24448, val loss: 0.27758\n",
      "Interaction training epoch: 165, train loss: 0.24314, val loss: 0.27541\n",
      "Interaction training epoch: 166, train loss: 0.24256, val loss: 0.27816\n",
      "Interaction training epoch: 167, train loss: 0.23985, val loss: 0.27196\n",
      "Interaction training epoch: 168, train loss: 0.24111, val loss: 0.27691\n",
      "Interaction training epoch: 169, train loss: 0.24180, val loss: 0.27636\n",
      "Interaction training epoch: 170, train loss: 0.24412, val loss: 0.27568\n",
      "Interaction training epoch: 171, train loss: 0.24087, val loss: 0.27471\n",
      "Interaction training epoch: 172, train loss: 0.24150, val loss: 0.27648\n",
      "Interaction training epoch: 173, train loss: 0.24304, val loss: 0.27419\n",
      "Interaction training epoch: 174, train loss: 0.23932, val loss: 0.27396\n",
      "Interaction training epoch: 175, train loss: 0.24272, val loss: 0.27579\n",
      "Interaction training epoch: 176, train loss: 0.24111, val loss: 0.27301\n",
      "Interaction training epoch: 177, train loss: 0.24099, val loss: 0.27603\n",
      "Interaction training epoch: 178, train loss: 0.24188, val loss: 0.27757\n",
      "Interaction training epoch: 179, train loss: 0.24360, val loss: 0.27730\n",
      "Interaction training epoch: 180, train loss: 0.24058, val loss: 0.27571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 181, train loss: 0.23865, val loss: 0.27270\n",
      "Interaction training epoch: 182, train loss: 0.24150, val loss: 0.27450\n",
      "Interaction training epoch: 183, train loss: 0.24137, val loss: 0.27655\n",
      "Interaction training epoch: 184, train loss: 0.24102, val loss: 0.27126\n",
      "Interaction training epoch: 185, train loss: 0.24432, val loss: 0.27740\n",
      "Interaction training epoch: 186, train loss: 0.24154, val loss: 0.27515\n",
      "Interaction training epoch: 187, train loss: 0.24029, val loss: 0.27755\n",
      "Interaction training epoch: 188, train loss: 0.24098, val loss: 0.27366\n",
      "Interaction training epoch: 189, train loss: 0.24041, val loss: 0.27539\n",
      "Interaction training epoch: 190, train loss: 0.24133, val loss: 0.27419\n",
      "Interaction training epoch: 191, train loss: 0.23901, val loss: 0.27254\n",
      "Interaction training epoch: 192, train loss: 0.23822, val loss: 0.27437\n",
      "Interaction training epoch: 193, train loss: 0.23777, val loss: 0.26996\n",
      "Interaction training epoch: 194, train loss: 0.23900, val loss: 0.27366\n",
      "Interaction training epoch: 195, train loss: 0.23617, val loss: 0.27475\n",
      "Interaction training epoch: 196, train loss: 0.24024, val loss: 0.27330\n",
      "Interaction training epoch: 197, train loss: 0.23721, val loss: 0.27351\n",
      "Interaction training epoch: 198, train loss: 0.23785, val loss: 0.27565\n",
      "Interaction training epoch: 199, train loss: 0.23815, val loss: 0.27115\n",
      "Interaction training epoch: 200, train loss: 0.23546, val loss: 0.27261\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########3 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.24751, val loss: 0.27121\n",
      "Interaction tuning epoch: 2, train loss: 0.24723, val loss: 0.27244\n",
      "Interaction tuning epoch: 3, train loss: 0.25107, val loss: 0.27322\n",
      "Interaction tuning epoch: 4, train loss: 0.24640, val loss: 0.27117\n",
      "Interaction tuning epoch: 5, train loss: 0.24648, val loss: 0.27052\n",
      "Interaction tuning epoch: 6, train loss: 0.24591, val loss: 0.27076\n",
      "Interaction tuning epoch: 7, train loss: 0.24821, val loss: 0.26891\n",
      "Interaction tuning epoch: 8, train loss: 0.24437, val loss: 0.26923\n",
      "Interaction tuning epoch: 9, train loss: 0.24654, val loss: 0.27175\n",
      "Interaction tuning epoch: 10, train loss: 0.24468, val loss: 0.26997\n",
      "Interaction tuning epoch: 11, train loss: 0.24465, val loss: 0.26806\n",
      "Interaction tuning epoch: 12, train loss: 0.24495, val loss: 0.26805\n",
      "Interaction tuning epoch: 13, train loss: 0.24467, val loss: 0.26836\n",
      "Interaction tuning epoch: 14, train loss: 0.24286, val loss: 0.26571\n",
      "Interaction tuning epoch: 15, train loss: 0.24568, val loss: 0.27100\n",
      "Interaction tuning epoch: 16, train loss: 0.24668, val loss: 0.27200\n",
      "Interaction tuning epoch: 17, train loss: 0.24453, val loss: 0.26554\n",
      "Interaction tuning epoch: 18, train loss: 0.24621, val loss: 0.27056\n",
      "Interaction tuning epoch: 19, train loss: 0.24594, val loss: 0.26609\n",
      "Interaction tuning epoch: 20, train loss: 0.24384, val loss: 0.26711\n",
      "Interaction tuning epoch: 21, train loss: 0.24500, val loss: 0.26645\n",
      "Interaction tuning epoch: 22, train loss: 0.24261, val loss: 0.26665\n",
      "Interaction tuning epoch: 23, train loss: 0.24838, val loss: 0.26928\n",
      "Interaction tuning epoch: 24, train loss: 0.24568, val loss: 0.27139\n",
      "Interaction tuning epoch: 25, train loss: 0.24696, val loss: 0.26787\n",
      "Interaction tuning epoch: 26, train loss: 0.24596, val loss: 0.27193\n",
      "Interaction tuning epoch: 27, train loss: 0.24284, val loss: 0.26659\n",
      "Interaction tuning epoch: 28, train loss: 0.24304, val loss: 0.26521\n",
      "Interaction tuning epoch: 29, train loss: 0.24231, val loss: 0.26953\n",
      "Interaction tuning epoch: 30, train loss: 0.24119, val loss: 0.26570\n",
      "Interaction tuning epoch: 31, train loss: 0.24277, val loss: 0.26647\n",
      "Interaction tuning epoch: 32, train loss: 0.24152, val loss: 0.26702\n",
      "Interaction tuning epoch: 33, train loss: 0.24354, val loss: 0.26458\n",
      "Interaction tuning epoch: 34, train loss: 0.24286, val loss: 0.26963\n",
      "Interaction tuning epoch: 35, train loss: 0.24106, val loss: 0.26678\n",
      "Interaction tuning epoch: 36, train loss: 0.24329, val loss: 0.26694\n",
      "Interaction tuning epoch: 37, train loss: 0.24556, val loss: 0.27210\n",
      "Interaction tuning epoch: 38, train loss: 0.24429, val loss: 0.26866\n",
      "Interaction tuning epoch: 39, train loss: 0.24408, val loss: 0.26676\n",
      "Interaction tuning epoch: 40, train loss: 0.24367, val loss: 0.26768\n",
      "Interaction tuning epoch: 41, train loss: 0.24506, val loss: 0.27145\n",
      "Interaction tuning epoch: 42, train loss: 0.24620, val loss: 0.26764\n",
      "Interaction tuning epoch: 43, train loss: 0.24341, val loss: 0.27080\n",
      "Interaction tuning epoch: 44, train loss: 0.24137, val loss: 0.26521\n",
      "Interaction tuning epoch: 45, train loss: 0.24392, val loss: 0.26928\n",
      "Interaction tuning epoch: 46, train loss: 0.24160, val loss: 0.26935\n",
      "Interaction tuning epoch: 47, train loss: 0.24176, val loss: 0.26373\n",
      "Interaction tuning epoch: 48, train loss: 0.24182, val loss: 0.26636\n",
      "Interaction tuning epoch: 49, train loss: 0.23996, val loss: 0.26790\n",
      "Interaction tuning epoch: 50, train loss: 0.24049, val loss: 0.26553\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 43.40327262878418\n",
      "After the gam stage, training error is 0.24049 , validation error is 0.26553\n",
      "missing value counts: 99151\n",
      "[SoftImpute] Max Singular Value of X_init = 3.712008\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed BCE=0.203855 validation BCE=0.295455,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 2: observed BCE=0.200211 validation BCE=0.294549,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.197875 validation BCE=0.293775,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.196494 validation BCE=0.281626,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.195794 validation BCE=0.280144,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.195048 validation BCE=0.278683,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.194283 validation BCE=0.277659,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.194481 validation BCE=0.266419,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.194897 validation BCE=0.265375,rank=5\n",
      "[SoftImpute] Iter 10: observed BCE=0.195239 validation BCE=0.263347,rank=5\n",
      "[SoftImpute] Iter 11: observed BCE=0.195187 validation BCE=0.262836,rank=5\n",
      "[SoftImpute] Iter 12: observed BCE=0.195220 validation BCE=0.261849,rank=5\n",
      "[SoftImpute] Iter 13: observed BCE=0.195378 validation BCE=0.261550,rank=5\n",
      "[SoftImpute] Iter 14: observed BCE=0.195383 validation BCE=0.260865,rank=5\n",
      "[SoftImpute] Iter 15: observed BCE=0.195228 validation BCE=0.259788,rank=5\n",
      "[SoftImpute] Iter 16: observed BCE=0.195208 validation BCE=0.259410,rank=5\n",
      "[SoftImpute] Iter 17: observed BCE=0.196707 validation BCE=0.260026,rank=5\n",
      "[SoftImpute] Iter 18: observed BCE=0.197923 validation BCE=0.259731,rank=5\n",
      "[SoftImpute] Iter 19: observed BCE=0.198598 validation BCE=0.259388,rank=5\n",
      "[SoftImpute] Iter 20: observed BCE=0.198501 validation BCE=0.259372,rank=5\n",
      "[SoftImpute] Iter 21: observed BCE=0.198582 validation BCE=0.259604,rank=5\n",
      "[SoftImpute] Iter 22: observed BCE=0.198735 validation BCE=0.259110,rank=5\n",
      "[SoftImpute] Iter 23: observed BCE=0.199211 validation BCE=0.259222,rank=5\n",
      "[SoftImpute] Iter 24: observed BCE=0.199256 validation BCE=0.259052,rank=5\n",
      "[SoftImpute] Iter 25: observed BCE=0.198965 validation BCE=0.259418,rank=5\n",
      "[SoftImpute] Iter 26: observed BCE=0.199676 validation BCE=0.259466,rank=5\n",
      "[SoftImpute] Iter 27: observed BCE=0.199845 validation BCE=0.250298,rank=5\n",
      "[SoftImpute] Iter 28: observed BCE=0.199787 validation BCE=0.259480,rank=5\n",
      "[SoftImpute] Iter 29: observed BCE=0.199576 validation BCE=0.259139,rank=5\n",
      "[SoftImpute] Iter 30: observed BCE=0.199601 validation BCE=0.249896,rank=5\n",
      "[SoftImpute] Iter 31: observed BCE=0.199645 validation BCE=0.252203,rank=5\n",
      "[SoftImpute] Iter 32: observed BCE=0.199539 validation BCE=0.259419,rank=5\n",
      "[SoftImpute] Iter 33: observed BCE=0.199594 validation BCE=0.248931,rank=5\n",
      "[SoftImpute] Iter 34: observed BCE=0.199670 validation BCE=0.250206,rank=5\n",
      "[SoftImpute] Iter 35: observed BCE=0.199539 validation BCE=0.251447,rank=5\n",
      "[SoftImpute] Iter 36: observed BCE=0.199454 validation BCE=0.249052,rank=5\n",
      "[SoftImpute] Iter 37: observed BCE=0.199556 validation BCE=0.249275,rank=5\n",
      "[SoftImpute] Iter 38: observed BCE=0.199632 validation BCE=0.250496,rank=5\n",
      "[SoftImpute] Iter 39: observed BCE=0.199454 validation BCE=0.248805,rank=5\n",
      "[SoftImpute] Iter 40: observed BCE=0.199658 validation BCE=0.249467,rank=5\n",
      "[SoftImpute] Iter 41: observed BCE=0.198716 validation BCE=0.249300,rank=5\n",
      "[SoftImpute] Iter 42: observed BCE=0.198285 validation BCE=0.258203,rank=5\n",
      "[SoftImpute] Iter 43: observed BCE=0.198168 validation BCE=0.258418,rank=5\n",
      "[SoftImpute] Iter 44: observed BCE=0.197913 validation BCE=0.258279,rank=5\n",
      "[SoftImpute] Iter 45: observed BCE=0.197893 validation BCE=0.258013,rank=5\n",
      "[SoftImpute] Iter 46: observed BCE=0.197754 validation BCE=0.258614,rank=5\n",
      "[SoftImpute] Iter 47: observed BCE=0.197810 validation BCE=0.258214,rank=5\n",
      "[SoftImpute] Iter 48: observed BCE=0.197798 validation BCE=0.258042,rank=5\n",
      "[SoftImpute] Iter 49: observed BCE=0.197880 validation BCE=0.258535,rank=5\n",
      "[SoftImpute] Iter 50: observed BCE=0.197600 validation BCE=0.258267,rank=5\n",
      "[SoftImpute] Iter 51: observed BCE=0.197618 validation BCE=0.257921,rank=5\n",
      "[SoftImpute] Iter 52: observed BCE=0.197419 validation BCE=0.258109,rank=5\n",
      "[SoftImpute] Iter 53: observed BCE=0.197423 validation BCE=0.257908,rank=5\n",
      "[SoftImpute] Iter 54: observed BCE=0.197220 validation BCE=0.257643,rank=5\n",
      "[SoftImpute] Iter 55: observed BCE=0.197409 validation BCE=0.258038,rank=5\n",
      "[SoftImpute] Iter 56: observed BCE=0.197483 validation BCE=0.257926,rank=5\n",
      "[SoftImpute] Iter 57: observed BCE=0.197599 validation BCE=0.257968,rank=5\n",
      "[SoftImpute] Iter 58: observed BCE=0.197217 validation BCE=0.257973,rank=5\n",
      "[SoftImpute] Iter 59: observed BCE=0.197365 validation BCE=0.258160,rank=5\n",
      "[SoftImpute] Stopped after iteration 59 for lambda=0.074240\n",
      "final num of user group: 1\n",
      "final num of item group: 8\n",
      "change mode state : True\n",
      "time cost: 4.127037763595581\n",
      "After the matrix factor stage, training error is 0.19737, validation error is 0.25816\n",
      "7\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.67839, val loss: 0.68028\n",
      "Main effects training epoch: 2, train loss: 0.67304, val loss: 0.67705\n",
      "Main effects training epoch: 3, train loss: 0.66685, val loss: 0.67000\n",
      "Main effects training epoch: 4, train loss: 0.65688, val loss: 0.65900\n",
      "Main effects training epoch: 5, train loss: 0.63795, val loss: 0.63959\n",
      "Main effects training epoch: 6, train loss: 0.60903, val loss: 0.60754\n",
      "Main effects training epoch: 7, train loss: 0.57209, val loss: 0.56398\n",
      "Main effects training epoch: 8, train loss: 0.54414, val loss: 0.52465\n",
      "Main effects training epoch: 9, train loss: 0.53137, val loss: 0.50741\n",
      "Main effects training epoch: 10, train loss: 0.52890, val loss: 0.50692\n",
      "Main effects training epoch: 11, train loss: 0.52474, val loss: 0.50413\n",
      "Main effects training epoch: 12, train loss: 0.52464, val loss: 0.50439\n",
      "Main effects training epoch: 13, train loss: 0.52541, val loss: 0.50569\n",
      "Main effects training epoch: 14, train loss: 0.52326, val loss: 0.50328\n",
      "Main effects training epoch: 15, train loss: 0.52408, val loss: 0.50222\n",
      "Main effects training epoch: 16, train loss: 0.52372, val loss: 0.50285\n",
      "Main effects training epoch: 17, train loss: 0.52294, val loss: 0.50183\n",
      "Main effects training epoch: 18, train loss: 0.52339, val loss: 0.50276\n",
      "Main effects training epoch: 19, train loss: 0.52287, val loss: 0.50173\n",
      "Main effects training epoch: 20, train loss: 0.52250, val loss: 0.50169\n",
      "Main effects training epoch: 21, train loss: 0.52250, val loss: 0.50095\n",
      "Main effects training epoch: 22, train loss: 0.52330, val loss: 0.50348\n",
      "Main effects training epoch: 23, train loss: 0.52352, val loss: 0.50164\n",
      "Main effects training epoch: 24, train loss: 0.52473, val loss: 0.50546\n",
      "Main effects training epoch: 25, train loss: 0.52319, val loss: 0.50203\n",
      "Main effects training epoch: 26, train loss: 0.52279, val loss: 0.50182\n",
      "Main effects training epoch: 27, train loss: 0.52240, val loss: 0.50156\n",
      "Main effects training epoch: 28, train loss: 0.52250, val loss: 0.50167\n",
      "Main effects training epoch: 29, train loss: 0.52192, val loss: 0.50131\n",
      "Main effects training epoch: 30, train loss: 0.52270, val loss: 0.50260\n",
      "Main effects training epoch: 31, train loss: 0.52340, val loss: 0.50280\n",
      "Main effects training epoch: 32, train loss: 0.52219, val loss: 0.50204\n",
      "Main effects training epoch: 33, train loss: 0.52231, val loss: 0.50103\n",
      "Main effects training epoch: 34, train loss: 0.52166, val loss: 0.50164\n",
      "Main effects training epoch: 35, train loss: 0.52173, val loss: 0.50084\n",
      "Main effects training epoch: 36, train loss: 0.52167, val loss: 0.50154\n",
      "Main effects training epoch: 37, train loss: 0.52170, val loss: 0.50062\n",
      "Main effects training epoch: 38, train loss: 0.52181, val loss: 0.50167\n",
      "Main effects training epoch: 39, train loss: 0.52154, val loss: 0.50118\n",
      "Main effects training epoch: 40, train loss: 0.52198, val loss: 0.50123\n",
      "Main effects training epoch: 41, train loss: 0.52313, val loss: 0.50381\n",
      "Main effects training epoch: 42, train loss: 0.52194, val loss: 0.50023\n",
      "Main effects training epoch: 43, train loss: 0.52191, val loss: 0.50138\n",
      "Main effects training epoch: 44, train loss: 0.52302, val loss: 0.50244\n",
      "Main effects training epoch: 45, train loss: 0.52149, val loss: 0.50167\n",
      "Main effects training epoch: 46, train loss: 0.52173, val loss: 0.50019\n",
      "Main effects training epoch: 47, train loss: 0.52175, val loss: 0.50313\n",
      "Main effects training epoch: 48, train loss: 0.52116, val loss: 0.50080\n",
      "Main effects training epoch: 49, train loss: 0.52117, val loss: 0.50039\n",
      "Main effects training epoch: 50, train loss: 0.52105, val loss: 0.50137\n",
      "Main effects training epoch: 51, train loss: 0.52096, val loss: 0.50120\n",
      "Main effects training epoch: 52, train loss: 0.52136, val loss: 0.50071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 53, train loss: 0.52133, val loss: 0.50188\n",
      "Main effects training epoch: 54, train loss: 0.52084, val loss: 0.50064\n",
      "Main effects training epoch: 55, train loss: 0.52077, val loss: 0.50112\n",
      "Main effects training epoch: 56, train loss: 0.52105, val loss: 0.50033\n",
      "Main effects training epoch: 57, train loss: 0.52172, val loss: 0.50303\n",
      "Main effects training epoch: 58, train loss: 0.52095, val loss: 0.50082\n",
      "Main effects training epoch: 59, train loss: 0.52074, val loss: 0.50110\n",
      "Main effects training epoch: 60, train loss: 0.52106, val loss: 0.50178\n",
      "Main effects training epoch: 61, train loss: 0.52080, val loss: 0.50063\n",
      "Main effects training epoch: 62, train loss: 0.52070, val loss: 0.50068\n",
      "Main effects training epoch: 63, train loss: 0.52070, val loss: 0.50175\n",
      "Main effects training epoch: 64, train loss: 0.52061, val loss: 0.50124\n",
      "Main effects training epoch: 65, train loss: 0.52036, val loss: 0.50097\n",
      "Main effects training epoch: 66, train loss: 0.52062, val loss: 0.50233\n",
      "Main effects training epoch: 67, train loss: 0.52089, val loss: 0.50187\n",
      "Main effects training epoch: 68, train loss: 0.52061, val loss: 0.50103\n",
      "Main effects training epoch: 69, train loss: 0.52041, val loss: 0.50228\n",
      "Main effects training epoch: 70, train loss: 0.52008, val loss: 0.50173\n",
      "Main effects training epoch: 71, train loss: 0.52060, val loss: 0.50218\n",
      "Main effects training epoch: 72, train loss: 0.52015, val loss: 0.50102\n",
      "Main effects training epoch: 73, train loss: 0.52064, val loss: 0.50315\n",
      "Main effects training epoch: 74, train loss: 0.52046, val loss: 0.50179\n",
      "Main effects training epoch: 75, train loss: 0.52024, val loss: 0.50256\n",
      "Main effects training epoch: 76, train loss: 0.52009, val loss: 0.50150\n",
      "Main effects training epoch: 77, train loss: 0.51994, val loss: 0.50240\n",
      "Main effects training epoch: 78, train loss: 0.51989, val loss: 0.50213\n",
      "Main effects training epoch: 79, train loss: 0.51991, val loss: 0.50083\n",
      "Main effects training epoch: 80, train loss: 0.52125, val loss: 0.50383\n",
      "Main effects training epoch: 81, train loss: 0.52166, val loss: 0.50338\n",
      "Main effects training epoch: 82, train loss: 0.52009, val loss: 0.50293\n",
      "Main effects training epoch: 83, train loss: 0.51962, val loss: 0.50155\n",
      "Main effects training epoch: 84, train loss: 0.51979, val loss: 0.50115\n",
      "Main effects training epoch: 85, train loss: 0.51974, val loss: 0.50207\n",
      "Main effects training epoch: 86, train loss: 0.52013, val loss: 0.50156\n",
      "Main effects training epoch: 87, train loss: 0.51988, val loss: 0.50198\n",
      "Main effects training epoch: 88, train loss: 0.51954, val loss: 0.50135\n",
      "Main effects training epoch: 89, train loss: 0.52047, val loss: 0.50242\n",
      "Main effects training epoch: 90, train loss: 0.52023, val loss: 0.50206\n",
      "Main effects training epoch: 91, train loss: 0.51953, val loss: 0.50273\n",
      "Main effects training epoch: 92, train loss: 0.51926, val loss: 0.50079\n",
      "Main effects training epoch: 93, train loss: 0.51944, val loss: 0.50110\n",
      "Main effects training epoch: 94, train loss: 0.51968, val loss: 0.50262\n",
      "Main effects training epoch: 95, train loss: 0.52026, val loss: 0.50207\n",
      "Main effects training epoch: 96, train loss: 0.52029, val loss: 0.50341\n",
      "Main effects training epoch: 97, train loss: 0.52024, val loss: 0.50202\n",
      "Main effects training epoch: 98, train loss: 0.51958, val loss: 0.50211\n",
      "Main effects training epoch: 99, train loss: 0.52075, val loss: 0.50435\n",
      "Main effects training epoch: 100, train loss: 0.51911, val loss: 0.50061\n",
      "Main effects training epoch: 101, train loss: 0.51907, val loss: 0.50301\n",
      "Main effects training epoch: 102, train loss: 0.51920, val loss: 0.50104\n",
      "Main effects training epoch: 103, train loss: 0.52015, val loss: 0.50353\n",
      "Main effects training epoch: 104, train loss: 0.51963, val loss: 0.50142\n",
      "Main effects training epoch: 105, train loss: 0.51917, val loss: 0.50216\n",
      "Main effects training epoch: 106, train loss: 0.51910, val loss: 0.50140\n",
      "Main effects training epoch: 107, train loss: 0.51911, val loss: 0.50126\n",
      "Main effects training epoch: 108, train loss: 0.51882, val loss: 0.50261\n",
      "Main effects training epoch: 109, train loss: 0.51907, val loss: 0.50218\n",
      "Main effects training epoch: 110, train loss: 0.51889, val loss: 0.50098\n",
      "Main effects training epoch: 111, train loss: 0.51884, val loss: 0.50262\n",
      "Main effects training epoch: 112, train loss: 0.51902, val loss: 0.50184\n",
      "Main effects training epoch: 113, train loss: 0.51873, val loss: 0.50129\n",
      "Main effects training epoch: 114, train loss: 0.51856, val loss: 0.50268\n",
      "Main effects training epoch: 115, train loss: 0.51844, val loss: 0.50067\n",
      "Main effects training epoch: 116, train loss: 0.51924, val loss: 0.50171\n",
      "Main effects training epoch: 117, train loss: 0.51904, val loss: 0.50266\n",
      "Main effects training epoch: 118, train loss: 0.51952, val loss: 0.50251\n",
      "Main effects training epoch: 119, train loss: 0.51898, val loss: 0.50279\n",
      "Main effects training epoch: 120, train loss: 0.51897, val loss: 0.50267\n",
      "Main effects training epoch: 121, train loss: 0.51914, val loss: 0.50081\n",
      "Main effects training epoch: 122, train loss: 0.51840, val loss: 0.50226\n",
      "Main effects training epoch: 123, train loss: 0.51902, val loss: 0.50237\n",
      "Main effects training epoch: 124, train loss: 0.51823, val loss: 0.50224\n",
      "Main effects training epoch: 125, train loss: 0.51824, val loss: 0.50164\n",
      "Main effects training epoch: 126, train loss: 0.51825, val loss: 0.50067\n",
      "Main effects training epoch: 127, train loss: 0.51844, val loss: 0.50296\n",
      "Main effects training epoch: 128, train loss: 0.51846, val loss: 0.50090\n",
      "Main effects training epoch: 129, train loss: 0.51817, val loss: 0.50299\n",
      "Main effects training epoch: 130, train loss: 0.51806, val loss: 0.50061\n",
      "Main effects training epoch: 131, train loss: 0.51827, val loss: 0.50037\n",
      "Main effects training epoch: 132, train loss: 0.51860, val loss: 0.50414\n",
      "Main effects training epoch: 133, train loss: 0.51838, val loss: 0.50115\n",
      "Main effects training epoch: 134, train loss: 0.51827, val loss: 0.50179\n",
      "Main effects training epoch: 135, train loss: 0.51794, val loss: 0.50214\n",
      "Main effects training epoch: 136, train loss: 0.51840, val loss: 0.50092\n",
      "Main effects training epoch: 137, train loss: 0.51784, val loss: 0.50234\n",
      "Main effects training epoch: 138, train loss: 0.51759, val loss: 0.50314\n",
      "Main effects training epoch: 139, train loss: 0.51784, val loss: 0.50020\n",
      "Main effects training epoch: 140, train loss: 0.51796, val loss: 0.50327\n",
      "Main effects training epoch: 141, train loss: 0.51801, val loss: 0.50122\n",
      "Main effects training epoch: 142, train loss: 0.51833, val loss: 0.50217\n",
      "Main effects training epoch: 143, train loss: 0.51756, val loss: 0.50238\n",
      "Main effects training epoch: 144, train loss: 0.51744, val loss: 0.50078\n",
      "Main effects training epoch: 145, train loss: 0.51802, val loss: 0.50245\n",
      "Main effects training epoch: 146, train loss: 0.51761, val loss: 0.50264\n",
      "Main effects training epoch: 147, train loss: 0.51732, val loss: 0.50149\n",
      "Early stop at epoch 147, with validation loss: 0.50149\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51904, val loss: 0.50222\n",
      "Main effects tuning epoch: 2, train loss: 0.51878, val loss: 0.50252\n",
      "Main effects tuning epoch: 3, train loss: 0.51883, val loss: 0.50246\n",
      "Main effects tuning epoch: 4, train loss: 0.51873, val loss: 0.50278\n",
      "Main effects tuning epoch: 5, train loss: 0.51906, val loss: 0.50095\n",
      "Main effects tuning epoch: 6, train loss: 0.51881, val loss: 0.50382\n",
      "Main effects tuning epoch: 7, train loss: 0.51880, val loss: 0.50158\n",
      "Main effects tuning epoch: 8, train loss: 0.51872, val loss: 0.50302\n",
      "Main effects tuning epoch: 9, train loss: 0.51908, val loss: 0.50073\n",
      "Main effects tuning epoch: 10, train loss: 0.51875, val loss: 0.50437\n",
      "Main effects tuning epoch: 11, train loss: 0.51885, val loss: 0.50174\n",
      "Main effects tuning epoch: 12, train loss: 0.51857, val loss: 0.50205\n",
      "Main effects tuning epoch: 13, train loss: 0.51863, val loss: 0.50219\n",
      "Main effects tuning epoch: 14, train loss: 0.51811, val loss: 0.50212\n",
      "Main effects tuning epoch: 15, train loss: 0.51834, val loss: 0.50340\n",
      "Main effects tuning epoch: 16, train loss: 0.51799, val loss: 0.50201\n",
      "Main effects tuning epoch: 17, train loss: 0.51786, val loss: 0.50226\n",
      "Main effects tuning epoch: 18, train loss: 0.51778, val loss: 0.50165\n",
      "Main effects tuning epoch: 19, train loss: 0.51797, val loss: 0.50413\n",
      "Main effects tuning epoch: 20, train loss: 0.51793, val loss: 0.50179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects tuning epoch: 21, train loss: 0.51819, val loss: 0.50347\n",
      "Main effects tuning epoch: 22, train loss: 0.51872, val loss: 0.50243\n",
      "Main effects tuning epoch: 23, train loss: 0.51821, val loss: 0.50383\n",
      "Main effects tuning epoch: 24, train loss: 0.51808, val loss: 0.50306\n",
      "Main effects tuning epoch: 25, train loss: 0.51869, val loss: 0.50323\n",
      "Main effects tuning epoch: 26, train loss: 0.51832, val loss: 0.50350\n",
      "Main effects tuning epoch: 27, train loss: 0.51747, val loss: 0.50379\n",
      "Main effects tuning epoch: 28, train loss: 0.51740, val loss: 0.50197\n",
      "Main effects tuning epoch: 29, train loss: 0.51887, val loss: 0.50488\n",
      "Main effects tuning epoch: 30, train loss: 0.51803, val loss: 0.50226\n",
      "Main effects tuning epoch: 31, train loss: 0.51741, val loss: 0.50315\n",
      "Main effects tuning epoch: 32, train loss: 0.51713, val loss: 0.50214\n",
      "Main effects tuning epoch: 33, train loss: 0.51721, val loss: 0.50347\n",
      "Main effects tuning epoch: 34, train loss: 0.51718, val loss: 0.50143\n",
      "Main effects tuning epoch: 35, train loss: 0.51729, val loss: 0.50390\n",
      "Main effects tuning epoch: 36, train loss: 0.51759, val loss: 0.50268\n",
      "Main effects tuning epoch: 37, train loss: 0.51785, val loss: 0.50302\n",
      "Main effects tuning epoch: 38, train loss: 0.51779, val loss: 0.50478\n",
      "Main effects tuning epoch: 39, train loss: 0.51755, val loss: 0.50265\n",
      "Main effects tuning epoch: 40, train loss: 0.51726, val loss: 0.50310\n",
      "Main effects tuning epoch: 41, train loss: 0.51692, val loss: 0.50319\n",
      "Main effects tuning epoch: 42, train loss: 0.51759, val loss: 0.50459\n",
      "Main effects tuning epoch: 43, train loss: 0.51683, val loss: 0.50159\n",
      "Main effects tuning epoch: 44, train loss: 0.51722, val loss: 0.50500\n",
      "Main effects tuning epoch: 45, train loss: 0.51738, val loss: 0.50154\n",
      "Main effects tuning epoch: 46, train loss: 0.51696, val loss: 0.50351\n",
      "Main effects tuning epoch: 47, train loss: 0.51683, val loss: 0.50145\n",
      "Main effects tuning epoch: 48, train loss: 0.51676, val loss: 0.50387\n",
      "Main effects tuning epoch: 49, train loss: 0.51664, val loss: 0.50243\n",
      "Main effects tuning epoch: 50, train loss: 0.51654, val loss: 0.50203\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.47584, val loss: 0.46837\n",
      "Interaction training epoch: 2, train loss: 0.33042, val loss: 0.33344\n",
      "Interaction training epoch: 3, train loss: 0.31934, val loss: 0.32694\n",
      "Interaction training epoch: 4, train loss: 0.30961, val loss: 0.30421\n",
      "Interaction training epoch: 5, train loss: 0.29243, val loss: 0.29800\n",
      "Interaction training epoch: 6, train loss: 0.29474, val loss: 0.29581\n",
      "Interaction training epoch: 7, train loss: 0.29052, val loss: 0.29590\n",
      "Interaction training epoch: 8, train loss: 0.28391, val loss: 0.28943\n",
      "Interaction training epoch: 9, train loss: 0.29110, val loss: 0.29340\n",
      "Interaction training epoch: 10, train loss: 0.28531, val loss: 0.29566\n",
      "Interaction training epoch: 11, train loss: 0.28220, val loss: 0.29037\n",
      "Interaction training epoch: 12, train loss: 0.28215, val loss: 0.29226\n",
      "Interaction training epoch: 13, train loss: 0.27683, val loss: 0.28485\n",
      "Interaction training epoch: 14, train loss: 0.28043, val loss: 0.29198\n",
      "Interaction training epoch: 15, train loss: 0.27988, val loss: 0.28783\n",
      "Interaction training epoch: 16, train loss: 0.28488, val loss: 0.29651\n",
      "Interaction training epoch: 17, train loss: 0.27940, val loss: 0.28796\n",
      "Interaction training epoch: 18, train loss: 0.27729, val loss: 0.29281\n",
      "Interaction training epoch: 19, train loss: 0.27413, val loss: 0.28688\n",
      "Interaction training epoch: 20, train loss: 0.27532, val loss: 0.28637\n",
      "Interaction training epoch: 21, train loss: 0.27418, val loss: 0.28789\n",
      "Interaction training epoch: 22, train loss: 0.27541, val loss: 0.28689\n",
      "Interaction training epoch: 23, train loss: 0.27261, val loss: 0.28705\n",
      "Interaction training epoch: 24, train loss: 0.27341, val loss: 0.28696\n",
      "Interaction training epoch: 25, train loss: 0.27406, val loss: 0.28641\n",
      "Interaction training epoch: 26, train loss: 0.27260, val loss: 0.28970\n",
      "Interaction training epoch: 27, train loss: 0.27139, val loss: 0.28499\n",
      "Interaction training epoch: 28, train loss: 0.26936, val loss: 0.28209\n",
      "Interaction training epoch: 29, train loss: 0.27508, val loss: 0.28702\n",
      "Interaction training epoch: 30, train loss: 0.27176, val loss: 0.28697\n",
      "Interaction training epoch: 31, train loss: 0.27015, val loss: 0.28484\n",
      "Interaction training epoch: 32, train loss: 0.26910, val loss: 0.28551\n",
      "Interaction training epoch: 33, train loss: 0.27141, val loss: 0.28613\n",
      "Interaction training epoch: 34, train loss: 0.27176, val loss: 0.28692\n",
      "Interaction training epoch: 35, train loss: 0.26770, val loss: 0.28435\n",
      "Interaction training epoch: 36, train loss: 0.27125, val loss: 0.28567\n",
      "Interaction training epoch: 37, train loss: 0.27113, val loss: 0.28669\n",
      "Interaction training epoch: 38, train loss: 0.26945, val loss: 0.28428\n",
      "Interaction training epoch: 39, train loss: 0.26846, val loss: 0.28159\n",
      "Interaction training epoch: 40, train loss: 0.26819, val loss: 0.28325\n",
      "Interaction training epoch: 41, train loss: 0.26972, val loss: 0.28257\n",
      "Interaction training epoch: 42, train loss: 0.27057, val loss: 0.28329\n",
      "Interaction training epoch: 43, train loss: 0.26592, val loss: 0.28456\n",
      "Interaction training epoch: 44, train loss: 0.26844, val loss: 0.28334\n",
      "Interaction training epoch: 45, train loss: 0.26815, val loss: 0.28202\n",
      "Interaction training epoch: 46, train loss: 0.26716, val loss: 0.28234\n",
      "Interaction training epoch: 47, train loss: 0.26816, val loss: 0.28661\n",
      "Interaction training epoch: 48, train loss: 0.26488, val loss: 0.27831\n",
      "Interaction training epoch: 49, train loss: 0.26714, val loss: 0.28434\n",
      "Interaction training epoch: 50, train loss: 0.26775, val loss: 0.28371\n",
      "Interaction training epoch: 51, train loss: 0.26531, val loss: 0.28014\n",
      "Interaction training epoch: 52, train loss: 0.26546, val loss: 0.28513\n",
      "Interaction training epoch: 53, train loss: 0.27113, val loss: 0.28618\n",
      "Interaction training epoch: 54, train loss: 0.26783, val loss: 0.28616\n",
      "Interaction training epoch: 55, train loss: 0.26522, val loss: 0.27980\n",
      "Interaction training epoch: 56, train loss: 0.27184, val loss: 0.28987\n",
      "Interaction training epoch: 57, train loss: 0.26897, val loss: 0.28928\n",
      "Interaction training epoch: 58, train loss: 0.26569, val loss: 0.28144\n",
      "Interaction training epoch: 59, train loss: 0.26801, val loss: 0.28355\n",
      "Interaction training epoch: 60, train loss: 0.26564, val loss: 0.28429\n",
      "Interaction training epoch: 61, train loss: 0.26451, val loss: 0.28113\n",
      "Interaction training epoch: 62, train loss: 0.26558, val loss: 0.28144\n",
      "Interaction training epoch: 63, train loss: 0.26650, val loss: 0.28570\n",
      "Interaction training epoch: 64, train loss: 0.26496, val loss: 0.28631\n",
      "Interaction training epoch: 65, train loss: 0.26367, val loss: 0.27903\n",
      "Interaction training epoch: 66, train loss: 0.26478, val loss: 0.28079\n",
      "Interaction training epoch: 67, train loss: 0.26394, val loss: 0.28242\n",
      "Interaction training epoch: 68, train loss: 0.26203, val loss: 0.27924\n",
      "Interaction training epoch: 69, train loss: 0.26474, val loss: 0.28226\n",
      "Interaction training epoch: 70, train loss: 0.26158, val loss: 0.28084\n",
      "Interaction training epoch: 71, train loss: 0.26265, val loss: 0.28039\n",
      "Interaction training epoch: 72, train loss: 0.26438, val loss: 0.28219\n",
      "Interaction training epoch: 73, train loss: 0.26030, val loss: 0.27841\n",
      "Interaction training epoch: 74, train loss: 0.26706, val loss: 0.28796\n",
      "Interaction training epoch: 75, train loss: 0.26213, val loss: 0.27769\n",
      "Interaction training epoch: 76, train loss: 0.26486, val loss: 0.28523\n",
      "Interaction training epoch: 77, train loss: 0.26226, val loss: 0.28236\n",
      "Interaction training epoch: 78, train loss: 0.26082, val loss: 0.27606\n",
      "Interaction training epoch: 79, train loss: 0.26677, val loss: 0.28622\n",
      "Interaction training epoch: 80, train loss: 0.26084, val loss: 0.27874\n",
      "Interaction training epoch: 81, train loss: 0.26765, val loss: 0.28522\n",
      "Interaction training epoch: 82, train loss: 0.26408, val loss: 0.28246\n",
      "Interaction training epoch: 83, train loss: 0.26287, val loss: 0.27983\n",
      "Interaction training epoch: 84, train loss: 0.26673, val loss: 0.28618\n",
      "Interaction training epoch: 85, train loss: 0.25924, val loss: 0.27979\n",
      "Interaction training epoch: 86, train loss: 0.26464, val loss: 0.28380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 87, train loss: 0.25889, val loss: 0.27899\n",
      "Interaction training epoch: 88, train loss: 0.26014, val loss: 0.27919\n",
      "Interaction training epoch: 89, train loss: 0.26199, val loss: 0.28041\n",
      "Interaction training epoch: 90, train loss: 0.26177, val loss: 0.28115\n",
      "Interaction training epoch: 91, train loss: 0.25923, val loss: 0.27909\n",
      "Interaction training epoch: 92, train loss: 0.25876, val loss: 0.27575\n",
      "Interaction training epoch: 93, train loss: 0.26130, val loss: 0.28179\n",
      "Interaction training epoch: 94, train loss: 0.25939, val loss: 0.27941\n",
      "Interaction training epoch: 95, train loss: 0.26085, val loss: 0.27944\n",
      "Interaction training epoch: 96, train loss: 0.26193, val loss: 0.28074\n",
      "Interaction training epoch: 97, train loss: 0.25850, val loss: 0.27962\n",
      "Interaction training epoch: 98, train loss: 0.26226, val loss: 0.27974\n",
      "Interaction training epoch: 99, train loss: 0.25998, val loss: 0.28048\n",
      "Interaction training epoch: 100, train loss: 0.25623, val loss: 0.27557\n",
      "Interaction training epoch: 101, train loss: 0.26062, val loss: 0.28093\n",
      "Interaction training epoch: 102, train loss: 0.25975, val loss: 0.27839\n",
      "Interaction training epoch: 103, train loss: 0.25666, val loss: 0.27795\n",
      "Interaction training epoch: 104, train loss: 0.26114, val loss: 0.27984\n",
      "Interaction training epoch: 105, train loss: 0.25966, val loss: 0.27870\n",
      "Interaction training epoch: 106, train loss: 0.25746, val loss: 0.27422\n",
      "Interaction training epoch: 107, train loss: 0.25953, val loss: 0.28106\n",
      "Interaction training epoch: 108, train loss: 0.25540, val loss: 0.27440\n",
      "Interaction training epoch: 109, train loss: 0.25750, val loss: 0.27679\n",
      "Interaction training epoch: 110, train loss: 0.25634, val loss: 0.27728\n",
      "Interaction training epoch: 111, train loss: 0.25818, val loss: 0.27602\n",
      "Interaction training epoch: 112, train loss: 0.25694, val loss: 0.27727\n",
      "Interaction training epoch: 113, train loss: 0.25814, val loss: 0.28105\n",
      "Interaction training epoch: 114, train loss: 0.25510, val loss: 0.27332\n",
      "Interaction training epoch: 115, train loss: 0.25829, val loss: 0.27686\n",
      "Interaction training epoch: 116, train loss: 0.25692, val loss: 0.27808\n",
      "Interaction training epoch: 117, train loss: 0.25434, val loss: 0.27610\n",
      "Interaction training epoch: 118, train loss: 0.25553, val loss: 0.27338\n",
      "Interaction training epoch: 119, train loss: 0.25494, val loss: 0.27696\n",
      "Interaction training epoch: 120, train loss: 0.25503, val loss: 0.27618\n",
      "Interaction training epoch: 121, train loss: 0.25433, val loss: 0.27660\n",
      "Interaction training epoch: 122, train loss: 0.25696, val loss: 0.27552\n",
      "Interaction training epoch: 123, train loss: 0.25658, val loss: 0.27624\n",
      "Interaction training epoch: 124, train loss: 0.25650, val loss: 0.27758\n",
      "Interaction training epoch: 125, train loss: 0.25676, val loss: 0.27581\n",
      "Interaction training epoch: 126, train loss: 0.25343, val loss: 0.27658\n",
      "Interaction training epoch: 127, train loss: 0.25448, val loss: 0.27473\n",
      "Interaction training epoch: 128, train loss: 0.25438, val loss: 0.27590\n",
      "Interaction training epoch: 129, train loss: 0.25800, val loss: 0.27967\n",
      "Interaction training epoch: 130, train loss: 0.25539, val loss: 0.27618\n",
      "Interaction training epoch: 131, train loss: 0.25340, val loss: 0.27395\n",
      "Interaction training epoch: 132, train loss: 0.25676, val loss: 0.27508\n",
      "Interaction training epoch: 133, train loss: 0.25126, val loss: 0.27535\n",
      "Interaction training epoch: 134, train loss: 0.25126, val loss: 0.27207\n",
      "Interaction training epoch: 135, train loss: 0.25146, val loss: 0.27207\n",
      "Interaction training epoch: 136, train loss: 0.25286, val loss: 0.27453\n",
      "Interaction training epoch: 137, train loss: 0.25142, val loss: 0.27175\n",
      "Interaction training epoch: 138, train loss: 0.25428, val loss: 0.27762\n",
      "Interaction training epoch: 139, train loss: 0.25092, val loss: 0.27431\n",
      "Interaction training epoch: 140, train loss: 0.25149, val loss: 0.27407\n",
      "Interaction training epoch: 141, train loss: 0.24904, val loss: 0.27271\n",
      "Interaction training epoch: 142, train loss: 0.25194, val loss: 0.27644\n",
      "Interaction training epoch: 143, train loss: 0.25097, val loss: 0.27497\n",
      "Interaction training epoch: 144, train loss: 0.24844, val loss: 0.27453\n",
      "Interaction training epoch: 145, train loss: 0.24777, val loss: 0.27214\n",
      "Interaction training epoch: 146, train loss: 0.25184, val loss: 0.28013\n",
      "Interaction training epoch: 147, train loss: 0.25320, val loss: 0.27750\n",
      "Interaction training epoch: 148, train loss: 0.25395, val loss: 0.28178\n",
      "Interaction training epoch: 149, train loss: 0.24994, val loss: 0.27735\n",
      "Interaction training epoch: 150, train loss: 0.25129, val loss: 0.27242\n",
      "Interaction training epoch: 151, train loss: 0.24850, val loss: 0.27506\n",
      "Interaction training epoch: 152, train loss: 0.24980, val loss: 0.27272\n",
      "Interaction training epoch: 153, train loss: 0.24826, val loss: 0.27677\n",
      "Interaction training epoch: 154, train loss: 0.25044, val loss: 0.27550\n",
      "Interaction training epoch: 155, train loss: 0.24687, val loss: 0.27453\n",
      "Interaction training epoch: 156, train loss: 0.25096, val loss: 0.27460\n",
      "Interaction training epoch: 157, train loss: 0.24897, val loss: 0.28111\n",
      "Interaction training epoch: 158, train loss: 0.24670, val loss: 0.26844\n",
      "Interaction training epoch: 159, train loss: 0.25068, val loss: 0.27839\n",
      "Interaction training epoch: 160, train loss: 0.24790, val loss: 0.27520\n",
      "Interaction training epoch: 161, train loss: 0.24816, val loss: 0.27428\n",
      "Interaction training epoch: 162, train loss: 0.24872, val loss: 0.27833\n",
      "Interaction training epoch: 163, train loss: 0.24860, val loss: 0.27627\n",
      "Interaction training epoch: 164, train loss: 0.24984, val loss: 0.28121\n",
      "Interaction training epoch: 165, train loss: 0.24535, val loss: 0.26952\n",
      "Interaction training epoch: 166, train loss: 0.24644, val loss: 0.27687\n",
      "Interaction training epoch: 167, train loss: 0.24588, val loss: 0.27261\n",
      "Interaction training epoch: 168, train loss: 0.24674, val loss: 0.27483\n",
      "Interaction training epoch: 169, train loss: 0.24620, val loss: 0.27140\n",
      "Interaction training epoch: 170, train loss: 0.24670, val loss: 0.27547\n",
      "Interaction training epoch: 171, train loss: 0.24342, val loss: 0.26707\n",
      "Interaction training epoch: 172, train loss: 0.24639, val loss: 0.27747\n",
      "Interaction training epoch: 173, train loss: 0.24462, val loss: 0.27353\n",
      "Interaction training epoch: 174, train loss: 0.24550, val loss: 0.27318\n",
      "Interaction training epoch: 175, train loss: 0.24480, val loss: 0.27395\n",
      "Interaction training epoch: 176, train loss: 0.24597, val loss: 0.27416\n",
      "Interaction training epoch: 177, train loss: 0.24547, val loss: 0.27029\n",
      "Interaction training epoch: 178, train loss: 0.24459, val loss: 0.27434\n",
      "Interaction training epoch: 179, train loss: 0.24400, val loss: 0.27399\n",
      "Interaction training epoch: 180, train loss: 0.24015, val loss: 0.26962\n",
      "Interaction training epoch: 181, train loss: 0.24972, val loss: 0.27882\n",
      "Interaction training epoch: 182, train loss: 0.24321, val loss: 0.27335\n",
      "Interaction training epoch: 183, train loss: 0.24309, val loss: 0.26952\n",
      "Interaction training epoch: 184, train loss: 0.24605, val loss: 0.27561\n",
      "Interaction training epoch: 185, train loss: 0.24360, val loss: 0.27554\n",
      "Interaction training epoch: 186, train loss: 0.24283, val loss: 0.27002\n",
      "Interaction training epoch: 187, train loss: 0.24455, val loss: 0.27202\n",
      "Interaction training epoch: 188, train loss: 0.24176, val loss: 0.27487\n",
      "Interaction training epoch: 189, train loss: 0.24487, val loss: 0.27442\n",
      "Interaction training epoch: 190, train loss: 0.24375, val loss: 0.27453\n",
      "Interaction training epoch: 191, train loss: 0.24302, val loss: 0.27245\n",
      "Interaction training epoch: 192, train loss: 0.24394, val loss: 0.27274\n",
      "Interaction training epoch: 193, train loss: 0.24087, val loss: 0.27046\n",
      "Interaction training epoch: 194, train loss: 0.24639, val loss: 0.27528\n",
      "Interaction training epoch: 195, train loss: 0.24108, val loss: 0.27375\n",
      "Interaction training epoch: 196, train loss: 0.24491, val loss: 0.27660\n",
      "Interaction training epoch: 197, train loss: 0.24297, val loss: 0.27364\n",
      "Interaction training epoch: 198, train loss: 0.24231, val loss: 0.27377\n",
      "Interaction training epoch: 199, train loss: 0.24563, val loss: 0.27583\n",
      "Interaction training epoch: 200, train loss: 0.23915, val loss: 0.26903\n",
      "##########Stage 2: interaction training stop.##########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########3 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.24993, val loss: 0.27478\n",
      "Interaction tuning epoch: 2, train loss: 0.24810, val loss: 0.27639\n",
      "Interaction tuning epoch: 3, train loss: 0.24662, val loss: 0.27201\n",
      "Interaction tuning epoch: 4, train loss: 0.24652, val loss: 0.27466\n",
      "Interaction tuning epoch: 5, train loss: 0.24485, val loss: 0.26982\n",
      "Interaction tuning epoch: 6, train loss: 0.24811, val loss: 0.27468\n",
      "Interaction tuning epoch: 7, train loss: 0.24509, val loss: 0.27367\n",
      "Interaction tuning epoch: 8, train loss: 0.24761, val loss: 0.27349\n",
      "Interaction tuning epoch: 9, train loss: 0.24438, val loss: 0.27412\n",
      "Interaction tuning epoch: 10, train loss: 0.24614, val loss: 0.27432\n",
      "Interaction tuning epoch: 11, train loss: 0.24473, val loss: 0.27484\n",
      "Interaction tuning epoch: 12, train loss: 0.24516, val loss: 0.27351\n",
      "Interaction tuning epoch: 13, train loss: 0.24595, val loss: 0.27185\n",
      "Interaction tuning epoch: 14, train loss: 0.24873, val loss: 0.28183\n",
      "Interaction tuning epoch: 15, train loss: 0.24453, val loss: 0.27298\n",
      "Interaction tuning epoch: 16, train loss: 0.24429, val loss: 0.27086\n",
      "Interaction tuning epoch: 17, train loss: 0.24750, val loss: 0.27731\n",
      "Interaction tuning epoch: 18, train loss: 0.24255, val loss: 0.27111\n",
      "Interaction tuning epoch: 19, train loss: 0.24676, val loss: 0.27331\n",
      "Interaction tuning epoch: 20, train loss: 0.24344, val loss: 0.27183\n",
      "Interaction tuning epoch: 21, train loss: 0.24568, val loss: 0.27594\n",
      "Interaction tuning epoch: 22, train loss: 0.24482, val loss: 0.27431\n",
      "Interaction tuning epoch: 23, train loss: 0.24582, val loss: 0.27643\n",
      "Interaction tuning epoch: 24, train loss: 0.24400, val loss: 0.27249\n",
      "Interaction tuning epoch: 25, train loss: 0.24564, val loss: 0.27655\n",
      "Interaction tuning epoch: 26, train loss: 0.24443, val loss: 0.27417\n",
      "Interaction tuning epoch: 27, train loss: 0.24389, val loss: 0.27153\n",
      "Interaction tuning epoch: 28, train loss: 0.24518, val loss: 0.27208\n",
      "Interaction tuning epoch: 29, train loss: 0.24498, val loss: 0.28043\n",
      "Interaction tuning epoch: 30, train loss: 0.24484, val loss: 0.27039\n",
      "Interaction tuning epoch: 31, train loss: 0.24512, val loss: 0.27373\n",
      "Interaction tuning epoch: 32, train loss: 0.24331, val loss: 0.27518\n",
      "Interaction tuning epoch: 33, train loss: 0.24576, val loss: 0.27411\n",
      "Interaction tuning epoch: 34, train loss: 0.24268, val loss: 0.27244\n",
      "Interaction tuning epoch: 35, train loss: 0.24492, val loss: 0.27246\n",
      "Interaction tuning epoch: 36, train loss: 0.24500, val loss: 0.27525\n",
      "Interaction tuning epoch: 37, train loss: 0.24536, val loss: 0.27556\n",
      "Interaction tuning epoch: 38, train loss: 0.24455, val loss: 0.27316\n",
      "Interaction tuning epoch: 39, train loss: 0.24483, val loss: 0.27519\n",
      "Interaction tuning epoch: 40, train loss: 0.24586, val loss: 0.27441\n",
      "Interaction tuning epoch: 41, train loss: 0.24279, val loss: 0.27066\n",
      "Interaction tuning epoch: 42, train loss: 0.24487, val loss: 0.27278\n",
      "Interaction tuning epoch: 43, train loss: 0.24632, val loss: 0.27770\n",
      "Interaction tuning epoch: 44, train loss: 0.24281, val loss: 0.27157\n",
      "Interaction tuning epoch: 45, train loss: 0.24431, val loss: 0.27177\n",
      "Interaction tuning epoch: 46, train loss: 0.24334, val loss: 0.27556\n",
      "Interaction tuning epoch: 47, train loss: 0.24527, val loss: 0.27022\n",
      "Interaction tuning epoch: 48, train loss: 0.24483, val loss: 0.27648\n",
      "Interaction tuning epoch: 49, train loss: 0.24199, val loss: 0.26868\n",
      "Interaction tuning epoch: 50, train loss: 0.24386, val loss: 0.27520\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 42.25874876976013\n",
      "After the gam stage, training error is 0.24386 , validation error is 0.27520\n",
      "missing value counts: 99163\n",
      "[SoftImpute] Max Singular Value of X_init = 3.631261\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed BCE=0.209568 validation BCE=0.296525,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 2: observed BCE=0.205791 validation BCE=0.296040,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.203846 validation BCE=0.295424,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.202625 validation BCE=0.286985,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.202058 validation BCE=0.285389,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.201636 validation BCE=0.283464,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.201376 validation BCE=0.281795,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.201156 validation BCE=0.281969,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.201020 validation BCE=0.281796,rank=5\n",
      "[SoftImpute] Iter 10: observed BCE=0.200687 validation BCE=0.281207,rank=5\n",
      "[SoftImpute] Iter 11: observed BCE=0.200787 validation BCE=0.279910,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 12: observed BCE=0.200858 validation BCE=0.280154,rank=5\n",
      "[SoftImpute] Iter 13: observed BCE=0.200463 validation BCE=0.279124,rank=5\n",
      "[SoftImpute] Iter 14: observed BCE=0.200291 validation BCE=0.279116,rank=5\n",
      "[SoftImpute] Iter 15: observed BCE=0.200289 validation BCE=0.278484,rank=5\n",
      "[SoftImpute] Iter 16: observed BCE=0.200348 validation BCE=0.279179,rank=5\n",
      "[SoftImpute] Iter 17: observed BCE=0.200604 validation BCE=0.278004,rank=5\n",
      "[SoftImpute] Iter 18: observed BCE=0.200664 validation BCE=0.277781,rank=5\n",
      "[SoftImpute] Iter 19: observed BCE=0.200480 validation BCE=0.278110,rank=5\n",
      "[SoftImpute] Iter 20: observed BCE=0.200497 validation BCE=0.277738,rank=5\n",
      "[SoftImpute] Iter 21: observed BCE=0.200212 validation BCE=0.277815,rank=5\n",
      "[SoftImpute] Iter 22: observed BCE=0.200060 validation BCE=0.277405,rank=5\n",
      "[SoftImpute] Iter 23: observed BCE=0.200077 validation BCE=0.277251,rank=5\n",
      "[SoftImpute] Iter 24: observed BCE=0.200350 validation BCE=0.277519,rank=5\n",
      "[SoftImpute] Iter 25: observed BCE=0.200432 validation BCE=0.277064,rank=5\n",
      "[SoftImpute] Iter 26: observed BCE=0.200323 validation BCE=0.277492,rank=5\n",
      "[SoftImpute] Iter 27: observed BCE=0.200110 validation BCE=0.277329,rank=5\n",
      "[SoftImpute] Iter 28: observed BCE=0.200102 validation BCE=0.277216,rank=5\n",
      "[SoftImpute] Iter 29: observed BCE=0.199896 validation BCE=0.276559,rank=5\n",
      "[SoftImpute] Iter 30: observed BCE=0.200225 validation BCE=0.277154,rank=5\n",
      "[SoftImpute] Iter 31: observed BCE=0.200044 validation BCE=0.276943,rank=5\n",
      "[SoftImpute] Iter 32: observed BCE=0.199866 validation BCE=0.277253,rank=5\n",
      "[SoftImpute] Iter 33: observed BCE=0.199931 validation BCE=0.276750,rank=5\n",
      "[SoftImpute] Iter 34: observed BCE=0.199985 validation BCE=0.276921,rank=5\n",
      "[SoftImpute] Iter 35: observed BCE=0.200134 validation BCE=0.276819,rank=5\n",
      "[SoftImpute] Iter 36: observed BCE=0.200069 validation BCE=0.277078,rank=5\n",
      "[SoftImpute] Iter 37: observed BCE=0.199969 validation BCE=0.276714,rank=5\n",
      "[SoftImpute] Iter 38: observed BCE=0.199854 validation BCE=0.276587,rank=5\n",
      "[SoftImpute] Iter 39: observed BCE=0.199974 validation BCE=0.276850,rank=5\n",
      "[SoftImpute] Iter 40: observed BCE=0.200761 validation BCE=0.276727,rank=5\n",
      "[SoftImpute] Iter 41: observed BCE=0.201445 validation BCE=0.275954,rank=5\n",
      "[SoftImpute] Iter 42: observed BCE=0.201427 validation BCE=0.276361,rank=5\n",
      "[SoftImpute] Iter 43: observed BCE=0.201201 validation BCE=0.275969,rank=5\n",
      "[SoftImpute] Iter 44: observed BCE=0.201436 validation BCE=0.276190,rank=5\n",
      "[SoftImpute] Iter 45: observed BCE=0.201591 validation BCE=0.275861,rank=5\n",
      "[SoftImpute] Iter 46: observed BCE=0.201530 validation BCE=0.275745,rank=5\n",
      "[SoftImpute] Iter 47: observed BCE=0.201783 validation BCE=0.275989,rank=5\n",
      "[SoftImpute] Iter 48: observed BCE=0.202120 validation BCE=0.275970,rank=5\n",
      "[SoftImpute] Iter 49: observed BCE=0.202105 validation BCE=0.276015,rank=5\n",
      "[SoftImpute] Iter 50: observed BCE=0.202343 validation BCE=0.275854,rank=5\n",
      "[SoftImpute] Iter 51: observed BCE=0.202221 validation BCE=0.275643,rank=5\n",
      "[SoftImpute] Iter 52: observed BCE=0.202020 validation BCE=0.275716,rank=5\n",
      "[SoftImpute] Iter 53: observed BCE=0.202152 validation BCE=0.275925,rank=5\n",
      "[SoftImpute] Iter 54: observed BCE=0.202049 validation BCE=0.275636,rank=5\n",
      "[SoftImpute] Iter 55: observed BCE=0.201835 validation BCE=0.275760,rank=5\n",
      "[SoftImpute] Iter 56: observed BCE=0.201824 validation BCE=0.275728,rank=5\n",
      "[SoftImpute] Iter 57: observed BCE=0.202200 validation BCE=0.275759,rank=5\n",
      "[SoftImpute] Iter 58: observed BCE=0.201968 validation BCE=0.275831,rank=5\n",
      "[SoftImpute] Iter 59: observed BCE=0.201951 validation BCE=0.275605,rank=5\n",
      "[SoftImpute] Iter 60: observed BCE=0.201954 validation BCE=0.275546,rank=5\n",
      "[SoftImpute] Iter 61: observed BCE=0.202023 validation BCE=0.275524,rank=5\n",
      "[SoftImpute] Iter 62: observed BCE=0.201967 validation BCE=0.275225,rank=5\n",
      "[SoftImpute] Iter 63: observed BCE=0.201939 validation BCE=0.275382,rank=5\n",
      "[SoftImpute] Iter 64: observed BCE=0.201887 validation BCE=0.275200,rank=5\n",
      "[SoftImpute] Iter 65: observed BCE=0.202129 validation BCE=0.275685,rank=5\n",
      "[SoftImpute] Iter 66: observed BCE=0.201879 validation BCE=0.275178,rank=5\n",
      "[SoftImpute] Iter 67: observed BCE=0.201676 validation BCE=0.275569,rank=5\n",
      "[SoftImpute] Iter 68: observed BCE=0.201999 validation BCE=0.275256,rank=5\n",
      "[SoftImpute] Iter 69: observed BCE=0.201940 validation BCE=0.275185,rank=5\n",
      "[SoftImpute] Iter 70: observed BCE=0.201764 validation BCE=0.274990,rank=5\n",
      "[SoftImpute] Iter 71: observed BCE=0.202038 validation BCE=0.275285,rank=5\n",
      "[SoftImpute] Iter 72: observed BCE=0.201879 validation BCE=0.275441,rank=5\n",
      "[SoftImpute] Iter 73: observed BCE=0.201724 validation BCE=0.275070,rank=5\n",
      "[SoftImpute] Iter 74: observed BCE=0.201914 validation BCE=0.275246,rank=5\n",
      "[SoftImpute] Iter 75: observed BCE=0.201786 validation BCE=0.275207,rank=5\n",
      "[SoftImpute] Iter 76: observed BCE=0.201710 validation BCE=0.275453,rank=5\n",
      "[SoftImpute] Iter 77: observed BCE=0.201755 validation BCE=0.275390,rank=5\n",
      "[SoftImpute] Iter 78: observed BCE=0.201762 validation BCE=0.275208,rank=5\n",
      "[SoftImpute] Iter 79: observed BCE=0.201781 validation BCE=0.275241,rank=5\n",
      "[SoftImpute] Iter 80: observed BCE=0.201835 validation BCE=0.274874,rank=5\n",
      "[SoftImpute] Iter 81: observed BCE=0.201980 validation BCE=0.275407,rank=5\n",
      "[SoftImpute] Iter 82: observed BCE=0.201935 validation BCE=0.274961,rank=5\n",
      "[SoftImpute] Iter 83: observed BCE=0.201859 validation BCE=0.275516,rank=5\n",
      "[SoftImpute] Iter 84: observed BCE=0.201935 validation BCE=0.274932,rank=5\n",
      "[SoftImpute] Iter 85: observed BCE=0.201934 validation BCE=0.275164,rank=5\n",
      "[SoftImpute] Iter 86: observed BCE=0.201893 validation BCE=0.275008,rank=5\n",
      "[SoftImpute] Iter 87: observed BCE=0.201524 validation BCE=0.274886,rank=5\n",
      "[SoftImpute] Iter 88: observed BCE=0.201480 validation BCE=0.274962,rank=5\n",
      "[SoftImpute] Iter 89: observed BCE=0.201488 validation BCE=0.275202,rank=5\n",
      "[SoftImpute] Iter 90: observed BCE=0.201861 validation BCE=0.275212,rank=5\n",
      "[SoftImpute] Stopped after iteration 90 for lambda=0.072625\n",
      "final num of user group: 5\n",
      "final num of item group: 6\n",
      "change mode state : True\n",
      "time cost: 5.893321990966797\n",
      "After the matrix factor stage, training error is 0.20186, validation error is 0.27521\n",
      "8\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68423, val loss: 0.68408\n",
      "Main effects training epoch: 2, train loss: 0.67654, val loss: 0.67763\n",
      "Main effects training epoch: 3, train loss: 0.67269, val loss: 0.67686\n",
      "Main effects training epoch: 4, train loss: 0.66771, val loss: 0.67108\n",
      "Main effects training epoch: 5, train loss: 0.65894, val loss: 0.65969\n",
      "Main effects training epoch: 6, train loss: 0.63818, val loss: 0.63649\n",
      "Main effects training epoch: 7, train loss: 0.59309, val loss: 0.58539\n",
      "Main effects training epoch: 8, train loss: 0.54170, val loss: 0.52426\n",
      "Main effects training epoch: 9, train loss: 0.52947, val loss: 0.50709\n",
      "Main effects training epoch: 10, train loss: 0.52780, val loss: 0.50560\n",
      "Main effects training epoch: 11, train loss: 0.52356, val loss: 0.50245\n",
      "Main effects training epoch: 12, train loss: 0.52400, val loss: 0.50695\n",
      "Main effects training epoch: 13, train loss: 0.52327, val loss: 0.50432\n",
      "Main effects training epoch: 14, train loss: 0.52240, val loss: 0.50278\n",
      "Main effects training epoch: 15, train loss: 0.52323, val loss: 0.50160\n",
      "Main effects training epoch: 16, train loss: 0.52399, val loss: 0.50383\n",
      "Main effects training epoch: 17, train loss: 0.52329, val loss: 0.50261\n",
      "Main effects training epoch: 18, train loss: 0.52282, val loss: 0.50248\n",
      "Main effects training epoch: 19, train loss: 0.52261, val loss: 0.50241\n",
      "Main effects training epoch: 20, train loss: 0.52200, val loss: 0.50125\n",
      "Main effects training epoch: 21, train loss: 0.52236, val loss: 0.50247\n",
      "Main effects training epoch: 22, train loss: 0.52185, val loss: 0.50167\n",
      "Main effects training epoch: 23, train loss: 0.52190, val loss: 0.50069\n",
      "Main effects training epoch: 24, train loss: 0.52234, val loss: 0.50184\n",
      "Main effects training epoch: 25, train loss: 0.52157, val loss: 0.50201\n",
      "Main effects training epoch: 26, train loss: 0.52161, val loss: 0.50104\n",
      "Main effects training epoch: 27, train loss: 0.52265, val loss: 0.50234\n",
      "Main effects training epoch: 28, train loss: 0.52187, val loss: 0.50218\n",
      "Main effects training epoch: 29, train loss: 0.52245, val loss: 0.50124\n",
      "Main effects training epoch: 30, train loss: 0.52332, val loss: 0.50429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 31, train loss: 0.52353, val loss: 0.50331\n",
      "Main effects training epoch: 32, train loss: 0.52256, val loss: 0.50326\n",
      "Main effects training epoch: 33, train loss: 0.52208, val loss: 0.50155\n",
      "Main effects training epoch: 34, train loss: 0.52164, val loss: 0.50193\n",
      "Main effects training epoch: 35, train loss: 0.52219, val loss: 0.50066\n",
      "Main effects training epoch: 36, train loss: 0.52179, val loss: 0.50251\n",
      "Main effects training epoch: 37, train loss: 0.52178, val loss: 0.50201\n",
      "Main effects training epoch: 38, train loss: 0.52178, val loss: 0.50050\n",
      "Main effects training epoch: 39, train loss: 0.52133, val loss: 0.50212\n",
      "Main effects training epoch: 40, train loss: 0.52111, val loss: 0.50078\n",
      "Main effects training epoch: 41, train loss: 0.52108, val loss: 0.50069\n",
      "Main effects training epoch: 42, train loss: 0.52127, val loss: 0.50217\n",
      "Main effects training epoch: 43, train loss: 0.52141, val loss: 0.50145\n",
      "Main effects training epoch: 44, train loss: 0.52190, val loss: 0.50158\n",
      "Main effects training epoch: 45, train loss: 0.52136, val loss: 0.50144\n",
      "Main effects training epoch: 46, train loss: 0.52135, val loss: 0.50124\n",
      "Main effects training epoch: 47, train loss: 0.52099, val loss: 0.50177\n",
      "Main effects training epoch: 48, train loss: 0.52129, val loss: 0.50007\n",
      "Main effects training epoch: 49, train loss: 0.52150, val loss: 0.50174\n",
      "Main effects training epoch: 50, train loss: 0.52095, val loss: 0.50148\n",
      "Main effects training epoch: 51, train loss: 0.52085, val loss: 0.50110\n",
      "Main effects training epoch: 52, train loss: 0.52079, val loss: 0.50084\n",
      "Main effects training epoch: 53, train loss: 0.52085, val loss: 0.50017\n",
      "Main effects training epoch: 54, train loss: 0.52081, val loss: 0.50215\n",
      "Main effects training epoch: 55, train loss: 0.52064, val loss: 0.50107\n",
      "Main effects training epoch: 56, train loss: 0.52108, val loss: 0.50202\n",
      "Main effects training epoch: 57, train loss: 0.52173, val loss: 0.50320\n",
      "Main effects training epoch: 58, train loss: 0.52079, val loss: 0.50116\n",
      "Main effects training epoch: 59, train loss: 0.52060, val loss: 0.50112\n",
      "Main effects training epoch: 60, train loss: 0.52054, val loss: 0.50045\n",
      "Main effects training epoch: 61, train loss: 0.52078, val loss: 0.50148\n",
      "Main effects training epoch: 62, train loss: 0.52070, val loss: 0.50226\n",
      "Main effects training epoch: 63, train loss: 0.52150, val loss: 0.50154\n",
      "Main effects training epoch: 64, train loss: 0.52092, val loss: 0.50181\n",
      "Main effects training epoch: 65, train loss: 0.52051, val loss: 0.50143\n",
      "Main effects training epoch: 66, train loss: 0.52087, val loss: 0.50250\n",
      "Main effects training epoch: 67, train loss: 0.52063, val loss: 0.50224\n",
      "Main effects training epoch: 68, train loss: 0.52203, val loss: 0.50306\n",
      "Main effects training epoch: 69, train loss: 0.52240, val loss: 0.50486\n",
      "Main effects training epoch: 70, train loss: 0.52166, val loss: 0.50305\n",
      "Main effects training epoch: 71, train loss: 0.52138, val loss: 0.50231\n",
      "Main effects training epoch: 72, train loss: 0.52109, val loss: 0.50228\n",
      "Main effects training epoch: 73, train loss: 0.52034, val loss: 0.50267\n",
      "Main effects training epoch: 74, train loss: 0.52062, val loss: 0.50099\n",
      "Main effects training epoch: 75, train loss: 0.52036, val loss: 0.50331\n",
      "Main effects training epoch: 76, train loss: 0.52009, val loss: 0.50155\n",
      "Main effects training epoch: 77, train loss: 0.52027, val loss: 0.50086\n",
      "Main effects training epoch: 78, train loss: 0.52013, val loss: 0.50273\n",
      "Main effects training epoch: 79, train loss: 0.52060, val loss: 0.50289\n",
      "Main effects training epoch: 80, train loss: 0.52007, val loss: 0.50228\n",
      "Main effects training epoch: 81, train loss: 0.51999, val loss: 0.50098\n",
      "Main effects training epoch: 82, train loss: 0.52003, val loss: 0.50181\n",
      "Main effects training epoch: 83, train loss: 0.51988, val loss: 0.50092\n",
      "Main effects training epoch: 84, train loss: 0.51978, val loss: 0.50230\n",
      "Main effects training epoch: 85, train loss: 0.51977, val loss: 0.50163\n",
      "Main effects training epoch: 86, train loss: 0.52018, val loss: 0.50100\n",
      "Main effects training epoch: 87, train loss: 0.52144, val loss: 0.50580\n",
      "Main effects training epoch: 88, train loss: 0.52090, val loss: 0.50175\n",
      "Main effects training epoch: 89, train loss: 0.52075, val loss: 0.50205\n",
      "Main effects training epoch: 90, train loss: 0.52028, val loss: 0.50269\n",
      "Main effects training epoch: 91, train loss: 0.51970, val loss: 0.50081\n",
      "Main effects training epoch: 92, train loss: 0.51983, val loss: 0.50075\n",
      "Main effects training epoch: 93, train loss: 0.51981, val loss: 0.50237\n",
      "Main effects training epoch: 94, train loss: 0.51992, val loss: 0.50098\n",
      "Main effects training epoch: 95, train loss: 0.51959, val loss: 0.50252\n",
      "Main effects training epoch: 96, train loss: 0.51973, val loss: 0.50173\n",
      "Main effects training epoch: 97, train loss: 0.51987, val loss: 0.50157\n",
      "Main effects training epoch: 98, train loss: 0.51979, val loss: 0.50332\n",
      "Main effects training epoch: 99, train loss: 0.52017, val loss: 0.50061\n",
      "Main effects training epoch: 100, train loss: 0.52042, val loss: 0.50357\n",
      "Main effects training epoch: 101, train loss: 0.52086, val loss: 0.50351\n",
      "Main effects training epoch: 102, train loss: 0.52115, val loss: 0.50366\n",
      "Main effects training epoch: 103, train loss: 0.52061, val loss: 0.50281\n",
      "Main effects training epoch: 104, train loss: 0.51989, val loss: 0.50156\n",
      "Main effects training epoch: 105, train loss: 0.52019, val loss: 0.50276\n",
      "Main effects training epoch: 106, train loss: 0.51963, val loss: 0.50116\n",
      "Main effects training epoch: 107, train loss: 0.51962, val loss: 0.50286\n",
      "Main effects training epoch: 108, train loss: 0.51983, val loss: 0.50147\n",
      "Main effects training epoch: 109, train loss: 0.51915, val loss: 0.50215\n",
      "Main effects training epoch: 110, train loss: 0.51976, val loss: 0.50199\n",
      "Main effects training epoch: 111, train loss: 0.51928, val loss: 0.50144\n",
      "Main effects training epoch: 112, train loss: 0.51928, val loss: 0.50124\n",
      "Main effects training epoch: 113, train loss: 0.51918, val loss: 0.50282\n",
      "Main effects training epoch: 114, train loss: 0.51901, val loss: 0.50123\n",
      "Main effects training epoch: 115, train loss: 0.51932, val loss: 0.50199\n",
      "Main effects training epoch: 116, train loss: 0.51903, val loss: 0.50203\n",
      "Main effects training epoch: 117, train loss: 0.51923, val loss: 0.50234\n",
      "Main effects training epoch: 118, train loss: 0.51896, val loss: 0.50196\n",
      "Main effects training epoch: 119, train loss: 0.51906, val loss: 0.50205\n",
      "Main effects training epoch: 120, train loss: 0.51944, val loss: 0.50156\n",
      "Main effects training epoch: 121, train loss: 0.51898, val loss: 0.50256\n",
      "Main effects training epoch: 122, train loss: 0.51923, val loss: 0.50092\n",
      "Main effects training epoch: 123, train loss: 0.51916, val loss: 0.50284\n",
      "Main effects training epoch: 124, train loss: 0.51927, val loss: 0.50222\n",
      "Main effects training epoch: 125, train loss: 0.51924, val loss: 0.50196\n",
      "Main effects training epoch: 126, train loss: 0.51923, val loss: 0.50233\n",
      "Main effects training epoch: 127, train loss: 0.51942, val loss: 0.50376\n",
      "Main effects training epoch: 128, train loss: 0.51873, val loss: 0.50106\n",
      "Main effects training epoch: 129, train loss: 0.51927, val loss: 0.50339\n",
      "Main effects training epoch: 130, train loss: 0.51868, val loss: 0.50128\n",
      "Main effects training epoch: 131, train loss: 0.51835, val loss: 0.50166\n",
      "Main effects training epoch: 132, train loss: 0.51845, val loss: 0.50238\n",
      "Main effects training epoch: 133, train loss: 0.51861, val loss: 0.50135\n",
      "Main effects training epoch: 134, train loss: 0.51848, val loss: 0.50165\n",
      "Main effects training epoch: 135, train loss: 0.51859, val loss: 0.50308\n",
      "Main effects training epoch: 136, train loss: 0.51845, val loss: 0.50090\n",
      "Main effects training epoch: 137, train loss: 0.51834, val loss: 0.50239\n",
      "Main effects training epoch: 138, train loss: 0.51871, val loss: 0.50099\n",
      "Main effects training epoch: 139, train loss: 0.51821, val loss: 0.50213\n",
      "Main effects training epoch: 140, train loss: 0.51804, val loss: 0.50150\n",
      "Main effects training epoch: 141, train loss: 0.51847, val loss: 0.50098\n",
      "Main effects training epoch: 142, train loss: 0.51801, val loss: 0.50252\n",
      "Main effects training epoch: 143, train loss: 0.51803, val loss: 0.50100\n",
      "Main effects training epoch: 144, train loss: 0.51778, val loss: 0.50157\n",
      "Main effects training epoch: 145, train loss: 0.51777, val loss: 0.50241\n",
      "Main effects training epoch: 146, train loss: 0.51770, val loss: 0.50170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 147, train loss: 0.51809, val loss: 0.50154\n",
      "Main effects training epoch: 148, train loss: 0.51799, val loss: 0.50209\n",
      "Main effects training epoch: 149, train loss: 0.51766, val loss: 0.50119\n",
      "Early stop at epoch 149, with validation loss: 0.50119\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51961, val loss: 0.50278\n",
      "Main effects tuning epoch: 2, train loss: 0.51961, val loss: 0.50258\n",
      "Main effects tuning epoch: 3, train loss: 0.51932, val loss: 0.50265\n",
      "Main effects tuning epoch: 4, train loss: 0.51923, val loss: 0.50143\n",
      "Main effects tuning epoch: 5, train loss: 0.51941, val loss: 0.50324\n",
      "Main effects tuning epoch: 6, train loss: 0.51923, val loss: 0.50236\n",
      "Main effects tuning epoch: 7, train loss: 0.51903, val loss: 0.50244\n",
      "Main effects tuning epoch: 8, train loss: 0.51890, val loss: 0.50283\n",
      "Main effects tuning epoch: 9, train loss: 0.51912, val loss: 0.50266\n",
      "Main effects tuning epoch: 10, train loss: 0.51911, val loss: 0.50186\n",
      "Main effects tuning epoch: 11, train loss: 0.51928, val loss: 0.50323\n",
      "Main effects tuning epoch: 12, train loss: 0.51906, val loss: 0.50238\n",
      "Main effects tuning epoch: 13, train loss: 0.51886, val loss: 0.50215\n",
      "Main effects tuning epoch: 14, train loss: 0.51861, val loss: 0.50166\n",
      "Main effects tuning epoch: 15, train loss: 0.51848, val loss: 0.50312\n",
      "Main effects tuning epoch: 16, train loss: 0.51855, val loss: 0.50136\n",
      "Main effects tuning epoch: 17, train loss: 0.51884, val loss: 0.50433\n",
      "Main effects tuning epoch: 18, train loss: 0.51890, val loss: 0.50161\n",
      "Main effects tuning epoch: 19, train loss: 0.51867, val loss: 0.50368\n",
      "Main effects tuning epoch: 20, train loss: 0.51941, val loss: 0.50266\n",
      "Main effects tuning epoch: 21, train loss: 0.51850, val loss: 0.50277\n",
      "Main effects tuning epoch: 22, train loss: 0.51844, val loss: 0.50253\n",
      "Main effects tuning epoch: 23, train loss: 0.51809, val loss: 0.50243\n",
      "Main effects tuning epoch: 24, train loss: 0.51825, val loss: 0.50258\n",
      "Main effects tuning epoch: 25, train loss: 0.51806, val loss: 0.50252\n",
      "Main effects tuning epoch: 26, train loss: 0.51812, val loss: 0.50263\n",
      "Main effects tuning epoch: 27, train loss: 0.51828, val loss: 0.50251\n",
      "Main effects tuning epoch: 28, train loss: 0.51787, val loss: 0.50302\n",
      "Main effects tuning epoch: 29, train loss: 0.51786, val loss: 0.50302\n",
      "Main effects tuning epoch: 30, train loss: 0.51852, val loss: 0.50265\n",
      "Main effects tuning epoch: 31, train loss: 0.51782, val loss: 0.50275\n",
      "Main effects tuning epoch: 32, train loss: 0.51762, val loss: 0.50229\n",
      "Main effects tuning epoch: 33, train loss: 0.51760, val loss: 0.50243\n",
      "Main effects tuning epoch: 34, train loss: 0.51750, val loss: 0.50286\n",
      "Main effects tuning epoch: 35, train loss: 0.51787, val loss: 0.50251\n",
      "Main effects tuning epoch: 36, train loss: 0.51802, val loss: 0.50317\n",
      "Main effects tuning epoch: 37, train loss: 0.51772, val loss: 0.50308\n",
      "Main effects tuning epoch: 38, train loss: 0.51753, val loss: 0.50204\n",
      "Main effects tuning epoch: 39, train loss: 0.51743, val loss: 0.50157\n",
      "Main effects tuning epoch: 40, train loss: 0.51775, val loss: 0.50424\n",
      "Main effects tuning epoch: 41, train loss: 0.51740, val loss: 0.50172\n",
      "Main effects tuning epoch: 42, train loss: 0.51728, val loss: 0.50243\n",
      "Main effects tuning epoch: 43, train loss: 0.51753, val loss: 0.50303\n",
      "Main effects tuning epoch: 44, train loss: 0.51760, val loss: 0.50270\n",
      "Main effects tuning epoch: 45, train loss: 0.51743, val loss: 0.50238\n",
      "Main effects tuning epoch: 46, train loss: 0.51783, val loss: 0.50236\n",
      "Main effects tuning epoch: 47, train loss: 0.51769, val loss: 0.50411\n",
      "Main effects tuning epoch: 48, train loss: 0.51711, val loss: 0.50170\n",
      "Main effects tuning epoch: 49, train loss: 0.51717, val loss: 0.50408\n",
      "Main effects tuning epoch: 50, train loss: 0.51696, val loss: 0.50158\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.49111, val loss: 0.47640\n",
      "Interaction training epoch: 2, train loss: 0.33202, val loss: 0.33510\n",
      "Interaction training epoch: 3, train loss: 0.32006, val loss: 0.32368\n",
      "Interaction training epoch: 4, train loss: 0.30594, val loss: 0.30891\n",
      "Interaction training epoch: 5, train loss: 0.30432, val loss: 0.31106\n",
      "Interaction training epoch: 6, train loss: 0.29312, val loss: 0.29825\n",
      "Interaction training epoch: 7, train loss: 0.28872, val loss: 0.29145\n",
      "Interaction training epoch: 8, train loss: 0.28802, val loss: 0.29172\n",
      "Interaction training epoch: 9, train loss: 0.28515, val loss: 0.28959\n",
      "Interaction training epoch: 10, train loss: 0.28715, val loss: 0.29285\n",
      "Interaction training epoch: 11, train loss: 0.28268, val loss: 0.29033\n",
      "Interaction training epoch: 12, train loss: 0.28126, val loss: 0.28935\n",
      "Interaction training epoch: 13, train loss: 0.27981, val loss: 0.28510\n",
      "Interaction training epoch: 14, train loss: 0.28022, val loss: 0.28572\n",
      "Interaction training epoch: 15, train loss: 0.28474, val loss: 0.29201\n",
      "Interaction training epoch: 16, train loss: 0.27826, val loss: 0.28657\n",
      "Interaction training epoch: 17, train loss: 0.28044, val loss: 0.28665\n",
      "Interaction training epoch: 18, train loss: 0.27672, val loss: 0.28299\n",
      "Interaction training epoch: 19, train loss: 0.28421, val loss: 0.28715\n",
      "Interaction training epoch: 20, train loss: 0.27759, val loss: 0.28329\n",
      "Interaction training epoch: 21, train loss: 0.27474, val loss: 0.28091\n",
      "Interaction training epoch: 22, train loss: 0.27778, val loss: 0.28266\n",
      "Interaction training epoch: 23, train loss: 0.27681, val loss: 0.28286\n",
      "Interaction training epoch: 24, train loss: 0.27371, val loss: 0.28076\n",
      "Interaction training epoch: 25, train loss: 0.28161, val loss: 0.28550\n",
      "Interaction training epoch: 26, train loss: 0.28181, val loss: 0.28881\n",
      "Interaction training epoch: 27, train loss: 0.27699, val loss: 0.28548\n",
      "Interaction training epoch: 28, train loss: 0.27273, val loss: 0.27808\n",
      "Interaction training epoch: 29, train loss: 0.27504, val loss: 0.28293\n",
      "Interaction training epoch: 30, train loss: 0.27160, val loss: 0.28129\n",
      "Interaction training epoch: 31, train loss: 0.27600, val loss: 0.28692\n",
      "Interaction training epoch: 32, train loss: 0.27549, val loss: 0.28172\n",
      "Interaction training epoch: 33, train loss: 0.27382, val loss: 0.28140\n",
      "Interaction training epoch: 34, train loss: 0.27099, val loss: 0.28272\n",
      "Interaction training epoch: 35, train loss: 0.27361, val loss: 0.28158\n",
      "Interaction training epoch: 36, train loss: 0.27212, val loss: 0.28431\n",
      "Interaction training epoch: 37, train loss: 0.26985, val loss: 0.28187\n",
      "Interaction training epoch: 38, train loss: 0.27847, val loss: 0.28612\n",
      "Interaction training epoch: 39, train loss: 0.27246, val loss: 0.28756\n",
      "Interaction training epoch: 40, train loss: 0.27130, val loss: 0.28283\n",
      "Interaction training epoch: 41, train loss: 0.26917, val loss: 0.28386\n",
      "Interaction training epoch: 42, train loss: 0.26903, val loss: 0.28423\n",
      "Interaction training epoch: 43, train loss: 0.27094, val loss: 0.28157\n",
      "Interaction training epoch: 44, train loss: 0.27548, val loss: 0.28703\n",
      "Interaction training epoch: 45, train loss: 0.27120, val loss: 0.29102\n",
      "Interaction training epoch: 46, train loss: 0.27228, val loss: 0.28229\n",
      "Interaction training epoch: 47, train loss: 0.27463, val loss: 0.28902\n",
      "Interaction training epoch: 48, train loss: 0.27011, val loss: 0.28510\n",
      "Interaction training epoch: 49, train loss: 0.27351, val loss: 0.28728\n",
      "Interaction training epoch: 50, train loss: 0.26623, val loss: 0.28171\n",
      "Interaction training epoch: 51, train loss: 0.26828, val loss: 0.28191\n",
      "Interaction training epoch: 52, train loss: 0.26778, val loss: 0.28148\n",
      "Interaction training epoch: 53, train loss: 0.26955, val loss: 0.28024\n",
      "Interaction training epoch: 54, train loss: 0.26984, val loss: 0.28368\n",
      "Interaction training epoch: 55, train loss: 0.26437, val loss: 0.28144\n",
      "Interaction training epoch: 56, train loss: 0.26811, val loss: 0.28246\n",
      "Interaction training epoch: 57, train loss: 0.26626, val loss: 0.28288\n",
      "Interaction training epoch: 58, train loss: 0.26712, val loss: 0.28342\n",
      "Interaction training epoch: 59, train loss: 0.26886, val loss: 0.28390\n",
      "Interaction training epoch: 60, train loss: 0.26639, val loss: 0.28589\n",
      "Interaction training epoch: 61, train loss: 0.26260, val loss: 0.27909\n",
      "Interaction training epoch: 62, train loss: 0.26508, val loss: 0.28383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 63, train loss: 0.26501, val loss: 0.27866\n",
      "Interaction training epoch: 64, train loss: 0.26496, val loss: 0.28749\n",
      "Interaction training epoch: 65, train loss: 0.26767, val loss: 0.28067\n",
      "Interaction training epoch: 66, train loss: 0.26570, val loss: 0.28509\n",
      "Interaction training epoch: 67, train loss: 0.26799, val loss: 0.29375\n",
      "Interaction training epoch: 68, train loss: 0.27089, val loss: 0.28335\n",
      "Interaction training epoch: 69, train loss: 0.26875, val loss: 0.28911\n",
      "Interaction training epoch: 70, train loss: 0.26344, val loss: 0.28311\n",
      "Interaction training epoch: 71, train loss: 0.26967, val loss: 0.28461\n",
      "Interaction training epoch: 72, train loss: 0.26590, val loss: 0.28313\n",
      "Interaction training epoch: 73, train loss: 0.26031, val loss: 0.27727\n",
      "Interaction training epoch: 74, train loss: 0.26355, val loss: 0.28226\n",
      "Interaction training epoch: 75, train loss: 0.26085, val loss: 0.28188\n",
      "Interaction training epoch: 76, train loss: 0.26319, val loss: 0.27984\n",
      "Interaction training epoch: 77, train loss: 0.26187, val loss: 0.28164\n",
      "Interaction training epoch: 78, train loss: 0.26427, val loss: 0.28182\n",
      "Interaction training epoch: 79, train loss: 0.26204, val loss: 0.28212\n",
      "Interaction training epoch: 80, train loss: 0.26703, val loss: 0.28487\n",
      "Interaction training epoch: 81, train loss: 0.26361, val loss: 0.28265\n",
      "Interaction training epoch: 82, train loss: 0.26249, val loss: 0.28052\n",
      "Interaction training epoch: 83, train loss: 0.25954, val loss: 0.27989\n",
      "Interaction training epoch: 84, train loss: 0.26225, val loss: 0.28024\n",
      "Interaction training epoch: 85, train loss: 0.26165, val loss: 0.28493\n",
      "Interaction training epoch: 86, train loss: 0.25745, val loss: 0.27941\n",
      "Interaction training epoch: 87, train loss: 0.25874, val loss: 0.27930\n",
      "Interaction training epoch: 88, train loss: 0.25982, val loss: 0.28170\n",
      "Interaction training epoch: 89, train loss: 0.25550, val loss: 0.27714\n",
      "Interaction training epoch: 90, train loss: 0.25808, val loss: 0.27572\n",
      "Interaction training epoch: 91, train loss: 0.26192, val loss: 0.29001\n",
      "Interaction training epoch: 92, train loss: 0.25907, val loss: 0.27597\n",
      "Interaction training epoch: 93, train loss: 0.25607, val loss: 0.27836\n",
      "Interaction training epoch: 94, train loss: 0.25585, val loss: 0.27951\n",
      "Interaction training epoch: 95, train loss: 0.25885, val loss: 0.27727\n",
      "Interaction training epoch: 96, train loss: 0.25703, val loss: 0.28222\n",
      "Interaction training epoch: 97, train loss: 0.25508, val loss: 0.27405\n",
      "Interaction training epoch: 98, train loss: 0.25715, val loss: 0.27904\n",
      "Interaction training epoch: 99, train loss: 0.25820, val loss: 0.28112\n",
      "Interaction training epoch: 100, train loss: 0.25549, val loss: 0.27917\n",
      "Interaction training epoch: 101, train loss: 0.25592, val loss: 0.27711\n",
      "Interaction training epoch: 102, train loss: 0.25520, val loss: 0.27534\n",
      "Interaction training epoch: 103, train loss: 0.25643, val loss: 0.28064\n",
      "Interaction training epoch: 104, train loss: 0.25502, val loss: 0.27743\n",
      "Interaction training epoch: 105, train loss: 0.25515, val loss: 0.27818\n",
      "Interaction training epoch: 106, train loss: 0.25513, val loss: 0.27853\n",
      "Interaction training epoch: 107, train loss: 0.25253, val loss: 0.27613\n",
      "Interaction training epoch: 108, train loss: 0.25470, val loss: 0.27802\n",
      "Interaction training epoch: 109, train loss: 0.25318, val loss: 0.27134\n",
      "Interaction training epoch: 110, train loss: 0.25324, val loss: 0.27877\n",
      "Interaction training epoch: 111, train loss: 0.25513, val loss: 0.27838\n",
      "Interaction training epoch: 112, train loss: 0.25524, val loss: 0.27699\n",
      "Interaction training epoch: 113, train loss: 0.24968, val loss: 0.26895\n",
      "Interaction training epoch: 114, train loss: 0.25251, val loss: 0.27513\n",
      "Interaction training epoch: 115, train loss: 0.25213, val loss: 0.27203\n",
      "Interaction training epoch: 116, train loss: 0.25083, val loss: 0.27030\n",
      "Interaction training epoch: 117, train loss: 0.24986, val loss: 0.27770\n",
      "Interaction training epoch: 118, train loss: 0.25111, val loss: 0.27136\n",
      "Interaction training epoch: 119, train loss: 0.24931, val loss: 0.27341\n",
      "Interaction training epoch: 120, train loss: 0.25411, val loss: 0.27731\n",
      "Interaction training epoch: 121, train loss: 0.24663, val loss: 0.26470\n",
      "Interaction training epoch: 122, train loss: 0.25274, val loss: 0.27577\n",
      "Interaction training epoch: 123, train loss: 0.25368, val loss: 0.27931\n",
      "Interaction training epoch: 124, train loss: 0.24915, val loss: 0.26979\n",
      "Interaction training epoch: 125, train loss: 0.25206, val loss: 0.27560\n",
      "Interaction training epoch: 126, train loss: 0.25017, val loss: 0.26947\n",
      "Interaction training epoch: 127, train loss: 0.24664, val loss: 0.26840\n",
      "Interaction training epoch: 128, train loss: 0.25329, val loss: 0.27527\n",
      "Interaction training epoch: 129, train loss: 0.24791, val loss: 0.27247\n",
      "Interaction training epoch: 130, train loss: 0.25603, val loss: 0.27998\n",
      "Interaction training epoch: 131, train loss: 0.24789, val loss: 0.27144\n",
      "Interaction training epoch: 132, train loss: 0.24870, val loss: 0.26808\n",
      "Interaction training epoch: 133, train loss: 0.24525, val loss: 0.26757\n",
      "Interaction training epoch: 134, train loss: 0.24747, val loss: 0.26641\n",
      "Interaction training epoch: 135, train loss: 0.24819, val loss: 0.27238\n",
      "Interaction training epoch: 136, train loss: 0.24617, val loss: 0.27221\n",
      "Interaction training epoch: 137, train loss: 0.24784, val loss: 0.26867\n",
      "Interaction training epoch: 138, train loss: 0.25130, val loss: 0.27474\n",
      "Interaction training epoch: 139, train loss: 0.25796, val loss: 0.27978\n",
      "Interaction training epoch: 140, train loss: 0.24837, val loss: 0.27015\n",
      "Interaction training epoch: 141, train loss: 0.24878, val loss: 0.26971\n",
      "Interaction training epoch: 142, train loss: 0.25124, val loss: 0.27545\n",
      "Interaction training epoch: 143, train loss: 0.24461, val loss: 0.26595\n",
      "Interaction training epoch: 144, train loss: 0.24519, val loss: 0.26702\n",
      "Interaction training epoch: 145, train loss: 0.24458, val loss: 0.27526\n",
      "Interaction training epoch: 146, train loss: 0.24754, val loss: 0.27057\n",
      "Interaction training epoch: 147, train loss: 0.24764, val loss: 0.27019\n",
      "Interaction training epoch: 148, train loss: 0.24284, val loss: 0.26714\n",
      "Interaction training epoch: 149, train loss: 0.24538, val loss: 0.26953\n",
      "Interaction training epoch: 150, train loss: 0.24386, val loss: 0.26964\n",
      "Interaction training epoch: 151, train loss: 0.24393, val loss: 0.27013\n",
      "Interaction training epoch: 152, train loss: 0.24687, val loss: 0.27058\n",
      "Interaction training epoch: 153, train loss: 0.24311, val loss: 0.26959\n",
      "Interaction training epoch: 154, train loss: 0.24610, val loss: 0.27094\n",
      "Interaction training epoch: 155, train loss: 0.24275, val loss: 0.26579\n",
      "Interaction training epoch: 156, train loss: 0.24332, val loss: 0.26791\n",
      "Interaction training epoch: 157, train loss: 0.24231, val loss: 0.27017\n",
      "Interaction training epoch: 158, train loss: 0.24495, val loss: 0.26716\n",
      "Interaction training epoch: 159, train loss: 0.24238, val loss: 0.27145\n",
      "Interaction training epoch: 160, train loss: 0.24299, val loss: 0.27024\n",
      "Interaction training epoch: 161, train loss: 0.24176, val loss: 0.26462\n",
      "Interaction training epoch: 162, train loss: 0.24053, val loss: 0.26897\n",
      "Interaction training epoch: 163, train loss: 0.24065, val loss: 0.26284\n",
      "Interaction training epoch: 164, train loss: 0.24433, val loss: 0.27656\n",
      "Interaction training epoch: 165, train loss: 0.24498, val loss: 0.26585\n",
      "Interaction training epoch: 166, train loss: 0.24024, val loss: 0.26961\n",
      "Interaction training epoch: 167, train loss: 0.24312, val loss: 0.26931\n",
      "Interaction training epoch: 168, train loss: 0.24070, val loss: 0.26377\n",
      "Interaction training epoch: 169, train loss: 0.24217, val loss: 0.26423\n",
      "Interaction training epoch: 170, train loss: 0.24208, val loss: 0.27054\n",
      "Interaction training epoch: 171, train loss: 0.23850, val loss: 0.26310\n",
      "Interaction training epoch: 172, train loss: 0.24289, val loss: 0.26705\n",
      "Interaction training epoch: 173, train loss: 0.23931, val loss: 0.26648\n",
      "Interaction training epoch: 174, train loss: 0.23883, val loss: 0.26156\n",
      "Interaction training epoch: 175, train loss: 0.23885, val loss: 0.26474\n",
      "Interaction training epoch: 176, train loss: 0.23772, val loss: 0.26483\n",
      "Interaction training epoch: 177, train loss: 0.24036, val loss: 0.26434\n",
      "Interaction training epoch: 178, train loss: 0.23885, val loss: 0.26406\n",
      "Interaction training epoch: 179, train loss: 0.23721, val loss: 0.26761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 180, train loss: 0.24076, val loss: 0.26814\n",
      "Interaction training epoch: 181, train loss: 0.23636, val loss: 0.26310\n",
      "Interaction training epoch: 182, train loss: 0.25163, val loss: 0.27711\n",
      "Interaction training epoch: 183, train loss: 0.23788, val loss: 0.26202\n",
      "Interaction training epoch: 184, train loss: 0.24116, val loss: 0.26718\n",
      "Interaction training epoch: 185, train loss: 0.24045, val loss: 0.26696\n",
      "Interaction training epoch: 186, train loss: 0.24246, val loss: 0.26624\n",
      "Interaction training epoch: 187, train loss: 0.23632, val loss: 0.26284\n",
      "Interaction training epoch: 188, train loss: 0.23816, val loss: 0.26913\n",
      "Interaction training epoch: 189, train loss: 0.23511, val loss: 0.26140\n",
      "Interaction training epoch: 190, train loss: 0.24013, val loss: 0.26530\n",
      "Interaction training epoch: 191, train loss: 0.23695, val loss: 0.26637\n",
      "Interaction training epoch: 192, train loss: 0.23789, val loss: 0.26200\n",
      "Interaction training epoch: 193, train loss: 0.23680, val loss: 0.26592\n",
      "Interaction training epoch: 194, train loss: 0.23796, val loss: 0.26490\n",
      "Interaction training epoch: 195, train loss: 0.23794, val loss: 0.26822\n",
      "Interaction training epoch: 196, train loss: 0.23636, val loss: 0.26387\n",
      "Interaction training epoch: 197, train loss: 0.24165, val loss: 0.27039\n",
      "Interaction training epoch: 198, train loss: 0.24191, val loss: 0.27239\n",
      "Interaction training epoch: 199, train loss: 0.24160, val loss: 0.26464\n",
      "Interaction training epoch: 200, train loss: 0.23573, val loss: 0.26432\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########1 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.23778, val loss: 0.26261\n",
      "Interaction tuning epoch: 2, train loss: 0.23582, val loss: 0.26431\n",
      "Interaction tuning epoch: 3, train loss: 0.23807, val loss: 0.26542\n",
      "Interaction tuning epoch: 4, train loss: 0.23569, val loss: 0.26207\n",
      "Interaction tuning epoch: 5, train loss: 0.23677, val loss: 0.26435\n",
      "Interaction tuning epoch: 6, train loss: 0.23846, val loss: 0.26398\n",
      "Interaction tuning epoch: 7, train loss: 0.23621, val loss: 0.27210\n",
      "Interaction tuning epoch: 8, train loss: 0.23948, val loss: 0.26843\n",
      "Interaction tuning epoch: 9, train loss: 0.23628, val loss: 0.26556\n",
      "Interaction tuning epoch: 10, train loss: 0.23566, val loss: 0.26416\n",
      "Interaction tuning epoch: 11, train loss: 0.23647, val loss: 0.26581\n",
      "Interaction tuning epoch: 12, train loss: 0.23610, val loss: 0.26147\n",
      "Interaction tuning epoch: 13, train loss: 0.23761, val loss: 0.26767\n",
      "Interaction tuning epoch: 14, train loss: 0.23725, val loss: 0.26514\n",
      "Interaction tuning epoch: 15, train loss: 0.23497, val loss: 0.26640\n",
      "Interaction tuning epoch: 16, train loss: 0.24248, val loss: 0.26715\n",
      "Interaction tuning epoch: 17, train loss: 0.23730, val loss: 0.26419\n",
      "Interaction tuning epoch: 18, train loss: 0.23238, val loss: 0.26635\n",
      "Interaction tuning epoch: 19, train loss: 0.23593, val loss: 0.26098\n",
      "Interaction tuning epoch: 20, train loss: 0.23500, val loss: 0.26440\n",
      "Interaction tuning epoch: 21, train loss: 0.23507, val loss: 0.26346\n",
      "Interaction tuning epoch: 22, train loss: 0.23667, val loss: 0.26670\n",
      "Interaction tuning epoch: 23, train loss: 0.23486, val loss: 0.26792\n",
      "Interaction tuning epoch: 24, train loss: 0.23489, val loss: 0.26274\n",
      "Interaction tuning epoch: 25, train loss: 0.23551, val loss: 0.26447\n",
      "Interaction tuning epoch: 26, train loss: 0.24277, val loss: 0.27278\n",
      "Interaction tuning epoch: 27, train loss: 0.23509, val loss: 0.26745\n",
      "Interaction tuning epoch: 28, train loss: 0.23282, val loss: 0.25873\n",
      "Interaction tuning epoch: 29, train loss: 0.23745, val loss: 0.26577\n",
      "Interaction tuning epoch: 30, train loss: 0.23567, val loss: 0.26938\n",
      "Interaction tuning epoch: 31, train loss: 0.23226, val loss: 0.26044\n",
      "Interaction tuning epoch: 32, train loss: 0.23420, val loss: 0.26327\n",
      "Interaction tuning epoch: 33, train loss: 0.23562, val loss: 0.26360\n",
      "Interaction tuning epoch: 34, train loss: 0.23216, val loss: 0.26253\n",
      "Interaction tuning epoch: 35, train loss: 0.23645, val loss: 0.26127\n",
      "Interaction tuning epoch: 36, train loss: 0.23281, val loss: 0.26551\n",
      "Interaction tuning epoch: 37, train loss: 0.23476, val loss: 0.26786\n",
      "Interaction tuning epoch: 38, train loss: 0.22972, val loss: 0.25999\n",
      "Interaction tuning epoch: 39, train loss: 0.23768, val loss: 0.26718\n",
      "Interaction tuning epoch: 40, train loss: 0.23145, val loss: 0.26272\n",
      "Interaction tuning epoch: 41, train loss: 0.23061, val loss: 0.26434\n",
      "Interaction tuning epoch: 42, train loss: 0.23238, val loss: 0.25810\n",
      "Interaction tuning epoch: 43, train loss: 0.23753, val loss: 0.26933\n",
      "Interaction tuning epoch: 44, train loss: 0.23385, val loss: 0.26364\n",
      "Interaction tuning epoch: 45, train loss: 0.23042, val loss: 0.25820\n",
      "Interaction tuning epoch: 46, train loss: 0.23329, val loss: 0.26797\n",
      "Interaction tuning epoch: 47, train loss: 0.23120, val loss: 0.26105\n",
      "Interaction tuning epoch: 48, train loss: 0.23251, val loss: 0.25749\n",
      "Interaction tuning epoch: 49, train loss: 0.23096, val loss: 0.26239\n",
      "Interaction tuning epoch: 50, train loss: 0.23399, val loss: 0.26180\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 46.566667318344116\n",
      "After the gam stage, training error is 0.23399 , validation error is 0.26180\n",
      "missing value counts: 99144\n",
      "[SoftImpute] Max Singular Value of X_init = 3.681421\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed BCE=0.196234 validation BCE=0.280508,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 2: observed BCE=0.192423 validation BCE=0.279834,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.189968 validation BCE=0.279070,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.188495 validation BCE=0.278934,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.187481 validation BCE=0.278896,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.186896 validation BCE=0.278835,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.186303 validation BCE=0.278994,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.186074 validation BCE=0.279428,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.185431 validation BCE=0.280194,rank=5\n",
      "[SoftImpute] Stopped after iteration 9 for lambda=0.073628\n",
      "final num of user group: 18\n",
      "final num of item group: 14\n",
      "change mode state : True\n",
      "time cost: 1.7822346687316895\n",
      "After the matrix factor stage, training error is 0.18543, validation error is 0.28019\n",
      "9\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68394, val loss: 0.68326\n",
      "Main effects training epoch: 2, train loss: 0.67696, val loss: 0.67783\n",
      "Main effects training epoch: 3, train loss: 0.67180, val loss: 0.67593\n",
      "Main effects training epoch: 4, train loss: 0.66738, val loss: 0.67150\n",
      "Main effects training epoch: 5, train loss: 0.65792, val loss: 0.65928\n",
      "Main effects training epoch: 6, train loss: 0.63466, val loss: 0.63228\n",
      "Main effects training epoch: 7, train loss: 0.59005, val loss: 0.58139\n",
      "Main effects training epoch: 8, train loss: 0.54483, val loss: 0.52701\n",
      "Main effects training epoch: 9, train loss: 0.53579, val loss: 0.51152\n",
      "Main effects training epoch: 10, train loss: 0.53386, val loss: 0.51123\n",
      "Main effects training epoch: 11, train loss: 0.52688, val loss: 0.50454\n",
      "Main effects training epoch: 12, train loss: 0.52664, val loss: 0.50764\n",
      "Main effects training epoch: 13, train loss: 0.52429, val loss: 0.50505\n",
      "Main effects training epoch: 14, train loss: 0.52398, val loss: 0.50361\n",
      "Main effects training epoch: 15, train loss: 0.52303, val loss: 0.50286\n",
      "Main effects training epoch: 16, train loss: 0.52254, val loss: 0.50265\n",
      "Main effects training epoch: 17, train loss: 0.52263, val loss: 0.50134\n",
      "Main effects training epoch: 18, train loss: 0.52252, val loss: 0.50233\n",
      "Main effects training epoch: 19, train loss: 0.52259, val loss: 0.50166\n",
      "Main effects training epoch: 20, train loss: 0.52289, val loss: 0.50197\n",
      "Main effects training epoch: 21, train loss: 0.52231, val loss: 0.50253\n",
      "Main effects training epoch: 22, train loss: 0.52247, val loss: 0.50147\n",
      "Main effects training epoch: 23, train loss: 0.52217, val loss: 0.50185\n",
      "Main effects training epoch: 24, train loss: 0.52204, val loss: 0.50142\n",
      "Main effects training epoch: 25, train loss: 0.52329, val loss: 0.50329\n",
      "Main effects training epoch: 26, train loss: 0.52311, val loss: 0.50213\n",
      "Main effects training epoch: 27, train loss: 0.52220, val loss: 0.50224\n",
      "Main effects training epoch: 28, train loss: 0.52197, val loss: 0.50200\n",
      "Main effects training epoch: 29, train loss: 0.52198, val loss: 0.50060\n",
      "Main effects training epoch: 30, train loss: 0.52206, val loss: 0.50237\n",
      "Main effects training epoch: 31, train loss: 0.52176, val loss: 0.50076\n",
      "Main effects training epoch: 32, train loss: 0.52190, val loss: 0.50053\n",
      "Main effects training epoch: 33, train loss: 0.52204, val loss: 0.50276\n",
      "Main effects training epoch: 34, train loss: 0.52178, val loss: 0.50102\n",
      "Main effects training epoch: 35, train loss: 0.52168, val loss: 0.50087\n",
      "Main effects training epoch: 36, train loss: 0.52166, val loss: 0.50206\n",
      "Main effects training epoch: 37, train loss: 0.52168, val loss: 0.50041\n",
      "Main effects training epoch: 38, train loss: 0.52201, val loss: 0.50215\n",
      "Main effects training epoch: 39, train loss: 0.52226, val loss: 0.50246\n",
      "Main effects training epoch: 40, train loss: 0.52257, val loss: 0.50201\n",
      "Main effects training epoch: 41, train loss: 0.52237, val loss: 0.50257\n",
      "Main effects training epoch: 42, train loss: 0.52276, val loss: 0.50266\n",
      "Main effects training epoch: 43, train loss: 0.52153, val loss: 0.50181\n",
      "Main effects training epoch: 44, train loss: 0.52165, val loss: 0.50240\n",
      "Main effects training epoch: 45, train loss: 0.52148, val loss: 0.50166\n",
      "Main effects training epoch: 46, train loss: 0.52186, val loss: 0.50209\n",
      "Main effects training epoch: 47, train loss: 0.52122, val loss: 0.50141\n",
      "Main effects training epoch: 48, train loss: 0.52102, val loss: 0.50156\n",
      "Main effects training epoch: 49, train loss: 0.52093, val loss: 0.50249\n",
      "Main effects training epoch: 50, train loss: 0.52108, val loss: 0.50189\n",
      "Main effects training epoch: 51, train loss: 0.52116, val loss: 0.50204\n",
      "Main effects training epoch: 52, train loss: 0.52099, val loss: 0.50257\n",
      "Main effects training epoch: 53, train loss: 0.52079, val loss: 0.50259\n",
      "Main effects training epoch: 54, train loss: 0.52090, val loss: 0.50111\n",
      "Main effects training epoch: 55, train loss: 0.52123, val loss: 0.50283\n",
      "Main effects training epoch: 56, train loss: 0.52163, val loss: 0.50108\n",
      "Main effects training epoch: 57, train loss: 0.52332, val loss: 0.50714\n",
      "Main effects training epoch: 58, train loss: 0.52182, val loss: 0.50182\n",
      "Main effects training epoch: 59, train loss: 0.52065, val loss: 0.50197\n",
      "Main effects training epoch: 60, train loss: 0.52053, val loss: 0.50177\n",
      "Main effects training epoch: 61, train loss: 0.52055, val loss: 0.50236\n",
      "Main effects training epoch: 62, train loss: 0.52100, val loss: 0.50327\n",
      "Main effects training epoch: 63, train loss: 0.52077, val loss: 0.50117\n",
      "Main effects training epoch: 64, train loss: 0.52073, val loss: 0.50164\n",
      "Main effects training epoch: 65, train loss: 0.52086, val loss: 0.50134\n",
      "Main effects training epoch: 66, train loss: 0.52052, val loss: 0.50239\n",
      "Main effects training epoch: 67, train loss: 0.52045, val loss: 0.50091\n",
      "Main effects training epoch: 68, train loss: 0.52095, val loss: 0.50303\n",
      "Main effects training epoch: 69, train loss: 0.52057, val loss: 0.50180\n",
      "Main effects training epoch: 70, train loss: 0.52027, val loss: 0.50211\n",
      "Main effects training epoch: 71, train loss: 0.52023, val loss: 0.50096\n",
      "Main effects training epoch: 72, train loss: 0.52089, val loss: 0.50225\n",
      "Main effects training epoch: 73, train loss: 0.52029, val loss: 0.50133\n",
      "Main effects training epoch: 74, train loss: 0.52028, val loss: 0.50328\n",
      "Main effects training epoch: 75, train loss: 0.52045, val loss: 0.50166\n",
      "Main effects training epoch: 76, train loss: 0.52061, val loss: 0.50220\n",
      "Main effects training epoch: 77, train loss: 0.52123, val loss: 0.50212\n",
      "Main effects training epoch: 78, train loss: 0.52092, val loss: 0.50372\n",
      "Main effects training epoch: 79, train loss: 0.52058, val loss: 0.50227\n",
      "Main effects training epoch: 80, train loss: 0.52110, val loss: 0.50157\n",
      "Main effects training epoch: 81, train loss: 0.51998, val loss: 0.50185\n",
      "Main effects training epoch: 82, train loss: 0.52016, val loss: 0.50051\n",
      "Main effects training epoch: 83, train loss: 0.52008, val loss: 0.50156\n",
      "Main effects training epoch: 84, train loss: 0.52057, val loss: 0.50346\n",
      "Main effects training epoch: 85, train loss: 0.52132, val loss: 0.50073\n",
      "Main effects training epoch: 86, train loss: 0.52070, val loss: 0.50379\n",
      "Main effects training epoch: 87, train loss: 0.52051, val loss: 0.50215\n",
      "Main effects training epoch: 88, train loss: 0.52018, val loss: 0.50245\n",
      "Main effects training epoch: 89, train loss: 0.52054, val loss: 0.50125\n",
      "Main effects training epoch: 90, train loss: 0.51982, val loss: 0.50247\n",
      "Main effects training epoch: 91, train loss: 0.51972, val loss: 0.50094\n",
      "Main effects training epoch: 92, train loss: 0.51970, val loss: 0.50157\n",
      "Main effects training epoch: 93, train loss: 0.52018, val loss: 0.50105\n",
      "Main effects training epoch: 94, train loss: 0.52033, val loss: 0.50291\n",
      "Main effects training epoch: 95, train loss: 0.52113, val loss: 0.50262\n",
      "Main effects training epoch: 96, train loss: 0.52176, val loss: 0.50524\n",
      "Main effects training epoch: 97, train loss: 0.52134, val loss: 0.50272\n",
      "Main effects training epoch: 98, train loss: 0.52102, val loss: 0.50325\n",
      "Main effects training epoch: 99, train loss: 0.52123, val loss: 0.50329\n",
      "Main effects training epoch: 100, train loss: 0.52088, val loss: 0.50347\n",
      "Main effects training epoch: 101, train loss: 0.52058, val loss: 0.50154\n",
      "Main effects training epoch: 102, train loss: 0.52002, val loss: 0.50199\n",
      "Main effects training epoch: 103, train loss: 0.51955, val loss: 0.50194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 104, train loss: 0.51963, val loss: 0.50088\n",
      "Main effects training epoch: 105, train loss: 0.51964, val loss: 0.50123\n",
      "Main effects training epoch: 106, train loss: 0.51996, val loss: 0.50282\n",
      "Main effects training epoch: 107, train loss: 0.52023, val loss: 0.50167\n",
      "Main effects training epoch: 108, train loss: 0.52014, val loss: 0.50180\n",
      "Main effects training epoch: 109, train loss: 0.51956, val loss: 0.50266\n",
      "Main effects training epoch: 110, train loss: 0.51928, val loss: 0.50138\n",
      "Main effects training epoch: 111, train loss: 0.51932, val loss: 0.50109\n",
      "Main effects training epoch: 112, train loss: 0.51969, val loss: 0.50107\n",
      "Main effects training epoch: 113, train loss: 0.51993, val loss: 0.50354\n",
      "Main effects training epoch: 114, train loss: 0.51969, val loss: 0.50100\n",
      "Main effects training epoch: 115, train loss: 0.52026, val loss: 0.50138\n",
      "Main effects training epoch: 116, train loss: 0.52024, val loss: 0.50524\n",
      "Main effects training epoch: 117, train loss: 0.51961, val loss: 0.50114\n",
      "Main effects training epoch: 118, train loss: 0.51890, val loss: 0.50167\n",
      "Main effects training epoch: 119, train loss: 0.51901, val loss: 0.50198\n",
      "Main effects training epoch: 120, train loss: 0.51898, val loss: 0.50043\n",
      "Main effects training epoch: 121, train loss: 0.51883, val loss: 0.50167\n",
      "Main effects training epoch: 122, train loss: 0.51890, val loss: 0.50052\n",
      "Main effects training epoch: 123, train loss: 0.51875, val loss: 0.50211\n",
      "Main effects training epoch: 124, train loss: 0.51930, val loss: 0.50061\n",
      "Main effects training epoch: 125, train loss: 0.51898, val loss: 0.50292\n",
      "Main effects training epoch: 126, train loss: 0.51880, val loss: 0.50023\n",
      "Main effects training epoch: 127, train loss: 0.51877, val loss: 0.50182\n",
      "Main effects training epoch: 128, train loss: 0.51907, val loss: 0.50289\n",
      "Main effects training epoch: 129, train loss: 0.51898, val loss: 0.50005\n",
      "Main effects training epoch: 130, train loss: 0.51881, val loss: 0.50318\n",
      "Main effects training epoch: 131, train loss: 0.51923, val loss: 0.50199\n",
      "Main effects training epoch: 132, train loss: 0.51895, val loss: 0.50136\n",
      "Main effects training epoch: 133, train loss: 0.51873, val loss: 0.50140\n",
      "Main effects training epoch: 134, train loss: 0.51871, val loss: 0.50310\n",
      "Main effects training epoch: 135, train loss: 0.51825, val loss: 0.50104\n",
      "Main effects training epoch: 136, train loss: 0.51803, val loss: 0.50144\n",
      "Main effects training epoch: 137, train loss: 0.51804, val loss: 0.50111\n",
      "Main effects training epoch: 138, train loss: 0.51799, val loss: 0.50137\n",
      "Main effects training epoch: 139, train loss: 0.51810, val loss: 0.50111\n",
      "Main effects training epoch: 140, train loss: 0.51815, val loss: 0.50137\n",
      "Main effects training epoch: 141, train loss: 0.51793, val loss: 0.50087\n",
      "Main effects training epoch: 142, train loss: 0.51787, val loss: 0.50157\n",
      "Main effects training epoch: 143, train loss: 0.51858, val loss: 0.50348\n",
      "Main effects training epoch: 144, train loss: 0.51868, val loss: 0.50090\n",
      "Main effects training epoch: 145, train loss: 0.51869, val loss: 0.50367\n",
      "Main effects training epoch: 146, train loss: 0.51781, val loss: 0.50073\n",
      "Main effects training epoch: 147, train loss: 0.51804, val loss: 0.50243\n",
      "Main effects training epoch: 148, train loss: 0.51791, val loss: 0.50054\n",
      "Main effects training epoch: 149, train loss: 0.51778, val loss: 0.50169\n",
      "Main effects training epoch: 150, train loss: 0.51802, val loss: 0.50293\n",
      "Main effects training epoch: 151, train loss: 0.51765, val loss: 0.50018\n",
      "Main effects training epoch: 152, train loss: 0.51752, val loss: 0.50256\n",
      "Main effects training epoch: 153, train loss: 0.51728, val loss: 0.50067\n",
      "Main effects training epoch: 154, train loss: 0.51719, val loss: 0.50121\n",
      "Main effects training epoch: 155, train loss: 0.51719, val loss: 0.50116\n",
      "Main effects training epoch: 156, train loss: 0.51705, val loss: 0.50151\n",
      "Main effects training epoch: 157, train loss: 0.51722, val loss: 0.50037\n",
      "Main effects training epoch: 158, train loss: 0.51741, val loss: 0.50239\n",
      "Main effects training epoch: 159, train loss: 0.51707, val loss: 0.49964\n",
      "Main effects training epoch: 160, train loss: 0.51735, val loss: 0.50237\n",
      "Main effects training epoch: 161, train loss: 0.51700, val loss: 0.50119\n",
      "Main effects training epoch: 162, train loss: 0.51718, val loss: 0.50063\n",
      "Main effects training epoch: 163, train loss: 0.51674, val loss: 0.49960\n",
      "Main effects training epoch: 164, train loss: 0.51615, val loss: 0.50005\n",
      "Main effects training epoch: 165, train loss: 0.51639, val loss: 0.50078\n",
      "Main effects training epoch: 166, train loss: 0.51668, val loss: 0.50015\n",
      "Main effects training epoch: 167, train loss: 0.51695, val loss: 0.50138\n",
      "Main effects training epoch: 168, train loss: 0.51627, val loss: 0.49831\n",
      "Main effects training epoch: 169, train loss: 0.51759, val loss: 0.50478\n",
      "Main effects training epoch: 170, train loss: 0.51603, val loss: 0.49937\n",
      "Main effects training epoch: 171, train loss: 0.51545, val loss: 0.49960\n",
      "Main effects training epoch: 172, train loss: 0.51531, val loss: 0.50044\n",
      "Main effects training epoch: 173, train loss: 0.51510, val loss: 0.49866\n",
      "Main effects training epoch: 174, train loss: 0.51579, val loss: 0.49998\n",
      "Main effects training epoch: 175, train loss: 0.51691, val loss: 0.49917\n",
      "Main effects training epoch: 176, train loss: 0.51537, val loss: 0.50077\n",
      "Main effects training epoch: 177, train loss: 0.51551, val loss: 0.49937\n",
      "Main effects training epoch: 178, train loss: 0.51551, val loss: 0.50131\n",
      "Main effects training epoch: 179, train loss: 0.51486, val loss: 0.49888\n",
      "Main effects training epoch: 180, train loss: 0.51494, val loss: 0.49945\n",
      "Main effects training epoch: 181, train loss: 0.51553, val loss: 0.49957\n",
      "Main effects training epoch: 182, train loss: 0.51559, val loss: 0.50060\n",
      "Main effects training epoch: 183, train loss: 0.51584, val loss: 0.49958\n",
      "Main effects training epoch: 184, train loss: 0.51578, val loss: 0.50131\n",
      "Main effects training epoch: 185, train loss: 0.51536, val loss: 0.49872\n",
      "Main effects training epoch: 186, train loss: 0.51482, val loss: 0.49974\n",
      "Main effects training epoch: 187, train loss: 0.51463, val loss: 0.49986\n",
      "Main effects training epoch: 188, train loss: 0.51451, val loss: 0.49853\n",
      "Main effects training epoch: 189, train loss: 0.51440, val loss: 0.49955\n",
      "Main effects training epoch: 190, train loss: 0.51454, val loss: 0.49981\n",
      "Main effects training epoch: 191, train loss: 0.51436, val loss: 0.49788\n",
      "Main effects training epoch: 192, train loss: 0.51469, val loss: 0.49996\n",
      "Main effects training epoch: 193, train loss: 0.51477, val loss: 0.50023\n",
      "Main effects training epoch: 194, train loss: 0.51451, val loss: 0.49856\n",
      "Main effects training epoch: 195, train loss: 0.51439, val loss: 0.49942\n",
      "Main effects training epoch: 196, train loss: 0.51458, val loss: 0.50051\n",
      "Main effects training epoch: 197, train loss: 0.51501, val loss: 0.49862\n",
      "Main effects training epoch: 198, train loss: 0.51533, val loss: 0.50262\n",
      "Main effects training epoch: 199, train loss: 0.51504, val loss: 0.49753\n",
      "Main effects training epoch: 200, train loss: 0.51439, val loss: 0.49979\n",
      "Main effects training epoch: 201, train loss: 0.51391, val loss: 0.49992\n",
      "Main effects training epoch: 202, train loss: 0.51411, val loss: 0.49774\n",
      "Main effects training epoch: 203, train loss: 0.51373, val loss: 0.49890\n",
      "Main effects training epoch: 204, train loss: 0.51433, val loss: 0.49964\n",
      "Main effects training epoch: 205, train loss: 0.51402, val loss: 0.49775\n",
      "Main effects training epoch: 206, train loss: 0.51376, val loss: 0.49749\n",
      "Main effects training epoch: 207, train loss: 0.51406, val loss: 0.50089\n",
      "Main effects training epoch: 208, train loss: 0.51462, val loss: 0.49749\n",
      "Main effects training epoch: 209, train loss: 0.51455, val loss: 0.50285\n",
      "Main effects training epoch: 210, train loss: 0.51375, val loss: 0.49797\n",
      "Main effects training epoch: 211, train loss: 0.51380, val loss: 0.49921\n",
      "Main effects training epoch: 212, train loss: 0.51450, val loss: 0.50001\n",
      "Main effects training epoch: 213, train loss: 0.51363, val loss: 0.49917\n",
      "Main effects training epoch: 214, train loss: 0.51376, val loss: 0.49753\n",
      "Main effects training epoch: 215, train loss: 0.51377, val loss: 0.50111\n",
      "Main effects training epoch: 216, train loss: 0.51338, val loss: 0.49826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 217, train loss: 0.51385, val loss: 0.49843\n",
      "Main effects training epoch: 218, train loss: 0.51388, val loss: 0.50056\n",
      "Main effects training epoch: 219, train loss: 0.51511, val loss: 0.49750\n",
      "Main effects training epoch: 220, train loss: 0.51411, val loss: 0.50102\n",
      "Main effects training epoch: 221, train loss: 0.51363, val loss: 0.49964\n",
      "Main effects training epoch: 222, train loss: 0.51377, val loss: 0.49827\n",
      "Main effects training epoch: 223, train loss: 0.51395, val loss: 0.49926\n",
      "Main effects training epoch: 224, train loss: 0.51322, val loss: 0.49957\n",
      "Main effects training epoch: 225, train loss: 0.51340, val loss: 0.49955\n",
      "Main effects training epoch: 226, train loss: 0.51359, val loss: 0.49914\n",
      "Main effects training epoch: 227, train loss: 0.51359, val loss: 0.49920\n",
      "Main effects training epoch: 228, train loss: 0.51297, val loss: 0.49818\n",
      "Main effects training epoch: 229, train loss: 0.51307, val loss: 0.49870\n",
      "Main effects training epoch: 230, train loss: 0.51274, val loss: 0.49828\n",
      "Main effects training epoch: 231, train loss: 0.51272, val loss: 0.49883\n",
      "Main effects training epoch: 232, train loss: 0.51308, val loss: 0.49933\n",
      "Main effects training epoch: 233, train loss: 0.51269, val loss: 0.49735\n",
      "Main effects training epoch: 234, train loss: 0.51255, val loss: 0.49791\n",
      "Main effects training epoch: 235, train loss: 0.51253, val loss: 0.49869\n",
      "Main effects training epoch: 236, train loss: 0.51262, val loss: 0.49760\n",
      "Main effects training epoch: 237, train loss: 0.51299, val loss: 0.49728\n",
      "Main effects training epoch: 238, train loss: 0.51267, val loss: 0.49963\n",
      "Main effects training epoch: 239, train loss: 0.51307, val loss: 0.49753\n",
      "Main effects training epoch: 240, train loss: 0.51322, val loss: 0.50027\n",
      "Main effects training epoch: 241, train loss: 0.51243, val loss: 0.49771\n",
      "Main effects training epoch: 242, train loss: 0.51239, val loss: 0.49849\n",
      "Main effects training epoch: 243, train loss: 0.51233, val loss: 0.49801\n",
      "Main effects training epoch: 244, train loss: 0.51257, val loss: 0.49765\n",
      "Main effects training epoch: 245, train loss: 0.51224, val loss: 0.49785\n",
      "Main effects training epoch: 246, train loss: 0.51214, val loss: 0.49872\n",
      "Main effects training epoch: 247, train loss: 0.51243, val loss: 0.49780\n",
      "Main effects training epoch: 248, train loss: 0.51239, val loss: 0.49861\n",
      "Main effects training epoch: 249, train loss: 0.51244, val loss: 0.49864\n",
      "Main effects training epoch: 250, train loss: 0.51213, val loss: 0.49856\n",
      "Main effects training epoch: 251, train loss: 0.51206, val loss: 0.49828\n",
      "Main effects training epoch: 252, train loss: 0.51228, val loss: 0.49682\n",
      "Main effects training epoch: 253, train loss: 0.51217, val loss: 0.49974\n",
      "Main effects training epoch: 254, train loss: 0.51224, val loss: 0.49783\n",
      "Main effects training epoch: 255, train loss: 0.51212, val loss: 0.49738\n",
      "Main effects training epoch: 256, train loss: 0.51193, val loss: 0.49846\n",
      "Main effects training epoch: 257, train loss: 0.51186, val loss: 0.49781\n",
      "Main effects training epoch: 258, train loss: 0.51183, val loss: 0.49807\n",
      "Main effects training epoch: 259, train loss: 0.51197, val loss: 0.49893\n",
      "Main effects training epoch: 260, train loss: 0.51195, val loss: 0.49774\n",
      "Main effects training epoch: 261, train loss: 0.51190, val loss: 0.49854\n",
      "Main effects training epoch: 262, train loss: 0.51156, val loss: 0.49852\n",
      "Main effects training epoch: 263, train loss: 0.51159, val loss: 0.49640\n",
      "Main effects training epoch: 264, train loss: 0.51168, val loss: 0.49769\n",
      "Main effects training epoch: 265, train loss: 0.51148, val loss: 0.49808\n",
      "Main effects training epoch: 266, train loss: 0.51155, val loss: 0.49821\n",
      "Main effects training epoch: 267, train loss: 0.51125, val loss: 0.49833\n",
      "Main effects training epoch: 268, train loss: 0.51147, val loss: 0.49693\n",
      "Main effects training epoch: 269, train loss: 0.51118, val loss: 0.49697\n",
      "Main effects training epoch: 270, train loss: 0.51165, val loss: 0.49784\n",
      "Main effects training epoch: 271, train loss: 0.51275, val loss: 0.50014\n",
      "Main effects training epoch: 272, train loss: 0.51195, val loss: 0.49768\n",
      "Main effects training epoch: 273, train loss: 0.51187, val loss: 0.49930\n",
      "Main effects training epoch: 274, train loss: 0.51174, val loss: 0.49824\n",
      "Main effects training epoch: 275, train loss: 0.51104, val loss: 0.49800\n",
      "Main effects training epoch: 276, train loss: 0.51108, val loss: 0.49772\n",
      "Main effects training epoch: 277, train loss: 0.51121, val loss: 0.49743\n",
      "Main effects training epoch: 278, train loss: 0.51150, val loss: 0.49947\n",
      "Main effects training epoch: 279, train loss: 0.51080, val loss: 0.49661\n",
      "Main effects training epoch: 280, train loss: 0.51069, val loss: 0.49778\n",
      "Main effects training epoch: 281, train loss: 0.51074, val loss: 0.49724\n",
      "Main effects training epoch: 282, train loss: 0.51065, val loss: 0.49808\n",
      "Main effects training epoch: 283, train loss: 0.51045, val loss: 0.49740\n",
      "Main effects training epoch: 284, train loss: 0.51050, val loss: 0.49716\n",
      "Main effects training epoch: 285, train loss: 0.51035, val loss: 0.49730\n",
      "Main effects training epoch: 286, train loss: 0.51054, val loss: 0.49912\n",
      "Main effects training epoch: 287, train loss: 0.51112, val loss: 0.49699\n",
      "Main effects training epoch: 288, train loss: 0.51033, val loss: 0.49851\n",
      "Main effects training epoch: 289, train loss: 0.51014, val loss: 0.49694\n",
      "Main effects training epoch: 290, train loss: 0.51001, val loss: 0.49723\n",
      "Main effects training epoch: 291, train loss: 0.51019, val loss: 0.49758\n",
      "Main effects training epoch: 292, train loss: 0.50999, val loss: 0.49710\n",
      "Main effects training epoch: 293, train loss: 0.51006, val loss: 0.49799\n",
      "Main effects training epoch: 294, train loss: 0.50969, val loss: 0.49657\n",
      "Main effects training epoch: 295, train loss: 0.50998, val loss: 0.49708\n",
      "Main effects training epoch: 296, train loss: 0.51008, val loss: 0.49657\n",
      "Main effects training epoch: 297, train loss: 0.50936, val loss: 0.49669\n",
      "Main effects training epoch: 298, train loss: 0.50938, val loss: 0.49652\n",
      "Main effects training epoch: 299, train loss: 0.50970, val loss: 0.49811\n",
      "Main effects training epoch: 300, train loss: 0.50918, val loss: 0.49763\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51374, val loss: 0.49904\n",
      "Main effects tuning epoch: 2, train loss: 0.51377, val loss: 0.50126\n",
      "Main effects tuning epoch: 3, train loss: 0.51341, val loss: 0.49952\n",
      "Main effects tuning epoch: 4, train loss: 0.51332, val loss: 0.50078\n",
      "Main effects tuning epoch: 5, train loss: 0.51328, val loss: 0.49976\n",
      "Main effects tuning epoch: 6, train loss: 0.51332, val loss: 0.50008\n",
      "Main effects tuning epoch: 7, train loss: 0.51314, val loss: 0.49914\n",
      "Main effects tuning epoch: 8, train loss: 0.51318, val loss: 0.50064\n",
      "Main effects tuning epoch: 9, train loss: 0.51302, val loss: 0.49984\n",
      "Main effects tuning epoch: 10, train loss: 0.51273, val loss: 0.49950\n",
      "Main effects tuning epoch: 11, train loss: 0.51270, val loss: 0.49976\n",
      "Main effects tuning epoch: 12, train loss: 0.51283, val loss: 0.49909\n",
      "Main effects tuning epoch: 13, train loss: 0.51301, val loss: 0.49904\n",
      "Main effects tuning epoch: 14, train loss: 0.51297, val loss: 0.50072\n",
      "Main effects tuning epoch: 15, train loss: 0.51334, val loss: 0.50119\n",
      "Main effects tuning epoch: 16, train loss: 0.51348, val loss: 0.50155\n",
      "Main effects tuning epoch: 17, train loss: 0.51255, val loss: 0.49971\n",
      "Main effects tuning epoch: 18, train loss: 0.51278, val loss: 0.50127\n",
      "Main effects tuning epoch: 19, train loss: 0.51251, val loss: 0.49924\n",
      "Main effects tuning epoch: 20, train loss: 0.51265, val loss: 0.50129\n",
      "Main effects tuning epoch: 21, train loss: 0.51232, val loss: 0.49835\n",
      "Main effects tuning epoch: 22, train loss: 0.51218, val loss: 0.50119\n",
      "Main effects tuning epoch: 23, train loss: 0.51182, val loss: 0.49872\n",
      "Main effects tuning epoch: 24, train loss: 0.51179, val loss: 0.49916\n",
      "Main effects tuning epoch: 25, train loss: 0.51177, val loss: 0.49970\n",
      "Main effects tuning epoch: 26, train loss: 0.51182, val loss: 0.49874\n",
      "Main effects tuning epoch: 27, train loss: 0.51201, val loss: 0.49933\n",
      "Main effects tuning epoch: 28, train loss: 0.51163, val loss: 0.49948\n",
      "Main effects tuning epoch: 29, train loss: 0.51142, val loss: 0.49946\n",
      "Main effects tuning epoch: 30, train loss: 0.51165, val loss: 0.49930\n",
      "Main effects tuning epoch: 31, train loss: 0.51228, val loss: 0.49929\n",
      "Main effects tuning epoch: 32, train loss: 0.51213, val loss: 0.49952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects tuning epoch: 33, train loss: 0.51170, val loss: 0.50021\n",
      "Main effects tuning epoch: 34, train loss: 0.51149, val loss: 0.49948\n",
      "Main effects tuning epoch: 35, train loss: 0.51175, val loss: 0.50028\n",
      "Main effects tuning epoch: 36, train loss: 0.51195, val loss: 0.49960\n",
      "Main effects tuning epoch: 37, train loss: 0.51127, val loss: 0.49919\n",
      "Main effects tuning epoch: 38, train loss: 0.51132, val loss: 0.50048\n",
      "Main effects tuning epoch: 39, train loss: 0.51132, val loss: 0.49885\n",
      "Main effects tuning epoch: 40, train loss: 0.51135, val loss: 0.49976\n",
      "Main effects tuning epoch: 41, train loss: 0.51195, val loss: 0.49896\n",
      "Main effects tuning epoch: 42, train loss: 0.51197, val loss: 0.50021\n",
      "Main effects tuning epoch: 43, train loss: 0.51170, val loss: 0.49894\n",
      "Main effects tuning epoch: 44, train loss: 0.51107, val loss: 0.49941\n",
      "Main effects tuning epoch: 45, train loss: 0.51147, val loss: 0.49798\n",
      "Main effects tuning epoch: 46, train loss: 0.51141, val loss: 0.50029\n",
      "Main effects tuning epoch: 47, train loss: 0.51100, val loss: 0.49975\n",
      "Main effects tuning epoch: 48, train loss: 0.51113, val loss: 0.49868\n",
      "Main effects tuning epoch: 49, train loss: 0.51126, val loss: 0.50093\n",
      "Main effects tuning epoch: 50, train loss: 0.51124, val loss: 0.49885\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.47198, val loss: 0.46261\n",
      "Interaction training epoch: 2, train loss: 0.36512, val loss: 0.37572\n",
      "Interaction training epoch: 3, train loss: 0.31375, val loss: 0.32386\n",
      "Interaction training epoch: 4, train loss: 0.32733, val loss: 0.33454\n",
      "Interaction training epoch: 5, train loss: 0.29052, val loss: 0.29952\n",
      "Interaction training epoch: 6, train loss: 0.29834, val loss: 0.30138\n",
      "Interaction training epoch: 7, train loss: 0.28181, val loss: 0.29065\n",
      "Interaction training epoch: 8, train loss: 0.28828, val loss: 0.29585\n",
      "Interaction training epoch: 9, train loss: 0.28372, val loss: 0.29605\n",
      "Interaction training epoch: 10, train loss: 0.27919, val loss: 0.29017\n",
      "Interaction training epoch: 11, train loss: 0.27785, val loss: 0.28564\n",
      "Interaction training epoch: 12, train loss: 0.28263, val loss: 0.28944\n",
      "Interaction training epoch: 13, train loss: 0.28572, val loss: 0.29998\n",
      "Interaction training epoch: 14, train loss: 0.28387, val loss: 0.29811\n",
      "Interaction training epoch: 15, train loss: 0.27766, val loss: 0.28908\n",
      "Interaction training epoch: 16, train loss: 0.27540, val loss: 0.28680\n",
      "Interaction training epoch: 17, train loss: 0.27464, val loss: 0.28885\n",
      "Interaction training epoch: 18, train loss: 0.28068, val loss: 0.29141\n",
      "Interaction training epoch: 19, train loss: 0.28469, val loss: 0.29666\n",
      "Interaction training epoch: 20, train loss: 0.27895, val loss: 0.29044\n",
      "Interaction training epoch: 21, train loss: 0.27509, val loss: 0.28725\n",
      "Interaction training epoch: 22, train loss: 0.27554, val loss: 0.28597\n",
      "Interaction training epoch: 23, train loss: 0.27639, val loss: 0.29297\n",
      "Interaction training epoch: 24, train loss: 0.27658, val loss: 0.28823\n",
      "Interaction training epoch: 25, train loss: 0.28275, val loss: 0.29722\n",
      "Interaction training epoch: 26, train loss: 0.27123, val loss: 0.28301\n",
      "Interaction training epoch: 27, train loss: 0.27833, val loss: 0.29401\n",
      "Interaction training epoch: 28, train loss: 0.27149, val loss: 0.28416\n",
      "Interaction training epoch: 29, train loss: 0.27212, val loss: 0.28418\n",
      "Interaction training epoch: 30, train loss: 0.27181, val loss: 0.29049\n",
      "Interaction training epoch: 31, train loss: 0.27335, val loss: 0.28741\n",
      "Interaction training epoch: 32, train loss: 0.27435, val loss: 0.28887\n",
      "Interaction training epoch: 33, train loss: 0.27310, val loss: 0.29004\n",
      "Interaction training epoch: 34, train loss: 0.27485, val loss: 0.28779\n",
      "Interaction training epoch: 35, train loss: 0.27443, val loss: 0.29073\n",
      "Interaction training epoch: 36, train loss: 0.27083, val loss: 0.28604\n",
      "Interaction training epoch: 37, train loss: 0.27106, val loss: 0.28690\n",
      "Interaction training epoch: 38, train loss: 0.27379, val loss: 0.29073\n",
      "Interaction training epoch: 39, train loss: 0.27155, val loss: 0.28927\n",
      "Interaction training epoch: 40, train loss: 0.27242, val loss: 0.28598\n",
      "Interaction training epoch: 41, train loss: 0.27124, val loss: 0.28897\n",
      "Interaction training epoch: 42, train loss: 0.26888, val loss: 0.28500\n",
      "Interaction training epoch: 43, train loss: 0.26783, val loss: 0.28524\n",
      "Interaction training epoch: 44, train loss: 0.26675, val loss: 0.28179\n",
      "Interaction training epoch: 45, train loss: 0.26606, val loss: 0.28365\n",
      "Interaction training epoch: 46, train loss: 0.26714, val loss: 0.28578\n",
      "Interaction training epoch: 47, train loss: 0.26546, val loss: 0.28216\n",
      "Interaction training epoch: 48, train loss: 0.26652, val loss: 0.28518\n",
      "Interaction training epoch: 49, train loss: 0.26623, val loss: 0.28168\n",
      "Interaction training epoch: 50, train loss: 0.26442, val loss: 0.27927\n",
      "Interaction training epoch: 51, train loss: 0.26431, val loss: 0.28487\n",
      "Interaction training epoch: 52, train loss: 0.26467, val loss: 0.27934\n",
      "Interaction training epoch: 53, train loss: 0.26286, val loss: 0.28226\n",
      "Interaction training epoch: 54, train loss: 0.26553, val loss: 0.28187\n",
      "Interaction training epoch: 55, train loss: 0.26501, val loss: 0.28538\n",
      "Interaction training epoch: 56, train loss: 0.26337, val loss: 0.27924\n",
      "Interaction training epoch: 57, train loss: 0.26200, val loss: 0.28002\n",
      "Interaction training epoch: 58, train loss: 0.26035, val loss: 0.27981\n",
      "Interaction training epoch: 59, train loss: 0.26055, val loss: 0.27461\n",
      "Interaction training epoch: 60, train loss: 0.26487, val loss: 0.28364\n",
      "Interaction training epoch: 61, train loss: 0.26166, val loss: 0.27925\n",
      "Interaction training epoch: 62, train loss: 0.26375, val loss: 0.28182\n",
      "Interaction training epoch: 63, train loss: 0.26427, val loss: 0.28612\n",
      "Interaction training epoch: 64, train loss: 0.26358, val loss: 0.27685\n",
      "Interaction training epoch: 65, train loss: 0.26386, val loss: 0.28552\n",
      "Interaction training epoch: 66, train loss: 0.25963, val loss: 0.27873\n",
      "Interaction training epoch: 67, train loss: 0.26100, val loss: 0.27937\n",
      "Interaction training epoch: 68, train loss: 0.26000, val loss: 0.27656\n",
      "Interaction training epoch: 69, train loss: 0.26387, val loss: 0.28657\n",
      "Interaction training epoch: 70, train loss: 0.25819, val loss: 0.27601\n",
      "Interaction training epoch: 71, train loss: 0.26046, val loss: 0.28213\n",
      "Interaction training epoch: 72, train loss: 0.26045, val loss: 0.27804\n",
      "Interaction training epoch: 73, train loss: 0.25645, val loss: 0.27593\n",
      "Interaction training epoch: 74, train loss: 0.26090, val loss: 0.28157\n",
      "Interaction training epoch: 75, train loss: 0.25845, val loss: 0.27914\n",
      "Interaction training epoch: 76, train loss: 0.26030, val loss: 0.28099\n",
      "Interaction training epoch: 77, train loss: 0.25853, val loss: 0.27834\n",
      "Interaction training epoch: 78, train loss: 0.25837, val loss: 0.27902\n",
      "Interaction training epoch: 79, train loss: 0.26126, val loss: 0.28416\n",
      "Interaction training epoch: 80, train loss: 0.25842, val loss: 0.27546\n",
      "Interaction training epoch: 81, train loss: 0.25774, val loss: 0.28227\n",
      "Interaction training epoch: 82, train loss: 0.25597, val loss: 0.27691\n",
      "Interaction training epoch: 83, train loss: 0.25893, val loss: 0.27985\n",
      "Interaction training epoch: 84, train loss: 0.25680, val loss: 0.27899\n",
      "Interaction training epoch: 85, train loss: 0.25624, val loss: 0.27888\n",
      "Interaction training epoch: 86, train loss: 0.25439, val loss: 0.27632\n",
      "Interaction training epoch: 87, train loss: 0.25515, val loss: 0.27837\n",
      "Interaction training epoch: 88, train loss: 0.25721, val loss: 0.27925\n",
      "Interaction training epoch: 89, train loss: 0.25495, val loss: 0.27804\n",
      "Interaction training epoch: 90, train loss: 0.25431, val loss: 0.27648\n",
      "Interaction training epoch: 91, train loss: 0.25623, val loss: 0.28188\n",
      "Interaction training epoch: 92, train loss: 0.25670, val loss: 0.27851\n",
      "Interaction training epoch: 93, train loss: 0.25240, val loss: 0.27643\n",
      "Interaction training epoch: 94, train loss: 0.25605, val loss: 0.27928\n",
      "Interaction training epoch: 95, train loss: 0.25229, val loss: 0.27739\n",
      "Interaction training epoch: 96, train loss: 0.25156, val loss: 0.27828\n",
      "Interaction training epoch: 97, train loss: 0.25363, val loss: 0.27968\n",
      "Interaction training epoch: 98, train loss: 0.25226, val loss: 0.27855\n",
      "Interaction training epoch: 99, train loss: 0.25109, val loss: 0.27784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 100, train loss: 0.25364, val loss: 0.27997\n",
      "Interaction training epoch: 101, train loss: 0.25057, val loss: 0.27732\n",
      "Interaction training epoch: 102, train loss: 0.24949, val loss: 0.27986\n",
      "Interaction training epoch: 103, train loss: 0.25348, val loss: 0.28131\n",
      "Interaction training epoch: 104, train loss: 0.25334, val loss: 0.28405\n",
      "Interaction training epoch: 105, train loss: 0.25127, val loss: 0.28004\n",
      "Interaction training epoch: 106, train loss: 0.25207, val loss: 0.28055\n",
      "Interaction training epoch: 107, train loss: 0.25002, val loss: 0.27801\n",
      "Interaction training epoch: 108, train loss: 0.24900, val loss: 0.27856\n",
      "Interaction training epoch: 109, train loss: 0.24917, val loss: 0.27864\n",
      "Interaction training epoch: 110, train loss: 0.25151, val loss: 0.28454\n",
      "Interaction training epoch: 111, train loss: 0.25157, val loss: 0.28078\n",
      "Interaction training epoch: 112, train loss: 0.24855, val loss: 0.28199\n",
      "Interaction training epoch: 113, train loss: 0.24778, val loss: 0.27522\n",
      "Interaction training epoch: 114, train loss: 0.24938, val loss: 0.28194\n",
      "Interaction training epoch: 115, train loss: 0.25011, val loss: 0.28033\n",
      "Interaction training epoch: 116, train loss: 0.25068, val loss: 0.28536\n",
      "Interaction training epoch: 117, train loss: 0.24977, val loss: 0.28056\n",
      "Interaction training epoch: 118, train loss: 0.25127, val loss: 0.28398\n",
      "Interaction training epoch: 119, train loss: 0.24797, val loss: 0.27762\n",
      "Interaction training epoch: 120, train loss: 0.25236, val loss: 0.28276\n",
      "Interaction training epoch: 121, train loss: 0.24895, val loss: 0.27735\n",
      "Interaction training epoch: 122, train loss: 0.24659, val loss: 0.28084\n",
      "Interaction training epoch: 123, train loss: 0.25224, val loss: 0.28205\n",
      "Interaction training epoch: 124, train loss: 0.24655, val loss: 0.28055\n",
      "Interaction training epoch: 125, train loss: 0.24596, val loss: 0.27877\n",
      "Interaction training epoch: 126, train loss: 0.24603, val loss: 0.28082\n",
      "Interaction training epoch: 127, train loss: 0.24622, val loss: 0.27646\n",
      "Interaction training epoch: 128, train loss: 0.24736, val loss: 0.28415\n",
      "Interaction training epoch: 129, train loss: 0.24544, val loss: 0.27946\n",
      "Interaction training epoch: 130, train loss: 0.24724, val loss: 0.28259\n",
      "Interaction training epoch: 131, train loss: 0.24669, val loss: 0.28460\n",
      "Interaction training epoch: 132, train loss: 0.24571, val loss: 0.28071\n",
      "Interaction training epoch: 133, train loss: 0.24500, val loss: 0.27923\n",
      "Interaction training epoch: 134, train loss: 0.24542, val loss: 0.28204\n",
      "Interaction training epoch: 135, train loss: 0.24593, val loss: 0.27972\n",
      "Interaction training epoch: 136, train loss: 0.24566, val loss: 0.28164\n",
      "Interaction training epoch: 137, train loss: 0.24354, val loss: 0.27921\n",
      "Interaction training epoch: 138, train loss: 0.24416, val loss: 0.27657\n",
      "Interaction training epoch: 139, train loss: 0.24465, val loss: 0.28138\n",
      "Interaction training epoch: 140, train loss: 0.24463, val loss: 0.27855\n",
      "Interaction training epoch: 141, train loss: 0.24406, val loss: 0.28217\n",
      "Interaction training epoch: 142, train loss: 0.24301, val loss: 0.27973\n",
      "Interaction training epoch: 143, train loss: 0.24152, val loss: 0.27695\n",
      "Interaction training epoch: 144, train loss: 0.24623, val loss: 0.28321\n",
      "Interaction training epoch: 145, train loss: 0.24187, val loss: 0.27812\n",
      "Interaction training epoch: 146, train loss: 0.24382, val loss: 0.27661\n",
      "Interaction training epoch: 147, train loss: 0.24369, val loss: 0.27949\n",
      "Interaction training epoch: 148, train loss: 0.24287, val loss: 0.27892\n",
      "Interaction training epoch: 149, train loss: 0.24320, val loss: 0.27514\n",
      "Interaction training epoch: 150, train loss: 0.24189, val loss: 0.28084\n",
      "Interaction training epoch: 151, train loss: 0.24290, val loss: 0.27204\n",
      "Interaction training epoch: 152, train loss: 0.24183, val loss: 0.28030\n",
      "Interaction training epoch: 153, train loss: 0.24112, val loss: 0.27406\n",
      "Interaction training epoch: 154, train loss: 0.24142, val loss: 0.27695\n",
      "Interaction training epoch: 155, train loss: 0.24162, val loss: 0.27623\n",
      "Interaction training epoch: 156, train loss: 0.24039, val loss: 0.27768\n",
      "Interaction training epoch: 157, train loss: 0.24153, val loss: 0.27440\n",
      "Interaction training epoch: 158, train loss: 0.24303, val loss: 0.28084\n",
      "Interaction training epoch: 159, train loss: 0.24232, val loss: 0.27859\n",
      "Interaction training epoch: 160, train loss: 0.23957, val loss: 0.27635\n",
      "Interaction training epoch: 161, train loss: 0.24012, val loss: 0.27322\n",
      "Interaction training epoch: 162, train loss: 0.23861, val loss: 0.27767\n",
      "Interaction training epoch: 163, train loss: 0.23668, val loss: 0.27215\n",
      "Interaction training epoch: 164, train loss: 0.24056, val loss: 0.27641\n",
      "Interaction training epoch: 165, train loss: 0.23866, val loss: 0.27627\n",
      "Interaction training epoch: 166, train loss: 0.23932, val loss: 0.27473\n",
      "Interaction training epoch: 167, train loss: 0.24072, val loss: 0.27647\n",
      "Interaction training epoch: 168, train loss: 0.23821, val loss: 0.27629\n",
      "Interaction training epoch: 169, train loss: 0.24042, val loss: 0.27711\n",
      "Interaction training epoch: 170, train loss: 0.23956, val loss: 0.27522\n",
      "Interaction training epoch: 171, train loss: 0.24286, val loss: 0.27697\n",
      "Interaction training epoch: 172, train loss: 0.24216, val loss: 0.27557\n",
      "Interaction training epoch: 173, train loss: 0.23967, val loss: 0.28163\n",
      "Interaction training epoch: 174, train loss: 0.24077, val loss: 0.27552\n",
      "Interaction training epoch: 175, train loss: 0.23751, val loss: 0.27439\n",
      "Interaction training epoch: 176, train loss: 0.24319, val loss: 0.27806\n",
      "Interaction training epoch: 177, train loss: 0.23950, val loss: 0.27905\n",
      "Interaction training epoch: 178, train loss: 0.23859, val loss: 0.27114\n",
      "Interaction training epoch: 179, train loss: 0.23842, val loss: 0.27270\n",
      "Interaction training epoch: 180, train loss: 0.23641, val loss: 0.27510\n",
      "Interaction training epoch: 181, train loss: 0.23794, val loss: 0.27729\n",
      "Interaction training epoch: 182, train loss: 0.23644, val loss: 0.26903\n",
      "Interaction training epoch: 183, train loss: 0.23557, val loss: 0.27575\n",
      "Interaction training epoch: 184, train loss: 0.23651, val loss: 0.27315\n",
      "Interaction training epoch: 185, train loss: 0.23479, val loss: 0.27293\n",
      "Interaction training epoch: 186, train loss: 0.23662, val loss: 0.27548\n",
      "Interaction training epoch: 187, train loss: 0.23600, val loss: 0.27097\n",
      "Interaction training epoch: 188, train loss: 0.23499, val loss: 0.27579\n",
      "Interaction training epoch: 189, train loss: 0.23764, val loss: 0.27434\n",
      "Interaction training epoch: 190, train loss: 0.23497, val loss: 0.27782\n",
      "Interaction training epoch: 191, train loss: 0.23502, val loss: 0.27118\n",
      "Interaction training epoch: 192, train loss: 0.23441, val loss: 0.27415\n",
      "Interaction training epoch: 193, train loss: 0.23401, val loss: 0.27259\n",
      "Interaction training epoch: 194, train loss: 0.23667, val loss: 0.27313\n",
      "Interaction training epoch: 195, train loss: 0.23467, val loss: 0.27751\n",
      "Interaction training epoch: 196, train loss: 0.23743, val loss: 0.27081\n",
      "Interaction training epoch: 197, train loss: 0.23638, val loss: 0.27667\n",
      "Interaction training epoch: 198, train loss: 0.23279, val loss: 0.27261\n",
      "Interaction training epoch: 199, train loss: 0.23510, val loss: 0.26997\n",
      "Interaction training epoch: 200, train loss: 0.23368, val loss: 0.27332\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########2 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.23576, val loss: 0.26889\n",
      "Interaction tuning epoch: 2, train loss: 0.23854, val loss: 0.27686\n",
      "Interaction tuning epoch: 3, train loss: 0.23773, val loss: 0.27696\n",
      "Interaction tuning epoch: 4, train loss: 0.23896, val loss: 0.27160\n",
      "Interaction tuning epoch: 5, train loss: 0.23893, val loss: 0.27539\n",
      "Interaction tuning epoch: 6, train loss: 0.23783, val loss: 0.27176\n",
      "Interaction tuning epoch: 7, train loss: 0.23777, val loss: 0.27189\n",
      "Interaction tuning epoch: 8, train loss: 0.23520, val loss: 0.27112\n",
      "Interaction tuning epoch: 9, train loss: 0.23873, val loss: 0.27349\n",
      "Interaction tuning epoch: 10, train loss: 0.23420, val loss: 0.26818\n",
      "Interaction tuning epoch: 11, train loss: 0.23870, val loss: 0.27094\n",
      "Interaction tuning epoch: 12, train loss: 0.23355, val loss: 0.26822\n",
      "Interaction tuning epoch: 13, train loss: 0.23649, val loss: 0.27171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 14, train loss: 0.23416, val loss: 0.26990\n",
      "Interaction tuning epoch: 15, train loss: 0.23592, val loss: 0.26737\n",
      "Interaction tuning epoch: 16, train loss: 0.23564, val loss: 0.27070\n",
      "Interaction tuning epoch: 17, train loss: 0.23527, val loss: 0.26960\n",
      "Interaction tuning epoch: 18, train loss: 0.23670, val loss: 0.27127\n",
      "Interaction tuning epoch: 19, train loss: 0.23599, val loss: 0.27284\n",
      "Interaction tuning epoch: 20, train loss: 0.23671, val loss: 0.26875\n",
      "Interaction tuning epoch: 21, train loss: 0.23534, val loss: 0.26888\n",
      "Interaction tuning epoch: 22, train loss: 0.23594, val loss: 0.27174\n",
      "Interaction tuning epoch: 23, train loss: 0.23368, val loss: 0.26534\n",
      "Interaction tuning epoch: 24, train loss: 0.23328, val loss: 0.27073\n",
      "Interaction tuning epoch: 25, train loss: 0.23488, val loss: 0.27152\n",
      "Interaction tuning epoch: 26, train loss: 0.23229, val loss: 0.26790\n",
      "Interaction tuning epoch: 27, train loss: 0.23341, val loss: 0.26840\n",
      "Interaction tuning epoch: 28, train loss: 0.23366, val loss: 0.27082\n",
      "Interaction tuning epoch: 29, train loss: 0.23731, val loss: 0.27232\n",
      "Interaction tuning epoch: 30, train loss: 0.23638, val loss: 0.27620\n",
      "Interaction tuning epoch: 31, train loss: 0.23424, val loss: 0.26954\n",
      "Interaction tuning epoch: 32, train loss: 0.23430, val loss: 0.27101\n",
      "Interaction tuning epoch: 33, train loss: 0.23234, val loss: 0.26797\n",
      "Interaction tuning epoch: 34, train loss: 0.23353, val loss: 0.26681\n",
      "Interaction tuning epoch: 35, train loss: 0.23282, val loss: 0.27196\n",
      "Interaction tuning epoch: 36, train loss: 0.23098, val loss: 0.26723\n",
      "Interaction tuning epoch: 37, train loss: 0.23475, val loss: 0.27325\n",
      "Interaction tuning epoch: 38, train loss: 0.23340, val loss: 0.26922\n",
      "Interaction tuning epoch: 39, train loss: 0.23236, val loss: 0.26641\n",
      "Interaction tuning epoch: 40, train loss: 0.23312, val loss: 0.27219\n",
      "Interaction tuning epoch: 41, train loss: 0.23153, val loss: 0.26958\n",
      "Interaction tuning epoch: 42, train loss: 0.23030, val loss: 0.26625\n",
      "Interaction tuning epoch: 43, train loss: 0.23213, val loss: 0.26723\n",
      "Interaction tuning epoch: 44, train loss: 0.23216, val loss: 0.26930\n",
      "Interaction tuning epoch: 45, train loss: 0.23193, val loss: 0.26939\n",
      "Interaction tuning epoch: 46, train loss: 0.23296, val loss: 0.26726\n",
      "Interaction tuning epoch: 47, train loss: 0.23145, val loss: 0.26855\n",
      "Interaction tuning epoch: 48, train loss: 0.23173, val loss: 0.26366\n",
      "Interaction tuning epoch: 49, train loss: 0.23220, val loss: 0.27031\n",
      "Interaction tuning epoch: 50, train loss: 0.22931, val loss: 0.26651\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 50.148874282836914\n",
      "After the gam stage, training error is 0.22931 , validation error is 0.26651\n",
      "missing value counts: 99188\n",
      "[SoftImpute] Max Singular Value of X_init = 3.514510\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed BCE=0.194407 validation BCE=0.285988,rank=5\n",
      "[SoftImpute] Iter 2: observed BCE=0.190728 validation BCE=0.296496,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.188486 validation BCE=0.295843,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.187165 validation BCE=0.295428,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.186243 validation BCE=0.295087,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.185657 validation BCE=0.283632,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.185571 validation BCE=0.283076,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.185681 validation BCE=0.281170,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.185857 validation BCE=0.280616,rank=5\n",
      "[SoftImpute] Iter 10: observed BCE=0.185504 validation BCE=0.279835,rank=5\n",
      "[SoftImpute] Iter 11: observed BCE=0.185542 validation BCE=0.279607,rank=5\n",
      "[SoftImpute] Iter 12: observed BCE=0.185552 validation BCE=0.279041,rank=5\n",
      "[SoftImpute] Iter 13: observed BCE=0.185522 validation BCE=0.278780,rank=5\n",
      "[SoftImpute] Iter 14: observed BCE=0.185331 validation BCE=0.278547,rank=5\n",
      "[SoftImpute] Iter 15: observed BCE=0.185265 validation BCE=0.278248,rank=5\n",
      "[SoftImpute] Iter 16: observed BCE=0.185031 validation BCE=0.278047,rank=5\n",
      "[SoftImpute] Iter 17: observed BCE=0.185067 validation BCE=0.277801,rank=5\n",
      "[SoftImpute] Iter 18: observed BCE=0.184699 validation BCE=0.267280,rank=5\n",
      "[SoftImpute] Iter 19: observed BCE=0.184500 validation BCE=0.266840,rank=5\n",
      "[SoftImpute] Iter 20: observed BCE=0.185183 validation BCE=0.266367,rank=5\n",
      "[SoftImpute] Iter 21: observed BCE=0.185259 validation BCE=0.266527,rank=5\n",
      "[SoftImpute] Iter 22: observed BCE=0.185470 validation BCE=0.265812,rank=5\n",
      "[SoftImpute] Iter 23: observed BCE=0.185337 validation BCE=0.265501,rank=5\n",
      "[SoftImpute] Iter 24: observed BCE=0.185282 validation BCE=0.265272,rank=5\n",
      "[SoftImpute] Iter 25: observed BCE=0.185235 validation BCE=0.265227,rank=5\n",
      "[SoftImpute] Iter 26: observed BCE=0.185190 validation BCE=0.264956,rank=5\n",
      "[SoftImpute] Iter 27: observed BCE=0.185101 validation BCE=0.264943,rank=5\n",
      "[SoftImpute] Iter 28: observed BCE=0.184903 validation BCE=0.264666,rank=5\n",
      "[SoftImpute] Iter 29: observed BCE=0.184891 validation BCE=0.264499,rank=5\n",
      "[SoftImpute] Iter 30: observed BCE=0.184733 validation BCE=0.264543,rank=5\n",
      "[SoftImpute] Iter 31: observed BCE=0.184793 validation BCE=0.264583,rank=5\n",
      "[SoftImpute] Iter 32: observed BCE=0.184892 validation BCE=0.264389,rank=5\n",
      "[SoftImpute] Iter 33: observed BCE=0.184523 validation BCE=0.263963,rank=5\n",
      "[SoftImpute] Iter 34: observed BCE=0.184463 validation BCE=0.263937,rank=5\n",
      "[SoftImpute] Iter 35: observed BCE=0.184756 validation BCE=0.264144,rank=5\n",
      "[SoftImpute] Iter 36: observed BCE=0.184382 validation BCE=0.263866,rank=5\n",
      "[SoftImpute] Iter 37: observed BCE=0.184393 validation BCE=0.263905,rank=5\n",
      "[SoftImpute] Iter 38: observed BCE=0.184804 validation BCE=0.264287,rank=5\n",
      "[SoftImpute] Iter 39: observed BCE=0.184633 validation BCE=0.263878,rank=5\n",
      "[SoftImpute] Iter 40: observed BCE=0.184495 validation BCE=0.263972,rank=5\n",
      "[SoftImpute] Iter 41: observed BCE=0.184428 validation BCE=0.263750,rank=5\n",
      "[SoftImpute] Iter 42: observed BCE=0.184497 validation BCE=0.263444,rank=5\n",
      "[SoftImpute] Iter 43: observed BCE=0.184368 validation BCE=0.263623,rank=5\n",
      "[SoftImpute] Iter 44: observed BCE=0.184573 validation BCE=0.264133,rank=5\n",
      "[SoftImpute] Iter 45: observed BCE=0.184346 validation BCE=0.263562,rank=5\n",
      "[SoftImpute] Iter 46: observed BCE=0.184378 validation BCE=0.263451,rank=5\n",
      "[SoftImpute] Iter 47: observed BCE=0.184447 validation BCE=0.263738,rank=5\n",
      "[SoftImpute] Iter 48: observed BCE=0.184515 validation BCE=0.263925,rank=5\n",
      "[SoftImpute] Iter 49: observed BCE=0.184528 validation BCE=0.263766,rank=5\n",
      "[SoftImpute] Iter 50: observed BCE=0.184337 validation BCE=0.263935,rank=5\n",
      "[SoftImpute] Iter 51: observed BCE=0.184106 validation BCE=0.254611,rank=5\n",
      "[SoftImpute] Iter 52: observed BCE=0.183971 validation BCE=0.254101,rank=5\n",
      "[SoftImpute] Iter 53: observed BCE=0.184319 validation BCE=0.253869,rank=5\n",
      "[SoftImpute] Iter 54: observed BCE=0.184004 validation BCE=0.253675,rank=5\n",
      "[SoftImpute] Iter 55: observed BCE=0.183630 validation BCE=0.253149,rank=5\n",
      "[SoftImpute] Iter 56: observed BCE=0.183778 validation BCE=0.253005,rank=5\n",
      "[SoftImpute] Iter 57: observed BCE=0.183732 validation BCE=0.252780,rank=5\n",
      "[SoftImpute] Iter 58: observed BCE=0.183733 validation BCE=0.252770,rank=5\n",
      "[SoftImpute] Iter 59: observed BCE=0.184012 validation BCE=0.252760,rank=5\n",
      "[SoftImpute] Iter 60: observed BCE=0.183907 validation BCE=0.252728,rank=5\n",
      "[SoftImpute] Iter 61: observed BCE=0.183690 validation BCE=0.252330,rank=5\n",
      "[SoftImpute] Iter 62: observed BCE=0.183822 validation BCE=0.252643,rank=5\n",
      "[SoftImpute] Iter 63: observed BCE=0.184027 validation BCE=0.252447,rank=5\n",
      "[SoftImpute] Iter 64: observed BCE=0.183794 validation BCE=0.252001,rank=5\n",
      "[SoftImpute] Iter 65: observed BCE=0.183601 validation BCE=0.252074,rank=5\n",
      "[SoftImpute] Iter 66: observed BCE=0.183693 validation BCE=0.252331,rank=5\n",
      "[SoftImpute] Iter 67: observed BCE=0.183608 validation BCE=0.252017,rank=5\n",
      "[SoftImpute] Iter 68: observed BCE=0.183821 validation BCE=0.252222,rank=5\n",
      "[SoftImpute] Iter 69: observed BCE=0.183888 validation BCE=0.252016,rank=5\n",
      "[SoftImpute] Iter 70: observed BCE=0.183875 validation BCE=0.251916,rank=5\n",
      "[SoftImpute] Iter 71: observed BCE=0.183747 validation BCE=0.251894,rank=5\n",
      "[SoftImpute] Iter 72: observed BCE=0.183800 validation BCE=0.252182,rank=5\n",
      "[SoftImpute] Iter 73: observed BCE=0.183664 validation BCE=0.251685,rank=5\n",
      "[SoftImpute] Iter 74: observed BCE=0.183650 validation BCE=0.252043,rank=5\n",
      "[SoftImpute] Iter 75: observed BCE=0.183629 validation BCE=0.252028,rank=5\n",
      "[SoftImpute] Iter 76: observed BCE=0.183698 validation BCE=0.251880,rank=5\n",
      "[SoftImpute] Iter 77: observed BCE=0.183791 validation BCE=0.251990,rank=5\n",
      "[SoftImpute] Iter 78: observed BCE=0.183730 validation BCE=0.252001,rank=5\n",
      "[SoftImpute] Iter 79: observed BCE=0.183734 validation BCE=0.251582,rank=5\n",
      "[SoftImpute] Iter 80: observed BCE=0.183732 validation BCE=0.251593,rank=5\n",
      "[SoftImpute] Iter 81: observed BCE=0.183789 validation BCE=0.251777,rank=5\n",
      "[SoftImpute] Iter 82: observed BCE=0.183830 validation BCE=0.251460,rank=5\n",
      "[SoftImpute] Iter 83: observed BCE=0.184037 validation BCE=0.251321,rank=5\n",
      "[SoftImpute] Iter 84: observed BCE=0.184035 validation BCE=0.251562,rank=5\n",
      "[SoftImpute] Iter 85: observed BCE=0.183914 validation BCE=0.251392,rank=5\n",
      "[SoftImpute] Iter 86: observed BCE=0.183772 validation BCE=0.251398,rank=5\n",
      "[SoftImpute] Iter 87: observed BCE=0.183870 validation BCE=0.251550,rank=5\n",
      "[SoftImpute] Iter 88: observed BCE=0.183716 validation BCE=0.251169,rank=5\n",
      "[SoftImpute] Iter 89: observed BCE=0.183689 validation BCE=0.251290,rank=5\n",
      "[SoftImpute] Iter 90: observed BCE=0.183642 validation BCE=0.251319,rank=5\n",
      "[SoftImpute] Iter 91: observed BCE=0.183878 validation BCE=0.251101,rank=5\n",
      "[SoftImpute] Iter 92: observed BCE=0.183801 validation BCE=0.251384,rank=5\n",
      "[SoftImpute] Iter 93: observed BCE=0.183985 validation BCE=0.251638,rank=5\n",
      "[SoftImpute] Iter 94: observed BCE=0.183664 validation BCE=0.251207,rank=5\n",
      "[SoftImpute] Iter 95: observed BCE=0.183780 validation BCE=0.251189,rank=5\n",
      "[SoftImpute] Iter 96: observed BCE=0.183929 validation BCE=0.251418,rank=5\n",
      "[SoftImpute] Iter 97: observed BCE=0.183924 validation BCE=0.251114,rank=5\n",
      "[SoftImpute] Iter 98: observed BCE=0.183733 validation BCE=0.251269,rank=5\n",
      "[SoftImpute] Iter 99: observed BCE=0.183613 validation BCE=0.251562,rank=5\n",
      "[SoftImpute] Iter 100: observed BCE=0.183698 validation BCE=0.251328,rank=5\n",
      "[SoftImpute] Stopped after iteration 100 for lambda=0.070290\n",
      "final num of user group: 9\n",
      "final num of item group: 3\n",
      "change mode state : True\n",
      "time cost: 7.625633239746094\n",
      "After the matrix factor stage, training error is 0.18370, validation error is 0.25133\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x00000137B5295268>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x00000137B3ABCA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x00000137B5295268>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x00000137B3ABCA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x00000137B5295268>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x00000137B3ABCA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x00000137B5295268>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x00000137B3ABCA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x00000137B5295268>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x00000137B3ABCA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x00000137B5295268>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x00000137B3ABCA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x00000137B5295268>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x00000137B3ABCA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x00000137B5295268>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x00000137B3ABCA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x00000137B5295268>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x00000137B3ABCA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x00000137B5295268>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x00000137B3ABCA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x00000137B5295268>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x00000137B3ABCA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x00000137B5295268>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x00000137B3ABCA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x00000137B5295268>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x00000137B3ABCA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x00000137B5295268>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x00000137B3ABCA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x00000137B5295268>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x00000137B3ABCA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x00000137B5295268>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x00000137B3ABCA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x00000137B5295268>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x00000137B3ABCA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x00000137B5295268>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x00000137B3ABCA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x00000137B5295268>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x00000137B3ABCA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x00000137B5295268>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x00000137B3ABCA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n"
     ]
    }
   ],
   "source": [
    "result_lvxnn = lvxnn('warm',tr_x, val_x, te_x, tr_y, val_y, te_y,tr_Xi, val_Xi, te_Xi, tr_idx, val_idx, meta_info, model_info, task_type , random_state=0, params=lx_params)\n",
    "result_svd = svd('warm',tr_x, val_x, te_x, tr_y, val_y, te_y,tr_Xi, val_Xi, te_Xi, tr_idx, val_idx, meta_info, model_info, task_type , random_state=0)\n",
    "result_deepfm, result_fm = deepfm_fm('warm',train,test,tr_x, val_x, te_x, tr_y, val_y, te_y,tr_Xi, val_Xi, te_Xi, tr_idx, val_idx, meta_info, model_info, task_type , random_state=0, epochs=300)\n",
    "result_xgb = xgb('warm',tr_x, val_x, te_x, tr_y, val_y, te_y,tr_Xi, val_Xi, te_Xi, tr_idx, val_idx, meta_info, model_info, task_type , random_state=0)\n",
    "\n",
    "result_sim_re = pd.concat([result_lvxnn,result_svd,result_xgb,result_deepfm,result_fm],0)\n",
    "\n",
    "result_sim_re.to_csv('simulation_classification_result.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
