{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0624 11:22:41.033458 17652 deprecation.py:323] From C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.86 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../..\\lvxnn\\DataReader.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 0.26 MB\n",
      "Decreased by 69.6%\n",
      "Memory usage of dataframe is 0.21 MB\n",
      "Memory usage after optimization is: 0.07 MB\n",
      "Decreased by 69.6%\n",
      "cold start user: 0\n",
      "cold start item: 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error,roc_auc_score,mean_absolute_error,log_loss\n",
    "import sys\n",
    "sys.path.append('../benchmark/')\n",
    "from lvxnn_test import lvxnn\n",
    "from xgb_test import xgb\n",
    "from svd_test import svd\n",
    "from deepfm_fm_test import deepfm_fm\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from lvxnn.LVXNN import LV_XNN\n",
    "from lvxnn.DataReader import data_initialize\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "data= pd.read_csv('../simulation/data/sim_0.9.csv')\n",
    "train , test = train_test_split(data,test_size=0.2,random_state=0)\n",
    "task_type = \"Regression\"\n",
    "\n",
    "meta_info = OrderedDict()\n",
    "\n",
    "meta_info['uf_1']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_2']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_3']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_4']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_5']={'type': 'continues','source':'user'}\n",
    "meta_info['if_1']={'type': 'continues','source':'item'}\n",
    "meta_info['if_2']={'type': 'continues','source':'item'}\n",
    "meta_info['if_3']={'type': 'continues','source':'item'}\n",
    "meta_info['if_4']={'type': 'continues','source':'item'}\n",
    "meta_info['if_5']={'type': 'continues','source':'item'}\n",
    "meta_info['user_id']={\"type\":\"id\",'source':'user'}\n",
    "meta_info['item_id']={\"type\":\"id\",'source':'item'}\n",
    "meta_info['target']={\"type\":\"target\",'source':''}\n",
    "\n",
    "lx_params = {\n",
    "        \"main_effect_epochs\":300,\n",
    "        \"interaction_epochs\" : 200 ,\n",
    "        \"tuning_epochs\" : 50 , \n",
    "        \"mf_training_iters\": 200,\n",
    "        \"u_group_num\":30,\n",
    "        \"i_group_num\":50\n",
    "    }\n",
    "\n",
    "xx, Xi, y, tr_idx, xx_t, Xi_t, y_t, val_x, val_y, val_idx, meta_info, model_info = data_initialize(train,test,meta_info,task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.02581, val loss: 4.15569\n",
      "Main effects training epoch: 2, train loss: 3.86663, val loss: 4.00300\n",
      "Main effects training epoch: 3, train loss: 3.66700, val loss: 3.81314\n",
      "Main effects training epoch: 4, train loss: 3.48207, val loss: 3.63384\n",
      "Main effects training epoch: 5, train loss: 3.38250, val loss: 3.50337\n",
      "Main effects training epoch: 6, train loss: 3.28135, val loss: 3.35840\n",
      "Main effects training epoch: 7, train loss: 3.28186, val loss: 3.35070\n",
      "Main effects training epoch: 8, train loss: 3.27336, val loss: 3.36040\n",
      "Main effects training epoch: 9, train loss: 3.25283, val loss: 3.34172\n",
      "Main effects training epoch: 10, train loss: 3.15250, val loss: 3.23796\n",
      "Main effects training epoch: 11, train loss: 3.05938, val loss: 3.14667\n",
      "Main effects training epoch: 12, train loss: 3.05048, val loss: 3.13776\n",
      "Main effects training epoch: 13, train loss: 2.97453, val loss: 3.06166\n",
      "Main effects training epoch: 14, train loss: 2.90463, val loss: 2.99088\n",
      "Main effects training epoch: 15, train loss: 2.85090, val loss: 2.93330\n",
      "Main effects training epoch: 16, train loss: 2.80705, val loss: 2.88247\n",
      "Main effects training epoch: 17, train loss: 2.72197, val loss: 2.79421\n",
      "Main effects training epoch: 18, train loss: 2.70970, val loss: 2.78070\n",
      "Main effects training epoch: 19, train loss: 2.64818, val loss: 2.71541\n",
      "Main effects training epoch: 20, train loss: 2.60938, val loss: 2.68330\n",
      "Main effects training epoch: 21, train loss: 2.57306, val loss: 2.65275\n",
      "Main effects training epoch: 22, train loss: 2.51936, val loss: 2.59316\n",
      "Main effects training epoch: 23, train loss: 2.51905, val loss: 2.60367\n",
      "Main effects training epoch: 24, train loss: 2.44088, val loss: 2.52255\n",
      "Main effects training epoch: 25, train loss: 2.41554, val loss: 2.50227\n",
      "Main effects training epoch: 26, train loss: 2.38866, val loss: 2.46996\n",
      "Main effects training epoch: 27, train loss: 2.36765, val loss: 2.45355\n",
      "Main effects training epoch: 28, train loss: 2.33153, val loss: 2.41112\n",
      "Main effects training epoch: 29, train loss: 2.28303, val loss: 2.36249\n",
      "Main effects training epoch: 30, train loss: 2.26930, val loss: 2.35093\n",
      "Main effects training epoch: 31, train loss: 2.21173, val loss: 2.29201\n",
      "Main effects training epoch: 32, train loss: 2.24060, val loss: 2.32154\n",
      "Main effects training epoch: 33, train loss: 2.17058, val loss: 2.25252\n",
      "Main effects training epoch: 34, train loss: 2.18446, val loss: 2.25689\n",
      "Main effects training epoch: 35, train loss: 2.16814, val loss: 2.24363\n",
      "Main effects training epoch: 36, train loss: 2.13803, val loss: 2.21848\n",
      "Main effects training epoch: 37, train loss: 2.07828, val loss: 2.15467\n",
      "Main effects training epoch: 38, train loss: 2.10415, val loss: 2.17893\n",
      "Main effects training epoch: 39, train loss: 2.06347, val loss: 2.14620\n",
      "Main effects training epoch: 40, train loss: 2.05131, val loss: 2.12958\n",
      "Main effects training epoch: 41, train loss: 2.04399, val loss: 2.11796\n",
      "Main effects training epoch: 42, train loss: 1.99832, val loss: 2.07447\n",
      "Main effects training epoch: 43, train loss: 1.99686, val loss: 2.07381\n",
      "Main effects training epoch: 44, train loss: 1.98219, val loss: 2.05986\n",
      "Main effects training epoch: 45, train loss: 1.98538, val loss: 2.06555\n",
      "Main effects training epoch: 46, train loss: 1.93199, val loss: 2.00777\n",
      "Main effects training epoch: 47, train loss: 1.92633, val loss: 2.00292\n",
      "Main effects training epoch: 48, train loss: 1.93781, val loss: 2.01180\n",
      "Main effects training epoch: 49, train loss: 1.90558, val loss: 1.97629\n",
      "Main effects training epoch: 50, train loss: 1.89436, val loss: 1.97301\n",
      "Main effects training epoch: 51, train loss: 1.88707, val loss: 1.95539\n",
      "Main effects training epoch: 52, train loss: 1.87418, val loss: 1.95203\n",
      "Main effects training epoch: 53, train loss: 1.88074, val loss: 1.95203\n",
      "Main effects training epoch: 54, train loss: 1.84766, val loss: 1.91670\n",
      "Main effects training epoch: 55, train loss: 1.86362, val loss: 1.93316\n",
      "Main effects training epoch: 56, train loss: 1.84504, val loss: 1.91578\n",
      "Main effects training epoch: 57, train loss: 1.83871, val loss: 1.90598\n",
      "Main effects training epoch: 58, train loss: 1.83635, val loss: 1.90794\n",
      "Main effects training epoch: 59, train loss: 1.81987, val loss: 1.88301\n",
      "Main effects training epoch: 60, train loss: 1.81411, val loss: 1.87753\n",
      "Main effects training epoch: 61, train loss: 1.79435, val loss: 1.85428\n",
      "Main effects training epoch: 62, train loss: 1.80737, val loss: 1.87724\n",
      "Main effects training epoch: 63, train loss: 1.78433, val loss: 1.84476\n",
      "Main effects training epoch: 64, train loss: 1.77484, val loss: 1.83380\n",
      "Main effects training epoch: 65, train loss: 1.76469, val loss: 1.82684\n",
      "Main effects training epoch: 66, train loss: 1.75610, val loss: 1.81696\n",
      "Main effects training epoch: 67, train loss: 1.74015, val loss: 1.80136\n",
      "Main effects training epoch: 68, train loss: 1.74855, val loss: 1.80547\n",
      "Main effects training epoch: 69, train loss: 1.73357, val loss: 1.79520\n",
      "Main effects training epoch: 70, train loss: 1.74203, val loss: 1.79823\n",
      "Main effects training epoch: 71, train loss: 1.73003, val loss: 1.79278\n",
      "Main effects training epoch: 72, train loss: 1.72470, val loss: 1.77914\n",
      "Main effects training epoch: 73, train loss: 1.72740, val loss: 1.78702\n",
      "Main effects training epoch: 74, train loss: 1.71997, val loss: 1.77884\n",
      "Main effects training epoch: 75, train loss: 1.72546, val loss: 1.77572\n",
      "Main effects training epoch: 76, train loss: 1.71292, val loss: 1.77190\n",
      "Main effects training epoch: 77, train loss: 1.71444, val loss: 1.77190\n",
      "Main effects training epoch: 78, train loss: 1.71019, val loss: 1.76935\n",
      "Main effects training epoch: 79, train loss: 1.70637, val loss: 1.75924\n",
      "Main effects training epoch: 80, train loss: 1.70559, val loss: 1.76570\n",
      "Main effects training epoch: 81, train loss: 1.70424, val loss: 1.76163\n",
      "Main effects training epoch: 82, train loss: 1.69625, val loss: 1.75276\n",
      "Main effects training epoch: 83, train loss: 1.70362, val loss: 1.76509\n",
      "Main effects training epoch: 84, train loss: 1.69010, val loss: 1.74551\n",
      "Main effects training epoch: 85, train loss: 1.69095, val loss: 1.75028\n",
      "Main effects training epoch: 86, train loss: 1.68340, val loss: 1.74746\n",
      "Main effects training epoch: 87, train loss: 1.68064, val loss: 1.74529\n",
      "Main effects training epoch: 88, train loss: 1.67380, val loss: 1.74022\n",
      "Main effects training epoch: 89, train loss: 1.67360, val loss: 1.74665\n",
      "Main effects training epoch: 90, train loss: 1.65844, val loss: 1.73033\n",
      "Main effects training epoch: 91, train loss: 1.65355, val loss: 1.72703\n",
      "Main effects training epoch: 92, train loss: 1.64214, val loss: 1.71970\n",
      "Main effects training epoch: 93, train loss: 1.64936, val loss: 1.73062\n",
      "Main effects training epoch: 94, train loss: 1.63636, val loss: 1.71888\n",
      "Main effects training epoch: 95, train loss: 1.63488, val loss: 1.72091\n",
      "Main effects training epoch: 96, train loss: 1.63066, val loss: 1.70495\n",
      "Main effects training epoch: 97, train loss: 1.63836, val loss: 1.72648\n",
      "Main effects training epoch: 98, train loss: 1.63448, val loss: 1.71359\n",
      "Main effects training epoch: 99, train loss: 1.62205, val loss: 1.71122\n",
      "Main effects training epoch: 100, train loss: 1.62696, val loss: 1.71339\n",
      "Main effects training epoch: 101, train loss: 1.62468, val loss: 1.70969\n",
      "Main effects training epoch: 102, train loss: 1.62361, val loss: 1.71057\n",
      "Main effects training epoch: 103, train loss: 1.61811, val loss: 1.70932\n",
      "Main effects training epoch: 104, train loss: 1.61437, val loss: 1.69638\n",
      "Main effects training epoch: 105, train loss: 1.61687, val loss: 1.70838\n",
      "Main effects training epoch: 106, train loss: 1.61267, val loss: 1.69917\n",
      "Main effects training epoch: 107, train loss: 1.60933, val loss: 1.69456\n",
      "Main effects training epoch: 108, train loss: 1.60899, val loss: 1.69789\n",
      "Main effects training epoch: 109, train loss: 1.61071, val loss: 1.70213\n",
      "Main effects training epoch: 110, train loss: 1.60834, val loss: 1.69356\n",
      "Main effects training epoch: 111, train loss: 1.60903, val loss: 1.69018\n",
      "Main effects training epoch: 112, train loss: 1.61192, val loss: 1.71417\n",
      "Main effects training epoch: 113, train loss: 1.60321, val loss: 1.68930\n",
      "Main effects training epoch: 114, train loss: 1.60681, val loss: 1.69810\n",
      "Main effects training epoch: 115, train loss: 1.61161, val loss: 1.69664\n",
      "Main effects training epoch: 116, train loss: 1.60462, val loss: 1.69884\n",
      "Main effects training epoch: 117, train loss: 1.60153, val loss: 1.68634\n",
      "Main effects training epoch: 118, train loss: 1.59715, val loss: 1.69219\n",
      "Main effects training epoch: 119, train loss: 1.59567, val loss: 1.68591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 120, train loss: 1.59747, val loss: 1.68979\n",
      "Main effects training epoch: 121, train loss: 1.59275, val loss: 1.68562\n",
      "Main effects training epoch: 122, train loss: 1.58952, val loss: 1.68187\n",
      "Main effects training epoch: 123, train loss: 1.59299, val loss: 1.69392\n",
      "Main effects training epoch: 124, train loss: 1.58823, val loss: 1.67589\n",
      "Main effects training epoch: 125, train loss: 1.58752, val loss: 1.67897\n",
      "Main effects training epoch: 126, train loss: 1.58643, val loss: 1.67941\n",
      "Main effects training epoch: 127, train loss: 1.58719, val loss: 1.68306\n",
      "Main effects training epoch: 128, train loss: 1.58105, val loss: 1.67735\n",
      "Main effects training epoch: 129, train loss: 1.58516, val loss: 1.67262\n",
      "Main effects training epoch: 130, train loss: 1.57827, val loss: 1.67244\n",
      "Main effects training epoch: 131, train loss: 1.57854, val loss: 1.66952\n",
      "Main effects training epoch: 132, train loss: 1.58368, val loss: 1.67841\n",
      "Main effects training epoch: 133, train loss: 1.57787, val loss: 1.66421\n",
      "Main effects training epoch: 134, train loss: 1.57550, val loss: 1.67620\n",
      "Main effects training epoch: 135, train loss: 1.57558, val loss: 1.66555\n",
      "Main effects training epoch: 136, train loss: 1.57565, val loss: 1.66808\n",
      "Main effects training epoch: 137, train loss: 1.57215, val loss: 1.65359\n",
      "Main effects training epoch: 138, train loss: 1.57597, val loss: 1.67812\n",
      "Main effects training epoch: 139, train loss: 1.56594, val loss: 1.65260\n",
      "Main effects training epoch: 140, train loss: 1.56379, val loss: 1.65452\n",
      "Main effects training epoch: 141, train loss: 1.56809, val loss: 1.65819\n",
      "Main effects training epoch: 142, train loss: 1.56959, val loss: 1.67084\n",
      "Main effects training epoch: 143, train loss: 1.56391, val loss: 1.65611\n",
      "Main effects training epoch: 144, train loss: 1.56557, val loss: 1.66064\n",
      "Main effects training epoch: 145, train loss: 1.56139, val loss: 1.65606\n",
      "Main effects training epoch: 146, train loss: 1.55847, val loss: 1.65215\n",
      "Main effects training epoch: 147, train loss: 1.55328, val loss: 1.65161\n",
      "Main effects training epoch: 148, train loss: 1.55521, val loss: 1.65337\n",
      "Main effects training epoch: 149, train loss: 1.55394, val loss: 1.64666\n",
      "Main effects training epoch: 150, train loss: 1.55689, val loss: 1.64452\n",
      "Main effects training epoch: 151, train loss: 1.55483, val loss: 1.64596\n",
      "Main effects training epoch: 152, train loss: 1.55640, val loss: 1.65974\n",
      "Main effects training epoch: 153, train loss: 1.55465, val loss: 1.64418\n",
      "Main effects training epoch: 154, train loss: 1.54827, val loss: 1.64315\n",
      "Main effects training epoch: 155, train loss: 1.54425, val loss: 1.63498\n",
      "Main effects training epoch: 156, train loss: 1.54566, val loss: 1.63837\n",
      "Main effects training epoch: 157, train loss: 1.54784, val loss: 1.64139\n",
      "Main effects training epoch: 158, train loss: 1.55597, val loss: 1.64081\n",
      "Main effects training epoch: 159, train loss: 1.54733, val loss: 1.64121\n",
      "Main effects training epoch: 160, train loss: 1.54370, val loss: 1.64175\n",
      "Main effects training epoch: 161, train loss: 1.54700, val loss: 1.63689\n",
      "Main effects training epoch: 162, train loss: 1.53894, val loss: 1.64314\n",
      "Main effects training epoch: 163, train loss: 1.55071, val loss: 1.63990\n",
      "Main effects training epoch: 164, train loss: 1.53857, val loss: 1.62758\n",
      "Main effects training epoch: 165, train loss: 1.53582, val loss: 1.63364\n",
      "Main effects training epoch: 166, train loss: 1.53784, val loss: 1.62337\n",
      "Main effects training epoch: 167, train loss: 1.53668, val loss: 1.61628\n",
      "Main effects training epoch: 168, train loss: 1.54187, val loss: 1.64968\n",
      "Main effects training epoch: 169, train loss: 1.53233, val loss: 1.61626\n",
      "Main effects training epoch: 170, train loss: 1.53301, val loss: 1.62002\n",
      "Main effects training epoch: 171, train loss: 1.53462, val loss: 1.63271\n",
      "Main effects training epoch: 172, train loss: 1.53059, val loss: 1.62420\n",
      "Main effects training epoch: 173, train loss: 1.52675, val loss: 1.61587\n",
      "Main effects training epoch: 174, train loss: 1.52574, val loss: 1.61472\n",
      "Main effects training epoch: 175, train loss: 1.52522, val loss: 1.61609\n",
      "Main effects training epoch: 176, train loss: 1.52517, val loss: 1.61189\n",
      "Main effects training epoch: 177, train loss: 1.52300, val loss: 1.61130\n",
      "Main effects training epoch: 178, train loss: 1.52251, val loss: 1.61256\n",
      "Main effects training epoch: 179, train loss: 1.52604, val loss: 1.62370\n",
      "Main effects training epoch: 180, train loss: 1.51909, val loss: 1.60909\n",
      "Main effects training epoch: 181, train loss: 1.52000, val loss: 1.60844\n",
      "Main effects training epoch: 182, train loss: 1.52126, val loss: 1.60803\n",
      "Main effects training epoch: 183, train loss: 1.51478, val loss: 1.60948\n",
      "Main effects training epoch: 184, train loss: 1.52037, val loss: 1.60605\n",
      "Main effects training epoch: 185, train loss: 1.51414, val loss: 1.59444\n",
      "Main effects training epoch: 186, train loss: 1.51951, val loss: 1.61258\n",
      "Main effects training epoch: 187, train loss: 1.52691, val loss: 1.60462\n",
      "Main effects training epoch: 188, train loss: 1.51070, val loss: 1.60830\n",
      "Main effects training epoch: 189, train loss: 1.50982, val loss: 1.59836\n",
      "Main effects training epoch: 190, train loss: 1.51110, val loss: 1.59181\n",
      "Main effects training epoch: 191, train loss: 1.50860, val loss: 1.60462\n",
      "Main effects training epoch: 192, train loss: 1.51019, val loss: 1.59339\n",
      "Main effects training epoch: 193, train loss: 1.50871, val loss: 1.59399\n",
      "Main effects training epoch: 194, train loss: 1.51010, val loss: 1.59995\n",
      "Main effects training epoch: 195, train loss: 1.50732, val loss: 1.58955\n",
      "Main effects training epoch: 196, train loss: 1.51943, val loss: 1.59980\n",
      "Main effects training epoch: 197, train loss: 1.52664, val loss: 1.61026\n",
      "Main effects training epoch: 198, train loss: 1.52697, val loss: 1.60535\n",
      "Main effects training epoch: 199, train loss: 1.51058, val loss: 1.59071\n",
      "Main effects training epoch: 200, train loss: 1.50977, val loss: 1.59562\n",
      "Main effects training epoch: 201, train loss: 1.50855, val loss: 1.58794\n",
      "Main effects training epoch: 202, train loss: 1.50613, val loss: 1.59355\n",
      "Main effects training epoch: 203, train loss: 1.50374, val loss: 1.58254\n",
      "Main effects training epoch: 204, train loss: 1.50417, val loss: 1.59863\n",
      "Main effects training epoch: 205, train loss: 1.49828, val loss: 1.57278\n",
      "Main effects training epoch: 206, train loss: 1.50065, val loss: 1.59706\n",
      "Main effects training epoch: 207, train loss: 1.50225, val loss: 1.57431\n",
      "Main effects training epoch: 208, train loss: 1.50469, val loss: 1.59505\n",
      "Main effects training epoch: 209, train loss: 1.49897, val loss: 1.58897\n",
      "Main effects training epoch: 210, train loss: 1.50055, val loss: 1.57327\n",
      "Main effects training epoch: 211, train loss: 1.51061, val loss: 1.59110\n",
      "Main effects training epoch: 212, train loss: 1.50414, val loss: 1.57268\n",
      "Main effects training epoch: 213, train loss: 1.50270, val loss: 1.59163\n",
      "Main effects training epoch: 214, train loss: 1.49817, val loss: 1.57566\n",
      "Main effects training epoch: 215, train loss: 1.49571, val loss: 1.57164\n",
      "Main effects training epoch: 216, train loss: 1.49560, val loss: 1.57660\n",
      "Main effects training epoch: 217, train loss: 1.49150, val loss: 1.57137\n",
      "Main effects training epoch: 218, train loss: 1.49647, val loss: 1.58332\n",
      "Main effects training epoch: 219, train loss: 1.49084, val loss: 1.56828\n",
      "Main effects training epoch: 220, train loss: 1.49340, val loss: 1.57736\n",
      "Main effects training epoch: 221, train loss: 1.50009, val loss: 1.56919\n",
      "Main effects training epoch: 222, train loss: 1.49716, val loss: 1.58686\n",
      "Main effects training epoch: 223, train loss: 1.49415, val loss: 1.56762\n",
      "Main effects training epoch: 224, train loss: 1.49731, val loss: 1.57543\n",
      "Main effects training epoch: 225, train loss: 1.49069, val loss: 1.56735\n",
      "Main effects training epoch: 226, train loss: 1.48744, val loss: 1.56514\n",
      "Main effects training epoch: 227, train loss: 1.48606, val loss: 1.56522\n",
      "Main effects training epoch: 228, train loss: 1.48761, val loss: 1.57127\n",
      "Main effects training epoch: 229, train loss: 1.48698, val loss: 1.56841\n",
      "Main effects training epoch: 230, train loss: 1.49084, val loss: 1.57460\n",
      "Main effects training epoch: 231, train loss: 1.48533, val loss: 1.56179\n",
      "Main effects training epoch: 232, train loss: 1.48691, val loss: 1.56537\n",
      "Main effects training epoch: 233, train loss: 1.49704, val loss: 1.57297\n",
      "Main effects training epoch: 234, train loss: 1.48505, val loss: 1.57149\n",
      "Main effects training epoch: 235, train loss: 1.49092, val loss: 1.56129\n",
      "Main effects training epoch: 236, train loss: 1.48097, val loss: 1.56570\n",
      "Main effects training epoch: 237, train loss: 1.48239, val loss: 1.55212\n",
      "Main effects training epoch: 238, train loss: 1.48273, val loss: 1.55997\n",
      "Main effects training epoch: 239, train loss: 1.48013, val loss: 1.55569\n",
      "Main effects training epoch: 240, train loss: 1.48035, val loss: 1.56105\n",
      "Main effects training epoch: 241, train loss: 1.48436, val loss: 1.55253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 242, train loss: 1.48507, val loss: 1.55589\n",
      "Main effects training epoch: 243, train loss: 1.48884, val loss: 1.56125\n",
      "Main effects training epoch: 244, train loss: 1.48401, val loss: 1.57373\n",
      "Main effects training epoch: 245, train loss: 1.49051, val loss: 1.55337\n",
      "Main effects training epoch: 246, train loss: 1.48160, val loss: 1.56023\n",
      "Main effects training epoch: 247, train loss: 1.47777, val loss: 1.55312\n",
      "Main effects training epoch: 248, train loss: 1.49156, val loss: 1.55815\n",
      "Main effects training epoch: 249, train loss: 1.48098, val loss: 1.55937\n",
      "Main effects training epoch: 250, train loss: 1.48385, val loss: 1.56199\n",
      "Main effects training epoch: 251, train loss: 1.48659, val loss: 1.56128\n",
      "Main effects training epoch: 252, train loss: 1.47685, val loss: 1.56157\n",
      "Main effects training epoch: 253, train loss: 1.48808, val loss: 1.54859\n",
      "Main effects training epoch: 254, train loss: 1.48407, val loss: 1.55806\n",
      "Main effects training epoch: 255, train loss: 1.48563, val loss: 1.55742\n",
      "Main effects training epoch: 256, train loss: 1.47918, val loss: 1.54952\n",
      "Main effects training epoch: 257, train loss: 1.47606, val loss: 1.55121\n",
      "Main effects training epoch: 258, train loss: 1.47372, val loss: 1.54436\n",
      "Main effects training epoch: 259, train loss: 1.47893, val loss: 1.55576\n",
      "Main effects training epoch: 260, train loss: 1.47852, val loss: 1.54907\n",
      "Main effects training epoch: 261, train loss: 1.49550, val loss: 1.55619\n",
      "Main effects training epoch: 262, train loss: 1.48585, val loss: 1.56521\n",
      "Main effects training epoch: 263, train loss: 1.47815, val loss: 1.52881\n",
      "Main effects training epoch: 264, train loss: 1.46899, val loss: 1.55029\n",
      "Main effects training epoch: 265, train loss: 1.46937, val loss: 1.54180\n",
      "Main effects training epoch: 266, train loss: 1.47614, val loss: 1.54285\n",
      "Main effects training epoch: 267, train loss: 1.46799, val loss: 1.53840\n",
      "Main effects training epoch: 268, train loss: 1.46506, val loss: 1.53744\n",
      "Main effects training epoch: 269, train loss: 1.46442, val loss: 1.53535\n",
      "Main effects training epoch: 270, train loss: 1.46777, val loss: 1.52940\n",
      "Main effects training epoch: 271, train loss: 1.46353, val loss: 1.53767\n",
      "Main effects training epoch: 272, train loss: 1.47036, val loss: 1.52646\n",
      "Main effects training epoch: 273, train loss: 1.46460, val loss: 1.53165\n",
      "Main effects training epoch: 274, train loss: 1.46643, val loss: 1.51927\n",
      "Main effects training epoch: 275, train loss: 1.46682, val loss: 1.54881\n",
      "Main effects training epoch: 276, train loss: 1.47766, val loss: 1.54330\n",
      "Main effects training epoch: 277, train loss: 1.46014, val loss: 1.53089\n",
      "Main effects training epoch: 278, train loss: 1.46303, val loss: 1.53656\n",
      "Main effects training epoch: 279, train loss: 1.46443, val loss: 1.52143\n",
      "Main effects training epoch: 280, train loss: 1.46422, val loss: 1.54040\n",
      "Main effects training epoch: 281, train loss: 1.45312, val loss: 1.51418\n",
      "Main effects training epoch: 282, train loss: 1.45666, val loss: 1.52099\n",
      "Main effects training epoch: 283, train loss: 1.45174, val loss: 1.51592\n",
      "Main effects training epoch: 284, train loss: 1.46646, val loss: 1.51831\n",
      "Main effects training epoch: 285, train loss: 1.45664, val loss: 1.51366\n",
      "Main effects training epoch: 286, train loss: 1.45831, val loss: 1.50973\n",
      "Main effects training epoch: 287, train loss: 1.46764, val loss: 1.54868\n",
      "Main effects training epoch: 288, train loss: 1.46280, val loss: 1.50655\n",
      "Main effects training epoch: 289, train loss: 1.45112, val loss: 1.52111\n",
      "Main effects training epoch: 290, train loss: 1.45658, val loss: 1.51546\n",
      "Main effects training epoch: 291, train loss: 1.45599, val loss: 1.51696\n",
      "Main effects training epoch: 292, train loss: 1.44961, val loss: 1.51482\n",
      "Main effects training epoch: 293, train loss: 1.46067, val loss: 1.51332\n",
      "Main effects training epoch: 294, train loss: 1.47385, val loss: 1.51202\n",
      "Main effects training epoch: 295, train loss: 1.45615, val loss: 1.52349\n",
      "Main effects training epoch: 296, train loss: 1.47318, val loss: 1.53020\n",
      "Main effects training epoch: 297, train loss: 1.46001, val loss: 1.50907\n",
      "Main effects training epoch: 298, train loss: 1.47136, val loss: 1.51133\n",
      "Main effects training epoch: 299, train loss: 1.46982, val loss: 1.53516\n",
      "Main effects training epoch: 300, train loss: 1.44960, val loss: 1.50548\n",
      "##########Stage 1: main effect training stop.##########\n",
      "3 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.47981, val loss: 1.52267\n",
      "Main effects tuning epoch: 2, train loss: 1.46331, val loss: 1.51472\n",
      "Main effects tuning epoch: 3, train loss: 1.46830, val loss: 1.51161\n",
      "Main effects tuning epoch: 4, train loss: 1.46896, val loss: 1.51178\n",
      "Main effects tuning epoch: 5, train loss: 1.47528, val loss: 1.51366\n",
      "Main effects tuning epoch: 6, train loss: 1.46808, val loss: 1.51692\n",
      "Main effects tuning epoch: 7, train loss: 1.46537, val loss: 1.51874\n",
      "Main effects tuning epoch: 8, train loss: 1.47975, val loss: 1.50987\n",
      "Main effects tuning epoch: 9, train loss: 1.46065, val loss: 1.51071\n",
      "Main effects tuning epoch: 10, train loss: 1.46917, val loss: 1.50453\n",
      "Main effects tuning epoch: 11, train loss: 1.47333, val loss: 1.50563\n",
      "Main effects tuning epoch: 12, train loss: 1.46157, val loss: 1.50754\n",
      "Main effects tuning epoch: 13, train loss: 1.48088, val loss: 1.52535\n",
      "Main effects tuning epoch: 14, train loss: 1.46625, val loss: 1.51370\n",
      "Main effects tuning epoch: 15, train loss: 1.46376, val loss: 1.51038\n",
      "Main effects tuning epoch: 16, train loss: 1.47410, val loss: 1.50254\n",
      "Main effects tuning epoch: 17, train loss: 1.46502, val loss: 1.52004\n",
      "Main effects tuning epoch: 18, train loss: 1.46385, val loss: 1.49553\n",
      "Main effects tuning epoch: 19, train loss: 1.46368, val loss: 1.50912\n",
      "Main effects tuning epoch: 20, train loss: 1.46234, val loss: 1.50258\n",
      "Main effects tuning epoch: 21, train loss: 1.46433, val loss: 1.49670\n",
      "Main effects tuning epoch: 22, train loss: 1.46370, val loss: 1.49430\n",
      "Main effects tuning epoch: 23, train loss: 1.46845, val loss: 1.49861\n",
      "Main effects tuning epoch: 24, train loss: 1.46385, val loss: 1.50414\n",
      "Main effects tuning epoch: 25, train loss: 1.45635, val loss: 1.50218\n",
      "Main effects tuning epoch: 26, train loss: 1.45837, val loss: 1.49553\n",
      "Main effects tuning epoch: 27, train loss: 1.46347, val loss: 1.50165\n",
      "Main effects tuning epoch: 28, train loss: 1.46517, val loss: 1.49487\n",
      "Main effects tuning epoch: 29, train loss: 1.45550, val loss: 1.49383\n",
      "Main effects tuning epoch: 30, train loss: 1.45760, val loss: 1.50101\n",
      "Main effects tuning epoch: 31, train loss: 1.47132, val loss: 1.49669\n",
      "Main effects tuning epoch: 32, train loss: 1.45995, val loss: 1.49988\n",
      "Main effects tuning epoch: 33, train loss: 1.47518, val loss: 1.50785\n",
      "Main effects tuning epoch: 34, train loss: 1.45653, val loss: 1.49390\n",
      "Main effects tuning epoch: 35, train loss: 1.46586, val loss: 1.48579\n",
      "Main effects tuning epoch: 36, train loss: 1.45584, val loss: 1.50565\n",
      "Main effects tuning epoch: 37, train loss: 1.45403, val loss: 1.48898\n",
      "Main effects tuning epoch: 38, train loss: 1.45523, val loss: 1.49846\n",
      "Main effects tuning epoch: 39, train loss: 1.45269, val loss: 1.48773\n",
      "Main effects tuning epoch: 40, train loss: 1.45165, val loss: 1.49717\n",
      "Main effects tuning epoch: 41, train loss: 1.45082, val loss: 1.49373\n",
      "Main effects tuning epoch: 42, train loss: 1.45648, val loss: 1.49967\n",
      "Main effects tuning epoch: 43, train loss: 1.45135, val loss: 1.48556\n",
      "Main effects tuning epoch: 44, train loss: 1.45106, val loss: 1.48879\n",
      "Main effects tuning epoch: 45, train loss: 1.45430, val loss: 1.49438\n",
      "Main effects tuning epoch: 46, train loss: 1.44805, val loss: 1.47819\n",
      "Main effects tuning epoch: 47, train loss: 1.45053, val loss: 1.49323\n",
      "Main effects tuning epoch: 48, train loss: 1.45437, val loss: 1.49447\n",
      "Main effects tuning epoch: 49, train loss: 1.46905, val loss: 1.50304\n",
      "Main effects tuning epoch: 50, train loss: 1.45347, val loss: 1.49706\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.38751, val loss: 1.42972\n",
      "Interaction training epoch: 2, train loss: 1.29788, val loss: 1.29251\n",
      "Interaction training epoch: 3, train loss: 1.29616, val loss: 1.32320\n",
      "Interaction training epoch: 4, train loss: 1.04085, val loss: 1.05417\n",
      "Interaction training epoch: 5, train loss: 1.03009, val loss: 1.03709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 6, train loss: 1.05054, val loss: 1.05218\n",
      "Interaction training epoch: 7, train loss: 1.02948, val loss: 1.02686\n",
      "Interaction training epoch: 8, train loss: 0.99902, val loss: 1.00530\n",
      "Interaction training epoch: 9, train loss: 0.98387, val loss: 0.98691\n",
      "Interaction training epoch: 10, train loss: 0.99080, val loss: 1.01337\n",
      "Interaction training epoch: 11, train loss: 0.96049, val loss: 0.96233\n",
      "Interaction training epoch: 12, train loss: 0.94945, val loss: 0.94754\n",
      "Interaction training epoch: 13, train loss: 0.95982, val loss: 0.96210\n",
      "Interaction training epoch: 14, train loss: 0.95414, val loss: 0.94884\n",
      "Interaction training epoch: 15, train loss: 0.94345, val loss: 0.94976\n",
      "Interaction training epoch: 16, train loss: 0.93793, val loss: 0.92820\n",
      "Interaction training epoch: 17, train loss: 0.93178, val loss: 0.93546\n",
      "Interaction training epoch: 18, train loss: 0.91425, val loss: 0.89675\n",
      "Interaction training epoch: 19, train loss: 0.91632, val loss: 0.92731\n",
      "Interaction training epoch: 20, train loss: 0.92236, val loss: 0.91942\n",
      "Interaction training epoch: 21, train loss: 0.90920, val loss: 0.90217\n",
      "Interaction training epoch: 22, train loss: 0.90385, val loss: 0.89343\n",
      "Interaction training epoch: 23, train loss: 0.89987, val loss: 0.89575\n",
      "Interaction training epoch: 24, train loss: 0.90721, val loss: 0.89862\n",
      "Interaction training epoch: 25, train loss: 0.92542, val loss: 0.91327\n",
      "Interaction training epoch: 26, train loss: 0.90072, val loss: 0.91308\n",
      "Interaction training epoch: 27, train loss: 0.90686, val loss: 0.91623\n",
      "Interaction training epoch: 28, train loss: 0.89684, val loss: 0.88755\n",
      "Interaction training epoch: 29, train loss: 0.89574, val loss: 0.88784\n",
      "Interaction training epoch: 30, train loss: 0.89629, val loss: 0.90104\n",
      "Interaction training epoch: 31, train loss: 0.88658, val loss: 0.88759\n",
      "Interaction training epoch: 32, train loss: 0.89231, val loss: 0.87914\n",
      "Interaction training epoch: 33, train loss: 0.88231, val loss: 0.88828\n",
      "Interaction training epoch: 34, train loss: 0.88436, val loss: 0.87844\n",
      "Interaction training epoch: 35, train loss: 0.89044, val loss: 0.88305\n",
      "Interaction training epoch: 36, train loss: 0.88164, val loss: 0.88330\n",
      "Interaction training epoch: 37, train loss: 0.87723, val loss: 0.87402\n",
      "Interaction training epoch: 38, train loss: 0.87424, val loss: 0.87164\n",
      "Interaction training epoch: 39, train loss: 0.88113, val loss: 0.88409\n",
      "Interaction training epoch: 40, train loss: 0.87380, val loss: 0.86950\n",
      "Interaction training epoch: 41, train loss: 0.87787, val loss: 0.87971\n",
      "Interaction training epoch: 42, train loss: 0.87474, val loss: 0.86993\n",
      "Interaction training epoch: 43, train loss: 0.87210, val loss: 0.86398\n",
      "Interaction training epoch: 44, train loss: 0.86534, val loss: 0.86351\n",
      "Interaction training epoch: 45, train loss: 0.87172, val loss: 0.86373\n",
      "Interaction training epoch: 46, train loss: 0.86930, val loss: 0.86788\n",
      "Interaction training epoch: 47, train loss: 0.87310, val loss: 0.86793\n",
      "Interaction training epoch: 48, train loss: 0.87626, val loss: 0.87352\n",
      "Interaction training epoch: 49, train loss: 0.86374, val loss: 0.86011\n",
      "Interaction training epoch: 50, train loss: 0.86237, val loss: 0.86165\n",
      "Interaction training epoch: 51, train loss: 0.85959, val loss: 0.85691\n",
      "Interaction training epoch: 52, train loss: 0.87314, val loss: 0.87168\n",
      "Interaction training epoch: 53, train loss: 0.86557, val loss: 0.85709\n",
      "Interaction training epoch: 54, train loss: 0.88728, val loss: 0.89249\n",
      "Interaction training epoch: 55, train loss: 0.86669, val loss: 0.86887\n",
      "Interaction training epoch: 56, train loss: 0.87323, val loss: 0.87294\n",
      "Interaction training epoch: 57, train loss: 0.85961, val loss: 0.85659\n",
      "Interaction training epoch: 58, train loss: 0.86651, val loss: 0.86275\n",
      "Interaction training epoch: 59, train loss: 0.86838, val loss: 0.87083\n",
      "Interaction training epoch: 60, train loss: 0.87023, val loss: 0.86088\n",
      "Interaction training epoch: 61, train loss: 0.85704, val loss: 0.85348\n",
      "Interaction training epoch: 62, train loss: 0.85447, val loss: 0.85588\n",
      "Interaction training epoch: 63, train loss: 0.85908, val loss: 0.85408\n",
      "Interaction training epoch: 64, train loss: 0.86018, val loss: 0.86011\n",
      "Interaction training epoch: 65, train loss: 0.85534, val loss: 0.84887\n",
      "Interaction training epoch: 66, train loss: 0.85576, val loss: 0.85693\n",
      "Interaction training epoch: 67, train loss: 0.85598, val loss: 0.84631\n",
      "Interaction training epoch: 68, train loss: 0.85123, val loss: 0.85383\n",
      "Interaction training epoch: 69, train loss: 0.84728, val loss: 0.84808\n",
      "Interaction training epoch: 70, train loss: 0.85633, val loss: 0.85398\n",
      "Interaction training epoch: 71, train loss: 0.86387, val loss: 0.85482\n",
      "Interaction training epoch: 72, train loss: 0.86148, val loss: 0.86960\n",
      "Interaction training epoch: 73, train loss: 0.85043, val loss: 0.84455\n",
      "Interaction training epoch: 74, train loss: 0.85069, val loss: 0.85530\n",
      "Interaction training epoch: 75, train loss: 0.84891, val loss: 0.84545\n",
      "Interaction training epoch: 76, train loss: 0.86129, val loss: 0.86666\n",
      "Interaction training epoch: 77, train loss: 0.84883, val loss: 0.84464\n",
      "Interaction training epoch: 78, train loss: 0.84876, val loss: 0.85347\n",
      "Interaction training epoch: 79, train loss: 0.84878, val loss: 0.84226\n",
      "Interaction training epoch: 80, train loss: 0.85118, val loss: 0.85842\n",
      "Interaction training epoch: 81, train loss: 0.85010, val loss: 0.84725\n",
      "Interaction training epoch: 82, train loss: 0.85496, val loss: 0.85152\n",
      "Interaction training epoch: 83, train loss: 0.84734, val loss: 0.84694\n",
      "Interaction training epoch: 84, train loss: 0.83982, val loss: 0.83801\n",
      "Interaction training epoch: 85, train loss: 0.85011, val loss: 0.84656\n",
      "Interaction training epoch: 86, train loss: 0.85688, val loss: 0.84358\n",
      "Interaction training epoch: 87, train loss: 0.86353, val loss: 0.86145\n",
      "Interaction training epoch: 88, train loss: 0.85386, val loss: 0.84938\n",
      "Interaction training epoch: 89, train loss: 0.84869, val loss: 0.85860\n",
      "Interaction training epoch: 90, train loss: 0.85809, val loss: 0.85626\n",
      "Interaction training epoch: 91, train loss: 0.84737, val loss: 0.84347\n",
      "Interaction training epoch: 92, train loss: 0.88299, val loss: 0.87539\n",
      "Interaction training epoch: 93, train loss: 0.85565, val loss: 0.84627\n",
      "Interaction training epoch: 94, train loss: 0.86188, val loss: 0.87089\n",
      "Interaction training epoch: 95, train loss: 0.86285, val loss: 0.85021\n",
      "Interaction training epoch: 96, train loss: 0.85232, val loss: 0.85564\n",
      "Interaction training epoch: 97, train loss: 0.84748, val loss: 0.84804\n",
      "Interaction training epoch: 98, train loss: 0.84486, val loss: 0.84667\n",
      "Interaction training epoch: 99, train loss: 0.85361, val loss: 0.85675\n",
      "Interaction training epoch: 100, train loss: 0.84448, val loss: 0.84568\n",
      "Interaction training epoch: 101, train loss: 0.84034, val loss: 0.84930\n",
      "Interaction training epoch: 102, train loss: 0.84522, val loss: 0.84170\n",
      "Interaction training epoch: 103, train loss: 0.84307, val loss: 0.84549\n",
      "Interaction training epoch: 104, train loss: 0.84887, val loss: 0.84818\n",
      "Interaction training epoch: 105, train loss: 0.84111, val loss: 0.84438\n",
      "Interaction training epoch: 106, train loss: 0.84481, val loss: 0.85072\n",
      "Interaction training epoch: 107, train loss: 0.83311, val loss: 0.83184\n",
      "Interaction training epoch: 108, train loss: 0.84402, val loss: 0.83933\n",
      "Interaction training epoch: 109, train loss: 0.84446, val loss: 0.84765\n",
      "Interaction training epoch: 110, train loss: 0.84550, val loss: 0.83716\n",
      "Interaction training epoch: 111, train loss: 0.84113, val loss: 0.85107\n",
      "Interaction training epoch: 112, train loss: 0.83914, val loss: 0.84731\n",
      "Interaction training epoch: 113, train loss: 0.83822, val loss: 0.83572\n",
      "Interaction training epoch: 114, train loss: 0.84078, val loss: 0.85020\n",
      "Interaction training epoch: 115, train loss: 0.83684, val loss: 0.84174\n",
      "Interaction training epoch: 116, train loss: 0.84198, val loss: 0.84449\n",
      "Interaction training epoch: 117, train loss: 0.84988, val loss: 0.85086\n",
      "Interaction training epoch: 118, train loss: 0.84723, val loss: 0.84895\n",
      "Interaction training epoch: 119, train loss: 0.84825, val loss: 0.85438\n",
      "Interaction training epoch: 120, train loss: 0.83353, val loss: 0.84249\n",
      "Interaction training epoch: 121, train loss: 0.84568, val loss: 0.84169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 122, train loss: 0.84632, val loss: 0.84663\n",
      "Interaction training epoch: 123, train loss: 0.84193, val loss: 0.84624\n",
      "Interaction training epoch: 124, train loss: 0.82580, val loss: 0.82487\n",
      "Interaction training epoch: 125, train loss: 0.84564, val loss: 0.85498\n",
      "Interaction training epoch: 126, train loss: 0.84185, val loss: 0.85316\n",
      "Interaction training epoch: 127, train loss: 0.82878, val loss: 0.83275\n",
      "Interaction training epoch: 128, train loss: 0.82934, val loss: 0.83971\n",
      "Interaction training epoch: 129, train loss: 0.84837, val loss: 0.85086\n",
      "Interaction training epoch: 130, train loss: 0.83089, val loss: 0.84414\n",
      "Interaction training epoch: 131, train loss: 0.83600, val loss: 0.84247\n",
      "Interaction training epoch: 132, train loss: 0.83097, val loss: 0.83826\n",
      "Interaction training epoch: 133, train loss: 0.83711, val loss: 0.84705\n",
      "Interaction training epoch: 134, train loss: 0.83164, val loss: 0.83379\n",
      "Interaction training epoch: 135, train loss: 0.82984, val loss: 0.83505\n",
      "Interaction training epoch: 136, train loss: 0.83540, val loss: 0.82933\n",
      "Interaction training epoch: 137, train loss: 0.83534, val loss: 0.84802\n",
      "Interaction training epoch: 138, train loss: 0.84032, val loss: 0.84233\n",
      "Interaction training epoch: 139, train loss: 0.82736, val loss: 0.83227\n",
      "Interaction training epoch: 140, train loss: 0.82745, val loss: 0.83496\n",
      "Interaction training epoch: 141, train loss: 0.82917, val loss: 0.83139\n",
      "Interaction training epoch: 142, train loss: 0.83277, val loss: 0.83556\n",
      "Interaction training epoch: 143, train loss: 0.83873, val loss: 0.83588\n",
      "Interaction training epoch: 144, train loss: 0.83220, val loss: 0.84184\n",
      "Interaction training epoch: 145, train loss: 0.83137, val loss: 0.83206\n",
      "Interaction training epoch: 146, train loss: 0.83064, val loss: 0.83581\n",
      "Interaction training epoch: 147, train loss: 0.83673, val loss: 0.84164\n",
      "Interaction training epoch: 148, train loss: 0.83926, val loss: 0.83894\n",
      "Interaction training epoch: 149, train loss: 0.82296, val loss: 0.82527\n",
      "Interaction training epoch: 150, train loss: 0.83546, val loss: 0.83520\n",
      "Interaction training epoch: 151, train loss: 0.83130, val loss: 0.83714\n",
      "Interaction training epoch: 152, train loss: 0.83576, val loss: 0.84786\n",
      "Interaction training epoch: 153, train loss: 0.84551, val loss: 0.85060\n",
      "Interaction training epoch: 154, train loss: 0.83219, val loss: 0.82960\n",
      "Interaction training epoch: 155, train loss: 0.83138, val loss: 0.84068\n",
      "Interaction training epoch: 156, train loss: 0.82575, val loss: 0.82219\n",
      "Interaction training epoch: 157, train loss: 0.83437, val loss: 0.83265\n",
      "Interaction training epoch: 158, train loss: 0.83791, val loss: 0.84165\n",
      "Interaction training epoch: 159, train loss: 0.82527, val loss: 0.81928\n",
      "Interaction training epoch: 160, train loss: 0.84681, val loss: 0.84812\n",
      "Interaction training epoch: 161, train loss: 0.84271, val loss: 0.83922\n",
      "Interaction training epoch: 162, train loss: 0.82548, val loss: 0.83312\n",
      "Interaction training epoch: 163, train loss: 0.84106, val loss: 0.84076\n",
      "Interaction training epoch: 164, train loss: 0.83649, val loss: 0.84125\n",
      "Interaction training epoch: 165, train loss: 0.83301, val loss: 0.83403\n",
      "Interaction training epoch: 166, train loss: 0.82190, val loss: 0.81532\n",
      "Interaction training epoch: 167, train loss: 0.82201, val loss: 0.82740\n",
      "Interaction training epoch: 168, train loss: 0.82725, val loss: 0.82684\n",
      "Interaction training epoch: 169, train loss: 0.82403, val loss: 0.82240\n",
      "Interaction training epoch: 170, train loss: 0.82601, val loss: 0.82373\n",
      "Interaction training epoch: 171, train loss: 0.82168, val loss: 0.83330\n",
      "Interaction training epoch: 172, train loss: 0.83113, val loss: 0.82711\n",
      "Interaction training epoch: 173, train loss: 0.82023, val loss: 0.81578\n",
      "Interaction training epoch: 174, train loss: 0.82378, val loss: 0.82676\n",
      "Interaction training epoch: 175, train loss: 0.82887, val loss: 0.83708\n",
      "Interaction training epoch: 176, train loss: 0.82882, val loss: 0.83049\n",
      "Interaction training epoch: 177, train loss: 0.83148, val loss: 0.84407\n",
      "Interaction training epoch: 178, train loss: 0.82600, val loss: 0.82280\n",
      "Interaction training epoch: 179, train loss: 0.83135, val loss: 0.83668\n",
      "Interaction training epoch: 180, train loss: 0.83581, val loss: 0.84194\n",
      "Interaction training epoch: 181, train loss: 0.82044, val loss: 0.82287\n",
      "Interaction training epoch: 182, train loss: 0.82135, val loss: 0.82401\n",
      "Interaction training epoch: 183, train loss: 0.82940, val loss: 0.82729\n",
      "Interaction training epoch: 184, train loss: 0.82159, val loss: 0.82257\n",
      "Interaction training epoch: 185, train loss: 0.82134, val loss: 0.81587\n",
      "Interaction training epoch: 186, train loss: 0.82022, val loss: 0.81890\n",
      "Interaction training epoch: 187, train loss: 0.82263, val loss: 0.82265\n",
      "Interaction training epoch: 188, train loss: 0.82953, val loss: 0.83362\n",
      "Interaction training epoch: 189, train loss: 0.83232, val loss: 0.83031\n",
      "Interaction training epoch: 190, train loss: 0.81332, val loss: 0.81501\n",
      "Interaction training epoch: 191, train loss: 0.81952, val loss: 0.82189\n",
      "Interaction training epoch: 192, train loss: 0.82166, val loss: 0.82530\n",
      "Interaction training epoch: 193, train loss: 0.83151, val loss: 0.84141\n",
      "Interaction training epoch: 194, train loss: 0.82638, val loss: 0.81517\n",
      "Interaction training epoch: 195, train loss: 0.83199, val loss: 0.82848\n",
      "Interaction training epoch: 196, train loss: 0.82739, val loss: 0.83803\n",
      "Interaction training epoch: 197, train loss: 0.82264, val loss: 0.82194\n",
      "Interaction training epoch: 198, train loss: 0.82310, val loss: 0.83420\n",
      "Interaction training epoch: 199, train loss: 0.81675, val loss: 0.81584\n",
      "Interaction training epoch: 200, train loss: 0.83348, val loss: 0.82797\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########1 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.82371, val loss: 0.82202\n",
      "Interaction tuning epoch: 2, train loss: 0.82096, val loss: 0.82463\n",
      "Interaction tuning epoch: 3, train loss: 0.81892, val loss: 0.82484\n",
      "Interaction tuning epoch: 4, train loss: 0.82479, val loss: 0.82251\n",
      "Interaction tuning epoch: 5, train loss: 0.81839, val loss: 0.82672\n",
      "Interaction tuning epoch: 6, train loss: 0.81689, val loss: 0.81462\n",
      "Interaction tuning epoch: 7, train loss: 0.81898, val loss: 0.82051\n",
      "Interaction tuning epoch: 8, train loss: 0.81312, val loss: 0.81754\n",
      "Interaction tuning epoch: 9, train loss: 0.81539, val loss: 0.80975\n",
      "Interaction tuning epoch: 10, train loss: 0.81745, val loss: 0.82210\n",
      "Interaction tuning epoch: 11, train loss: 0.81418, val loss: 0.82079\n",
      "Interaction tuning epoch: 12, train loss: 0.82485, val loss: 0.82715\n",
      "Interaction tuning epoch: 13, train loss: 0.81825, val loss: 0.81712\n",
      "Interaction tuning epoch: 14, train loss: 0.82252, val loss: 0.82478\n",
      "Interaction tuning epoch: 15, train loss: 0.82245, val loss: 0.82111\n",
      "Interaction tuning epoch: 16, train loss: 0.81510, val loss: 0.81301\n",
      "Interaction tuning epoch: 17, train loss: 0.84084, val loss: 0.84370\n",
      "Interaction tuning epoch: 18, train loss: 0.82361, val loss: 0.83073\n",
      "Interaction tuning epoch: 19, train loss: 0.81988, val loss: 0.81411\n",
      "Interaction tuning epoch: 20, train loss: 0.82053, val loss: 0.82244\n",
      "Interaction tuning epoch: 21, train loss: 0.81551, val loss: 0.81523\n",
      "Interaction tuning epoch: 22, train loss: 0.82296, val loss: 0.82491\n",
      "Interaction tuning epoch: 23, train loss: 0.81978, val loss: 0.81538\n",
      "Interaction tuning epoch: 24, train loss: 0.81054, val loss: 0.81135\n",
      "Interaction tuning epoch: 25, train loss: 0.81548, val loss: 0.81068\n",
      "Interaction tuning epoch: 26, train loss: 0.81618, val loss: 0.81822\n",
      "Interaction tuning epoch: 27, train loss: 0.80571, val loss: 0.81150\n",
      "Interaction tuning epoch: 28, train loss: 0.81969, val loss: 0.82267\n",
      "Interaction tuning epoch: 29, train loss: 0.81576, val loss: 0.80756\n",
      "Interaction tuning epoch: 30, train loss: 0.81227, val loss: 0.81788\n",
      "Interaction tuning epoch: 31, train loss: 0.82466, val loss: 0.81669\n",
      "Interaction tuning epoch: 32, train loss: 0.82289, val loss: 0.82492\n",
      "Interaction tuning epoch: 33, train loss: 0.81647, val loss: 0.81767\n",
      "Interaction tuning epoch: 34, train loss: 0.82142, val loss: 0.82184\n",
      "Interaction tuning epoch: 35, train loss: 0.84295, val loss: 0.83084\n",
      "Interaction tuning epoch: 36, train loss: 0.81647, val loss: 0.81656\n",
      "Interaction tuning epoch: 37, train loss: 0.81434, val loss: 0.80192\n",
      "Interaction tuning epoch: 38, train loss: 0.81647, val loss: 0.81865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 39, train loss: 0.80969, val loss: 0.80605\n",
      "Interaction tuning epoch: 40, train loss: 0.81152, val loss: 0.81092\n",
      "Interaction tuning epoch: 41, train loss: 0.81000, val loss: 0.80475\n",
      "Interaction tuning epoch: 42, train loss: 0.80874, val loss: 0.80624\n",
      "Interaction tuning epoch: 43, train loss: 0.80997, val loss: 0.80494\n",
      "Interaction tuning epoch: 44, train loss: 0.81356, val loss: 0.80658\n",
      "Interaction tuning epoch: 45, train loss: 0.81452, val loss: 0.81623\n",
      "Interaction tuning epoch: 46, train loss: 0.80775, val loss: 0.81179\n",
      "Interaction tuning epoch: 47, train loss: 0.81477, val loss: 0.80927\n",
      "Interaction tuning epoch: 48, train loss: 0.81802, val loss: 0.81413\n",
      "Interaction tuning epoch: 49, train loss: 0.81307, val loss: 0.81744\n",
      "Interaction tuning epoch: 50, train loss: 0.81885, val loss: 0.80675\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 35.1372652053833\n",
      "After the gam stage, training error is 0.81885 , validation error is 0.80675\n",
      "missing value counts: 92783\n",
      "#####start auto_tuning#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best shrinkage is 0.918555\n",
      "the best combination is 0.649684\n",
      "[SoftImpute] Max Singular Value of X_init = 20.277104\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed MAE=0.677297 validation MAE=0.762794,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.631156 validation MAE=0.742853,rank=5\n",
      "[SoftImpute] Iter 3: observed MAE=0.592613 validation MAE=0.725005,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.559605 validation MAE=0.709481,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.531319 validation MAE=0.695226,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.506952 validation MAE=0.682395,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.485431 validation MAE=0.670384,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.467114 validation MAE=0.659989,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.450240 validation MAE=0.650759,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.435482 validation MAE=0.641796,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.422757 validation MAE=0.634180,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.411489 validation MAE=0.627747,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.400496 validation MAE=0.621225,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.391326 validation MAE=0.615218,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.382530 validation MAE=0.610160,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.374274 validation MAE=0.605351,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.366903 validation MAE=0.600474,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.360425 validation MAE=0.596655,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.354330 validation MAE=0.592966,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.349380 validation MAE=0.589654,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.343530 validation MAE=0.586568,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.339341 validation MAE=0.583424,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.335023 validation MAE=0.581174,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.330725 validation MAE=0.578720,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.327059 validation MAE=0.576214,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.324040 validation MAE=0.574279,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.320354 validation MAE=0.572698,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.316694 validation MAE=0.570277,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.314297 validation MAE=0.568783,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.311592 validation MAE=0.566946,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.308674 validation MAE=0.565395,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.306247 validation MAE=0.563979,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.304326 validation MAE=0.562543,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.301845 validation MAE=0.561485,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.300472 validation MAE=0.560799,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.297898 validation MAE=0.559393,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.296130 validation MAE=0.558224,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.294328 validation MAE=0.556974,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.291459 validation MAE=0.556229,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.289932 validation MAE=0.555318,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.288985 validation MAE=0.554548,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.286786 validation MAE=0.553611,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.285997 validation MAE=0.552986,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.284073 validation MAE=0.552318,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.283076 validation MAE=0.551117,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.281887 validation MAE=0.550370,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.280058 validation MAE=0.550134,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.278901 validation MAE=0.549448,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.278301 validation MAE=0.549188,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.277149 validation MAE=0.548161,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.276385 validation MAE=0.547691,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.275482 validation MAE=0.547216,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.273810 validation MAE=0.546961,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.272922 validation MAE=0.546446,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.272831 validation MAE=0.546824,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.271698 validation MAE=0.545472,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.270903 validation MAE=0.545337,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.270220 validation MAE=0.544636,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.269105 validation MAE=0.543726,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.268145 validation MAE=0.543766,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.268223 validation MAE=0.543234,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.267023 validation MAE=0.542975,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.266164 validation MAE=0.543558,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.266094 validation MAE=0.542526,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.265527 validation MAE=0.542045,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.264423 validation MAE=0.541396,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.263547 validation MAE=0.540846,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.263247 validation MAE=0.540291,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.262188 validation MAE=0.540561,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.262183 validation MAE=0.540404,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.260929 validation MAE=0.539678,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.260545 validation MAE=0.539065,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.260491 validation MAE=0.538655,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.260163 validation MAE=0.538440,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.258824 validation MAE=0.538185,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.258013 validation MAE=0.537095,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.258600 validation MAE=0.537443,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.257454 validation MAE=0.536383,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.257026 validation MAE=0.536405,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.257045 validation MAE=0.536115,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.256566 validation MAE=0.535839,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.255571 validation MAE=0.535345,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.255648 validation MAE=0.535169,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.254719 validation MAE=0.534403,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.254797 validation MAE=0.534176,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.253997 validation MAE=0.533703,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.253814 validation MAE=0.533487,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.253317 validation MAE=0.533025,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.252980 validation MAE=0.532636,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.252623 validation MAE=0.532331,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.252278 validation MAE=0.532193,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.251805 validation MAE=0.532128,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.251843 validation MAE=0.532284,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.250522 validation MAE=0.531024,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.251008 validation MAE=0.531027,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.250084 validation MAE=0.530020,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.250417 validation MAE=0.530147,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.250057 validation MAE=0.529925,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.249588 validation MAE=0.529316,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.249205 validation MAE=0.529837,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.249507 validation MAE=0.528996,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.248574 validation MAE=0.528144,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.248324 validation MAE=0.528862,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.247655 validation MAE=0.528145,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.248109 validation MAE=0.528504,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.246500 validation MAE=0.527446,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.247283 validation MAE=0.527694,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.246576 validation MAE=0.526839,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 109: observed MAE=0.245983 validation MAE=0.526568,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.245923 validation MAE=0.526172,rank=5\n",
      "[SoftImpute] Iter 111: observed MAE=0.246201 validation MAE=0.526370,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.244849 validation MAE=0.526418,rank=5\n",
      "[SoftImpute] Iter 113: observed MAE=0.245028 validation MAE=0.526318,rank=5\n",
      "[SoftImpute] Iter 114: observed MAE=0.245522 validation MAE=0.525881,rank=5\n",
      "[SoftImpute] Iter 115: observed MAE=0.245244 validation MAE=0.526032,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.244454 validation MAE=0.525420,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.244349 validation MAE=0.525444,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.243696 validation MAE=0.524520,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.243734 validation MAE=0.524592,rank=5\n",
      "[SoftImpute] Iter 120: observed MAE=0.243174 validation MAE=0.524611,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.243082 validation MAE=0.524619,rank=5\n",
      "[SoftImpute] Stopped after iteration 121 for lambda=0.405542\n",
      "final num of user group: 30\n",
      "final num of item group: 47\n",
      "change mode state : True\n",
      "time cost: 366.8317849636078\n",
      "After the matrix factor stage, training error is 0.24308, validation error is 0.52462\n",
      "0\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.02581, val loss: 4.15569\n",
      "Main effects training epoch: 2, train loss: 3.86663, val loss: 4.00300\n",
      "Main effects training epoch: 3, train loss: 3.66700, val loss: 3.81314\n",
      "Main effects training epoch: 4, train loss: 3.48207, val loss: 3.63384\n",
      "Main effects training epoch: 5, train loss: 3.38250, val loss: 3.50337\n",
      "Main effects training epoch: 6, train loss: 3.28135, val loss: 3.35840\n",
      "Main effects training epoch: 7, train loss: 3.28186, val loss: 3.35070\n",
      "Main effects training epoch: 8, train loss: 3.27336, val loss: 3.36040\n",
      "Main effects training epoch: 9, train loss: 3.25283, val loss: 3.34172\n",
      "Main effects training epoch: 10, train loss: 3.15250, val loss: 3.23796\n",
      "Main effects training epoch: 11, train loss: 3.05938, val loss: 3.14667\n",
      "Main effects training epoch: 12, train loss: 3.05048, val loss: 3.13776\n",
      "Main effects training epoch: 13, train loss: 2.97453, val loss: 3.06166\n",
      "Main effects training epoch: 14, train loss: 2.90463, val loss: 2.99088\n",
      "Main effects training epoch: 15, train loss: 2.85090, val loss: 2.93330\n",
      "Main effects training epoch: 16, train loss: 2.80705, val loss: 2.88247\n",
      "Main effects training epoch: 17, train loss: 2.72197, val loss: 2.79421\n",
      "Main effects training epoch: 18, train loss: 2.70970, val loss: 2.78070\n",
      "Main effects training epoch: 19, train loss: 2.64818, val loss: 2.71541\n",
      "Main effects training epoch: 20, train loss: 2.60938, val loss: 2.68330\n",
      "Main effects training epoch: 21, train loss: 2.57306, val loss: 2.65275\n",
      "Main effects training epoch: 22, train loss: 2.51936, val loss: 2.59316\n",
      "Main effects training epoch: 23, train loss: 2.51905, val loss: 2.60367\n",
      "Main effects training epoch: 24, train loss: 2.44088, val loss: 2.52255\n",
      "Main effects training epoch: 25, train loss: 2.41554, val loss: 2.50227\n",
      "Main effects training epoch: 26, train loss: 2.38866, val loss: 2.46996\n",
      "Main effects training epoch: 27, train loss: 2.36765, val loss: 2.45355\n",
      "Main effects training epoch: 28, train loss: 2.33153, val loss: 2.41112\n",
      "Main effects training epoch: 29, train loss: 2.28303, val loss: 2.36249\n",
      "Main effects training epoch: 30, train loss: 2.26930, val loss: 2.35093\n",
      "Main effects training epoch: 31, train loss: 2.21173, val loss: 2.29201\n",
      "Main effects training epoch: 32, train loss: 2.24060, val loss: 2.32154\n",
      "Main effects training epoch: 33, train loss: 2.17058, val loss: 2.25252\n",
      "Main effects training epoch: 34, train loss: 2.18446, val loss: 2.25689\n",
      "Main effects training epoch: 35, train loss: 2.16814, val loss: 2.24363\n",
      "Main effects training epoch: 36, train loss: 2.13803, val loss: 2.21848\n",
      "Main effects training epoch: 37, train loss: 2.07828, val loss: 2.15467\n",
      "Main effects training epoch: 38, train loss: 2.10415, val loss: 2.17893\n",
      "Main effects training epoch: 39, train loss: 2.06347, val loss: 2.14620\n",
      "Main effects training epoch: 40, train loss: 2.05131, val loss: 2.12958\n",
      "Main effects training epoch: 41, train loss: 2.04399, val loss: 2.11796\n",
      "Main effects training epoch: 42, train loss: 1.99832, val loss: 2.07447\n",
      "Main effects training epoch: 43, train loss: 1.99686, val loss: 2.07381\n",
      "Main effects training epoch: 44, train loss: 1.98219, val loss: 2.05986\n",
      "Main effects training epoch: 45, train loss: 1.98538, val loss: 2.06555\n",
      "Main effects training epoch: 46, train loss: 1.93199, val loss: 2.00777\n",
      "Main effects training epoch: 47, train loss: 1.92633, val loss: 2.00292\n",
      "Main effects training epoch: 48, train loss: 1.93781, val loss: 2.01180\n",
      "Main effects training epoch: 49, train loss: 1.90558, val loss: 1.97629\n",
      "Main effects training epoch: 50, train loss: 1.89436, val loss: 1.97301\n",
      "Main effects training epoch: 51, train loss: 1.88707, val loss: 1.95539\n",
      "Main effects training epoch: 52, train loss: 1.87418, val loss: 1.95203\n",
      "Main effects training epoch: 53, train loss: 1.88074, val loss: 1.95203\n",
      "Main effects training epoch: 54, train loss: 1.84766, val loss: 1.91670\n",
      "Main effects training epoch: 55, train loss: 1.86362, val loss: 1.93316\n",
      "Main effects training epoch: 56, train loss: 1.84504, val loss: 1.91578\n",
      "Main effects training epoch: 57, train loss: 1.83871, val loss: 1.90598\n",
      "Main effects training epoch: 58, train loss: 1.83635, val loss: 1.90794\n",
      "Main effects training epoch: 59, train loss: 1.81987, val loss: 1.88301\n",
      "Main effects training epoch: 60, train loss: 1.81411, val loss: 1.87753\n",
      "Main effects training epoch: 61, train loss: 1.79435, val loss: 1.85428\n",
      "Main effects training epoch: 62, train loss: 1.80737, val loss: 1.87724\n",
      "Main effects training epoch: 63, train loss: 1.78433, val loss: 1.84476\n",
      "Main effects training epoch: 64, train loss: 1.77484, val loss: 1.83380\n",
      "Main effects training epoch: 65, train loss: 1.76469, val loss: 1.82684\n",
      "Main effects training epoch: 66, train loss: 1.75610, val loss: 1.81696\n",
      "Main effects training epoch: 67, train loss: 1.74015, val loss: 1.80136\n",
      "Main effects training epoch: 68, train loss: 1.74855, val loss: 1.80547\n",
      "Main effects training epoch: 69, train loss: 1.73357, val loss: 1.79520\n",
      "Main effects training epoch: 70, train loss: 1.74203, val loss: 1.79823\n",
      "Main effects training epoch: 71, train loss: 1.73003, val loss: 1.79278\n",
      "Main effects training epoch: 72, train loss: 1.72470, val loss: 1.77914\n",
      "Main effects training epoch: 73, train loss: 1.72740, val loss: 1.78702\n",
      "Main effects training epoch: 74, train loss: 1.71997, val loss: 1.77884\n",
      "Main effects training epoch: 75, train loss: 1.72546, val loss: 1.77572\n",
      "Main effects training epoch: 76, train loss: 1.71292, val loss: 1.77190\n",
      "Main effects training epoch: 77, train loss: 1.71444, val loss: 1.77190\n",
      "Main effects training epoch: 78, train loss: 1.71019, val loss: 1.76935\n",
      "Main effects training epoch: 79, train loss: 1.70637, val loss: 1.75924\n",
      "Main effects training epoch: 80, train loss: 1.70559, val loss: 1.76570\n",
      "Main effects training epoch: 81, train loss: 1.70424, val loss: 1.76163\n",
      "Main effects training epoch: 82, train loss: 1.69625, val loss: 1.75276\n",
      "Main effects training epoch: 83, train loss: 1.70362, val loss: 1.76509\n",
      "Main effects training epoch: 84, train loss: 1.69010, val loss: 1.74551\n",
      "Main effects training epoch: 85, train loss: 1.69095, val loss: 1.75028\n",
      "Main effects training epoch: 86, train loss: 1.68340, val loss: 1.74746\n",
      "Main effects training epoch: 87, train loss: 1.68064, val loss: 1.74529\n",
      "Main effects training epoch: 88, train loss: 1.67380, val loss: 1.74022\n",
      "Main effects training epoch: 89, train loss: 1.67360, val loss: 1.74665\n",
      "Main effects training epoch: 90, train loss: 1.65844, val loss: 1.73033\n",
      "Main effects training epoch: 91, train loss: 1.65355, val loss: 1.72703\n",
      "Main effects training epoch: 92, train loss: 1.64214, val loss: 1.71970\n",
      "Main effects training epoch: 93, train loss: 1.64936, val loss: 1.73062\n",
      "Main effects training epoch: 94, train loss: 1.63636, val loss: 1.71888\n",
      "Main effects training epoch: 95, train loss: 1.63488, val loss: 1.72091\n",
      "Main effects training epoch: 96, train loss: 1.63066, val loss: 1.70495\n",
      "Main effects training epoch: 97, train loss: 1.63836, val loss: 1.72648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 98, train loss: 1.63448, val loss: 1.71359\n",
      "Main effects training epoch: 99, train loss: 1.62205, val loss: 1.71122\n",
      "Main effects training epoch: 100, train loss: 1.62696, val loss: 1.71339\n",
      "Main effects training epoch: 101, train loss: 1.62468, val loss: 1.70969\n",
      "Main effects training epoch: 102, train loss: 1.62361, val loss: 1.71057\n",
      "Main effects training epoch: 103, train loss: 1.61811, val loss: 1.70932\n",
      "Main effects training epoch: 104, train loss: 1.61437, val loss: 1.69638\n",
      "Main effects training epoch: 105, train loss: 1.61687, val loss: 1.70838\n",
      "Main effects training epoch: 106, train loss: 1.61267, val loss: 1.69917\n",
      "Main effects training epoch: 107, train loss: 1.60933, val loss: 1.69456\n",
      "Main effects training epoch: 108, train loss: 1.60899, val loss: 1.69789\n",
      "Main effects training epoch: 109, train loss: 1.61071, val loss: 1.70213\n",
      "Main effects training epoch: 110, train loss: 1.60834, val loss: 1.69356\n",
      "Main effects training epoch: 111, train loss: 1.60903, val loss: 1.69018\n",
      "Main effects training epoch: 112, train loss: 1.61192, val loss: 1.71417\n",
      "Main effects training epoch: 113, train loss: 1.60321, val loss: 1.68930\n",
      "Main effects training epoch: 114, train loss: 1.60681, val loss: 1.69810\n",
      "Main effects training epoch: 115, train loss: 1.61161, val loss: 1.69664\n",
      "Main effects training epoch: 116, train loss: 1.60462, val loss: 1.69884\n",
      "Main effects training epoch: 117, train loss: 1.60153, val loss: 1.68634\n",
      "Main effects training epoch: 118, train loss: 1.59715, val loss: 1.69219\n",
      "Main effects training epoch: 119, train loss: 1.59567, val loss: 1.68591\n",
      "Main effects training epoch: 120, train loss: 1.59747, val loss: 1.68979\n",
      "Main effects training epoch: 121, train loss: 1.59275, val loss: 1.68562\n",
      "Main effects training epoch: 122, train loss: 1.58952, val loss: 1.68187\n",
      "Main effects training epoch: 123, train loss: 1.59299, val loss: 1.69392\n",
      "Main effects training epoch: 124, train loss: 1.58823, val loss: 1.67589\n",
      "Main effects training epoch: 125, train loss: 1.58752, val loss: 1.67897\n",
      "Main effects training epoch: 126, train loss: 1.58643, val loss: 1.67941\n",
      "Main effects training epoch: 127, train loss: 1.58719, val loss: 1.68306\n",
      "Main effects training epoch: 128, train loss: 1.58105, val loss: 1.67735\n",
      "Main effects training epoch: 129, train loss: 1.58516, val loss: 1.67262\n",
      "Main effects training epoch: 130, train loss: 1.57827, val loss: 1.67244\n",
      "Main effects training epoch: 131, train loss: 1.57854, val loss: 1.66952\n",
      "Main effects training epoch: 132, train loss: 1.58368, val loss: 1.67841\n",
      "Main effects training epoch: 133, train loss: 1.57787, val loss: 1.66421\n",
      "Main effects training epoch: 134, train loss: 1.57550, val loss: 1.67620\n",
      "Main effects training epoch: 135, train loss: 1.57558, val loss: 1.66555\n",
      "Main effects training epoch: 136, train loss: 1.57565, val loss: 1.66808\n",
      "Main effects training epoch: 137, train loss: 1.57215, val loss: 1.65359\n",
      "Main effects training epoch: 138, train loss: 1.57597, val loss: 1.67812\n",
      "Main effects training epoch: 139, train loss: 1.56594, val loss: 1.65260\n",
      "Main effects training epoch: 140, train loss: 1.56379, val loss: 1.65452\n",
      "Main effects training epoch: 141, train loss: 1.56809, val loss: 1.65819\n",
      "Main effects training epoch: 142, train loss: 1.56959, val loss: 1.67084\n",
      "Main effects training epoch: 143, train loss: 1.56391, val loss: 1.65611\n",
      "Main effects training epoch: 144, train loss: 1.56557, val loss: 1.66064\n",
      "Main effects training epoch: 145, train loss: 1.56139, val loss: 1.65606\n",
      "Main effects training epoch: 146, train loss: 1.55847, val loss: 1.65215\n",
      "Main effects training epoch: 147, train loss: 1.55328, val loss: 1.65161\n",
      "Main effects training epoch: 148, train loss: 1.55521, val loss: 1.65337\n",
      "Main effects training epoch: 149, train loss: 1.55394, val loss: 1.64666\n",
      "Main effects training epoch: 150, train loss: 1.55689, val loss: 1.64452\n",
      "Main effects training epoch: 151, train loss: 1.55483, val loss: 1.64596\n",
      "Main effects training epoch: 152, train loss: 1.55640, val loss: 1.65974\n",
      "Main effects training epoch: 153, train loss: 1.55465, val loss: 1.64418\n",
      "Main effects training epoch: 154, train loss: 1.54827, val loss: 1.64315\n",
      "Main effects training epoch: 155, train loss: 1.54425, val loss: 1.63498\n",
      "Main effects training epoch: 156, train loss: 1.54566, val loss: 1.63837\n",
      "Main effects training epoch: 157, train loss: 1.54784, val loss: 1.64139\n",
      "Main effects training epoch: 158, train loss: 1.55597, val loss: 1.64081\n",
      "Main effects training epoch: 159, train loss: 1.54733, val loss: 1.64121\n",
      "Main effects training epoch: 160, train loss: 1.54370, val loss: 1.64175\n",
      "Main effects training epoch: 161, train loss: 1.54700, val loss: 1.63689\n",
      "Main effects training epoch: 162, train loss: 1.53894, val loss: 1.64314\n",
      "Main effects training epoch: 163, train loss: 1.55071, val loss: 1.63990\n",
      "Main effects training epoch: 164, train loss: 1.53857, val loss: 1.62758\n",
      "Main effects training epoch: 165, train loss: 1.53582, val loss: 1.63364\n",
      "Main effects training epoch: 166, train loss: 1.53784, val loss: 1.62337\n",
      "Main effects training epoch: 167, train loss: 1.53668, val loss: 1.61628\n",
      "Main effects training epoch: 168, train loss: 1.54187, val loss: 1.64968\n",
      "Main effects training epoch: 169, train loss: 1.53233, val loss: 1.61626\n",
      "Main effects training epoch: 170, train loss: 1.53301, val loss: 1.62002\n",
      "Main effects training epoch: 171, train loss: 1.53462, val loss: 1.63271\n",
      "Main effects training epoch: 172, train loss: 1.53059, val loss: 1.62420\n",
      "Main effects training epoch: 173, train loss: 1.52675, val loss: 1.61587\n",
      "Main effects training epoch: 174, train loss: 1.52574, val loss: 1.61472\n",
      "Main effects training epoch: 175, train loss: 1.52522, val loss: 1.61609\n",
      "Main effects training epoch: 176, train loss: 1.52517, val loss: 1.61189\n",
      "Main effects training epoch: 177, train loss: 1.52300, val loss: 1.61130\n",
      "Main effects training epoch: 178, train loss: 1.52251, val loss: 1.61256\n",
      "Main effects training epoch: 179, train loss: 1.52604, val loss: 1.62370\n",
      "Main effects training epoch: 180, train loss: 1.51909, val loss: 1.60909\n",
      "Main effects training epoch: 181, train loss: 1.52000, val loss: 1.60844\n",
      "Main effects training epoch: 182, train loss: 1.52126, val loss: 1.60803\n",
      "Main effects training epoch: 183, train loss: 1.51478, val loss: 1.60948\n",
      "Main effects training epoch: 184, train loss: 1.52037, val loss: 1.60605\n",
      "Main effects training epoch: 185, train loss: 1.51414, val loss: 1.59444\n",
      "Main effects training epoch: 186, train loss: 1.51951, val loss: 1.61258\n",
      "Main effects training epoch: 187, train loss: 1.52691, val loss: 1.60462\n",
      "Main effects training epoch: 188, train loss: 1.51070, val loss: 1.60830\n",
      "Main effects training epoch: 189, train loss: 1.50982, val loss: 1.59836\n",
      "Main effects training epoch: 190, train loss: 1.51110, val loss: 1.59181\n",
      "Main effects training epoch: 191, train loss: 1.50860, val loss: 1.60462\n",
      "Main effects training epoch: 192, train loss: 1.51019, val loss: 1.59339\n",
      "Main effects training epoch: 193, train loss: 1.50871, val loss: 1.59399\n",
      "Main effects training epoch: 194, train loss: 1.51010, val loss: 1.59995\n",
      "Main effects training epoch: 195, train loss: 1.50732, val loss: 1.58955\n",
      "Main effects training epoch: 196, train loss: 1.51943, val loss: 1.59980\n",
      "Main effects training epoch: 197, train loss: 1.52664, val loss: 1.61026\n",
      "Main effects training epoch: 198, train loss: 1.52697, val loss: 1.60535\n",
      "Main effects training epoch: 199, train loss: 1.51058, val loss: 1.59071\n",
      "Main effects training epoch: 200, train loss: 1.50977, val loss: 1.59562\n",
      "Main effects training epoch: 201, train loss: 1.50855, val loss: 1.58794\n",
      "Main effects training epoch: 202, train loss: 1.50613, val loss: 1.59355\n",
      "Main effects training epoch: 203, train loss: 1.50374, val loss: 1.58254\n",
      "Main effects training epoch: 204, train loss: 1.50417, val loss: 1.59863\n",
      "Main effects training epoch: 205, train loss: 1.49828, val loss: 1.57278\n",
      "Main effects training epoch: 206, train loss: 1.50065, val loss: 1.59706\n",
      "Main effects training epoch: 207, train loss: 1.50225, val loss: 1.57431\n",
      "Main effects training epoch: 208, train loss: 1.50469, val loss: 1.59505\n",
      "Main effects training epoch: 209, train loss: 1.49897, val loss: 1.58897\n",
      "Main effects training epoch: 210, train loss: 1.50055, val loss: 1.57327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 211, train loss: 1.51061, val loss: 1.59110\n",
      "Main effects training epoch: 212, train loss: 1.50414, val loss: 1.57268\n",
      "Main effects training epoch: 213, train loss: 1.50270, val loss: 1.59163\n",
      "Main effects training epoch: 214, train loss: 1.49817, val loss: 1.57566\n",
      "Main effects training epoch: 215, train loss: 1.49571, val loss: 1.57164\n",
      "Main effects training epoch: 216, train loss: 1.49560, val loss: 1.57660\n",
      "Main effects training epoch: 217, train loss: 1.49150, val loss: 1.57137\n",
      "Main effects training epoch: 218, train loss: 1.49647, val loss: 1.58332\n",
      "Main effects training epoch: 219, train loss: 1.49084, val loss: 1.56828\n",
      "Main effects training epoch: 220, train loss: 1.49340, val loss: 1.57736\n",
      "Main effects training epoch: 221, train loss: 1.50009, val loss: 1.56919\n",
      "Main effects training epoch: 222, train loss: 1.49716, val loss: 1.58686\n",
      "Main effects training epoch: 223, train loss: 1.49415, val loss: 1.56762\n",
      "Main effects training epoch: 224, train loss: 1.49731, val loss: 1.57543\n",
      "Main effects training epoch: 225, train loss: 1.49069, val loss: 1.56735\n",
      "Main effects training epoch: 226, train loss: 1.48744, val loss: 1.56514\n",
      "Main effects training epoch: 227, train loss: 1.48606, val loss: 1.56522\n",
      "Main effects training epoch: 228, train loss: 1.48761, val loss: 1.57127\n",
      "Main effects training epoch: 229, train loss: 1.48698, val loss: 1.56841\n",
      "Main effects training epoch: 230, train loss: 1.49084, val loss: 1.57460\n",
      "Main effects training epoch: 231, train loss: 1.48533, val loss: 1.56179\n",
      "Main effects training epoch: 232, train loss: 1.48691, val loss: 1.56537\n",
      "Main effects training epoch: 233, train loss: 1.49704, val loss: 1.57297\n",
      "Main effects training epoch: 234, train loss: 1.48505, val loss: 1.57149\n",
      "Main effects training epoch: 235, train loss: 1.49092, val loss: 1.56129\n",
      "Main effects training epoch: 236, train loss: 1.48097, val loss: 1.56570\n",
      "Main effects training epoch: 237, train loss: 1.48239, val loss: 1.55212\n",
      "Main effects training epoch: 238, train loss: 1.48273, val loss: 1.55997\n",
      "Main effects training epoch: 239, train loss: 1.48013, val loss: 1.55569\n",
      "Main effects training epoch: 240, train loss: 1.48035, val loss: 1.56105\n",
      "Main effects training epoch: 241, train loss: 1.48436, val loss: 1.55253\n",
      "Main effects training epoch: 242, train loss: 1.48507, val loss: 1.55589\n",
      "Main effects training epoch: 243, train loss: 1.48884, val loss: 1.56125\n",
      "Main effects training epoch: 244, train loss: 1.48401, val loss: 1.57373\n",
      "Main effects training epoch: 245, train loss: 1.49051, val loss: 1.55337\n",
      "Main effects training epoch: 246, train loss: 1.48160, val loss: 1.56023\n",
      "Main effects training epoch: 247, train loss: 1.47777, val loss: 1.55312\n",
      "Main effects training epoch: 248, train loss: 1.49156, val loss: 1.55815\n",
      "Main effects training epoch: 249, train loss: 1.48098, val loss: 1.55937\n",
      "Main effects training epoch: 250, train loss: 1.48385, val loss: 1.56199\n",
      "Main effects training epoch: 251, train loss: 1.48659, val loss: 1.56128\n",
      "Main effects training epoch: 252, train loss: 1.47685, val loss: 1.56157\n",
      "Main effects training epoch: 253, train loss: 1.48808, val loss: 1.54859\n",
      "Main effects training epoch: 254, train loss: 1.48407, val loss: 1.55806\n",
      "Main effects training epoch: 255, train loss: 1.48563, val loss: 1.55742\n",
      "Main effects training epoch: 256, train loss: 1.47918, val loss: 1.54952\n",
      "Main effects training epoch: 257, train loss: 1.47606, val loss: 1.55121\n",
      "Main effects training epoch: 258, train loss: 1.47372, val loss: 1.54436\n",
      "Main effects training epoch: 259, train loss: 1.47893, val loss: 1.55576\n",
      "Main effects training epoch: 260, train loss: 1.47852, val loss: 1.54907\n",
      "Main effects training epoch: 261, train loss: 1.49550, val loss: 1.55619\n",
      "Main effects training epoch: 262, train loss: 1.48585, val loss: 1.56521\n",
      "Main effects training epoch: 263, train loss: 1.47815, val loss: 1.52881\n",
      "Main effects training epoch: 264, train loss: 1.46899, val loss: 1.55029\n",
      "Main effects training epoch: 265, train loss: 1.46937, val loss: 1.54180\n",
      "Main effects training epoch: 266, train loss: 1.47614, val loss: 1.54285\n",
      "Main effects training epoch: 267, train loss: 1.46799, val loss: 1.53840\n",
      "Main effects training epoch: 268, train loss: 1.46506, val loss: 1.53744\n",
      "Main effects training epoch: 269, train loss: 1.46442, val loss: 1.53535\n",
      "Main effects training epoch: 270, train loss: 1.46777, val loss: 1.52940\n",
      "Main effects training epoch: 271, train loss: 1.46353, val loss: 1.53767\n",
      "Main effects training epoch: 272, train loss: 1.47036, val loss: 1.52646\n",
      "Main effects training epoch: 273, train loss: 1.46460, val loss: 1.53165\n",
      "Main effects training epoch: 274, train loss: 1.46643, val loss: 1.51927\n",
      "Main effects training epoch: 275, train loss: 1.46682, val loss: 1.54881\n",
      "Main effects training epoch: 276, train loss: 1.47766, val loss: 1.54330\n",
      "Main effects training epoch: 277, train loss: 1.46014, val loss: 1.53089\n",
      "Main effects training epoch: 278, train loss: 1.46303, val loss: 1.53656\n",
      "Main effects training epoch: 279, train loss: 1.46443, val loss: 1.52143\n",
      "Main effects training epoch: 280, train loss: 1.46422, val loss: 1.54040\n",
      "Main effects training epoch: 281, train loss: 1.45312, val loss: 1.51418\n",
      "Main effects training epoch: 282, train loss: 1.45666, val loss: 1.52099\n",
      "Main effects training epoch: 283, train loss: 1.45174, val loss: 1.51592\n",
      "Main effects training epoch: 284, train loss: 1.46646, val loss: 1.51831\n",
      "Main effects training epoch: 285, train loss: 1.45664, val loss: 1.51366\n",
      "Main effects training epoch: 286, train loss: 1.45831, val loss: 1.50973\n",
      "Main effects training epoch: 287, train loss: 1.46764, val loss: 1.54868\n",
      "Main effects training epoch: 288, train loss: 1.46280, val loss: 1.50655\n",
      "Main effects training epoch: 289, train loss: 1.45112, val loss: 1.52111\n",
      "Main effects training epoch: 290, train loss: 1.45658, val loss: 1.51546\n",
      "Main effects training epoch: 291, train loss: 1.45599, val loss: 1.51696\n",
      "Main effects training epoch: 292, train loss: 1.44961, val loss: 1.51482\n",
      "Main effects training epoch: 293, train loss: 1.46067, val loss: 1.51332\n",
      "Main effects training epoch: 294, train loss: 1.47385, val loss: 1.51202\n",
      "Main effects training epoch: 295, train loss: 1.45615, val loss: 1.52349\n",
      "Main effects training epoch: 296, train loss: 1.47318, val loss: 1.53020\n",
      "Main effects training epoch: 297, train loss: 1.46001, val loss: 1.50907\n",
      "Main effects training epoch: 298, train loss: 1.47136, val loss: 1.51133\n",
      "Main effects training epoch: 299, train loss: 1.46982, val loss: 1.53516\n",
      "Main effects training epoch: 300, train loss: 1.44960, val loss: 1.50548\n",
      "##########Stage 1: main effect training stop.##########\n",
      "3 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.47981, val loss: 1.52267\n",
      "Main effects tuning epoch: 2, train loss: 1.46331, val loss: 1.51472\n",
      "Main effects tuning epoch: 3, train loss: 1.46830, val loss: 1.51161\n",
      "Main effects tuning epoch: 4, train loss: 1.46896, val loss: 1.51178\n",
      "Main effects tuning epoch: 5, train loss: 1.47528, val loss: 1.51366\n",
      "Main effects tuning epoch: 6, train loss: 1.46808, val loss: 1.51692\n",
      "Main effects tuning epoch: 7, train loss: 1.46537, val loss: 1.51874\n",
      "Main effects tuning epoch: 8, train loss: 1.47975, val loss: 1.50987\n",
      "Main effects tuning epoch: 9, train loss: 1.46065, val loss: 1.51071\n",
      "Main effects tuning epoch: 10, train loss: 1.46917, val loss: 1.50453\n",
      "Main effects tuning epoch: 11, train loss: 1.47333, val loss: 1.50563\n",
      "Main effects tuning epoch: 12, train loss: 1.46157, val loss: 1.50754\n",
      "Main effects tuning epoch: 13, train loss: 1.48088, val loss: 1.52535\n",
      "Main effects tuning epoch: 14, train loss: 1.46625, val loss: 1.51370\n",
      "Main effects tuning epoch: 15, train loss: 1.46376, val loss: 1.51038\n",
      "Main effects tuning epoch: 16, train loss: 1.47410, val loss: 1.50254\n",
      "Main effects tuning epoch: 17, train loss: 1.46502, val loss: 1.52004\n",
      "Main effects tuning epoch: 18, train loss: 1.46385, val loss: 1.49553\n",
      "Main effects tuning epoch: 19, train loss: 1.46368, val loss: 1.50912\n",
      "Main effects tuning epoch: 20, train loss: 1.46234, val loss: 1.50258\n",
      "Main effects tuning epoch: 21, train loss: 1.46433, val loss: 1.49670\n",
      "Main effects tuning epoch: 22, train loss: 1.46370, val loss: 1.49430\n",
      "Main effects tuning epoch: 23, train loss: 1.46845, val loss: 1.49861\n",
      "Main effects tuning epoch: 24, train loss: 1.46385, val loss: 1.50414\n",
      "Main effects tuning epoch: 25, train loss: 1.45635, val loss: 1.50218\n",
      "Main effects tuning epoch: 26, train loss: 1.45837, val loss: 1.49553\n",
      "Main effects tuning epoch: 27, train loss: 1.46347, val loss: 1.50165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects tuning epoch: 28, train loss: 1.46517, val loss: 1.49487\n",
      "Main effects tuning epoch: 29, train loss: 1.45550, val loss: 1.49383\n",
      "Main effects tuning epoch: 30, train loss: 1.45760, val loss: 1.50101\n",
      "Main effects tuning epoch: 31, train loss: 1.47132, val loss: 1.49669\n",
      "Main effects tuning epoch: 32, train loss: 1.45995, val loss: 1.49988\n",
      "Main effects tuning epoch: 33, train loss: 1.47518, val loss: 1.50785\n",
      "Main effects tuning epoch: 34, train loss: 1.45653, val loss: 1.49390\n",
      "Main effects tuning epoch: 35, train loss: 1.46586, val loss: 1.48579\n",
      "Main effects tuning epoch: 36, train loss: 1.45584, val loss: 1.50565\n",
      "Main effects tuning epoch: 37, train loss: 1.45403, val loss: 1.48898\n",
      "Main effects tuning epoch: 38, train loss: 1.45523, val loss: 1.49846\n",
      "Main effects tuning epoch: 39, train loss: 1.45269, val loss: 1.48773\n",
      "Main effects tuning epoch: 40, train loss: 1.45165, val loss: 1.49717\n",
      "Main effects tuning epoch: 41, train loss: 1.45082, val loss: 1.49373\n",
      "Main effects tuning epoch: 42, train loss: 1.45648, val loss: 1.49967\n",
      "Main effects tuning epoch: 43, train loss: 1.45135, val loss: 1.48556\n",
      "Main effects tuning epoch: 44, train loss: 1.45106, val loss: 1.48879\n",
      "Main effects tuning epoch: 45, train loss: 1.45430, val loss: 1.49438\n",
      "Main effects tuning epoch: 46, train loss: 1.44805, val loss: 1.47819\n",
      "Main effects tuning epoch: 47, train loss: 1.45053, val loss: 1.49323\n",
      "Main effects tuning epoch: 48, train loss: 1.45437, val loss: 1.49447\n",
      "Main effects tuning epoch: 49, train loss: 1.46905, val loss: 1.50304\n",
      "Main effects tuning epoch: 50, train loss: 1.45347, val loss: 1.49706\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.38751, val loss: 1.42972\n",
      "Interaction training epoch: 2, train loss: 1.29788, val loss: 1.29251\n",
      "Interaction training epoch: 3, train loss: 1.29616, val loss: 1.32320\n",
      "Interaction training epoch: 4, train loss: 1.04085, val loss: 1.05417\n",
      "Interaction training epoch: 5, train loss: 1.03009, val loss: 1.03709\n",
      "Interaction training epoch: 6, train loss: 1.05054, val loss: 1.05218\n",
      "Interaction training epoch: 7, train loss: 1.02948, val loss: 1.02686\n",
      "Interaction training epoch: 8, train loss: 0.99902, val loss: 1.00530\n",
      "Interaction training epoch: 9, train loss: 0.98387, val loss: 0.98691\n",
      "Interaction training epoch: 10, train loss: 0.99080, val loss: 1.01337\n",
      "Interaction training epoch: 11, train loss: 0.96049, val loss: 0.96233\n",
      "Interaction training epoch: 12, train loss: 0.94945, val loss: 0.94754\n",
      "Interaction training epoch: 13, train loss: 0.95982, val loss: 0.96210\n",
      "Interaction training epoch: 14, train loss: 0.95414, val loss: 0.94884\n",
      "Interaction training epoch: 15, train loss: 0.94345, val loss: 0.94976\n",
      "Interaction training epoch: 16, train loss: 0.93793, val loss: 0.92820\n",
      "Interaction training epoch: 17, train loss: 0.93178, val loss: 0.93546\n",
      "Interaction training epoch: 18, train loss: 0.91425, val loss: 0.89675\n",
      "Interaction training epoch: 19, train loss: 0.91632, val loss: 0.92731\n",
      "Interaction training epoch: 20, train loss: 0.92236, val loss: 0.91942\n",
      "Interaction training epoch: 21, train loss: 0.90920, val loss: 0.90217\n",
      "Interaction training epoch: 22, train loss: 0.90385, val loss: 0.89343\n",
      "Interaction training epoch: 23, train loss: 0.89987, val loss: 0.89575\n",
      "Interaction training epoch: 24, train loss: 0.90721, val loss: 0.89862\n",
      "Interaction training epoch: 25, train loss: 0.92542, val loss: 0.91327\n",
      "Interaction training epoch: 26, train loss: 0.90072, val loss: 0.91308\n",
      "Interaction training epoch: 27, train loss: 0.90686, val loss: 0.91623\n",
      "Interaction training epoch: 28, train loss: 0.89684, val loss: 0.88755\n",
      "Interaction training epoch: 29, train loss: 0.89574, val loss: 0.88784\n",
      "Interaction training epoch: 30, train loss: 0.89629, val loss: 0.90104\n",
      "Interaction training epoch: 31, train loss: 0.88658, val loss: 0.88759\n",
      "Interaction training epoch: 32, train loss: 0.89231, val loss: 0.87914\n",
      "Interaction training epoch: 33, train loss: 0.88231, val loss: 0.88828\n",
      "Interaction training epoch: 34, train loss: 0.88436, val loss: 0.87844\n",
      "Interaction training epoch: 35, train loss: 0.89044, val loss: 0.88305\n",
      "Interaction training epoch: 36, train loss: 0.88164, val loss: 0.88330\n",
      "Interaction training epoch: 37, train loss: 0.87723, val loss: 0.87402\n",
      "Interaction training epoch: 38, train loss: 0.87424, val loss: 0.87164\n",
      "Interaction training epoch: 39, train loss: 0.88113, val loss: 0.88409\n",
      "Interaction training epoch: 40, train loss: 0.87380, val loss: 0.86950\n",
      "Interaction training epoch: 41, train loss: 0.87787, val loss: 0.87971\n",
      "Interaction training epoch: 42, train loss: 0.87474, val loss: 0.86993\n",
      "Interaction training epoch: 43, train loss: 0.87210, val loss: 0.86398\n",
      "Interaction training epoch: 44, train loss: 0.86534, val loss: 0.86351\n",
      "Interaction training epoch: 45, train loss: 0.87172, val loss: 0.86373\n",
      "Interaction training epoch: 46, train loss: 0.86930, val loss: 0.86788\n",
      "Interaction training epoch: 47, train loss: 0.87310, val loss: 0.86793\n",
      "Interaction training epoch: 48, train loss: 0.87626, val loss: 0.87352\n",
      "Interaction training epoch: 49, train loss: 0.86374, val loss: 0.86011\n",
      "Interaction training epoch: 50, train loss: 0.86237, val loss: 0.86165\n",
      "Interaction training epoch: 51, train loss: 0.85959, val loss: 0.85691\n",
      "Interaction training epoch: 52, train loss: 0.87314, val loss: 0.87168\n",
      "Interaction training epoch: 53, train loss: 0.86557, val loss: 0.85709\n",
      "Interaction training epoch: 54, train loss: 0.88728, val loss: 0.89249\n",
      "Interaction training epoch: 55, train loss: 0.86669, val loss: 0.86887\n",
      "Interaction training epoch: 56, train loss: 0.87323, val loss: 0.87294\n",
      "Interaction training epoch: 57, train loss: 0.85961, val loss: 0.85659\n",
      "Interaction training epoch: 58, train loss: 0.86651, val loss: 0.86275\n",
      "Interaction training epoch: 59, train loss: 0.86838, val loss: 0.87083\n",
      "Interaction training epoch: 60, train loss: 0.87023, val loss: 0.86088\n",
      "Interaction training epoch: 61, train loss: 0.85704, val loss: 0.85348\n",
      "Interaction training epoch: 62, train loss: 0.85447, val loss: 0.85588\n",
      "Interaction training epoch: 63, train loss: 0.85908, val loss: 0.85408\n",
      "Interaction training epoch: 64, train loss: 0.86018, val loss: 0.86011\n",
      "Interaction training epoch: 65, train loss: 0.85534, val loss: 0.84887\n",
      "Interaction training epoch: 66, train loss: 0.85576, val loss: 0.85693\n",
      "Interaction training epoch: 67, train loss: 0.85598, val loss: 0.84631\n",
      "Interaction training epoch: 68, train loss: 0.85123, val loss: 0.85383\n",
      "Interaction training epoch: 69, train loss: 0.84728, val loss: 0.84808\n",
      "Interaction training epoch: 70, train loss: 0.85633, val loss: 0.85398\n",
      "Interaction training epoch: 71, train loss: 0.86387, val loss: 0.85482\n",
      "Interaction training epoch: 72, train loss: 0.86148, val loss: 0.86960\n",
      "Interaction training epoch: 73, train loss: 0.85043, val loss: 0.84455\n",
      "Interaction training epoch: 74, train loss: 0.85069, val loss: 0.85530\n",
      "Interaction training epoch: 75, train loss: 0.84891, val loss: 0.84545\n",
      "Interaction training epoch: 76, train loss: 0.86129, val loss: 0.86666\n",
      "Interaction training epoch: 77, train loss: 0.84883, val loss: 0.84464\n",
      "Interaction training epoch: 78, train loss: 0.84876, val loss: 0.85347\n",
      "Interaction training epoch: 79, train loss: 0.84878, val loss: 0.84226\n",
      "Interaction training epoch: 80, train loss: 0.85118, val loss: 0.85842\n",
      "Interaction training epoch: 81, train loss: 0.85010, val loss: 0.84725\n",
      "Interaction training epoch: 82, train loss: 0.85496, val loss: 0.85152\n",
      "Interaction training epoch: 83, train loss: 0.84734, val loss: 0.84694\n",
      "Interaction training epoch: 84, train loss: 0.83982, val loss: 0.83801\n",
      "Interaction training epoch: 85, train loss: 0.85011, val loss: 0.84656\n",
      "Interaction training epoch: 86, train loss: 0.85688, val loss: 0.84358\n",
      "Interaction training epoch: 87, train loss: 0.86353, val loss: 0.86145\n",
      "Interaction training epoch: 88, train loss: 0.85386, val loss: 0.84938\n",
      "Interaction training epoch: 89, train loss: 0.84869, val loss: 0.85860\n",
      "Interaction training epoch: 90, train loss: 0.85809, val loss: 0.85626\n",
      "Interaction training epoch: 91, train loss: 0.84737, val loss: 0.84347\n",
      "Interaction training epoch: 92, train loss: 0.88299, val loss: 0.87539\n",
      "Interaction training epoch: 93, train loss: 0.85565, val loss: 0.84627\n",
      "Interaction training epoch: 94, train loss: 0.86188, val loss: 0.87089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 95, train loss: 0.86285, val loss: 0.85021\n",
      "Interaction training epoch: 96, train loss: 0.85232, val loss: 0.85564\n",
      "Interaction training epoch: 97, train loss: 0.84748, val loss: 0.84804\n",
      "Interaction training epoch: 98, train loss: 0.84486, val loss: 0.84667\n",
      "Interaction training epoch: 99, train loss: 0.85361, val loss: 0.85675\n",
      "Interaction training epoch: 100, train loss: 0.84448, val loss: 0.84568\n",
      "Interaction training epoch: 101, train loss: 0.84034, val loss: 0.84930\n",
      "Interaction training epoch: 102, train loss: 0.84522, val loss: 0.84170\n",
      "Interaction training epoch: 103, train loss: 0.84307, val loss: 0.84549\n",
      "Interaction training epoch: 104, train loss: 0.84887, val loss: 0.84818\n",
      "Interaction training epoch: 105, train loss: 0.84111, val loss: 0.84438\n",
      "Interaction training epoch: 106, train loss: 0.84481, val loss: 0.85072\n",
      "Interaction training epoch: 107, train loss: 0.83311, val loss: 0.83184\n",
      "Interaction training epoch: 108, train loss: 0.84402, val loss: 0.83933\n",
      "Interaction training epoch: 109, train loss: 0.84446, val loss: 0.84765\n",
      "Interaction training epoch: 110, train loss: 0.84550, val loss: 0.83716\n",
      "Interaction training epoch: 111, train loss: 0.84113, val loss: 0.85107\n",
      "Interaction training epoch: 112, train loss: 0.83914, val loss: 0.84731\n",
      "Interaction training epoch: 113, train loss: 0.83822, val loss: 0.83572\n",
      "Interaction training epoch: 114, train loss: 0.84078, val loss: 0.85020\n",
      "Interaction training epoch: 115, train loss: 0.83684, val loss: 0.84174\n",
      "Interaction training epoch: 116, train loss: 0.84198, val loss: 0.84449\n",
      "Interaction training epoch: 117, train loss: 0.84988, val loss: 0.85086\n",
      "Interaction training epoch: 118, train loss: 0.84723, val loss: 0.84895\n",
      "Interaction training epoch: 119, train loss: 0.84825, val loss: 0.85438\n",
      "Interaction training epoch: 120, train loss: 0.83353, val loss: 0.84249\n",
      "Interaction training epoch: 121, train loss: 0.84568, val loss: 0.84169\n",
      "Interaction training epoch: 122, train loss: 0.84632, val loss: 0.84663\n",
      "Interaction training epoch: 123, train loss: 0.84193, val loss: 0.84624\n",
      "Interaction training epoch: 124, train loss: 0.82580, val loss: 0.82487\n",
      "Interaction training epoch: 125, train loss: 0.84564, val loss: 0.85498\n",
      "Interaction training epoch: 126, train loss: 0.84185, val loss: 0.85316\n",
      "Interaction training epoch: 127, train loss: 0.82878, val loss: 0.83275\n",
      "Interaction training epoch: 128, train loss: 0.82934, val loss: 0.83971\n",
      "Interaction training epoch: 129, train loss: 0.84837, val loss: 0.85086\n",
      "Interaction training epoch: 130, train loss: 0.83089, val loss: 0.84414\n",
      "Interaction training epoch: 131, train loss: 0.83600, val loss: 0.84247\n",
      "Interaction training epoch: 132, train loss: 0.83097, val loss: 0.83826\n",
      "Interaction training epoch: 133, train loss: 0.83711, val loss: 0.84705\n",
      "Interaction training epoch: 134, train loss: 0.83164, val loss: 0.83379\n",
      "Interaction training epoch: 135, train loss: 0.82984, val loss: 0.83505\n",
      "Interaction training epoch: 136, train loss: 0.83540, val loss: 0.82933\n",
      "Interaction training epoch: 137, train loss: 0.83534, val loss: 0.84802\n",
      "Interaction training epoch: 138, train loss: 0.84032, val loss: 0.84233\n",
      "Interaction training epoch: 139, train loss: 0.82736, val loss: 0.83227\n",
      "Interaction training epoch: 140, train loss: 0.82745, val loss: 0.83496\n",
      "Interaction training epoch: 141, train loss: 0.82917, val loss: 0.83139\n",
      "Interaction training epoch: 142, train loss: 0.83277, val loss: 0.83556\n",
      "Interaction training epoch: 143, train loss: 0.83873, val loss: 0.83588\n",
      "Interaction training epoch: 144, train loss: 0.83220, val loss: 0.84184\n",
      "Interaction training epoch: 145, train loss: 0.83137, val loss: 0.83206\n",
      "Interaction training epoch: 146, train loss: 0.83064, val loss: 0.83581\n",
      "Interaction training epoch: 147, train loss: 0.83673, val loss: 0.84164\n",
      "Interaction training epoch: 148, train loss: 0.83926, val loss: 0.83894\n",
      "Interaction training epoch: 149, train loss: 0.82296, val loss: 0.82527\n",
      "Interaction training epoch: 150, train loss: 0.83546, val loss: 0.83520\n",
      "Interaction training epoch: 151, train loss: 0.83130, val loss: 0.83714\n",
      "Interaction training epoch: 152, train loss: 0.83576, val loss: 0.84786\n",
      "Interaction training epoch: 153, train loss: 0.84551, val loss: 0.85060\n",
      "Interaction training epoch: 154, train loss: 0.83219, val loss: 0.82960\n",
      "Interaction training epoch: 155, train loss: 0.83138, val loss: 0.84068\n",
      "Interaction training epoch: 156, train loss: 0.82575, val loss: 0.82219\n",
      "Interaction training epoch: 157, train loss: 0.83437, val loss: 0.83265\n",
      "Interaction training epoch: 158, train loss: 0.83791, val loss: 0.84165\n",
      "Interaction training epoch: 159, train loss: 0.82527, val loss: 0.81928\n",
      "Interaction training epoch: 160, train loss: 0.84681, val loss: 0.84812\n",
      "Interaction training epoch: 161, train loss: 0.84271, val loss: 0.83922\n",
      "Interaction training epoch: 162, train loss: 0.82548, val loss: 0.83312\n",
      "Interaction training epoch: 163, train loss: 0.84106, val loss: 0.84076\n",
      "Interaction training epoch: 164, train loss: 0.83649, val loss: 0.84125\n",
      "Interaction training epoch: 165, train loss: 0.83301, val loss: 0.83403\n",
      "Interaction training epoch: 166, train loss: 0.82190, val loss: 0.81532\n",
      "Interaction training epoch: 167, train loss: 0.82201, val loss: 0.82740\n",
      "Interaction training epoch: 168, train loss: 0.82725, val loss: 0.82684\n",
      "Interaction training epoch: 169, train loss: 0.82403, val loss: 0.82240\n",
      "Interaction training epoch: 170, train loss: 0.82601, val loss: 0.82373\n",
      "Interaction training epoch: 171, train loss: 0.82168, val loss: 0.83330\n",
      "Interaction training epoch: 172, train loss: 0.83113, val loss: 0.82711\n",
      "Interaction training epoch: 173, train loss: 0.82023, val loss: 0.81578\n",
      "Interaction training epoch: 174, train loss: 0.82378, val loss: 0.82676\n",
      "Interaction training epoch: 175, train loss: 0.82887, val loss: 0.83708\n",
      "Interaction training epoch: 176, train loss: 0.82882, val loss: 0.83049\n",
      "Interaction training epoch: 177, train loss: 0.83148, val loss: 0.84407\n",
      "Interaction training epoch: 178, train loss: 0.82600, val loss: 0.82280\n",
      "Interaction training epoch: 179, train loss: 0.83135, val loss: 0.83668\n",
      "Interaction training epoch: 180, train loss: 0.83581, val loss: 0.84194\n",
      "Interaction training epoch: 181, train loss: 0.82044, val loss: 0.82287\n",
      "Interaction training epoch: 182, train loss: 0.82135, val loss: 0.82401\n",
      "Interaction training epoch: 183, train loss: 0.82940, val loss: 0.82729\n",
      "Interaction training epoch: 184, train loss: 0.82159, val loss: 0.82257\n",
      "Interaction training epoch: 185, train loss: 0.82134, val loss: 0.81587\n",
      "Interaction training epoch: 186, train loss: 0.82022, val loss: 0.81890\n",
      "Interaction training epoch: 187, train loss: 0.82263, val loss: 0.82265\n",
      "Interaction training epoch: 188, train loss: 0.82953, val loss: 0.83362\n",
      "Interaction training epoch: 189, train loss: 0.83232, val loss: 0.83031\n",
      "Interaction training epoch: 190, train loss: 0.81332, val loss: 0.81501\n",
      "Interaction training epoch: 191, train loss: 0.81952, val loss: 0.82189\n",
      "Interaction training epoch: 192, train loss: 0.82166, val loss: 0.82530\n",
      "Interaction training epoch: 193, train loss: 0.83151, val loss: 0.84141\n",
      "Interaction training epoch: 194, train loss: 0.82638, val loss: 0.81517\n",
      "Interaction training epoch: 195, train loss: 0.83199, val loss: 0.82848\n",
      "Interaction training epoch: 196, train loss: 0.82739, val loss: 0.83803\n",
      "Interaction training epoch: 197, train loss: 0.82264, val loss: 0.82194\n",
      "Interaction training epoch: 198, train loss: 0.82310, val loss: 0.83420\n",
      "Interaction training epoch: 199, train loss: 0.81675, val loss: 0.81584\n",
      "Interaction training epoch: 200, train loss: 0.83348, val loss: 0.82797\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########1 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.82371, val loss: 0.82202\n",
      "Interaction tuning epoch: 2, train loss: 0.82096, val loss: 0.82463\n",
      "Interaction tuning epoch: 3, train loss: 0.81892, val loss: 0.82484\n",
      "Interaction tuning epoch: 4, train loss: 0.82479, val loss: 0.82251\n",
      "Interaction tuning epoch: 5, train loss: 0.81839, val loss: 0.82672\n",
      "Interaction tuning epoch: 6, train loss: 0.81689, val loss: 0.81462\n",
      "Interaction tuning epoch: 7, train loss: 0.81898, val loss: 0.82051\n",
      "Interaction tuning epoch: 8, train loss: 0.81312, val loss: 0.81754\n",
      "Interaction tuning epoch: 9, train loss: 0.81539, val loss: 0.80975\n",
      "Interaction tuning epoch: 10, train loss: 0.81745, val loss: 0.82210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 11, train loss: 0.81418, val loss: 0.82079\n",
      "Interaction tuning epoch: 12, train loss: 0.82485, val loss: 0.82715\n",
      "Interaction tuning epoch: 13, train loss: 0.81825, val loss: 0.81712\n",
      "Interaction tuning epoch: 14, train loss: 0.82252, val loss: 0.82478\n",
      "Interaction tuning epoch: 15, train loss: 0.82245, val loss: 0.82111\n",
      "Interaction tuning epoch: 16, train loss: 0.81510, val loss: 0.81301\n",
      "Interaction tuning epoch: 17, train loss: 0.84084, val loss: 0.84370\n",
      "Interaction tuning epoch: 18, train loss: 0.82361, val loss: 0.83073\n",
      "Interaction tuning epoch: 19, train loss: 0.81988, val loss: 0.81411\n",
      "Interaction tuning epoch: 20, train loss: 0.82053, val loss: 0.82244\n",
      "Interaction tuning epoch: 21, train loss: 0.81551, val loss: 0.81523\n",
      "Interaction tuning epoch: 22, train loss: 0.82296, val loss: 0.82491\n",
      "Interaction tuning epoch: 23, train loss: 0.81978, val loss: 0.81538\n",
      "Interaction tuning epoch: 24, train loss: 0.81054, val loss: 0.81135\n",
      "Interaction tuning epoch: 25, train loss: 0.81548, val loss: 0.81068\n",
      "Interaction tuning epoch: 26, train loss: 0.81618, val loss: 0.81822\n",
      "Interaction tuning epoch: 27, train loss: 0.80571, val loss: 0.81150\n",
      "Interaction tuning epoch: 28, train loss: 0.81969, val loss: 0.82267\n",
      "Interaction tuning epoch: 29, train loss: 0.81576, val loss: 0.80756\n",
      "Interaction tuning epoch: 30, train loss: 0.81227, val loss: 0.81788\n",
      "Interaction tuning epoch: 31, train loss: 0.82466, val loss: 0.81669\n",
      "Interaction tuning epoch: 32, train loss: 0.82289, val loss: 0.82492\n",
      "Interaction tuning epoch: 33, train loss: 0.81647, val loss: 0.81767\n",
      "Interaction tuning epoch: 34, train loss: 0.82142, val loss: 0.82184\n",
      "Interaction tuning epoch: 35, train loss: 0.84295, val loss: 0.83084\n",
      "Interaction tuning epoch: 36, train loss: 0.81647, val loss: 0.81656\n",
      "Interaction tuning epoch: 37, train loss: 0.81434, val loss: 0.80192\n",
      "Interaction tuning epoch: 38, train loss: 0.81647, val loss: 0.81865\n",
      "Interaction tuning epoch: 39, train loss: 0.80969, val loss: 0.80605\n",
      "Interaction tuning epoch: 40, train loss: 0.81152, val loss: 0.81092\n",
      "Interaction tuning epoch: 41, train loss: 0.81000, val loss: 0.80475\n",
      "Interaction tuning epoch: 42, train loss: 0.80874, val loss: 0.80624\n",
      "Interaction tuning epoch: 43, train loss: 0.80997, val loss: 0.80494\n",
      "Interaction tuning epoch: 44, train loss: 0.81356, val loss: 0.80658\n",
      "Interaction tuning epoch: 45, train loss: 0.81452, val loss: 0.81623\n",
      "Interaction tuning epoch: 46, train loss: 0.80775, val loss: 0.81179\n",
      "Interaction tuning epoch: 47, train loss: 0.81477, val loss: 0.80927\n",
      "Interaction tuning epoch: 48, train loss: 0.81802, val loss: 0.81413\n",
      "Interaction tuning epoch: 49, train loss: 0.81307, val loss: 0.81744\n",
      "Interaction tuning epoch: 50, train loss: 0.81885, val loss: 0.80675\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 40.53660321235657\n",
      "After the gam stage, training error is 0.81885 , validation error is 0.80675\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 20.277104\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.677466 validation MAE=0.762817,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 2: observed MAE=0.631641 validation MAE=0.742025,rank=5\n",
      "[SoftImpute] Iter 3: observed MAE=0.593323 validation MAE=0.724163,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.560033 validation MAE=0.708986,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 5: observed MAE=0.532037 validation MAE=0.695235,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.509045 validation MAE=0.682666,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.487951 validation MAE=0.671388,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.468989 validation MAE=0.661021,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.451511 validation MAE=0.651702,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.435278 validation MAE=0.643039,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.420787 validation MAE=0.634938,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.407194 validation MAE=0.627773,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.396107 validation MAE=0.621979,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.385311 validation MAE=0.615556,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.375958 validation MAE=0.611214,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.366296 validation MAE=0.606014,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.357968 validation MAE=0.602142,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.350123 validation MAE=0.597691,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.343909 validation MAE=0.593553,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.335922 validation MAE=0.589237,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.330294 validation MAE=0.586124,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.325343 validation MAE=0.582573,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.319956 validation MAE=0.579318,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.314770 validation MAE=0.575836,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.311036 validation MAE=0.573176,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.305761 validation MAE=0.570632,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.301848 validation MAE=0.567893,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.298431 validation MAE=0.565390,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.294946 validation MAE=0.562981,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.290449 validation MAE=0.560578,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.287957 validation MAE=0.558251,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.284309 validation MAE=0.556597,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.281485 validation MAE=0.554334,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.277871 validation MAE=0.552345,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.275569 validation MAE=0.550438,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.273201 validation MAE=0.548497,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.270851 validation MAE=0.546522,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.268774 validation MAE=0.545358,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.265852 validation MAE=0.543460,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.264672 validation MAE=0.542217,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.262287 validation MAE=0.540602,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.260090 validation MAE=0.539174,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.257457 validation MAE=0.537220,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.255774 validation MAE=0.536489,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.253897 validation MAE=0.535409,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.253279 validation MAE=0.534891,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.251254 validation MAE=0.533552,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.249347 validation MAE=0.531793,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.247918 validation MAE=0.531173,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.246600 validation MAE=0.530296,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.244774 validation MAE=0.529206,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.243992 validation MAE=0.528272,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.241688 validation MAE=0.527827,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.241035 validation MAE=0.526468,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.241827 validation MAE=0.525367,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.243544 validation MAE=0.524349,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.243527 validation MAE=0.522610,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.243752 validation MAE=0.521778,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.242217 validation MAE=0.520368,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.240393 validation MAE=0.518576,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.239962 validation MAE=0.518719,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.238165 validation MAE=0.517314,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.236569 validation MAE=0.516018,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.234373 validation MAE=0.515606,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.233599 validation MAE=0.514824,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.232233 validation MAE=0.513920,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.231524 validation MAE=0.513277,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.229765 validation MAE=0.512464,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.229083 validation MAE=0.511984,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.228961 validation MAE=0.510689,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.227900 validation MAE=0.510432,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.226756 validation MAE=0.509841,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.226104 validation MAE=0.508903,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.223893 validation MAE=0.508197,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.222874 validation MAE=0.507186,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.223179 validation MAE=0.507033,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.222431 validation MAE=0.506272,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.220613 validation MAE=0.505531,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.220349 validation MAE=0.504494,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.218682 validation MAE=0.504034,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.218140 validation MAE=0.503757,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.217021 validation MAE=0.502910,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.216150 validation MAE=0.502771,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.214637 validation MAE=0.501846,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.214531 validation MAE=0.501914,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.213663 validation MAE=0.500161,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.212733 validation MAE=0.500438,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.211107 validation MAE=0.499776,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.211218 validation MAE=0.499306,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.210908 validation MAE=0.497894,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.211016 validation MAE=0.497996,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.210405 validation MAE=0.498582,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.210929 validation MAE=0.497558,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.209730 validation MAE=0.497375,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.208997 validation MAE=0.496363,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.207817 validation MAE=0.495577,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.207405 validation MAE=0.495305,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.206791 validation MAE=0.494695,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.205488 validation MAE=0.494264,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.204722 validation MAE=0.494031,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.204990 validation MAE=0.494018,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.204401 validation MAE=0.493422,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.203512 validation MAE=0.492781,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.203435 validation MAE=0.492109,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.203173 validation MAE=0.491922,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.203246 validation MAE=0.491435,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.201885 validation MAE=0.491244,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.200792 validation MAE=0.491055,rank=5\n",
      "[SoftImpute] Iter 109: observed MAE=0.201226 validation MAE=0.490614,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.199711 validation MAE=0.489851,rank=5\n",
      "[SoftImpute] Iter 111: observed MAE=0.199317 validation MAE=0.489058,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.200119 validation MAE=0.489674,rank=5\n",
      "[SoftImpute] Iter 113: observed MAE=0.199100 validation MAE=0.488838,rank=5\n",
      "[SoftImpute] Iter 114: observed MAE=0.198370 validation MAE=0.488277,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 115: observed MAE=0.197825 validation MAE=0.488360,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.197671 validation MAE=0.487636,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.196960 validation MAE=0.487665,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.198141 validation MAE=0.487226,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.196985 validation MAE=0.487297,rank=5\n",
      "[SoftImpute] Iter 120: observed MAE=0.196073 validation MAE=0.486712,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.196114 validation MAE=0.486223,rank=5\n",
      "[SoftImpute] Iter 122: observed MAE=0.195743 validation MAE=0.485752,rank=5\n",
      "[SoftImpute] Iter 123: observed MAE=0.195320 validation MAE=0.486015,rank=5\n",
      "[SoftImpute] Iter 124: observed MAE=0.195213 validation MAE=0.485716,rank=5\n",
      "[SoftImpute] Iter 125: observed MAE=0.194142 validation MAE=0.484614,rank=5\n",
      "[SoftImpute] Iter 126: observed MAE=0.193958 validation MAE=0.484824,rank=5\n",
      "[SoftImpute] Iter 127: observed MAE=0.193849 validation MAE=0.484994,rank=5\n",
      "[SoftImpute] Iter 128: observed MAE=0.194621 validation MAE=0.484562,rank=5\n",
      "[SoftImpute] Iter 129: observed MAE=0.193415 validation MAE=0.483821,rank=5\n",
      "[SoftImpute] Iter 130: observed MAE=0.192913 validation MAE=0.483461,rank=5\n",
      "[SoftImpute] Iter 131: observed MAE=0.192740 validation MAE=0.483533,rank=5\n",
      "[SoftImpute] Iter 132: observed MAE=0.192407 validation MAE=0.482947,rank=5\n",
      "[SoftImpute] Iter 133: observed MAE=0.192587 validation MAE=0.482640,rank=5\n",
      "[SoftImpute] Iter 134: observed MAE=0.192469 validation MAE=0.482456,rank=5\n",
      "[SoftImpute] Iter 135: observed MAE=0.191340 validation MAE=0.481980,rank=5\n",
      "[SoftImpute] Iter 136: observed MAE=0.191503 validation MAE=0.481816,rank=5\n",
      "[SoftImpute] Iter 137: observed MAE=0.191189 validation MAE=0.481414,rank=5\n",
      "[SoftImpute] Iter 138: observed MAE=0.190161 validation MAE=0.481304,rank=5\n",
      "[SoftImpute] Iter 139: observed MAE=0.190796 validation MAE=0.480974,rank=5\n",
      "[SoftImpute] Iter 140: observed MAE=0.190415 validation MAE=0.481246,rank=5\n",
      "[SoftImpute] Iter 141: observed MAE=0.190390 validation MAE=0.480566,rank=5\n",
      "[SoftImpute] Iter 142: observed MAE=0.190334 validation MAE=0.479718,rank=5\n",
      "[SoftImpute] Iter 143: observed MAE=0.189298 validation MAE=0.479806,rank=5\n",
      "[SoftImpute] Iter 144: observed MAE=0.189343 validation MAE=0.479237,rank=5\n",
      "[SoftImpute] Iter 145: observed MAE=0.189726 validation MAE=0.478941,rank=5\n",
      "[SoftImpute] Iter 146: observed MAE=0.188714 validation MAE=0.479151,rank=5\n",
      "[SoftImpute] Iter 147: observed MAE=0.188989 validation MAE=0.479141,rank=5\n",
      "[SoftImpute] Iter 148: observed MAE=0.189167 validation MAE=0.478459,rank=5\n",
      "[SoftImpute] Iter 149: observed MAE=0.188363 validation MAE=0.478021,rank=5\n",
      "[SoftImpute] Iter 150: observed MAE=0.187624 validation MAE=0.478063,rank=5\n",
      "[SoftImpute] Iter 151: observed MAE=0.187870 validation MAE=0.477543,rank=5\n",
      "[SoftImpute] Iter 152: observed MAE=0.187305 validation MAE=0.477844,rank=5\n",
      "[SoftImpute] Iter 153: observed MAE=0.187385 validation MAE=0.477550,rank=5\n",
      "[SoftImpute] Iter 154: observed MAE=0.187301 validation MAE=0.477488,rank=5\n",
      "[SoftImpute] Iter 155: observed MAE=0.186275 validation MAE=0.477039,rank=5\n",
      "[SoftImpute] Iter 156: observed MAE=0.186543 validation MAE=0.476120,rank=5\n",
      "[SoftImpute] Iter 157: observed MAE=0.186583 validation MAE=0.475496,rank=5\n",
      "[SoftImpute] Iter 158: observed MAE=0.186517 validation MAE=0.476000,rank=5\n",
      "[SoftImpute] Iter 159: observed MAE=0.186459 validation MAE=0.476013,rank=5\n",
      "[SoftImpute] Iter 160: observed MAE=0.186014 validation MAE=0.475694,rank=5\n",
      "[SoftImpute] Iter 161: observed MAE=0.185535 validation MAE=0.475743,rank=5\n",
      "[SoftImpute] Iter 162: observed MAE=0.185050 validation MAE=0.474723,rank=5\n",
      "[SoftImpute] Iter 163: observed MAE=0.185452 validation MAE=0.474177,rank=5\n",
      "[SoftImpute] Iter 164: observed MAE=0.185501 validation MAE=0.474842,rank=5\n",
      "[SoftImpute] Iter 165: observed MAE=0.185851 validation MAE=0.474431,rank=5\n",
      "[SoftImpute] Iter 166: observed MAE=0.185186 validation MAE=0.473784,rank=5\n",
      "[SoftImpute] Iter 167: observed MAE=0.184734 validation MAE=0.473886,rank=5\n",
      "[SoftImpute] Iter 168: observed MAE=0.184446 validation MAE=0.473515,rank=5\n",
      "[SoftImpute] Iter 169: observed MAE=0.184629 validation MAE=0.472866,rank=5\n",
      "[SoftImpute] Iter 170: observed MAE=0.184100 validation MAE=0.473247,rank=5\n",
      "[SoftImpute] Iter 171: observed MAE=0.183746 validation MAE=0.473141,rank=5\n",
      "[SoftImpute] Iter 172: observed MAE=0.183427 validation MAE=0.472748,rank=5\n",
      "[SoftImpute] Iter 173: observed MAE=0.183182 validation MAE=0.472427,rank=5\n",
      "[SoftImpute] Iter 174: observed MAE=0.183290 validation MAE=0.472228,rank=5\n",
      "[SoftImpute] Iter 175: observed MAE=0.183594 validation MAE=0.471832,rank=5\n",
      "[SoftImpute] Iter 176: observed MAE=0.183619 validation MAE=0.471785,rank=5\n",
      "[SoftImpute] Iter 177: observed MAE=0.183079 validation MAE=0.471352,rank=5\n",
      "[SoftImpute] Iter 178: observed MAE=0.182585 validation MAE=0.470744,rank=5\n",
      "[SoftImpute] Iter 179: observed MAE=0.182062 validation MAE=0.470775,rank=5\n",
      "[SoftImpute] Iter 180: observed MAE=0.182503 validation MAE=0.470920,rank=5\n",
      "[SoftImpute] Iter 181: observed MAE=0.183091 validation MAE=0.470705,rank=5\n",
      "[SoftImpute] Iter 182: observed MAE=0.182565 validation MAE=0.470886,rank=5\n",
      "[SoftImpute] Iter 183: observed MAE=0.182126 validation MAE=0.469939,rank=5\n",
      "[SoftImpute] Iter 184: observed MAE=0.181792 validation MAE=0.469434,rank=5\n",
      "[SoftImpute] Iter 185: observed MAE=0.181170 validation MAE=0.469293,rank=5\n",
      "[SoftImpute] Iter 186: observed MAE=0.181421 validation MAE=0.469238,rank=5\n",
      "[SoftImpute] Iter 187: observed MAE=0.181900 validation MAE=0.468870,rank=5\n",
      "[SoftImpute] Iter 188: observed MAE=0.181684 validation MAE=0.468697,rank=5\n",
      "[SoftImpute] Iter 189: observed MAE=0.181485 validation MAE=0.468951,rank=5\n",
      "[SoftImpute] Iter 190: observed MAE=0.181219 validation MAE=0.468742,rank=5\n",
      "[SoftImpute] Iter 191: observed MAE=0.180560 validation MAE=0.468142,rank=5\n",
      "[SoftImpute] Iter 192: observed MAE=0.180215 validation MAE=0.467887,rank=5\n",
      "[SoftImpute] Iter 193: observed MAE=0.180329 validation MAE=0.468058,rank=5\n",
      "[SoftImpute] Iter 194: observed MAE=0.179735 validation MAE=0.467720,rank=5\n",
      "[SoftImpute] Iter 195: observed MAE=0.179331 validation MAE=0.467792,rank=5\n",
      "[SoftImpute] Iter 196: observed MAE=0.179931 validation MAE=0.467487,rank=5\n",
      "[SoftImpute] Iter 197: observed MAE=0.179995 validation MAE=0.467048,rank=5\n",
      "[SoftImpute] Iter 198: observed MAE=0.179488 validation MAE=0.467073,rank=5\n",
      "[SoftImpute] Iter 199: observed MAE=0.180410 validation MAE=0.467181,rank=5\n",
      "[SoftImpute] Iter 200: observed MAE=0.179472 validation MAE=0.467084,rank=5\n",
      "[SoftImpute] Stopped after iteration 200 for lambda=0.405542\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 8.380686283111572\n",
      "After the matrix factor stage, training error is 0.17947, validation error is 0.46708\n",
      "1\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.05604, val loss: 4.08078\n",
      "Main effects training epoch: 2, train loss: 3.89375, val loss: 3.92503\n",
      "Main effects training epoch: 3, train loss: 3.69001, val loss: 3.72899\n",
      "Main effects training epoch: 4, train loss: 3.47998, val loss: 3.51294\n",
      "Main effects training epoch: 5, train loss: 3.32494, val loss: 3.36613\n",
      "Main effects training epoch: 6, train loss: 3.38941, val loss: 3.44246\n",
      "Main effects training epoch: 7, train loss: 3.27954, val loss: 3.34646\n",
      "Main effects training epoch: 8, train loss: 3.27897, val loss: 3.32979\n",
      "Main effects training epoch: 9, train loss: 3.19258, val loss: 3.26076\n",
      "Main effects training epoch: 10, train loss: 3.12273, val loss: 3.17725\n",
      "Main effects training epoch: 11, train loss: 3.04303, val loss: 3.09643\n",
      "Main effects training epoch: 12, train loss: 3.04014, val loss: 3.09778\n",
      "Main effects training epoch: 13, train loss: 3.03301, val loss: 3.08403\n",
      "Main effects training epoch: 14, train loss: 2.92412, val loss: 2.97179\n",
      "Main effects training epoch: 15, train loss: 2.90135, val loss: 2.94851\n",
      "Main effects training epoch: 16, train loss: 2.86251, val loss: 2.90852\n",
      "Main effects training epoch: 17, train loss: 2.77854, val loss: 2.82476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 18, train loss: 2.70137, val loss: 2.73679\n",
      "Main effects training epoch: 19, train loss: 2.65428, val loss: 2.71479\n",
      "Main effects training epoch: 20, train loss: 2.62348, val loss: 2.66730\n",
      "Main effects training epoch: 21, train loss: 2.58159, val loss: 2.61154\n",
      "Main effects training epoch: 22, train loss: 2.52616, val loss: 2.56419\n",
      "Main effects training epoch: 23, train loss: 2.49303, val loss: 2.52300\n",
      "Main effects training epoch: 24, train loss: 2.48642, val loss: 2.51362\n",
      "Main effects training epoch: 25, train loss: 2.44158, val loss: 2.44936\n",
      "Main effects training epoch: 26, train loss: 2.43460, val loss: 2.47889\n",
      "Main effects training epoch: 27, train loss: 2.38821, val loss: 2.39719\n",
      "Main effects training epoch: 28, train loss: 2.33429, val loss: 2.35618\n",
      "Main effects training epoch: 29, train loss: 2.30210, val loss: 2.32331\n",
      "Main effects training epoch: 30, train loss: 2.26689, val loss: 2.28515\n",
      "Main effects training epoch: 31, train loss: 2.25989, val loss: 2.27681\n",
      "Main effects training epoch: 32, train loss: 2.21890, val loss: 2.24437\n",
      "Main effects training epoch: 33, train loss: 2.19132, val loss: 2.19778\n",
      "Main effects training epoch: 34, train loss: 2.17766, val loss: 2.19876\n",
      "Main effects training epoch: 35, train loss: 2.16409, val loss: 2.17541\n",
      "Main effects training epoch: 36, train loss: 2.14414, val loss: 2.16131\n",
      "Main effects training epoch: 37, train loss: 2.13646, val loss: 2.15225\n",
      "Main effects training epoch: 38, train loss: 2.08119, val loss: 2.09608\n",
      "Main effects training epoch: 39, train loss: 2.08411, val loss: 2.09968\n",
      "Main effects training epoch: 40, train loss: 2.05534, val loss: 2.07621\n",
      "Main effects training epoch: 41, train loss: 2.04171, val loss: 2.05828\n",
      "Main effects training epoch: 42, train loss: 2.01885, val loss: 2.03461\n",
      "Main effects training epoch: 43, train loss: 2.00833, val loss: 2.02689\n",
      "Main effects training epoch: 44, train loss: 1.98233, val loss: 1.99997\n",
      "Main effects training epoch: 45, train loss: 1.96060, val loss: 1.97363\n",
      "Main effects training epoch: 46, train loss: 1.97293, val loss: 1.98702\n",
      "Main effects training epoch: 47, train loss: 1.93617, val loss: 1.94357\n",
      "Main effects training epoch: 48, train loss: 1.92075, val loss: 1.93242\n",
      "Main effects training epoch: 49, train loss: 1.93248, val loss: 1.94323\n",
      "Main effects training epoch: 50, train loss: 1.89717, val loss: 1.90760\n",
      "Main effects training epoch: 51, train loss: 1.88952, val loss: 1.89783\n",
      "Main effects training epoch: 52, train loss: 1.91336, val loss: 1.92758\n",
      "Main effects training epoch: 53, train loss: 1.85572, val loss: 1.86008\n",
      "Main effects training epoch: 54, train loss: 1.88401, val loss: 1.89607\n",
      "Main effects training epoch: 55, train loss: 1.86717, val loss: 1.87552\n",
      "Main effects training epoch: 56, train loss: 1.84449, val loss: 1.84824\n",
      "Main effects training epoch: 57, train loss: 1.84267, val loss: 1.84882\n",
      "Main effects training epoch: 58, train loss: 1.84866, val loss: 1.85631\n",
      "Main effects training epoch: 59, train loss: 1.83850, val loss: 1.84255\n",
      "Main effects training epoch: 60, train loss: 1.82016, val loss: 1.82446\n",
      "Main effects training epoch: 61, train loss: 1.82311, val loss: 1.82335\n",
      "Main effects training epoch: 62, train loss: 1.82256, val loss: 1.83250\n",
      "Main effects training epoch: 63, train loss: 1.81371, val loss: 1.81566\n",
      "Main effects training epoch: 64, train loss: 1.79982, val loss: 1.79850\n",
      "Main effects training epoch: 65, train loss: 1.80782, val loss: 1.80985\n",
      "Main effects training epoch: 66, train loss: 1.79642, val loss: 1.79329\n",
      "Main effects training epoch: 67, train loss: 1.78851, val loss: 1.78264\n",
      "Main effects training epoch: 68, train loss: 1.79210, val loss: 1.78924\n",
      "Main effects training epoch: 69, train loss: 1.78455, val loss: 1.77891\n",
      "Main effects training epoch: 70, train loss: 1.78199, val loss: 1.77296\n",
      "Main effects training epoch: 71, train loss: 1.77722, val loss: 1.76991\n",
      "Main effects training epoch: 72, train loss: 1.77620, val loss: 1.76784\n",
      "Main effects training epoch: 73, train loss: 1.78070, val loss: 1.77447\n",
      "Main effects training epoch: 74, train loss: 1.77308, val loss: 1.76526\n",
      "Main effects training epoch: 75, train loss: 1.76781, val loss: 1.75885\n",
      "Main effects training epoch: 76, train loss: 1.76509, val loss: 1.75377\n",
      "Main effects training epoch: 77, train loss: 1.76105, val loss: 1.74902\n",
      "Main effects training epoch: 78, train loss: 1.76191, val loss: 1.74999\n",
      "Main effects training epoch: 79, train loss: 1.75755, val loss: 1.74065\n",
      "Main effects training epoch: 80, train loss: 1.75286, val loss: 1.74062\n",
      "Main effects training epoch: 81, train loss: 1.75342, val loss: 1.73829\n",
      "Main effects training epoch: 82, train loss: 1.74906, val loss: 1.73336\n",
      "Main effects training epoch: 83, train loss: 1.74560, val loss: 1.73613\n",
      "Main effects training epoch: 84, train loss: 1.73022, val loss: 1.71703\n",
      "Main effects training epoch: 85, train loss: 1.71926, val loss: 1.71142\n",
      "Main effects training epoch: 86, train loss: 1.70987, val loss: 1.69709\n",
      "Main effects training epoch: 87, train loss: 1.69822, val loss: 1.69694\n",
      "Main effects training epoch: 88, train loss: 1.69821, val loss: 1.69083\n",
      "Main effects training epoch: 89, train loss: 1.69404, val loss: 1.69016\n",
      "Main effects training epoch: 90, train loss: 1.69014, val loss: 1.68094\n",
      "Main effects training epoch: 91, train loss: 1.68261, val loss: 1.66868\n",
      "Main effects training epoch: 92, train loss: 1.68587, val loss: 1.67618\n",
      "Main effects training epoch: 93, train loss: 1.67892, val loss: 1.66997\n",
      "Main effects training epoch: 94, train loss: 1.67371, val loss: 1.65904\n",
      "Main effects training epoch: 95, train loss: 1.66687, val loss: 1.65084\n",
      "Main effects training epoch: 96, train loss: 1.66642, val loss: 1.64468\n",
      "Main effects training epoch: 97, train loss: 1.66357, val loss: 1.65279\n",
      "Main effects training epoch: 98, train loss: 1.65835, val loss: 1.63371\n",
      "Main effects training epoch: 99, train loss: 1.66043, val loss: 1.64563\n",
      "Main effects training epoch: 100, train loss: 1.65850, val loss: 1.64190\n",
      "Main effects training epoch: 101, train loss: 1.64485, val loss: 1.61512\n",
      "Main effects training epoch: 102, train loss: 1.64014, val loss: 1.62942\n",
      "Main effects training epoch: 103, train loss: 1.63890, val loss: 1.61380\n",
      "Main effects training epoch: 104, train loss: 1.63340, val loss: 1.60897\n",
      "Main effects training epoch: 105, train loss: 1.63989, val loss: 1.62959\n",
      "Main effects training epoch: 106, train loss: 1.63629, val loss: 1.61276\n",
      "Main effects training epoch: 107, train loss: 1.64154, val loss: 1.62246\n",
      "Main effects training epoch: 108, train loss: 1.63023, val loss: 1.60771\n",
      "Main effects training epoch: 109, train loss: 1.63178, val loss: 1.61847\n",
      "Main effects training epoch: 110, train loss: 1.62661, val loss: 1.60721\n",
      "Main effects training epoch: 111, train loss: 1.62581, val loss: 1.60291\n",
      "Main effects training epoch: 112, train loss: 1.63235, val loss: 1.61450\n",
      "Main effects training epoch: 113, train loss: 1.62143, val loss: 1.60221\n",
      "Main effects training epoch: 114, train loss: 1.62407, val loss: 1.61167\n",
      "Main effects training epoch: 115, train loss: 1.62939, val loss: 1.61524\n",
      "Main effects training epoch: 116, train loss: 1.62267, val loss: 1.60120\n",
      "Main effects training epoch: 117, train loss: 1.63551, val loss: 1.63102\n",
      "Main effects training epoch: 118, train loss: 1.62773, val loss: 1.59745\n",
      "Main effects training epoch: 119, train loss: 1.62433, val loss: 1.61365\n",
      "Main effects training epoch: 120, train loss: 1.62349, val loss: 1.60305\n",
      "Main effects training epoch: 121, train loss: 1.62999, val loss: 1.60935\n",
      "Main effects training epoch: 122, train loss: 1.62585, val loss: 1.60367\n",
      "Main effects training epoch: 123, train loss: 1.62222, val loss: 1.61396\n",
      "Main effects training epoch: 124, train loss: 1.62559, val loss: 1.61171\n",
      "Main effects training epoch: 125, train loss: 1.61669, val loss: 1.59516\n",
      "Main effects training epoch: 126, train loss: 1.62170, val loss: 1.60457\n",
      "Main effects training epoch: 127, train loss: 1.62773, val loss: 1.61321\n",
      "Main effects training epoch: 128, train loss: 1.62869, val loss: 1.62581\n",
      "Main effects training epoch: 129, train loss: 1.62064, val loss: 1.59864\n",
      "Main effects training epoch: 130, train loss: 1.61465, val loss: 1.59633\n",
      "Main effects training epoch: 131, train loss: 1.61704, val loss: 1.60358\n",
      "Main effects training epoch: 132, train loss: 1.61654, val loss: 1.60068\n",
      "Main effects training epoch: 133, train loss: 1.61067, val loss: 1.59585\n",
      "Main effects training epoch: 134, train loss: 1.61213, val loss: 1.59903\n",
      "Main effects training epoch: 135, train loss: 1.61258, val loss: 1.59987\n",
      "Main effects training epoch: 136, train loss: 1.61868, val loss: 1.60761\n",
      "Main effects training epoch: 137, train loss: 1.61127, val loss: 1.59038\n",
      "Main effects training epoch: 138, train loss: 1.61509, val loss: 1.60660\n",
      "Main effects training epoch: 139, train loss: 1.61553, val loss: 1.59099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 140, train loss: 1.61901, val loss: 1.61243\n",
      "Main effects training epoch: 141, train loss: 1.61592, val loss: 1.59993\n",
      "Main effects training epoch: 142, train loss: 1.62053, val loss: 1.61015\n",
      "Main effects training epoch: 143, train loss: 1.60533, val loss: 1.59228\n",
      "Main effects training epoch: 144, train loss: 1.61192, val loss: 1.59760\n",
      "Main effects training epoch: 145, train loss: 1.60662, val loss: 1.58940\n",
      "Main effects training epoch: 146, train loss: 1.60656, val loss: 1.59396\n",
      "Main effects training epoch: 147, train loss: 1.60461, val loss: 1.58279\n",
      "Main effects training epoch: 148, train loss: 1.60598, val loss: 1.60250\n",
      "Main effects training epoch: 149, train loss: 1.60860, val loss: 1.59245\n",
      "Main effects training epoch: 150, train loss: 1.60561, val loss: 1.60119\n",
      "Main effects training epoch: 151, train loss: 1.61062, val loss: 1.58291\n",
      "Main effects training epoch: 152, train loss: 1.60217, val loss: 1.59084\n",
      "Main effects training epoch: 153, train loss: 1.60265, val loss: 1.59554\n",
      "Main effects training epoch: 154, train loss: 1.60313, val loss: 1.58652\n",
      "Main effects training epoch: 155, train loss: 1.59893, val loss: 1.58367\n",
      "Main effects training epoch: 156, train loss: 1.60276, val loss: 1.58940\n",
      "Main effects training epoch: 157, train loss: 1.60364, val loss: 1.59174\n",
      "Main effects training epoch: 158, train loss: 1.60445, val loss: 1.59455\n",
      "Main effects training epoch: 159, train loss: 1.60657, val loss: 1.58598\n",
      "Main effects training epoch: 160, train loss: 1.59408, val loss: 1.58156\n",
      "Main effects training epoch: 161, train loss: 1.59541, val loss: 1.58766\n",
      "Main effects training epoch: 162, train loss: 1.59548, val loss: 1.57080\n",
      "Main effects training epoch: 163, train loss: 1.59767, val loss: 1.58705\n",
      "Main effects training epoch: 164, train loss: 1.59539, val loss: 1.57870\n",
      "Main effects training epoch: 165, train loss: 1.59379, val loss: 1.58210\n",
      "Main effects training epoch: 166, train loss: 1.59786, val loss: 1.59261\n",
      "Main effects training epoch: 167, train loss: 1.60049, val loss: 1.58791\n",
      "Main effects training epoch: 168, train loss: 1.59947, val loss: 1.59423\n",
      "Main effects training epoch: 169, train loss: 1.60148, val loss: 1.58282\n",
      "Main effects training epoch: 170, train loss: 1.58880, val loss: 1.57174\n",
      "Main effects training epoch: 171, train loss: 1.58758, val loss: 1.58388\n",
      "Main effects training epoch: 172, train loss: 1.58771, val loss: 1.57492\n",
      "Main effects training epoch: 173, train loss: 1.58516, val loss: 1.57234\n",
      "Main effects training epoch: 174, train loss: 1.58827, val loss: 1.57042\n",
      "Main effects training epoch: 175, train loss: 1.58415, val loss: 1.57496\n",
      "Main effects training epoch: 176, train loss: 1.58569, val loss: 1.57890\n",
      "Main effects training epoch: 177, train loss: 1.58807, val loss: 1.57473\n",
      "Main effects training epoch: 178, train loss: 1.58082, val loss: 1.57589\n",
      "Main effects training epoch: 179, train loss: 1.57808, val loss: 1.56513\n",
      "Main effects training epoch: 180, train loss: 1.58601, val loss: 1.58904\n",
      "Main effects training epoch: 181, train loss: 1.58022, val loss: 1.57597\n",
      "Main effects training epoch: 182, train loss: 1.58452, val loss: 1.57815\n",
      "Main effects training epoch: 183, train loss: 1.58109, val loss: 1.57412\n",
      "Main effects training epoch: 184, train loss: 1.57859, val loss: 1.57033\n",
      "Main effects training epoch: 185, train loss: 1.58531, val loss: 1.58888\n",
      "Main effects training epoch: 186, train loss: 1.57155, val loss: 1.56442\n",
      "Main effects training epoch: 187, train loss: 1.58168, val loss: 1.56773\n",
      "Main effects training epoch: 188, train loss: 1.57210, val loss: 1.56682\n",
      "Main effects training epoch: 189, train loss: 1.56820, val loss: 1.56518\n",
      "Main effects training epoch: 190, train loss: 1.57835, val loss: 1.57483\n",
      "Main effects training epoch: 191, train loss: 1.58120, val loss: 1.58368\n",
      "Main effects training epoch: 192, train loss: 1.58336, val loss: 1.58289\n",
      "Main effects training epoch: 193, train loss: 1.55917, val loss: 1.55262\n",
      "Main effects training epoch: 194, train loss: 1.57001, val loss: 1.57349\n",
      "Main effects training epoch: 195, train loss: 1.55890, val loss: 1.55109\n",
      "Main effects training epoch: 196, train loss: 1.56428, val loss: 1.55444\n",
      "Main effects training epoch: 197, train loss: 1.56435, val loss: 1.56229\n",
      "Main effects training epoch: 198, train loss: 1.56940, val loss: 1.55974\n",
      "Main effects training epoch: 199, train loss: 1.56154, val loss: 1.56124\n",
      "Main effects training epoch: 200, train loss: 1.56032, val loss: 1.55366\n",
      "Main effects training epoch: 201, train loss: 1.55544, val loss: 1.55023\n",
      "Main effects training epoch: 202, train loss: 1.55175, val loss: 1.55254\n",
      "Main effects training epoch: 203, train loss: 1.56037, val loss: 1.57113\n",
      "Main effects training epoch: 204, train loss: 1.55350, val loss: 1.54586\n",
      "Main effects training epoch: 205, train loss: 1.55483, val loss: 1.55486\n",
      "Main effects training epoch: 206, train loss: 1.56210, val loss: 1.56033\n",
      "Main effects training epoch: 207, train loss: 1.55140, val loss: 1.54692\n",
      "Main effects training epoch: 208, train loss: 1.55359, val loss: 1.56090\n",
      "Main effects training epoch: 209, train loss: 1.55810, val loss: 1.57331\n",
      "Main effects training epoch: 210, train loss: 1.55094, val loss: 1.56064\n",
      "Main effects training epoch: 211, train loss: 1.54588, val loss: 1.54198\n",
      "Main effects training epoch: 212, train loss: 1.55186, val loss: 1.56319\n",
      "Main effects training epoch: 213, train loss: 1.54929, val loss: 1.55481\n",
      "Main effects training epoch: 214, train loss: 1.55737, val loss: 1.56352\n",
      "Main effects training epoch: 215, train loss: 1.58663, val loss: 1.60287\n",
      "Main effects training epoch: 216, train loss: 1.55308, val loss: 1.54827\n",
      "Main effects training epoch: 217, train loss: 1.56508, val loss: 1.57886\n",
      "Main effects training epoch: 218, train loss: 1.57294, val loss: 1.55785\n",
      "Main effects training epoch: 219, train loss: 1.57306, val loss: 1.58382\n",
      "Main effects training epoch: 220, train loss: 1.55345, val loss: 1.55259\n",
      "Main effects training epoch: 221, train loss: 1.55707, val loss: 1.56565\n",
      "Main effects training epoch: 222, train loss: 1.54662, val loss: 1.55196\n",
      "Main effects training epoch: 223, train loss: 1.55068, val loss: 1.56533\n",
      "Main effects training epoch: 224, train loss: 1.57140, val loss: 1.56256\n",
      "Main effects training epoch: 225, train loss: 1.55080, val loss: 1.55676\n",
      "Main effects training epoch: 226, train loss: 1.53824, val loss: 1.54293\n",
      "Main effects training epoch: 227, train loss: 1.54395, val loss: 1.55607\n",
      "Main effects training epoch: 228, train loss: 1.53990, val loss: 1.54567\n",
      "Main effects training epoch: 229, train loss: 1.54115, val loss: 1.55409\n",
      "Main effects training epoch: 230, train loss: 1.53543, val loss: 1.54763\n",
      "Main effects training epoch: 231, train loss: 1.57524, val loss: 1.58207\n",
      "Main effects training epoch: 232, train loss: 1.54102, val loss: 1.55675\n",
      "Main effects training epoch: 233, train loss: 1.53545, val loss: 1.54384\n",
      "Main effects training epoch: 234, train loss: 1.53790, val loss: 1.54115\n",
      "Main effects training epoch: 235, train loss: 1.54795, val loss: 1.55740\n",
      "Main effects training epoch: 236, train loss: 1.54701, val loss: 1.54448\n",
      "Main effects training epoch: 237, train loss: 1.54131, val loss: 1.56461\n",
      "Main effects training epoch: 238, train loss: 1.55582, val loss: 1.56375\n",
      "Main effects training epoch: 239, train loss: 1.53305, val loss: 1.54850\n",
      "Main effects training epoch: 240, train loss: 1.53308, val loss: 1.54327\n",
      "Main effects training epoch: 241, train loss: 1.52632, val loss: 1.53255\n",
      "Main effects training epoch: 242, train loss: 1.52875, val loss: 1.54120\n",
      "Main effects training epoch: 243, train loss: 1.53593, val loss: 1.54725\n",
      "Main effects training epoch: 244, train loss: 1.53077, val loss: 1.54525\n",
      "Main effects training epoch: 245, train loss: 1.53726, val loss: 1.54440\n",
      "Main effects training epoch: 246, train loss: 1.52560, val loss: 1.53156\n",
      "Main effects training epoch: 247, train loss: 1.51564, val loss: 1.52225\n",
      "Main effects training epoch: 248, train loss: 1.52912, val loss: 1.54209\n",
      "Main effects training epoch: 249, train loss: 1.52019, val loss: 1.52319\n",
      "Main effects training epoch: 250, train loss: 1.52007, val loss: 1.53844\n",
      "Main effects training epoch: 251, train loss: 1.52528, val loss: 1.53361\n",
      "Main effects training epoch: 252, train loss: 1.52127, val loss: 1.53854\n",
      "Main effects training epoch: 253, train loss: 1.52069, val loss: 1.54341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 254, train loss: 1.51561, val loss: 1.52176\n",
      "Main effects training epoch: 255, train loss: 1.50930, val loss: 1.52334\n",
      "Main effects training epoch: 256, train loss: 1.52238, val loss: 1.53852\n",
      "Main effects training epoch: 257, train loss: 1.51499, val loss: 1.52532\n",
      "Main effects training epoch: 258, train loss: 1.51897, val loss: 1.53797\n",
      "Main effects training epoch: 259, train loss: 1.50956, val loss: 1.51940\n",
      "Main effects training epoch: 260, train loss: 1.51077, val loss: 1.52802\n",
      "Main effects training epoch: 261, train loss: 1.50759, val loss: 1.51896\n",
      "Main effects training epoch: 262, train loss: 1.50740, val loss: 1.52935\n",
      "Main effects training epoch: 263, train loss: 1.51389, val loss: 1.52063\n",
      "Main effects training epoch: 264, train loss: 1.50575, val loss: 1.53010\n",
      "Main effects training epoch: 265, train loss: 1.51414, val loss: 1.52557\n",
      "Main effects training epoch: 266, train loss: 1.51319, val loss: 1.52922\n",
      "Main effects training epoch: 267, train loss: 1.50423, val loss: 1.52039\n",
      "Main effects training epoch: 268, train loss: 1.50729, val loss: 1.52232\n",
      "Main effects training epoch: 269, train loss: 1.50362, val loss: 1.51983\n",
      "Main effects training epoch: 270, train loss: 1.50201, val loss: 1.52134\n",
      "Main effects training epoch: 271, train loss: 1.50906, val loss: 1.53477\n",
      "Main effects training epoch: 272, train loss: 1.50669, val loss: 1.53502\n",
      "Main effects training epoch: 273, train loss: 1.50137, val loss: 1.52210\n",
      "Main effects training epoch: 274, train loss: 1.50101, val loss: 1.52291\n",
      "Main effects training epoch: 275, train loss: 1.50441, val loss: 1.52502\n",
      "Main effects training epoch: 276, train loss: 1.49854, val loss: 1.51900\n",
      "Main effects training epoch: 277, train loss: 1.49650, val loss: 1.51715\n",
      "Main effects training epoch: 278, train loss: 1.49554, val loss: 1.51392\n",
      "Main effects training epoch: 279, train loss: 1.49668, val loss: 1.52100\n",
      "Main effects training epoch: 280, train loss: 1.49660, val loss: 1.51337\n",
      "Main effects training epoch: 281, train loss: 1.49965, val loss: 1.52807\n",
      "Main effects training epoch: 282, train loss: 1.50074, val loss: 1.52221\n",
      "Main effects training epoch: 283, train loss: 1.50778, val loss: 1.54155\n",
      "Main effects training epoch: 284, train loss: 1.50767, val loss: 1.52478\n",
      "Main effects training epoch: 285, train loss: 1.50731, val loss: 1.53500\n",
      "Main effects training epoch: 286, train loss: 1.50447, val loss: 1.52872\n",
      "Main effects training epoch: 287, train loss: 1.49867, val loss: 1.52659\n",
      "Main effects training epoch: 288, train loss: 1.50344, val loss: 1.52692\n",
      "Main effects training epoch: 289, train loss: 1.49929, val loss: 1.52327\n",
      "Main effects training epoch: 290, train loss: 1.49291, val loss: 1.51833\n",
      "Main effects training epoch: 291, train loss: 1.49874, val loss: 1.53107\n",
      "Main effects training epoch: 292, train loss: 1.50628, val loss: 1.53387\n",
      "Main effects training epoch: 293, train loss: 1.50263, val loss: 1.52813\n",
      "Main effects training epoch: 294, train loss: 1.49916, val loss: 1.53046\n",
      "Main effects training epoch: 295, train loss: 1.50012, val loss: 1.52908\n",
      "Main effects training epoch: 296, train loss: 1.49290, val loss: 1.52100\n",
      "Main effects training epoch: 297, train loss: 1.49248, val loss: 1.51362\n",
      "Main effects training epoch: 298, train loss: 1.50020, val loss: 1.53178\n",
      "Main effects training epoch: 299, train loss: 1.50175, val loss: 1.51825\n",
      "Main effects training epoch: 300, train loss: 1.49510, val loss: 1.52693\n",
      "##########Stage 1: main effect training stop.##########\n",
      "2 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.49795, val loss: 1.51808\n",
      "Main effects tuning epoch: 2, train loss: 1.49796, val loss: 1.53053\n",
      "Main effects tuning epoch: 3, train loss: 1.49549, val loss: 1.52060\n",
      "Main effects tuning epoch: 4, train loss: 1.49633, val loss: 1.52398\n",
      "Main effects tuning epoch: 5, train loss: 1.49171, val loss: 1.52208\n",
      "Main effects tuning epoch: 6, train loss: 1.48976, val loss: 1.51760\n",
      "Main effects tuning epoch: 7, train loss: 1.49284, val loss: 1.51507\n",
      "Main effects tuning epoch: 8, train loss: 1.49142, val loss: 1.51745\n",
      "Main effects tuning epoch: 9, train loss: 1.49450, val loss: 1.51912\n",
      "Main effects tuning epoch: 10, train loss: 1.50252, val loss: 1.52888\n",
      "Main effects tuning epoch: 11, train loss: 1.50490, val loss: 1.54025\n",
      "Main effects tuning epoch: 12, train loss: 1.49143, val loss: 1.51004\n",
      "Main effects tuning epoch: 13, train loss: 1.49383, val loss: 1.53145\n",
      "Main effects tuning epoch: 14, train loss: 1.49793, val loss: 1.51127\n",
      "Main effects tuning epoch: 15, train loss: 1.48851, val loss: 1.51402\n",
      "Main effects tuning epoch: 16, train loss: 1.48493, val loss: 1.50388\n",
      "Main effects tuning epoch: 17, train loss: 1.49493, val loss: 1.52668\n",
      "Main effects tuning epoch: 18, train loss: 1.49498, val loss: 1.52785\n",
      "Main effects tuning epoch: 19, train loss: 1.49234, val loss: 1.52193\n",
      "Main effects tuning epoch: 20, train loss: 1.50698, val loss: 1.53544\n",
      "Main effects tuning epoch: 21, train loss: 1.48579, val loss: 1.51408\n",
      "Main effects tuning epoch: 22, train loss: 1.50159, val loss: 1.53360\n",
      "Main effects tuning epoch: 23, train loss: 1.48619, val loss: 1.50934\n",
      "Main effects tuning epoch: 24, train loss: 1.48580, val loss: 1.51672\n",
      "Main effects tuning epoch: 25, train loss: 1.48401, val loss: 1.51287\n",
      "Main effects tuning epoch: 26, train loss: 1.48621, val loss: 1.50901\n",
      "Main effects tuning epoch: 27, train loss: 1.49404, val loss: 1.52920\n",
      "Main effects tuning epoch: 28, train loss: 1.49141, val loss: 1.52194\n",
      "Main effects tuning epoch: 29, train loss: 1.48367, val loss: 1.52152\n",
      "Main effects tuning epoch: 30, train loss: 1.49263, val loss: 1.51915\n",
      "Main effects tuning epoch: 31, train loss: 1.48620, val loss: 1.50723\n",
      "Main effects tuning epoch: 32, train loss: 1.48100, val loss: 1.51445\n",
      "Main effects tuning epoch: 33, train loss: 1.48888, val loss: 1.52269\n",
      "Main effects tuning epoch: 34, train loss: 1.49167, val loss: 1.53315\n",
      "Main effects tuning epoch: 35, train loss: 1.48691, val loss: 1.52036\n",
      "Main effects tuning epoch: 36, train loss: 1.48398, val loss: 1.51624\n",
      "Main effects tuning epoch: 37, train loss: 1.48625, val loss: 1.52526\n",
      "Main effects tuning epoch: 38, train loss: 1.48597, val loss: 1.51854\n",
      "Main effects tuning epoch: 39, train loss: 1.49016, val loss: 1.51597\n",
      "Main effects tuning epoch: 40, train loss: 1.49214, val loss: 1.52909\n",
      "Main effects tuning epoch: 41, train loss: 1.48260, val loss: 1.50998\n",
      "Main effects tuning epoch: 42, train loss: 1.47784, val loss: 1.51230\n",
      "Main effects tuning epoch: 43, train loss: 1.48146, val loss: 1.51174\n",
      "Main effects tuning epoch: 44, train loss: 1.47242, val loss: 1.50549\n",
      "Main effects tuning epoch: 45, train loss: 1.47593, val loss: 1.51309\n",
      "Main effects tuning epoch: 46, train loss: 1.47438, val loss: 1.51457\n",
      "Main effects tuning epoch: 47, train loss: 1.47556, val loss: 1.50680\n",
      "Main effects tuning epoch: 48, train loss: 1.47326, val loss: 1.51206\n",
      "Main effects tuning epoch: 49, train loss: 1.47089, val loss: 1.49979\n",
      "Main effects tuning epoch: 50, train loss: 1.48147, val loss: 1.52752\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.43599, val loss: 1.48502\n",
      "Interaction training epoch: 2, train loss: 1.83888, val loss: 1.80253\n",
      "Interaction training epoch: 3, train loss: 1.14116, val loss: 1.17014\n",
      "Interaction training epoch: 4, train loss: 1.10400, val loss: 1.09373\n",
      "Interaction training epoch: 5, train loss: 1.06729, val loss: 1.09147\n",
      "Interaction training epoch: 6, train loss: 1.00877, val loss: 1.03738\n",
      "Interaction training epoch: 7, train loss: 0.99849, val loss: 1.01460\n",
      "Interaction training epoch: 8, train loss: 1.03625, val loss: 1.05951\n",
      "Interaction training epoch: 9, train loss: 1.00492, val loss: 1.02049\n",
      "Interaction training epoch: 10, train loss: 0.97438, val loss: 1.00558\n",
      "Interaction training epoch: 11, train loss: 0.98164, val loss: 1.00356\n",
      "Interaction training epoch: 12, train loss: 0.96119, val loss: 0.97311\n",
      "Interaction training epoch: 13, train loss: 0.95361, val loss: 0.96869\n",
      "Interaction training epoch: 14, train loss: 0.94424, val loss: 0.94843\n",
      "Interaction training epoch: 15, train loss: 0.98397, val loss: 0.99918\n",
      "Interaction training epoch: 16, train loss: 0.94241, val loss: 0.95192\n",
      "Interaction training epoch: 17, train loss: 0.92824, val loss: 0.94304\n",
      "Interaction training epoch: 18, train loss: 0.92908, val loss: 0.95034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 19, train loss: 0.93822, val loss: 0.95189\n",
      "Interaction training epoch: 20, train loss: 0.90379, val loss: 0.92226\n",
      "Interaction training epoch: 21, train loss: 0.92062, val loss: 0.94079\n",
      "Interaction training epoch: 22, train loss: 0.90665, val loss: 0.90925\n",
      "Interaction training epoch: 23, train loss: 0.89789, val loss: 0.90739\n",
      "Interaction training epoch: 24, train loss: 0.89513, val loss: 0.91083\n",
      "Interaction training epoch: 25, train loss: 0.89476, val loss: 0.89942\n",
      "Interaction training epoch: 26, train loss: 0.90169, val loss: 0.89776\n",
      "Interaction training epoch: 27, train loss: 0.89500, val loss: 0.89363\n",
      "Interaction training epoch: 28, train loss: 0.88834, val loss: 0.89689\n",
      "Interaction training epoch: 29, train loss: 0.89300, val loss: 0.91540\n",
      "Interaction training epoch: 30, train loss: 0.88593, val loss: 0.89099\n",
      "Interaction training epoch: 31, train loss: 0.88232, val loss: 0.88420\n",
      "Interaction training epoch: 32, train loss: 0.87795, val loss: 0.89869\n",
      "Interaction training epoch: 33, train loss: 0.88229, val loss: 0.87778\n",
      "Interaction training epoch: 34, train loss: 0.87276, val loss: 0.88122\n",
      "Interaction training epoch: 35, train loss: 0.87050, val loss: 0.87660\n",
      "Interaction training epoch: 36, train loss: 0.87119, val loss: 0.88370\n",
      "Interaction training epoch: 37, train loss: 0.85851, val loss: 0.85793\n",
      "Interaction training epoch: 38, train loss: 0.87218, val loss: 0.86608\n",
      "Interaction training epoch: 39, train loss: 0.86082, val loss: 0.85429\n",
      "Interaction training epoch: 40, train loss: 0.86181, val loss: 0.86354\n",
      "Interaction training epoch: 41, train loss: 0.87264, val loss: 0.87880\n",
      "Interaction training epoch: 42, train loss: 0.86603, val loss: 0.85442\n",
      "Interaction training epoch: 43, train loss: 0.85887, val loss: 0.86135\n",
      "Interaction training epoch: 44, train loss: 0.85921, val loss: 0.86532\n",
      "Interaction training epoch: 45, train loss: 0.86117, val loss: 0.85905\n",
      "Interaction training epoch: 46, train loss: 0.85473, val loss: 0.85629\n",
      "Interaction training epoch: 47, train loss: 0.85120, val loss: 0.85272\n",
      "Interaction training epoch: 48, train loss: 0.85216, val loss: 0.84998\n",
      "Interaction training epoch: 49, train loss: 0.86369, val loss: 0.87328\n",
      "Interaction training epoch: 50, train loss: 0.85810, val loss: 0.85727\n",
      "Interaction training epoch: 51, train loss: 0.85320, val loss: 0.85728\n",
      "Interaction training epoch: 52, train loss: 0.84821, val loss: 0.83908\n",
      "Interaction training epoch: 53, train loss: 0.84502, val loss: 0.85590\n",
      "Interaction training epoch: 54, train loss: 0.85908, val loss: 0.86485\n",
      "Interaction training epoch: 55, train loss: 0.84582, val loss: 0.84084\n",
      "Interaction training epoch: 56, train loss: 0.85167, val loss: 0.85189\n",
      "Interaction training epoch: 57, train loss: 0.84306, val loss: 0.84492\n",
      "Interaction training epoch: 58, train loss: 0.83839, val loss: 0.84691\n",
      "Interaction training epoch: 59, train loss: 0.84043, val loss: 0.84572\n",
      "Interaction training epoch: 60, train loss: 0.84783, val loss: 0.84589\n",
      "Interaction training epoch: 61, train loss: 0.83532, val loss: 0.83855\n",
      "Interaction training epoch: 62, train loss: 0.84256, val loss: 0.85829\n",
      "Interaction training epoch: 63, train loss: 0.83755, val loss: 0.84423\n",
      "Interaction training epoch: 64, train loss: 0.83873, val loss: 0.84348\n",
      "Interaction training epoch: 65, train loss: 0.84072, val loss: 0.84775\n",
      "Interaction training epoch: 66, train loss: 0.84299, val loss: 0.84149\n",
      "Interaction training epoch: 67, train loss: 0.83577, val loss: 0.84779\n",
      "Interaction training epoch: 68, train loss: 0.84102, val loss: 0.85662\n",
      "Interaction training epoch: 69, train loss: 0.84088, val loss: 0.83936\n",
      "Interaction training epoch: 70, train loss: 0.85537, val loss: 0.85477\n",
      "Interaction training epoch: 71, train loss: 0.85068, val loss: 0.85405\n",
      "Interaction training epoch: 72, train loss: 0.83862, val loss: 0.84369\n",
      "Interaction training epoch: 73, train loss: 0.84432, val loss: 0.84463\n",
      "Interaction training epoch: 74, train loss: 0.83380, val loss: 0.83828\n",
      "Interaction training epoch: 75, train loss: 0.83648, val loss: 0.83876\n",
      "Interaction training epoch: 76, train loss: 0.83997, val loss: 0.83931\n",
      "Interaction training epoch: 77, train loss: 0.83370, val loss: 0.84113\n",
      "Interaction training epoch: 78, train loss: 0.83476, val loss: 0.84145\n",
      "Interaction training epoch: 79, train loss: 0.83898, val loss: 0.84541\n",
      "Interaction training epoch: 80, train loss: 0.82564, val loss: 0.83930\n",
      "Interaction training epoch: 81, train loss: 0.83567, val loss: 0.84180\n",
      "Interaction training epoch: 82, train loss: 0.83041, val loss: 0.82941\n",
      "Interaction training epoch: 83, train loss: 0.83104, val loss: 0.84232\n",
      "Interaction training epoch: 84, train loss: 0.83523, val loss: 0.83803\n",
      "Interaction training epoch: 85, train loss: 0.82827, val loss: 0.83519\n",
      "Interaction training epoch: 86, train loss: 0.83057, val loss: 0.82543\n",
      "Interaction training epoch: 87, train loss: 0.83066, val loss: 0.84501\n",
      "Interaction training epoch: 88, train loss: 0.82898, val loss: 0.82899\n",
      "Interaction training epoch: 89, train loss: 0.83813, val loss: 0.84695\n",
      "Interaction training epoch: 90, train loss: 0.83300, val loss: 0.83929\n",
      "Interaction training epoch: 91, train loss: 0.83272, val loss: 0.83372\n",
      "Interaction training epoch: 92, train loss: 0.83701, val loss: 0.85214\n",
      "Interaction training epoch: 93, train loss: 0.82634, val loss: 0.83557\n",
      "Interaction training epoch: 94, train loss: 0.83496, val loss: 0.83734\n",
      "Interaction training epoch: 95, train loss: 0.82466, val loss: 0.83705\n",
      "Interaction training epoch: 96, train loss: 0.82721, val loss: 0.83212\n",
      "Interaction training epoch: 97, train loss: 0.82558, val loss: 0.83135\n",
      "Interaction training epoch: 98, train loss: 0.83211, val loss: 0.84646\n",
      "Interaction training epoch: 99, train loss: 0.82454, val loss: 0.83720\n",
      "Interaction training epoch: 100, train loss: 0.82461, val loss: 0.83622\n",
      "Interaction training epoch: 101, train loss: 0.82641, val loss: 0.82969\n",
      "Interaction training epoch: 102, train loss: 0.82019, val loss: 0.82898\n",
      "Interaction training epoch: 103, train loss: 0.82778, val loss: 0.82704\n",
      "Interaction training epoch: 104, train loss: 0.82016, val loss: 0.82773\n",
      "Interaction training epoch: 105, train loss: 0.82611, val loss: 0.83745\n",
      "Interaction training epoch: 106, train loss: 0.82628, val loss: 0.83527\n",
      "Interaction training epoch: 107, train loss: 0.82762, val loss: 0.84067\n",
      "Interaction training epoch: 108, train loss: 0.81851, val loss: 0.82349\n",
      "Interaction training epoch: 109, train loss: 0.82581, val loss: 0.84316\n",
      "Interaction training epoch: 110, train loss: 0.81674, val loss: 0.82455\n",
      "Interaction training epoch: 111, train loss: 0.82917, val loss: 0.84243\n",
      "Interaction training epoch: 112, train loss: 0.81984, val loss: 0.83199\n",
      "Interaction training epoch: 113, train loss: 0.81856, val loss: 0.83380\n",
      "Interaction training epoch: 114, train loss: 0.81418, val loss: 0.82355\n",
      "Interaction training epoch: 115, train loss: 0.82513, val loss: 0.83829\n",
      "Interaction training epoch: 116, train loss: 0.81992, val loss: 0.83315\n",
      "Interaction training epoch: 117, train loss: 0.82041, val loss: 0.83533\n",
      "Interaction training epoch: 118, train loss: 0.82066, val loss: 0.82553\n",
      "Interaction training epoch: 119, train loss: 0.81540, val loss: 0.82763\n",
      "Interaction training epoch: 120, train loss: 0.81817, val loss: 0.83637\n",
      "Interaction training epoch: 121, train loss: 0.81543, val loss: 0.82636\n",
      "Interaction training epoch: 122, train loss: 0.82244, val loss: 0.82649\n",
      "Interaction training epoch: 123, train loss: 0.81456, val loss: 0.83089\n",
      "Interaction training epoch: 124, train loss: 0.82229, val loss: 0.83494\n",
      "Interaction training epoch: 125, train loss: 0.82175, val loss: 0.83236\n",
      "Interaction training epoch: 126, train loss: 0.81671, val loss: 0.82440\n",
      "Interaction training epoch: 127, train loss: 0.81799, val loss: 0.83254\n",
      "Interaction training epoch: 128, train loss: 0.81148, val loss: 0.81904\n",
      "Interaction training epoch: 129, train loss: 0.81803, val loss: 0.84037\n",
      "Interaction training epoch: 130, train loss: 0.81853, val loss: 0.82614\n",
      "Interaction training epoch: 131, train loss: 0.81254, val loss: 0.81760\n",
      "Interaction training epoch: 132, train loss: 0.82220, val loss: 0.83231\n",
      "Interaction training epoch: 133, train loss: 0.81216, val loss: 0.82696\n",
      "Interaction training epoch: 134, train loss: 0.81912, val loss: 0.82334\n",
      "Interaction training epoch: 135, train loss: 0.81343, val loss: 0.83233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 136, train loss: 0.81798, val loss: 0.82444\n",
      "Interaction training epoch: 137, train loss: 0.81846, val loss: 0.82801\n",
      "Interaction training epoch: 138, train loss: 0.81638, val loss: 0.83008\n",
      "Interaction training epoch: 139, train loss: 0.81936, val loss: 0.83377\n",
      "Interaction training epoch: 140, train loss: 0.81420, val loss: 0.82992\n",
      "Interaction training epoch: 141, train loss: 0.81491, val loss: 0.82458\n",
      "Interaction training epoch: 142, train loss: 0.81930, val loss: 0.84017\n",
      "Interaction training epoch: 143, train loss: 0.80793, val loss: 0.81820\n",
      "Interaction training epoch: 144, train loss: 0.81396, val loss: 0.83313\n",
      "Interaction training epoch: 145, train loss: 0.81271, val loss: 0.82521\n",
      "Interaction training epoch: 146, train loss: 0.81473, val loss: 0.83642\n",
      "Interaction training epoch: 147, train loss: 0.81308, val loss: 0.82749\n",
      "Interaction training epoch: 148, train loss: 0.80843, val loss: 0.81980\n",
      "Interaction training epoch: 149, train loss: 0.81780, val loss: 0.83010\n",
      "Interaction training epoch: 150, train loss: 0.81393, val loss: 0.83578\n",
      "Interaction training epoch: 151, train loss: 0.82195, val loss: 0.83170\n",
      "Interaction training epoch: 152, train loss: 0.81214, val loss: 0.83292\n",
      "Interaction training epoch: 153, train loss: 0.81165, val loss: 0.83744\n",
      "Interaction training epoch: 154, train loss: 0.81280, val loss: 0.82104\n",
      "Interaction training epoch: 155, train loss: 0.81470, val loss: 0.83555\n",
      "Interaction training epoch: 156, train loss: 0.81471, val loss: 0.82946\n",
      "Interaction training epoch: 157, train loss: 0.81167, val loss: 0.82474\n",
      "Interaction training epoch: 158, train loss: 0.81075, val loss: 0.82472\n",
      "Interaction training epoch: 159, train loss: 0.80848, val loss: 0.83018\n",
      "Interaction training epoch: 160, train loss: 0.80566, val loss: 0.82495\n",
      "Interaction training epoch: 161, train loss: 0.81492, val loss: 0.82423\n",
      "Interaction training epoch: 162, train loss: 0.81458, val loss: 0.83178\n",
      "Interaction training epoch: 163, train loss: 0.80970, val loss: 0.81931\n",
      "Interaction training epoch: 164, train loss: 0.80274, val loss: 0.82257\n",
      "Interaction training epoch: 165, train loss: 0.81146, val loss: 0.82986\n",
      "Interaction training epoch: 166, train loss: 0.80805, val loss: 0.82347\n",
      "Interaction training epoch: 167, train loss: 0.80781, val loss: 0.82764\n",
      "Interaction training epoch: 168, train loss: 0.80804, val loss: 0.82690\n",
      "Interaction training epoch: 169, train loss: 0.80388, val loss: 0.82172\n",
      "Interaction training epoch: 170, train loss: 0.81179, val loss: 0.82928\n",
      "Interaction training epoch: 171, train loss: 0.81075, val loss: 0.82965\n",
      "Interaction training epoch: 172, train loss: 0.80553, val loss: 0.82953\n",
      "Interaction training epoch: 173, train loss: 0.81071, val loss: 0.82202\n",
      "Interaction training epoch: 174, train loss: 0.80189, val loss: 0.82714\n",
      "Interaction training epoch: 175, train loss: 0.81212, val loss: 0.82318\n",
      "Interaction training epoch: 176, train loss: 0.80970, val loss: 0.83245\n",
      "Interaction training epoch: 177, train loss: 0.81468, val loss: 0.82227\n",
      "Interaction training epoch: 178, train loss: 0.80591, val loss: 0.82743\n",
      "Interaction training epoch: 179, train loss: 0.80656, val loss: 0.82410\n",
      "Interaction training epoch: 180, train loss: 0.80914, val loss: 0.82121\n",
      "Interaction training epoch: 181, train loss: 0.80710, val loss: 0.83034\n",
      "Interaction training epoch: 182, train loss: 0.80190, val loss: 0.81933\n",
      "Interaction training epoch: 183, train loss: 0.80544, val loss: 0.82755\n",
      "Interaction training epoch: 184, train loss: 0.80537, val loss: 0.82430\n",
      "Interaction training epoch: 185, train loss: 0.81162, val loss: 0.83076\n",
      "Interaction training epoch: 186, train loss: 0.81454, val loss: 0.83193\n",
      "Interaction training epoch: 187, train loss: 0.80833, val loss: 0.83672\n",
      "Interaction training epoch: 188, train loss: 0.80712, val loss: 0.81454\n",
      "Interaction training epoch: 189, train loss: 0.80674, val loss: 0.83594\n",
      "Interaction training epoch: 190, train loss: 0.80540, val loss: 0.82644\n",
      "Interaction training epoch: 191, train loss: 0.80747, val loss: 0.82695\n",
      "Interaction training epoch: 192, train loss: 0.80507, val loss: 0.83131\n",
      "Interaction training epoch: 193, train loss: 0.80608, val loss: 0.81952\n",
      "Interaction training epoch: 194, train loss: 0.80202, val loss: 0.82758\n",
      "Interaction training epoch: 195, train loss: 0.80429, val loss: 0.82798\n",
      "Interaction training epoch: 196, train loss: 0.80170, val loss: 0.81974\n",
      "Interaction training epoch: 197, train loss: 0.80086, val loss: 0.82005\n",
      "Interaction training epoch: 198, train loss: 0.80021, val loss: 0.82274\n",
      "Interaction training epoch: 199, train loss: 0.80250, val loss: 0.82189\n",
      "Interaction training epoch: 200, train loss: 0.79829, val loss: 0.82095\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########1 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.80870, val loss: 0.82929\n",
      "Interaction tuning epoch: 2, train loss: 0.81106, val loss: 0.83281\n",
      "Interaction tuning epoch: 3, train loss: 0.81147, val loss: 0.83305\n",
      "Interaction tuning epoch: 4, train loss: 0.80252, val loss: 0.82059\n",
      "Interaction tuning epoch: 5, train loss: 0.80146, val loss: 0.82542\n",
      "Interaction tuning epoch: 6, train loss: 0.80801, val loss: 0.82252\n",
      "Interaction tuning epoch: 7, train loss: 0.80533, val loss: 0.82393\n",
      "Interaction tuning epoch: 8, train loss: 0.80279, val loss: 0.82372\n",
      "Interaction tuning epoch: 9, train loss: 0.80288, val loss: 0.81889\n",
      "Interaction tuning epoch: 10, train loss: 0.79913, val loss: 0.82406\n",
      "Interaction tuning epoch: 11, train loss: 0.80231, val loss: 0.82696\n",
      "Interaction tuning epoch: 12, train loss: 0.80152, val loss: 0.81619\n",
      "Interaction tuning epoch: 13, train loss: 0.80651, val loss: 0.82838\n",
      "Interaction tuning epoch: 14, train loss: 0.80009, val loss: 0.82374\n",
      "Interaction tuning epoch: 15, train loss: 0.81092, val loss: 0.82747\n",
      "Interaction tuning epoch: 16, train loss: 0.80437, val loss: 0.83810\n",
      "Interaction tuning epoch: 17, train loss: 0.80228, val loss: 0.81771\n",
      "Interaction tuning epoch: 18, train loss: 0.81021, val loss: 0.83181\n",
      "Interaction tuning epoch: 19, train loss: 0.80646, val loss: 0.82252\n",
      "Interaction tuning epoch: 20, train loss: 0.80581, val loss: 0.82657\n",
      "Interaction tuning epoch: 21, train loss: 0.79939, val loss: 0.82153\n",
      "Interaction tuning epoch: 22, train loss: 0.80533, val loss: 0.83107\n",
      "Interaction tuning epoch: 23, train loss: 0.80231, val loss: 0.82963\n",
      "Interaction tuning epoch: 24, train loss: 0.79942, val loss: 0.81872\n",
      "Interaction tuning epoch: 25, train loss: 0.80474, val loss: 0.81952\n",
      "Interaction tuning epoch: 26, train loss: 0.79557, val loss: 0.81966\n",
      "Interaction tuning epoch: 27, train loss: 0.80183, val loss: 0.82866\n",
      "Interaction tuning epoch: 28, train loss: 0.79583, val loss: 0.81595\n",
      "Interaction tuning epoch: 29, train loss: 0.79846, val loss: 0.82041\n",
      "Interaction tuning epoch: 30, train loss: 0.79865, val loss: 0.81866\n",
      "Interaction tuning epoch: 31, train loss: 0.79765, val loss: 0.82263\n",
      "Interaction tuning epoch: 32, train loss: 0.80020, val loss: 0.81838\n",
      "Interaction tuning epoch: 33, train loss: 0.79845, val loss: 0.81901\n",
      "Interaction tuning epoch: 34, train loss: 0.79726, val loss: 0.82556\n",
      "Interaction tuning epoch: 35, train loss: 0.79895, val loss: 0.82613\n",
      "Interaction tuning epoch: 36, train loss: 0.79874, val loss: 0.81724\n",
      "Interaction tuning epoch: 37, train loss: 0.79827, val loss: 0.82098\n",
      "Interaction tuning epoch: 38, train loss: 0.79570, val loss: 0.81928\n",
      "Interaction tuning epoch: 39, train loss: 0.79626, val loss: 0.82336\n",
      "Interaction tuning epoch: 40, train loss: 0.80058, val loss: 0.81858\n",
      "Interaction tuning epoch: 41, train loss: 0.79831, val loss: 0.82433\n",
      "Interaction tuning epoch: 42, train loss: 0.79782, val loss: 0.81988\n",
      "Interaction tuning epoch: 43, train loss: 0.80644, val loss: 0.83034\n",
      "Interaction tuning epoch: 44, train loss: 0.79962, val loss: 0.82451\n",
      "Interaction tuning epoch: 45, train loss: 0.80482, val loss: 0.82514\n",
      "Interaction tuning epoch: 46, train loss: 0.80158, val loss: 0.82811\n",
      "Interaction tuning epoch: 47, train loss: 0.79566, val loss: 0.81345\n",
      "Interaction tuning epoch: 48, train loss: 0.79923, val loss: 0.81799\n",
      "Interaction tuning epoch: 49, train loss: 0.80376, val loss: 0.82362\n",
      "Interaction tuning epoch: 50, train loss: 0.79587, val loss: 0.81927\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 40.78621006011963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After the gam stage, training error is 0.79587 , validation error is 0.81927\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 19.770922\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.662153 validation MAE=0.778655,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.619308 validation MAE=0.761066,rank=5\n",
      "[SoftImpute] Iter 3: observed MAE=0.583595 validation MAE=0.745534,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.553348 validation MAE=0.731724,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 5: observed MAE=0.527657 validation MAE=0.719163,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.506055 validation MAE=0.707651,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.487344 validation MAE=0.698141,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.469839 validation MAE=0.689266,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.455080 validation MAE=0.680157,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.441188 validation MAE=0.673049,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.428831 validation MAE=0.665767,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.416999 validation MAE=0.657475,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.407751 validation MAE=0.651356,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.398604 validation MAE=0.645305,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.390568 validation MAE=0.639151,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.383099 validation MAE=0.633570,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.374704 validation MAE=0.628537,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.367973 validation MAE=0.623661,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.360706 validation MAE=0.618850,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.355216 validation MAE=0.615046,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.348506 validation MAE=0.611203,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.343828 validation MAE=0.607197,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.337659 validation MAE=0.603703,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.332452 validation MAE=0.599995,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.328354 validation MAE=0.596862,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.324593 validation MAE=0.594079,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.319860 validation MAE=0.591216,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.314701 validation MAE=0.587409,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.311844 validation MAE=0.586157,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.307229 validation MAE=0.583470,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.303253 validation MAE=0.580280,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.300609 validation MAE=0.578664,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.298932 validation MAE=0.577394,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.295781 validation MAE=0.574433,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.292873 validation MAE=0.572262,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.290700 validation MAE=0.569946,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.288063 validation MAE=0.568557,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.285608 validation MAE=0.567252,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.282497 validation MAE=0.564639,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.280645 validation MAE=0.562919,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.278496 validation MAE=0.561373,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.276314 validation MAE=0.560566,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.273751 validation MAE=0.559009,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.272498 validation MAE=0.557459,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.269982 validation MAE=0.555828,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.268963 validation MAE=0.554823,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.266852 validation MAE=0.553574,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.265023 validation MAE=0.552929,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.263921 validation MAE=0.551251,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.262994 validation MAE=0.550668,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.260425 validation MAE=0.549060,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.259576 validation MAE=0.547997,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.257789 validation MAE=0.546036,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.254578 validation MAE=0.545926,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.253903 validation MAE=0.544219,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.252795 validation MAE=0.543321,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.252098 validation MAE=0.542847,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.250941 validation MAE=0.541506,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.250409 validation MAE=0.541297,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.248158 validation MAE=0.539779,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.246170 validation MAE=0.538146,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.245161 validation MAE=0.537984,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.243669 validation MAE=0.537003,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.242869 validation MAE=0.536230,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.241232 validation MAE=0.536395,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.241114 validation MAE=0.535126,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.240723 validation MAE=0.534503,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.239663 validation MAE=0.533705,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.238001 validation MAE=0.532747,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.237320 validation MAE=0.532489,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.235959 validation MAE=0.531817,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.235498 validation MAE=0.530202,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.234551 validation MAE=0.530594,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.233903 validation MAE=0.529071,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.232385 validation MAE=0.528211,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.232309 validation MAE=0.528450,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.230540 validation MAE=0.527944,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.230258 validation MAE=0.526900,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.229436 validation MAE=0.526617,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.228878 validation MAE=0.525443,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.228454 validation MAE=0.524890,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.228213 validation MAE=0.525155,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.227257 validation MAE=0.524335,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.226455 validation MAE=0.523333,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.226473 validation MAE=0.523528,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.226075 validation MAE=0.522632,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.224766 validation MAE=0.520991,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.223589 validation MAE=0.521314,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.222501 validation MAE=0.520165,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.223280 validation MAE=0.519514,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.223406 validation MAE=0.519343,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.222606 validation MAE=0.518765,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.221202 validation MAE=0.517456,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.221289 validation MAE=0.517923,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.219864 validation MAE=0.516505,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.219932 validation MAE=0.515949,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.218984 validation MAE=0.515639,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.218731 validation MAE=0.515344,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.217193 validation MAE=0.514026,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.217615 validation MAE=0.514053,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.217555 validation MAE=0.513987,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.217618 validation MAE=0.512906,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.216124 validation MAE=0.512253,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.215944 validation MAE=0.512442,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.215037 validation MAE=0.511113,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.214917 validation MAE=0.510608,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.214876 validation MAE=0.510671,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.214943 validation MAE=0.509888,rank=5\n",
      "[SoftImpute] Iter 109: observed MAE=0.213474 validation MAE=0.509418,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.213106 validation MAE=0.508581,rank=5\n",
      "[SoftImpute] Iter 111: observed MAE=0.212942 validation MAE=0.508718,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.212836 validation MAE=0.508164,rank=5\n",
      "[SoftImpute] Iter 113: observed MAE=0.212726 validation MAE=0.507252,rank=5\n",
      "[SoftImpute] Iter 114: observed MAE=0.212673 validation MAE=0.507313,rank=5\n",
      "[SoftImpute] Iter 115: observed MAE=0.212516 validation MAE=0.506730,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.211553 validation MAE=0.505706,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.211111 validation MAE=0.505041,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.211027 validation MAE=0.505049,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.211388 validation MAE=0.504493,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 120: observed MAE=0.210426 validation MAE=0.503460,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.210764 validation MAE=0.504142,rank=5\n",
      "[SoftImpute] Iter 122: observed MAE=0.209373 validation MAE=0.503072,rank=5\n",
      "[SoftImpute] Iter 123: observed MAE=0.209235 validation MAE=0.502483,rank=5\n",
      "[SoftImpute] Iter 124: observed MAE=0.208077 validation MAE=0.501557,rank=5\n",
      "[SoftImpute] Iter 125: observed MAE=0.207816 validation MAE=0.501390,rank=5\n",
      "[SoftImpute] Iter 126: observed MAE=0.208724 validation MAE=0.500984,rank=5\n",
      "[SoftImpute] Iter 127: observed MAE=0.208896 validation MAE=0.501007,rank=5\n",
      "[SoftImpute] Iter 128: observed MAE=0.208345 validation MAE=0.501286,rank=5\n",
      "[SoftImpute] Iter 129: observed MAE=0.207736 validation MAE=0.499749,rank=5\n",
      "[SoftImpute] Iter 130: observed MAE=0.207127 validation MAE=0.499456,rank=5\n",
      "[SoftImpute] Iter 131: observed MAE=0.206605 validation MAE=0.498642,rank=5\n",
      "[SoftImpute] Iter 132: observed MAE=0.206749 validation MAE=0.498843,rank=5\n",
      "[SoftImpute] Iter 133: observed MAE=0.206132 validation MAE=0.498326,rank=5\n",
      "[SoftImpute] Iter 134: observed MAE=0.206051 validation MAE=0.497281,rank=5\n",
      "[SoftImpute] Iter 135: observed MAE=0.205580 validation MAE=0.497255,rank=5\n",
      "[SoftImpute] Iter 136: observed MAE=0.204705 validation MAE=0.497005,rank=5\n",
      "[SoftImpute] Iter 137: observed MAE=0.204727 validation MAE=0.495986,rank=5\n",
      "[SoftImpute] Iter 138: observed MAE=0.204409 validation MAE=0.495742,rank=5\n",
      "[SoftImpute] Iter 139: observed MAE=0.205627 validation MAE=0.496047,rank=5\n",
      "[SoftImpute] Iter 140: observed MAE=0.205542 validation MAE=0.494914,rank=5\n",
      "[SoftImpute] Iter 141: observed MAE=0.204618 validation MAE=0.494554,rank=5\n",
      "[SoftImpute] Iter 142: observed MAE=0.203998 validation MAE=0.494428,rank=5\n",
      "[SoftImpute] Iter 143: observed MAE=0.203294 validation MAE=0.493490,rank=5\n",
      "[SoftImpute] Iter 144: observed MAE=0.203504 validation MAE=0.493060,rank=5\n",
      "[SoftImpute] Iter 145: observed MAE=0.203166 validation MAE=0.492556,rank=5\n",
      "[SoftImpute] Iter 146: observed MAE=0.203224 validation MAE=0.493247,rank=5\n",
      "[SoftImpute] Iter 147: observed MAE=0.203612 validation MAE=0.492143,rank=5\n",
      "[SoftImpute] Iter 148: observed MAE=0.202651 validation MAE=0.491465,rank=5\n",
      "[SoftImpute] Iter 149: observed MAE=0.202585 validation MAE=0.491142,rank=5\n",
      "[SoftImpute] Iter 150: observed MAE=0.202384 validation MAE=0.490739,rank=5\n",
      "[SoftImpute] Iter 151: observed MAE=0.202823 validation MAE=0.490473,rank=5\n",
      "[SoftImpute] Iter 152: observed MAE=0.201567 validation MAE=0.489702,rank=5\n",
      "[SoftImpute] Iter 153: observed MAE=0.201983 validation MAE=0.489379,rank=5\n",
      "[SoftImpute] Iter 154: observed MAE=0.201671 validation MAE=0.489756,rank=5\n",
      "[SoftImpute] Iter 155: observed MAE=0.201515 validation MAE=0.489446,rank=5\n",
      "[SoftImpute] Iter 156: observed MAE=0.201086 validation MAE=0.488224,rank=5\n",
      "[SoftImpute] Iter 157: observed MAE=0.200785 validation MAE=0.488406,rank=5\n",
      "[SoftImpute] Iter 158: observed MAE=0.201279 validation MAE=0.488309,rank=5\n",
      "[SoftImpute] Iter 159: observed MAE=0.199981 validation MAE=0.487023,rank=5\n",
      "[SoftImpute] Iter 160: observed MAE=0.200389 validation MAE=0.486650,rank=5\n",
      "[SoftImpute] Iter 161: observed MAE=0.200311 validation MAE=0.487112,rank=5\n",
      "[SoftImpute] Iter 162: observed MAE=0.199579 validation MAE=0.485989,rank=5\n",
      "[SoftImpute] Iter 163: observed MAE=0.199861 validation MAE=0.485117,rank=5\n",
      "[SoftImpute] Iter 164: observed MAE=0.199691 validation MAE=0.484653,rank=5\n",
      "[SoftImpute] Iter 165: observed MAE=0.199853 validation MAE=0.485565,rank=5\n",
      "[SoftImpute] Iter 166: observed MAE=0.200171 validation MAE=0.485275,rank=5\n",
      "[SoftImpute] Iter 167: observed MAE=0.199301 validation MAE=0.484342,rank=5\n",
      "[SoftImpute] Iter 168: observed MAE=0.198621 validation MAE=0.483909,rank=5\n",
      "[SoftImpute] Iter 169: observed MAE=0.198888 validation MAE=0.484165,rank=5\n",
      "[SoftImpute] Iter 170: observed MAE=0.198377 validation MAE=0.483014,rank=5\n",
      "[SoftImpute] Iter 171: observed MAE=0.198241 validation MAE=0.482744,rank=5\n",
      "[SoftImpute] Iter 172: observed MAE=0.197328 validation MAE=0.481798,rank=5\n",
      "[SoftImpute] Iter 173: observed MAE=0.198510 validation MAE=0.482642,rank=5\n",
      "[SoftImpute] Iter 174: observed MAE=0.198984 validation MAE=0.482207,rank=5\n",
      "[SoftImpute] Iter 175: observed MAE=0.198280 validation MAE=0.482062,rank=5\n",
      "[SoftImpute] Iter 176: observed MAE=0.197930 validation MAE=0.481672,rank=5\n",
      "[SoftImpute] Iter 177: observed MAE=0.197657 validation MAE=0.481415,rank=5\n",
      "[SoftImpute] Iter 178: observed MAE=0.197936 validation MAE=0.481214,rank=5\n",
      "[SoftImpute] Iter 179: observed MAE=0.197035 validation MAE=0.480350,rank=5\n",
      "[SoftImpute] Iter 180: observed MAE=0.197046 validation MAE=0.480066,rank=5\n",
      "[SoftImpute] Iter 181: observed MAE=0.196698 validation MAE=0.479958,rank=5\n",
      "[SoftImpute] Iter 182: observed MAE=0.196889 validation MAE=0.479507,rank=5\n",
      "[SoftImpute] Iter 183: observed MAE=0.196011 validation MAE=0.478903,rank=5\n",
      "[SoftImpute] Iter 184: observed MAE=0.196141 validation MAE=0.478057,rank=5\n",
      "[SoftImpute] Iter 185: observed MAE=0.197449 validation MAE=0.478933,rank=5\n",
      "[SoftImpute] Iter 186: observed MAE=0.197264 validation MAE=0.478577,rank=5\n",
      "[SoftImpute] Iter 187: observed MAE=0.196674 validation MAE=0.478373,rank=5\n",
      "[SoftImpute] Iter 188: observed MAE=0.197099 validation MAE=0.477526,rank=5\n",
      "[SoftImpute] Iter 189: observed MAE=0.196293 validation MAE=0.477745,rank=5\n",
      "[SoftImpute] Iter 190: observed MAE=0.196132 validation MAE=0.477689,rank=5\n",
      "[SoftImpute] Iter 191: observed MAE=0.195655 validation MAE=0.477219,rank=5\n",
      "[SoftImpute] Iter 192: observed MAE=0.194409 validation MAE=0.476766,rank=5\n",
      "[SoftImpute] Iter 193: observed MAE=0.196210 validation MAE=0.477549,rank=5\n",
      "[SoftImpute] Iter 194: observed MAE=0.195984 validation MAE=0.476761,rank=5\n",
      "[SoftImpute] Iter 195: observed MAE=0.194767 validation MAE=0.476639,rank=5\n",
      "[SoftImpute] Iter 196: observed MAE=0.194840 validation MAE=0.476302,rank=5\n",
      "[SoftImpute] Iter 197: observed MAE=0.194566 validation MAE=0.475429,rank=5\n",
      "[SoftImpute] Iter 198: observed MAE=0.194600 validation MAE=0.475176,rank=5\n",
      "[SoftImpute] Iter 199: observed MAE=0.195555 validation MAE=0.474781,rank=5\n",
      "[SoftImpute] Iter 200: observed MAE=0.195175 validation MAE=0.474677,rank=5\n",
      "[SoftImpute] Stopped after iteration 200 for lambda=0.395418\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 8.479379415512085\n",
      "After the matrix factor stage, training error is 0.19517, validation error is 0.47468\n",
      "2\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.04250, val loss: 3.98677\n",
      "Main effects training epoch: 2, train loss: 3.83470, val loss: 3.80568\n",
      "Main effects training epoch: 3, train loss: 3.60835, val loss: 3.63542\n",
      "Main effects training epoch: 4, train loss: 3.47520, val loss: 3.53727\n",
      "Main effects training epoch: 5, train loss: 3.38172, val loss: 3.47016\n",
      "Main effects training epoch: 6, train loss: 3.34301, val loss: 3.41906\n",
      "Main effects training epoch: 7, train loss: 3.29686, val loss: 3.36987\n",
      "Main effects training epoch: 8, train loss: 3.28729, val loss: 3.35218\n",
      "Main effects training epoch: 9, train loss: 3.20846, val loss: 3.27016\n",
      "Main effects training epoch: 10, train loss: 3.16695, val loss: 3.22002\n",
      "Main effects training epoch: 11, train loss: 3.10378, val loss: 3.15290\n",
      "Main effects training epoch: 12, train loss: 3.06709, val loss: 3.11360\n",
      "Main effects training epoch: 13, train loss: 3.01625, val loss: 3.05949\n",
      "Main effects training epoch: 14, train loss: 2.93087, val loss: 2.96154\n",
      "Main effects training epoch: 15, train loss: 2.85961, val loss: 2.88451\n",
      "Main effects training epoch: 16, train loss: 2.77752, val loss: 2.78923\n",
      "Main effects training epoch: 17, train loss: 2.70232, val loss: 2.70235\n",
      "Main effects training epoch: 18, train loss: 2.63823, val loss: 2.61504\n",
      "Main effects training epoch: 19, train loss: 2.62686, val loss: 2.60424\n",
      "Main effects training epoch: 20, train loss: 2.58174, val loss: 2.56211\n",
      "Main effects training epoch: 21, train loss: 2.51642, val loss: 2.48077\n",
      "Main effects training epoch: 22, train loss: 2.44556, val loss: 2.41105\n",
      "Main effects training epoch: 23, train loss: 2.44266, val loss: 2.40233\n",
      "Main effects training epoch: 24, train loss: 2.37419, val loss: 2.33796\n",
      "Main effects training epoch: 25, train loss: 2.38036, val loss: 2.33852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 26, train loss: 2.32430, val loss: 2.28435\n",
      "Main effects training epoch: 27, train loss: 2.30723, val loss: 2.27304\n",
      "Main effects training epoch: 28, train loss: 2.28235, val loss: 2.23202\n",
      "Main effects training epoch: 29, train loss: 2.27014, val loss: 2.23488\n",
      "Main effects training epoch: 30, train loss: 2.21184, val loss: 2.15472\n",
      "Main effects training epoch: 31, train loss: 2.18619, val loss: 2.14386\n",
      "Main effects training epoch: 32, train loss: 2.17982, val loss: 2.11410\n",
      "Main effects training epoch: 33, train loss: 2.14398, val loss: 2.09689\n",
      "Main effects training epoch: 34, train loss: 2.13496, val loss: 2.06930\n",
      "Main effects training epoch: 35, train loss: 2.10340, val loss: 2.05492\n",
      "Main effects training epoch: 36, train loss: 2.08270, val loss: 2.01383\n",
      "Main effects training epoch: 37, train loss: 2.07889, val loss: 2.02023\n",
      "Main effects training epoch: 38, train loss: 2.02082, val loss: 1.94706\n",
      "Main effects training epoch: 39, train loss: 2.02639, val loss: 1.96052\n",
      "Main effects training epoch: 40, train loss: 2.01022, val loss: 1.93712\n",
      "Main effects training epoch: 41, train loss: 1.99143, val loss: 1.91501\n",
      "Main effects training epoch: 42, train loss: 1.95344, val loss: 1.87490\n",
      "Main effects training epoch: 43, train loss: 1.96143, val loss: 1.88503\n",
      "Main effects training epoch: 44, train loss: 1.94922, val loss: 1.86785\n",
      "Main effects training epoch: 45, train loss: 1.92848, val loss: 1.84717\n",
      "Main effects training epoch: 46, train loss: 1.92976, val loss: 1.84837\n",
      "Main effects training epoch: 47, train loss: 1.89545, val loss: 1.80752\n",
      "Main effects training epoch: 48, train loss: 1.89695, val loss: 1.81099\n",
      "Main effects training epoch: 49, train loss: 1.90570, val loss: 1.82303\n",
      "Main effects training epoch: 50, train loss: 1.86872, val loss: 1.77595\n",
      "Main effects training epoch: 51, train loss: 1.86281, val loss: 1.77725\n",
      "Main effects training epoch: 52, train loss: 1.85647, val loss: 1.75908\n",
      "Main effects training epoch: 53, train loss: 1.84460, val loss: 1.75064\n",
      "Main effects training epoch: 54, train loss: 1.84341, val loss: 1.74400\n",
      "Main effects training epoch: 55, train loss: 1.82235, val loss: 1.72771\n",
      "Main effects training epoch: 56, train loss: 1.82711, val loss: 1.72912\n",
      "Main effects training epoch: 57, train loss: 1.81754, val loss: 1.71420\n",
      "Main effects training epoch: 58, train loss: 1.80575, val loss: 1.70676\n",
      "Main effects training epoch: 59, train loss: 1.79727, val loss: 1.69282\n",
      "Main effects training epoch: 60, train loss: 1.80277, val loss: 1.69725\n",
      "Main effects training epoch: 61, train loss: 1.79267, val loss: 1.68871\n",
      "Main effects training epoch: 62, train loss: 1.78474, val loss: 1.67637\n",
      "Main effects training epoch: 63, train loss: 1.78280, val loss: 1.67673\n",
      "Main effects training epoch: 64, train loss: 1.78094, val loss: 1.67215\n",
      "Main effects training epoch: 65, train loss: 1.77697, val loss: 1.66749\n",
      "Main effects training epoch: 66, train loss: 1.77237, val loss: 1.66199\n",
      "Main effects training epoch: 67, train loss: 1.76359, val loss: 1.64905\n",
      "Main effects training epoch: 68, train loss: 1.77100, val loss: 1.66344\n",
      "Main effects training epoch: 69, train loss: 1.75267, val loss: 1.63959\n",
      "Main effects training epoch: 70, train loss: 1.75937, val loss: 1.64624\n",
      "Main effects training epoch: 71, train loss: 1.75250, val loss: 1.63964\n",
      "Main effects training epoch: 72, train loss: 1.75438, val loss: 1.64083\n",
      "Main effects training epoch: 73, train loss: 1.74541, val loss: 1.62822\n",
      "Main effects training epoch: 74, train loss: 1.74758, val loss: 1.63114\n",
      "Main effects training epoch: 75, train loss: 1.74815, val loss: 1.63559\n",
      "Main effects training epoch: 76, train loss: 1.74363, val loss: 1.62450\n",
      "Main effects training epoch: 77, train loss: 1.73984, val loss: 1.62460\n",
      "Main effects training epoch: 78, train loss: 1.73946, val loss: 1.62097\n",
      "Main effects training epoch: 79, train loss: 1.73294, val loss: 1.61529\n",
      "Main effects training epoch: 80, train loss: 1.73267, val loss: 1.60740\n",
      "Main effects training epoch: 81, train loss: 1.73174, val loss: 1.61715\n",
      "Main effects training epoch: 82, train loss: 1.73276, val loss: 1.61005\n",
      "Main effects training epoch: 83, train loss: 1.73007, val loss: 1.61493\n",
      "Main effects training epoch: 84, train loss: 1.72960, val loss: 1.61118\n",
      "Main effects training epoch: 85, train loss: 1.72804, val loss: 1.60791\n",
      "Main effects training epoch: 86, train loss: 1.72287, val loss: 1.59815\n",
      "Main effects training epoch: 87, train loss: 1.72400, val loss: 1.60756\n",
      "Main effects training epoch: 88, train loss: 1.72324, val loss: 1.60072\n",
      "Main effects training epoch: 89, train loss: 1.71919, val loss: 1.59995\n",
      "Main effects training epoch: 90, train loss: 1.72294, val loss: 1.60346\n",
      "Main effects training epoch: 91, train loss: 1.71732, val loss: 1.59748\n",
      "Main effects training epoch: 92, train loss: 1.71617, val loss: 1.60067\n",
      "Main effects training epoch: 93, train loss: 1.71191, val loss: 1.58359\n",
      "Main effects training epoch: 94, train loss: 1.71346, val loss: 1.60209\n",
      "Main effects training epoch: 95, train loss: 1.70964, val loss: 1.58564\n",
      "Main effects training epoch: 96, train loss: 1.70858, val loss: 1.59422\n",
      "Main effects training epoch: 97, train loss: 1.70868, val loss: 1.58948\n",
      "Main effects training epoch: 98, train loss: 1.70078, val loss: 1.58569\n",
      "Main effects training epoch: 99, train loss: 1.70076, val loss: 1.59070\n",
      "Main effects training epoch: 100, train loss: 1.69600, val loss: 1.57982\n",
      "Main effects training epoch: 101, train loss: 1.69155, val loss: 1.57991\n",
      "Main effects training epoch: 102, train loss: 1.68994, val loss: 1.58308\n",
      "Main effects training epoch: 103, train loss: 1.68552, val loss: 1.57371\n",
      "Main effects training epoch: 104, train loss: 1.68730, val loss: 1.58458\n",
      "Main effects training epoch: 105, train loss: 1.67766, val loss: 1.56607\n",
      "Main effects training epoch: 106, train loss: 1.67326, val loss: 1.56802\n",
      "Main effects training epoch: 107, train loss: 1.66957, val loss: 1.56973\n",
      "Main effects training epoch: 108, train loss: 1.66270, val loss: 1.55524\n",
      "Main effects training epoch: 109, train loss: 1.66343, val loss: 1.56192\n",
      "Main effects training epoch: 110, train loss: 1.66418, val loss: 1.55069\n",
      "Main effects training epoch: 111, train loss: 1.66194, val loss: 1.55783\n",
      "Main effects training epoch: 112, train loss: 1.65266, val loss: 1.54034\n",
      "Main effects training epoch: 113, train loss: 1.64780, val loss: 1.53411\n",
      "Main effects training epoch: 114, train loss: 1.64702, val loss: 1.54956\n",
      "Main effects training epoch: 115, train loss: 1.64057, val loss: 1.53169\n",
      "Main effects training epoch: 116, train loss: 1.63825, val loss: 1.53432\n",
      "Main effects training epoch: 117, train loss: 1.63889, val loss: 1.52847\n",
      "Main effects training epoch: 118, train loss: 1.64221, val loss: 1.54275\n",
      "Main effects training epoch: 119, train loss: 1.64117, val loss: 1.53203\n",
      "Main effects training epoch: 120, train loss: 1.63407, val loss: 1.52182\n",
      "Main effects training epoch: 121, train loss: 1.63460, val loss: 1.52535\n",
      "Main effects training epoch: 122, train loss: 1.63417, val loss: 1.51853\n",
      "Main effects training epoch: 123, train loss: 1.63087, val loss: 1.52649\n",
      "Main effects training epoch: 124, train loss: 1.63336, val loss: 1.53002\n",
      "Main effects training epoch: 125, train loss: 1.62881, val loss: 1.51646\n",
      "Main effects training epoch: 126, train loss: 1.63154, val loss: 1.53428\n",
      "Main effects training epoch: 127, train loss: 1.63713, val loss: 1.52079\n",
      "Main effects training epoch: 128, train loss: 1.63280, val loss: 1.51732\n",
      "Main effects training epoch: 129, train loss: 1.62863, val loss: 1.52467\n",
      "Main effects training epoch: 130, train loss: 1.62417, val loss: 1.51722\n",
      "Main effects training epoch: 131, train loss: 1.63774, val loss: 1.52254\n",
      "Main effects training epoch: 132, train loss: 1.62965, val loss: 1.53014\n",
      "Main effects training epoch: 133, train loss: 1.62813, val loss: 1.52136\n",
      "Main effects training epoch: 134, train loss: 1.62332, val loss: 1.52217\n",
      "Main effects training epoch: 135, train loss: 1.62053, val loss: 1.51519\n",
      "Main effects training epoch: 136, train loss: 1.61974, val loss: 1.51061\n",
      "Main effects training epoch: 137, train loss: 1.62088, val loss: 1.51590\n",
      "Main effects training epoch: 138, train loss: 1.62217, val loss: 1.50931\n",
      "Main effects training epoch: 139, train loss: 1.62548, val loss: 1.53127\n",
      "Main effects training epoch: 140, train loss: 1.62054, val loss: 1.50956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 141, train loss: 1.61455, val loss: 1.51284\n",
      "Main effects training epoch: 142, train loss: 1.61608, val loss: 1.51350\n",
      "Main effects training epoch: 143, train loss: 1.61819, val loss: 1.50711\n",
      "Main effects training epoch: 144, train loss: 1.61102, val loss: 1.51215\n",
      "Main effects training epoch: 145, train loss: 1.62129, val loss: 1.49845\n",
      "Main effects training epoch: 146, train loss: 1.60884, val loss: 1.51319\n",
      "Main effects training epoch: 147, train loss: 1.60717, val loss: 1.50320\n",
      "Main effects training epoch: 148, train loss: 1.60794, val loss: 1.50361\n",
      "Main effects training epoch: 149, train loss: 1.61273, val loss: 1.50233\n",
      "Main effects training epoch: 150, train loss: 1.61120, val loss: 1.50563\n",
      "Main effects training epoch: 151, train loss: 1.60319, val loss: 1.50279\n",
      "Main effects training epoch: 152, train loss: 1.60125, val loss: 1.49451\n",
      "Main effects training epoch: 153, train loss: 1.60162, val loss: 1.49453\n",
      "Main effects training epoch: 154, train loss: 1.60586, val loss: 1.51369\n",
      "Main effects training epoch: 155, train loss: 1.60362, val loss: 1.50151\n",
      "Main effects training epoch: 156, train loss: 1.59018, val loss: 1.48805\n",
      "Main effects training epoch: 157, train loss: 1.59157, val loss: 1.49276\n",
      "Main effects training epoch: 158, train loss: 1.59900, val loss: 1.49488\n",
      "Main effects training epoch: 159, train loss: 1.58583, val loss: 1.48343\n",
      "Main effects training epoch: 160, train loss: 1.58477, val loss: 1.47946\n",
      "Main effects training epoch: 161, train loss: 1.58394, val loss: 1.48757\n",
      "Main effects training epoch: 162, train loss: 1.58011, val loss: 1.48422\n",
      "Main effects training epoch: 163, train loss: 1.57983, val loss: 1.47377\n",
      "Main effects training epoch: 164, train loss: 1.57676, val loss: 1.47965\n",
      "Main effects training epoch: 165, train loss: 1.58051, val loss: 1.48172\n",
      "Main effects training epoch: 166, train loss: 1.56918, val loss: 1.47381\n",
      "Main effects training epoch: 167, train loss: 1.56945, val loss: 1.47675\n",
      "Main effects training epoch: 168, train loss: 1.57017, val loss: 1.47730\n",
      "Main effects training epoch: 169, train loss: 1.56538, val loss: 1.46171\n",
      "Main effects training epoch: 170, train loss: 1.56135, val loss: 1.47157\n",
      "Main effects training epoch: 171, train loss: 1.56387, val loss: 1.46715\n",
      "Main effects training epoch: 172, train loss: 1.56136, val loss: 1.47320\n",
      "Main effects training epoch: 173, train loss: 1.56008, val loss: 1.46787\n",
      "Main effects training epoch: 174, train loss: 1.55717, val loss: 1.45834\n",
      "Main effects training epoch: 175, train loss: 1.56630, val loss: 1.46746\n",
      "Main effects training epoch: 176, train loss: 1.56663, val loss: 1.47437\n",
      "Main effects training epoch: 177, train loss: 1.55898, val loss: 1.46494\n",
      "Main effects training epoch: 178, train loss: 1.55631, val loss: 1.47033\n",
      "Main effects training epoch: 179, train loss: 1.54696, val loss: 1.46200\n",
      "Main effects training epoch: 180, train loss: 1.55137, val loss: 1.45433\n",
      "Main effects training epoch: 181, train loss: 1.54358, val loss: 1.45928\n",
      "Main effects training epoch: 182, train loss: 1.54569, val loss: 1.45082\n",
      "Main effects training epoch: 183, train loss: 1.54308, val loss: 1.45158\n",
      "Main effects training epoch: 184, train loss: 1.54637, val loss: 1.46254\n",
      "Main effects training epoch: 185, train loss: 1.54626, val loss: 1.46308\n",
      "Main effects training epoch: 186, train loss: 1.54289, val loss: 1.45398\n",
      "Main effects training epoch: 187, train loss: 1.54699, val loss: 1.45247\n",
      "Main effects training epoch: 188, train loss: 1.53180, val loss: 1.43672\n",
      "Main effects training epoch: 189, train loss: 1.54072, val loss: 1.46631\n",
      "Main effects training epoch: 190, train loss: 1.54446, val loss: 1.45214\n",
      "Main effects training epoch: 191, train loss: 1.52960, val loss: 1.43234\n",
      "Main effects training epoch: 192, train loss: 1.54320, val loss: 1.46908\n",
      "Main effects training epoch: 193, train loss: 1.52692, val loss: 1.44276\n",
      "Main effects training epoch: 194, train loss: 1.52948, val loss: 1.43531\n",
      "Main effects training epoch: 195, train loss: 1.52491, val loss: 1.44012\n",
      "Main effects training epoch: 196, train loss: 1.51641, val loss: 1.43405\n",
      "Main effects training epoch: 197, train loss: 1.51755, val loss: 1.43817\n",
      "Main effects training epoch: 198, train loss: 1.52362, val loss: 1.43488\n",
      "Main effects training epoch: 199, train loss: 1.52804, val loss: 1.45033\n",
      "Main effects training epoch: 200, train loss: 1.52453, val loss: 1.45395\n",
      "Main effects training epoch: 201, train loss: 1.51397, val loss: 1.43165\n",
      "Main effects training epoch: 202, train loss: 1.51034, val loss: 1.43284\n",
      "Main effects training epoch: 203, train loss: 1.51696, val loss: 1.43866\n",
      "Main effects training epoch: 204, train loss: 1.51507, val loss: 1.44435\n",
      "Main effects training epoch: 205, train loss: 1.51759, val loss: 1.43212\n",
      "Main effects training epoch: 206, train loss: 1.51456, val loss: 1.44278\n",
      "Main effects training epoch: 207, train loss: 1.52785, val loss: 1.45724\n",
      "Main effects training epoch: 208, train loss: 1.51293, val loss: 1.44466\n",
      "Main effects training epoch: 209, train loss: 1.52554, val loss: 1.45104\n",
      "Main effects training epoch: 210, train loss: 1.51400, val loss: 1.42774\n",
      "Main effects training epoch: 211, train loss: 1.51086, val loss: 1.43949\n",
      "Main effects training epoch: 212, train loss: 1.50628, val loss: 1.42390\n",
      "Main effects training epoch: 213, train loss: 1.50530, val loss: 1.43061\n",
      "Main effects training epoch: 214, train loss: 1.51327, val loss: 1.44517\n",
      "Main effects training epoch: 215, train loss: 1.50562, val loss: 1.42326\n",
      "Main effects training epoch: 216, train loss: 1.50238, val loss: 1.42711\n",
      "Main effects training epoch: 217, train loss: 1.50597, val loss: 1.43756\n",
      "Main effects training epoch: 218, train loss: 1.50074, val loss: 1.40781\n",
      "Main effects training epoch: 219, train loss: 1.50307, val loss: 1.43833\n",
      "Main effects training epoch: 220, train loss: 1.49696, val loss: 1.42054\n",
      "Main effects training epoch: 221, train loss: 1.50559, val loss: 1.44001\n",
      "Main effects training epoch: 222, train loss: 1.49189, val loss: 1.41117\n",
      "Main effects training epoch: 223, train loss: 1.49514, val loss: 1.42339\n",
      "Main effects training epoch: 224, train loss: 1.49352, val loss: 1.41821\n",
      "Main effects training epoch: 225, train loss: 1.49437, val loss: 1.42403\n",
      "Main effects training epoch: 226, train loss: 1.48977, val loss: 1.41761\n",
      "Main effects training epoch: 227, train loss: 1.48683, val loss: 1.41074\n",
      "Main effects training epoch: 228, train loss: 1.48442, val loss: 1.41496\n",
      "Main effects training epoch: 229, train loss: 1.48525, val loss: 1.42443\n",
      "Main effects training epoch: 230, train loss: 1.47816, val loss: 1.40285\n",
      "Main effects training epoch: 231, train loss: 1.48645, val loss: 1.41184\n",
      "Main effects training epoch: 232, train loss: 1.49410, val loss: 1.43281\n",
      "Main effects training epoch: 233, train loss: 1.47691, val loss: 1.41205\n",
      "Main effects training epoch: 234, train loss: 1.49242, val loss: 1.42887\n",
      "Main effects training epoch: 235, train loss: 1.48899, val loss: 1.42193\n",
      "Main effects training epoch: 236, train loss: 1.48924, val loss: 1.41926\n",
      "Main effects training epoch: 237, train loss: 1.50253, val loss: 1.43551\n",
      "Main effects training epoch: 238, train loss: 1.48780, val loss: 1.43460\n",
      "Main effects training epoch: 239, train loss: 1.48027, val loss: 1.40943\n",
      "Main effects training epoch: 240, train loss: 1.47521, val loss: 1.41668\n",
      "Main effects training epoch: 241, train loss: 1.47118, val loss: 1.40935\n",
      "Main effects training epoch: 242, train loss: 1.49862, val loss: 1.44214\n",
      "Main effects training epoch: 243, train loss: 1.49111, val loss: 1.42788\n",
      "Main effects training epoch: 244, train loss: 1.48751, val loss: 1.42372\n",
      "Main effects training epoch: 245, train loss: 1.48293, val loss: 1.43670\n",
      "Main effects training epoch: 246, train loss: 1.49300, val loss: 1.42757\n",
      "Main effects training epoch: 247, train loss: 1.47490, val loss: 1.41127\n",
      "Main effects training epoch: 248, train loss: 1.46624, val loss: 1.40543\n",
      "Main effects training epoch: 249, train loss: 1.46323, val loss: 1.40675\n",
      "Main effects training epoch: 250, train loss: 1.45815, val loss: 1.40253\n",
      "Main effects training epoch: 251, train loss: 1.46746, val loss: 1.42326\n",
      "Main effects training epoch: 252, train loss: 1.46237, val loss: 1.40039\n",
      "Main effects training epoch: 253, train loss: 1.45520, val loss: 1.39376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 254, train loss: 1.46328, val loss: 1.41938\n",
      "Main effects training epoch: 255, train loss: 1.46110, val loss: 1.40593\n",
      "Main effects training epoch: 256, train loss: 1.45466, val loss: 1.39927\n",
      "Main effects training epoch: 257, train loss: 1.46227, val loss: 1.41529\n",
      "Main effects training epoch: 258, train loss: 1.45665, val loss: 1.40271\n",
      "Main effects training epoch: 259, train loss: 1.46132, val loss: 1.41095\n",
      "Main effects training epoch: 260, train loss: 1.45484, val loss: 1.39331\n",
      "Main effects training epoch: 261, train loss: 1.45538, val loss: 1.41150\n",
      "Main effects training epoch: 262, train loss: 1.45468, val loss: 1.39912\n",
      "Main effects training epoch: 263, train loss: 1.44942, val loss: 1.39398\n",
      "Main effects training epoch: 264, train loss: 1.44944, val loss: 1.40323\n",
      "Main effects training epoch: 265, train loss: 1.47123, val loss: 1.41994\n",
      "Main effects training epoch: 266, train loss: 1.45082, val loss: 1.40900\n",
      "Main effects training epoch: 267, train loss: 1.45245, val loss: 1.40860\n",
      "Main effects training epoch: 268, train loss: 1.45649, val loss: 1.40137\n",
      "Main effects training epoch: 269, train loss: 1.45519, val loss: 1.40332\n",
      "Main effects training epoch: 270, train loss: 1.45303, val loss: 1.39798\n",
      "Main effects training epoch: 271, train loss: 1.45263, val loss: 1.40089\n",
      "Main effects training epoch: 272, train loss: 1.44754, val loss: 1.40791\n",
      "Main effects training epoch: 273, train loss: 1.45392, val loss: 1.38627\n",
      "Main effects training epoch: 274, train loss: 1.45882, val loss: 1.41472\n",
      "Main effects training epoch: 275, train loss: 1.44486, val loss: 1.40385\n",
      "Main effects training epoch: 276, train loss: 1.44484, val loss: 1.39740\n",
      "Main effects training epoch: 277, train loss: 1.44777, val loss: 1.39285\n",
      "Main effects training epoch: 278, train loss: 1.45895, val loss: 1.40447\n",
      "Main effects training epoch: 279, train loss: 1.45959, val loss: 1.42600\n",
      "Main effects training epoch: 280, train loss: 1.44982, val loss: 1.39219\n",
      "Main effects training epoch: 281, train loss: 1.44947, val loss: 1.40399\n",
      "Main effects training epoch: 282, train loss: 1.44617, val loss: 1.40612\n",
      "Main effects training epoch: 283, train loss: 1.45137, val loss: 1.40965\n",
      "Main effects training epoch: 284, train loss: 1.45477, val loss: 1.41471\n",
      "Main effects training epoch: 285, train loss: 1.46049, val loss: 1.40634\n",
      "Main effects training epoch: 286, train loss: 1.45594, val loss: 1.40482\n",
      "Main effects training epoch: 287, train loss: 1.44288, val loss: 1.38812\n",
      "Main effects training epoch: 288, train loss: 1.44992, val loss: 1.40719\n",
      "Main effects training epoch: 289, train loss: 1.45078, val loss: 1.41478\n",
      "Main effects training epoch: 290, train loss: 1.44850, val loss: 1.39717\n",
      "Main effects training epoch: 291, train loss: 1.43999, val loss: 1.38974\n",
      "Main effects training epoch: 292, train loss: 1.45662, val loss: 1.39902\n",
      "Main effects training epoch: 293, train loss: 1.44941, val loss: 1.39343\n",
      "Main effects training epoch: 294, train loss: 1.43710, val loss: 1.39357\n",
      "Main effects training epoch: 295, train loss: 1.43710, val loss: 1.39235\n",
      "Main effects training epoch: 296, train loss: 1.43919, val loss: 1.40076\n",
      "Main effects training epoch: 297, train loss: 1.44184, val loss: 1.39883\n",
      "Main effects training epoch: 298, train loss: 1.44546, val loss: 1.40534\n",
      "Main effects training epoch: 299, train loss: 1.44299, val loss: 1.39019\n",
      "Main effects training epoch: 300, train loss: 1.44428, val loss: 1.40690\n",
      "##########Stage 1: main effect training stop.##########\n",
      "2 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.44401, val loss: 1.38088\n",
      "Main effects tuning epoch: 2, train loss: 1.44485, val loss: 1.39127\n",
      "Main effects tuning epoch: 3, train loss: 1.44813, val loss: 1.38489\n",
      "Main effects tuning epoch: 4, train loss: 1.44613, val loss: 1.40415\n",
      "Main effects tuning epoch: 5, train loss: 1.43899, val loss: 1.38691\n",
      "Main effects tuning epoch: 6, train loss: 1.44801, val loss: 1.40242\n",
      "Main effects tuning epoch: 7, train loss: 1.45614, val loss: 1.40073\n",
      "Main effects tuning epoch: 8, train loss: 1.43848, val loss: 1.38256\n",
      "Main effects tuning epoch: 9, train loss: 1.45422, val loss: 1.39752\n",
      "Main effects tuning epoch: 10, train loss: 1.44345, val loss: 1.38896\n",
      "Main effects tuning epoch: 11, train loss: 1.44444, val loss: 1.38664\n",
      "Main effects tuning epoch: 12, train loss: 1.44439, val loss: 1.39088\n",
      "Main effects tuning epoch: 13, train loss: 1.45447, val loss: 1.41050\n",
      "Main effects tuning epoch: 14, train loss: 1.44543, val loss: 1.39265\n",
      "Main effects tuning epoch: 15, train loss: 1.44266, val loss: 1.39907\n",
      "Main effects tuning epoch: 16, train loss: 1.44482, val loss: 1.39284\n",
      "Main effects tuning epoch: 17, train loss: 1.45213, val loss: 1.39412\n",
      "Main effects tuning epoch: 18, train loss: 1.44460, val loss: 1.38690\n",
      "Main effects tuning epoch: 19, train loss: 1.45633, val loss: 1.41857\n",
      "Main effects tuning epoch: 20, train loss: 1.44957, val loss: 1.40274\n",
      "Main effects tuning epoch: 21, train loss: 1.43625, val loss: 1.38646\n",
      "Main effects tuning epoch: 22, train loss: 1.44834, val loss: 1.40345\n",
      "Main effects tuning epoch: 23, train loss: 1.43974, val loss: 1.38305\n",
      "Main effects tuning epoch: 24, train loss: 1.44040, val loss: 1.38703\n",
      "Main effects tuning epoch: 25, train loss: 1.44440, val loss: 1.40371\n",
      "Main effects tuning epoch: 26, train loss: 1.44597, val loss: 1.39263\n",
      "Main effects tuning epoch: 27, train loss: 1.45406, val loss: 1.39689\n",
      "Main effects tuning epoch: 28, train loss: 1.45053, val loss: 1.40505\n",
      "Main effects tuning epoch: 29, train loss: 1.44267, val loss: 1.39978\n",
      "Main effects tuning epoch: 30, train loss: 1.44722, val loss: 1.38624\n",
      "Main effects tuning epoch: 31, train loss: 1.43936, val loss: 1.39194\n",
      "Main effects tuning epoch: 32, train loss: 1.43671, val loss: 1.38206\n",
      "Main effects tuning epoch: 33, train loss: 1.43784, val loss: 1.39302\n",
      "Main effects tuning epoch: 34, train loss: 1.44430, val loss: 1.39591\n",
      "Main effects tuning epoch: 35, train loss: 1.43534, val loss: 1.38734\n",
      "Main effects tuning epoch: 36, train loss: 1.43830, val loss: 1.39274\n",
      "Main effects tuning epoch: 37, train loss: 1.43663, val loss: 1.38446\n",
      "Main effects tuning epoch: 38, train loss: 1.44181, val loss: 1.39607\n",
      "Main effects tuning epoch: 39, train loss: 1.44346, val loss: 1.38142\n",
      "Main effects tuning epoch: 40, train loss: 1.43226, val loss: 1.38531\n",
      "Main effects tuning epoch: 41, train loss: 1.43961, val loss: 1.39269\n",
      "Main effects tuning epoch: 42, train loss: 1.43161, val loss: 1.38262\n",
      "Main effects tuning epoch: 43, train loss: 1.43774, val loss: 1.39252\n",
      "Main effects tuning epoch: 44, train loss: 1.43657, val loss: 1.38110\n",
      "Main effects tuning epoch: 45, train loss: 1.43565, val loss: 1.38858\n",
      "Main effects tuning epoch: 46, train loss: 1.45549, val loss: 1.40033\n",
      "Main effects tuning epoch: 47, train loss: 1.43264, val loss: 1.39546\n",
      "Main effects tuning epoch: 48, train loss: 1.43654, val loss: 1.37947\n",
      "Main effects tuning epoch: 49, train loss: 1.44994, val loss: 1.40935\n",
      "Main effects tuning epoch: 50, train loss: 1.43422, val loss: 1.37928\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.39625, val loss: 1.35532\n",
      "Interaction training epoch: 2, train loss: 1.25277, val loss: 1.19782\n",
      "Interaction training epoch: 3, train loss: 1.41367, val loss: 1.39272\n",
      "Interaction training epoch: 4, train loss: 1.06822, val loss: 1.00027\n",
      "Interaction training epoch: 5, train loss: 1.01876, val loss: 0.98468\n",
      "Interaction training epoch: 6, train loss: 1.01141, val loss: 0.94253\n",
      "Interaction training epoch: 7, train loss: 1.01770, val loss: 0.95185\n",
      "Interaction training epoch: 8, train loss: 0.97537, val loss: 0.91514\n",
      "Interaction training epoch: 9, train loss: 0.96820, val loss: 0.90635\n",
      "Interaction training epoch: 10, train loss: 0.95808, val loss: 0.89632\n",
      "Interaction training epoch: 11, train loss: 0.96794, val loss: 0.91311\n",
      "Interaction training epoch: 12, train loss: 0.93955, val loss: 0.88582\n",
      "Interaction training epoch: 13, train loss: 0.95591, val loss: 0.90079\n",
      "Interaction training epoch: 14, train loss: 0.95219, val loss: 0.88545\n",
      "Interaction training epoch: 15, train loss: 0.96102, val loss: 0.91336\n",
      "Interaction training epoch: 16, train loss: 0.96939, val loss: 0.92528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 17, train loss: 0.97034, val loss: 0.91447\n",
      "Interaction training epoch: 18, train loss: 0.92404, val loss: 0.85475\n",
      "Interaction training epoch: 19, train loss: 0.90882, val loss: 0.86128\n",
      "Interaction training epoch: 20, train loss: 0.92309, val loss: 0.87055\n",
      "Interaction training epoch: 21, train loss: 0.91399, val loss: 0.86931\n",
      "Interaction training epoch: 22, train loss: 0.89220, val loss: 0.83593\n",
      "Interaction training epoch: 23, train loss: 0.89154, val loss: 0.83433\n",
      "Interaction training epoch: 24, train loss: 0.88534, val loss: 0.82762\n",
      "Interaction training epoch: 25, train loss: 0.88830, val loss: 0.83290\n",
      "Interaction training epoch: 26, train loss: 0.88697, val loss: 0.83374\n",
      "Interaction training epoch: 27, train loss: 0.88640, val loss: 0.82948\n",
      "Interaction training epoch: 28, train loss: 0.88271, val loss: 0.83859\n",
      "Interaction training epoch: 29, train loss: 0.87219, val loss: 0.81334\n",
      "Interaction training epoch: 30, train loss: 0.88518, val loss: 0.83656\n",
      "Interaction training epoch: 31, train loss: 0.87615, val loss: 0.82536\n",
      "Interaction training epoch: 32, train loss: 0.87139, val loss: 0.83244\n",
      "Interaction training epoch: 33, train loss: 0.85911, val loss: 0.81176\n",
      "Interaction training epoch: 34, train loss: 0.87368, val loss: 0.83044\n",
      "Interaction training epoch: 35, train loss: 0.85591, val loss: 0.81666\n",
      "Interaction training epoch: 36, train loss: 0.85722, val loss: 0.81703\n",
      "Interaction training epoch: 37, train loss: 0.85415, val loss: 0.81313\n",
      "Interaction training epoch: 38, train loss: 0.84299, val loss: 0.79399\n",
      "Interaction training epoch: 39, train loss: 0.85383, val loss: 0.80549\n",
      "Interaction training epoch: 40, train loss: 0.84133, val loss: 0.79527\n",
      "Interaction training epoch: 41, train loss: 0.84704, val loss: 0.79744\n",
      "Interaction training epoch: 42, train loss: 0.84087, val loss: 0.79684\n",
      "Interaction training epoch: 43, train loss: 0.86371, val loss: 0.81544\n",
      "Interaction training epoch: 44, train loss: 0.85007, val loss: 0.81382\n",
      "Interaction training epoch: 45, train loss: 0.84478, val loss: 0.79101\n",
      "Interaction training epoch: 46, train loss: 0.84159, val loss: 0.79412\n",
      "Interaction training epoch: 47, train loss: 0.85221, val loss: 0.81795\n",
      "Interaction training epoch: 48, train loss: 0.84632, val loss: 0.80287\n",
      "Interaction training epoch: 49, train loss: 0.83385, val loss: 0.78543\n",
      "Interaction training epoch: 50, train loss: 0.85416, val loss: 0.81258\n",
      "Interaction training epoch: 51, train loss: 0.85328, val loss: 0.80805\n",
      "Interaction training epoch: 52, train loss: 0.84052, val loss: 0.79723\n",
      "Interaction training epoch: 53, train loss: 0.84098, val loss: 0.79767\n",
      "Interaction training epoch: 54, train loss: 0.82954, val loss: 0.79736\n",
      "Interaction training epoch: 55, train loss: 0.82434, val loss: 0.78417\n",
      "Interaction training epoch: 56, train loss: 0.83798, val loss: 0.80426\n",
      "Interaction training epoch: 57, train loss: 0.82682, val loss: 0.78649\n",
      "Interaction training epoch: 58, train loss: 0.82020, val loss: 0.78310\n",
      "Interaction training epoch: 59, train loss: 0.83039, val loss: 0.79050\n",
      "Interaction training epoch: 60, train loss: 0.83936, val loss: 0.80290\n",
      "Interaction training epoch: 61, train loss: 0.83760, val loss: 0.78847\n",
      "Interaction training epoch: 62, train loss: 0.82541, val loss: 0.79596\n",
      "Interaction training epoch: 63, train loss: 0.82669, val loss: 0.79090\n",
      "Interaction training epoch: 64, train loss: 0.82668, val loss: 0.80182\n",
      "Interaction training epoch: 65, train loss: 0.83453, val loss: 0.79493\n",
      "Interaction training epoch: 66, train loss: 0.82425, val loss: 0.79874\n",
      "Interaction training epoch: 67, train loss: 0.82267, val loss: 0.78161\n",
      "Interaction training epoch: 68, train loss: 0.81327, val loss: 0.78148\n",
      "Interaction training epoch: 69, train loss: 0.82922, val loss: 0.79915\n",
      "Interaction training epoch: 70, train loss: 0.82343, val loss: 0.78403\n",
      "Interaction training epoch: 71, train loss: 0.81626, val loss: 0.78169\n",
      "Interaction training epoch: 72, train loss: 0.82005, val loss: 0.79279\n",
      "Interaction training epoch: 73, train loss: 0.82670, val loss: 0.79457\n",
      "Interaction training epoch: 74, train loss: 0.82211, val loss: 0.79302\n",
      "Interaction training epoch: 75, train loss: 0.82707, val loss: 0.79489\n",
      "Interaction training epoch: 76, train loss: 0.81623, val loss: 0.78474\n",
      "Interaction training epoch: 77, train loss: 0.82127, val loss: 0.79115\n",
      "Interaction training epoch: 78, train loss: 0.82509, val loss: 0.79128\n",
      "Interaction training epoch: 79, train loss: 0.82509, val loss: 0.79366\n",
      "Interaction training epoch: 80, train loss: 0.81265, val loss: 0.78187\n",
      "Interaction training epoch: 81, train loss: 0.81573, val loss: 0.78963\n",
      "Interaction training epoch: 82, train loss: 0.81666, val loss: 0.78348\n",
      "Interaction training epoch: 83, train loss: 0.81704, val loss: 0.78212\n",
      "Interaction training epoch: 84, train loss: 0.80776, val loss: 0.78875\n",
      "Interaction training epoch: 85, train loss: 0.81248, val loss: 0.78244\n",
      "Interaction training epoch: 86, train loss: 0.81951, val loss: 0.78380\n",
      "Interaction training epoch: 87, train loss: 0.81163, val loss: 0.78824\n",
      "Interaction training epoch: 88, train loss: 0.80164, val loss: 0.78117\n",
      "Interaction training epoch: 89, train loss: 0.81331, val loss: 0.78582\n",
      "Interaction training epoch: 90, train loss: 0.82137, val loss: 0.79542\n",
      "Interaction training epoch: 91, train loss: 0.80854, val loss: 0.77734\n",
      "Interaction training epoch: 92, train loss: 0.81241, val loss: 0.77994\n",
      "Interaction training epoch: 93, train loss: 0.81383, val loss: 0.79005\n",
      "Interaction training epoch: 94, train loss: 0.81033, val loss: 0.78768\n",
      "Interaction training epoch: 95, train loss: 0.80874, val loss: 0.78721\n",
      "Interaction training epoch: 96, train loss: 0.82364, val loss: 0.79496\n",
      "Interaction training epoch: 97, train loss: 0.81201, val loss: 0.79415\n",
      "Interaction training epoch: 98, train loss: 0.80877, val loss: 0.78987\n",
      "Interaction training epoch: 99, train loss: 0.79297, val loss: 0.77850\n",
      "Interaction training epoch: 100, train loss: 0.79547, val loss: 0.77573\n",
      "Interaction training epoch: 101, train loss: 0.79544, val loss: 0.78284\n",
      "Interaction training epoch: 102, train loss: 0.79413, val loss: 0.77893\n",
      "Interaction training epoch: 103, train loss: 0.79027, val loss: 0.78462\n",
      "Interaction training epoch: 104, train loss: 0.79539, val loss: 0.78256\n",
      "Interaction training epoch: 105, train loss: 0.78259, val loss: 0.77679\n",
      "Interaction training epoch: 106, train loss: 0.77960, val loss: 0.77321\n",
      "Interaction training epoch: 107, train loss: 0.79813, val loss: 0.79174\n",
      "Interaction training epoch: 108, train loss: 0.80357, val loss: 0.79871\n",
      "Interaction training epoch: 109, train loss: 0.78134, val loss: 0.76921\n",
      "Interaction training epoch: 110, train loss: 0.77718, val loss: 0.77154\n",
      "Interaction training epoch: 111, train loss: 0.78154, val loss: 0.78298\n",
      "Interaction training epoch: 112, train loss: 0.78721, val loss: 0.77831\n",
      "Interaction training epoch: 113, train loss: 0.78022, val loss: 0.77906\n",
      "Interaction training epoch: 114, train loss: 0.78260, val loss: 0.78611\n",
      "Interaction training epoch: 115, train loss: 0.78428, val loss: 0.78272\n",
      "Interaction training epoch: 116, train loss: 0.78872, val loss: 0.78427\n",
      "Interaction training epoch: 117, train loss: 0.78721, val loss: 0.78445\n",
      "Interaction training epoch: 118, train loss: 0.78087, val loss: 0.77956\n",
      "Interaction training epoch: 119, train loss: 0.78085, val loss: 0.78435\n",
      "Interaction training epoch: 120, train loss: 0.78827, val loss: 0.79372\n",
      "Interaction training epoch: 121, train loss: 0.77977, val loss: 0.77895\n",
      "Interaction training epoch: 122, train loss: 0.77910, val loss: 0.77640\n",
      "Interaction training epoch: 123, train loss: 0.78682, val loss: 0.78589\n",
      "Interaction training epoch: 124, train loss: 0.78035, val loss: 0.78167\n",
      "Interaction training epoch: 125, train loss: 0.79408, val loss: 0.79601\n",
      "Interaction training epoch: 126, train loss: 0.77705, val loss: 0.78189\n",
      "Interaction training epoch: 127, train loss: 0.78685, val loss: 0.78566\n",
      "Interaction training epoch: 128, train loss: 0.77795, val loss: 0.78162\n",
      "Interaction training epoch: 129, train loss: 0.77725, val loss: 0.77421\n",
      "Interaction training epoch: 130, train loss: 0.77987, val loss: 0.78412\n",
      "Interaction training epoch: 131, train loss: 0.77813, val loss: 0.78850\n",
      "Interaction training epoch: 132, train loss: 0.78637, val loss: 0.77812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 133, train loss: 0.78357, val loss: 0.78146\n",
      "Interaction training epoch: 134, train loss: 0.77694, val loss: 0.77653\n",
      "Interaction training epoch: 135, train loss: 0.79077, val loss: 0.78853\n",
      "Interaction training epoch: 136, train loss: 0.77526, val loss: 0.77551\n",
      "Interaction training epoch: 137, train loss: 0.78242, val loss: 0.78073\n",
      "Interaction training epoch: 138, train loss: 0.77756, val loss: 0.78606\n",
      "Interaction training epoch: 139, train loss: 0.78779, val loss: 0.78871\n",
      "Interaction training epoch: 140, train loss: 0.77683, val loss: 0.77747\n",
      "Interaction training epoch: 141, train loss: 0.77836, val loss: 0.77321\n",
      "Interaction training epoch: 142, train loss: 0.76566, val loss: 0.77380\n",
      "Interaction training epoch: 143, train loss: 0.77991, val loss: 0.77917\n",
      "Interaction training epoch: 144, train loss: 0.77964, val loss: 0.77961\n",
      "Interaction training epoch: 145, train loss: 0.77226, val loss: 0.76916\n",
      "Interaction training epoch: 146, train loss: 0.77343, val loss: 0.77538\n",
      "Interaction training epoch: 147, train loss: 0.77255, val loss: 0.77193\n",
      "Interaction training epoch: 148, train loss: 0.77376, val loss: 0.78087\n",
      "Interaction training epoch: 149, train loss: 0.77454, val loss: 0.77627\n",
      "Interaction training epoch: 150, train loss: 0.78363, val loss: 0.78299\n",
      "Interaction training epoch: 151, train loss: 0.80170, val loss: 0.80500\n",
      "Interaction training epoch: 152, train loss: 0.78249, val loss: 0.78022\n",
      "Interaction training epoch: 153, train loss: 0.76953, val loss: 0.77284\n",
      "Interaction training epoch: 154, train loss: 0.78082, val loss: 0.78009\n",
      "Interaction training epoch: 155, train loss: 0.78143, val loss: 0.78267\n",
      "Interaction training epoch: 156, train loss: 0.77857, val loss: 0.78227\n",
      "Interaction training epoch: 157, train loss: 0.77639, val loss: 0.77441\n",
      "Interaction training epoch: 158, train loss: 0.77168, val loss: 0.77404\n",
      "Interaction training epoch: 159, train loss: 0.77309, val loss: 0.77320\n",
      "Interaction training epoch: 160, train loss: 0.77364, val loss: 0.78196\n",
      "Interaction training epoch: 161, train loss: 0.77747, val loss: 0.78146\n",
      "Interaction training epoch: 162, train loss: 0.77028, val loss: 0.77925\n",
      "Interaction training epoch: 163, train loss: 0.78043, val loss: 0.78295\n",
      "Interaction training epoch: 164, train loss: 0.76176, val loss: 0.76802\n",
      "Interaction training epoch: 165, train loss: 0.77069, val loss: 0.77658\n",
      "Interaction training epoch: 166, train loss: 0.77822, val loss: 0.77505\n",
      "Interaction training epoch: 167, train loss: 0.76739, val loss: 0.77529\n",
      "Interaction training epoch: 168, train loss: 0.78030, val loss: 0.78405\n",
      "Interaction training epoch: 169, train loss: 0.78559, val loss: 0.78633\n",
      "Interaction training epoch: 170, train loss: 0.77206, val loss: 0.77579\n",
      "Interaction training epoch: 171, train loss: 0.78474, val loss: 0.77915\n",
      "Interaction training epoch: 172, train loss: 0.76270, val loss: 0.76818\n",
      "Interaction training epoch: 173, train loss: 0.77308, val loss: 0.77325\n",
      "Interaction training epoch: 174, train loss: 0.76343, val loss: 0.76048\n",
      "Interaction training epoch: 175, train loss: 0.77223, val loss: 0.77724\n",
      "Interaction training epoch: 176, train loss: 0.76468, val loss: 0.76964\n",
      "Interaction training epoch: 177, train loss: 0.77173, val loss: 0.76945\n",
      "Interaction training epoch: 178, train loss: 0.77050, val loss: 0.77338\n",
      "Interaction training epoch: 179, train loss: 0.77987, val loss: 0.78205\n",
      "Interaction training epoch: 180, train loss: 0.76941, val loss: 0.76992\n",
      "Interaction training epoch: 181, train loss: 0.77336, val loss: 0.77646\n",
      "Interaction training epoch: 182, train loss: 0.76529, val loss: 0.77190\n",
      "Interaction training epoch: 183, train loss: 0.77631, val loss: 0.77978\n",
      "Interaction training epoch: 184, train loss: 0.77069, val loss: 0.77099\n",
      "Interaction training epoch: 185, train loss: 0.77133, val loss: 0.77793\n",
      "Interaction training epoch: 186, train loss: 0.77375, val loss: 0.77557\n",
      "Interaction training epoch: 187, train loss: 0.76127, val loss: 0.77383\n",
      "Interaction training epoch: 188, train loss: 0.77684, val loss: 0.77263\n",
      "Interaction training epoch: 189, train loss: 0.76446, val loss: 0.77038\n",
      "Interaction training epoch: 190, train loss: 0.77026, val loss: 0.77451\n",
      "Interaction training epoch: 191, train loss: 0.77397, val loss: 0.77401\n",
      "Interaction training epoch: 192, train loss: 0.76475, val loss: 0.76761\n",
      "Interaction training epoch: 193, train loss: 0.76889, val loss: 0.77037\n",
      "Interaction training epoch: 194, train loss: 0.77328, val loss: 0.77393\n",
      "Interaction training epoch: 195, train loss: 0.75614, val loss: 0.76265\n",
      "Interaction training epoch: 196, train loss: 0.77422, val loss: 0.77752\n",
      "Interaction training epoch: 197, train loss: 0.76804, val loss: 0.77225\n",
      "Interaction training epoch: 198, train loss: 0.76183, val loss: 0.76681\n",
      "Interaction training epoch: 199, train loss: 0.77246, val loss: 0.77720\n",
      "Interaction training epoch: 200, train loss: 0.76628, val loss: 0.76976\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########1 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.76822, val loss: 0.77086\n",
      "Interaction tuning epoch: 2, train loss: 0.77048, val loss: 0.76728\n",
      "Interaction tuning epoch: 3, train loss: 0.76935, val loss: 0.77547\n",
      "Interaction tuning epoch: 4, train loss: 0.76702, val loss: 0.76940\n",
      "Interaction tuning epoch: 5, train loss: 0.77966, val loss: 0.78818\n",
      "Interaction tuning epoch: 6, train loss: 0.77006, val loss: 0.76984\n",
      "Interaction tuning epoch: 7, train loss: 0.76885, val loss: 0.77048\n",
      "Interaction tuning epoch: 8, train loss: 0.76956, val loss: 0.76854\n",
      "Interaction tuning epoch: 9, train loss: 0.77716, val loss: 0.78382\n",
      "Interaction tuning epoch: 10, train loss: 0.76331, val loss: 0.76452\n",
      "Interaction tuning epoch: 11, train loss: 0.77231, val loss: 0.77105\n",
      "Interaction tuning epoch: 12, train loss: 0.77019, val loss: 0.78032\n",
      "Interaction tuning epoch: 13, train loss: 0.78897, val loss: 0.77464\n",
      "Interaction tuning epoch: 14, train loss: 0.75570, val loss: 0.76909\n",
      "Interaction tuning epoch: 15, train loss: 0.77583, val loss: 0.78127\n",
      "Interaction tuning epoch: 16, train loss: 0.77609, val loss: 0.78002\n",
      "Interaction tuning epoch: 17, train loss: 0.76930, val loss: 0.77000\n",
      "Interaction tuning epoch: 18, train loss: 0.76460, val loss: 0.76630\n",
      "Interaction tuning epoch: 19, train loss: 0.76933, val loss: 0.77503\n",
      "Interaction tuning epoch: 20, train loss: 0.76863, val loss: 0.77040\n",
      "Interaction tuning epoch: 21, train loss: 0.76737, val loss: 0.76949\n",
      "Interaction tuning epoch: 22, train loss: 0.76708, val loss: 0.77383\n",
      "Interaction tuning epoch: 23, train loss: 0.77653, val loss: 0.77145\n",
      "Interaction tuning epoch: 24, train loss: 0.76178, val loss: 0.76869\n",
      "Interaction tuning epoch: 25, train loss: 0.77019, val loss: 0.76626\n",
      "Interaction tuning epoch: 26, train loss: 0.76942, val loss: 0.77753\n",
      "Interaction tuning epoch: 27, train loss: 0.77303, val loss: 0.76758\n",
      "Interaction tuning epoch: 28, train loss: 0.77391, val loss: 0.77514\n",
      "Interaction tuning epoch: 29, train loss: 0.76396, val loss: 0.76754\n",
      "Interaction tuning epoch: 30, train loss: 0.77164, val loss: 0.77277\n",
      "Interaction tuning epoch: 31, train loss: 0.76987, val loss: 0.77245\n",
      "Interaction tuning epoch: 32, train loss: 0.76293, val loss: 0.76574\n",
      "Interaction tuning epoch: 33, train loss: 0.77926, val loss: 0.78048\n",
      "Interaction tuning epoch: 34, train loss: 0.76575, val loss: 0.77026\n",
      "Interaction tuning epoch: 35, train loss: 0.77255, val loss: 0.77664\n",
      "Interaction tuning epoch: 36, train loss: 0.76826, val loss: 0.76984\n",
      "Interaction tuning epoch: 37, train loss: 0.77133, val loss: 0.77561\n",
      "Interaction tuning epoch: 38, train loss: 0.77737, val loss: 0.77826\n",
      "Interaction tuning epoch: 39, train loss: 0.77570, val loss: 0.77520\n",
      "Interaction tuning epoch: 40, train loss: 0.77054, val loss: 0.77490\n",
      "Interaction tuning epoch: 41, train loss: 0.76767, val loss: 0.76850\n",
      "Interaction tuning epoch: 42, train loss: 0.76730, val loss: 0.77037\n",
      "Interaction tuning epoch: 43, train loss: 0.76490, val loss: 0.77296\n",
      "Interaction tuning epoch: 44, train loss: 0.76262, val loss: 0.76891\n",
      "Interaction tuning epoch: 45, train loss: 0.77086, val loss: 0.77484\n",
      "Interaction tuning epoch: 46, train loss: 0.76193, val loss: 0.76501\n",
      "Interaction tuning epoch: 47, train loss: 0.77002, val loss: 0.77306\n",
      "Interaction tuning epoch: 48, train loss: 0.76039, val loss: 0.76619\n",
      "Interaction tuning epoch: 49, train loss: 0.77522, val loss: 0.77484\n",
      "Interaction tuning epoch: 50, train loss: 0.76760, val loss: 0.77267\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 38.177565574645996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After the gam stage, training error is 0.76760 , validation error is 0.77267\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 18.967425\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.633422 validation MAE=0.725257,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 2: observed MAE=0.590381 validation MAE=0.706392,rank=5\n",
      "[SoftImpute] Iter 3: observed MAE=0.554736 validation MAE=0.690345,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.524892 validation MAE=0.676497,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.498949 validation MAE=0.664433,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.475948 validation MAE=0.654188,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.457462 validation MAE=0.645835,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.439939 validation MAE=0.637638,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.424698 validation MAE=0.630569,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.408913 validation MAE=0.623375,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.396017 validation MAE=0.617940,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.383673 validation MAE=0.612267,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.371866 validation MAE=0.607000,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.362238 validation MAE=0.602638,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.352634 validation MAE=0.597911,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.343303 validation MAE=0.594321,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.335285 validation MAE=0.589924,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.327555 validation MAE=0.586817,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.320754 validation MAE=0.583498,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.314615 validation MAE=0.580733,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.308495 validation MAE=0.577888,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.304705 validation MAE=0.575099,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.303089 validation MAE=0.572702,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.301040 validation MAE=0.570021,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.297808 validation MAE=0.568168,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.293902 validation MAE=0.565966,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.289986 validation MAE=0.563553,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.284659 validation MAE=0.561342,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.282234 validation MAE=0.560021,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.279426 validation MAE=0.557831,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.276908 validation MAE=0.555915,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.273801 validation MAE=0.554673,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.269944 validation MAE=0.552687,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.267081 validation MAE=0.551501,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.264296 validation MAE=0.550364,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.261569 validation MAE=0.548623,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.259728 validation MAE=0.547179,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.258121 validation MAE=0.545854,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.254317 validation MAE=0.543947,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.253114 validation MAE=0.543474,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.250173 validation MAE=0.542113,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.249126 validation MAE=0.541305,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.246380 validation MAE=0.539969,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.243859 validation MAE=0.539142,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.243339 validation MAE=0.538420,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.240889 validation MAE=0.537255,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.238763 validation MAE=0.536634,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.237545 validation MAE=0.535271,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.235943 validation MAE=0.534780,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.234709 validation MAE=0.533066,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.233245 validation MAE=0.532793,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.231426 validation MAE=0.531563,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.230058 validation MAE=0.531026,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.228950 validation MAE=0.530034,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.227117 validation MAE=0.529815,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.226489 validation MAE=0.528072,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.225357 validation MAE=0.527303,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.224197 validation MAE=0.526543,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.222883 validation MAE=0.525755,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.221460 validation MAE=0.524965,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.219397 validation MAE=0.523974,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.218881 validation MAE=0.523818,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.218622 validation MAE=0.522973,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.217599 validation MAE=0.521666,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.215970 validation MAE=0.521229,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.214650 validation MAE=0.520884,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.214617 validation MAE=0.520070,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.213792 validation MAE=0.518781,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.211807 validation MAE=0.518325,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.212186 validation MAE=0.517968,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.211165 validation MAE=0.516485,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.209282 validation MAE=0.516204,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.208903 validation MAE=0.515920,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.207525 validation MAE=0.515107,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.207138 validation MAE=0.515019,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.207009 validation MAE=0.513785,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.206298 validation MAE=0.512837,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.204538 validation MAE=0.511515,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.203314 validation MAE=0.510810,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.203364 validation MAE=0.510475,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.201688 validation MAE=0.509711,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.201928 validation MAE=0.509538,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.201040 validation MAE=0.508292,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.200737 validation MAE=0.507810,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.200567 validation MAE=0.507555,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.199541 validation MAE=0.506870,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.199306 validation MAE=0.506103,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.197866 validation MAE=0.505004,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.196368 validation MAE=0.504286,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.195598 validation MAE=0.503874,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.196151 validation MAE=0.503585,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.195583 validation MAE=0.503067,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.195764 validation MAE=0.501983,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.194377 validation MAE=0.501183,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.193863 validation MAE=0.501019,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.193098 validation MAE=0.500378,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.192071 validation MAE=0.499696,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.192428 validation MAE=0.498652,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.191886 validation MAE=0.498353,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.191321 validation MAE=0.497617,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.190799 validation MAE=0.497209,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.189483 validation MAE=0.496239,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.188647 validation MAE=0.496018,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.188161 validation MAE=0.494798,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.188447 validation MAE=0.494959,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.188070 validation MAE=0.493966,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.188256 validation MAE=0.493667,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.187540 validation MAE=0.492625,rank=5\n",
      "[SoftImpute] Iter 109: observed MAE=0.186670 validation MAE=0.491932,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.186391 validation MAE=0.491711,rank=5\n",
      "[SoftImpute] Iter 111: observed MAE=0.185538 validation MAE=0.491431,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.185535 validation MAE=0.490320,rank=5\n",
      "[SoftImpute] Iter 113: observed MAE=0.185682 validation MAE=0.490197,rank=5\n",
      "[SoftImpute] Iter 114: observed MAE=0.184564 validation MAE=0.489548,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 115: observed MAE=0.183574 validation MAE=0.488920,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.183129 validation MAE=0.488192,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.183245 validation MAE=0.487669,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.182886 validation MAE=0.487237,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.182723 validation MAE=0.486618,rank=5\n",
      "[SoftImpute] Iter 120: observed MAE=0.182021 validation MAE=0.486212,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.181742 validation MAE=0.485267,rank=5\n",
      "[SoftImpute] Iter 122: observed MAE=0.181679 validation MAE=0.484736,rank=5\n",
      "[SoftImpute] Iter 123: observed MAE=0.181218 validation MAE=0.484382,rank=5\n",
      "[SoftImpute] Iter 124: observed MAE=0.180621 validation MAE=0.483440,rank=5\n",
      "[SoftImpute] Iter 125: observed MAE=0.181280 validation MAE=0.483198,rank=5\n",
      "[SoftImpute] Iter 126: observed MAE=0.180407 validation MAE=0.482592,rank=5\n",
      "[SoftImpute] Iter 127: observed MAE=0.179857 validation MAE=0.482017,rank=5\n",
      "[SoftImpute] Iter 128: observed MAE=0.179354 validation MAE=0.481128,rank=5\n",
      "[SoftImpute] Iter 129: observed MAE=0.179075 validation MAE=0.480647,rank=5\n",
      "[SoftImpute] Iter 130: observed MAE=0.178650 validation MAE=0.480170,rank=5\n",
      "[SoftImpute] Iter 131: observed MAE=0.178228 validation MAE=0.479475,rank=5\n",
      "[SoftImpute] Iter 132: observed MAE=0.178676 validation MAE=0.479046,rank=5\n",
      "[SoftImpute] Iter 133: observed MAE=0.178197 validation MAE=0.478343,rank=5\n",
      "[SoftImpute] Iter 134: observed MAE=0.177705 validation MAE=0.478536,rank=5\n",
      "[SoftImpute] Iter 135: observed MAE=0.177007 validation MAE=0.477963,rank=5\n",
      "[SoftImpute] Iter 136: observed MAE=0.176864 validation MAE=0.477200,rank=5\n",
      "[SoftImpute] Iter 137: observed MAE=0.176233 validation MAE=0.476899,rank=5\n",
      "[SoftImpute] Iter 138: observed MAE=0.176794 validation MAE=0.475814,rank=5\n",
      "[SoftImpute] Iter 139: observed MAE=0.176878 validation MAE=0.475556,rank=5\n",
      "[SoftImpute] Iter 140: observed MAE=0.176672 validation MAE=0.475124,rank=5\n",
      "[SoftImpute] Iter 141: observed MAE=0.176766 validation MAE=0.474575,rank=5\n",
      "[SoftImpute] Iter 142: observed MAE=0.176435 validation MAE=0.474655,rank=5\n",
      "[SoftImpute] Iter 143: observed MAE=0.176161 validation MAE=0.473851,rank=5\n",
      "[SoftImpute] Iter 144: observed MAE=0.175801 validation MAE=0.473136,rank=5\n",
      "[SoftImpute] Iter 145: observed MAE=0.175538 validation MAE=0.472737,rank=5\n",
      "[SoftImpute] Iter 146: observed MAE=0.174885 validation MAE=0.472385,rank=5\n",
      "[SoftImpute] Iter 147: observed MAE=0.174576 validation MAE=0.472104,rank=5\n",
      "[SoftImpute] Iter 148: observed MAE=0.173731 validation MAE=0.471504,rank=5\n",
      "[SoftImpute] Iter 149: observed MAE=0.173383 validation MAE=0.471309,rank=5\n",
      "[SoftImpute] Iter 150: observed MAE=0.174195 validation MAE=0.470890,rank=5\n",
      "[SoftImpute] Iter 151: observed MAE=0.173932 validation MAE=0.470387,rank=5\n",
      "[SoftImpute] Iter 152: observed MAE=0.173200 validation MAE=0.469948,rank=5\n",
      "[SoftImpute] Iter 153: observed MAE=0.172939 validation MAE=0.469789,rank=5\n",
      "[SoftImpute] Iter 154: observed MAE=0.172881 validation MAE=0.469215,rank=5\n",
      "[SoftImpute] Iter 155: observed MAE=0.173034 validation MAE=0.469165,rank=5\n",
      "[SoftImpute] Iter 156: observed MAE=0.173414 validation MAE=0.468230,rank=5\n",
      "[SoftImpute] Iter 157: observed MAE=0.173586 validation MAE=0.467769,rank=5\n",
      "[SoftImpute] Iter 158: observed MAE=0.172742 validation MAE=0.467458,rank=5\n",
      "[SoftImpute] Iter 159: observed MAE=0.172749 validation MAE=0.467324,rank=5\n",
      "[SoftImpute] Iter 160: observed MAE=0.171761 validation MAE=0.466652,rank=5\n",
      "[SoftImpute] Iter 161: observed MAE=0.172074 validation MAE=0.466892,rank=5\n",
      "[SoftImpute] Iter 162: observed MAE=0.172095 validation MAE=0.466624,rank=5\n",
      "[SoftImpute] Iter 163: observed MAE=0.171966 validation MAE=0.465805,rank=5\n",
      "[SoftImpute] Iter 164: observed MAE=0.170638 validation MAE=0.465357,rank=5\n",
      "[SoftImpute] Iter 165: observed MAE=0.170941 validation MAE=0.464900,rank=5\n",
      "[SoftImpute] Iter 166: observed MAE=0.170329 validation MAE=0.464480,rank=5\n",
      "[SoftImpute] Iter 167: observed MAE=0.170377 validation MAE=0.463903,rank=5\n",
      "[SoftImpute] Iter 168: observed MAE=0.170338 validation MAE=0.463389,rank=5\n",
      "[SoftImpute] Iter 169: observed MAE=0.170679 validation MAE=0.463617,rank=5\n",
      "[SoftImpute] Iter 170: observed MAE=0.170467 validation MAE=0.463639,rank=5\n",
      "[SoftImpute] Iter 171: observed MAE=0.170314 validation MAE=0.462810,rank=5\n",
      "[SoftImpute] Iter 172: observed MAE=0.169553 validation MAE=0.462660,rank=5\n",
      "[SoftImpute] Iter 173: observed MAE=0.169825 validation MAE=0.462199,rank=5\n",
      "[SoftImpute] Iter 174: observed MAE=0.169381 validation MAE=0.461726,rank=5\n",
      "[SoftImpute] Iter 175: observed MAE=0.169879 validation MAE=0.461352,rank=5\n",
      "[SoftImpute] Iter 176: observed MAE=0.169859 validation MAE=0.460848,rank=5\n",
      "[SoftImpute] Iter 177: observed MAE=0.169824 validation MAE=0.460692,rank=5\n",
      "[SoftImpute] Iter 178: observed MAE=0.168880 validation MAE=0.460085,rank=5\n",
      "[SoftImpute] Iter 179: observed MAE=0.169359 validation MAE=0.459189,rank=5\n",
      "[SoftImpute] Iter 180: observed MAE=0.168886 validation MAE=0.459032,rank=5\n",
      "[SoftImpute] Iter 181: observed MAE=0.168937 validation MAE=0.459053,rank=5\n",
      "[SoftImpute] Iter 182: observed MAE=0.168693 validation MAE=0.459021,rank=5\n",
      "[SoftImpute] Iter 183: observed MAE=0.168039 validation MAE=0.458516,rank=5\n",
      "[SoftImpute] Iter 184: observed MAE=0.167813 validation MAE=0.458151,rank=5\n",
      "[SoftImpute] Iter 185: observed MAE=0.168316 validation MAE=0.457710,rank=5\n",
      "[SoftImpute] Iter 186: observed MAE=0.168035 validation MAE=0.457250,rank=5\n",
      "[SoftImpute] Iter 187: observed MAE=0.168214 validation MAE=0.456846,rank=5\n",
      "[SoftImpute] Iter 188: observed MAE=0.167701 validation MAE=0.456715,rank=5\n",
      "[SoftImpute] Iter 189: observed MAE=0.167606 validation MAE=0.456135,rank=5\n",
      "[SoftImpute] Iter 190: observed MAE=0.167921 validation MAE=0.455745,rank=5\n",
      "[SoftImpute] Iter 191: observed MAE=0.167739 validation MAE=0.455620,rank=5\n",
      "[SoftImpute] Iter 192: observed MAE=0.167223 validation MAE=0.455377,rank=5\n",
      "[SoftImpute] Iter 193: observed MAE=0.167229 validation MAE=0.455282,rank=5\n",
      "[SoftImpute] Iter 194: observed MAE=0.166677 validation MAE=0.454731,rank=5\n",
      "[SoftImpute] Iter 195: observed MAE=0.166605 validation MAE=0.454255,rank=5\n",
      "[SoftImpute] Iter 196: observed MAE=0.166723 validation MAE=0.453759,rank=5\n",
      "[SoftImpute] Iter 197: observed MAE=0.166692 validation MAE=0.453705,rank=5\n",
      "[SoftImpute] Iter 198: observed MAE=0.166143 validation MAE=0.453056,rank=5\n",
      "[SoftImpute] Iter 199: observed MAE=0.166097 validation MAE=0.452784,rank=5\n",
      "[SoftImpute] Iter 200: observed MAE=0.165464 validation MAE=0.452525,rank=5\n",
      "[SoftImpute] Stopped after iteration 200 for lambda=0.379349\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 7.103621959686279\n",
      "After the matrix factor stage, training error is 0.16546, validation error is 0.45252\n",
      "3\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.02657, val loss: 3.99853\n",
      "Main effects training epoch: 2, train loss: 3.85487, val loss: 3.82095\n",
      "Main effects training epoch: 3, train loss: 3.66470, val loss: 3.61457\n",
      "Main effects training epoch: 4, train loss: 3.50054, val loss: 3.44080\n",
      "Main effects training epoch: 5, train loss: 3.44398, val loss: 3.36816\n",
      "Main effects training epoch: 6, train loss: 3.34572, val loss: 3.26284\n",
      "Main effects training epoch: 7, train loss: 3.33067, val loss: 3.26329\n",
      "Main effects training epoch: 8, train loss: 3.33082, val loss: 3.26940\n",
      "Main effects training epoch: 9, train loss: 3.20773, val loss: 3.13788\n",
      "Main effects training epoch: 10, train loss: 3.12923, val loss: 3.05523\n",
      "Main effects training epoch: 11, train loss: 3.12484, val loss: 3.04933\n",
      "Main effects training epoch: 12, train loss: 3.06786, val loss: 2.99107\n",
      "Main effects training epoch: 13, train loss: 3.04787, val loss: 2.97613\n",
      "Main effects training epoch: 14, train loss: 2.95319, val loss: 2.88542\n",
      "Main effects training epoch: 15, train loss: 2.89636, val loss: 2.82656\n",
      "Main effects training epoch: 16, train loss: 2.81747, val loss: 2.75146\n",
      "Main effects training epoch: 17, train loss: 2.75226, val loss: 2.69112\n",
      "Main effects training epoch: 18, train loss: 2.70738, val loss: 2.65633\n",
      "Main effects training epoch: 19, train loss: 2.63248, val loss: 2.58219\n",
      "Main effects training epoch: 20, train loss: 2.58568, val loss: 2.53762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 21, train loss: 2.50641, val loss: 2.46148\n",
      "Main effects training epoch: 22, train loss: 2.52657, val loss: 2.47994\n",
      "Main effects training epoch: 23, train loss: 2.44061, val loss: 2.40216\n",
      "Main effects training epoch: 24, train loss: 2.39749, val loss: 2.35327\n",
      "Main effects training epoch: 25, train loss: 2.39312, val loss: 2.35800\n",
      "Main effects training epoch: 26, train loss: 2.33631, val loss: 2.29628\n",
      "Main effects training epoch: 27, train loss: 2.33168, val loss: 2.29953\n",
      "Main effects training epoch: 28, train loss: 2.26036, val loss: 2.22319\n",
      "Main effects training epoch: 29, train loss: 2.26433, val loss: 2.22919\n",
      "Main effects training epoch: 30, train loss: 2.21246, val loss: 2.17872\n",
      "Main effects training epoch: 31, train loss: 2.20480, val loss: 2.18066\n",
      "Main effects training epoch: 32, train loss: 2.13691, val loss: 2.10740\n",
      "Main effects training epoch: 33, train loss: 2.14782, val loss: 2.11946\n",
      "Main effects training epoch: 34, train loss: 2.07735, val loss: 2.05198\n",
      "Main effects training epoch: 35, train loss: 2.08391, val loss: 2.05786\n",
      "Main effects training epoch: 36, train loss: 2.05081, val loss: 2.02334\n",
      "Main effects training epoch: 37, train loss: 2.03867, val loss: 2.01715\n",
      "Main effects training epoch: 38, train loss: 2.01036, val loss: 1.98786\n",
      "Main effects training epoch: 39, train loss: 2.02340, val loss: 1.99755\n",
      "Main effects training epoch: 40, train loss: 1.98932, val loss: 1.96345\n",
      "Main effects training epoch: 41, train loss: 1.96926, val loss: 1.94767\n",
      "Main effects training epoch: 42, train loss: 1.95529, val loss: 1.93571\n",
      "Main effects training epoch: 43, train loss: 1.94780, val loss: 1.93177\n",
      "Main effects training epoch: 44, train loss: 1.92657, val loss: 1.90176\n",
      "Main effects training epoch: 45, train loss: 1.91518, val loss: 1.89083\n",
      "Main effects training epoch: 46, train loss: 1.89772, val loss: 1.87272\n",
      "Main effects training epoch: 47, train loss: 1.88881, val loss: 1.87315\n",
      "Main effects training epoch: 48, train loss: 1.89133, val loss: 1.86815\n",
      "Main effects training epoch: 49, train loss: 1.88033, val loss: 1.86137\n",
      "Main effects training epoch: 50, train loss: 1.87145, val loss: 1.85501\n",
      "Main effects training epoch: 51, train loss: 1.84959, val loss: 1.83096\n",
      "Main effects training epoch: 52, train loss: 1.84211, val loss: 1.82414\n",
      "Main effects training epoch: 53, train loss: 1.83229, val loss: 1.80932\n",
      "Main effects training epoch: 54, train loss: 1.81664, val loss: 1.80037\n",
      "Main effects training epoch: 55, train loss: 1.81186, val loss: 1.78741\n",
      "Main effects training epoch: 56, train loss: 1.81718, val loss: 1.80889\n",
      "Main effects training epoch: 57, train loss: 1.80507, val loss: 1.77796\n",
      "Main effects training epoch: 58, train loss: 1.80490, val loss: 1.79304\n",
      "Main effects training epoch: 59, train loss: 1.79189, val loss: 1.77169\n",
      "Main effects training epoch: 60, train loss: 1.77514, val loss: 1.75606\n",
      "Main effects training epoch: 61, train loss: 1.77552, val loss: 1.76067\n",
      "Main effects training epoch: 62, train loss: 1.78172, val loss: 1.75972\n",
      "Main effects training epoch: 63, train loss: 1.77354, val loss: 1.75968\n",
      "Main effects training epoch: 64, train loss: 1.75704, val loss: 1.74205\n",
      "Main effects training epoch: 65, train loss: 1.76723, val loss: 1.74891\n",
      "Main effects training epoch: 66, train loss: 1.75638, val loss: 1.73912\n",
      "Main effects training epoch: 67, train loss: 1.75123, val loss: 1.73696\n",
      "Main effects training epoch: 68, train loss: 1.74563, val loss: 1.72493\n",
      "Main effects training epoch: 69, train loss: 1.75932, val loss: 1.74683\n",
      "Main effects training epoch: 70, train loss: 1.74433, val loss: 1.73037\n",
      "Main effects training epoch: 71, train loss: 1.73901, val loss: 1.72897\n",
      "Main effects training epoch: 72, train loss: 1.73545, val loss: 1.72252\n",
      "Main effects training epoch: 73, train loss: 1.74007, val loss: 1.72779\n",
      "Main effects training epoch: 74, train loss: 1.73551, val loss: 1.72900\n",
      "Main effects training epoch: 75, train loss: 1.72664, val loss: 1.70933\n",
      "Main effects training epoch: 76, train loss: 1.73838, val loss: 1.72913\n",
      "Main effects training epoch: 77, train loss: 1.72800, val loss: 1.71661\n",
      "Main effects training epoch: 78, train loss: 1.72469, val loss: 1.71734\n",
      "Main effects training epoch: 79, train loss: 1.72092, val loss: 1.71182\n",
      "Main effects training epoch: 80, train loss: 1.72389, val loss: 1.71239\n",
      "Main effects training epoch: 81, train loss: 1.71688, val loss: 1.71107\n",
      "Main effects training epoch: 82, train loss: 1.71771, val loss: 1.70612\n",
      "Main effects training epoch: 83, train loss: 1.71364, val loss: 1.70966\n",
      "Main effects training epoch: 84, train loss: 1.71683, val loss: 1.70812\n",
      "Main effects training epoch: 85, train loss: 1.71294, val loss: 1.70677\n",
      "Main effects training epoch: 86, train loss: 1.70986, val loss: 1.70247\n",
      "Main effects training epoch: 87, train loss: 1.70944, val loss: 1.69997\n",
      "Main effects training epoch: 88, train loss: 1.70778, val loss: 1.70626\n",
      "Main effects training epoch: 89, train loss: 1.70746, val loss: 1.70030\n",
      "Main effects training epoch: 90, train loss: 1.70205, val loss: 1.70125\n",
      "Main effects training epoch: 91, train loss: 1.70056, val loss: 1.69596\n",
      "Main effects training epoch: 92, train loss: 1.69977, val loss: 1.70164\n",
      "Main effects training epoch: 93, train loss: 1.69588, val loss: 1.69261\n",
      "Main effects training epoch: 94, train loss: 1.69917, val loss: 1.69772\n",
      "Main effects training epoch: 95, train loss: 1.69256, val loss: 1.69384\n",
      "Main effects training epoch: 96, train loss: 1.69180, val loss: 1.68531\n",
      "Main effects training epoch: 97, train loss: 1.68225, val loss: 1.68210\n",
      "Main effects training epoch: 98, train loss: 1.68132, val loss: 1.68245\n",
      "Main effects training epoch: 99, train loss: 1.67353, val loss: 1.67338\n",
      "Main effects training epoch: 100, train loss: 1.66893, val loss: 1.67631\n",
      "Main effects training epoch: 101, train loss: 1.66680, val loss: 1.67930\n",
      "Main effects training epoch: 102, train loss: 1.66370, val loss: 1.66646\n",
      "Main effects training epoch: 103, train loss: 1.65161, val loss: 1.67099\n",
      "Main effects training epoch: 104, train loss: 1.65343, val loss: 1.66083\n",
      "Main effects training epoch: 105, train loss: 1.64968, val loss: 1.67368\n",
      "Main effects training epoch: 106, train loss: 1.64473, val loss: 1.67021\n",
      "Main effects training epoch: 107, train loss: 1.65028, val loss: 1.66173\n",
      "Main effects training epoch: 108, train loss: 1.64637, val loss: 1.67640\n",
      "Main effects training epoch: 109, train loss: 1.64222, val loss: 1.66150\n",
      "Main effects training epoch: 110, train loss: 1.64399, val loss: 1.65375\n",
      "Main effects training epoch: 111, train loss: 1.64444, val loss: 1.67209\n",
      "Main effects training epoch: 112, train loss: 1.63412, val loss: 1.65862\n",
      "Main effects training epoch: 113, train loss: 1.62997, val loss: 1.65620\n",
      "Main effects training epoch: 114, train loss: 1.64042, val loss: 1.65153\n",
      "Main effects training epoch: 115, train loss: 1.64212, val loss: 1.67923\n",
      "Main effects training epoch: 116, train loss: 1.62788, val loss: 1.64262\n",
      "Main effects training epoch: 117, train loss: 1.62859, val loss: 1.65407\n",
      "Main effects training epoch: 118, train loss: 1.61919, val loss: 1.64306\n",
      "Main effects training epoch: 119, train loss: 1.62645, val loss: 1.65741\n",
      "Main effects training epoch: 120, train loss: 1.62128, val loss: 1.65038\n",
      "Main effects training epoch: 121, train loss: 1.62298, val loss: 1.64622\n",
      "Main effects training epoch: 122, train loss: 1.62782, val loss: 1.65210\n",
      "Main effects training epoch: 123, train loss: 1.61169, val loss: 1.63825\n",
      "Main effects training epoch: 124, train loss: 1.61118, val loss: 1.63593\n",
      "Main effects training epoch: 125, train loss: 1.62604, val loss: 1.66070\n",
      "Main effects training epoch: 126, train loss: 1.61805, val loss: 1.63716\n",
      "Main effects training epoch: 127, train loss: 1.60904, val loss: 1.64534\n",
      "Main effects training epoch: 128, train loss: 1.61047, val loss: 1.63839\n",
      "Main effects training epoch: 129, train loss: 1.61058, val loss: 1.64423\n",
      "Main effects training epoch: 130, train loss: 1.60398, val loss: 1.63045\n",
      "Main effects training epoch: 131, train loss: 1.60221, val loss: 1.63601\n",
      "Main effects training epoch: 132, train loss: 1.59725, val loss: 1.63341\n",
      "Main effects training epoch: 133, train loss: 1.60421, val loss: 1.63671\n",
      "Main effects training epoch: 134, train loss: 1.59972, val loss: 1.63525\n",
      "Main effects training epoch: 135, train loss: 1.59737, val loss: 1.63757\n",
      "Main effects training epoch: 136, train loss: 1.60025, val loss: 1.63027\n",
      "Main effects training epoch: 137, train loss: 1.60342, val loss: 1.64442\n",
      "Main effects training epoch: 138, train loss: 1.59711, val loss: 1.62318\n",
      "Main effects training epoch: 139, train loss: 1.58993, val loss: 1.63281\n",
      "Main effects training epoch: 140, train loss: 1.58867, val loss: 1.62995\n",
      "Main effects training epoch: 141, train loss: 1.59085, val loss: 1.62505\n",
      "Main effects training epoch: 142, train loss: 1.58587, val loss: 1.62633\n",
      "Main effects training epoch: 143, train loss: 1.58967, val loss: 1.63124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 144, train loss: 1.58550, val loss: 1.61846\n",
      "Main effects training epoch: 145, train loss: 1.58521, val loss: 1.62663\n",
      "Main effects training epoch: 146, train loss: 1.58111, val loss: 1.62024\n",
      "Main effects training epoch: 147, train loss: 1.58128, val loss: 1.62240\n",
      "Main effects training epoch: 148, train loss: 1.57902, val loss: 1.62488\n",
      "Main effects training epoch: 149, train loss: 1.58400, val loss: 1.61615\n",
      "Main effects training epoch: 150, train loss: 1.58344, val loss: 1.63380\n",
      "Main effects training epoch: 151, train loss: 1.57934, val loss: 1.62315\n",
      "Main effects training epoch: 152, train loss: 1.57646, val loss: 1.61352\n",
      "Main effects training epoch: 153, train loss: 1.57913, val loss: 1.62505\n",
      "Main effects training epoch: 154, train loss: 1.59471, val loss: 1.63353\n",
      "Main effects training epoch: 155, train loss: 1.57971, val loss: 1.61835\n",
      "Main effects training epoch: 156, train loss: 1.57778, val loss: 1.61537\n",
      "Main effects training epoch: 157, train loss: 1.57411, val loss: 1.62767\n",
      "Main effects training epoch: 158, train loss: 1.57364, val loss: 1.61097\n",
      "Main effects training epoch: 159, train loss: 1.57165, val loss: 1.61522\n",
      "Main effects training epoch: 160, train loss: 1.57420, val loss: 1.62208\n",
      "Main effects training epoch: 161, train loss: 1.57601, val loss: 1.62210\n",
      "Main effects training epoch: 162, train loss: 1.57205, val loss: 1.60487\n",
      "Main effects training epoch: 163, train loss: 1.57202, val loss: 1.61759\n",
      "Main effects training epoch: 164, train loss: 1.57608, val loss: 1.62493\n",
      "Main effects training epoch: 165, train loss: 1.56984, val loss: 1.60429\n",
      "Main effects training epoch: 166, train loss: 1.57796, val loss: 1.62823\n",
      "Main effects training epoch: 167, train loss: 1.57003, val loss: 1.60273\n",
      "Main effects training epoch: 168, train loss: 1.56715, val loss: 1.60825\n",
      "Main effects training epoch: 169, train loss: 1.56913, val loss: 1.61645\n",
      "Main effects training epoch: 170, train loss: 1.56904, val loss: 1.62209\n",
      "Main effects training epoch: 171, train loss: 1.57223, val loss: 1.60325\n",
      "Main effects training epoch: 172, train loss: 1.57600, val loss: 1.63005\n",
      "Main effects training epoch: 173, train loss: 1.56405, val loss: 1.60761\n",
      "Main effects training epoch: 174, train loss: 1.55927, val loss: 1.59922\n",
      "Main effects training epoch: 175, train loss: 1.56722, val loss: 1.61574\n",
      "Main effects training epoch: 176, train loss: 1.55995, val loss: 1.60469\n",
      "Main effects training epoch: 177, train loss: 1.56451, val loss: 1.59971\n",
      "Main effects training epoch: 178, train loss: 1.56835, val loss: 1.62878\n",
      "Main effects training epoch: 179, train loss: 1.55648, val loss: 1.59757\n",
      "Main effects training epoch: 180, train loss: 1.55010, val loss: 1.60234\n",
      "Main effects training epoch: 181, train loss: 1.55031, val loss: 1.60145\n",
      "Main effects training epoch: 182, train loss: 1.55071, val loss: 1.61539\n",
      "Main effects training epoch: 183, train loss: 1.54779, val loss: 1.59340\n",
      "Main effects training epoch: 184, train loss: 1.55334, val loss: 1.62184\n",
      "Main effects training epoch: 185, train loss: 1.54866, val loss: 1.60934\n",
      "Main effects training epoch: 186, train loss: 1.54450, val loss: 1.59586\n",
      "Main effects training epoch: 187, train loss: 1.54352, val loss: 1.59942\n",
      "Main effects training epoch: 188, train loss: 1.54020, val loss: 1.59874\n",
      "Main effects training epoch: 189, train loss: 1.54022, val loss: 1.59977\n",
      "Main effects training epoch: 190, train loss: 1.54136, val loss: 1.58740\n",
      "Main effects training epoch: 191, train loss: 1.53678, val loss: 1.59660\n",
      "Main effects training epoch: 192, train loss: 1.54221, val loss: 1.61083\n",
      "Main effects training epoch: 193, train loss: 1.53884, val loss: 1.60873\n",
      "Main effects training epoch: 194, train loss: 1.55295, val loss: 1.59626\n",
      "Main effects training epoch: 195, train loss: 1.54011, val loss: 1.60039\n",
      "Main effects training epoch: 196, train loss: 1.54222, val loss: 1.60350\n",
      "Main effects training epoch: 197, train loss: 1.53867, val loss: 1.60056\n",
      "Main effects training epoch: 198, train loss: 1.53790, val loss: 1.60618\n",
      "Main effects training epoch: 199, train loss: 1.53686, val loss: 1.59410\n",
      "Main effects training epoch: 200, train loss: 1.53216, val loss: 1.59335\n",
      "Main effects training epoch: 201, train loss: 1.53017, val loss: 1.59486\n",
      "Main effects training epoch: 202, train loss: 1.53996, val loss: 1.60168\n",
      "Main effects training epoch: 203, train loss: 1.53124, val loss: 1.59853\n",
      "Main effects training epoch: 204, train loss: 1.52766, val loss: 1.59914\n",
      "Main effects training epoch: 205, train loss: 1.53669, val loss: 1.59918\n",
      "Main effects training epoch: 206, train loss: 1.53126, val loss: 1.60520\n",
      "Main effects training epoch: 207, train loss: 1.52669, val loss: 1.58339\n",
      "Main effects training epoch: 208, train loss: 1.53173, val loss: 1.60574\n",
      "Main effects training epoch: 209, train loss: 1.52596, val loss: 1.58788\n",
      "Main effects training epoch: 210, train loss: 1.52518, val loss: 1.58803\n",
      "Main effects training epoch: 211, train loss: 1.52605, val loss: 1.60312\n",
      "Main effects training epoch: 212, train loss: 1.52401, val loss: 1.58563\n",
      "Main effects training epoch: 213, train loss: 1.53098, val loss: 1.59200\n",
      "Main effects training epoch: 214, train loss: 1.52700, val loss: 1.59875\n",
      "Main effects training epoch: 215, train loss: 1.53283, val loss: 1.61103\n",
      "Main effects training epoch: 216, train loss: 1.52394, val loss: 1.59720\n",
      "Main effects training epoch: 217, train loss: 1.52593, val loss: 1.58339\n",
      "Main effects training epoch: 218, train loss: 1.52605, val loss: 1.57684\n",
      "Main effects training epoch: 219, train loss: 1.52646, val loss: 1.60118\n",
      "Main effects training epoch: 220, train loss: 1.52773, val loss: 1.59285\n",
      "Main effects training epoch: 221, train loss: 1.52940, val loss: 1.59595\n",
      "Main effects training epoch: 222, train loss: 1.54098, val loss: 1.61511\n",
      "Main effects training epoch: 223, train loss: 1.51786, val loss: 1.58087\n",
      "Main effects training epoch: 224, train loss: 1.52514, val loss: 1.58733\n",
      "Main effects training epoch: 225, train loss: 1.54367, val loss: 1.58148\n",
      "Main effects training epoch: 226, train loss: 1.51712, val loss: 1.59257\n",
      "Main effects training epoch: 227, train loss: 1.51768, val loss: 1.59139\n",
      "Main effects training epoch: 228, train loss: 1.52194, val loss: 1.58570\n",
      "Main effects training epoch: 229, train loss: 1.51975, val loss: 1.59545\n",
      "Main effects training epoch: 230, train loss: 1.51935, val loss: 1.57825\n",
      "Main effects training epoch: 231, train loss: 1.51284, val loss: 1.58278\n",
      "Main effects training epoch: 232, train loss: 1.51028, val loss: 1.57865\n",
      "Main effects training epoch: 233, train loss: 1.51948, val loss: 1.59264\n",
      "Main effects training epoch: 234, train loss: 1.51713, val loss: 1.58711\n",
      "Main effects training epoch: 235, train loss: 1.51698, val loss: 1.58568\n",
      "Main effects training epoch: 236, train loss: 1.51792, val loss: 1.58250\n",
      "Main effects training epoch: 237, train loss: 1.51880, val loss: 1.58193\n",
      "Main effects training epoch: 238, train loss: 1.51929, val loss: 1.57368\n",
      "Main effects training epoch: 239, train loss: 1.53461, val loss: 1.59419\n",
      "Main effects training epoch: 240, train loss: 1.51843, val loss: 1.57545\n",
      "Main effects training epoch: 241, train loss: 1.50933, val loss: 1.56692\n",
      "Main effects training epoch: 242, train loss: 1.50740, val loss: 1.57751\n",
      "Main effects training epoch: 243, train loss: 1.50856, val loss: 1.57526\n",
      "Main effects training epoch: 244, train loss: 1.51383, val loss: 1.58333\n",
      "Main effects training epoch: 245, train loss: 1.51501, val loss: 1.59644\n",
      "Main effects training epoch: 246, train loss: 1.51055, val loss: 1.58295\n",
      "Main effects training epoch: 247, train loss: 1.50987, val loss: 1.57967\n",
      "Main effects training epoch: 248, train loss: 1.50726, val loss: 1.57238\n",
      "Main effects training epoch: 249, train loss: 1.50301, val loss: 1.57781\n",
      "Main effects training epoch: 250, train loss: 1.50303, val loss: 1.57321\n",
      "Main effects training epoch: 251, train loss: 1.50718, val loss: 1.57583\n",
      "Main effects training epoch: 252, train loss: 1.50618, val loss: 1.57169\n",
      "Main effects training epoch: 253, train loss: 1.50357, val loss: 1.58040\n",
      "Main effects training epoch: 254, train loss: 1.50538, val loss: 1.57453\n",
      "Main effects training epoch: 255, train loss: 1.51288, val loss: 1.58745\n",
      "Main effects training epoch: 256, train loss: 1.50828, val loss: 1.57981\n",
      "Main effects training epoch: 257, train loss: 1.52019, val loss: 1.57671\n",
      "Main effects training epoch: 258, train loss: 1.50320, val loss: 1.57769\n",
      "Main effects training epoch: 259, train loss: 1.51092, val loss: 1.58637\n",
      "Main effects training epoch: 260, train loss: 1.50480, val loss: 1.56761\n",
      "Main effects training epoch: 261, train loss: 1.49806, val loss: 1.56985\n",
      "Main effects training epoch: 262, train loss: 1.50258, val loss: 1.57515\n",
      "Main effects training epoch: 263, train loss: 1.50348, val loss: 1.57381\n",
      "Main effects training epoch: 264, train loss: 1.49924, val loss: 1.57095\n",
      "Main effects training epoch: 265, train loss: 1.50064, val loss: 1.57649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 266, train loss: 1.49889, val loss: 1.57465\n",
      "Main effects training epoch: 267, train loss: 1.50330, val loss: 1.56561\n",
      "Main effects training epoch: 268, train loss: 1.50142, val loss: 1.56910\n",
      "Main effects training epoch: 269, train loss: 1.49538, val loss: 1.57874\n",
      "Main effects training epoch: 270, train loss: 1.50002, val loss: 1.58116\n",
      "Main effects training epoch: 271, train loss: 1.49729, val loss: 1.56108\n",
      "Main effects training epoch: 272, train loss: 1.49477, val loss: 1.56166\n",
      "Main effects training epoch: 273, train loss: 1.51310, val loss: 1.58351\n",
      "Main effects training epoch: 274, train loss: 1.49325, val loss: 1.56539\n",
      "Main effects training epoch: 275, train loss: 1.49830, val loss: 1.57840\n",
      "Main effects training epoch: 276, train loss: 1.50058, val loss: 1.57500\n",
      "Main effects training epoch: 277, train loss: 1.50896, val loss: 1.55109\n",
      "Main effects training epoch: 278, train loss: 1.50495, val loss: 1.57227\n",
      "Main effects training epoch: 279, train loss: 1.50228, val loss: 1.55140\n",
      "Main effects training epoch: 280, train loss: 1.49880, val loss: 1.57412\n",
      "Main effects training epoch: 281, train loss: 1.49448, val loss: 1.55856\n",
      "Main effects training epoch: 282, train loss: 1.49766, val loss: 1.56709\n",
      "Main effects training epoch: 283, train loss: 1.52921, val loss: 1.59072\n",
      "Main effects training epoch: 284, train loss: 1.50745, val loss: 1.58559\n",
      "Main effects training epoch: 285, train loss: 1.49936, val loss: 1.56601\n",
      "Main effects training epoch: 286, train loss: 1.50315, val loss: 1.55114\n",
      "Main effects training epoch: 287, train loss: 1.48822, val loss: 1.55654\n",
      "Main effects training epoch: 288, train loss: 1.49037, val loss: 1.55803\n",
      "Main effects training epoch: 289, train loss: 1.48377, val loss: 1.56253\n",
      "Main effects training epoch: 290, train loss: 1.48711, val loss: 1.56105\n",
      "Main effects training epoch: 291, train loss: 1.48357, val loss: 1.55015\n",
      "Main effects training epoch: 292, train loss: 1.48583, val loss: 1.55025\n",
      "Main effects training epoch: 293, train loss: 1.49045, val loss: 1.57167\n",
      "Main effects training epoch: 294, train loss: 1.50088, val loss: 1.56592\n",
      "Main effects training epoch: 295, train loss: 1.49064, val loss: 1.57572\n",
      "Main effects training epoch: 296, train loss: 1.47917, val loss: 1.54749\n",
      "Main effects training epoch: 297, train loss: 1.48288, val loss: 1.54064\n",
      "Main effects training epoch: 298, train loss: 1.48776, val loss: 1.54656\n",
      "Main effects training epoch: 299, train loss: 1.49228, val loss: 1.53461\n",
      "Main effects training epoch: 300, train loss: 1.48674, val loss: 1.53939\n",
      "##########Stage 1: main effect training stop.##########\n",
      "3 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.49991, val loss: 1.55285\n",
      "Main effects tuning epoch: 2, train loss: 1.49535, val loss: 1.55606\n",
      "Main effects tuning epoch: 3, train loss: 1.49465, val loss: 1.55106\n",
      "Main effects tuning epoch: 4, train loss: 1.49818, val loss: 1.55017\n",
      "Main effects tuning epoch: 5, train loss: 1.49691, val loss: 1.55261\n",
      "Main effects tuning epoch: 6, train loss: 1.49686, val loss: 1.54822\n",
      "Main effects tuning epoch: 7, train loss: 1.49401, val loss: 1.55414\n",
      "Main effects tuning epoch: 8, train loss: 1.50280, val loss: 1.54628\n",
      "Main effects tuning epoch: 9, train loss: 1.49243, val loss: 1.54365\n",
      "Main effects tuning epoch: 10, train loss: 1.49344, val loss: 1.54841\n",
      "Main effects tuning epoch: 11, train loss: 1.50061, val loss: 1.54881\n",
      "Main effects tuning epoch: 12, train loss: 1.49155, val loss: 1.54899\n",
      "Main effects tuning epoch: 13, train loss: 1.48992, val loss: 1.54710\n",
      "Main effects tuning epoch: 14, train loss: 1.49332, val loss: 1.54800\n",
      "Main effects tuning epoch: 15, train loss: 1.48884, val loss: 1.54488\n",
      "Main effects tuning epoch: 16, train loss: 1.49048, val loss: 1.54104\n",
      "Main effects tuning epoch: 17, train loss: 1.49941, val loss: 1.53920\n",
      "Main effects tuning epoch: 18, train loss: 1.50761, val loss: 1.54066\n",
      "Main effects tuning epoch: 19, train loss: 1.53404, val loss: 1.57424\n",
      "Main effects tuning epoch: 20, train loss: 1.48941, val loss: 1.54873\n",
      "Main effects tuning epoch: 21, train loss: 1.50389, val loss: 1.56429\n",
      "Main effects tuning epoch: 22, train loss: 1.50976, val loss: 1.56382\n",
      "Main effects tuning epoch: 23, train loss: 1.49702, val loss: 1.56266\n",
      "Main effects tuning epoch: 24, train loss: 1.49288, val loss: 1.54906\n",
      "Main effects tuning epoch: 25, train loss: 1.51632, val loss: 1.54852\n",
      "Main effects tuning epoch: 26, train loss: 1.49038, val loss: 1.53628\n",
      "Main effects tuning epoch: 27, train loss: 1.48701, val loss: 1.53949\n",
      "Main effects tuning epoch: 28, train loss: 1.49009, val loss: 1.54293\n",
      "Main effects tuning epoch: 29, train loss: 1.50055, val loss: 1.55640\n",
      "Main effects tuning epoch: 30, train loss: 1.49168, val loss: 1.53876\n",
      "Main effects tuning epoch: 31, train loss: 1.49656, val loss: 1.55901\n",
      "Main effects tuning epoch: 32, train loss: 1.50439, val loss: 1.54139\n",
      "Main effects tuning epoch: 33, train loss: 1.50875, val loss: 1.53864\n",
      "Main effects tuning epoch: 34, train loss: 1.48677, val loss: 1.54393\n",
      "Main effects tuning epoch: 35, train loss: 1.49078, val loss: 1.53108\n",
      "Main effects tuning epoch: 36, train loss: 1.49080, val loss: 1.55591\n",
      "Main effects tuning epoch: 37, train loss: 1.48571, val loss: 1.53864\n",
      "Main effects tuning epoch: 38, train loss: 1.48839, val loss: 1.54471\n",
      "Main effects tuning epoch: 39, train loss: 1.50697, val loss: 1.56438\n",
      "Main effects tuning epoch: 40, train loss: 1.48298, val loss: 1.53727\n",
      "Main effects tuning epoch: 41, train loss: 1.48948, val loss: 1.53014\n",
      "Main effects tuning epoch: 42, train loss: 1.48637, val loss: 1.53798\n",
      "Main effects tuning epoch: 43, train loss: 1.48500, val loss: 1.53493\n",
      "Main effects tuning epoch: 44, train loss: 1.49397, val loss: 1.53643\n",
      "Main effects tuning epoch: 45, train loss: 1.48609, val loss: 1.53624\n",
      "Main effects tuning epoch: 46, train loss: 1.48222, val loss: 1.54734\n",
      "Main effects tuning epoch: 47, train loss: 1.48724, val loss: 1.54981\n",
      "Main effects tuning epoch: 48, train loss: 1.48772, val loss: 1.54781\n",
      "Main effects tuning epoch: 49, train loss: 1.48757, val loss: 1.55312\n",
      "Main effects tuning epoch: 50, train loss: 1.48483, val loss: 1.53140\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.38776, val loss: 1.41529\n",
      "Interaction training epoch: 2, train loss: 1.79988, val loss: 1.81613\n",
      "Interaction training epoch: 3, train loss: 1.07501, val loss: 1.08140\n",
      "Interaction training epoch: 4, train loss: 1.10199, val loss: 1.13257\n",
      "Interaction training epoch: 5, train loss: 1.02560, val loss: 1.02010\n",
      "Interaction training epoch: 6, train loss: 1.03502, val loss: 1.03919\n",
      "Interaction training epoch: 7, train loss: 1.01354, val loss: 1.03846\n",
      "Interaction training epoch: 8, train loss: 1.03167, val loss: 1.04017\n",
      "Interaction training epoch: 9, train loss: 1.03012, val loss: 1.03925\n",
      "Interaction training epoch: 10, train loss: 0.97944, val loss: 0.98961\n",
      "Interaction training epoch: 11, train loss: 0.98935, val loss: 0.99506\n",
      "Interaction training epoch: 12, train loss: 1.00088, val loss: 1.01741\n",
      "Interaction training epoch: 13, train loss: 0.98229, val loss: 1.00019\n",
      "Interaction training epoch: 14, train loss: 0.97318, val loss: 0.98796\n",
      "Interaction training epoch: 15, train loss: 0.96017, val loss: 0.96743\n",
      "Interaction training epoch: 16, train loss: 0.95118, val loss: 0.97152\n",
      "Interaction training epoch: 17, train loss: 0.95660, val loss: 0.96902\n",
      "Interaction training epoch: 18, train loss: 0.94625, val loss: 0.97723\n",
      "Interaction training epoch: 19, train loss: 0.96130, val loss: 0.96905\n",
      "Interaction training epoch: 20, train loss: 0.94687, val loss: 0.96049\n",
      "Interaction training epoch: 21, train loss: 0.94557, val loss: 0.95259\n",
      "Interaction training epoch: 22, train loss: 0.92566, val loss: 0.95042\n",
      "Interaction training epoch: 23, train loss: 0.93118, val loss: 0.94189\n",
      "Interaction training epoch: 24, train loss: 0.93611, val loss: 0.96348\n",
      "Interaction training epoch: 25, train loss: 0.92943, val loss: 0.95412\n",
      "Interaction training epoch: 26, train loss: 0.92755, val loss: 0.95595\n",
      "Interaction training epoch: 27, train loss: 0.90499, val loss: 0.93052\n",
      "Interaction training epoch: 28, train loss: 0.91123, val loss: 0.93684\n",
      "Interaction training epoch: 29, train loss: 0.91158, val loss: 0.93728\n",
      "Interaction training epoch: 30, train loss: 0.90563, val loss: 0.92287\n",
      "Interaction training epoch: 31, train loss: 0.89338, val loss: 0.91894\n",
      "Interaction training epoch: 32, train loss: 0.92659, val loss: 0.95598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 33, train loss: 0.91883, val loss: 0.93138\n",
      "Interaction training epoch: 34, train loss: 0.90863, val loss: 0.94101\n",
      "Interaction training epoch: 35, train loss: 0.88421, val loss: 0.91366\n",
      "Interaction training epoch: 36, train loss: 0.86837, val loss: 0.89649\n",
      "Interaction training epoch: 37, train loss: 0.87360, val loss: 0.90512\n",
      "Interaction training epoch: 38, train loss: 0.86526, val loss: 0.88269\n",
      "Interaction training epoch: 39, train loss: 0.89964, val loss: 0.92345\n",
      "Interaction training epoch: 40, train loss: 0.86643, val loss: 0.90912\n",
      "Interaction training epoch: 41, train loss: 0.91508, val loss: 0.93853\n",
      "Interaction training epoch: 42, train loss: 0.85456, val loss: 0.88743\n",
      "Interaction training epoch: 43, train loss: 0.86558, val loss: 0.88629\n",
      "Interaction training epoch: 44, train loss: 0.86861, val loss: 0.90077\n",
      "Interaction training epoch: 45, train loss: 0.87248, val loss: 0.89302\n",
      "Interaction training epoch: 46, train loss: 0.86493, val loss: 0.87760\n",
      "Interaction training epoch: 47, train loss: 0.87805, val loss: 0.90085\n",
      "Interaction training epoch: 48, train loss: 0.86722, val loss: 0.89782\n",
      "Interaction training epoch: 49, train loss: 0.85437, val loss: 0.86346\n",
      "Interaction training epoch: 50, train loss: 0.84878, val loss: 0.86686\n",
      "Interaction training epoch: 51, train loss: 0.86656, val loss: 0.89672\n",
      "Interaction training epoch: 52, train loss: 0.86149, val loss: 0.87887\n",
      "Interaction training epoch: 53, train loss: 0.87420, val loss: 0.88541\n",
      "Interaction training epoch: 54, train loss: 0.87959, val loss: 0.88838\n",
      "Interaction training epoch: 55, train loss: 0.82849, val loss: 0.86082\n",
      "Interaction training epoch: 56, train loss: 0.87658, val loss: 0.90196\n",
      "Interaction training epoch: 57, train loss: 0.83962, val loss: 0.85387\n",
      "Interaction training epoch: 58, train loss: 0.84074, val loss: 0.86398\n",
      "Interaction training epoch: 59, train loss: 0.81538, val loss: 0.83539\n",
      "Interaction training epoch: 60, train loss: 0.83220, val loss: 0.86097\n",
      "Interaction training epoch: 61, train loss: 0.83253, val loss: 0.86943\n",
      "Interaction training epoch: 62, train loss: 0.82837, val loss: 0.85763\n",
      "Interaction training epoch: 63, train loss: 0.82285, val loss: 0.85547\n",
      "Interaction training epoch: 64, train loss: 0.82980, val loss: 0.85003\n",
      "Interaction training epoch: 65, train loss: 0.84597, val loss: 0.87824\n",
      "Interaction training epoch: 66, train loss: 0.84202, val loss: 0.86410\n",
      "Interaction training epoch: 67, train loss: 0.82536, val loss: 0.85600\n",
      "Interaction training epoch: 68, train loss: 0.81750, val loss: 0.85246\n",
      "Interaction training epoch: 69, train loss: 0.84987, val loss: 0.87692\n",
      "Interaction training epoch: 70, train loss: 0.82927, val loss: 0.85479\n",
      "Interaction training epoch: 71, train loss: 0.83644, val loss: 0.85659\n",
      "Interaction training epoch: 72, train loss: 0.85102, val loss: 0.87027\n",
      "Interaction training epoch: 73, train loss: 0.81055, val loss: 0.85022\n",
      "Interaction training epoch: 74, train loss: 0.82852, val loss: 0.85870\n",
      "Interaction training epoch: 75, train loss: 0.82093, val loss: 0.84895\n",
      "Interaction training epoch: 76, train loss: 0.85162, val loss: 0.86320\n",
      "Interaction training epoch: 77, train loss: 0.89554, val loss: 0.90982\n",
      "Interaction training epoch: 78, train loss: 0.82504, val loss: 0.84952\n",
      "Interaction training epoch: 79, train loss: 0.86995, val loss: 0.88779\n",
      "Interaction training epoch: 80, train loss: 0.81919, val loss: 0.84143\n",
      "Interaction training epoch: 81, train loss: 0.81642, val loss: 0.84116\n",
      "Interaction training epoch: 82, train loss: 0.80604, val loss: 0.82175\n",
      "Interaction training epoch: 83, train loss: 0.80494, val loss: 0.84752\n",
      "Interaction training epoch: 84, train loss: 0.83013, val loss: 0.84744\n",
      "Interaction training epoch: 85, train loss: 0.80637, val loss: 0.84213\n",
      "Interaction training epoch: 86, train loss: 0.81751, val loss: 0.84280\n",
      "Interaction training epoch: 87, train loss: 0.82231, val loss: 0.84387\n",
      "Interaction training epoch: 88, train loss: 0.80891, val loss: 0.83393\n",
      "Interaction training epoch: 89, train loss: 0.80628, val loss: 0.83342\n",
      "Interaction training epoch: 90, train loss: 0.79723, val loss: 0.82692\n",
      "Interaction training epoch: 91, train loss: 0.82240, val loss: 0.84516\n",
      "Interaction training epoch: 92, train loss: 0.81268, val loss: 0.85207\n",
      "Interaction training epoch: 93, train loss: 0.81295, val loss: 0.84148\n",
      "Interaction training epoch: 94, train loss: 0.81386, val loss: 0.84451\n",
      "Interaction training epoch: 95, train loss: 0.82929, val loss: 0.84674\n",
      "Interaction training epoch: 96, train loss: 0.83365, val loss: 0.86716\n",
      "Interaction training epoch: 97, train loss: 0.84490, val loss: 0.86158\n",
      "Interaction training epoch: 98, train loss: 0.80399, val loss: 0.83695\n",
      "Interaction training epoch: 99, train loss: 0.81576, val loss: 0.83941\n",
      "Interaction training epoch: 100, train loss: 0.83482, val loss: 0.85833\n",
      "Interaction training epoch: 101, train loss: 0.81095, val loss: 0.84695\n",
      "Interaction training epoch: 102, train loss: 0.81288, val loss: 0.84074\n",
      "Interaction training epoch: 103, train loss: 0.79801, val loss: 0.83021\n",
      "Interaction training epoch: 104, train loss: 0.80582, val loss: 0.83241\n",
      "Interaction training epoch: 105, train loss: 0.83051, val loss: 0.85466\n",
      "Interaction training epoch: 106, train loss: 0.82094, val loss: 0.85632\n",
      "Interaction training epoch: 107, train loss: 0.85702, val loss: 0.88493\n",
      "Interaction training epoch: 108, train loss: 0.82202, val loss: 0.84425\n",
      "Interaction training epoch: 109, train loss: 0.82001, val loss: 0.83775\n",
      "Interaction training epoch: 110, train loss: 0.80778, val loss: 0.83748\n",
      "Interaction training epoch: 111, train loss: 0.80976, val loss: 0.84499\n",
      "Interaction training epoch: 112, train loss: 0.80025, val loss: 0.83112\n",
      "Interaction training epoch: 113, train loss: 0.81872, val loss: 0.83449\n",
      "Interaction training epoch: 114, train loss: 0.84091, val loss: 0.86758\n",
      "Interaction training epoch: 115, train loss: 0.81770, val loss: 0.85171\n",
      "Interaction training epoch: 116, train loss: 0.82591, val loss: 0.84352\n",
      "Interaction training epoch: 117, train loss: 0.79514, val loss: 0.83954\n",
      "Interaction training epoch: 118, train loss: 0.80419, val loss: 0.83926\n",
      "Interaction training epoch: 119, train loss: 0.79224, val loss: 0.82080\n",
      "Interaction training epoch: 120, train loss: 0.82519, val loss: 0.85922\n",
      "Interaction training epoch: 121, train loss: 0.80831, val loss: 0.84323\n",
      "Interaction training epoch: 122, train loss: 0.86328, val loss: 0.88207\n",
      "Interaction training epoch: 123, train loss: 0.83419, val loss: 0.85657\n",
      "Interaction training epoch: 124, train loss: 0.83352, val loss: 0.86035\n",
      "Interaction training epoch: 125, train loss: 0.84956, val loss: 0.86267\n",
      "Interaction training epoch: 126, train loss: 0.79036, val loss: 0.83550\n",
      "Interaction training epoch: 127, train loss: 0.82002, val loss: 0.84211\n",
      "Interaction training epoch: 128, train loss: 0.78745, val loss: 0.83466\n",
      "Interaction training epoch: 129, train loss: 0.82365, val loss: 0.84958\n",
      "Interaction training epoch: 130, train loss: 0.81038, val loss: 0.83823\n",
      "Interaction training epoch: 131, train loss: 0.83295, val loss: 0.85249\n",
      "Interaction training epoch: 132, train loss: 0.79045, val loss: 0.82302\n",
      "Interaction training epoch: 133, train loss: 0.81560, val loss: 0.84806\n",
      "Interaction training epoch: 134, train loss: 0.79565, val loss: 0.83023\n",
      "Interaction training epoch: 135, train loss: 0.84346, val loss: 0.86374\n",
      "Interaction training epoch: 136, train loss: 0.80747, val loss: 0.84607\n",
      "Interaction training epoch: 137, train loss: 0.78926, val loss: 0.82299\n",
      "Interaction training epoch: 138, train loss: 0.82665, val loss: 0.85612\n",
      "Interaction training epoch: 139, train loss: 0.80050, val loss: 0.84210\n",
      "Interaction training epoch: 140, train loss: 0.82287, val loss: 0.84663\n",
      "Interaction training epoch: 141, train loss: 0.79527, val loss: 0.82704\n",
      "Interaction training epoch: 142, train loss: 0.81128, val loss: 0.84590\n",
      "Interaction training epoch: 143, train loss: 0.80489, val loss: 0.83538\n",
      "Interaction training epoch: 144, train loss: 0.81979, val loss: 0.85230\n",
      "Interaction training epoch: 145, train loss: 0.80331, val loss: 0.83047\n",
      "Interaction training epoch: 146, train loss: 0.80831, val loss: 0.83721\n",
      "Interaction training epoch: 147, train loss: 0.80374, val loss: 0.83926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 148, train loss: 0.80015, val loss: 0.83174\n",
      "Interaction training epoch: 149, train loss: 0.82475, val loss: 0.85705\n",
      "Interaction training epoch: 150, train loss: 0.79588, val loss: 0.83762\n",
      "Interaction training epoch: 151, train loss: 0.80227, val loss: 0.83406\n",
      "Interaction training epoch: 152, train loss: 0.79451, val loss: 0.82532\n",
      "Interaction training epoch: 153, train loss: 0.80160, val loss: 0.83045\n",
      "Interaction training epoch: 154, train loss: 0.80099, val loss: 0.84091\n",
      "Interaction training epoch: 155, train loss: 0.80401, val loss: 0.83464\n",
      "Interaction training epoch: 156, train loss: 0.79876, val loss: 0.83049\n",
      "Interaction training epoch: 157, train loss: 0.80286, val loss: 0.83172\n",
      "Interaction training epoch: 158, train loss: 0.79899, val loss: 0.83945\n",
      "Interaction training epoch: 159, train loss: 0.80675, val loss: 0.83465\n",
      "Interaction training epoch: 160, train loss: 0.79804, val loss: 0.82328\n",
      "Interaction training epoch: 161, train loss: 0.79878, val loss: 0.84581\n",
      "Interaction training epoch: 162, train loss: 0.80421, val loss: 0.83513\n",
      "Interaction training epoch: 163, train loss: 0.80345, val loss: 0.83286\n",
      "Interaction training epoch: 164, train loss: 0.79465, val loss: 0.83088\n",
      "Interaction training epoch: 165, train loss: 0.80037, val loss: 0.85269\n",
      "Interaction training epoch: 166, train loss: 0.81046, val loss: 0.83687\n",
      "Interaction training epoch: 167, train loss: 0.83103, val loss: 0.85784\n",
      "Interaction training epoch: 168, train loss: 0.79352, val loss: 0.83329\n",
      "Interaction training epoch: 169, train loss: 0.80858, val loss: 0.83401\n",
      "Interaction training epoch: 170, train loss: 0.80271, val loss: 0.83196\n",
      "Interaction training epoch: 171, train loss: 0.79404, val loss: 0.83381\n",
      "Interaction training epoch: 172, train loss: 0.81255, val loss: 0.84091\n",
      "Interaction training epoch: 173, train loss: 0.80947, val loss: 0.84266\n",
      "Interaction training epoch: 174, train loss: 0.80536, val loss: 0.83913\n",
      "Interaction training epoch: 175, train loss: 0.81524, val loss: 0.84063\n",
      "Interaction training epoch: 176, train loss: 0.79326, val loss: 0.83498\n",
      "Interaction training epoch: 177, train loss: 0.83126, val loss: 0.85703\n",
      "Interaction training epoch: 178, train loss: 0.78974, val loss: 0.82865\n",
      "Interaction training epoch: 179, train loss: 0.79127, val loss: 0.83440\n",
      "Interaction training epoch: 180, train loss: 0.81321, val loss: 0.85086\n",
      "Interaction training epoch: 181, train loss: 0.80961, val loss: 0.84390\n",
      "Interaction training epoch: 182, train loss: 0.80100, val loss: 0.82890\n",
      "Interaction training epoch: 183, train loss: 0.78399, val loss: 0.82460\n",
      "Interaction training epoch: 184, train loss: 0.81972, val loss: 0.84295\n",
      "Interaction training epoch: 185, train loss: 0.77952, val loss: 0.83181\n",
      "Interaction training epoch: 186, train loss: 0.80309, val loss: 0.83535\n",
      "Interaction training epoch: 187, train loss: 0.78828, val loss: 0.83112\n",
      "Interaction training epoch: 188, train loss: 0.81301, val loss: 0.84393\n",
      "Interaction training epoch: 189, train loss: 0.81020, val loss: 0.84605\n",
      "Interaction training epoch: 190, train loss: 0.79807, val loss: 0.83189\n",
      "Interaction training epoch: 191, train loss: 0.79189, val loss: 0.82873\n",
      "Interaction training epoch: 192, train loss: 0.80531, val loss: 0.83863\n",
      "Interaction training epoch: 193, train loss: 0.78945, val loss: 0.83179\n",
      "Interaction training epoch: 194, train loss: 0.79920, val loss: 0.83563\n",
      "Interaction training epoch: 195, train loss: 0.82124, val loss: 0.85151\n",
      "Interaction training epoch: 196, train loss: 0.79058, val loss: 0.82656\n",
      "Interaction training epoch: 197, train loss: 0.78999, val loss: 0.82767\n",
      "Interaction training epoch: 198, train loss: 0.85165, val loss: 0.87341\n",
      "Interaction training epoch: 199, train loss: 0.79924, val loss: 0.84268\n",
      "Interaction training epoch: 200, train loss: 0.79669, val loss: 0.83220\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########1 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.81933, val loss: 0.85065\n",
      "Interaction tuning epoch: 2, train loss: 0.77998, val loss: 0.82296\n",
      "Interaction tuning epoch: 3, train loss: 0.80642, val loss: 0.84109\n",
      "Interaction tuning epoch: 4, train loss: 0.80837, val loss: 0.84074\n",
      "Interaction tuning epoch: 5, train loss: 0.80218, val loss: 0.83414\n",
      "Interaction tuning epoch: 6, train loss: 0.79818, val loss: 0.83102\n",
      "Interaction tuning epoch: 7, train loss: 0.79175, val loss: 0.82997\n",
      "Interaction tuning epoch: 8, train loss: 0.82330, val loss: 0.85392\n",
      "Interaction tuning epoch: 9, train loss: 0.81099, val loss: 0.84723\n",
      "Interaction tuning epoch: 10, train loss: 0.79599, val loss: 0.82325\n",
      "Interaction tuning epoch: 11, train loss: 0.79216, val loss: 0.82704\n",
      "Interaction tuning epoch: 12, train loss: 0.80667, val loss: 0.83620\n",
      "Interaction tuning epoch: 13, train loss: 0.78766, val loss: 0.82372\n",
      "Interaction tuning epoch: 14, train loss: 0.80440, val loss: 0.83602\n",
      "Interaction tuning epoch: 15, train loss: 0.79060, val loss: 0.83070\n",
      "Interaction tuning epoch: 16, train loss: 0.81913, val loss: 0.85842\n",
      "Interaction tuning epoch: 17, train loss: 0.80522, val loss: 0.83477\n",
      "Interaction tuning epoch: 18, train loss: 0.82662, val loss: 0.86416\n",
      "Interaction tuning epoch: 19, train loss: 0.78253, val loss: 0.82019\n",
      "Interaction tuning epoch: 20, train loss: 0.81312, val loss: 0.84579\n",
      "Interaction tuning epoch: 21, train loss: 0.79942, val loss: 0.83148\n",
      "Interaction tuning epoch: 22, train loss: 0.78020, val loss: 0.82087\n",
      "Interaction tuning epoch: 23, train loss: 0.80552, val loss: 0.83789\n",
      "Interaction tuning epoch: 24, train loss: 0.79661, val loss: 0.82699\n",
      "Interaction tuning epoch: 25, train loss: 0.79680, val loss: 0.84018\n",
      "Interaction tuning epoch: 26, train loss: 0.78245, val loss: 0.82208\n",
      "Interaction tuning epoch: 27, train loss: 0.80300, val loss: 0.84652\n",
      "Interaction tuning epoch: 28, train loss: 0.81669, val loss: 0.84996\n",
      "Interaction tuning epoch: 29, train loss: 0.79417, val loss: 0.83379\n",
      "Interaction tuning epoch: 30, train loss: 0.79768, val loss: 0.82631\n",
      "Interaction tuning epoch: 31, train loss: 0.79067, val loss: 0.82375\n",
      "Interaction tuning epoch: 32, train loss: 0.79727, val loss: 0.83767\n",
      "Interaction tuning epoch: 33, train loss: 0.81733, val loss: 0.83949\n",
      "Interaction tuning epoch: 34, train loss: 0.78036, val loss: 0.82431\n",
      "Interaction tuning epoch: 35, train loss: 0.83092, val loss: 0.86136\n",
      "Interaction tuning epoch: 36, train loss: 0.80583, val loss: 0.83543\n",
      "Interaction tuning epoch: 37, train loss: 0.80105, val loss: 0.84551\n",
      "Interaction tuning epoch: 38, train loss: 0.84801, val loss: 0.86535\n",
      "Interaction tuning epoch: 39, train loss: 0.80467, val loss: 0.83667\n",
      "Interaction tuning epoch: 40, train loss: 0.78048, val loss: 0.82546\n",
      "Interaction tuning epoch: 41, train loss: 0.79927, val loss: 0.83361\n",
      "Interaction tuning epoch: 42, train loss: 0.81553, val loss: 0.84543\n",
      "Interaction tuning epoch: 43, train loss: 0.79620, val loss: 0.82922\n",
      "Interaction tuning epoch: 44, train loss: 0.79113, val loss: 0.82950\n",
      "Interaction tuning epoch: 45, train loss: 0.80687, val loss: 0.83768\n",
      "Interaction tuning epoch: 46, train loss: 0.79152, val loss: 0.83144\n",
      "Interaction tuning epoch: 47, train loss: 0.80698, val loss: 0.83914\n",
      "Interaction tuning epoch: 48, train loss: 0.78557, val loss: 0.82995\n",
      "Interaction tuning epoch: 49, train loss: 0.79275, val loss: 0.82468\n",
      "Interaction tuning epoch: 50, train loss: 0.79950, val loss: 0.83509\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 35.581955671310425\n",
      "After the gam stage, training error is 0.79950 , validation error is 0.83509\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 19.624497\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.660572 validation MAE=0.787755,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.614829 validation MAE=0.766774,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 3: observed MAE=0.576776 validation MAE=0.748147,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.545369 validation MAE=0.732051,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.517142 validation MAE=0.717455,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.494231 validation MAE=0.704678,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.475441 validation MAE=0.694235,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.458529 validation MAE=0.684427,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.444421 validation MAE=0.675673,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.430843 validation MAE=0.667694,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.419411 validation MAE=0.660043,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.408768 validation MAE=0.653019,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.397496 validation MAE=0.646316,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.388285 validation MAE=0.640263,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.380022 validation MAE=0.634820,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.370565 validation MAE=0.629082,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.363558 validation MAE=0.624386,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.356565 validation MAE=0.619783,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.350826 validation MAE=0.615266,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.344229 validation MAE=0.610909,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.339649 validation MAE=0.606901,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.333961 validation MAE=0.603177,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.329316 validation MAE=0.600234,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.324786 validation MAE=0.596417,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.320790 validation MAE=0.593091,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.316927 validation MAE=0.589912,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.313511 validation MAE=0.587454,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.309874 validation MAE=0.584502,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.307548 validation MAE=0.582238,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.303567 validation MAE=0.579171,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.300981 validation MAE=0.577101,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.298096 validation MAE=0.575084,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.296773 validation MAE=0.573182,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.293917 validation MAE=0.571140,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.291011 validation MAE=0.569255,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.288917 validation MAE=0.567176,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.286567 validation MAE=0.565463,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.284471 validation MAE=0.563699,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.281284 validation MAE=0.561823,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.279532 validation MAE=0.560462,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.278781 validation MAE=0.559497,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.277945 validation MAE=0.557987,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.275792 validation MAE=0.556096,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.273774 validation MAE=0.554545,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.272512 validation MAE=0.552870,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.270263 validation MAE=0.551983,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.269534 validation MAE=0.551120,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.268845 validation MAE=0.549743,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.266669 validation MAE=0.548733,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.265246 validation MAE=0.547738,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.263908 validation MAE=0.546282,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.262352 validation MAE=0.545265,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.261477 validation MAE=0.544787,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.260540 validation MAE=0.543453,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.259366 validation MAE=0.542329,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.259071 validation MAE=0.542098,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.258145 validation MAE=0.540962,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.256216 validation MAE=0.539664,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.254641 validation MAE=0.538817,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.254523 validation MAE=0.538229,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.253292 validation MAE=0.537213,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.252621 validation MAE=0.536943,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.251423 validation MAE=0.535865,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.251048 validation MAE=0.535401,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.250392 validation MAE=0.534157,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.249143 validation MAE=0.534135,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.248987 validation MAE=0.533526,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.248107 validation MAE=0.532599,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.247476 validation MAE=0.531867,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.245981 validation MAE=0.531006,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.245895 validation MAE=0.530677,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.243986 validation MAE=0.529521,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.244239 validation MAE=0.529412,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.243496 validation MAE=0.528882,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.243398 validation MAE=0.527979,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.243101 validation MAE=0.527297,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.242001 validation MAE=0.526831,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.241632 validation MAE=0.526795,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.241738 validation MAE=0.526034,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.241032 validation MAE=0.526031,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.239526 validation MAE=0.525148,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.238599 validation MAE=0.524995,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.238351 validation MAE=0.524312,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.238505 validation MAE=0.524421,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.237816 validation MAE=0.523120,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.236645 validation MAE=0.523118,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.236272 validation MAE=0.522625,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.235785 validation MAE=0.522073,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.235553 validation MAE=0.521844,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.235242 validation MAE=0.522286,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.234982 validation MAE=0.521669,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.235137 validation MAE=0.521498,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.233887 validation MAE=0.520837,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.232759 validation MAE=0.520461,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.232659 validation MAE=0.520332,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.232555 validation MAE=0.520603,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.232808 validation MAE=0.519572,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.231703 validation MAE=0.519541,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.231344 validation MAE=0.518757,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.231081 validation MAE=0.519087,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.231097 validation MAE=0.519275,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.230109 validation MAE=0.518698,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.229818 validation MAE=0.518376,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.229871 validation MAE=0.518249,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.229342 validation MAE=0.518127,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.228154 validation MAE=0.517644,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.227738 validation MAE=0.517150,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.227347 validation MAE=0.516762,rank=5\n",
      "[SoftImpute] Iter 109: observed MAE=0.227398 validation MAE=0.516298,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.227248 validation MAE=0.516815,rank=5\n",
      "[SoftImpute] Iter 111: observed MAE=0.227545 validation MAE=0.517011,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.227056 validation MAE=0.516324,rank=5\n",
      "[SoftImpute] Iter 113: observed MAE=0.227468 validation MAE=0.516743,rank=5\n",
      "[SoftImpute] Iter 114: observed MAE=0.225905 validation MAE=0.515446,rank=5\n",
      "[SoftImpute] Iter 115: observed MAE=0.226768 validation MAE=0.515471,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.226318 validation MAE=0.516519,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.225951 validation MAE=0.515515,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.225159 validation MAE=0.515936,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.225750 validation MAE=0.515006,rank=5\n",
      "[SoftImpute] Iter 120: observed MAE=0.224544 validation MAE=0.515026,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 121: observed MAE=0.225144 validation MAE=0.515040,rank=5\n",
      "[SoftImpute] Iter 122: observed MAE=0.224313 validation MAE=0.515090,rank=5\n",
      "[SoftImpute] Stopped after iteration 122 for lambda=0.392490\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 3.964975118637085\n",
      "After the matrix factor stage, training error is 0.22431, validation error is 0.51509\n",
      "4\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.06259, val loss: 4.21620\n",
      "Main effects training epoch: 2, train loss: 3.92674, val loss: 4.07278\n",
      "Main effects training epoch: 3, train loss: 3.76178, val loss: 3.90341\n",
      "Main effects training epoch: 4, train loss: 3.67297, val loss: 3.80482\n",
      "Main effects training epoch: 5, train loss: 3.43809, val loss: 3.55495\n",
      "Main effects training epoch: 6, train loss: 3.26435, val loss: 3.37280\n",
      "Main effects training epoch: 7, train loss: 3.20512, val loss: 3.31453\n",
      "Main effects training epoch: 8, train loss: 3.23646, val loss: 3.34090\n",
      "Main effects training epoch: 9, train loss: 3.22415, val loss: 3.32508\n",
      "Main effects training epoch: 10, train loss: 3.18825, val loss: 3.29413\n",
      "Main effects training epoch: 11, train loss: 3.08800, val loss: 3.19131\n",
      "Main effects training epoch: 12, train loss: 3.08805, val loss: 3.18397\n",
      "Main effects training epoch: 13, train loss: 2.98495, val loss: 3.08232\n",
      "Main effects training epoch: 14, train loss: 2.96672, val loss: 3.06949\n",
      "Main effects training epoch: 15, train loss: 2.90246, val loss: 2.99673\n",
      "Main effects training epoch: 16, train loss: 2.85688, val loss: 2.94688\n",
      "Main effects training epoch: 17, train loss: 2.82674, val loss: 2.91760\n",
      "Main effects training epoch: 18, train loss: 2.76113, val loss: 2.84720\n",
      "Main effects training epoch: 19, train loss: 2.70734, val loss: 2.77911\n",
      "Main effects training epoch: 20, train loss: 2.65868, val loss: 2.72801\n",
      "Main effects training epoch: 21, train loss: 2.56879, val loss: 2.62693\n",
      "Main effects training epoch: 22, train loss: 2.51177, val loss: 2.56415\n",
      "Main effects training epoch: 23, train loss: 2.49067, val loss: 2.53498\n",
      "Main effects training epoch: 24, train loss: 2.41938, val loss: 2.47172\n",
      "Main effects training epoch: 25, train loss: 2.37016, val loss: 2.42279\n",
      "Main effects training epoch: 26, train loss: 2.31992, val loss: 2.37597\n",
      "Main effects training epoch: 27, train loss: 2.30860, val loss: 2.36477\n",
      "Main effects training epoch: 28, train loss: 2.24728, val loss: 2.30826\n",
      "Main effects training epoch: 29, train loss: 2.24477, val loss: 2.30397\n",
      "Main effects training epoch: 30, train loss: 2.18537, val loss: 2.24343\n",
      "Main effects training epoch: 31, train loss: 2.18212, val loss: 2.24164\n",
      "Main effects training epoch: 32, train loss: 2.15673, val loss: 2.22064\n",
      "Main effects training epoch: 33, train loss: 2.12251, val loss: 2.18900\n",
      "Main effects training epoch: 34, train loss: 2.08694, val loss: 2.14801\n",
      "Main effects training epoch: 35, train loss: 2.09576, val loss: 2.16163\n",
      "Main effects training epoch: 36, train loss: 2.09207, val loss: 2.15572\n",
      "Main effects training epoch: 37, train loss: 2.02033, val loss: 2.08284\n",
      "Main effects training epoch: 38, train loss: 2.01569, val loss: 2.08254\n",
      "Main effects training epoch: 39, train loss: 2.00809, val loss: 2.07227\n",
      "Main effects training epoch: 40, train loss: 2.00016, val loss: 2.06802\n",
      "Main effects training epoch: 41, train loss: 1.97368, val loss: 2.03647\n",
      "Main effects training epoch: 42, train loss: 1.94323, val loss: 2.00877\n",
      "Main effects training epoch: 43, train loss: 1.95599, val loss: 2.01863\n",
      "Main effects training epoch: 44, train loss: 1.91518, val loss: 1.98042\n",
      "Main effects training epoch: 45, train loss: 1.91981, val loss: 1.98041\n",
      "Main effects training epoch: 46, train loss: 1.90370, val loss: 1.96598\n",
      "Main effects training epoch: 47, train loss: 1.88878, val loss: 1.95340\n",
      "Main effects training epoch: 48, train loss: 1.88251, val loss: 1.94361\n",
      "Main effects training epoch: 49, train loss: 1.85504, val loss: 1.91785\n",
      "Main effects training epoch: 50, train loss: 1.85631, val loss: 1.91122\n",
      "Main effects training epoch: 51, train loss: 1.84767, val loss: 1.91202\n",
      "Main effects training epoch: 52, train loss: 1.83652, val loss: 1.89368\n",
      "Main effects training epoch: 53, train loss: 1.82672, val loss: 1.88171\n",
      "Main effects training epoch: 54, train loss: 1.82045, val loss: 1.87603\n",
      "Main effects training epoch: 55, train loss: 1.81286, val loss: 1.87219\n",
      "Main effects training epoch: 56, train loss: 1.80408, val loss: 1.85651\n",
      "Main effects training epoch: 57, train loss: 1.81151, val loss: 1.86921\n",
      "Main effects training epoch: 58, train loss: 1.78601, val loss: 1.84168\n",
      "Main effects training epoch: 59, train loss: 1.79035, val loss: 1.84526\n",
      "Main effects training epoch: 60, train loss: 1.76871, val loss: 1.82817\n",
      "Main effects training epoch: 61, train loss: 1.78581, val loss: 1.84498\n",
      "Main effects training epoch: 62, train loss: 1.76518, val loss: 1.82148\n",
      "Main effects training epoch: 63, train loss: 1.76856, val loss: 1.82448\n",
      "Main effects training epoch: 64, train loss: 1.76320, val loss: 1.81878\n",
      "Main effects training epoch: 65, train loss: 1.75699, val loss: 1.81387\n",
      "Main effects training epoch: 66, train loss: 1.75328, val loss: 1.81105\n",
      "Main effects training epoch: 67, train loss: 1.74886, val loss: 1.79827\n",
      "Main effects training epoch: 68, train loss: 1.74881, val loss: 1.80429\n",
      "Main effects training epoch: 69, train loss: 1.74521, val loss: 1.80134\n",
      "Main effects training epoch: 70, train loss: 1.74334, val loss: 1.80281\n",
      "Main effects training epoch: 71, train loss: 1.73750, val loss: 1.78908\n",
      "Main effects training epoch: 72, train loss: 1.72858, val loss: 1.78755\n",
      "Main effects training epoch: 73, train loss: 1.73316, val loss: 1.78956\n",
      "Main effects training epoch: 74, train loss: 1.72650, val loss: 1.78089\n",
      "Main effects training epoch: 75, train loss: 1.72286, val loss: 1.77520\n",
      "Main effects training epoch: 76, train loss: 1.72384, val loss: 1.77708\n",
      "Main effects training epoch: 77, train loss: 1.72425, val loss: 1.78034\n",
      "Main effects training epoch: 78, train loss: 1.72002, val loss: 1.77290\n",
      "Main effects training epoch: 79, train loss: 1.71489, val loss: 1.76883\n",
      "Main effects training epoch: 80, train loss: 1.71887, val loss: 1.77766\n",
      "Main effects training epoch: 81, train loss: 1.71280, val loss: 1.76570\n",
      "Main effects training epoch: 82, train loss: 1.71629, val loss: 1.77275\n",
      "Main effects training epoch: 83, train loss: 1.71088, val loss: 1.76609\n",
      "Main effects training epoch: 84, train loss: 1.70804, val loss: 1.76026\n",
      "Main effects training epoch: 85, train loss: 1.70772, val loss: 1.76063\n",
      "Main effects training epoch: 86, train loss: 1.70350, val loss: 1.76285\n",
      "Main effects training epoch: 87, train loss: 1.70797, val loss: 1.76110\n",
      "Main effects training epoch: 88, train loss: 1.70414, val loss: 1.76166\n",
      "Main effects training epoch: 89, train loss: 1.70220, val loss: 1.75465\n",
      "Main effects training epoch: 90, train loss: 1.70254, val loss: 1.75261\n",
      "Main effects training epoch: 91, train loss: 1.69704, val loss: 1.75898\n",
      "Main effects training epoch: 92, train loss: 1.70163, val loss: 1.73900\n",
      "Main effects training epoch: 93, train loss: 1.68793, val loss: 1.75230\n",
      "Main effects training epoch: 94, train loss: 1.69206, val loss: 1.75442\n",
      "Main effects training epoch: 95, train loss: 1.68379, val loss: 1.74087\n",
      "Main effects training epoch: 96, train loss: 1.67871, val loss: 1.74145\n",
      "Main effects training epoch: 97, train loss: 1.67816, val loss: 1.74218\n",
      "Main effects training epoch: 98, train loss: 1.67320, val loss: 1.73242\n",
      "Main effects training epoch: 99, train loss: 1.67495, val loss: 1.72775\n",
      "Main effects training epoch: 100, train loss: 1.66632, val loss: 1.72728\n",
      "Main effects training epoch: 101, train loss: 1.66874, val loss: 1.72752\n",
      "Main effects training epoch: 102, train loss: 1.66602, val loss: 1.70959\n",
      "Main effects training epoch: 103, train loss: 1.66280, val loss: 1.73205\n",
      "Main effects training epoch: 104, train loss: 1.65755, val loss: 1.70858\n",
      "Main effects training epoch: 105, train loss: 1.65766, val loss: 1.71059\n",
      "Main effects training epoch: 106, train loss: 1.65347, val loss: 1.71206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 107, train loss: 1.64887, val loss: 1.69743\n",
      "Main effects training epoch: 108, train loss: 1.64387, val loss: 1.69369\n",
      "Main effects training epoch: 109, train loss: 1.63950, val loss: 1.68445\n",
      "Main effects training epoch: 110, train loss: 1.63605, val loss: 1.68484\n",
      "Main effects training epoch: 111, train loss: 1.63629, val loss: 1.67792\n",
      "Main effects training epoch: 112, train loss: 1.62587, val loss: 1.66865\n",
      "Main effects training epoch: 113, train loss: 1.63061, val loss: 1.66090\n",
      "Main effects training epoch: 114, train loss: 1.62642, val loss: 1.67115\n",
      "Main effects training epoch: 115, train loss: 1.61938, val loss: 1.66522\n",
      "Main effects training epoch: 116, train loss: 1.61140, val loss: 1.64267\n",
      "Main effects training epoch: 117, train loss: 1.62219, val loss: 1.64587\n",
      "Main effects training epoch: 118, train loss: 1.61114, val loss: 1.64708\n",
      "Main effects training epoch: 119, train loss: 1.60748, val loss: 1.64141\n",
      "Main effects training epoch: 120, train loss: 1.60806, val loss: 1.62966\n",
      "Main effects training epoch: 121, train loss: 1.60683, val loss: 1.64081\n",
      "Main effects training epoch: 122, train loss: 1.61082, val loss: 1.63344\n",
      "Main effects training epoch: 123, train loss: 1.60073, val loss: 1.63136\n",
      "Main effects training epoch: 124, train loss: 1.60128, val loss: 1.63879\n",
      "Main effects training epoch: 125, train loss: 1.60171, val loss: 1.62831\n",
      "Main effects training epoch: 126, train loss: 1.59545, val loss: 1.62672\n",
      "Main effects training epoch: 127, train loss: 1.59761, val loss: 1.62265\n",
      "Main effects training epoch: 128, train loss: 1.59409, val loss: 1.62712\n",
      "Main effects training epoch: 129, train loss: 1.59509, val loss: 1.62690\n",
      "Main effects training epoch: 130, train loss: 1.59336, val loss: 1.62085\n",
      "Main effects training epoch: 131, train loss: 1.59135, val loss: 1.61248\n",
      "Main effects training epoch: 132, train loss: 1.58993, val loss: 1.61845\n",
      "Main effects training epoch: 133, train loss: 1.59004, val loss: 1.62008\n",
      "Main effects training epoch: 134, train loss: 1.59252, val loss: 1.62260\n",
      "Main effects training epoch: 135, train loss: 1.58967, val loss: 1.61380\n",
      "Main effects training epoch: 136, train loss: 1.58912, val loss: 1.61294\n",
      "Main effects training epoch: 137, train loss: 1.58721, val loss: 1.61251\n",
      "Main effects training epoch: 138, train loss: 1.59000, val loss: 1.62017\n",
      "Main effects training epoch: 139, train loss: 1.59357, val loss: 1.60941\n",
      "Main effects training epoch: 140, train loss: 1.59297, val loss: 1.61517\n",
      "Main effects training epoch: 141, train loss: 1.59372, val loss: 1.62326\n",
      "Main effects training epoch: 142, train loss: 1.59116, val loss: 1.62202\n",
      "Main effects training epoch: 143, train loss: 1.59030, val loss: 1.60521\n",
      "Main effects training epoch: 144, train loss: 1.59430, val loss: 1.63593\n",
      "Main effects training epoch: 145, train loss: 1.58535, val loss: 1.60490\n",
      "Main effects training epoch: 146, train loss: 1.58662, val loss: 1.60887\n",
      "Main effects training epoch: 147, train loss: 1.58456, val loss: 1.60313\n",
      "Main effects training epoch: 148, train loss: 1.58936, val loss: 1.60629\n",
      "Main effects training epoch: 149, train loss: 1.59276, val loss: 1.61946\n",
      "Main effects training epoch: 150, train loss: 1.58725, val loss: 1.60494\n",
      "Main effects training epoch: 151, train loss: 1.58526, val loss: 1.60820\n",
      "Main effects training epoch: 152, train loss: 1.58906, val loss: 1.60458\n",
      "Main effects training epoch: 153, train loss: 1.58315, val loss: 1.60372\n",
      "Main effects training epoch: 154, train loss: 1.58197, val loss: 1.60628\n",
      "Main effects training epoch: 155, train loss: 1.59070, val loss: 1.59993\n",
      "Main effects training epoch: 156, train loss: 1.58156, val loss: 1.60517\n",
      "Main effects training epoch: 157, train loss: 1.58295, val loss: 1.61067\n",
      "Main effects training epoch: 158, train loss: 1.58209, val loss: 1.59302\n",
      "Main effects training epoch: 159, train loss: 1.58039, val loss: 1.60090\n",
      "Main effects training epoch: 160, train loss: 1.58152, val loss: 1.59803\n",
      "Main effects training epoch: 161, train loss: 1.58175, val loss: 1.60055\n",
      "Main effects training epoch: 162, train loss: 1.57898, val loss: 1.60385\n",
      "Main effects training epoch: 163, train loss: 1.58262, val loss: 1.59093\n",
      "Main effects training epoch: 164, train loss: 1.58000, val loss: 1.59406\n",
      "Main effects training epoch: 165, train loss: 1.58268, val loss: 1.60631\n",
      "Main effects training epoch: 166, train loss: 1.58237, val loss: 1.58803\n",
      "Main effects training epoch: 167, train loss: 1.57634, val loss: 1.58860\n",
      "Main effects training epoch: 168, train loss: 1.58137, val loss: 1.60208\n",
      "Main effects training epoch: 169, train loss: 1.57633, val loss: 1.59946\n",
      "Main effects training epoch: 170, train loss: 1.57351, val loss: 1.57735\n",
      "Main effects training epoch: 171, train loss: 1.57064, val loss: 1.58825\n",
      "Main effects training epoch: 172, train loss: 1.57649, val loss: 1.59631\n",
      "Main effects training epoch: 173, train loss: 1.56974, val loss: 1.58911\n",
      "Main effects training epoch: 174, train loss: 1.57086, val loss: 1.57583\n",
      "Main effects training epoch: 175, train loss: 1.57695, val loss: 1.59721\n",
      "Main effects training epoch: 176, train loss: 1.56603, val loss: 1.58176\n",
      "Main effects training epoch: 177, train loss: 1.56937, val loss: 1.57376\n",
      "Main effects training epoch: 178, train loss: 1.56830, val loss: 1.57481\n",
      "Main effects training epoch: 179, train loss: 1.57881, val loss: 1.59691\n",
      "Main effects training epoch: 180, train loss: 1.57103, val loss: 1.57682\n",
      "Main effects training epoch: 181, train loss: 1.57112, val loss: 1.56954\n",
      "Main effects training epoch: 182, train loss: 1.56487, val loss: 1.57241\n",
      "Main effects training epoch: 183, train loss: 1.56356, val loss: 1.56844\n",
      "Main effects training epoch: 184, train loss: 1.55993, val loss: 1.56822\n",
      "Main effects training epoch: 185, train loss: 1.56026, val loss: 1.56621\n",
      "Main effects training epoch: 186, train loss: 1.56052, val loss: 1.56450\n",
      "Main effects training epoch: 187, train loss: 1.56438, val loss: 1.56819\n",
      "Main effects training epoch: 188, train loss: 1.56646, val loss: 1.56416\n",
      "Main effects training epoch: 189, train loss: 1.56458, val loss: 1.57131\n",
      "Main effects training epoch: 190, train loss: 1.55665, val loss: 1.56159\n",
      "Main effects training epoch: 191, train loss: 1.56181, val loss: 1.56742\n",
      "Main effects training epoch: 192, train loss: 1.56064, val loss: 1.56715\n",
      "Main effects training epoch: 193, train loss: 1.55976, val loss: 1.57048\n",
      "Main effects training epoch: 194, train loss: 1.56239, val loss: 1.56066\n",
      "Main effects training epoch: 195, train loss: 1.56100, val loss: 1.55201\n",
      "Main effects training epoch: 196, train loss: 1.56013, val loss: 1.57184\n",
      "Main effects training epoch: 197, train loss: 1.55489, val loss: 1.55930\n",
      "Main effects training epoch: 198, train loss: 1.55628, val loss: 1.55173\n",
      "Main effects training epoch: 199, train loss: 1.55573, val loss: 1.55741\n",
      "Main effects training epoch: 200, train loss: 1.55869, val loss: 1.56939\n",
      "Main effects training epoch: 201, train loss: 1.55534, val loss: 1.56086\n",
      "Main effects training epoch: 202, train loss: 1.56326, val loss: 1.56053\n",
      "Main effects training epoch: 203, train loss: 1.55295, val loss: 1.56231\n",
      "Main effects training epoch: 204, train loss: 1.55423, val loss: 1.55434\n",
      "Main effects training epoch: 205, train loss: 1.55368, val loss: 1.55461\n",
      "Main effects training epoch: 206, train loss: 1.55490, val loss: 1.56242\n",
      "Main effects training epoch: 207, train loss: 1.55651, val loss: 1.55558\n",
      "Main effects training epoch: 208, train loss: 1.55646, val loss: 1.54831\n",
      "Main effects training epoch: 209, train loss: 1.56660, val loss: 1.58333\n",
      "Main effects training epoch: 210, train loss: 1.57009, val loss: 1.55502\n",
      "Main effects training epoch: 211, train loss: 1.55874, val loss: 1.56646\n",
      "Main effects training epoch: 212, train loss: 1.55213, val loss: 1.55578\n",
      "Main effects training epoch: 213, train loss: 1.55802, val loss: 1.55669\n",
      "Main effects training epoch: 214, train loss: 1.56169, val loss: 1.57376\n",
      "Main effects training epoch: 215, train loss: 1.55423, val loss: 1.54634\n",
      "Main effects training epoch: 216, train loss: 1.55741, val loss: 1.55221\n",
      "Main effects training epoch: 217, train loss: 1.56283, val loss: 1.56593\n",
      "Main effects training epoch: 218, train loss: 1.55431, val loss: 1.56278\n",
      "Main effects training epoch: 219, train loss: 1.55005, val loss: 1.54655\n",
      "Main effects training epoch: 220, train loss: 1.55005, val loss: 1.55709\n",
      "Main effects training epoch: 221, train loss: 1.54942, val loss: 1.55639\n",
      "Main effects training epoch: 222, train loss: 1.55192, val loss: 1.55150\n",
      "Main effects training epoch: 223, train loss: 1.55069, val loss: 1.56012\n",
      "Main effects training epoch: 224, train loss: 1.54816, val loss: 1.54145\n",
      "Main effects training epoch: 225, train loss: 1.54979, val loss: 1.55247\n",
      "Main effects training epoch: 226, train loss: 1.54992, val loss: 1.54871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 227, train loss: 1.55000, val loss: 1.55407\n",
      "Main effects training epoch: 228, train loss: 1.54729, val loss: 1.54394\n",
      "Main effects training epoch: 229, train loss: 1.54894, val loss: 1.53697\n",
      "Main effects training epoch: 230, train loss: 1.55231, val loss: 1.55980\n",
      "Main effects training epoch: 231, train loss: 1.55232, val loss: 1.54508\n",
      "Main effects training epoch: 232, train loss: 1.54748, val loss: 1.54207\n",
      "Main effects training epoch: 233, train loss: 1.54640, val loss: 1.54939\n",
      "Main effects training epoch: 234, train loss: 1.54752, val loss: 1.55181\n",
      "Main effects training epoch: 235, train loss: 1.55038, val loss: 1.55247\n",
      "Main effects training epoch: 236, train loss: 1.55404, val loss: 1.54317\n",
      "Main effects training epoch: 237, train loss: 1.54560, val loss: 1.54984\n",
      "Main effects training epoch: 238, train loss: 1.54529, val loss: 1.54679\n",
      "Main effects training epoch: 239, train loss: 1.54183, val loss: 1.54142\n",
      "Main effects training epoch: 240, train loss: 1.54433, val loss: 1.55195\n",
      "Main effects training epoch: 241, train loss: 1.54241, val loss: 1.54244\n",
      "Main effects training epoch: 242, train loss: 1.54486, val loss: 1.55056\n",
      "Main effects training epoch: 243, train loss: 1.54496, val loss: 1.53757\n",
      "Main effects training epoch: 244, train loss: 1.54080, val loss: 1.53688\n",
      "Main effects training epoch: 245, train loss: 1.54388, val loss: 1.53948\n",
      "Main effects training epoch: 246, train loss: 1.54348, val loss: 1.54763\n",
      "Main effects training epoch: 247, train loss: 1.54942, val loss: 1.55217\n",
      "Main effects training epoch: 248, train loss: 1.54551, val loss: 1.54594\n",
      "Main effects training epoch: 249, train loss: 1.54565, val loss: 1.53982\n",
      "Main effects training epoch: 250, train loss: 1.54183, val loss: 1.54448\n",
      "Main effects training epoch: 251, train loss: 1.54128, val loss: 1.54639\n",
      "Main effects training epoch: 252, train loss: 1.54152, val loss: 1.53669\n",
      "Main effects training epoch: 253, train loss: 1.54410, val loss: 1.54973\n",
      "Main effects training epoch: 254, train loss: 1.54274, val loss: 1.53943\n",
      "Main effects training epoch: 255, train loss: 1.54028, val loss: 1.53364\n",
      "Main effects training epoch: 256, train loss: 1.53955, val loss: 1.54563\n",
      "Main effects training epoch: 257, train loss: 1.53841, val loss: 1.53829\n",
      "Main effects training epoch: 258, train loss: 1.53883, val loss: 1.53229\n",
      "Main effects training epoch: 259, train loss: 1.53643, val loss: 1.54358\n",
      "Main effects training epoch: 260, train loss: 1.53832, val loss: 1.53175\n",
      "Main effects training epoch: 261, train loss: 1.53714, val loss: 1.53545\n",
      "Main effects training epoch: 262, train loss: 1.53454, val loss: 1.53746\n",
      "Main effects training epoch: 263, train loss: 1.53639, val loss: 1.54482\n",
      "Main effects training epoch: 264, train loss: 1.53508, val loss: 1.53741\n",
      "Main effects training epoch: 265, train loss: 1.54259, val loss: 1.54483\n",
      "Main effects training epoch: 266, train loss: 1.53962, val loss: 1.53388\n",
      "Main effects training epoch: 267, train loss: 1.54372, val loss: 1.54482\n",
      "Main effects training epoch: 268, train loss: 1.54207, val loss: 1.55046\n",
      "Main effects training epoch: 269, train loss: 1.53817, val loss: 1.54974\n",
      "Main effects training epoch: 270, train loss: 1.53426, val loss: 1.53103\n",
      "Main effects training epoch: 271, train loss: 1.53607, val loss: 1.53560\n",
      "Main effects training epoch: 272, train loss: 1.54558, val loss: 1.55160\n",
      "Main effects training epoch: 273, train loss: 1.55193, val loss: 1.54607\n",
      "Main effects training epoch: 274, train loss: 1.54774, val loss: 1.55307\n",
      "Main effects training epoch: 275, train loss: 1.53721, val loss: 1.53726\n",
      "Main effects training epoch: 276, train loss: 1.53703, val loss: 1.53856\n",
      "Main effects training epoch: 277, train loss: 1.53082, val loss: 1.52387\n",
      "Main effects training epoch: 278, train loss: 1.53033, val loss: 1.53185\n",
      "Main effects training epoch: 279, train loss: 1.52846, val loss: 1.53683\n",
      "Main effects training epoch: 280, train loss: 1.53281, val loss: 1.54340\n",
      "Main effects training epoch: 281, train loss: 1.52920, val loss: 1.52582\n",
      "Main effects training epoch: 282, train loss: 1.52689, val loss: 1.53797\n",
      "Main effects training epoch: 283, train loss: 1.53014, val loss: 1.54555\n",
      "Main effects training epoch: 284, train loss: 1.52867, val loss: 1.52691\n",
      "Main effects training epoch: 285, train loss: 1.52293, val loss: 1.53301\n",
      "Main effects training epoch: 286, train loss: 1.52833, val loss: 1.53113\n",
      "Main effects training epoch: 287, train loss: 1.52310, val loss: 1.53612\n",
      "Main effects training epoch: 288, train loss: 1.52056, val loss: 1.52396\n",
      "Main effects training epoch: 289, train loss: 1.51636, val loss: 1.52023\n",
      "Main effects training epoch: 290, train loss: 1.51769, val loss: 1.52310\n",
      "Main effects training epoch: 291, train loss: 1.52001, val loss: 1.52788\n",
      "Main effects training epoch: 292, train loss: 1.51556, val loss: 1.51603\n",
      "Main effects training epoch: 293, train loss: 1.51555, val loss: 1.52582\n",
      "Main effects training epoch: 294, train loss: 1.52020, val loss: 1.52005\n",
      "Main effects training epoch: 295, train loss: 1.51739, val loss: 1.51753\n",
      "Main effects training epoch: 296, train loss: 1.51848, val loss: 1.53769\n",
      "Main effects training epoch: 297, train loss: 1.51484, val loss: 1.52259\n",
      "Main effects training epoch: 298, train loss: 1.51419, val loss: 1.51813\n",
      "Main effects training epoch: 299, train loss: 1.50863, val loss: 1.52774\n",
      "Main effects training epoch: 300, train loss: 1.50737, val loss: 1.51780\n",
      "##########Stage 1: main effect training stop.##########\n",
      "3 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.52416, val loss: 1.54138\n",
      "Main effects tuning epoch: 2, train loss: 1.52518, val loss: 1.53650\n",
      "Main effects tuning epoch: 3, train loss: 1.52405, val loss: 1.53237\n",
      "Main effects tuning epoch: 4, train loss: 1.52503, val loss: 1.53431\n",
      "Main effects tuning epoch: 5, train loss: 1.52036, val loss: 1.53420\n",
      "Main effects tuning epoch: 6, train loss: 1.52224, val loss: 1.54023\n",
      "Main effects tuning epoch: 7, train loss: 1.52032, val loss: 1.53665\n",
      "Main effects tuning epoch: 8, train loss: 1.52218, val loss: 1.52729\n",
      "Main effects tuning epoch: 9, train loss: 1.52313, val loss: 1.52555\n",
      "Main effects tuning epoch: 10, train loss: 1.54861, val loss: 1.57209\n",
      "Main effects tuning epoch: 11, train loss: 1.52598, val loss: 1.53452\n",
      "Main effects tuning epoch: 12, train loss: 1.53025, val loss: 1.51536\n",
      "Main effects tuning epoch: 13, train loss: 1.51904, val loss: 1.52483\n",
      "Main effects tuning epoch: 14, train loss: 1.51809, val loss: 1.52345\n",
      "Main effects tuning epoch: 15, train loss: 1.52255, val loss: 1.53400\n",
      "Main effects tuning epoch: 16, train loss: 1.52781, val loss: 1.53708\n",
      "Main effects tuning epoch: 17, train loss: 1.53431, val loss: 1.55518\n",
      "Main effects tuning epoch: 18, train loss: 1.51891, val loss: 1.53084\n",
      "Main effects tuning epoch: 19, train loss: 1.52402, val loss: 1.51448\n",
      "Main effects tuning epoch: 20, train loss: 1.51839, val loss: 1.53209\n",
      "Main effects tuning epoch: 21, train loss: 1.51434, val loss: 1.52585\n",
      "Main effects tuning epoch: 22, train loss: 1.50970, val loss: 1.51994\n",
      "Main effects tuning epoch: 23, train loss: 1.51463, val loss: 1.51434\n",
      "Main effects tuning epoch: 24, train loss: 1.51333, val loss: 1.52409\n",
      "Main effects tuning epoch: 25, train loss: 1.52516, val loss: 1.53410\n",
      "Main effects tuning epoch: 26, train loss: 1.51965, val loss: 1.52576\n",
      "Main effects tuning epoch: 27, train loss: 1.51612, val loss: 1.52042\n",
      "Main effects tuning epoch: 28, train loss: 1.51155, val loss: 1.52884\n",
      "Main effects tuning epoch: 29, train loss: 1.51038, val loss: 1.50424\n",
      "Main effects tuning epoch: 30, train loss: 1.51217, val loss: 1.53010\n",
      "Main effects tuning epoch: 31, train loss: 1.50558, val loss: 1.51611\n",
      "Main effects tuning epoch: 32, train loss: 1.51413, val loss: 1.51758\n",
      "Main effects tuning epoch: 33, train loss: 1.50225, val loss: 1.51380\n",
      "Main effects tuning epoch: 34, train loss: 1.51758, val loss: 1.52803\n",
      "Main effects tuning epoch: 35, train loss: 1.51175, val loss: 1.51605\n",
      "Main effects tuning epoch: 36, train loss: 1.51080, val loss: 1.51435\n",
      "Main effects tuning epoch: 37, train loss: 1.50479, val loss: 1.51075\n",
      "Main effects tuning epoch: 38, train loss: 1.51822, val loss: 1.52724\n",
      "Main effects tuning epoch: 39, train loss: 1.51115, val loss: 1.50123\n",
      "Main effects tuning epoch: 40, train loss: 1.50409, val loss: 1.52128\n",
      "Main effects tuning epoch: 41, train loss: 1.50095, val loss: 1.50402\n",
      "Main effects tuning epoch: 42, train loss: 1.50369, val loss: 1.51448\n",
      "Main effects tuning epoch: 43, train loss: 1.51092, val loss: 1.52222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects tuning epoch: 44, train loss: 1.49744, val loss: 1.51168\n",
      "Main effects tuning epoch: 45, train loss: 1.50096, val loss: 1.49988\n",
      "Main effects tuning epoch: 46, train loss: 1.49685, val loss: 1.49997\n",
      "Main effects tuning epoch: 47, train loss: 1.49616, val loss: 1.50616\n",
      "Main effects tuning epoch: 48, train loss: 1.50266, val loss: 1.51476\n",
      "Main effects tuning epoch: 49, train loss: 1.49938, val loss: 1.50840\n",
      "Main effects tuning epoch: 50, train loss: 1.49961, val loss: 1.50333\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.41205, val loss: 1.40451\n",
      "Interaction training epoch: 2, train loss: 1.57999, val loss: 1.54835\n",
      "Interaction training epoch: 3, train loss: 1.12335, val loss: 1.10982\n",
      "Interaction training epoch: 4, train loss: 1.09741, val loss: 1.07532\n",
      "Interaction training epoch: 5, train loss: 1.10605, val loss: 1.10805\n",
      "Interaction training epoch: 6, train loss: 1.01159, val loss: 1.00550\n",
      "Interaction training epoch: 7, train loss: 1.03151, val loss: 0.98940\n",
      "Interaction training epoch: 8, train loss: 1.03901, val loss: 1.02558\n",
      "Interaction training epoch: 9, train loss: 0.99210, val loss: 0.98085\n",
      "Interaction training epoch: 10, train loss: 0.98762, val loss: 0.97206\n",
      "Interaction training epoch: 11, train loss: 0.98117, val loss: 0.96838\n",
      "Interaction training epoch: 12, train loss: 0.97602, val loss: 0.96433\n",
      "Interaction training epoch: 13, train loss: 0.97522, val loss: 0.96699\n",
      "Interaction training epoch: 14, train loss: 0.96577, val loss: 0.95364\n",
      "Interaction training epoch: 15, train loss: 0.93344, val loss: 0.90954\n",
      "Interaction training epoch: 16, train loss: 0.95581, val loss: 0.91859\n",
      "Interaction training epoch: 17, train loss: 0.93685, val loss: 0.92590\n",
      "Interaction training epoch: 18, train loss: 0.93928, val loss: 0.91403\n",
      "Interaction training epoch: 19, train loss: 0.92036, val loss: 0.88644\n",
      "Interaction training epoch: 20, train loss: 0.92541, val loss: 0.89854\n",
      "Interaction training epoch: 21, train loss: 0.92297, val loss: 0.88826\n",
      "Interaction training epoch: 22, train loss: 0.94470, val loss: 0.92769\n",
      "Interaction training epoch: 23, train loss: 0.91273, val loss: 0.88120\n",
      "Interaction training epoch: 24, train loss: 0.91740, val loss: 0.88616\n",
      "Interaction training epoch: 25, train loss: 0.93416, val loss: 0.91050\n",
      "Interaction training epoch: 26, train loss: 0.91651, val loss: 0.88748\n",
      "Interaction training epoch: 27, train loss: 0.92831, val loss: 0.88959\n",
      "Interaction training epoch: 28, train loss: 0.90645, val loss: 0.88218\n",
      "Interaction training epoch: 29, train loss: 0.91191, val loss: 0.87345\n",
      "Interaction training epoch: 30, train loss: 0.91859, val loss: 0.88643\n",
      "Interaction training epoch: 31, train loss: 0.90280, val loss: 0.87451\n",
      "Interaction training epoch: 32, train loss: 0.89700, val loss: 0.86838\n",
      "Interaction training epoch: 33, train loss: 0.90343, val loss: 0.86716\n",
      "Interaction training epoch: 34, train loss: 0.91825, val loss: 0.88963\n",
      "Interaction training epoch: 35, train loss: 0.89286, val loss: 0.85723\n",
      "Interaction training epoch: 36, train loss: 0.89820, val loss: 0.86701\n",
      "Interaction training epoch: 37, train loss: 0.91739, val loss: 0.89405\n",
      "Interaction training epoch: 38, train loss: 0.88863, val loss: 0.86135\n",
      "Interaction training epoch: 39, train loss: 0.89062, val loss: 0.86128\n",
      "Interaction training epoch: 40, train loss: 0.88840, val loss: 0.86286\n",
      "Interaction training epoch: 41, train loss: 0.89894, val loss: 0.87479\n",
      "Interaction training epoch: 42, train loss: 0.90339, val loss: 0.86603\n",
      "Interaction training epoch: 43, train loss: 0.88593, val loss: 0.86130\n",
      "Interaction training epoch: 44, train loss: 0.89011, val loss: 0.85531\n",
      "Interaction training epoch: 45, train loss: 0.88253, val loss: 0.86037\n",
      "Interaction training epoch: 46, train loss: 0.90888, val loss: 0.88244\n",
      "Interaction training epoch: 47, train loss: 0.88563, val loss: 0.85311\n",
      "Interaction training epoch: 48, train loss: 0.88817, val loss: 0.85956\n",
      "Interaction training epoch: 49, train loss: 0.89124, val loss: 0.87399\n",
      "Interaction training epoch: 50, train loss: 0.89433, val loss: 0.85775\n",
      "Interaction training epoch: 51, train loss: 0.89128, val loss: 0.86359\n",
      "Interaction training epoch: 52, train loss: 0.88138, val loss: 0.85895\n",
      "Interaction training epoch: 53, train loss: 0.86881, val loss: 0.84713\n",
      "Interaction training epoch: 54, train loss: 0.87148, val loss: 0.84833\n",
      "Interaction training epoch: 55, train loss: 0.88601, val loss: 0.85681\n",
      "Interaction training epoch: 56, train loss: 0.86985, val loss: 0.86023\n",
      "Interaction training epoch: 57, train loss: 0.87626, val loss: 0.84923\n",
      "Interaction training epoch: 58, train loss: 0.87255, val loss: 0.85047\n",
      "Interaction training epoch: 59, train loss: 0.86686, val loss: 0.85427\n",
      "Interaction training epoch: 60, train loss: 0.86734, val loss: 0.84387\n",
      "Interaction training epoch: 61, train loss: 0.86682, val loss: 0.84255\n",
      "Interaction training epoch: 62, train loss: 0.87048, val loss: 0.86001\n",
      "Interaction training epoch: 63, train loss: 0.85734, val loss: 0.84003\n",
      "Interaction training epoch: 64, train loss: 0.86720, val loss: 0.84100\n",
      "Interaction training epoch: 65, train loss: 0.87708, val loss: 0.88081\n",
      "Interaction training epoch: 66, train loss: 0.87868, val loss: 0.84853\n",
      "Interaction training epoch: 67, train loss: 0.86548, val loss: 0.84960\n",
      "Interaction training epoch: 68, train loss: 0.86245, val loss: 0.84812\n",
      "Interaction training epoch: 69, train loss: 0.88289, val loss: 0.85487\n",
      "Interaction training epoch: 70, train loss: 0.87251, val loss: 0.85119\n",
      "Interaction training epoch: 71, train loss: 0.85199, val loss: 0.84718\n",
      "Interaction training epoch: 72, train loss: 0.85469, val loss: 0.84072\n",
      "Interaction training epoch: 73, train loss: 0.85259, val loss: 0.83467\n",
      "Interaction training epoch: 74, train loss: 0.85653, val loss: 0.84726\n",
      "Interaction training epoch: 75, train loss: 0.85663, val loss: 0.83709\n",
      "Interaction training epoch: 76, train loss: 0.86210, val loss: 0.84279\n",
      "Interaction training epoch: 77, train loss: 0.84974, val loss: 0.84055\n",
      "Interaction training epoch: 78, train loss: 0.85136, val loss: 0.83780\n",
      "Interaction training epoch: 79, train loss: 0.84951, val loss: 0.84135\n",
      "Interaction training epoch: 80, train loss: 0.86832, val loss: 0.85884\n",
      "Interaction training epoch: 81, train loss: 0.86569, val loss: 0.85118\n",
      "Interaction training epoch: 82, train loss: 0.85936, val loss: 0.83181\n",
      "Interaction training epoch: 83, train loss: 0.86541, val loss: 0.86077\n",
      "Interaction training epoch: 84, train loss: 0.84661, val loss: 0.83020\n",
      "Interaction training epoch: 85, train loss: 0.85122, val loss: 0.83671\n",
      "Interaction training epoch: 86, train loss: 0.85610, val loss: 0.83954\n",
      "Interaction training epoch: 87, train loss: 0.85575, val loss: 0.84674\n",
      "Interaction training epoch: 88, train loss: 0.84685, val loss: 0.83205\n",
      "Interaction training epoch: 89, train loss: 0.84265, val loss: 0.83141\n",
      "Interaction training epoch: 90, train loss: 0.84233, val loss: 0.83732\n",
      "Interaction training epoch: 91, train loss: 0.84178, val loss: 0.82879\n",
      "Interaction training epoch: 92, train loss: 0.84275, val loss: 0.82736\n",
      "Interaction training epoch: 93, train loss: 0.84970, val loss: 0.83930\n",
      "Interaction training epoch: 94, train loss: 0.84490, val loss: 0.83047\n",
      "Interaction training epoch: 95, train loss: 0.84236, val loss: 0.82763\n",
      "Interaction training epoch: 96, train loss: 0.83986, val loss: 0.82646\n",
      "Interaction training epoch: 97, train loss: 0.85048, val loss: 0.83503\n",
      "Interaction training epoch: 98, train loss: 0.84045, val loss: 0.82900\n",
      "Interaction training epoch: 99, train loss: 0.84499, val loss: 0.82753\n",
      "Interaction training epoch: 100, train loss: 0.83785, val loss: 0.82872\n",
      "Interaction training epoch: 101, train loss: 0.84303, val loss: 0.83602\n",
      "Interaction training epoch: 102, train loss: 0.83628, val loss: 0.82502\n",
      "Interaction training epoch: 103, train loss: 0.84394, val loss: 0.83539\n",
      "Interaction training epoch: 104, train loss: 0.83860, val loss: 0.82556\n",
      "Interaction training epoch: 105, train loss: 0.83977, val loss: 0.83108\n",
      "Interaction training epoch: 106, train loss: 0.84505, val loss: 0.83517\n",
      "Interaction training epoch: 107, train loss: 0.84353, val loss: 0.83464\n",
      "Interaction training epoch: 108, train loss: 0.83538, val loss: 0.82342\n",
      "Interaction training epoch: 109, train loss: 0.83353, val loss: 0.81921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 110, train loss: 0.84075, val loss: 0.83304\n",
      "Interaction training epoch: 111, train loss: 0.84744, val loss: 0.82962\n",
      "Interaction training epoch: 112, train loss: 0.83540, val loss: 0.83428\n",
      "Interaction training epoch: 113, train loss: 0.83916, val loss: 0.82915\n",
      "Interaction training epoch: 114, train loss: 0.83885, val loss: 0.82648\n",
      "Interaction training epoch: 115, train loss: 0.83596, val loss: 0.82407\n",
      "Interaction training epoch: 116, train loss: 0.84027, val loss: 0.82608\n",
      "Interaction training epoch: 117, train loss: 0.83313, val loss: 0.83208\n",
      "Interaction training epoch: 118, train loss: 0.83478, val loss: 0.82306\n",
      "Interaction training epoch: 119, train loss: 0.84964, val loss: 0.84529\n",
      "Interaction training epoch: 120, train loss: 0.83004, val loss: 0.81836\n",
      "Interaction training epoch: 121, train loss: 0.83243, val loss: 0.81713\n",
      "Interaction training epoch: 122, train loss: 0.83449, val loss: 0.82799\n",
      "Interaction training epoch: 123, train loss: 0.84185, val loss: 0.83685\n",
      "Interaction training epoch: 124, train loss: 0.83439, val loss: 0.83182\n",
      "Interaction training epoch: 125, train loss: 0.83767, val loss: 0.82151\n",
      "Interaction training epoch: 126, train loss: 0.83352, val loss: 0.82158\n",
      "Interaction training epoch: 127, train loss: 0.83267, val loss: 0.82121\n",
      "Interaction training epoch: 128, train loss: 0.83336, val loss: 0.82486\n",
      "Interaction training epoch: 129, train loss: 0.83180, val loss: 0.82177\n",
      "Interaction training epoch: 130, train loss: 0.84032, val loss: 0.82957\n",
      "Interaction training epoch: 131, train loss: 0.83167, val loss: 0.81995\n",
      "Interaction training epoch: 132, train loss: 0.82614, val loss: 0.81811\n",
      "Interaction training epoch: 133, train loss: 0.83772, val loss: 0.82504\n",
      "Interaction training epoch: 134, train loss: 0.83054, val loss: 0.83114\n",
      "Interaction training epoch: 135, train loss: 0.83301, val loss: 0.81968\n",
      "Interaction training epoch: 136, train loss: 0.82792, val loss: 0.81883\n",
      "Interaction training epoch: 137, train loss: 0.82944, val loss: 0.81349\n",
      "Interaction training epoch: 138, train loss: 0.82814, val loss: 0.82270\n",
      "Interaction training epoch: 139, train loss: 0.83485, val loss: 0.82924\n",
      "Interaction training epoch: 140, train loss: 0.83526, val loss: 0.81920\n",
      "Interaction training epoch: 141, train loss: 0.82542, val loss: 0.82292\n",
      "Interaction training epoch: 142, train loss: 0.82752, val loss: 0.82364\n",
      "Interaction training epoch: 143, train loss: 0.83117, val loss: 0.82341\n",
      "Interaction training epoch: 144, train loss: 0.83122, val loss: 0.82132\n",
      "Interaction training epoch: 145, train loss: 0.82985, val loss: 0.81902\n",
      "Interaction training epoch: 146, train loss: 0.82295, val loss: 0.81757\n",
      "Interaction training epoch: 147, train loss: 0.82675, val loss: 0.82184\n",
      "Interaction training epoch: 148, train loss: 0.82854, val loss: 0.81391\n",
      "Interaction training epoch: 149, train loss: 0.82981, val loss: 0.81614\n",
      "Interaction training epoch: 150, train loss: 0.82824, val loss: 0.82961\n",
      "Interaction training epoch: 151, train loss: 0.82512, val loss: 0.81096\n",
      "Interaction training epoch: 152, train loss: 0.82714, val loss: 0.82055\n",
      "Interaction training epoch: 153, train loss: 0.82690, val loss: 0.82471\n",
      "Interaction training epoch: 154, train loss: 0.81902, val loss: 0.80764\n",
      "Interaction training epoch: 155, train loss: 0.82451, val loss: 0.81940\n",
      "Interaction training epoch: 156, train loss: 0.82705, val loss: 0.81835\n",
      "Interaction training epoch: 157, train loss: 0.81425, val loss: 0.80891\n",
      "Interaction training epoch: 158, train loss: 0.83032, val loss: 0.82060\n",
      "Interaction training epoch: 159, train loss: 0.82275, val loss: 0.82245\n",
      "Interaction training epoch: 160, train loss: 0.81915, val loss: 0.81153\n",
      "Interaction training epoch: 161, train loss: 0.83032, val loss: 0.82578\n",
      "Interaction training epoch: 162, train loss: 0.82561, val loss: 0.81406\n",
      "Interaction training epoch: 163, train loss: 0.82552, val loss: 0.82911\n",
      "Interaction training epoch: 164, train loss: 0.82601, val loss: 0.81846\n",
      "Interaction training epoch: 165, train loss: 0.82322, val loss: 0.82800\n",
      "Interaction training epoch: 166, train loss: 0.82766, val loss: 0.82791\n",
      "Interaction training epoch: 167, train loss: 0.82664, val loss: 0.81785\n",
      "Interaction training epoch: 168, train loss: 0.81845, val loss: 0.81132\n",
      "Interaction training epoch: 169, train loss: 0.82417, val loss: 0.81775\n",
      "Interaction training epoch: 170, train loss: 0.81907, val loss: 0.81822\n",
      "Interaction training epoch: 171, train loss: 0.82291, val loss: 0.81634\n",
      "Interaction training epoch: 172, train loss: 0.82134, val loss: 0.81436\n",
      "Interaction training epoch: 173, train loss: 0.82033, val loss: 0.82649\n",
      "Interaction training epoch: 174, train loss: 0.81376, val loss: 0.79722\n",
      "Interaction training epoch: 175, train loss: 0.82989, val loss: 0.82836\n",
      "Interaction training epoch: 176, train loss: 0.81987, val loss: 0.82052\n",
      "Interaction training epoch: 177, train loss: 0.82129, val loss: 0.81508\n",
      "Interaction training epoch: 178, train loss: 0.82691, val loss: 0.81809\n",
      "Interaction training epoch: 179, train loss: 0.81636, val loss: 0.81541\n",
      "Interaction training epoch: 180, train loss: 0.82323, val loss: 0.81276\n",
      "Interaction training epoch: 181, train loss: 0.81928, val loss: 0.82299\n",
      "Interaction training epoch: 182, train loss: 0.81532, val loss: 0.80742\n",
      "Interaction training epoch: 183, train loss: 0.82838, val loss: 0.81800\n",
      "Interaction training epoch: 184, train loss: 0.82392, val loss: 0.82459\n",
      "Interaction training epoch: 185, train loss: 0.81526, val loss: 0.81340\n",
      "Interaction training epoch: 186, train loss: 0.81673, val loss: 0.81478\n",
      "Interaction training epoch: 187, train loss: 0.82398, val loss: 0.82131\n",
      "Interaction training epoch: 188, train loss: 0.82056, val loss: 0.82010\n",
      "Interaction training epoch: 189, train loss: 0.82334, val loss: 0.81834\n",
      "Interaction training epoch: 190, train loss: 0.81565, val loss: 0.81315\n",
      "Interaction training epoch: 191, train loss: 0.82158, val loss: 0.81949\n",
      "Interaction training epoch: 192, train loss: 0.81610, val loss: 0.81360\n",
      "Interaction training epoch: 193, train loss: 0.82351, val loss: 0.82147\n",
      "Interaction training epoch: 194, train loss: 0.81030, val loss: 0.80906\n",
      "Interaction training epoch: 195, train loss: 0.81384, val loss: 0.80437\n",
      "Interaction training epoch: 196, train loss: 0.81809, val loss: 0.82098\n",
      "Interaction training epoch: 197, train loss: 0.81986, val loss: 0.80661\n",
      "Interaction training epoch: 198, train loss: 0.82237, val loss: 0.82228\n",
      "Interaction training epoch: 199, train loss: 0.82434, val loss: 0.81512\n",
      "Interaction training epoch: 200, train loss: 0.81545, val loss: 0.81837\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########2 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.82259, val loss: 0.81237\n",
      "Interaction tuning epoch: 2, train loss: 0.81581, val loss: 0.80779\n",
      "Interaction tuning epoch: 3, train loss: 0.81833, val loss: 0.80906\n",
      "Interaction tuning epoch: 4, train loss: 0.82447, val loss: 0.81657\n",
      "Interaction tuning epoch: 5, train loss: 0.81740, val loss: 0.81015\n",
      "Interaction tuning epoch: 6, train loss: 0.82332, val loss: 0.81271\n",
      "Interaction tuning epoch: 7, train loss: 0.81501, val loss: 0.80494\n",
      "Interaction tuning epoch: 8, train loss: 0.81277, val loss: 0.80694\n",
      "Interaction tuning epoch: 9, train loss: 0.81585, val loss: 0.80398\n",
      "Interaction tuning epoch: 10, train loss: 0.82114, val loss: 0.80735\n",
      "Interaction tuning epoch: 11, train loss: 0.81302, val loss: 0.81144\n",
      "Interaction tuning epoch: 12, train loss: 0.82176, val loss: 0.81939\n",
      "Interaction tuning epoch: 13, train loss: 0.82320, val loss: 0.81285\n",
      "Interaction tuning epoch: 14, train loss: 0.81546, val loss: 0.80586\n",
      "Interaction tuning epoch: 15, train loss: 0.82553, val loss: 0.81557\n",
      "Interaction tuning epoch: 16, train loss: 0.81358, val loss: 0.80986\n",
      "Interaction tuning epoch: 17, train loss: 0.81438, val loss: 0.80318\n",
      "Interaction tuning epoch: 18, train loss: 0.81922, val loss: 0.80864\n",
      "Interaction tuning epoch: 19, train loss: 0.81498, val loss: 0.80644\n",
      "Interaction tuning epoch: 20, train loss: 0.82252, val loss: 0.81292\n",
      "Interaction tuning epoch: 21, train loss: 0.80980, val loss: 0.80422\n",
      "Interaction tuning epoch: 22, train loss: 0.82641, val loss: 0.81316\n",
      "Interaction tuning epoch: 23, train loss: 0.81824, val loss: 0.81906\n",
      "Interaction tuning epoch: 24, train loss: 0.82241, val loss: 0.81165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 25, train loss: 0.81607, val loss: 0.80853\n",
      "Interaction tuning epoch: 26, train loss: 0.81850, val loss: 0.80792\n",
      "Interaction tuning epoch: 27, train loss: 0.81946, val loss: 0.81076\n",
      "Interaction tuning epoch: 28, train loss: 0.81127, val loss: 0.79975\n",
      "Interaction tuning epoch: 29, train loss: 0.82015, val loss: 0.81514\n",
      "Interaction tuning epoch: 30, train loss: 0.80804, val loss: 0.80196\n",
      "Interaction tuning epoch: 31, train loss: 0.81461, val loss: 0.80315\n",
      "Interaction tuning epoch: 32, train loss: 0.81548, val loss: 0.80983\n",
      "Interaction tuning epoch: 33, train loss: 0.80969, val loss: 0.80439\n",
      "Interaction tuning epoch: 34, train loss: 0.81338, val loss: 0.80515\n",
      "Interaction tuning epoch: 35, train loss: 0.82094, val loss: 0.81341\n",
      "Interaction tuning epoch: 36, train loss: 0.81042, val loss: 0.80806\n",
      "Interaction tuning epoch: 37, train loss: 0.81348, val loss: 0.80758\n",
      "Interaction tuning epoch: 38, train loss: 0.81940, val loss: 0.81022\n",
      "Interaction tuning epoch: 39, train loss: 0.81197, val loss: 0.80479\n",
      "Interaction tuning epoch: 40, train loss: 0.81258, val loss: 0.80452\n",
      "Interaction tuning epoch: 41, train loss: 0.81566, val loss: 0.80389\n",
      "Interaction tuning epoch: 42, train loss: 0.81476, val loss: 0.81042\n",
      "Interaction tuning epoch: 43, train loss: 0.80795, val loss: 0.80021\n",
      "Interaction tuning epoch: 44, train loss: 0.81776, val loss: 0.80992\n",
      "Interaction tuning epoch: 45, train loss: 0.81189, val loss: 0.80724\n",
      "Interaction tuning epoch: 46, train loss: 0.80668, val loss: 0.79819\n",
      "Interaction tuning epoch: 47, train loss: 0.81296, val loss: 0.80189\n",
      "Interaction tuning epoch: 48, train loss: 0.81909, val loss: 0.81334\n",
      "Interaction tuning epoch: 49, train loss: 0.81210, val loss: 0.80815\n",
      "Interaction tuning epoch: 50, train loss: 0.81656, val loss: 0.81239\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 46.33082914352417\n",
      "After the gam stage, training error is 0.81656 , validation error is 0.81239\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 20.550663\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.672333 validation MAE=0.770927,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.624841 validation MAE=0.752681,rank=5\n",
      "[SoftImpute] Iter 3: observed MAE=0.585636 validation MAE=0.736283,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.553884 validation MAE=0.721846,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.526623 validation MAE=0.708493,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.502444 validation MAE=0.695397,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.482816 validation MAE=0.684325,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.465892 validation MAE=0.675325,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.450819 validation MAE=0.666633,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.435884 validation MAE=0.657960,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.422921 validation MAE=0.650499,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.409868 validation MAE=0.643310,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.398760 validation MAE=0.637014,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.388594 validation MAE=0.631262,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.378835 validation MAE=0.625910,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.369459 validation MAE=0.620371,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.361264 validation MAE=0.615301,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.353896 validation MAE=0.611316,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.347208 validation MAE=0.607348,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.340036 validation MAE=0.603481,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.335379 validation MAE=0.599998,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.328993 validation MAE=0.596710,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.323402 validation MAE=0.593845,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.318319 validation MAE=0.590073,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.313984 validation MAE=0.588221,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.310174 validation MAE=0.585136,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.306772 validation MAE=0.583035,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.302341 validation MAE=0.580260,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.299642 validation MAE=0.578534,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.294325 validation MAE=0.575947,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.290072 validation MAE=0.574041,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.288737 validation MAE=0.571954,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.284528 validation MAE=0.569891,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.282346 validation MAE=0.567911,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.280364 validation MAE=0.567221,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.277246 validation MAE=0.565564,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.274038 validation MAE=0.564175,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.272746 validation MAE=0.562261,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.270424 validation MAE=0.561228,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.268831 validation MAE=0.560214,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.266724 validation MAE=0.558819,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.263678 validation MAE=0.557879,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.262227 validation MAE=0.556879,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.260006 validation MAE=0.555507,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.259339 validation MAE=0.555302,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.257497 validation MAE=0.554340,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.255476 validation MAE=0.553457,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.253283 validation MAE=0.552188,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.252283 validation MAE=0.551672,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.251705 validation MAE=0.551143,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.250055 validation MAE=0.549699,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.248083 validation MAE=0.548726,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.248463 validation MAE=0.549252,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.246545 validation MAE=0.548087,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.244301 validation MAE=0.547494,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.243028 validation MAE=0.546156,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.242226 validation MAE=0.546220,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.240657 validation MAE=0.545718,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.240001 validation MAE=0.545186,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.239069 validation MAE=0.543805,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.238303 validation MAE=0.543570,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.237942 validation MAE=0.543655,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.236058 validation MAE=0.542607,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.235301 validation MAE=0.541824,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.234394 validation MAE=0.541450,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.234027 validation MAE=0.540847,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.232496 validation MAE=0.540596,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.231282 validation MAE=0.540438,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.230639 validation MAE=0.539458,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.228980 validation MAE=0.538581,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.227343 validation MAE=0.538159,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.227537 validation MAE=0.537550,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.226414 validation MAE=0.537186,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.226139 validation MAE=0.537416,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.225620 validation MAE=0.537160,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.224988 validation MAE=0.536415,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.224631 validation MAE=0.535758,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.223943 validation MAE=0.535565,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.222795 validation MAE=0.535110,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.221983 validation MAE=0.534412,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.221331 validation MAE=0.534353,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.219917 validation MAE=0.534155,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.219521 validation MAE=0.533870,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.219468 validation MAE=0.532980,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.218869 validation MAE=0.532747,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.218147 validation MAE=0.532365,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.217211 validation MAE=0.532175,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.216096 validation MAE=0.531971,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.216002 validation MAE=0.531998,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.216109 validation MAE=0.531091,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.214547 validation MAE=0.530280,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.213112 validation MAE=0.529638,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.212317 validation MAE=0.529602,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.211957 validation MAE=0.529213,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.212533 validation MAE=0.528586,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.210711 validation MAE=0.528325,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.210642 validation MAE=0.528393,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.210030 validation MAE=0.527415,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.209373 validation MAE=0.526747,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.209162 validation MAE=0.526706,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.208588 validation MAE=0.526854,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.208364 validation MAE=0.525909,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.207394 validation MAE=0.525594,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.206184 validation MAE=0.525245,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.205566 validation MAE=0.525475,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.206345 validation MAE=0.524443,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.206137 validation MAE=0.524156,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.205709 validation MAE=0.523967,rank=5\n",
      "[SoftImpute] Iter 109: observed MAE=0.204861 validation MAE=0.523710,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.203882 validation MAE=0.523003,rank=5\n",
      "[SoftImpute] Iter 111: observed MAE=0.203433 validation MAE=0.522685,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.203406 validation MAE=0.521997,rank=5\n",
      "[SoftImpute] Iter 113: observed MAE=0.202949 validation MAE=0.521602,rank=5\n",
      "[SoftImpute] Iter 114: observed MAE=0.202592 validation MAE=0.521445,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 115: observed MAE=0.201838 validation MAE=0.521494,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.200957 validation MAE=0.520314,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.201364 validation MAE=0.520156,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.201049 validation MAE=0.519952,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.199980 validation MAE=0.519393,rank=5\n",
      "[SoftImpute] Iter 120: observed MAE=0.199356 validation MAE=0.518377,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.198826 validation MAE=0.518676,rank=5\n",
      "[SoftImpute] Iter 122: observed MAE=0.197714 validation MAE=0.518305,rank=5\n",
      "[SoftImpute] Iter 123: observed MAE=0.197535 validation MAE=0.517969,rank=5\n",
      "[SoftImpute] Iter 124: observed MAE=0.198501 validation MAE=0.517664,rank=5\n",
      "[SoftImpute] Iter 125: observed MAE=0.198315 validation MAE=0.517415,rank=5\n",
      "[SoftImpute] Iter 126: observed MAE=0.198396 validation MAE=0.517251,rank=5\n",
      "[SoftImpute] Iter 127: observed MAE=0.197181 validation MAE=0.516789,rank=5\n",
      "[SoftImpute] Iter 128: observed MAE=0.197975 validation MAE=0.516162,rank=5\n",
      "[SoftImpute] Iter 129: observed MAE=0.197237 validation MAE=0.516035,rank=5\n",
      "[SoftImpute] Iter 130: observed MAE=0.196312 validation MAE=0.515709,rank=5\n",
      "[SoftImpute] Iter 131: observed MAE=0.196158 validation MAE=0.515154,rank=5\n",
      "[SoftImpute] Iter 132: observed MAE=0.195883 validation MAE=0.514751,rank=5\n",
      "[SoftImpute] Iter 133: observed MAE=0.195651 validation MAE=0.514217,rank=5\n",
      "[SoftImpute] Iter 134: observed MAE=0.195295 validation MAE=0.513392,rank=5\n",
      "[SoftImpute] Iter 135: observed MAE=0.194208 validation MAE=0.513454,rank=5\n",
      "[SoftImpute] Iter 136: observed MAE=0.193582 validation MAE=0.512729,rank=5\n",
      "[SoftImpute] Iter 137: observed MAE=0.193657 validation MAE=0.512070,rank=5\n",
      "[SoftImpute] Iter 138: observed MAE=0.193114 validation MAE=0.512333,rank=5\n",
      "[SoftImpute] Iter 139: observed MAE=0.192548 validation MAE=0.512084,rank=5\n",
      "[SoftImpute] Iter 140: observed MAE=0.192995 validation MAE=0.510893,rank=5\n",
      "[SoftImpute] Iter 141: observed MAE=0.192464 validation MAE=0.511347,rank=5\n",
      "[SoftImpute] Iter 142: observed MAE=0.191681 validation MAE=0.511400,rank=5\n",
      "[SoftImpute] Iter 143: observed MAE=0.191292 validation MAE=0.510668,rank=5\n",
      "[SoftImpute] Iter 144: observed MAE=0.190565 validation MAE=0.510357,rank=5\n",
      "[SoftImpute] Iter 145: observed MAE=0.190851 validation MAE=0.510006,rank=5\n",
      "[SoftImpute] Iter 146: observed MAE=0.191211 validation MAE=0.509471,rank=5\n",
      "[SoftImpute] Iter 147: observed MAE=0.190786 validation MAE=0.509538,rank=5\n",
      "[SoftImpute] Iter 148: observed MAE=0.190782 validation MAE=0.508907,rank=5\n",
      "[SoftImpute] Iter 149: observed MAE=0.190294 validation MAE=0.508089,rank=5\n",
      "[SoftImpute] Iter 150: observed MAE=0.189766 validation MAE=0.508406,rank=5\n",
      "[SoftImpute] Iter 151: observed MAE=0.190540 validation MAE=0.507940,rank=5\n",
      "[SoftImpute] Iter 152: observed MAE=0.189596 validation MAE=0.507576,rank=5\n",
      "[SoftImpute] Iter 153: observed MAE=0.188838 validation MAE=0.507266,rank=5\n",
      "[SoftImpute] Iter 154: observed MAE=0.188961 validation MAE=0.506635,rank=5\n",
      "[SoftImpute] Iter 155: observed MAE=0.188214 validation MAE=0.506449,rank=5\n",
      "[SoftImpute] Iter 156: observed MAE=0.187728 validation MAE=0.506474,rank=5\n",
      "[SoftImpute] Iter 157: observed MAE=0.187082 validation MAE=0.506283,rank=5\n",
      "[SoftImpute] Iter 158: observed MAE=0.187232 validation MAE=0.506353,rank=5\n",
      "[SoftImpute] Iter 159: observed MAE=0.186743 validation MAE=0.506099,rank=5\n",
      "[SoftImpute] Iter 160: observed MAE=0.186403 validation MAE=0.505516,rank=5\n",
      "[SoftImpute] Iter 161: observed MAE=0.185739 validation MAE=0.505566,rank=5\n",
      "[SoftImpute] Iter 162: observed MAE=0.186258 validation MAE=0.505223,rank=5\n",
      "[SoftImpute] Iter 163: observed MAE=0.186315 validation MAE=0.505040,rank=5\n",
      "[SoftImpute] Iter 164: observed MAE=0.185998 validation MAE=0.504665,rank=5\n",
      "[SoftImpute] Iter 165: observed MAE=0.185984 validation MAE=0.503796,rank=5\n",
      "[SoftImpute] Iter 166: observed MAE=0.185621 validation MAE=0.503859,rank=5\n",
      "[SoftImpute] Iter 167: observed MAE=0.184988 validation MAE=0.503738,rank=5\n",
      "[SoftImpute] Iter 168: observed MAE=0.184513 validation MAE=0.503631,rank=5\n",
      "[SoftImpute] Iter 169: observed MAE=0.184083 validation MAE=0.503026,rank=5\n",
      "[SoftImpute] Iter 170: observed MAE=0.184548 validation MAE=0.503112,rank=5\n",
      "[SoftImpute] Iter 171: observed MAE=0.184934 validation MAE=0.502369,rank=5\n",
      "[SoftImpute] Iter 172: observed MAE=0.184243 validation MAE=0.502082,rank=5\n",
      "[SoftImpute] Iter 173: observed MAE=0.183942 validation MAE=0.502152,rank=5\n",
      "[SoftImpute] Iter 174: observed MAE=0.184498 validation MAE=0.501970,rank=5\n",
      "[SoftImpute] Iter 175: observed MAE=0.183374 validation MAE=0.501640,rank=5\n",
      "[SoftImpute] Iter 176: observed MAE=0.183081 validation MAE=0.501315,rank=5\n",
      "[SoftImpute] Iter 177: observed MAE=0.183061 validation MAE=0.501381,rank=5\n",
      "[SoftImpute] Iter 178: observed MAE=0.182870 validation MAE=0.500508,rank=5\n",
      "[SoftImpute] Iter 179: observed MAE=0.182436 validation MAE=0.500817,rank=5\n",
      "[SoftImpute] Iter 180: observed MAE=0.182348 validation MAE=0.500352,rank=5\n",
      "[SoftImpute] Iter 181: observed MAE=0.181537 validation MAE=0.500462,rank=5\n",
      "[SoftImpute] Iter 182: observed MAE=0.181109 validation MAE=0.499931,rank=5\n",
      "[SoftImpute] Iter 183: observed MAE=0.181734 validation MAE=0.499994,rank=5\n",
      "[SoftImpute] Iter 184: observed MAE=0.181822 validation MAE=0.499153,rank=5\n",
      "[SoftImpute] Iter 185: observed MAE=0.181866 validation MAE=0.498892,rank=5\n",
      "[SoftImpute] Iter 186: observed MAE=0.181453 validation MAE=0.498723,rank=5\n",
      "[SoftImpute] Iter 187: observed MAE=0.180945 validation MAE=0.498379,rank=5\n",
      "[SoftImpute] Iter 188: observed MAE=0.181178 validation MAE=0.498123,rank=5\n",
      "[SoftImpute] Iter 189: observed MAE=0.180219 validation MAE=0.498115,rank=5\n",
      "[SoftImpute] Iter 190: observed MAE=0.179717 validation MAE=0.498225,rank=5\n",
      "[SoftImpute] Iter 191: observed MAE=0.179777 validation MAE=0.497946,rank=5\n",
      "[SoftImpute] Iter 192: observed MAE=0.179394 validation MAE=0.497091,rank=5\n",
      "[SoftImpute] Iter 193: observed MAE=0.179825 validation MAE=0.497399,rank=5\n",
      "[SoftImpute] Iter 194: observed MAE=0.180063 validation MAE=0.497393,rank=5\n",
      "[SoftImpute] Iter 195: observed MAE=0.179780 validation MAE=0.496746,rank=5\n",
      "[SoftImpute] Iter 196: observed MAE=0.180039 validation MAE=0.496565,rank=5\n",
      "[SoftImpute] Iter 197: observed MAE=0.179839 validation MAE=0.496556,rank=5\n",
      "[SoftImpute] Iter 198: observed MAE=0.179684 validation MAE=0.496307,rank=5\n",
      "[SoftImpute] Iter 199: observed MAE=0.179460 validation MAE=0.496356,rank=5\n",
      "[SoftImpute] Iter 200: observed MAE=0.179130 validation MAE=0.496038,rank=5\n",
      "[SoftImpute] Stopped after iteration 200 for lambda=0.411013\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 6.833630800247192\n",
      "After the matrix factor stage, training error is 0.17913, validation error is 0.49604\n",
      "5\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.07027, val loss: 4.06602\n",
      "Main effects training epoch: 2, train loss: 3.93379, val loss: 3.94513\n",
      "Main effects training epoch: 3, train loss: 3.74430, val loss: 3.76803\n",
      "Main effects training epoch: 4, train loss: 3.51329, val loss: 3.55463\n",
      "Main effects training epoch: 5, train loss: 3.28900, val loss: 3.39083\n",
      "Main effects training epoch: 6, train loss: 3.31699, val loss: 3.45473\n",
      "Main effects training epoch: 7, train loss: 3.25032, val loss: 3.37568\n",
      "Main effects training epoch: 8, train loss: 3.21222, val loss: 3.32370\n",
      "Main effects training epoch: 9, train loss: 3.19005, val loss: 3.30643\n",
      "Main effects training epoch: 10, train loss: 3.17434, val loss: 3.29641\n",
      "Main effects training epoch: 11, train loss: 3.15048, val loss: 3.26013\n",
      "Main effects training epoch: 12, train loss: 3.04995, val loss: 3.16643\n",
      "Main effects training epoch: 13, train loss: 2.99159, val loss: 3.10395\n",
      "Main effects training epoch: 14, train loss: 2.96218, val loss: 3.06633\n",
      "Main effects training epoch: 15, train loss: 2.92717, val loss: 3.04091\n",
      "Main effects training epoch: 16, train loss: 2.88144, val loss: 2.99020\n",
      "Main effects training epoch: 17, train loss: 2.84539, val loss: 2.95038\n",
      "Main effects training epoch: 18, train loss: 2.78748, val loss: 2.89862\n",
      "Main effects training epoch: 19, train loss: 2.73403, val loss: 2.84394\n",
      "Main effects training epoch: 20, train loss: 2.69072, val loss: 2.79679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 21, train loss: 2.66177, val loss: 2.77472\n",
      "Main effects training epoch: 22, train loss: 2.58677, val loss: 2.70071\n",
      "Main effects training epoch: 23, train loss: 2.53871, val loss: 2.64719\n",
      "Main effects training epoch: 24, train loss: 2.49384, val loss: 2.60397\n",
      "Main effects training epoch: 25, train loss: 2.45163, val loss: 2.55566\n",
      "Main effects training epoch: 26, train loss: 2.42234, val loss: 2.53253\n",
      "Main effects training epoch: 27, train loss: 2.36186, val loss: 2.45233\n",
      "Main effects training epoch: 28, train loss: 2.34956, val loss: 2.44235\n",
      "Main effects training epoch: 29, train loss: 2.29459, val loss: 2.38560\n",
      "Main effects training epoch: 30, train loss: 2.29433, val loss: 2.38664\n",
      "Main effects training epoch: 31, train loss: 2.23489, val loss: 2.31601\n",
      "Main effects training epoch: 32, train loss: 2.23363, val loss: 2.30874\n",
      "Main effects training epoch: 33, train loss: 2.17169, val loss: 2.24975\n",
      "Main effects training epoch: 34, train loss: 2.13082, val loss: 2.19530\n",
      "Main effects training epoch: 35, train loss: 2.10294, val loss: 2.16828\n",
      "Main effects training epoch: 36, train loss: 2.06815, val loss: 2.11997\n",
      "Main effects training epoch: 37, train loss: 2.04500, val loss: 2.09966\n",
      "Main effects training epoch: 38, train loss: 2.02552, val loss: 2.06479\n",
      "Main effects training epoch: 39, train loss: 2.01674, val loss: 2.06088\n",
      "Main effects training epoch: 40, train loss: 1.98796, val loss: 2.03221\n",
      "Main effects training epoch: 41, train loss: 1.96329, val loss: 1.99332\n",
      "Main effects training epoch: 42, train loss: 1.96394, val loss: 2.00894\n",
      "Main effects training epoch: 43, train loss: 1.92775, val loss: 1.95782\n",
      "Main effects training epoch: 44, train loss: 1.94724, val loss: 1.97959\n",
      "Main effects training epoch: 45, train loss: 1.91692, val loss: 1.94706\n",
      "Main effects training epoch: 46, train loss: 1.89615, val loss: 1.92490\n",
      "Main effects training epoch: 47, train loss: 1.89024, val loss: 1.91429\n",
      "Main effects training epoch: 48, train loss: 1.88377, val loss: 1.90999\n",
      "Main effects training epoch: 49, train loss: 1.85427, val loss: 1.86578\n",
      "Main effects training epoch: 50, train loss: 1.86257, val loss: 1.88980\n",
      "Main effects training epoch: 51, train loss: 1.84960, val loss: 1.86705\n",
      "Main effects training epoch: 52, train loss: 1.85082, val loss: 1.87672\n",
      "Main effects training epoch: 53, train loss: 1.82528, val loss: 1.83493\n",
      "Main effects training epoch: 54, train loss: 1.81536, val loss: 1.84547\n",
      "Main effects training epoch: 55, train loss: 1.81169, val loss: 1.81867\n",
      "Main effects training epoch: 56, train loss: 1.80857, val loss: 1.83521\n",
      "Main effects training epoch: 57, train loss: 1.79891, val loss: 1.81976\n",
      "Main effects training epoch: 58, train loss: 1.79758, val loss: 1.81807\n",
      "Main effects training epoch: 59, train loss: 1.78416, val loss: 1.79618\n",
      "Main effects training epoch: 60, train loss: 1.78183, val loss: 1.80669\n",
      "Main effects training epoch: 61, train loss: 1.77624, val loss: 1.78774\n",
      "Main effects training epoch: 62, train loss: 1.76022, val loss: 1.76426\n",
      "Main effects training epoch: 63, train loss: 1.77404, val loss: 1.79725\n",
      "Main effects training epoch: 64, train loss: 1.76280, val loss: 1.76836\n",
      "Main effects training epoch: 65, train loss: 1.76275, val loss: 1.77390\n",
      "Main effects training epoch: 66, train loss: 1.75471, val loss: 1.76282\n",
      "Main effects training epoch: 67, train loss: 1.75234, val loss: 1.76236\n",
      "Main effects training epoch: 68, train loss: 1.74863, val loss: 1.75851\n",
      "Main effects training epoch: 69, train loss: 1.74149, val loss: 1.75017\n",
      "Main effects training epoch: 70, train loss: 1.73664, val loss: 1.73995\n",
      "Main effects training epoch: 71, train loss: 1.74164, val loss: 1.74598\n",
      "Main effects training epoch: 72, train loss: 1.73481, val loss: 1.74508\n",
      "Main effects training epoch: 73, train loss: 1.73113, val loss: 1.72845\n",
      "Main effects training epoch: 74, train loss: 1.73084, val loss: 1.72755\n",
      "Main effects training epoch: 75, train loss: 1.72490, val loss: 1.72840\n",
      "Main effects training epoch: 76, train loss: 1.73092, val loss: 1.73230\n",
      "Main effects training epoch: 77, train loss: 1.72474, val loss: 1.72019\n",
      "Main effects training epoch: 78, train loss: 1.72423, val loss: 1.72671\n",
      "Main effects training epoch: 79, train loss: 1.71977, val loss: 1.71332\n",
      "Main effects training epoch: 80, train loss: 1.72154, val loss: 1.72066\n",
      "Main effects training epoch: 81, train loss: 1.71883, val loss: 1.71533\n",
      "Main effects training epoch: 82, train loss: 1.71946, val loss: 1.72037\n",
      "Main effects training epoch: 83, train loss: 1.71597, val loss: 1.70932\n",
      "Main effects training epoch: 84, train loss: 1.71436, val loss: 1.70977\n",
      "Main effects training epoch: 85, train loss: 1.71502, val loss: 1.71150\n",
      "Main effects training epoch: 86, train loss: 1.71384, val loss: 1.71540\n",
      "Main effects training epoch: 87, train loss: 1.71463, val loss: 1.71444\n",
      "Main effects training epoch: 88, train loss: 1.71028, val loss: 1.71313\n",
      "Main effects training epoch: 89, train loss: 1.70920, val loss: 1.69404\n",
      "Main effects training epoch: 90, train loss: 1.70331, val loss: 1.69783\n",
      "Main effects training epoch: 91, train loss: 1.70428, val loss: 1.70896\n",
      "Main effects training epoch: 92, train loss: 1.70124, val loss: 1.69199\n",
      "Main effects training epoch: 93, train loss: 1.69925, val loss: 1.70343\n",
      "Main effects training epoch: 94, train loss: 1.69623, val loss: 1.69582\n",
      "Main effects training epoch: 95, train loss: 1.69174, val loss: 1.69470\n",
      "Main effects training epoch: 96, train loss: 1.69145, val loss: 1.68793\n",
      "Main effects training epoch: 97, train loss: 1.68781, val loss: 1.69509\n",
      "Main effects training epoch: 98, train loss: 1.68476, val loss: 1.68788\n",
      "Main effects training epoch: 99, train loss: 1.68144, val loss: 1.69361\n",
      "Main effects training epoch: 100, train loss: 1.67593, val loss: 1.68079\n",
      "Main effects training epoch: 101, train loss: 1.67109, val loss: 1.68313\n",
      "Main effects training epoch: 102, train loss: 1.66591, val loss: 1.68120\n",
      "Main effects training epoch: 103, train loss: 1.66150, val loss: 1.67493\n",
      "Main effects training epoch: 104, train loss: 1.65884, val loss: 1.67324\n",
      "Main effects training epoch: 105, train loss: 1.66107, val loss: 1.66732\n",
      "Main effects training epoch: 106, train loss: 1.65174, val loss: 1.67615\n",
      "Main effects training epoch: 107, train loss: 1.64076, val loss: 1.65137\n",
      "Main effects training epoch: 108, train loss: 1.64356, val loss: 1.65236\n",
      "Main effects training epoch: 109, train loss: 1.64208, val loss: 1.66237\n",
      "Main effects training epoch: 110, train loss: 1.64096, val loss: 1.64890\n",
      "Main effects training epoch: 111, train loss: 1.63688, val loss: 1.64680\n",
      "Main effects training epoch: 112, train loss: 1.63526, val loss: 1.65536\n",
      "Main effects training epoch: 113, train loss: 1.63228, val loss: 1.64641\n",
      "Main effects training epoch: 114, train loss: 1.63032, val loss: 1.64217\n",
      "Main effects training epoch: 115, train loss: 1.62961, val loss: 1.63943\n",
      "Main effects training epoch: 116, train loss: 1.63332, val loss: 1.63346\n",
      "Main effects training epoch: 117, train loss: 1.63304, val loss: 1.64152\n",
      "Main effects training epoch: 118, train loss: 1.62698, val loss: 1.63884\n",
      "Main effects training epoch: 119, train loss: 1.62810, val loss: 1.63731\n",
      "Main effects training epoch: 120, train loss: 1.62561, val loss: 1.63249\n",
      "Main effects training epoch: 121, train loss: 1.62500, val loss: 1.63631\n",
      "Main effects training epoch: 122, train loss: 1.62368, val loss: 1.62784\n",
      "Main effects training epoch: 123, train loss: 1.62635, val loss: 1.63627\n",
      "Main effects training epoch: 124, train loss: 1.63426, val loss: 1.62597\n",
      "Main effects training epoch: 125, train loss: 1.63210, val loss: 1.64703\n",
      "Main effects training epoch: 126, train loss: 1.62739, val loss: 1.62623\n",
      "Main effects training epoch: 127, train loss: 1.62545, val loss: 1.62601\n",
      "Main effects training epoch: 128, train loss: 1.62320, val loss: 1.62415\n",
      "Main effects training epoch: 129, train loss: 1.62158, val loss: 1.62368\n",
      "Main effects training epoch: 130, train loss: 1.62611, val loss: 1.63137\n",
      "Main effects training epoch: 131, train loss: 1.62442, val loss: 1.62764\n",
      "Main effects training epoch: 132, train loss: 1.61972, val loss: 1.62274\n",
      "Main effects training epoch: 133, train loss: 1.62033, val loss: 1.61811\n",
      "Main effects training epoch: 134, train loss: 1.62755, val loss: 1.62563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 135, train loss: 1.62094, val loss: 1.62105\n",
      "Main effects training epoch: 136, train loss: 1.62030, val loss: 1.61820\n",
      "Main effects training epoch: 137, train loss: 1.62649, val loss: 1.62870\n",
      "Main effects training epoch: 138, train loss: 1.62321, val loss: 1.62357\n",
      "Main effects training epoch: 139, train loss: 1.62303, val loss: 1.63023\n",
      "Main effects training epoch: 140, train loss: 1.61541, val loss: 1.61405\n",
      "Main effects training epoch: 141, train loss: 1.61516, val loss: 1.62004\n",
      "Main effects training epoch: 142, train loss: 1.61579, val loss: 1.61361\n",
      "Main effects training epoch: 143, train loss: 1.61337, val loss: 1.61528\n",
      "Main effects training epoch: 144, train loss: 1.61367, val loss: 1.60775\n",
      "Main effects training epoch: 145, train loss: 1.61312, val loss: 1.61461\n",
      "Main effects training epoch: 146, train loss: 1.61222, val loss: 1.61169\n",
      "Main effects training epoch: 147, train loss: 1.61830, val loss: 1.62402\n",
      "Main effects training epoch: 148, train loss: 1.61487, val loss: 1.62111\n",
      "Main effects training epoch: 149, train loss: 1.61816, val loss: 1.61805\n",
      "Main effects training epoch: 150, train loss: 1.62125, val loss: 1.61596\n",
      "Main effects training epoch: 151, train loss: 1.61284, val loss: 1.61429\n",
      "Main effects training epoch: 152, train loss: 1.61269, val loss: 1.60936\n",
      "Main effects training epoch: 153, train loss: 1.61568, val loss: 1.60968\n",
      "Main effects training epoch: 154, train loss: 1.61604, val loss: 1.62622\n",
      "Main effects training epoch: 155, train loss: 1.61046, val loss: 1.60702\n",
      "Main effects training epoch: 156, train loss: 1.61217, val loss: 1.61182\n",
      "Main effects training epoch: 157, train loss: 1.61732, val loss: 1.61763\n",
      "Main effects training epoch: 158, train loss: 1.61272, val loss: 1.60901\n",
      "Main effects training epoch: 159, train loss: 1.60972, val loss: 1.60400\n",
      "Main effects training epoch: 160, train loss: 1.60885, val loss: 1.61417\n",
      "Main effects training epoch: 161, train loss: 1.60960, val loss: 1.60387\n",
      "Main effects training epoch: 162, train loss: 1.61238, val loss: 1.61104\n",
      "Main effects training epoch: 163, train loss: 1.60754, val loss: 1.60883\n",
      "Main effects training epoch: 164, train loss: 1.60508, val loss: 1.59162\n",
      "Main effects training epoch: 165, train loss: 1.60541, val loss: 1.59907\n",
      "Main effects training epoch: 166, train loss: 1.60562, val loss: 1.60693\n",
      "Main effects training epoch: 167, train loss: 1.60365, val loss: 1.59620\n",
      "Main effects training epoch: 168, train loss: 1.60450, val loss: 1.60455\n",
      "Main effects training epoch: 169, train loss: 1.59999, val loss: 1.59469\n",
      "Main effects training epoch: 170, train loss: 1.59973, val loss: 1.59361\n",
      "Main effects training epoch: 171, train loss: 1.60304, val loss: 1.59357\n",
      "Main effects training epoch: 172, train loss: 1.59933, val loss: 1.60288\n",
      "Main effects training epoch: 173, train loss: 1.60066, val loss: 1.59204\n",
      "Main effects training epoch: 174, train loss: 1.59712, val loss: 1.59143\n",
      "Main effects training epoch: 175, train loss: 1.59910, val loss: 1.59350\n",
      "Main effects training epoch: 176, train loss: 1.59912, val loss: 1.59392\n",
      "Main effects training epoch: 177, train loss: 1.59959, val loss: 1.59218\n",
      "Main effects training epoch: 178, train loss: 1.59807, val loss: 1.59395\n",
      "Main effects training epoch: 179, train loss: 1.59956, val loss: 1.59867\n",
      "Main effects training epoch: 180, train loss: 1.59905, val loss: 1.59748\n",
      "Main effects training epoch: 181, train loss: 1.59772, val loss: 1.60441\n",
      "Main effects training epoch: 182, train loss: 1.59421, val loss: 1.58688\n",
      "Main effects training epoch: 183, train loss: 1.59548, val loss: 1.59793\n",
      "Main effects training epoch: 184, train loss: 1.59303, val loss: 1.59115\n",
      "Main effects training epoch: 185, train loss: 1.59020, val loss: 1.58359\n",
      "Main effects training epoch: 186, train loss: 1.59682, val loss: 1.59952\n",
      "Main effects training epoch: 187, train loss: 1.59797, val loss: 1.59546\n",
      "Main effects training epoch: 188, train loss: 1.58764, val loss: 1.59235\n",
      "Main effects training epoch: 189, train loss: 1.58902, val loss: 1.58168\n",
      "Main effects training epoch: 190, train loss: 1.59010, val loss: 1.58694\n",
      "Main effects training epoch: 191, train loss: 1.58756, val loss: 1.58766\n",
      "Main effects training epoch: 192, train loss: 1.58712, val loss: 1.58818\n",
      "Main effects training epoch: 193, train loss: 1.58751, val loss: 1.58621\n",
      "Main effects training epoch: 194, train loss: 1.58373, val loss: 1.58353\n",
      "Main effects training epoch: 195, train loss: 1.58490, val loss: 1.58262\n",
      "Main effects training epoch: 196, train loss: 1.59467, val loss: 1.59492\n",
      "Main effects training epoch: 197, train loss: 1.58721, val loss: 1.58966\n",
      "Main effects training epoch: 198, train loss: 1.58356, val loss: 1.57758\n",
      "Main effects training epoch: 199, train loss: 1.58470, val loss: 1.58678\n",
      "Main effects training epoch: 200, train loss: 1.58303, val loss: 1.58593\n",
      "Main effects training epoch: 201, train loss: 1.58334, val loss: 1.58150\n",
      "Main effects training epoch: 202, train loss: 1.57646, val loss: 1.57791\n",
      "Main effects training epoch: 203, train loss: 1.57461, val loss: 1.58322\n",
      "Main effects training epoch: 204, train loss: 1.57590, val loss: 1.57541\n",
      "Main effects training epoch: 205, train loss: 1.57336, val loss: 1.57758\n",
      "Main effects training epoch: 206, train loss: 1.57182, val loss: 1.58321\n",
      "Main effects training epoch: 207, train loss: 1.56971, val loss: 1.56854\n",
      "Main effects training epoch: 208, train loss: 1.58130, val loss: 1.59848\n",
      "Main effects training epoch: 209, train loss: 1.57580, val loss: 1.57908\n",
      "Main effects training epoch: 210, train loss: 1.56587, val loss: 1.57079\n",
      "Main effects training epoch: 211, train loss: 1.56923, val loss: 1.57129\n",
      "Main effects training epoch: 212, train loss: 1.56869, val loss: 1.58219\n",
      "Main effects training epoch: 213, train loss: 1.57109, val loss: 1.57790\n",
      "Main effects training epoch: 214, train loss: 1.56548, val loss: 1.56119\n",
      "Main effects training epoch: 215, train loss: 1.56086, val loss: 1.57005\n",
      "Main effects training epoch: 216, train loss: 1.55913, val loss: 1.56935\n",
      "Main effects training epoch: 217, train loss: 1.56017, val loss: 1.57088\n",
      "Main effects training epoch: 218, train loss: 1.55716, val loss: 1.56188\n",
      "Main effects training epoch: 219, train loss: 1.55857, val loss: 1.56968\n",
      "Main effects training epoch: 220, train loss: 1.56119, val loss: 1.58029\n",
      "Main effects training epoch: 221, train loss: 1.55279, val loss: 1.56058\n",
      "Main effects training epoch: 222, train loss: 1.55265, val loss: 1.56299\n",
      "Main effects training epoch: 223, train loss: 1.56422, val loss: 1.57571\n",
      "Main effects training epoch: 224, train loss: 1.55786, val loss: 1.57735\n",
      "Main effects training epoch: 225, train loss: 1.55411, val loss: 1.56567\n",
      "Main effects training epoch: 226, train loss: 1.56379, val loss: 1.56999\n",
      "Main effects training epoch: 227, train loss: 1.55999, val loss: 1.57447\n",
      "Main effects training epoch: 228, train loss: 1.55838, val loss: 1.57521\n",
      "Main effects training epoch: 229, train loss: 1.55446, val loss: 1.55819\n",
      "Main effects training epoch: 230, train loss: 1.55076, val loss: 1.57725\n",
      "Main effects training epoch: 231, train loss: 1.55057, val loss: 1.55620\n",
      "Main effects training epoch: 232, train loss: 1.55165, val loss: 1.56911\n",
      "Main effects training epoch: 233, train loss: 1.55187, val loss: 1.57554\n",
      "Main effects training epoch: 234, train loss: 1.54559, val loss: 1.55512\n",
      "Main effects training epoch: 235, train loss: 1.54457, val loss: 1.55873\n",
      "Main effects training epoch: 236, train loss: 1.54910, val loss: 1.56664\n",
      "Main effects training epoch: 237, train loss: 1.54893, val loss: 1.56169\n",
      "Main effects training epoch: 238, train loss: 1.54647, val loss: 1.55886\n",
      "Main effects training epoch: 239, train loss: 1.55002, val loss: 1.55795\n",
      "Main effects training epoch: 240, train loss: 1.55431, val loss: 1.56549\n",
      "Main effects training epoch: 241, train loss: 1.54953, val loss: 1.57060\n",
      "Main effects training epoch: 242, train loss: 1.54433, val loss: 1.56829\n",
      "Main effects training epoch: 243, train loss: 1.54511, val loss: 1.55806\n",
      "Main effects training epoch: 244, train loss: 1.54290, val loss: 1.56253\n",
      "Main effects training epoch: 245, train loss: 1.54612, val loss: 1.55853\n",
      "Main effects training epoch: 246, train loss: 1.54811, val loss: 1.56497\n",
      "Main effects training epoch: 247, train loss: 1.55443, val loss: 1.57308\n",
      "Main effects training epoch: 248, train loss: 1.54423, val loss: 1.55957\n",
      "Main effects training epoch: 249, train loss: 1.54159, val loss: 1.55553\n",
      "Main effects training epoch: 250, train loss: 1.53820, val loss: 1.55469\n",
      "Main effects training epoch: 251, train loss: 1.53789, val loss: 1.55473\n",
      "Main effects training epoch: 252, train loss: 1.55073, val loss: 1.58220\n",
      "Main effects training epoch: 253, train loss: 1.54090, val loss: 1.55735\n",
      "Main effects training epoch: 254, train loss: 1.54473, val loss: 1.55922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 255, train loss: 1.55058, val loss: 1.57645\n",
      "Main effects training epoch: 256, train loss: 1.53881, val loss: 1.56247\n",
      "Main effects training epoch: 257, train loss: 1.54571, val loss: 1.55785\n",
      "Main effects training epoch: 258, train loss: 1.54880, val loss: 1.56303\n",
      "Main effects training epoch: 259, train loss: 1.53891, val loss: 1.56192\n",
      "Main effects training epoch: 260, train loss: 1.54032, val loss: 1.56002\n",
      "Main effects training epoch: 261, train loss: 1.53754, val loss: 1.55022\n",
      "Main effects training epoch: 262, train loss: 1.53506, val loss: 1.54877\n",
      "Main effects training epoch: 263, train loss: 1.53522, val loss: 1.55295\n",
      "Main effects training epoch: 264, train loss: 1.53542, val loss: 1.54677\n",
      "Main effects training epoch: 265, train loss: 1.53329, val loss: 1.54859\n",
      "Main effects training epoch: 266, train loss: 1.53042, val loss: 1.54803\n",
      "Main effects training epoch: 267, train loss: 1.52795, val loss: 1.55447\n",
      "Main effects training epoch: 268, train loss: 1.53731, val loss: 1.56121\n",
      "Main effects training epoch: 269, train loss: 1.54225, val loss: 1.56682\n",
      "Main effects training epoch: 270, train loss: 1.52980, val loss: 1.55467\n",
      "Main effects training epoch: 271, train loss: 1.54362, val loss: 1.55489\n",
      "Main effects training epoch: 272, train loss: 1.53671, val loss: 1.54865\n",
      "Main effects training epoch: 273, train loss: 1.53725, val loss: 1.57212\n",
      "Main effects training epoch: 274, train loss: 1.53220, val loss: 1.55576\n",
      "Main effects training epoch: 275, train loss: 1.53226, val loss: 1.56429\n",
      "Main effects training epoch: 276, train loss: 1.52349, val loss: 1.54741\n",
      "Main effects training epoch: 277, train loss: 1.52497, val loss: 1.54008\n",
      "Main effects training epoch: 278, train loss: 1.52846, val loss: 1.55558\n",
      "Main effects training epoch: 279, train loss: 1.52663, val loss: 1.54944\n",
      "Main effects training epoch: 280, train loss: 1.52724, val loss: 1.53714\n",
      "Main effects training epoch: 281, train loss: 1.52436, val loss: 1.54869\n",
      "Main effects training epoch: 282, train loss: 1.53636, val loss: 1.55245\n",
      "Main effects training epoch: 283, train loss: 1.52934, val loss: 1.54213\n",
      "Main effects training epoch: 284, train loss: 1.53141, val loss: 1.54353\n",
      "Main effects training epoch: 285, train loss: 1.52410, val loss: 1.54447\n",
      "Main effects training epoch: 286, train loss: 1.52744, val loss: 1.55292\n",
      "Main effects training epoch: 287, train loss: 1.52755, val loss: 1.55184\n",
      "Main effects training epoch: 288, train loss: 1.52768, val loss: 1.54552\n",
      "Main effects training epoch: 289, train loss: 1.52635, val loss: 1.54286\n",
      "Main effects training epoch: 290, train loss: 1.52897, val loss: 1.56055\n",
      "Main effects training epoch: 291, train loss: 1.51812, val loss: 1.53614\n",
      "Main effects training epoch: 292, train loss: 1.52195, val loss: 1.54369\n",
      "Main effects training epoch: 293, train loss: 1.51942, val loss: 1.53624\n",
      "Main effects training epoch: 294, train loss: 1.51861, val loss: 1.53598\n",
      "Main effects training epoch: 295, train loss: 1.51903, val loss: 1.54189\n",
      "Main effects training epoch: 296, train loss: 1.52854, val loss: 1.54416\n",
      "Main effects training epoch: 297, train loss: 1.52570, val loss: 1.54351\n",
      "Main effects training epoch: 298, train loss: 1.52755, val loss: 1.54282\n",
      "Main effects training epoch: 299, train loss: 1.53758, val loss: 1.54327\n",
      "Main effects training epoch: 300, train loss: 1.51867, val loss: 1.53415\n",
      "##########Stage 1: main effect training stop.##########\n",
      "2 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.53481, val loss: 1.55050\n",
      "Main effects tuning epoch: 2, train loss: 1.52471, val loss: 1.54838\n",
      "Main effects tuning epoch: 3, train loss: 1.52293, val loss: 1.52950\n",
      "Main effects tuning epoch: 4, train loss: 1.52155, val loss: 1.53726\n",
      "Main effects tuning epoch: 5, train loss: 1.52541, val loss: 1.53867\n",
      "Main effects tuning epoch: 6, train loss: 1.53383, val loss: 1.53703\n",
      "Main effects tuning epoch: 7, train loss: 1.52640, val loss: 1.53992\n",
      "Main effects tuning epoch: 8, train loss: 1.52870, val loss: 1.54368\n",
      "Main effects tuning epoch: 9, train loss: 1.52452, val loss: 1.54036\n",
      "Main effects tuning epoch: 10, train loss: 1.52806, val loss: 1.53767\n",
      "Main effects tuning epoch: 11, train loss: 1.53194, val loss: 1.53584\n",
      "Main effects tuning epoch: 12, train loss: 1.51971, val loss: 1.53144\n",
      "Main effects tuning epoch: 13, train loss: 1.51958, val loss: 1.53300\n",
      "Main effects tuning epoch: 14, train loss: 1.52161, val loss: 1.52747\n",
      "Main effects tuning epoch: 15, train loss: 1.52195, val loss: 1.53205\n",
      "Main effects tuning epoch: 16, train loss: 1.52510, val loss: 1.53624\n",
      "Main effects tuning epoch: 17, train loss: 1.52732, val loss: 1.54668\n",
      "Main effects tuning epoch: 18, train loss: 1.52211, val loss: 1.53388\n",
      "Main effects tuning epoch: 19, train loss: 1.52705, val loss: 1.54038\n",
      "Main effects tuning epoch: 20, train loss: 1.51845, val loss: 1.52340\n",
      "Main effects tuning epoch: 21, train loss: 1.53351, val loss: 1.53715\n",
      "Main effects tuning epoch: 22, train loss: 1.51714, val loss: 1.52902\n",
      "Main effects tuning epoch: 23, train loss: 1.51494, val loss: 1.53119\n",
      "Main effects tuning epoch: 24, train loss: 1.51762, val loss: 1.52620\n",
      "Main effects tuning epoch: 25, train loss: 1.51773, val loss: 1.52311\n",
      "Main effects tuning epoch: 26, train loss: 1.52905, val loss: 1.54331\n",
      "Main effects tuning epoch: 27, train loss: 1.53219, val loss: 1.55552\n",
      "Main effects tuning epoch: 28, train loss: 1.54045, val loss: 1.55997\n",
      "Main effects tuning epoch: 29, train loss: 1.54607, val loss: 1.54420\n",
      "Main effects tuning epoch: 30, train loss: 1.52175, val loss: 1.53080\n",
      "Main effects tuning epoch: 31, train loss: 1.51855, val loss: 1.54008\n",
      "Main effects tuning epoch: 32, train loss: 1.52538, val loss: 1.54003\n",
      "Main effects tuning epoch: 33, train loss: 1.52025, val loss: 1.53984\n",
      "Main effects tuning epoch: 34, train loss: 1.52753, val loss: 1.53073\n",
      "Main effects tuning epoch: 35, train loss: 1.52636, val loss: 1.53002\n",
      "Main effects tuning epoch: 36, train loss: 1.52012, val loss: 1.52921\n",
      "Main effects tuning epoch: 37, train loss: 1.51389, val loss: 1.52617\n",
      "Main effects tuning epoch: 38, train loss: 1.52565, val loss: 1.54177\n",
      "Main effects tuning epoch: 39, train loss: 1.52441, val loss: 1.54457\n",
      "Main effects tuning epoch: 40, train loss: 1.51568, val loss: 1.52703\n",
      "Main effects tuning epoch: 41, train loss: 1.51426, val loss: 1.52928\n",
      "Main effects tuning epoch: 42, train loss: 1.52126, val loss: 1.53434\n",
      "Main effects tuning epoch: 43, train loss: 1.52061, val loss: 1.53694\n",
      "Main effects tuning epoch: 44, train loss: 1.52734, val loss: 1.53738\n",
      "Main effects tuning epoch: 45, train loss: 1.53074, val loss: 1.53740\n",
      "Main effects tuning epoch: 46, train loss: 1.52459, val loss: 1.54019\n",
      "Main effects tuning epoch: 47, train loss: 1.51228, val loss: 1.52455\n",
      "Main effects tuning epoch: 48, train loss: 1.51433, val loss: 1.53108\n",
      "Main effects tuning epoch: 49, train loss: 1.51585, val loss: 1.54039\n",
      "Main effects tuning epoch: 50, train loss: 1.51581, val loss: 1.52671\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.45141, val loss: 1.45219\n",
      "Interaction training epoch: 2, train loss: 1.31160, val loss: 1.32609\n",
      "Interaction training epoch: 3, train loss: 1.09780, val loss: 1.10267\n",
      "Interaction training epoch: 4, train loss: 1.06476, val loss: 1.07504\n",
      "Interaction training epoch: 5, train loss: 1.12266, val loss: 1.13542\n",
      "Interaction training epoch: 6, train loss: 1.17844, val loss: 1.20783\n",
      "Interaction training epoch: 7, train loss: 1.20289, val loss: 1.24559\n",
      "Interaction training epoch: 8, train loss: 1.05781, val loss: 1.07324\n",
      "Interaction training epoch: 9, train loss: 0.97955, val loss: 0.99798\n",
      "Interaction training epoch: 10, train loss: 0.94567, val loss: 0.94424\n",
      "Interaction training epoch: 11, train loss: 1.00952, val loss: 0.99696\n",
      "Interaction training epoch: 12, train loss: 0.95698, val loss: 0.95299\n",
      "Interaction training epoch: 13, train loss: 0.93176, val loss: 0.92788\n",
      "Interaction training epoch: 14, train loss: 0.95809, val loss: 0.95807\n",
      "Interaction training epoch: 15, train loss: 0.95189, val loss: 0.95615\n",
      "Interaction training epoch: 16, train loss: 0.94141, val loss: 0.93794\n",
      "Interaction training epoch: 17, train loss: 0.92409, val loss: 0.91946\n",
      "Interaction training epoch: 18, train loss: 0.94871, val loss: 0.94911\n",
      "Interaction training epoch: 19, train loss: 0.92772, val loss: 0.90824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 20, train loss: 0.93323, val loss: 0.92926\n",
      "Interaction training epoch: 21, train loss: 0.93321, val loss: 0.93150\n",
      "Interaction training epoch: 22, train loss: 0.91923, val loss: 0.90164\n",
      "Interaction training epoch: 23, train loss: 0.93879, val loss: 0.93384\n",
      "Interaction training epoch: 24, train loss: 0.93287, val loss: 0.92704\n",
      "Interaction training epoch: 25, train loss: 0.93653, val loss: 0.93759\n",
      "Interaction training epoch: 26, train loss: 0.90645, val loss: 0.89872\n",
      "Interaction training epoch: 27, train loss: 0.90449, val loss: 0.89270\n",
      "Interaction training epoch: 28, train loss: 0.89819, val loss: 0.88853\n",
      "Interaction training epoch: 29, train loss: 0.90655, val loss: 0.90219\n",
      "Interaction training epoch: 30, train loss: 0.89302, val loss: 0.88773\n",
      "Interaction training epoch: 31, train loss: 0.89314, val loss: 0.88276\n",
      "Interaction training epoch: 32, train loss: 0.89979, val loss: 0.89016\n",
      "Interaction training epoch: 33, train loss: 0.90863, val loss: 0.91419\n",
      "Interaction training epoch: 34, train loss: 0.89557, val loss: 0.89020\n",
      "Interaction training epoch: 35, train loss: 0.89224, val loss: 0.88343\n",
      "Interaction training epoch: 36, train loss: 0.89150, val loss: 0.87294\n",
      "Interaction training epoch: 37, train loss: 0.87961, val loss: 0.87515\n",
      "Interaction training epoch: 38, train loss: 0.89871, val loss: 0.90510\n",
      "Interaction training epoch: 39, train loss: 0.88696, val loss: 0.86373\n",
      "Interaction training epoch: 40, train loss: 0.87309, val loss: 0.86543\n",
      "Interaction training epoch: 41, train loss: 0.87753, val loss: 0.87123\n",
      "Interaction training epoch: 42, train loss: 0.87182, val loss: 0.86333\n",
      "Interaction training epoch: 43, train loss: 0.87209, val loss: 0.86671\n",
      "Interaction training epoch: 44, train loss: 0.86904, val loss: 0.85738\n",
      "Interaction training epoch: 45, train loss: 0.86957, val loss: 0.84690\n",
      "Interaction training epoch: 46, train loss: 0.86897, val loss: 0.86551\n",
      "Interaction training epoch: 47, train loss: 0.86676, val loss: 0.84032\n",
      "Interaction training epoch: 48, train loss: 0.85870, val loss: 0.85312\n",
      "Interaction training epoch: 49, train loss: 0.86962, val loss: 0.85082\n",
      "Interaction training epoch: 50, train loss: 0.85745, val loss: 0.86283\n",
      "Interaction training epoch: 51, train loss: 0.85297, val loss: 0.85113\n",
      "Interaction training epoch: 52, train loss: 0.86686, val loss: 0.85706\n",
      "Interaction training epoch: 53, train loss: 0.85512, val loss: 0.83859\n",
      "Interaction training epoch: 54, train loss: 0.85668, val loss: 0.84134\n",
      "Interaction training epoch: 55, train loss: 0.85211, val loss: 0.84537\n",
      "Interaction training epoch: 56, train loss: 0.84982, val loss: 0.85017\n",
      "Interaction training epoch: 57, train loss: 0.84778, val loss: 0.83098\n",
      "Interaction training epoch: 58, train loss: 0.86746, val loss: 0.87023\n",
      "Interaction training epoch: 59, train loss: 0.85243, val loss: 0.85351\n",
      "Interaction training epoch: 60, train loss: 0.84479, val loss: 0.82903\n",
      "Interaction training epoch: 61, train loss: 0.84348, val loss: 0.82946\n",
      "Interaction training epoch: 62, train loss: 0.84093, val loss: 0.83788\n",
      "Interaction training epoch: 63, train loss: 0.84751, val loss: 0.82553\n",
      "Interaction training epoch: 64, train loss: 0.84367, val loss: 0.82452\n",
      "Interaction training epoch: 65, train loss: 0.85379, val loss: 0.84653\n",
      "Interaction training epoch: 66, train loss: 0.84075, val loss: 0.83307\n",
      "Interaction training epoch: 67, train loss: 0.84295, val loss: 0.81528\n",
      "Interaction training epoch: 68, train loss: 0.82998, val loss: 0.81849\n",
      "Interaction training epoch: 69, train loss: 0.84726, val loss: 0.83278\n",
      "Interaction training epoch: 70, train loss: 0.83382, val loss: 0.82825\n",
      "Interaction training epoch: 71, train loss: 0.83138, val loss: 0.81283\n",
      "Interaction training epoch: 72, train loss: 0.83331, val loss: 0.83148\n",
      "Interaction training epoch: 73, train loss: 0.83009, val loss: 0.82342\n",
      "Interaction training epoch: 74, train loss: 0.83178, val loss: 0.81128\n",
      "Interaction training epoch: 75, train loss: 0.84916, val loss: 0.83488\n",
      "Interaction training epoch: 76, train loss: 0.83314, val loss: 0.82866\n",
      "Interaction training epoch: 77, train loss: 0.82788, val loss: 0.82206\n",
      "Interaction training epoch: 78, train loss: 0.84312, val loss: 0.83737\n",
      "Interaction training epoch: 79, train loss: 0.83899, val loss: 0.82364\n",
      "Interaction training epoch: 80, train loss: 0.82403, val loss: 0.81306\n",
      "Interaction training epoch: 81, train loss: 0.82650, val loss: 0.81931\n",
      "Interaction training epoch: 82, train loss: 0.83746, val loss: 0.81734\n",
      "Interaction training epoch: 83, train loss: 0.83071, val loss: 0.81585\n",
      "Interaction training epoch: 84, train loss: 0.82785, val loss: 0.82592\n",
      "Interaction training epoch: 85, train loss: 0.82822, val loss: 0.81727\n",
      "Interaction training epoch: 86, train loss: 0.83232, val loss: 0.81527\n",
      "Interaction training epoch: 87, train loss: 0.82185, val loss: 0.81355\n",
      "Interaction training epoch: 88, train loss: 0.82841, val loss: 0.82048\n",
      "Interaction training epoch: 89, train loss: 0.83554, val loss: 0.82588\n",
      "Interaction training epoch: 90, train loss: 0.82269, val loss: 0.80286\n",
      "Interaction training epoch: 91, train loss: 0.83046, val loss: 0.81208\n",
      "Interaction training epoch: 92, train loss: 0.83432, val loss: 0.83220\n",
      "Interaction training epoch: 93, train loss: 0.82703, val loss: 0.81992\n",
      "Interaction training epoch: 94, train loss: 0.86094, val loss: 0.84516\n",
      "Interaction training epoch: 95, train loss: 0.82283, val loss: 0.81312\n",
      "Interaction training epoch: 96, train loss: 0.82020, val loss: 0.80149\n",
      "Interaction training epoch: 97, train loss: 0.83131, val loss: 0.81883\n",
      "Interaction training epoch: 98, train loss: 0.82474, val loss: 0.81255\n",
      "Interaction training epoch: 99, train loss: 0.82349, val loss: 0.81329\n",
      "Interaction training epoch: 100, train loss: 0.82958, val loss: 0.81361\n",
      "Interaction training epoch: 101, train loss: 0.82324, val loss: 0.81064\n",
      "Interaction training epoch: 102, train loss: 0.82732, val loss: 0.82063\n",
      "Interaction training epoch: 103, train loss: 0.83052, val loss: 0.81948\n",
      "Interaction training epoch: 104, train loss: 0.83180, val loss: 0.81475\n",
      "Interaction training epoch: 105, train loss: 0.83078, val loss: 0.82173\n",
      "Interaction training epoch: 106, train loss: 0.82378, val loss: 0.80888\n",
      "Interaction training epoch: 107, train loss: 0.83751, val loss: 0.82375\n",
      "Interaction training epoch: 108, train loss: 0.83331, val loss: 0.82216\n",
      "Interaction training epoch: 109, train loss: 0.81870, val loss: 0.80046\n",
      "Interaction training epoch: 110, train loss: 0.82328, val loss: 0.81198\n",
      "Interaction training epoch: 111, train loss: 0.82252, val loss: 0.81136\n",
      "Interaction training epoch: 112, train loss: 0.81897, val loss: 0.80683\n",
      "Interaction training epoch: 113, train loss: 0.81697, val loss: 0.81052\n",
      "Interaction training epoch: 114, train loss: 0.82271, val loss: 0.81313\n",
      "Interaction training epoch: 115, train loss: 0.82108, val loss: 0.79952\n",
      "Interaction training epoch: 116, train loss: 0.82647, val loss: 0.82788\n",
      "Interaction training epoch: 117, train loss: 0.81582, val loss: 0.80068\n",
      "Interaction training epoch: 118, train loss: 0.82388, val loss: 0.81626\n",
      "Interaction training epoch: 119, train loss: 0.81580, val loss: 0.80174\n",
      "Interaction training epoch: 120, train loss: 0.81273, val loss: 0.80681\n",
      "Interaction training epoch: 121, train loss: 0.83414, val loss: 0.81703\n",
      "Interaction training epoch: 122, train loss: 0.81929, val loss: 0.80142\n",
      "Interaction training epoch: 123, train loss: 0.82425, val loss: 0.81090\n",
      "Interaction training epoch: 124, train loss: 0.82094, val loss: 0.80570\n",
      "Interaction training epoch: 125, train loss: 0.81384, val loss: 0.80117\n",
      "Interaction training epoch: 126, train loss: 0.81198, val loss: 0.80186\n",
      "Interaction training epoch: 127, train loss: 0.81768, val loss: 0.81201\n",
      "Interaction training epoch: 128, train loss: 0.81741, val loss: 0.81532\n",
      "Interaction training epoch: 129, train loss: 0.81991, val loss: 0.80375\n",
      "Interaction training epoch: 130, train loss: 0.81887, val loss: 0.80751\n",
      "Interaction training epoch: 131, train loss: 0.82902, val loss: 0.81909\n",
      "Interaction training epoch: 132, train loss: 0.81909, val loss: 0.80178\n",
      "Interaction training epoch: 133, train loss: 0.82695, val loss: 0.81829\n",
      "Interaction training epoch: 134, train loss: 0.81953, val loss: 0.80672\n",
      "Interaction training epoch: 135, train loss: 0.81840, val loss: 0.80249\n",
      "Interaction training epoch: 136, train loss: 0.81460, val loss: 0.79873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 137, train loss: 0.81774, val loss: 0.81087\n",
      "Interaction training epoch: 138, train loss: 0.81677, val loss: 0.79633\n",
      "Interaction training epoch: 139, train loss: 0.80936, val loss: 0.79757\n",
      "Interaction training epoch: 140, train loss: 0.80845, val loss: 0.79681\n",
      "Interaction training epoch: 141, train loss: 0.81222, val loss: 0.79992\n",
      "Interaction training epoch: 142, train loss: 0.81246, val loss: 0.79738\n",
      "Interaction training epoch: 143, train loss: 0.81658, val loss: 0.80904\n",
      "Interaction training epoch: 144, train loss: 0.81359, val loss: 0.79659\n",
      "Interaction training epoch: 145, train loss: 0.81253, val loss: 0.80179\n",
      "Interaction training epoch: 146, train loss: 0.81502, val loss: 0.80826\n",
      "Interaction training epoch: 147, train loss: 0.81112, val loss: 0.79590\n",
      "Interaction training epoch: 148, train loss: 0.80940, val loss: 0.79722\n",
      "Interaction training epoch: 149, train loss: 0.81328, val loss: 0.79498\n",
      "Interaction training epoch: 150, train loss: 0.81173, val loss: 0.79891\n",
      "Interaction training epoch: 151, train loss: 0.81864, val loss: 0.80851\n",
      "Interaction training epoch: 152, train loss: 0.82010, val loss: 0.80565\n",
      "Interaction training epoch: 153, train loss: 0.82134, val loss: 0.80868\n",
      "Interaction training epoch: 154, train loss: 0.81783, val loss: 0.80552\n",
      "Interaction training epoch: 155, train loss: 0.81400, val loss: 0.79512\n",
      "Interaction training epoch: 156, train loss: 0.81786, val loss: 0.81641\n",
      "Interaction training epoch: 157, train loss: 0.82022, val loss: 0.79431\n",
      "Interaction training epoch: 158, train loss: 0.81393, val loss: 0.81027\n",
      "Interaction training epoch: 159, train loss: 0.81718, val loss: 0.80263\n",
      "Interaction training epoch: 160, train loss: 0.81424, val loss: 0.79906\n",
      "Interaction training epoch: 161, train loss: 0.81309, val loss: 0.79909\n",
      "Interaction training epoch: 162, train loss: 0.80956, val loss: 0.80223\n",
      "Interaction training epoch: 163, train loss: 0.81324, val loss: 0.79812\n",
      "Interaction training epoch: 164, train loss: 0.80748, val loss: 0.80215\n",
      "Interaction training epoch: 165, train loss: 0.80806, val loss: 0.79610\n",
      "Interaction training epoch: 166, train loss: 0.82376, val loss: 0.81065\n",
      "Interaction training epoch: 167, train loss: 0.81576, val loss: 0.80174\n",
      "Interaction training epoch: 168, train loss: 0.81683, val loss: 0.80407\n",
      "Interaction training epoch: 169, train loss: 0.80979, val loss: 0.79347\n",
      "Interaction training epoch: 170, train loss: 0.81809, val loss: 0.79703\n",
      "Interaction training epoch: 171, train loss: 0.81329, val loss: 0.80848\n",
      "Interaction training epoch: 172, train loss: 0.81248, val loss: 0.79254\n",
      "Interaction training epoch: 173, train loss: 0.81440, val loss: 0.81131\n",
      "Interaction training epoch: 174, train loss: 0.80506, val loss: 0.78492\n",
      "Interaction training epoch: 175, train loss: 0.81101, val loss: 0.80405\n",
      "Interaction training epoch: 176, train loss: 0.81579, val loss: 0.80296\n",
      "Interaction training epoch: 177, train loss: 0.80958, val loss: 0.79801\n",
      "Interaction training epoch: 178, train loss: 0.80783, val loss: 0.79355\n",
      "Interaction training epoch: 179, train loss: 0.81185, val loss: 0.79343\n",
      "Interaction training epoch: 180, train loss: 0.80881, val loss: 0.80080\n",
      "Interaction training epoch: 181, train loss: 0.81627, val loss: 0.80407\n",
      "Interaction training epoch: 182, train loss: 0.80921, val loss: 0.79521\n",
      "Interaction training epoch: 183, train loss: 0.81030, val loss: 0.79976\n",
      "Interaction training epoch: 184, train loss: 0.81422, val loss: 0.80193\n",
      "Interaction training epoch: 185, train loss: 0.81334, val loss: 0.80373\n",
      "Interaction training epoch: 186, train loss: 0.81193, val loss: 0.79494\n",
      "Interaction training epoch: 187, train loss: 0.81156, val loss: 0.80485\n",
      "Interaction training epoch: 188, train loss: 0.81384, val loss: 0.79660\n",
      "Interaction training epoch: 189, train loss: 0.81544, val loss: 0.81201\n",
      "Interaction training epoch: 190, train loss: 0.81686, val loss: 0.79641\n",
      "Interaction training epoch: 191, train loss: 0.81653, val loss: 0.79975\n",
      "Interaction training epoch: 192, train loss: 0.81419, val loss: 0.79653\n",
      "Interaction training epoch: 193, train loss: 0.81648, val loss: 0.80307\n",
      "Interaction training epoch: 194, train loss: 0.80141, val loss: 0.78890\n",
      "Interaction training epoch: 195, train loss: 0.80857, val loss: 0.78877\n",
      "Interaction training epoch: 196, train loss: 0.81204, val loss: 0.80613\n",
      "Interaction training epoch: 197, train loss: 0.81391, val loss: 0.79227\n",
      "Interaction training epoch: 198, train loss: 0.80498, val loss: 0.79973\n",
      "Interaction training epoch: 199, train loss: 0.81614, val loss: 0.79972\n",
      "Interaction training epoch: 200, train loss: 0.80621, val loss: 0.79058\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########No main interaction is pruned, the tuning step is skipped.\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 32.80030536651611\n",
      "After the gam stage, training error is 0.80621 , validation error is 0.79058\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 19.428421\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.670904 validation MAE=0.754125,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.626331 validation MAE=0.738293,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 3: observed MAE=0.590120 validation MAE=0.723939,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.559495 validation MAE=0.710245,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.532129 validation MAE=0.698012,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.509458 validation MAE=0.687221,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.488988 validation MAE=0.677350,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.472490 validation MAE=0.668361,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.457321 validation MAE=0.659550,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.444079 validation MAE=0.651568,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.433564 validation MAE=0.644304,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.422431 validation MAE=0.637263,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.411648 validation MAE=0.630453,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.404761 validation MAE=0.624570,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.395958 validation MAE=0.617220,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.389030 validation MAE=0.612379,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.381109 validation MAE=0.606395,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.373658 validation MAE=0.601131,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.367572 validation MAE=0.596781,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.360839 validation MAE=0.592282,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.354028 validation MAE=0.588576,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.348136 validation MAE=0.584008,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.342492 validation MAE=0.580360,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.336991 validation MAE=0.577423,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.331814 validation MAE=0.574012,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.328033 validation MAE=0.571015,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.322936 validation MAE=0.567749,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.318958 validation MAE=0.565558,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.315320 validation MAE=0.562468,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.310723 validation MAE=0.560090,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.305823 validation MAE=0.558298,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.302588 validation MAE=0.556356,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.299265 validation MAE=0.554351,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.296345 validation MAE=0.552601,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.292591 validation MAE=0.550624,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.289954 validation MAE=0.548717,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.287376 validation MAE=0.547297,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.284561 validation MAE=0.545401,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.282693 validation MAE=0.544491,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.279516 validation MAE=0.543067,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.276359 validation MAE=0.541630,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.274153 validation MAE=0.540119,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.272010 validation MAE=0.538891,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.268224 validation MAE=0.537842,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.266774 validation MAE=0.536914,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.264670 validation MAE=0.534670,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.263219 validation MAE=0.534455,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.261242 validation MAE=0.532955,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.260053 validation MAE=0.531595,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.257433 validation MAE=0.530544,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.255826 validation MAE=0.529268,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.254765 validation MAE=0.527916,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.251912 validation MAE=0.527676,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.250917 validation MAE=0.526464,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.248384 validation MAE=0.525918,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.246834 validation MAE=0.524580,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.245782 validation MAE=0.523555,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.244561 validation MAE=0.522763,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.242057 validation MAE=0.521722,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.241167 validation MAE=0.521017,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.239571 validation MAE=0.520163,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.238815 validation MAE=0.518860,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.237585 validation MAE=0.517902,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.236675 validation MAE=0.517196,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.234685 validation MAE=0.515542,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.234200 validation MAE=0.515282,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.232020 validation MAE=0.513755,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.232009 validation MAE=0.512761,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.229956 validation MAE=0.512272,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.229253 validation MAE=0.511548,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.227575 validation MAE=0.509813,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.226483 validation MAE=0.509198,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.225078 validation MAE=0.508019,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.224509 validation MAE=0.507510,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.223204 validation MAE=0.506823,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.222157 validation MAE=0.505384,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.221551 validation MAE=0.504404,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.220980 validation MAE=0.503788,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.219028 validation MAE=0.502993,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.217401 validation MAE=0.501797,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.218027 validation MAE=0.500939,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.217015 validation MAE=0.500333,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.216598 validation MAE=0.500220,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.215620 validation MAE=0.499161,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.214614 validation MAE=0.498106,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.213340 validation MAE=0.497219,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.211627 validation MAE=0.496190,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.211436 validation MAE=0.494536,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.211060 validation MAE=0.494831,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.210488 validation MAE=0.493287,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.209699 validation MAE=0.492408,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.209287 validation MAE=0.491840,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.208035 validation MAE=0.491045,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.207145 validation MAE=0.490119,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.206522 validation MAE=0.489987,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.205714 validation MAE=0.489186,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.204106 validation MAE=0.488272,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.203685 validation MAE=0.486981,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.201860 validation MAE=0.486192,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.201958 validation MAE=0.485533,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.202023 validation MAE=0.485347,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.201131 validation MAE=0.484481,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.200622 validation MAE=0.483609,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.199776 validation MAE=0.482761,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.199436 validation MAE=0.481638,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.198805 validation MAE=0.480916,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.197597 validation MAE=0.480534,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.196419 validation MAE=0.479503,rank=5\n",
      "[SoftImpute] Iter 109: observed MAE=0.196382 validation MAE=0.478809,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.195454 validation MAE=0.478379,rank=5\n",
      "[SoftImpute] Iter 111: observed MAE=0.194085 validation MAE=0.477362,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.194573 validation MAE=0.476770,rank=5\n",
      "[SoftImpute] Iter 113: observed MAE=0.193295 validation MAE=0.476001,rank=5\n",
      "[SoftImpute] Iter 114: observed MAE=0.192909 validation MAE=0.475593,rank=5\n",
      "[SoftImpute] Iter 115: observed MAE=0.193095 validation MAE=0.475221,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.192068 validation MAE=0.474118,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.191110 validation MAE=0.473656,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.190891 validation MAE=0.473138,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.190090 validation MAE=0.473054,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 120: observed MAE=0.189328 validation MAE=0.471845,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.189142 validation MAE=0.471208,rank=5\n",
      "[SoftImpute] Iter 122: observed MAE=0.188598 validation MAE=0.470755,rank=5\n",
      "[SoftImpute] Iter 123: observed MAE=0.188369 validation MAE=0.469373,rank=5\n",
      "[SoftImpute] Iter 124: observed MAE=0.186686 validation MAE=0.468550,rank=5\n",
      "[SoftImpute] Iter 125: observed MAE=0.187213 validation MAE=0.468160,rank=5\n",
      "[SoftImpute] Iter 126: observed MAE=0.186877 validation MAE=0.467580,rank=5\n",
      "[SoftImpute] Iter 127: observed MAE=0.185753 validation MAE=0.467068,rank=5\n",
      "[SoftImpute] Iter 128: observed MAE=0.186346 validation MAE=0.467020,rank=5\n",
      "[SoftImpute] Iter 129: observed MAE=0.185053 validation MAE=0.466224,rank=5\n",
      "[SoftImpute] Iter 130: observed MAE=0.184154 validation MAE=0.465629,rank=5\n",
      "[SoftImpute] Iter 131: observed MAE=0.183578 validation MAE=0.465059,rank=5\n",
      "[SoftImpute] Iter 132: observed MAE=0.183126 validation MAE=0.464431,rank=5\n",
      "[SoftImpute] Iter 133: observed MAE=0.183663 validation MAE=0.464048,rank=5\n",
      "[SoftImpute] Iter 134: observed MAE=0.182407 validation MAE=0.463436,rank=5\n",
      "[SoftImpute] Iter 135: observed MAE=0.181260 validation MAE=0.462439,rank=5\n",
      "[SoftImpute] Iter 136: observed MAE=0.181893 validation MAE=0.461974,rank=5\n",
      "[SoftImpute] Iter 137: observed MAE=0.181536 validation MAE=0.462079,rank=5\n",
      "[SoftImpute] Iter 138: observed MAE=0.180877 validation MAE=0.461273,rank=5\n",
      "[SoftImpute] Iter 139: observed MAE=0.179415 validation MAE=0.460851,rank=5\n",
      "[SoftImpute] Iter 140: observed MAE=0.180400 validation MAE=0.460268,rank=5\n",
      "[SoftImpute] Iter 141: observed MAE=0.180375 validation MAE=0.459850,rank=5\n",
      "[SoftImpute] Iter 142: observed MAE=0.179882 validation MAE=0.459233,rank=5\n",
      "[SoftImpute] Iter 143: observed MAE=0.179548 validation MAE=0.459298,rank=5\n",
      "[SoftImpute] Iter 144: observed MAE=0.178709 validation MAE=0.458131,rank=5\n",
      "[SoftImpute] Iter 145: observed MAE=0.177974 validation MAE=0.457864,rank=5\n",
      "[SoftImpute] Iter 146: observed MAE=0.177387 validation MAE=0.457400,rank=5\n",
      "[SoftImpute] Iter 147: observed MAE=0.177589 validation MAE=0.456372,rank=5\n",
      "[SoftImpute] Iter 148: observed MAE=0.178180 validation MAE=0.456614,rank=5\n",
      "[SoftImpute] Iter 149: observed MAE=0.177621 validation MAE=0.456397,rank=5\n",
      "[SoftImpute] Iter 150: observed MAE=0.176758 validation MAE=0.455553,rank=5\n",
      "[SoftImpute] Iter 151: observed MAE=0.175753 validation MAE=0.455185,rank=5\n",
      "[SoftImpute] Iter 152: observed MAE=0.175693 validation MAE=0.454602,rank=5\n",
      "[SoftImpute] Iter 153: observed MAE=0.175522 validation MAE=0.454661,rank=5\n",
      "[SoftImpute] Iter 154: observed MAE=0.176339 validation MAE=0.454198,rank=5\n",
      "[SoftImpute] Iter 155: observed MAE=0.174966 validation MAE=0.454036,rank=5\n",
      "[SoftImpute] Iter 156: observed MAE=0.174985 validation MAE=0.452835,rank=5\n",
      "[SoftImpute] Iter 157: observed MAE=0.174350 validation MAE=0.452244,rank=5\n",
      "[SoftImpute] Iter 158: observed MAE=0.175185 validation MAE=0.452223,rank=5\n",
      "[SoftImpute] Iter 159: observed MAE=0.174863 validation MAE=0.451403,rank=5\n",
      "[SoftImpute] Iter 160: observed MAE=0.174085 validation MAE=0.451013,rank=5\n",
      "[SoftImpute] Iter 161: observed MAE=0.175078 validation MAE=0.450775,rank=5\n",
      "[SoftImpute] Iter 162: observed MAE=0.174152 validation MAE=0.450414,rank=5\n",
      "[SoftImpute] Iter 163: observed MAE=0.173140 validation MAE=0.450047,rank=5\n",
      "[SoftImpute] Iter 164: observed MAE=0.172922 validation MAE=0.449675,rank=5\n",
      "[SoftImpute] Iter 165: observed MAE=0.172308 validation MAE=0.448873,rank=5\n",
      "[SoftImpute] Iter 166: observed MAE=0.171794 validation MAE=0.448685,rank=5\n",
      "[SoftImpute] Iter 167: observed MAE=0.170830 validation MAE=0.448168,rank=5\n",
      "[SoftImpute] Iter 168: observed MAE=0.171067 validation MAE=0.447533,rank=5\n",
      "[SoftImpute] Iter 169: observed MAE=0.170099 validation MAE=0.447239,rank=5\n",
      "[SoftImpute] Iter 170: observed MAE=0.171289 validation MAE=0.447117,rank=5\n",
      "[SoftImpute] Iter 171: observed MAE=0.170526 validation MAE=0.447248,rank=5\n",
      "[SoftImpute] Iter 172: observed MAE=0.170973 validation MAE=0.446586,rank=5\n",
      "[SoftImpute] Iter 173: observed MAE=0.169611 validation MAE=0.446666,rank=5\n",
      "[SoftImpute] Iter 174: observed MAE=0.170255 validation MAE=0.446109,rank=5\n",
      "[SoftImpute] Iter 175: observed MAE=0.169101 validation MAE=0.446055,rank=5\n",
      "[SoftImpute] Iter 176: observed MAE=0.168857 validation MAE=0.445180,rank=5\n",
      "[SoftImpute] Iter 177: observed MAE=0.168412 validation MAE=0.444373,rank=5\n",
      "[SoftImpute] Iter 178: observed MAE=0.168545 validation MAE=0.444281,rank=5\n",
      "[SoftImpute] Iter 179: observed MAE=0.167767 validation MAE=0.444228,rank=5\n",
      "[SoftImpute] Iter 180: observed MAE=0.168763 validation MAE=0.443900,rank=5\n",
      "[SoftImpute] Iter 181: observed MAE=0.168110 validation MAE=0.443140,rank=5\n",
      "[SoftImpute] Iter 182: observed MAE=0.168872 validation MAE=0.443435,rank=5\n",
      "[SoftImpute] Iter 183: observed MAE=0.167828 validation MAE=0.442818,rank=5\n",
      "[SoftImpute] Iter 184: observed MAE=0.167663 validation MAE=0.442474,rank=5\n",
      "[SoftImpute] Iter 185: observed MAE=0.168029 validation MAE=0.442180,rank=5\n",
      "[SoftImpute] Iter 186: observed MAE=0.167922 validation MAE=0.442333,rank=5\n",
      "[SoftImpute] Iter 187: observed MAE=0.166838 validation MAE=0.442259,rank=5\n",
      "[SoftImpute] Iter 188: observed MAE=0.167608 validation MAE=0.441381,rank=5\n",
      "[SoftImpute] Iter 189: observed MAE=0.166591 validation MAE=0.440751,rank=5\n",
      "[SoftImpute] Iter 190: observed MAE=0.166468 validation MAE=0.441087,rank=5\n",
      "[SoftImpute] Iter 191: observed MAE=0.166416 validation MAE=0.440731,rank=5\n",
      "[SoftImpute] Iter 192: observed MAE=0.165855 validation MAE=0.440317,rank=5\n",
      "[SoftImpute] Iter 193: observed MAE=0.165497 validation MAE=0.439889,rank=5\n",
      "[SoftImpute] Iter 194: observed MAE=0.165356 validation MAE=0.439877,rank=5\n",
      "[SoftImpute] Iter 195: observed MAE=0.165175 validation MAE=0.440120,rank=5\n",
      "[SoftImpute] Iter 196: observed MAE=0.165104 validation MAE=0.439410,rank=5\n",
      "[SoftImpute] Iter 197: observed MAE=0.164732 validation MAE=0.439002,rank=5\n",
      "[SoftImpute] Iter 198: observed MAE=0.164940 validation MAE=0.438720,rank=5\n",
      "[SoftImpute] Iter 199: observed MAE=0.164154 validation MAE=0.438430,rank=5\n",
      "[SoftImpute] Iter 200: observed MAE=0.164171 validation MAE=0.438203,rank=5\n",
      "[SoftImpute] Stopped after iteration 200 for lambda=0.388568\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 6.0192883014678955\n",
      "After the matrix factor stage, training error is 0.16417, validation error is 0.43820\n",
      "6\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.08862, val loss: 4.09872\n",
      "Main effects training epoch: 2, train loss: 3.93830, val loss: 3.94426\n",
      "Main effects training epoch: 3, train loss: 3.72340, val loss: 3.71152\n",
      "Main effects training epoch: 4, train loss: 3.49652, val loss: 3.46520\n",
      "Main effects training epoch: 5, train loss: 3.34583, val loss: 3.28527\n",
      "Main effects training epoch: 6, train loss: 3.31419, val loss: 3.22814\n",
      "Main effects training epoch: 7, train loss: 3.25417, val loss: 3.17763\n",
      "Main effects training epoch: 8, train loss: 3.23626, val loss: 3.17607\n",
      "Main effects training epoch: 9, train loss: 3.27063, val loss: 3.21942\n",
      "Main effects training epoch: 10, train loss: 3.20663, val loss: 3.15372\n",
      "Main effects training epoch: 11, train loss: 3.07700, val loss: 3.02360\n",
      "Main effects training epoch: 12, train loss: 3.05089, val loss: 3.00434\n",
      "Main effects training epoch: 13, train loss: 2.98840, val loss: 2.93753\n",
      "Main effects training epoch: 14, train loss: 2.94497, val loss: 2.90851\n",
      "Main effects training epoch: 15, train loss: 2.90640, val loss: 2.86633\n",
      "Main effects training epoch: 16, train loss: 2.82690, val loss: 2.78570\n",
      "Main effects training epoch: 17, train loss: 2.75161, val loss: 2.71663\n",
      "Main effects training epoch: 18, train loss: 2.70700, val loss: 2.67401\n",
      "Main effects training epoch: 19, train loss: 2.63140, val loss: 2.58746\n",
      "Main effects training epoch: 20, train loss: 2.59203, val loss: 2.54076\n",
      "Main effects training epoch: 21, train loss: 2.55353, val loss: 2.49889\n",
      "Main effects training epoch: 22, train loss: 2.49049, val loss: 2.43673\n",
      "Main effects training epoch: 23, train loss: 2.46863, val loss: 2.41987\n",
      "Main effects training epoch: 24, train loss: 2.41018, val loss: 2.36092\n",
      "Main effects training epoch: 25, train loss: 2.37130, val loss: 2.32453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 26, train loss: 2.34161, val loss: 2.29050\n",
      "Main effects training epoch: 27, train loss: 2.31291, val loss: 2.26926\n",
      "Main effects training epoch: 28, train loss: 2.29024, val loss: 2.24338\n",
      "Main effects training epoch: 29, train loss: 2.24765, val loss: 2.20643\n",
      "Main effects training epoch: 30, train loss: 2.21094, val loss: 2.17425\n",
      "Main effects training epoch: 31, train loss: 2.18531, val loss: 2.15077\n",
      "Main effects training epoch: 32, train loss: 2.18436, val loss: 2.14876\n",
      "Main effects training epoch: 33, train loss: 2.13348, val loss: 2.10863\n",
      "Main effects training epoch: 34, train loss: 2.11292, val loss: 2.08499\n",
      "Main effects training epoch: 35, train loss: 2.10354, val loss: 2.07843\n",
      "Main effects training epoch: 36, train loss: 2.07789, val loss: 2.05629\n",
      "Main effects training epoch: 37, train loss: 2.04334, val loss: 2.02824\n",
      "Main effects training epoch: 38, train loss: 2.04582, val loss: 2.01963\n",
      "Main effects training epoch: 39, train loss: 2.01611, val loss: 2.00522\n",
      "Main effects training epoch: 40, train loss: 1.97647, val loss: 1.96589\n",
      "Main effects training epoch: 41, train loss: 1.99810, val loss: 1.98156\n",
      "Main effects training epoch: 42, train loss: 1.96466, val loss: 1.95565\n",
      "Main effects training epoch: 43, train loss: 1.93944, val loss: 1.93323\n",
      "Main effects training epoch: 44, train loss: 1.94460, val loss: 1.93523\n",
      "Main effects training epoch: 45, train loss: 1.91990, val loss: 1.91121\n",
      "Main effects training epoch: 46, train loss: 1.90302, val loss: 1.89512\n",
      "Main effects training epoch: 47, train loss: 1.89083, val loss: 1.88724\n",
      "Main effects training epoch: 48, train loss: 1.88102, val loss: 1.87690\n",
      "Main effects training epoch: 49, train loss: 1.84692, val loss: 1.84767\n",
      "Main effects training epoch: 50, train loss: 1.87397, val loss: 1.86755\n",
      "Main effects training epoch: 51, train loss: 1.83573, val loss: 1.83681\n",
      "Main effects training epoch: 52, train loss: 1.84165, val loss: 1.84288\n",
      "Main effects training epoch: 53, train loss: 1.83114, val loss: 1.83266\n",
      "Main effects training epoch: 54, train loss: 1.80935, val loss: 1.81030\n",
      "Main effects training epoch: 55, train loss: 1.82392, val loss: 1.82675\n",
      "Main effects training epoch: 56, train loss: 1.80020, val loss: 1.80258\n",
      "Main effects training epoch: 57, train loss: 1.79380, val loss: 1.79869\n",
      "Main effects training epoch: 58, train loss: 1.79725, val loss: 1.80425\n",
      "Main effects training epoch: 59, train loss: 1.78142, val loss: 1.78631\n",
      "Main effects training epoch: 60, train loss: 1.77614, val loss: 1.78739\n",
      "Main effects training epoch: 61, train loss: 1.77746, val loss: 1.78090\n",
      "Main effects training epoch: 62, train loss: 1.76492, val loss: 1.77648\n",
      "Main effects training epoch: 63, train loss: 1.76548, val loss: 1.77359\n",
      "Main effects training epoch: 64, train loss: 1.76089, val loss: 1.77108\n",
      "Main effects training epoch: 65, train loss: 1.74885, val loss: 1.76299\n",
      "Main effects training epoch: 66, train loss: 1.75481, val loss: 1.76272\n",
      "Main effects training epoch: 67, train loss: 1.74648, val loss: 1.76158\n",
      "Main effects training epoch: 68, train loss: 1.73685, val loss: 1.75088\n",
      "Main effects training epoch: 69, train loss: 1.73796, val loss: 1.75574\n",
      "Main effects training epoch: 70, train loss: 1.74009, val loss: 1.74946\n",
      "Main effects training epoch: 71, train loss: 1.73552, val loss: 1.75142\n",
      "Main effects training epoch: 72, train loss: 1.73451, val loss: 1.74718\n",
      "Main effects training epoch: 73, train loss: 1.73104, val loss: 1.74625\n",
      "Main effects training epoch: 74, train loss: 1.72296, val loss: 1.74062\n",
      "Main effects training epoch: 75, train loss: 1.72088, val loss: 1.73631\n",
      "Main effects training epoch: 76, train loss: 1.71965, val loss: 1.73636\n",
      "Main effects training epoch: 77, train loss: 1.72141, val loss: 1.73983\n",
      "Main effects training epoch: 78, train loss: 1.72256, val loss: 1.73733\n",
      "Main effects training epoch: 79, train loss: 1.71709, val loss: 1.73788\n",
      "Main effects training epoch: 80, train loss: 1.71167, val loss: 1.72439\n",
      "Main effects training epoch: 81, train loss: 1.71180, val loss: 1.73799\n",
      "Main effects training epoch: 82, train loss: 1.70660, val loss: 1.72693\n",
      "Main effects training epoch: 83, train loss: 1.70451, val loss: 1.72655\n",
      "Main effects training epoch: 84, train loss: 1.70510, val loss: 1.72664\n",
      "Main effects training epoch: 85, train loss: 1.70103, val loss: 1.72296\n",
      "Main effects training epoch: 86, train loss: 1.70126, val loss: 1.72282\n",
      "Main effects training epoch: 87, train loss: 1.70188, val loss: 1.72313\n",
      "Main effects training epoch: 88, train loss: 1.69899, val loss: 1.72017\n",
      "Main effects training epoch: 89, train loss: 1.69496, val loss: 1.72231\n",
      "Main effects training epoch: 90, train loss: 1.69441, val loss: 1.72127\n",
      "Main effects training epoch: 91, train loss: 1.68828, val loss: 1.71207\n",
      "Main effects training epoch: 92, train loss: 1.68453, val loss: 1.70857\n",
      "Main effects training epoch: 93, train loss: 1.68446, val loss: 1.70846\n",
      "Main effects training epoch: 94, train loss: 1.68928, val loss: 1.71700\n",
      "Main effects training epoch: 95, train loss: 1.68549, val loss: 1.71777\n",
      "Main effects training epoch: 96, train loss: 1.67577, val loss: 1.70579\n",
      "Main effects training epoch: 97, train loss: 1.66932, val loss: 1.70180\n",
      "Main effects training epoch: 98, train loss: 1.66400, val loss: 1.70061\n",
      "Main effects training epoch: 99, train loss: 1.65845, val loss: 1.69489\n",
      "Main effects training epoch: 100, train loss: 1.65070, val loss: 1.69658\n",
      "Main effects training epoch: 101, train loss: 1.64383, val loss: 1.68706\n",
      "Main effects training epoch: 102, train loss: 1.63399, val loss: 1.68585\n",
      "Main effects training epoch: 103, train loss: 1.63658, val loss: 1.68638\n",
      "Main effects training epoch: 104, train loss: 1.62542, val loss: 1.68952\n",
      "Main effects training epoch: 105, train loss: 1.62392, val loss: 1.68523\n",
      "Main effects training epoch: 106, train loss: 1.62413, val loss: 1.69161\n",
      "Main effects training epoch: 107, train loss: 1.61617, val loss: 1.68710\n",
      "Main effects training epoch: 108, train loss: 1.61689, val loss: 1.69146\n",
      "Main effects training epoch: 109, train loss: 1.61560, val loss: 1.68931\n",
      "Main effects training epoch: 110, train loss: 1.61640, val loss: 1.69346\n",
      "Main effects training epoch: 111, train loss: 1.60825, val loss: 1.68913\n",
      "Main effects training epoch: 112, train loss: 1.61000, val loss: 1.68442\n",
      "Main effects training epoch: 113, train loss: 1.61276, val loss: 1.70090\n",
      "Main effects training epoch: 114, train loss: 1.61159, val loss: 1.68847\n",
      "Main effects training epoch: 115, train loss: 1.60795, val loss: 1.68946\n",
      "Main effects training epoch: 116, train loss: 1.60070, val loss: 1.68335\n",
      "Main effects training epoch: 117, train loss: 1.60079, val loss: 1.68881\n",
      "Main effects training epoch: 118, train loss: 1.59938, val loss: 1.68292\n",
      "Main effects training epoch: 119, train loss: 1.60039, val loss: 1.68554\n",
      "Main effects training epoch: 120, train loss: 1.59744, val loss: 1.68273\n",
      "Main effects training epoch: 121, train loss: 1.59533, val loss: 1.68605\n",
      "Main effects training epoch: 122, train loss: 1.60072, val loss: 1.68166\n",
      "Main effects training epoch: 123, train loss: 1.60889, val loss: 1.69841\n",
      "Main effects training epoch: 124, train loss: 1.59223, val loss: 1.67784\n",
      "Main effects training epoch: 125, train loss: 1.59357, val loss: 1.68152\n",
      "Main effects training epoch: 126, train loss: 1.59967, val loss: 1.69180\n",
      "Main effects training epoch: 127, train loss: 1.59687, val loss: 1.68319\n",
      "Main effects training epoch: 128, train loss: 1.58912, val loss: 1.67912\n",
      "Main effects training epoch: 129, train loss: 1.59574, val loss: 1.69494\n",
      "Main effects training epoch: 130, train loss: 1.59015, val loss: 1.67921\n",
      "Main effects training epoch: 131, train loss: 1.59183, val loss: 1.67944\n",
      "Main effects training epoch: 132, train loss: 1.58590, val loss: 1.67838\n",
      "Main effects training epoch: 133, train loss: 1.57874, val loss: 1.66446\n",
      "Main effects training epoch: 134, train loss: 1.58154, val loss: 1.66945\n",
      "Main effects training epoch: 135, train loss: 1.57945, val loss: 1.67009\n",
      "Main effects training epoch: 136, train loss: 1.57525, val loss: 1.66686\n",
      "Main effects training epoch: 137, train loss: 1.57434, val loss: 1.66545\n",
      "Main effects training epoch: 138, train loss: 1.56986, val loss: 1.66545\n",
      "Main effects training epoch: 139, train loss: 1.56860, val loss: 1.66487\n",
      "Main effects training epoch: 140, train loss: 1.56690, val loss: 1.66061\n",
      "Main effects training epoch: 141, train loss: 1.56702, val loss: 1.65627\n",
      "Main effects training epoch: 142, train loss: 1.56859, val loss: 1.66472\n",
      "Main effects training epoch: 143, train loss: 1.56344, val loss: 1.66062\n",
      "Main effects training epoch: 144, train loss: 1.56006, val loss: 1.65700\n",
      "Main effects training epoch: 145, train loss: 1.57217, val loss: 1.66069\n",
      "Main effects training epoch: 146, train loss: 1.56629, val loss: 1.66288\n",
      "Main effects training epoch: 147, train loss: 1.55883, val loss: 1.65705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 148, train loss: 1.55668, val loss: 1.65568\n",
      "Main effects training epoch: 149, train loss: 1.55415, val loss: 1.64914\n",
      "Main effects training epoch: 150, train loss: 1.55288, val loss: 1.64954\n",
      "Main effects training epoch: 151, train loss: 1.54985, val loss: 1.64845\n",
      "Main effects training epoch: 152, train loss: 1.54946, val loss: 1.64144\n",
      "Main effects training epoch: 153, train loss: 1.55892, val loss: 1.64846\n",
      "Main effects training epoch: 154, train loss: 1.55099, val loss: 1.65716\n",
      "Main effects training epoch: 155, train loss: 1.54449, val loss: 1.64429\n",
      "Main effects training epoch: 156, train loss: 1.54365, val loss: 1.64187\n",
      "Main effects training epoch: 157, train loss: 1.53963, val loss: 1.63992\n",
      "Main effects training epoch: 158, train loss: 1.54748, val loss: 1.64657\n",
      "Main effects training epoch: 159, train loss: 1.54430, val loss: 1.64718\n",
      "Main effects training epoch: 160, train loss: 1.54003, val loss: 1.63845\n",
      "Main effects training epoch: 161, train loss: 1.54033, val loss: 1.63758\n",
      "Main effects training epoch: 162, train loss: 1.53964, val loss: 1.64736\n",
      "Main effects training epoch: 163, train loss: 1.54107, val loss: 1.63633\n",
      "Main effects training epoch: 164, train loss: 1.54719, val loss: 1.63700\n",
      "Main effects training epoch: 165, train loss: 1.53651, val loss: 1.64047\n",
      "Main effects training epoch: 166, train loss: 1.54296, val loss: 1.63930\n",
      "Main effects training epoch: 167, train loss: 1.54579, val loss: 1.64468\n",
      "Main effects training epoch: 168, train loss: 1.53647, val loss: 1.63288\n",
      "Main effects training epoch: 169, train loss: 1.53953, val loss: 1.64381\n",
      "Main effects training epoch: 170, train loss: 1.53246, val loss: 1.62524\n",
      "Main effects training epoch: 171, train loss: 1.53308, val loss: 1.63850\n",
      "Main effects training epoch: 172, train loss: 1.53538, val loss: 1.62688\n",
      "Main effects training epoch: 173, train loss: 1.52921, val loss: 1.63268\n",
      "Main effects training epoch: 174, train loss: 1.53232, val loss: 1.63813\n",
      "Main effects training epoch: 175, train loss: 1.52846, val loss: 1.63042\n",
      "Main effects training epoch: 176, train loss: 1.52918, val loss: 1.63380\n",
      "Main effects training epoch: 177, train loss: 1.53307, val loss: 1.63078\n",
      "Main effects training epoch: 178, train loss: 1.52663, val loss: 1.62906\n",
      "Main effects training epoch: 179, train loss: 1.52485, val loss: 1.63136\n",
      "Main effects training epoch: 180, train loss: 1.51960, val loss: 1.62525\n",
      "Main effects training epoch: 181, train loss: 1.52031, val loss: 1.62748\n",
      "Main effects training epoch: 182, train loss: 1.51811, val loss: 1.63219\n",
      "Main effects training epoch: 183, train loss: 1.51827, val loss: 1.62367\n",
      "Main effects training epoch: 184, train loss: 1.51844, val loss: 1.62373\n",
      "Main effects training epoch: 185, train loss: 1.51612, val loss: 1.62689\n",
      "Main effects training epoch: 186, train loss: 1.51565, val loss: 1.62842\n",
      "Main effects training epoch: 187, train loss: 1.51359, val loss: 1.62174\n",
      "Main effects training epoch: 188, train loss: 1.51856, val loss: 1.62980\n",
      "Main effects training epoch: 189, train loss: 1.52591, val loss: 1.63017\n",
      "Main effects training epoch: 190, train loss: 1.53007, val loss: 1.63712\n",
      "Main effects training epoch: 191, train loss: 1.51891, val loss: 1.62172\n",
      "Main effects training epoch: 192, train loss: 1.51048, val loss: 1.62517\n",
      "Main effects training epoch: 193, train loss: 1.50988, val loss: 1.62506\n",
      "Main effects training epoch: 194, train loss: 1.50525, val loss: 1.61878\n",
      "Main effects training epoch: 195, train loss: 1.51277, val loss: 1.61831\n",
      "Main effects training epoch: 196, train loss: 1.50634, val loss: 1.61474\n",
      "Main effects training epoch: 197, train loss: 1.50431, val loss: 1.62354\n",
      "Main effects training epoch: 198, train loss: 1.50332, val loss: 1.60821\n",
      "Main effects training epoch: 199, train loss: 1.50016, val loss: 1.61627\n",
      "Main effects training epoch: 200, train loss: 1.50270, val loss: 1.60816\n",
      "Main effects training epoch: 201, train loss: 1.50258, val loss: 1.61609\n",
      "Main effects training epoch: 202, train loss: 1.50423, val loss: 1.61220\n",
      "Main effects training epoch: 203, train loss: 1.50274, val loss: 1.62514\n",
      "Main effects training epoch: 204, train loss: 1.50054, val loss: 1.60386\n",
      "Main effects training epoch: 205, train loss: 1.49738, val loss: 1.61401\n",
      "Main effects training epoch: 206, train loss: 1.49408, val loss: 1.60458\n",
      "Main effects training epoch: 207, train loss: 1.51464, val loss: 1.63043\n",
      "Main effects training epoch: 208, train loss: 1.49430, val loss: 1.61000\n",
      "Main effects training epoch: 209, train loss: 1.49666, val loss: 1.60754\n",
      "Main effects training epoch: 210, train loss: 1.50936, val loss: 1.60766\n",
      "Main effects training epoch: 211, train loss: 1.50459, val loss: 1.63023\n",
      "Main effects training epoch: 212, train loss: 1.49475, val loss: 1.60071\n",
      "Main effects training epoch: 213, train loss: 1.50186, val loss: 1.61603\n",
      "Main effects training epoch: 214, train loss: 1.49264, val loss: 1.60883\n",
      "Main effects training epoch: 215, train loss: 1.48977, val loss: 1.59891\n",
      "Main effects training epoch: 216, train loss: 1.50566, val loss: 1.60800\n",
      "Main effects training epoch: 217, train loss: 1.49598, val loss: 1.61615\n",
      "Main effects training epoch: 218, train loss: 1.48684, val loss: 1.59452\n",
      "Main effects training epoch: 219, train loss: 1.49533, val loss: 1.60141\n",
      "Main effects training epoch: 220, train loss: 1.48544, val loss: 1.59365\n",
      "Main effects training epoch: 221, train loss: 1.48359, val loss: 1.60616\n",
      "Main effects training epoch: 222, train loss: 1.48523, val loss: 1.59476\n",
      "Main effects training epoch: 223, train loss: 1.48077, val loss: 1.59446\n",
      "Main effects training epoch: 224, train loss: 1.48002, val loss: 1.58890\n",
      "Main effects training epoch: 225, train loss: 1.48211, val loss: 1.60297\n",
      "Main effects training epoch: 226, train loss: 1.48355, val loss: 1.59458\n",
      "Main effects training epoch: 227, train loss: 1.48358, val loss: 1.59207\n",
      "Main effects training epoch: 228, train loss: 1.48739, val loss: 1.59711\n",
      "Main effects training epoch: 229, train loss: 1.48400, val loss: 1.59826\n",
      "Main effects training epoch: 230, train loss: 1.48606, val loss: 1.58605\n",
      "Main effects training epoch: 231, train loss: 1.48569, val loss: 1.59849\n",
      "Main effects training epoch: 232, train loss: 1.47361, val loss: 1.58619\n",
      "Main effects training epoch: 233, train loss: 1.47653, val loss: 1.58676\n",
      "Main effects training epoch: 234, train loss: 1.47852, val loss: 1.58390\n",
      "Main effects training epoch: 235, train loss: 1.48208, val loss: 1.59015\n",
      "Main effects training epoch: 236, train loss: 1.48039, val loss: 1.57897\n",
      "Main effects training epoch: 237, train loss: 1.47661, val loss: 1.58882\n",
      "Main effects training epoch: 238, train loss: 1.48421, val loss: 1.58782\n",
      "Main effects training epoch: 239, train loss: 1.47550, val loss: 1.58389\n",
      "Main effects training epoch: 240, train loss: 1.48158, val loss: 1.58328\n",
      "Main effects training epoch: 241, train loss: 1.47100, val loss: 1.57910\n",
      "Main effects training epoch: 242, train loss: 1.47407, val loss: 1.57603\n",
      "Main effects training epoch: 243, train loss: 1.46918, val loss: 1.57464\n",
      "Main effects training epoch: 244, train loss: 1.46953, val loss: 1.57663\n",
      "Main effects training epoch: 245, train loss: 1.47800, val loss: 1.57183\n",
      "Main effects training epoch: 246, train loss: 1.47068, val loss: 1.57775\n",
      "Main effects training epoch: 247, train loss: 1.48568, val loss: 1.59080\n",
      "Main effects training epoch: 248, train loss: 1.47163, val loss: 1.57703\n",
      "Main effects training epoch: 249, train loss: 1.49239, val loss: 1.57063\n",
      "Main effects training epoch: 250, train loss: 1.46792, val loss: 1.57271\n",
      "Main effects training epoch: 251, train loss: 1.47942, val loss: 1.58283\n",
      "Main effects training epoch: 252, train loss: 1.48782, val loss: 1.58481\n",
      "Main effects training epoch: 253, train loss: 1.48136, val loss: 1.58449\n",
      "Main effects training epoch: 254, train loss: 1.47208, val loss: 1.56279\n",
      "Main effects training epoch: 255, train loss: 1.46718, val loss: 1.57391\n",
      "Main effects training epoch: 256, train loss: 1.47091, val loss: 1.56410\n",
      "Main effects training epoch: 257, train loss: 1.46473, val loss: 1.57004\n",
      "Main effects training epoch: 258, train loss: 1.46076, val loss: 1.55871\n",
      "Main effects training epoch: 259, train loss: 1.45991, val loss: 1.56170\n",
      "Main effects training epoch: 260, train loss: 1.46295, val loss: 1.56231\n",
      "Main effects training epoch: 261, train loss: 1.45948, val loss: 1.55463\n",
      "Main effects training epoch: 262, train loss: 1.46826, val loss: 1.56840\n",
      "Main effects training epoch: 263, train loss: 1.47130, val loss: 1.56519\n",
      "Main effects training epoch: 264, train loss: 1.45834, val loss: 1.56105\n",
      "Main effects training epoch: 265, train loss: 1.45818, val loss: 1.55106\n",
      "Main effects training epoch: 266, train loss: 1.46797, val loss: 1.56517\n",
      "Main effects training epoch: 267, train loss: 1.45545, val loss: 1.55044\n",
      "Main effects training epoch: 268, train loss: 1.45763, val loss: 1.55151\n",
      "Main effects training epoch: 269, train loss: 1.45653, val loss: 1.55239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 270, train loss: 1.44941, val loss: 1.53803\n",
      "Main effects training epoch: 271, train loss: 1.45663, val loss: 1.54697\n",
      "Main effects training epoch: 272, train loss: 1.45972, val loss: 1.56030\n",
      "Main effects training epoch: 273, train loss: 1.45647, val loss: 1.54109\n",
      "Main effects training epoch: 274, train loss: 1.45572, val loss: 1.55753\n",
      "Main effects training epoch: 275, train loss: 1.45404, val loss: 1.54123\n",
      "Main effects training epoch: 276, train loss: 1.45404, val loss: 1.55553\n",
      "Main effects training epoch: 277, train loss: 1.44668, val loss: 1.53290\n",
      "Main effects training epoch: 278, train loss: 1.45275, val loss: 1.54265\n",
      "Main effects training epoch: 279, train loss: 1.45100, val loss: 1.54138\n",
      "Main effects training epoch: 280, train loss: 1.44423, val loss: 1.53974\n",
      "Main effects training epoch: 281, train loss: 1.44704, val loss: 1.53325\n",
      "Main effects training epoch: 282, train loss: 1.45213, val loss: 1.53096\n",
      "Main effects training epoch: 283, train loss: 1.44494, val loss: 1.54294\n",
      "Main effects training epoch: 284, train loss: 1.45471, val loss: 1.55558\n",
      "Main effects training epoch: 285, train loss: 1.45283, val loss: 1.54131\n",
      "Main effects training epoch: 286, train loss: 1.44614, val loss: 1.52391\n",
      "Main effects training epoch: 287, train loss: 1.44219, val loss: 1.53613\n",
      "Main effects training epoch: 288, train loss: 1.45036, val loss: 1.53062\n",
      "Main effects training epoch: 289, train loss: 1.44346, val loss: 1.53639\n",
      "Main effects training epoch: 290, train loss: 1.43765, val loss: 1.52572\n",
      "Main effects training epoch: 291, train loss: 1.44087, val loss: 1.53552\n",
      "Main effects training epoch: 292, train loss: 1.45915, val loss: 1.55393\n",
      "Main effects training epoch: 293, train loss: 1.44226, val loss: 1.54216\n",
      "Main effects training epoch: 294, train loss: 1.44707, val loss: 1.52195\n",
      "Main effects training epoch: 295, train loss: 1.45062, val loss: 1.54359\n",
      "Main effects training epoch: 296, train loss: 1.45659, val loss: 1.55264\n",
      "Main effects training epoch: 297, train loss: 1.45993, val loss: 1.54519\n",
      "Main effects training epoch: 298, train loss: 1.46833, val loss: 1.53037\n",
      "Main effects training epoch: 299, train loss: 1.44147, val loss: 1.52292\n",
      "Main effects training epoch: 300, train loss: 1.44231, val loss: 1.53392\n",
      "##########Stage 1: main effect training stop.##########\n",
      "3 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.46542, val loss: 1.55467\n",
      "Main effects tuning epoch: 2, train loss: 1.45828, val loss: 1.53008\n",
      "Main effects tuning epoch: 3, train loss: 1.46427, val loss: 1.53612\n",
      "Main effects tuning epoch: 4, train loss: 1.46118, val loss: 1.53933\n",
      "Main effects tuning epoch: 5, train loss: 1.46062, val loss: 1.53830\n",
      "Main effects tuning epoch: 6, train loss: 1.47292, val loss: 1.54469\n",
      "Main effects tuning epoch: 7, train loss: 1.45842, val loss: 1.52927\n",
      "Main effects tuning epoch: 8, train loss: 1.46926, val loss: 1.54190\n",
      "Main effects tuning epoch: 9, train loss: 1.45317, val loss: 1.53765\n",
      "Main effects tuning epoch: 10, train loss: 1.45406, val loss: 1.53287\n",
      "Main effects tuning epoch: 11, train loss: 1.45512, val loss: 1.53137\n",
      "Main effects tuning epoch: 12, train loss: 1.46774, val loss: 1.54888\n",
      "Main effects tuning epoch: 13, train loss: 1.46909, val loss: 1.55100\n",
      "Main effects tuning epoch: 14, train loss: 1.45193, val loss: 1.52105\n",
      "Main effects tuning epoch: 15, train loss: 1.44895, val loss: 1.52644\n",
      "Main effects tuning epoch: 16, train loss: 1.44666, val loss: 1.52558\n",
      "Main effects tuning epoch: 17, train loss: 1.45456, val loss: 1.53149\n",
      "Main effects tuning epoch: 18, train loss: 1.46331, val loss: 1.54574\n",
      "Main effects tuning epoch: 19, train loss: 1.46592, val loss: 1.53476\n",
      "Main effects tuning epoch: 20, train loss: 1.45980, val loss: 1.53262\n",
      "Main effects tuning epoch: 21, train loss: 1.45162, val loss: 1.52835\n",
      "Main effects tuning epoch: 22, train loss: 1.45170, val loss: 1.52445\n",
      "Main effects tuning epoch: 23, train loss: 1.46429, val loss: 1.54516\n",
      "Main effects tuning epoch: 24, train loss: 1.44976, val loss: 1.52549\n",
      "Main effects tuning epoch: 25, train loss: 1.45714, val loss: 1.52384\n",
      "Main effects tuning epoch: 26, train loss: 1.44938, val loss: 1.51629\n",
      "Main effects tuning epoch: 27, train loss: 1.44921, val loss: 1.52797\n",
      "Main effects tuning epoch: 28, train loss: 1.44392, val loss: 1.52194\n",
      "Main effects tuning epoch: 29, train loss: 1.44493, val loss: 1.51999\n",
      "Main effects tuning epoch: 30, train loss: 1.44443, val loss: 1.51833\n",
      "Main effects tuning epoch: 31, train loss: 1.45208, val loss: 1.52704\n",
      "Main effects tuning epoch: 32, train loss: 1.44876, val loss: 1.52781\n",
      "Main effects tuning epoch: 33, train loss: 1.44407, val loss: 1.52105\n",
      "Main effects tuning epoch: 34, train loss: 1.44440, val loss: 1.51716\n",
      "Main effects tuning epoch: 35, train loss: 1.44495, val loss: 1.52260\n",
      "Main effects tuning epoch: 36, train loss: 1.44195, val loss: 1.51367\n",
      "Main effects tuning epoch: 37, train loss: 1.44388, val loss: 1.52815\n",
      "Main effects tuning epoch: 38, train loss: 1.46100, val loss: 1.52459\n",
      "Main effects tuning epoch: 39, train loss: 1.46648, val loss: 1.54755\n",
      "Main effects tuning epoch: 40, train loss: 1.45319, val loss: 1.51967\n",
      "Main effects tuning epoch: 41, train loss: 1.45410, val loss: 1.53367\n",
      "Main effects tuning epoch: 42, train loss: 1.46877, val loss: 1.53932\n",
      "Main effects tuning epoch: 43, train loss: 1.45339, val loss: 1.52782\n",
      "Main effects tuning epoch: 44, train loss: 1.44703, val loss: 1.52274\n",
      "Main effects tuning epoch: 45, train loss: 1.48153, val loss: 1.53553\n",
      "Main effects tuning epoch: 46, train loss: 1.44241, val loss: 1.51502\n",
      "Main effects tuning epoch: 47, train loss: 1.44973, val loss: 1.52414\n",
      "Main effects tuning epoch: 48, train loss: 1.44797, val loss: 1.51882\n",
      "Main effects tuning epoch: 49, train loss: 1.43956, val loss: 1.51930\n",
      "Main effects tuning epoch: 50, train loss: 1.44489, val loss: 1.52623\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.35956, val loss: 1.44004\n",
      "Interaction training epoch: 2, train loss: 1.16266, val loss: 1.15729\n",
      "Interaction training epoch: 3, train loss: 1.07515, val loss: 1.09680\n",
      "Interaction training epoch: 4, train loss: 1.08959, val loss: 1.09799\n",
      "Interaction training epoch: 5, train loss: 1.02562, val loss: 1.05150\n",
      "Interaction training epoch: 6, train loss: 1.05040, val loss: 1.07472\n",
      "Interaction training epoch: 7, train loss: 1.02736, val loss: 1.04630\n",
      "Interaction training epoch: 8, train loss: 1.01313, val loss: 1.03966\n",
      "Interaction training epoch: 9, train loss: 1.00369, val loss: 1.03090\n",
      "Interaction training epoch: 10, train loss: 1.00247, val loss: 1.02208\n",
      "Interaction training epoch: 11, train loss: 0.98621, val loss: 1.01223\n",
      "Interaction training epoch: 12, train loss: 0.99083, val loss: 1.01528\n",
      "Interaction training epoch: 13, train loss: 0.98893, val loss: 1.01983\n",
      "Interaction training epoch: 14, train loss: 0.98402, val loss: 1.01339\n",
      "Interaction training epoch: 15, train loss: 0.96272, val loss: 0.99922\n",
      "Interaction training epoch: 16, train loss: 0.96436, val loss: 0.99601\n",
      "Interaction training epoch: 17, train loss: 0.94949, val loss: 0.99299\n",
      "Interaction training epoch: 18, train loss: 0.95474, val loss: 0.97821\n",
      "Interaction training epoch: 19, train loss: 0.94761, val loss: 0.97282\n",
      "Interaction training epoch: 20, train loss: 0.93571, val loss: 0.97906\n",
      "Interaction training epoch: 21, train loss: 0.93547, val loss: 0.96650\n",
      "Interaction training epoch: 22, train loss: 0.93761, val loss: 0.97724\n",
      "Interaction training epoch: 23, train loss: 0.92325, val loss: 0.95897\n",
      "Interaction training epoch: 24, train loss: 0.91304, val loss: 0.95533\n",
      "Interaction training epoch: 25, train loss: 0.91186, val loss: 0.94930\n",
      "Interaction training epoch: 26, train loss: 0.92447, val loss: 0.94242\n",
      "Interaction training epoch: 27, train loss: 0.94090, val loss: 0.97367\n",
      "Interaction training epoch: 28, train loss: 0.91982, val loss: 0.94358\n",
      "Interaction training epoch: 29, train loss: 0.89924, val loss: 0.93031\n",
      "Interaction training epoch: 30, train loss: 0.90579, val loss: 0.93657\n",
      "Interaction training epoch: 31, train loss: 0.89575, val loss: 0.92671\n",
      "Interaction training epoch: 32, train loss: 0.89498, val loss: 0.92143\n",
      "Interaction training epoch: 33, train loss: 0.90050, val loss: 0.93500\n",
      "Interaction training epoch: 34, train loss: 0.91867, val loss: 0.93886\n",
      "Interaction training epoch: 35, train loss: 0.88603, val loss: 0.92462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 36, train loss: 0.87986, val loss: 0.90572\n",
      "Interaction training epoch: 37, train loss: 0.88920, val loss: 0.91067\n",
      "Interaction training epoch: 38, train loss: 0.87625, val loss: 0.89954\n",
      "Interaction training epoch: 39, train loss: 0.88512, val loss: 0.91470\n",
      "Interaction training epoch: 40, train loss: 0.87401, val loss: 0.89891\n",
      "Interaction training epoch: 41, train loss: 0.87958, val loss: 0.90508\n",
      "Interaction training epoch: 42, train loss: 0.88510, val loss: 0.92577\n",
      "Interaction training epoch: 43, train loss: 0.88289, val loss: 0.90315\n",
      "Interaction training epoch: 44, train loss: 0.87507, val loss: 0.90408\n",
      "Interaction training epoch: 45, train loss: 0.88150, val loss: 0.91455\n",
      "Interaction training epoch: 46, train loss: 0.86788, val loss: 0.89524\n",
      "Interaction training epoch: 47, train loss: 0.87330, val loss: 0.90603\n",
      "Interaction training epoch: 48, train loss: 0.86735, val loss: 0.89192\n",
      "Interaction training epoch: 49, train loss: 0.87837, val loss: 0.90580\n",
      "Interaction training epoch: 50, train loss: 0.87093, val loss: 0.88620\n",
      "Interaction training epoch: 51, train loss: 0.86258, val loss: 0.88838\n",
      "Interaction training epoch: 52, train loss: 0.86881, val loss: 0.89827\n",
      "Interaction training epoch: 53, train loss: 0.85946, val loss: 0.88099\n",
      "Interaction training epoch: 54, train loss: 0.86594, val loss: 0.89348\n",
      "Interaction training epoch: 55, train loss: 0.85430, val loss: 0.89546\n",
      "Interaction training epoch: 56, train loss: 0.86356, val loss: 0.89473\n",
      "Interaction training epoch: 57, train loss: 0.85588, val loss: 0.89591\n",
      "Interaction training epoch: 58, train loss: 0.85848, val loss: 0.88465\n",
      "Interaction training epoch: 59, train loss: 0.85289, val loss: 0.88974\n",
      "Interaction training epoch: 60, train loss: 0.85817, val loss: 0.88702\n",
      "Interaction training epoch: 61, train loss: 0.86530, val loss: 0.88868\n",
      "Interaction training epoch: 62, train loss: 0.85730, val loss: 0.89176\n",
      "Interaction training epoch: 63, train loss: 0.83509, val loss: 0.86787\n",
      "Interaction training epoch: 64, train loss: 0.86807, val loss: 0.90404\n",
      "Interaction training epoch: 65, train loss: 0.84738, val loss: 0.88217\n",
      "Interaction training epoch: 66, train loss: 0.84354, val loss: 0.87647\n",
      "Interaction training epoch: 67, train loss: 0.84338, val loss: 0.87313\n",
      "Interaction training epoch: 68, train loss: 0.83704, val loss: 0.87095\n",
      "Interaction training epoch: 69, train loss: 0.84399, val loss: 0.88242\n",
      "Interaction training epoch: 70, train loss: 0.83868, val loss: 0.86668\n",
      "Interaction training epoch: 71, train loss: 0.83756, val loss: 0.87909\n",
      "Interaction training epoch: 72, train loss: 0.84964, val loss: 0.87586\n",
      "Interaction training epoch: 73, train loss: 0.83162, val loss: 0.86976\n",
      "Interaction training epoch: 74, train loss: 0.84188, val loss: 0.87705\n",
      "Interaction training epoch: 75, train loss: 0.84206, val loss: 0.87947\n",
      "Interaction training epoch: 76, train loss: 0.84148, val loss: 0.87219\n",
      "Interaction training epoch: 77, train loss: 0.83403, val loss: 0.87234\n",
      "Interaction training epoch: 78, train loss: 0.84488, val loss: 0.88194\n",
      "Interaction training epoch: 79, train loss: 0.83126, val loss: 0.86657\n",
      "Interaction training epoch: 80, train loss: 0.83406, val loss: 0.86751\n",
      "Interaction training epoch: 81, train loss: 0.83259, val loss: 0.86593\n",
      "Interaction training epoch: 82, train loss: 0.84144, val loss: 0.87496\n",
      "Interaction training epoch: 83, train loss: 0.83771, val loss: 0.86121\n",
      "Interaction training epoch: 84, train loss: 0.83609, val loss: 0.85608\n",
      "Interaction training epoch: 85, train loss: 0.84070, val loss: 0.87631\n",
      "Interaction training epoch: 86, train loss: 0.84256, val loss: 0.87491\n",
      "Interaction training epoch: 87, train loss: 0.85410, val loss: 0.89512\n",
      "Interaction training epoch: 88, train loss: 0.83074, val loss: 0.85550\n",
      "Interaction training epoch: 89, train loss: 0.83547, val loss: 0.86836\n",
      "Interaction training epoch: 90, train loss: 0.83311, val loss: 0.86605\n",
      "Interaction training epoch: 91, train loss: 0.83697, val loss: 0.86848\n",
      "Interaction training epoch: 92, train loss: 0.83107, val loss: 0.87271\n",
      "Interaction training epoch: 93, train loss: 0.82750, val loss: 0.85246\n",
      "Interaction training epoch: 94, train loss: 0.82198, val loss: 0.85787\n",
      "Interaction training epoch: 95, train loss: 0.83097, val loss: 0.86753\n",
      "Interaction training epoch: 96, train loss: 0.83119, val loss: 0.86596\n",
      "Interaction training epoch: 97, train loss: 0.84056, val loss: 0.86746\n",
      "Interaction training epoch: 98, train loss: 0.83513, val loss: 0.87306\n",
      "Interaction training epoch: 99, train loss: 0.82373, val loss: 0.86292\n",
      "Interaction training epoch: 100, train loss: 0.81627, val loss: 0.84604\n",
      "Interaction training epoch: 101, train loss: 0.82400, val loss: 0.85562\n",
      "Interaction training epoch: 102, train loss: 0.82878, val loss: 0.86467\n",
      "Interaction training epoch: 103, train loss: 0.82735, val loss: 0.85483\n",
      "Interaction training epoch: 104, train loss: 0.82166, val loss: 0.85541\n",
      "Interaction training epoch: 105, train loss: 0.82404, val loss: 0.85543\n",
      "Interaction training epoch: 106, train loss: 0.82330, val loss: 0.86001\n",
      "Interaction training epoch: 107, train loss: 0.82319, val loss: 0.85585\n",
      "Interaction training epoch: 108, train loss: 0.81799, val loss: 0.85330\n",
      "Interaction training epoch: 109, train loss: 0.82463, val loss: 0.85725\n",
      "Interaction training epoch: 110, train loss: 0.81599, val loss: 0.84552\n",
      "Interaction training epoch: 111, train loss: 0.82273, val loss: 0.85840\n",
      "Interaction training epoch: 112, train loss: 0.83371, val loss: 0.86300\n",
      "Interaction training epoch: 113, train loss: 0.82205, val loss: 0.86482\n",
      "Interaction training epoch: 114, train loss: 0.82281, val loss: 0.84550\n",
      "Interaction training epoch: 115, train loss: 0.81942, val loss: 0.85127\n",
      "Interaction training epoch: 116, train loss: 0.82768, val loss: 0.87217\n",
      "Interaction training epoch: 117, train loss: 0.81768, val loss: 0.85444\n",
      "Interaction training epoch: 118, train loss: 0.81819, val loss: 0.84796\n",
      "Interaction training epoch: 119, train loss: 0.81939, val loss: 0.84624\n",
      "Interaction training epoch: 120, train loss: 0.82340, val loss: 0.86014\n",
      "Interaction training epoch: 121, train loss: 0.81343, val loss: 0.84896\n",
      "Interaction training epoch: 122, train loss: 0.81550, val loss: 0.85077\n",
      "Interaction training epoch: 123, train loss: 0.81316, val loss: 0.84941\n",
      "Interaction training epoch: 124, train loss: 0.82164, val loss: 0.85910\n",
      "Interaction training epoch: 125, train loss: 0.82242, val loss: 0.85146\n",
      "Interaction training epoch: 126, train loss: 0.80898, val loss: 0.84261\n",
      "Interaction training epoch: 127, train loss: 0.81651, val loss: 0.85681\n",
      "Interaction training epoch: 128, train loss: 0.82797, val loss: 0.85938\n",
      "Interaction training epoch: 129, train loss: 0.82071, val loss: 0.84372\n",
      "Interaction training epoch: 130, train loss: 0.81439, val loss: 0.85272\n",
      "Interaction training epoch: 131, train loss: 0.81579, val loss: 0.85840\n",
      "Interaction training epoch: 132, train loss: 0.81732, val loss: 0.84537\n",
      "Interaction training epoch: 133, train loss: 0.81967, val loss: 0.85014\n",
      "Interaction training epoch: 134, train loss: 0.80848, val loss: 0.85204\n",
      "Interaction training epoch: 135, train loss: 0.81793, val loss: 0.85399\n",
      "Interaction training epoch: 136, train loss: 0.81127, val loss: 0.84530\n",
      "Interaction training epoch: 137, train loss: 0.81281, val loss: 0.84978\n",
      "Interaction training epoch: 138, train loss: 0.82074, val loss: 0.86511\n",
      "Interaction training epoch: 139, train loss: 0.82138, val loss: 0.85016\n",
      "Interaction training epoch: 140, train loss: 0.81083, val loss: 0.84596\n",
      "Interaction training epoch: 141, train loss: 0.81199, val loss: 0.84463\n",
      "Interaction training epoch: 142, train loss: 0.80739, val loss: 0.84848\n",
      "Interaction training epoch: 143, train loss: 0.81462, val loss: 0.84955\n",
      "Interaction training epoch: 144, train loss: 0.81049, val loss: 0.84038\n",
      "Interaction training epoch: 145, train loss: 0.82056, val loss: 0.86002\n",
      "Interaction training epoch: 146, train loss: 0.81919, val loss: 0.84861\n",
      "Interaction training epoch: 147, train loss: 0.80874, val loss: 0.85060\n",
      "Interaction training epoch: 148, train loss: 0.80972, val loss: 0.83730\n",
      "Interaction training epoch: 149, train loss: 0.80554, val loss: 0.84654\n",
      "Interaction training epoch: 150, train loss: 0.81081, val loss: 0.84428\n",
      "Interaction training epoch: 151, train loss: 0.82008, val loss: 0.86437\n",
      "Interaction training epoch: 152, train loss: 0.81465, val loss: 0.84223\n",
      "Interaction training epoch: 153, train loss: 0.80942, val loss: 0.84544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 154, train loss: 0.81683, val loss: 0.85424\n",
      "Interaction training epoch: 155, train loss: 0.80498, val loss: 0.84300\n",
      "Interaction training epoch: 156, train loss: 0.80570, val loss: 0.84624\n",
      "Interaction training epoch: 157, train loss: 0.81183, val loss: 0.84700\n",
      "Interaction training epoch: 158, train loss: 0.81094, val loss: 0.84826\n",
      "Interaction training epoch: 159, train loss: 0.81565, val loss: 0.84908\n",
      "Interaction training epoch: 160, train loss: 0.81221, val loss: 0.84221\n",
      "Interaction training epoch: 161, train loss: 0.80634, val loss: 0.85106\n",
      "Interaction training epoch: 162, train loss: 0.80876, val loss: 0.84489\n",
      "Interaction training epoch: 163, train loss: 0.81259, val loss: 0.84766\n",
      "Interaction training epoch: 164, train loss: 0.80771, val loss: 0.83652\n",
      "Interaction training epoch: 165, train loss: 0.80931, val loss: 0.85416\n",
      "Interaction training epoch: 166, train loss: 0.80306, val loss: 0.84405\n",
      "Interaction training epoch: 167, train loss: 0.80295, val loss: 0.83638\n",
      "Interaction training epoch: 168, train loss: 0.80231, val loss: 0.83510\n",
      "Interaction training epoch: 169, train loss: 0.81097, val loss: 0.84390\n",
      "Interaction training epoch: 170, train loss: 0.81484, val loss: 0.84552\n",
      "Interaction training epoch: 171, train loss: 0.82006, val loss: 0.85974\n",
      "Interaction training epoch: 172, train loss: 0.81696, val loss: 0.85283\n",
      "Interaction training epoch: 173, train loss: 0.81090, val loss: 0.84311\n",
      "Interaction training epoch: 174, train loss: 0.80107, val loss: 0.83393\n",
      "Interaction training epoch: 175, train loss: 0.80938, val loss: 0.84732\n",
      "Interaction training epoch: 176, train loss: 0.80506, val loss: 0.84097\n",
      "Interaction training epoch: 177, train loss: 0.81178, val loss: 0.85151\n",
      "Interaction training epoch: 178, train loss: 0.80761, val loss: 0.84509\n",
      "Interaction training epoch: 179, train loss: 0.80250, val loss: 0.83209\n",
      "Interaction training epoch: 180, train loss: 0.79717, val loss: 0.83554\n",
      "Interaction training epoch: 181, train loss: 0.80281, val loss: 0.83874\n",
      "Interaction training epoch: 182, train loss: 0.80797, val loss: 0.84677\n",
      "Interaction training epoch: 183, train loss: 0.80733, val loss: 0.84581\n",
      "Interaction training epoch: 184, train loss: 0.80080, val loss: 0.83591\n",
      "Interaction training epoch: 185, train loss: 0.80897, val loss: 0.84410\n",
      "Interaction training epoch: 186, train loss: 0.80477, val loss: 0.84023\n",
      "Interaction training epoch: 187, train loss: 0.79660, val loss: 0.83708\n",
      "Interaction training epoch: 188, train loss: 0.80182, val loss: 0.83419\n",
      "Interaction training epoch: 189, train loss: 0.80245, val loss: 0.83946\n",
      "Interaction training epoch: 190, train loss: 0.80716, val loss: 0.84376\n",
      "Interaction training epoch: 191, train loss: 0.80034, val loss: 0.84167\n",
      "Interaction training epoch: 192, train loss: 0.80129, val loss: 0.83811\n",
      "Interaction training epoch: 193, train loss: 0.81024, val loss: 0.84435\n",
      "Interaction training epoch: 194, train loss: 0.80616, val loss: 0.84611\n",
      "Interaction training epoch: 195, train loss: 0.80702, val loss: 0.84926\n",
      "Interaction training epoch: 196, train loss: 0.80533, val loss: 0.84144\n",
      "Interaction training epoch: 197, train loss: 0.80756, val loss: 0.85090\n",
      "Interaction training epoch: 198, train loss: 0.79990, val loss: 0.83448\n",
      "Interaction training epoch: 199, train loss: 0.80629, val loss: 0.84261\n",
      "Interaction training epoch: 200, train loss: 0.80512, val loss: 0.84408\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########3 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.81592, val loss: 0.84055\n",
      "Interaction tuning epoch: 2, train loss: 0.81950, val loss: 0.84501\n",
      "Interaction tuning epoch: 3, train loss: 0.81259, val loss: 0.84780\n",
      "Interaction tuning epoch: 4, train loss: 0.81577, val loss: 0.84881\n",
      "Interaction tuning epoch: 5, train loss: 0.81688, val loss: 0.83808\n",
      "Interaction tuning epoch: 6, train loss: 0.80850, val loss: 0.84555\n",
      "Interaction tuning epoch: 7, train loss: 0.80946, val loss: 0.83372\n",
      "Interaction tuning epoch: 8, train loss: 0.80731, val loss: 0.84274\n",
      "Interaction tuning epoch: 9, train loss: 0.81819, val loss: 0.84290\n",
      "Interaction tuning epoch: 10, train loss: 0.80767, val loss: 0.83833\n",
      "Interaction tuning epoch: 11, train loss: 0.80657, val loss: 0.83448\n",
      "Interaction tuning epoch: 12, train loss: 0.80680, val loss: 0.83998\n",
      "Interaction tuning epoch: 13, train loss: 0.80540, val loss: 0.83211\n",
      "Interaction tuning epoch: 14, train loss: 0.80460, val loss: 0.83256\n",
      "Interaction tuning epoch: 15, train loss: 0.80278, val loss: 0.83943\n",
      "Interaction tuning epoch: 16, train loss: 0.81095, val loss: 0.84211\n",
      "Interaction tuning epoch: 17, train loss: 0.80905, val loss: 0.83829\n",
      "Interaction tuning epoch: 18, train loss: 0.80650, val loss: 0.83462\n",
      "Interaction tuning epoch: 19, train loss: 0.81845, val loss: 0.85697\n",
      "Interaction tuning epoch: 20, train loss: 0.80875, val loss: 0.84020\n",
      "Interaction tuning epoch: 21, train loss: 0.80441, val loss: 0.83783\n",
      "Interaction tuning epoch: 22, train loss: 0.81194, val loss: 0.84192\n",
      "Interaction tuning epoch: 23, train loss: 0.80616, val loss: 0.84611\n",
      "Interaction tuning epoch: 24, train loss: 0.81846, val loss: 0.84446\n",
      "Interaction tuning epoch: 25, train loss: 0.81432, val loss: 0.84806\n",
      "Interaction tuning epoch: 26, train loss: 0.80809, val loss: 0.84088\n",
      "Interaction tuning epoch: 27, train loss: 0.80590, val loss: 0.83711\n",
      "Interaction tuning epoch: 28, train loss: 0.80963, val loss: 0.84216\n",
      "Interaction tuning epoch: 29, train loss: 0.80208, val loss: 0.83875\n",
      "Interaction tuning epoch: 30, train loss: 0.80493, val loss: 0.83268\n",
      "Interaction tuning epoch: 31, train loss: 0.80841, val loss: 0.84527\n",
      "Interaction tuning epoch: 32, train loss: 0.80748, val loss: 0.83929\n",
      "Interaction tuning epoch: 33, train loss: 0.80750, val loss: 0.84312\n",
      "Interaction tuning epoch: 34, train loss: 0.80301, val loss: 0.84292\n",
      "Interaction tuning epoch: 35, train loss: 0.80774, val loss: 0.83560\n",
      "Interaction tuning epoch: 36, train loss: 0.80555, val loss: 0.83802\n",
      "Interaction tuning epoch: 37, train loss: 0.80499, val loss: 0.84204\n",
      "Interaction tuning epoch: 38, train loss: 0.80446, val loss: 0.83974\n",
      "Interaction tuning epoch: 39, train loss: 0.80752, val loss: 0.84332\n",
      "Interaction tuning epoch: 40, train loss: 0.80484, val loss: 0.83258\n",
      "Interaction tuning epoch: 41, train loss: 0.80195, val loss: 0.84098\n",
      "Interaction tuning epoch: 42, train loss: 0.80334, val loss: 0.84273\n",
      "Interaction tuning epoch: 43, train loss: 0.80260, val loss: 0.83410\n",
      "Interaction tuning epoch: 44, train loss: 0.80256, val loss: 0.83365\n",
      "Interaction tuning epoch: 45, train loss: 0.80429, val loss: 0.84047\n",
      "Interaction tuning epoch: 46, train loss: 0.80745, val loss: 0.83905\n",
      "Interaction tuning epoch: 47, train loss: 0.80601, val loss: 0.83912\n",
      "Interaction tuning epoch: 48, train loss: 0.80334, val loss: 0.84125\n",
      "Interaction tuning epoch: 49, train loss: 0.81128, val loss: 0.83634\n",
      "Interaction tuning epoch: 50, train loss: 0.80132, val loss: 0.83368\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 36.235124588012695\n",
      "After the gam stage, training error is 0.80132 , validation error is 0.83368\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 19.544620\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.665335 validation MAE=0.780818,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.620811 validation MAE=0.759891,rank=5\n",
      "[SoftImpute] Iter 3: observed MAE=0.581814 validation MAE=0.740848,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.548980 validation MAE=0.723740,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.520451 validation MAE=0.707937,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.495566 validation MAE=0.693801,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.474822 validation MAE=0.681662,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.457590 validation MAE=0.671239,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 9: observed MAE=0.439572 validation MAE=0.662034,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.427122 validation MAE=0.654661,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.414563 validation MAE=0.647749,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.402528 validation MAE=0.640986,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.390701 validation MAE=0.634699,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.379662 validation MAE=0.629235,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.369999 validation MAE=0.623325,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.359858 validation MAE=0.618553,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.352141 validation MAE=0.614223,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.344223 validation MAE=0.610037,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.337004 validation MAE=0.606609,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.330017 validation MAE=0.602601,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.324541 validation MAE=0.599680,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.318422 validation MAE=0.596472,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.312959 validation MAE=0.592899,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.308029 validation MAE=0.591144,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.302941 validation MAE=0.588485,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.298732 validation MAE=0.586034,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.294785 validation MAE=0.584271,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.290920 validation MAE=0.581928,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.287386 validation MAE=0.579758,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.284038 validation MAE=0.578068,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.280999 validation MAE=0.576582,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.278110 validation MAE=0.575795,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.274730 validation MAE=0.573861,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.272579 validation MAE=0.572377,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.269549 validation MAE=0.570594,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.267127 validation MAE=0.569086,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.264792 validation MAE=0.568081,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.262157 validation MAE=0.566703,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.260245 validation MAE=0.565770,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.257941 validation MAE=0.564395,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.255936 validation MAE=0.563784,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.254213 validation MAE=0.562021,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.252833 validation MAE=0.560601,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.251482 validation MAE=0.559774,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.249406 validation MAE=0.558883,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.247837 validation MAE=0.558393,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.246488 validation MAE=0.557460,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.244783 validation MAE=0.556382,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.243799 validation MAE=0.555450,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.241652 validation MAE=0.554792,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.240510 validation MAE=0.554080,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.239598 validation MAE=0.553642,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.238242 validation MAE=0.552917,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.236406 validation MAE=0.551911,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.236340 validation MAE=0.551752,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.234614 validation MAE=0.550454,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.232986 validation MAE=0.549716,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.231738 validation MAE=0.548909,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.230876 validation MAE=0.547964,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.229890 validation MAE=0.547819,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.229040 validation MAE=0.546979,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.228463 validation MAE=0.546509,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.227737 validation MAE=0.546342,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.226929 validation MAE=0.545666,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.225740 validation MAE=0.545787,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.225076 validation MAE=0.545388,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.223846 validation MAE=0.543636,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.223515 validation MAE=0.543228,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.222516 validation MAE=0.542565,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.221877 validation MAE=0.542491,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.221086 validation MAE=0.541789,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.220762 validation MAE=0.541405,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.219596 validation MAE=0.541253,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.219186 validation MAE=0.540494,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.218090 validation MAE=0.539576,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.217683 validation MAE=0.538990,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.216769 validation MAE=0.538904,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.215982 validation MAE=0.539140,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.215043 validation MAE=0.538154,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.214521 validation MAE=0.537534,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.214580 validation MAE=0.537231,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.213904 validation MAE=0.536681,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.213550 validation MAE=0.536882,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.212714 validation MAE=0.536625,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.211843 validation MAE=0.535749,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.210809 validation MAE=0.535744,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.210455 validation MAE=0.535212,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.210040 validation MAE=0.534772,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.209912 validation MAE=0.535226,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.209250 validation MAE=0.534660,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.208782 validation MAE=0.534329,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.208495 validation MAE=0.534512,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.206971 validation MAE=0.533605,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.206570 validation MAE=0.533301,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.207800 validation MAE=0.533686,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.206674 validation MAE=0.533719,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.205615 validation MAE=0.532952,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.205176 validation MAE=0.533104,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.204492 validation MAE=0.532572,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.204443 validation MAE=0.531705,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.204208 validation MAE=0.532358,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.203087 validation MAE=0.531769,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.203023 validation MAE=0.531457,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.202747 validation MAE=0.530775,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.202268 validation MAE=0.531018,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.202248 validation MAE=0.531110,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.201658 validation MAE=0.530498,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.201688 validation MAE=0.530326,rank=5\n",
      "[SoftImpute] Iter 109: observed MAE=0.200701 validation MAE=0.530172,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.200190 validation MAE=0.529285,rank=5\n",
      "[SoftImpute] Iter 111: observed MAE=0.200360 validation MAE=0.529742,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.200146 validation MAE=0.529327,rank=5\n",
      "[SoftImpute] Iter 113: observed MAE=0.199253 validation MAE=0.528940,rank=5\n",
      "[SoftImpute] Iter 114: observed MAE=0.198710 validation MAE=0.529146,rank=5\n",
      "[SoftImpute] Iter 115: observed MAE=0.198492 validation MAE=0.528999,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.197906 validation MAE=0.528919,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.197107 validation MAE=0.528497,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.196585 validation MAE=0.527818,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.196442 validation MAE=0.527389,rank=5\n",
      "[SoftImpute] Iter 120: observed MAE=0.197160 validation MAE=0.527601,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.196641 validation MAE=0.527783,rank=5\n",
      "[SoftImpute] Iter 122: observed MAE=0.195959 validation MAE=0.527552,rank=5\n",
      "[SoftImpute] Iter 123: observed MAE=0.196057 validation MAE=0.526896,rank=5\n",
      "[SoftImpute] Iter 124: observed MAE=0.195291 validation MAE=0.526442,rank=5\n",
      "[SoftImpute] Iter 125: observed MAE=0.195261 validation MAE=0.526775,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 126: observed MAE=0.195239 validation MAE=0.526840,rank=5\n",
      "[SoftImpute] Iter 127: observed MAE=0.194691 validation MAE=0.525996,rank=5\n",
      "[SoftImpute] Iter 128: observed MAE=0.194601 validation MAE=0.526126,rank=5\n",
      "[SoftImpute] Iter 129: observed MAE=0.194247 validation MAE=0.525669,rank=5\n",
      "[SoftImpute] Iter 130: observed MAE=0.193931 validation MAE=0.525383,rank=5\n",
      "[SoftImpute] Iter 131: observed MAE=0.193795 validation MAE=0.524852,rank=5\n",
      "[SoftImpute] Iter 132: observed MAE=0.193067 validation MAE=0.524649,rank=5\n",
      "[SoftImpute] Iter 133: observed MAE=0.192764 validation MAE=0.524710,rank=5\n",
      "[SoftImpute] Iter 134: observed MAE=0.192852 validation MAE=0.524786,rank=5\n",
      "[SoftImpute] Iter 135: observed MAE=0.192474 validation MAE=0.524392,rank=5\n",
      "[SoftImpute] Iter 136: observed MAE=0.192682 validation MAE=0.524622,rank=5\n",
      "[SoftImpute] Iter 137: observed MAE=0.191486 validation MAE=0.523747,rank=5\n",
      "[SoftImpute] Iter 138: observed MAE=0.191634 validation MAE=0.524178,rank=5\n",
      "[SoftImpute] Iter 139: observed MAE=0.191647 validation MAE=0.523509,rank=5\n",
      "[SoftImpute] Iter 140: observed MAE=0.191667 validation MAE=0.522743,rank=5\n",
      "[SoftImpute] Iter 141: observed MAE=0.190821 validation MAE=0.522321,rank=5\n",
      "[SoftImpute] Iter 142: observed MAE=0.190821 validation MAE=0.522070,rank=5\n",
      "[SoftImpute] Iter 143: observed MAE=0.190260 validation MAE=0.522347,rank=5\n",
      "[SoftImpute] Iter 144: observed MAE=0.189538 validation MAE=0.522020,rank=5\n",
      "[SoftImpute] Iter 145: observed MAE=0.189484 validation MAE=0.522133,rank=5\n",
      "[SoftImpute] Iter 146: observed MAE=0.189457 validation MAE=0.521308,rank=5\n",
      "[SoftImpute] Iter 147: observed MAE=0.189040 validation MAE=0.520752,rank=5\n",
      "[SoftImpute] Iter 148: observed MAE=0.188795 validation MAE=0.521007,rank=5\n",
      "[SoftImpute] Iter 149: observed MAE=0.189160 validation MAE=0.520400,rank=5\n",
      "[SoftImpute] Iter 150: observed MAE=0.188551 validation MAE=0.520756,rank=5\n",
      "[SoftImpute] Iter 151: observed MAE=0.188236 validation MAE=0.519950,rank=5\n",
      "[SoftImpute] Iter 152: observed MAE=0.188270 validation MAE=0.519949,rank=5\n",
      "[SoftImpute] Iter 153: observed MAE=0.187629 validation MAE=0.519782,rank=5\n",
      "[SoftImpute] Iter 154: observed MAE=0.187765 validation MAE=0.519566,rank=5\n",
      "[SoftImpute] Iter 155: observed MAE=0.187678 validation MAE=0.519136,rank=5\n",
      "[SoftImpute] Iter 156: observed MAE=0.187698 validation MAE=0.519109,rank=5\n",
      "[SoftImpute] Iter 157: observed MAE=0.187596 validation MAE=0.518792,rank=5\n",
      "[SoftImpute] Iter 158: observed MAE=0.187521 validation MAE=0.518501,rank=5\n",
      "[SoftImpute] Iter 159: observed MAE=0.187202 validation MAE=0.518073,rank=5\n",
      "[SoftImpute] Iter 160: observed MAE=0.186780 validation MAE=0.518269,rank=5\n",
      "[SoftImpute] Iter 161: observed MAE=0.186441 validation MAE=0.517128,rank=5\n",
      "[SoftImpute] Iter 162: observed MAE=0.186831 validation MAE=0.517587,rank=5\n",
      "[SoftImpute] Iter 163: observed MAE=0.185744 validation MAE=0.516899,rank=5\n",
      "[SoftImpute] Iter 164: observed MAE=0.185859 validation MAE=0.516515,rank=5\n",
      "[SoftImpute] Iter 165: observed MAE=0.185585 validation MAE=0.516533,rank=5\n",
      "[SoftImpute] Iter 166: observed MAE=0.185119 validation MAE=0.515556,rank=5\n",
      "[SoftImpute] Iter 167: observed MAE=0.184868 validation MAE=0.515884,rank=5\n",
      "[SoftImpute] Iter 168: observed MAE=0.184018 validation MAE=0.515144,rank=5\n",
      "[SoftImpute] Iter 169: observed MAE=0.184076 validation MAE=0.515252,rank=5\n",
      "[SoftImpute] Iter 170: observed MAE=0.184526 validation MAE=0.515310,rank=5\n",
      "[SoftImpute] Iter 171: observed MAE=0.184673 validation MAE=0.514934,rank=5\n",
      "[SoftImpute] Iter 172: observed MAE=0.184270 validation MAE=0.514414,rank=5\n",
      "[SoftImpute] Iter 173: observed MAE=0.183547 validation MAE=0.513907,rank=5\n",
      "[SoftImpute] Iter 174: observed MAE=0.183684 validation MAE=0.513530,rank=5\n",
      "[SoftImpute] Iter 175: observed MAE=0.183322 validation MAE=0.513235,rank=5\n",
      "[SoftImpute] Iter 176: observed MAE=0.183277 validation MAE=0.512508,rank=5\n",
      "[SoftImpute] Iter 177: observed MAE=0.183657 validation MAE=0.513045,rank=5\n",
      "[SoftImpute] Iter 178: observed MAE=0.183614 validation MAE=0.512849,rank=5\n",
      "[SoftImpute] Iter 179: observed MAE=0.183072 validation MAE=0.512223,rank=5\n",
      "[SoftImpute] Iter 180: observed MAE=0.182338 validation MAE=0.511433,rank=5\n",
      "[SoftImpute] Iter 181: observed MAE=0.181954 validation MAE=0.511764,rank=5\n",
      "[SoftImpute] Iter 182: observed MAE=0.182400 validation MAE=0.511290,rank=5\n",
      "[SoftImpute] Iter 183: observed MAE=0.182591 validation MAE=0.510412,rank=5\n",
      "[SoftImpute] Iter 184: observed MAE=0.182703 validation MAE=0.510362,rank=5\n",
      "[SoftImpute] Iter 185: observed MAE=0.181888 validation MAE=0.510359,rank=5\n",
      "[SoftImpute] Iter 186: observed MAE=0.181672 validation MAE=0.510273,rank=5\n",
      "[SoftImpute] Iter 187: observed MAE=0.181379 validation MAE=0.509172,rank=5\n",
      "[SoftImpute] Iter 188: observed MAE=0.181347 validation MAE=0.509157,rank=5\n",
      "[SoftImpute] Iter 189: observed MAE=0.181158 validation MAE=0.508828,rank=5\n",
      "[SoftImpute] Iter 190: observed MAE=0.181116 validation MAE=0.508704,rank=5\n",
      "[SoftImpute] Iter 191: observed MAE=0.180899 validation MAE=0.508286,rank=5\n",
      "[SoftImpute] Iter 192: observed MAE=0.180531 validation MAE=0.508319,rank=5\n",
      "[SoftImpute] Iter 193: observed MAE=0.180438 validation MAE=0.507798,rank=5\n",
      "[SoftImpute] Iter 194: observed MAE=0.180530 validation MAE=0.507438,rank=5\n",
      "[SoftImpute] Iter 195: observed MAE=0.180577 validation MAE=0.507100,rank=5\n",
      "[SoftImpute] Iter 196: observed MAE=0.180357 validation MAE=0.507283,rank=5\n",
      "[SoftImpute] Iter 197: observed MAE=0.180002 validation MAE=0.506814,rank=5\n",
      "[SoftImpute] Iter 198: observed MAE=0.179596 validation MAE=0.506168,rank=5\n",
      "[SoftImpute] Iter 199: observed MAE=0.179505 validation MAE=0.506200,rank=5\n",
      "[SoftImpute] Iter 200: observed MAE=0.179133 validation MAE=0.506240,rank=5\n",
      "[SoftImpute] Stopped after iteration 200 for lambda=0.390892\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 6.020052909851074\n",
      "After the matrix factor stage, training error is 0.17913, validation error is 0.50624\n",
      "7\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.08023, val loss: 4.06692\n",
      "Main effects training epoch: 2, train loss: 3.91747, val loss: 3.90472\n",
      "Main effects training epoch: 3, train loss: 3.69655, val loss: 3.70213\n",
      "Main effects training epoch: 4, train loss: 3.49976, val loss: 3.53258\n",
      "Main effects training epoch: 5, train loss: 3.44905, val loss: 3.49241\n",
      "Main effects training epoch: 6, train loss: 3.37150, val loss: 3.41970\n",
      "Main effects training epoch: 7, train loss: 3.21099, val loss: 3.26654\n",
      "Main effects training epoch: 8, train loss: 3.29749, val loss: 3.35707\n",
      "Main effects training epoch: 9, train loss: 3.27218, val loss: 3.31944\n",
      "Main effects training epoch: 10, train loss: 3.21720, val loss: 3.26377\n",
      "Main effects training epoch: 11, train loss: 3.15324, val loss: 3.20345\n",
      "Main effects training epoch: 12, train loss: 3.01899, val loss: 3.06751\n",
      "Main effects training epoch: 13, train loss: 2.99563, val loss: 3.04072\n",
      "Main effects training epoch: 14, train loss: 2.95042, val loss: 2.99413\n",
      "Main effects training epoch: 15, train loss: 2.89355, val loss: 2.94190\n",
      "Main effects training epoch: 16, train loss: 2.82609, val loss: 2.87957\n",
      "Main effects training epoch: 17, train loss: 2.71368, val loss: 2.75370\n",
      "Main effects training epoch: 18, train loss: 2.67188, val loss: 2.70853\n",
      "Main effects training epoch: 19, train loss: 2.60314, val loss: 2.64950\n",
      "Main effects training epoch: 20, train loss: 2.57826, val loss: 2.63096\n",
      "Main effects training epoch: 21, train loss: 2.56056, val loss: 2.60516\n",
      "Main effects training epoch: 22, train loss: 2.46890, val loss: 2.51777\n",
      "Main effects training epoch: 23, train loss: 2.44056, val loss: 2.48364\n",
      "Main effects training epoch: 24, train loss: 2.37587, val loss: 2.42143\n",
      "Main effects training epoch: 25, train loss: 2.38448, val loss: 2.42406\n",
      "Main effects training epoch: 26, train loss: 2.30990, val loss: 2.35127\n",
      "Main effects training epoch: 27, train loss: 2.29660, val loss: 2.33365\n",
      "Main effects training epoch: 28, train loss: 2.26765, val loss: 2.30284\n",
      "Main effects training epoch: 29, train loss: 2.24566, val loss: 2.28235\n",
      "Main effects training epoch: 30, train loss: 2.21619, val loss: 2.25312\n",
      "Main effects training epoch: 31, train loss: 2.16091, val loss: 2.20150\n",
      "Main effects training epoch: 32, train loss: 2.17710, val loss: 2.21702\n",
      "Main effects training epoch: 33, train loss: 2.10534, val loss: 2.13736\n",
      "Main effects training epoch: 34, train loss: 2.12455, val loss: 2.16672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 35, train loss: 2.09543, val loss: 2.13239\n",
      "Main effects training epoch: 36, train loss: 2.06823, val loss: 2.11102\n",
      "Main effects training epoch: 37, train loss: 2.05404, val loss: 2.09582\n",
      "Main effects training epoch: 38, train loss: 2.02496, val loss: 2.06510\n",
      "Main effects training epoch: 39, train loss: 2.00780, val loss: 2.04940\n",
      "Main effects training epoch: 40, train loss: 2.00743, val loss: 2.05001\n",
      "Main effects training epoch: 41, train loss: 1.95740, val loss: 1.99996\n",
      "Main effects training epoch: 42, train loss: 1.97382, val loss: 2.01981\n",
      "Main effects training epoch: 43, train loss: 1.95737, val loss: 1.99896\n",
      "Main effects training epoch: 44, train loss: 1.90874, val loss: 1.95092\n",
      "Main effects training epoch: 45, train loss: 1.92221, val loss: 1.96690\n",
      "Main effects training epoch: 46, train loss: 1.91214, val loss: 1.95892\n",
      "Main effects training epoch: 47, train loss: 1.87975, val loss: 1.92009\n",
      "Main effects training epoch: 48, train loss: 1.87656, val loss: 1.92295\n",
      "Main effects training epoch: 49, train loss: 1.87551, val loss: 1.91692\n",
      "Main effects training epoch: 50, train loss: 1.85707, val loss: 1.90067\n",
      "Main effects training epoch: 51, train loss: 1.83814, val loss: 1.87912\n",
      "Main effects training epoch: 52, train loss: 1.83907, val loss: 1.88427\n",
      "Main effects training epoch: 53, train loss: 1.82373, val loss: 1.86445\n",
      "Main effects training epoch: 54, train loss: 1.81987, val loss: 1.86108\n",
      "Main effects training epoch: 55, train loss: 1.81047, val loss: 1.85192\n",
      "Main effects training epoch: 56, train loss: 1.81066, val loss: 1.84852\n",
      "Main effects training epoch: 57, train loss: 1.78532, val loss: 1.82983\n",
      "Main effects training epoch: 58, train loss: 1.80078, val loss: 1.84428\n",
      "Main effects training epoch: 59, train loss: 1.78526, val loss: 1.82446\n",
      "Main effects training epoch: 60, train loss: 1.76641, val loss: 1.80986\n",
      "Main effects training epoch: 61, train loss: 1.77669, val loss: 1.81656\n",
      "Main effects training epoch: 62, train loss: 1.76455, val loss: 1.80748\n",
      "Main effects training epoch: 63, train loss: 1.76586, val loss: 1.80480\n",
      "Main effects training epoch: 64, train loss: 1.75079, val loss: 1.78544\n",
      "Main effects training epoch: 65, train loss: 1.75497, val loss: 1.79871\n",
      "Main effects training epoch: 66, train loss: 1.75294, val loss: 1.79629\n",
      "Main effects training epoch: 67, train loss: 1.75840, val loss: 1.79862\n",
      "Main effects training epoch: 68, train loss: 1.74420, val loss: 1.78579\n",
      "Main effects training epoch: 69, train loss: 1.73862, val loss: 1.78141\n",
      "Main effects training epoch: 70, train loss: 1.73636, val loss: 1.77178\n",
      "Main effects training epoch: 71, train loss: 1.73267, val loss: 1.77597\n",
      "Main effects training epoch: 72, train loss: 1.73045, val loss: 1.76676\n",
      "Main effects training epoch: 73, train loss: 1.72820, val loss: 1.76822\n",
      "Main effects training epoch: 74, train loss: 1.72804, val loss: 1.76659\n",
      "Main effects training epoch: 75, train loss: 1.72650, val loss: 1.76759\n",
      "Main effects training epoch: 76, train loss: 1.71693, val loss: 1.75688\n",
      "Main effects training epoch: 77, train loss: 1.72182, val loss: 1.75311\n",
      "Main effects training epoch: 78, train loss: 1.71905, val loss: 1.76248\n",
      "Main effects training epoch: 79, train loss: 1.71142, val loss: 1.75614\n",
      "Main effects training epoch: 80, train loss: 1.71326, val loss: 1.74811\n",
      "Main effects training epoch: 81, train loss: 1.71325, val loss: 1.76089\n",
      "Main effects training epoch: 82, train loss: 1.71299, val loss: 1.74392\n",
      "Main effects training epoch: 83, train loss: 1.71300, val loss: 1.75613\n",
      "Main effects training epoch: 84, train loss: 1.70182, val loss: 1.74368\n",
      "Main effects training epoch: 85, train loss: 1.69693, val loss: 1.74175\n",
      "Main effects training epoch: 86, train loss: 1.69426, val loss: 1.73586\n",
      "Main effects training epoch: 87, train loss: 1.69138, val loss: 1.73671\n",
      "Main effects training epoch: 88, train loss: 1.68696, val loss: 1.73982\n",
      "Main effects training epoch: 89, train loss: 1.68666, val loss: 1.72886\n",
      "Main effects training epoch: 90, train loss: 1.67672, val loss: 1.72370\n",
      "Main effects training epoch: 91, train loss: 1.67296, val loss: 1.71649\n",
      "Main effects training epoch: 92, train loss: 1.67145, val loss: 1.71906\n",
      "Main effects training epoch: 93, train loss: 1.67330, val loss: 1.72836\n",
      "Main effects training epoch: 94, train loss: 1.66787, val loss: 1.70481\n",
      "Main effects training epoch: 95, train loss: 1.65208, val loss: 1.70486\n",
      "Main effects training epoch: 96, train loss: 1.65445, val loss: 1.70096\n",
      "Main effects training epoch: 97, train loss: 1.64886, val loss: 1.68796\n",
      "Main effects training epoch: 98, train loss: 1.65515, val loss: 1.70611\n",
      "Main effects training epoch: 99, train loss: 1.64379, val loss: 1.68468\n",
      "Main effects training epoch: 100, train loss: 1.63851, val loss: 1.68755\n",
      "Main effects training epoch: 101, train loss: 1.62556, val loss: 1.67935\n",
      "Main effects training epoch: 102, train loss: 1.63260, val loss: 1.66559\n",
      "Main effects training epoch: 103, train loss: 1.63343, val loss: 1.69553\n",
      "Main effects training epoch: 104, train loss: 1.62331, val loss: 1.67485\n",
      "Main effects training epoch: 105, train loss: 1.61824, val loss: 1.66324\n",
      "Main effects training epoch: 106, train loss: 1.61832, val loss: 1.66498\n",
      "Main effects training epoch: 107, train loss: 1.61719, val loss: 1.66856\n",
      "Main effects training epoch: 108, train loss: 1.61960, val loss: 1.66871\n",
      "Main effects training epoch: 109, train loss: 1.62646, val loss: 1.66648\n",
      "Main effects training epoch: 110, train loss: 1.61634, val loss: 1.66874\n",
      "Main effects training epoch: 111, train loss: 1.61572, val loss: 1.64937\n",
      "Main effects training epoch: 112, train loss: 1.61820, val loss: 1.66502\n",
      "Main effects training epoch: 113, train loss: 1.61405, val loss: 1.65764\n",
      "Main effects training epoch: 114, train loss: 1.61377, val loss: 1.65665\n",
      "Main effects training epoch: 115, train loss: 1.61279, val loss: 1.65849\n",
      "Main effects training epoch: 116, train loss: 1.61296, val loss: 1.65316\n",
      "Main effects training epoch: 117, train loss: 1.61156, val loss: 1.65040\n",
      "Main effects training epoch: 118, train loss: 1.61109, val loss: 1.65447\n",
      "Main effects training epoch: 119, train loss: 1.61335, val loss: 1.65909\n",
      "Main effects training epoch: 120, train loss: 1.60842, val loss: 1.65228\n",
      "Main effects training epoch: 121, train loss: 1.60986, val loss: 1.65412\n",
      "Main effects training epoch: 122, train loss: 1.61145, val loss: 1.65981\n",
      "Main effects training epoch: 123, train loss: 1.60836, val loss: 1.64188\n",
      "Main effects training epoch: 124, train loss: 1.60909, val loss: 1.64684\n",
      "Main effects training epoch: 125, train loss: 1.60681, val loss: 1.64939\n",
      "Main effects training epoch: 126, train loss: 1.60651, val loss: 1.64988\n",
      "Main effects training epoch: 127, train loss: 1.60692, val loss: 1.65185\n",
      "Main effects training epoch: 128, train loss: 1.60688, val loss: 1.64630\n",
      "Main effects training epoch: 129, train loss: 1.60608, val loss: 1.64617\n",
      "Main effects training epoch: 130, train loss: 1.61026, val loss: 1.64655\n",
      "Main effects training epoch: 131, train loss: 1.60988, val loss: 1.66042\n",
      "Main effects training epoch: 132, train loss: 1.60615, val loss: 1.63950\n",
      "Main effects training epoch: 133, train loss: 1.60577, val loss: 1.64662\n",
      "Main effects training epoch: 134, train loss: 1.60584, val loss: 1.64997\n",
      "Main effects training epoch: 135, train loss: 1.60609, val loss: 1.64865\n",
      "Main effects training epoch: 136, train loss: 1.60519, val loss: 1.64338\n",
      "Main effects training epoch: 137, train loss: 1.60662, val loss: 1.64342\n",
      "Main effects training epoch: 138, train loss: 1.61121, val loss: 1.65048\n",
      "Main effects training epoch: 139, train loss: 1.60506, val loss: 1.64893\n",
      "Main effects training epoch: 140, train loss: 1.60491, val loss: 1.64299\n",
      "Main effects training epoch: 141, train loss: 1.60984, val loss: 1.66142\n",
      "Main effects training epoch: 142, train loss: 1.60686, val loss: 1.63954\n",
      "Main effects training epoch: 143, train loss: 1.60465, val loss: 1.64927\n",
      "Main effects training epoch: 144, train loss: 1.60543, val loss: 1.64612\n",
      "Main effects training epoch: 145, train loss: 1.60494, val loss: 1.64519\n",
      "Main effects training epoch: 146, train loss: 1.60508, val loss: 1.64728\n",
      "Main effects training epoch: 147, train loss: 1.60442, val loss: 1.63910\n",
      "Main effects training epoch: 148, train loss: 1.60464, val loss: 1.64544\n",
      "Main effects training epoch: 149, train loss: 1.60663, val loss: 1.64316\n",
      "Main effects training epoch: 150, train loss: 1.60452, val loss: 1.64451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 151, train loss: 1.61247, val loss: 1.65548\n",
      "Main effects training epoch: 152, train loss: 1.62080, val loss: 1.65587\n",
      "Main effects training epoch: 153, train loss: 1.61054, val loss: 1.64695\n",
      "Main effects training epoch: 154, train loss: 1.60448, val loss: 1.64049\n",
      "Main effects training epoch: 155, train loss: 1.60209, val loss: 1.64615\n",
      "Main effects training epoch: 156, train loss: 1.60599, val loss: 1.63969\n",
      "Main effects training epoch: 157, train loss: 1.61165, val loss: 1.65662\n",
      "Main effects training epoch: 158, train loss: 1.60796, val loss: 1.64219\n",
      "Main effects training epoch: 159, train loss: 1.60187, val loss: 1.64192\n",
      "Main effects training epoch: 160, train loss: 1.60318, val loss: 1.63912\n",
      "Main effects training epoch: 161, train loss: 1.60367, val loss: 1.64712\n",
      "Main effects training epoch: 162, train loss: 1.60312, val loss: 1.64266\n",
      "Main effects training epoch: 163, train loss: 1.61037, val loss: 1.64363\n",
      "Main effects training epoch: 164, train loss: 1.60527, val loss: 1.65845\n",
      "Main effects training epoch: 165, train loss: 1.60247, val loss: 1.63723\n",
      "Main effects training epoch: 166, train loss: 1.60314, val loss: 1.64545\n",
      "Main effects training epoch: 167, train loss: 1.60230, val loss: 1.63502\n",
      "Main effects training epoch: 168, train loss: 1.60074, val loss: 1.63717\n",
      "Main effects training epoch: 169, train loss: 1.60309, val loss: 1.63956\n",
      "Main effects training epoch: 170, train loss: 1.60135, val loss: 1.64161\n",
      "Main effects training epoch: 171, train loss: 1.60543, val loss: 1.64209\n",
      "Main effects training epoch: 172, train loss: 1.60972, val loss: 1.64301\n",
      "Main effects training epoch: 173, train loss: 1.61001, val loss: 1.65333\n",
      "Main effects training epoch: 174, train loss: 1.60193, val loss: 1.64300\n",
      "Main effects training epoch: 175, train loss: 1.59912, val loss: 1.63072\n",
      "Main effects training epoch: 176, train loss: 1.59825, val loss: 1.63570\n",
      "Main effects training epoch: 177, train loss: 1.59794, val loss: 1.63104\n",
      "Main effects training epoch: 178, train loss: 1.59960, val loss: 1.64562\n",
      "Main effects training epoch: 179, train loss: 1.60175, val loss: 1.62933\n",
      "Main effects training epoch: 180, train loss: 1.60745, val loss: 1.64426\n",
      "Main effects training epoch: 181, train loss: 1.61207, val loss: 1.64070\n",
      "Main effects training epoch: 182, train loss: 1.60025, val loss: 1.63916\n",
      "Main effects training epoch: 183, train loss: 1.60043, val loss: 1.63625\n",
      "Main effects training epoch: 184, train loss: 1.59764, val loss: 1.62738\n",
      "Main effects training epoch: 185, train loss: 1.59823, val loss: 1.63871\n",
      "Main effects training epoch: 186, train loss: 1.59671, val loss: 1.62759\n",
      "Main effects training epoch: 187, train loss: 1.59645, val loss: 1.62899\n",
      "Main effects training epoch: 188, train loss: 1.59382, val loss: 1.62539\n",
      "Main effects training epoch: 189, train loss: 1.59708, val loss: 1.62704\n",
      "Main effects training epoch: 190, train loss: 1.59585, val loss: 1.62856\n",
      "Main effects training epoch: 191, train loss: 1.59399, val loss: 1.61985\n",
      "Main effects training epoch: 192, train loss: 1.59488, val loss: 1.63571\n",
      "Main effects training epoch: 193, train loss: 1.59424, val loss: 1.61933\n",
      "Main effects training epoch: 194, train loss: 1.59669, val loss: 1.63961\n",
      "Main effects training epoch: 195, train loss: 1.59509, val loss: 1.61774\n",
      "Main effects training epoch: 196, train loss: 1.59315, val loss: 1.63380\n",
      "Main effects training epoch: 197, train loss: 1.59313, val loss: 1.61764\n",
      "Main effects training epoch: 198, train loss: 1.59128, val loss: 1.62601\n",
      "Main effects training epoch: 199, train loss: 1.58986, val loss: 1.62416\n",
      "Main effects training epoch: 200, train loss: 1.58820, val loss: 1.61606\n",
      "Main effects training epoch: 201, train loss: 1.58582, val loss: 1.61758\n",
      "Main effects training epoch: 202, train loss: 1.59521, val loss: 1.62567\n",
      "Main effects training epoch: 203, train loss: 1.59286, val loss: 1.63174\n",
      "Main effects training epoch: 204, train loss: 1.58867, val loss: 1.61094\n",
      "Main effects training epoch: 205, train loss: 1.58620, val loss: 1.62061\n",
      "Main effects training epoch: 206, train loss: 1.58421, val loss: 1.61396\n",
      "Main effects training epoch: 207, train loss: 1.58149, val loss: 1.61522\n",
      "Main effects training epoch: 208, train loss: 1.57956, val loss: 1.61273\n",
      "Main effects training epoch: 209, train loss: 1.58472, val loss: 1.61717\n",
      "Main effects training epoch: 210, train loss: 1.57904, val loss: 1.60913\n",
      "Main effects training epoch: 211, train loss: 1.58406, val loss: 1.62117\n",
      "Main effects training epoch: 212, train loss: 1.57638, val loss: 1.61144\n",
      "Main effects training epoch: 213, train loss: 1.57724, val loss: 1.61353\n",
      "Main effects training epoch: 214, train loss: 1.57420, val loss: 1.61444\n",
      "Main effects training epoch: 215, train loss: 1.58017, val loss: 1.61610\n",
      "Main effects training epoch: 216, train loss: 1.57091, val loss: 1.60853\n",
      "Main effects training epoch: 217, train loss: 1.56728, val loss: 1.60953\n",
      "Main effects training epoch: 218, train loss: 1.57122, val loss: 1.61745\n",
      "Main effects training epoch: 219, train loss: 1.56421, val loss: 1.60196\n",
      "Main effects training epoch: 220, train loss: 1.56627, val loss: 1.61498\n",
      "Main effects training epoch: 221, train loss: 1.56472, val loss: 1.61068\n",
      "Main effects training epoch: 222, train loss: 1.57630, val loss: 1.61972\n",
      "Main effects training epoch: 223, train loss: 1.56402, val loss: 1.60564\n",
      "Main effects training epoch: 224, train loss: 1.55937, val loss: 1.61068\n",
      "Main effects training epoch: 225, train loss: 1.55940, val loss: 1.60640\n",
      "Main effects training epoch: 226, train loss: 1.55847, val loss: 1.60462\n",
      "Main effects training epoch: 227, train loss: 1.55458, val loss: 1.60559\n",
      "Main effects training epoch: 228, train loss: 1.55675, val loss: 1.59962\n",
      "Main effects training epoch: 229, train loss: 1.55795, val loss: 1.60864\n",
      "Main effects training epoch: 230, train loss: 1.55752, val loss: 1.60469\n",
      "Main effects training epoch: 231, train loss: 1.55979, val loss: 1.60224\n",
      "Main effects training epoch: 232, train loss: 1.56066, val loss: 1.60961\n",
      "Main effects training epoch: 233, train loss: 1.55675, val loss: 1.59993\n",
      "Main effects training epoch: 234, train loss: 1.55569, val loss: 1.60876\n",
      "Main effects training epoch: 235, train loss: 1.55148, val loss: 1.59656\n",
      "Main effects training epoch: 236, train loss: 1.55613, val loss: 1.60474\n",
      "Main effects training epoch: 237, train loss: 1.55111, val loss: 1.60099\n",
      "Main effects training epoch: 238, train loss: 1.55001, val loss: 1.59467\n",
      "Main effects training epoch: 239, train loss: 1.54642, val loss: 1.59575\n",
      "Main effects training epoch: 240, train loss: 1.54801, val loss: 1.59448\n",
      "Main effects training epoch: 241, train loss: 1.55012, val loss: 1.60754\n",
      "Main effects training epoch: 242, train loss: 1.55193, val loss: 1.59050\n",
      "Main effects training epoch: 243, train loss: 1.54895, val loss: 1.60973\n",
      "Main effects training epoch: 244, train loss: 1.55650, val loss: 1.59957\n",
      "Main effects training epoch: 245, train loss: 1.54693, val loss: 1.59665\n",
      "Main effects training epoch: 246, train loss: 1.55260, val loss: 1.59522\n",
      "Main effects training epoch: 247, train loss: 1.54993, val loss: 1.60736\n",
      "Main effects training epoch: 248, train loss: 1.54733, val loss: 1.59083\n",
      "Main effects training epoch: 249, train loss: 1.54445, val loss: 1.59251\n",
      "Main effects training epoch: 250, train loss: 1.54652, val loss: 1.60146\n",
      "Main effects training epoch: 251, train loss: 1.54806, val loss: 1.57769\n",
      "Main effects training epoch: 252, train loss: 1.54650, val loss: 1.60270\n",
      "Main effects training epoch: 253, train loss: 1.54698, val loss: 1.59255\n",
      "Main effects training epoch: 254, train loss: 1.54455, val loss: 1.58671\n",
      "Main effects training epoch: 255, train loss: 1.54897, val loss: 1.60381\n",
      "Main effects training epoch: 256, train loss: 1.54336, val loss: 1.58685\n",
      "Main effects training epoch: 257, train loss: 1.54897, val loss: 1.59884\n",
      "Main effects training epoch: 258, train loss: 1.54131, val loss: 1.59164\n",
      "Main effects training epoch: 259, train loss: 1.54959, val loss: 1.58247\n",
      "Main effects training epoch: 260, train loss: 1.54205, val loss: 1.59876\n",
      "Main effects training epoch: 261, train loss: 1.53981, val loss: 1.58843\n",
      "Main effects training epoch: 262, train loss: 1.54617, val loss: 1.58845\n",
      "Main effects training epoch: 263, train loss: 1.54395, val loss: 1.60598\n",
      "Main effects training epoch: 264, train loss: 1.54182, val loss: 1.58603\n",
      "Main effects training epoch: 265, train loss: 1.53981, val loss: 1.58803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 266, train loss: 1.54439, val loss: 1.59087\n",
      "Main effects training epoch: 267, train loss: 1.54658, val loss: 1.60148\n",
      "Main effects training epoch: 268, train loss: 1.54528, val loss: 1.58884\n",
      "Main effects training epoch: 269, train loss: 1.54003, val loss: 1.58755\n",
      "Main effects training epoch: 270, train loss: 1.53740, val loss: 1.58156\n",
      "Main effects training epoch: 271, train loss: 1.53750, val loss: 1.58618\n",
      "Main effects training epoch: 272, train loss: 1.53597, val loss: 1.58416\n",
      "Main effects training epoch: 273, train loss: 1.53498, val loss: 1.58458\n",
      "Main effects training epoch: 274, train loss: 1.53456, val loss: 1.58354\n",
      "Main effects training epoch: 275, train loss: 1.53853, val loss: 1.58697\n",
      "Main effects training epoch: 276, train loss: 1.53755, val loss: 1.58141\n",
      "Main effects training epoch: 277, train loss: 1.53579, val loss: 1.58933\n",
      "Main effects training epoch: 278, train loss: 1.53806, val loss: 1.58013\n",
      "Main effects training epoch: 279, train loss: 1.53137, val loss: 1.58154\n",
      "Main effects training epoch: 280, train loss: 1.53653, val loss: 1.58910\n",
      "Main effects training epoch: 281, train loss: 1.53934, val loss: 1.58860\n",
      "Main effects training epoch: 282, train loss: 1.53775, val loss: 1.59802\n",
      "Main effects training epoch: 283, train loss: 1.53616, val loss: 1.57512\n",
      "Main effects training epoch: 284, train loss: 1.53649, val loss: 1.58776\n",
      "Main effects training epoch: 285, train loss: 1.54432, val loss: 1.59109\n",
      "Main effects training epoch: 286, train loss: 1.53680, val loss: 1.58688\n",
      "Main effects training epoch: 287, train loss: 1.53306, val loss: 1.58049\n",
      "Main effects training epoch: 288, train loss: 1.53794, val loss: 1.57744\n",
      "Main effects training epoch: 289, train loss: 1.53373, val loss: 1.58699\n",
      "Main effects training epoch: 290, train loss: 1.52802, val loss: 1.57401\n",
      "Main effects training epoch: 291, train loss: 1.53592, val loss: 1.57540\n",
      "Main effects training epoch: 292, train loss: 1.53026, val loss: 1.57892\n",
      "Main effects training epoch: 293, train loss: 1.53131, val loss: 1.57519\n",
      "Main effects training epoch: 294, train loss: 1.53494, val loss: 1.57508\n",
      "Main effects training epoch: 295, train loss: 1.52974, val loss: 1.57457\n",
      "Main effects training epoch: 296, train loss: 1.53813, val loss: 1.60192\n",
      "Main effects training epoch: 297, train loss: 1.53294, val loss: 1.57546\n",
      "Main effects training epoch: 298, train loss: 1.53346, val loss: 1.57497\n",
      "Main effects training epoch: 299, train loss: 1.53105, val loss: 1.58288\n",
      "Main effects training epoch: 300, train loss: 1.53234, val loss: 1.57522\n",
      "##########Stage 1: main effect training stop.##########\n",
      "2 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.53372, val loss: 1.58005\n",
      "Main effects tuning epoch: 2, train loss: 1.55382, val loss: 1.60997\n",
      "Main effects tuning epoch: 3, train loss: 1.54882, val loss: 1.56650\n",
      "Main effects tuning epoch: 4, train loss: 1.54734, val loss: 1.59852\n",
      "Main effects tuning epoch: 5, train loss: 1.56563, val loss: 1.60621\n",
      "Main effects tuning epoch: 6, train loss: 1.54720, val loss: 1.58499\n",
      "Main effects tuning epoch: 7, train loss: 1.53145, val loss: 1.56707\n",
      "Main effects tuning epoch: 8, train loss: 1.54553, val loss: 1.59603\n",
      "Main effects tuning epoch: 9, train loss: 1.52802, val loss: 1.57149\n",
      "Main effects tuning epoch: 10, train loss: 1.52962, val loss: 1.56954\n",
      "Main effects tuning epoch: 11, train loss: 1.52767, val loss: 1.56796\n",
      "Main effects tuning epoch: 12, train loss: 1.52836, val loss: 1.57441\n",
      "Main effects tuning epoch: 13, train loss: 1.52942, val loss: 1.57113\n",
      "Main effects tuning epoch: 14, train loss: 1.52811, val loss: 1.57056\n",
      "Main effects tuning epoch: 15, train loss: 1.53031, val loss: 1.58248\n",
      "Main effects tuning epoch: 16, train loss: 1.53643, val loss: 1.57260\n",
      "Main effects tuning epoch: 17, train loss: 1.53041, val loss: 1.56860\n",
      "Main effects tuning epoch: 18, train loss: 1.52731, val loss: 1.57602\n",
      "Main effects tuning epoch: 19, train loss: 1.54253, val loss: 1.56905\n",
      "Main effects tuning epoch: 20, train loss: 1.53048, val loss: 1.58324\n",
      "Main effects tuning epoch: 21, train loss: 1.52820, val loss: 1.57500\n",
      "Main effects tuning epoch: 22, train loss: 1.52882, val loss: 1.56480\n",
      "Main effects tuning epoch: 23, train loss: 1.52613, val loss: 1.56677\n",
      "Main effects tuning epoch: 24, train loss: 1.52682, val loss: 1.56902\n",
      "Main effects tuning epoch: 25, train loss: 1.52279, val loss: 1.56281\n",
      "Main effects tuning epoch: 26, train loss: 1.53177, val loss: 1.57640\n",
      "Main effects tuning epoch: 27, train loss: 1.52443, val loss: 1.55962\n",
      "Main effects tuning epoch: 28, train loss: 1.53040, val loss: 1.57927\n",
      "Main effects tuning epoch: 29, train loss: 1.52752, val loss: 1.56703\n",
      "Main effects tuning epoch: 30, train loss: 1.52365, val loss: 1.56713\n",
      "Main effects tuning epoch: 31, train loss: 1.52480, val loss: 1.56274\n",
      "Main effects tuning epoch: 32, train loss: 1.52180, val loss: 1.55856\n",
      "Main effects tuning epoch: 33, train loss: 1.52053, val loss: 1.56302\n",
      "Main effects tuning epoch: 34, train loss: 1.52352, val loss: 1.56185\n",
      "Main effects tuning epoch: 35, train loss: 1.51936, val loss: 1.55568\n",
      "Main effects tuning epoch: 36, train loss: 1.52395, val loss: 1.57088\n",
      "Main effects tuning epoch: 37, train loss: 1.52465, val loss: 1.55976\n",
      "Main effects tuning epoch: 38, train loss: 1.53335, val loss: 1.58718\n",
      "Main effects tuning epoch: 39, train loss: 1.52426, val loss: 1.55319\n",
      "Main effects tuning epoch: 40, train loss: 1.52149, val loss: 1.56244\n",
      "Main effects tuning epoch: 41, train loss: 1.52194, val loss: 1.57033\n",
      "Main effects tuning epoch: 42, train loss: 1.52014, val loss: 1.55211\n",
      "Main effects tuning epoch: 43, train loss: 1.51893, val loss: 1.56739\n",
      "Main effects tuning epoch: 44, train loss: 1.51902, val loss: 1.55292\n",
      "Main effects tuning epoch: 45, train loss: 1.51597, val loss: 1.55145\n",
      "Main effects tuning epoch: 46, train loss: 1.51938, val loss: 1.56712\n",
      "Main effects tuning epoch: 47, train loss: 1.51943, val loss: 1.55975\n",
      "Main effects tuning epoch: 48, train loss: 1.51812, val loss: 1.55448\n",
      "Main effects tuning epoch: 49, train loss: 1.51508, val loss: 1.55361\n",
      "Main effects tuning epoch: 50, train loss: 1.51946, val loss: 1.55936\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.38997, val loss: 1.43326\n",
      "Interaction training epoch: 2, train loss: 1.13993, val loss: 1.15151\n",
      "Interaction training epoch: 3, train loss: 1.10395, val loss: 1.07639\n",
      "Interaction training epoch: 4, train loss: 1.07820, val loss: 1.10877\n",
      "Interaction training epoch: 5, train loss: 1.02149, val loss: 1.01929\n",
      "Interaction training epoch: 6, train loss: 1.01117, val loss: 1.02433\n",
      "Interaction training epoch: 7, train loss: 0.99387, val loss: 0.99383\n",
      "Interaction training epoch: 8, train loss: 0.98863, val loss: 0.97804\n",
      "Interaction training epoch: 9, train loss: 0.98006, val loss: 0.98939\n",
      "Interaction training epoch: 10, train loss: 0.98380, val loss: 0.97707\n",
      "Interaction training epoch: 11, train loss: 0.97737, val loss: 0.96959\n",
      "Interaction training epoch: 12, train loss: 0.97324, val loss: 0.97114\n",
      "Interaction training epoch: 13, train loss: 0.96809, val loss: 0.97215\n",
      "Interaction training epoch: 14, train loss: 0.96166, val loss: 0.95895\n",
      "Interaction training epoch: 15, train loss: 0.96027, val loss: 0.95768\n",
      "Interaction training epoch: 16, train loss: 0.96202, val loss: 0.97240\n",
      "Interaction training epoch: 17, train loss: 0.97356, val loss: 0.99075\n",
      "Interaction training epoch: 18, train loss: 0.96205, val loss: 0.96833\n",
      "Interaction training epoch: 19, train loss: 0.94066, val loss: 0.94138\n",
      "Interaction training epoch: 20, train loss: 0.92416, val loss: 0.92552\n",
      "Interaction training epoch: 21, train loss: 0.92451, val loss: 0.93027\n",
      "Interaction training epoch: 22, train loss: 0.93209, val loss: 0.94400\n",
      "Interaction training epoch: 23, train loss: 0.90942, val loss: 0.89715\n",
      "Interaction training epoch: 24, train loss: 0.93011, val loss: 0.94101\n",
      "Interaction training epoch: 25, train loss: 0.93542, val loss: 0.92746\n",
      "Interaction training epoch: 26, train loss: 0.90441, val loss: 0.91932\n",
      "Interaction training epoch: 27, train loss: 0.88928, val loss: 0.89695\n",
      "Interaction training epoch: 28, train loss: 0.87535, val loss: 0.86939\n",
      "Interaction training epoch: 29, train loss: 0.87498, val loss: 0.86499\n",
      "Interaction training epoch: 30, train loss: 0.89094, val loss: 0.89432\n",
      "Interaction training epoch: 31, train loss: 0.87445, val loss: 0.86872\n",
      "Interaction training epoch: 32, train loss: 0.85871, val loss: 0.86297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 33, train loss: 0.87242, val loss: 0.87105\n",
      "Interaction training epoch: 34, train loss: 0.85280, val loss: 0.85347\n",
      "Interaction training epoch: 35, train loss: 0.84882, val loss: 0.84831\n",
      "Interaction training epoch: 36, train loss: 0.84606, val loss: 0.84193\n",
      "Interaction training epoch: 37, train loss: 0.84120, val loss: 0.84829\n",
      "Interaction training epoch: 38, train loss: 0.84622, val loss: 0.86209\n",
      "Interaction training epoch: 39, train loss: 0.84282, val loss: 0.85127\n",
      "Interaction training epoch: 40, train loss: 0.84121, val loss: 0.83859\n",
      "Interaction training epoch: 41, train loss: 0.83788, val loss: 0.83927\n",
      "Interaction training epoch: 42, train loss: 0.83013, val loss: 0.84531\n",
      "Interaction training epoch: 43, train loss: 0.85468, val loss: 0.85628\n",
      "Interaction training epoch: 44, train loss: 0.83080, val loss: 0.83723\n",
      "Interaction training epoch: 45, train loss: 0.82380, val loss: 0.83119\n",
      "Interaction training epoch: 46, train loss: 0.82493, val loss: 0.83126\n",
      "Interaction training epoch: 47, train loss: 0.82841, val loss: 0.85253\n",
      "Interaction training epoch: 48, train loss: 0.81678, val loss: 0.80907\n",
      "Interaction training epoch: 49, train loss: 0.84935, val loss: 0.87612\n",
      "Interaction training epoch: 50, train loss: 0.82844, val loss: 0.82825\n",
      "Interaction training epoch: 51, train loss: 0.81502, val loss: 0.84041\n",
      "Interaction training epoch: 52, train loss: 0.81762, val loss: 0.82434\n",
      "Interaction training epoch: 53, train loss: 0.82406, val loss: 0.83977\n",
      "Interaction training epoch: 54, train loss: 0.81913, val loss: 0.83185\n",
      "Interaction training epoch: 55, train loss: 0.80939, val loss: 0.82547\n",
      "Interaction training epoch: 56, train loss: 0.82237, val loss: 0.81799\n",
      "Interaction training epoch: 57, train loss: 0.80729, val loss: 0.83375\n",
      "Interaction training epoch: 58, train loss: 0.79759, val loss: 0.80671\n",
      "Interaction training epoch: 59, train loss: 0.80297, val loss: 0.81356\n",
      "Interaction training epoch: 60, train loss: 0.80182, val loss: 0.82638\n",
      "Interaction training epoch: 61, train loss: 0.80946, val loss: 0.82170\n",
      "Interaction training epoch: 62, train loss: 0.80425, val loss: 0.82741\n",
      "Interaction training epoch: 63, train loss: 0.80045, val loss: 0.80943\n",
      "Interaction training epoch: 64, train loss: 0.81920, val loss: 0.82982\n",
      "Interaction training epoch: 65, train loss: 0.80668, val loss: 0.82364\n",
      "Interaction training epoch: 66, train loss: 0.80424, val loss: 0.81197\n",
      "Interaction training epoch: 67, train loss: 0.80137, val loss: 0.82496\n",
      "Interaction training epoch: 68, train loss: 0.80788, val loss: 0.82359\n",
      "Interaction training epoch: 69, train loss: 0.80103, val loss: 0.81281\n",
      "Interaction training epoch: 70, train loss: 0.79723, val loss: 0.81816\n",
      "Interaction training epoch: 71, train loss: 0.79117, val loss: 0.80891\n",
      "Interaction training epoch: 72, train loss: 0.79180, val loss: 0.81629\n",
      "Interaction training epoch: 73, train loss: 0.79527, val loss: 0.81553\n",
      "Interaction training epoch: 74, train loss: 0.79207, val loss: 0.80863\n",
      "Interaction training epoch: 75, train loss: 0.80007, val loss: 0.81445\n",
      "Interaction training epoch: 76, train loss: 0.79799, val loss: 0.81749\n",
      "Interaction training epoch: 77, train loss: 0.80003, val loss: 0.81841\n",
      "Interaction training epoch: 78, train loss: 0.80372, val loss: 0.82289\n",
      "Interaction training epoch: 79, train loss: 0.79268, val loss: 0.80274\n",
      "Interaction training epoch: 80, train loss: 0.80171, val loss: 0.83168\n",
      "Interaction training epoch: 81, train loss: 0.79386, val loss: 0.81042\n",
      "Interaction training epoch: 82, train loss: 0.79465, val loss: 0.80530\n",
      "Interaction training epoch: 83, train loss: 0.79653, val loss: 0.82563\n",
      "Interaction training epoch: 84, train loss: 0.81264, val loss: 0.81764\n",
      "Interaction training epoch: 85, train loss: 0.79608, val loss: 0.81657\n",
      "Interaction training epoch: 86, train loss: 0.80054, val loss: 0.81686\n",
      "Interaction training epoch: 87, train loss: 0.79877, val loss: 0.80542\n",
      "Interaction training epoch: 88, train loss: 0.79261, val loss: 0.82387\n",
      "Interaction training epoch: 89, train loss: 0.79039, val loss: 0.80292\n",
      "Interaction training epoch: 90, train loss: 0.78906, val loss: 0.81051\n",
      "Interaction training epoch: 91, train loss: 0.78665, val loss: 0.79887\n",
      "Interaction training epoch: 92, train loss: 0.79623, val loss: 0.81318\n",
      "Interaction training epoch: 93, train loss: 0.78772, val loss: 0.80808\n",
      "Interaction training epoch: 94, train loss: 0.78574, val loss: 0.80553\n",
      "Interaction training epoch: 95, train loss: 0.78134, val loss: 0.80017\n",
      "Interaction training epoch: 96, train loss: 0.78542, val loss: 0.80883\n",
      "Interaction training epoch: 97, train loss: 0.79330, val loss: 0.81283\n",
      "Interaction training epoch: 98, train loss: 0.79261, val loss: 0.81959\n",
      "Interaction training epoch: 99, train loss: 0.78187, val loss: 0.80238\n",
      "Interaction training epoch: 100, train loss: 0.79298, val loss: 0.80814\n",
      "Interaction training epoch: 101, train loss: 0.79213, val loss: 0.81055\n",
      "Interaction training epoch: 102, train loss: 0.78627, val loss: 0.81172\n",
      "Interaction training epoch: 103, train loss: 0.78608, val loss: 0.79685\n",
      "Interaction training epoch: 104, train loss: 0.78597, val loss: 0.81374\n",
      "Interaction training epoch: 105, train loss: 0.78514, val loss: 0.79611\n",
      "Interaction training epoch: 106, train loss: 0.78216, val loss: 0.80258\n",
      "Interaction training epoch: 107, train loss: 0.78335, val loss: 0.79970\n",
      "Interaction training epoch: 108, train loss: 0.80942, val loss: 0.83255\n",
      "Interaction training epoch: 109, train loss: 0.79472, val loss: 0.80983\n",
      "Interaction training epoch: 110, train loss: 0.78280, val loss: 0.80773\n",
      "Interaction training epoch: 111, train loss: 0.78065, val loss: 0.79725\n",
      "Interaction training epoch: 112, train loss: 0.77769, val loss: 0.80147\n",
      "Interaction training epoch: 113, train loss: 0.77667, val loss: 0.78754\n",
      "Interaction training epoch: 114, train loss: 0.77783, val loss: 0.79933\n",
      "Interaction training epoch: 115, train loss: 0.78150, val loss: 0.80393\n",
      "Interaction training epoch: 116, train loss: 0.79748, val loss: 0.81839\n",
      "Interaction training epoch: 117, train loss: 0.78967, val loss: 0.81588\n",
      "Interaction training epoch: 118, train loss: 0.78770, val loss: 0.81110\n",
      "Interaction training epoch: 119, train loss: 0.78526, val loss: 0.80040\n",
      "Interaction training epoch: 120, train loss: 0.81379, val loss: 0.83327\n",
      "Interaction training epoch: 121, train loss: 0.78954, val loss: 0.81321\n",
      "Interaction training epoch: 122, train loss: 0.78044, val loss: 0.80334\n",
      "Interaction training epoch: 123, train loss: 0.78234, val loss: 0.80263\n",
      "Interaction training epoch: 124, train loss: 0.78338, val loss: 0.80201\n",
      "Interaction training epoch: 125, train loss: 0.78293, val loss: 0.81103\n",
      "Interaction training epoch: 126, train loss: 0.78386, val loss: 0.79042\n",
      "Interaction training epoch: 127, train loss: 0.78846, val loss: 0.81153\n",
      "Interaction training epoch: 128, train loss: 0.79023, val loss: 0.79597\n",
      "Interaction training epoch: 129, train loss: 0.78744, val loss: 0.80987\n",
      "Interaction training epoch: 130, train loss: 0.78791, val loss: 0.80368\n",
      "Interaction training epoch: 131, train loss: 0.79076, val loss: 0.80657\n",
      "Interaction training epoch: 132, train loss: 0.78185, val loss: 0.79571\n",
      "Interaction training epoch: 133, train loss: 0.77943, val loss: 0.79976\n",
      "Interaction training epoch: 134, train loss: 0.78391, val loss: 0.79811\n",
      "Interaction training epoch: 135, train loss: 0.78569, val loss: 0.80431\n",
      "Interaction training epoch: 136, train loss: 0.78087, val loss: 0.79755\n",
      "Interaction training epoch: 137, train loss: 0.78070, val loss: 0.80086\n",
      "Interaction training epoch: 138, train loss: 0.78150, val loss: 0.79852\n",
      "Interaction training epoch: 139, train loss: 0.77948, val loss: 0.80006\n",
      "Interaction training epoch: 140, train loss: 0.78188, val loss: 0.79590\n",
      "Interaction training epoch: 141, train loss: 0.77668, val loss: 0.79492\n",
      "Interaction training epoch: 142, train loss: 0.77449, val loss: 0.79043\n",
      "Interaction training epoch: 143, train loss: 0.78179, val loss: 0.80126\n",
      "Interaction training epoch: 144, train loss: 0.78117, val loss: 0.80057\n",
      "Interaction training epoch: 145, train loss: 0.78722, val loss: 0.80725\n",
      "Interaction training epoch: 146, train loss: 0.78693, val loss: 0.80251\n",
      "Interaction training epoch: 147, train loss: 0.77791, val loss: 0.80425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 148, train loss: 0.77721, val loss: 0.79035\n",
      "Interaction training epoch: 149, train loss: 0.78218, val loss: 0.80862\n",
      "Interaction training epoch: 150, train loss: 0.77945, val loss: 0.79317\n",
      "Interaction training epoch: 151, train loss: 0.77728, val loss: 0.79785\n",
      "Interaction training epoch: 152, train loss: 0.77501, val loss: 0.79635\n",
      "Interaction training epoch: 153, train loss: 0.77552, val loss: 0.79386\n",
      "Interaction training epoch: 154, train loss: 0.77395, val loss: 0.78747\n",
      "Interaction training epoch: 155, train loss: 0.77933, val loss: 0.78696\n",
      "Interaction training epoch: 156, train loss: 0.78348, val loss: 0.80739\n",
      "Interaction training epoch: 157, train loss: 0.77525, val loss: 0.79169\n",
      "Interaction training epoch: 158, train loss: 0.77060, val loss: 0.79091\n",
      "Interaction training epoch: 159, train loss: 0.78391, val loss: 0.80244\n",
      "Interaction training epoch: 160, train loss: 0.78231, val loss: 0.80420\n",
      "Interaction training epoch: 161, train loss: 0.77744, val loss: 0.79210\n",
      "Interaction training epoch: 162, train loss: 0.77743, val loss: 0.78857\n",
      "Interaction training epoch: 163, train loss: 0.78047, val loss: 0.80091\n",
      "Interaction training epoch: 164, train loss: 0.77822, val loss: 0.79006\n",
      "Interaction training epoch: 165, train loss: 0.77481, val loss: 0.79161\n",
      "Interaction training epoch: 166, train loss: 0.78149, val loss: 0.79281\n",
      "Interaction training epoch: 167, train loss: 0.78912, val loss: 0.80265\n",
      "Interaction training epoch: 168, train loss: 0.77892, val loss: 0.79927\n",
      "Interaction training epoch: 169, train loss: 0.77354, val loss: 0.77922\n",
      "Interaction training epoch: 170, train loss: 0.77598, val loss: 0.79593\n",
      "Interaction training epoch: 171, train loss: 0.77957, val loss: 0.79825\n",
      "Interaction training epoch: 172, train loss: 0.77359, val loss: 0.78604\n",
      "Interaction training epoch: 173, train loss: 0.77757, val loss: 0.79459\n",
      "Interaction training epoch: 174, train loss: 0.77641, val loss: 0.79481\n",
      "Interaction training epoch: 175, train loss: 0.77381, val loss: 0.79085\n",
      "Interaction training epoch: 176, train loss: 0.78409, val loss: 0.81183\n",
      "Interaction training epoch: 177, train loss: 0.77726, val loss: 0.78109\n",
      "Interaction training epoch: 178, train loss: 0.78855, val loss: 0.80254\n",
      "Interaction training epoch: 179, train loss: 0.77692, val loss: 0.80232\n",
      "Interaction training epoch: 180, train loss: 0.77515, val loss: 0.79870\n",
      "Interaction training epoch: 181, train loss: 0.77965, val loss: 0.79107\n",
      "Interaction training epoch: 182, train loss: 0.77722, val loss: 0.80800\n",
      "Interaction training epoch: 183, train loss: 0.78020, val loss: 0.78676\n",
      "Interaction training epoch: 184, train loss: 0.77956, val loss: 0.80018\n",
      "Interaction training epoch: 185, train loss: 0.77442, val loss: 0.78791\n",
      "Interaction training epoch: 186, train loss: 0.77978, val loss: 0.80026\n",
      "Interaction training epoch: 187, train loss: 0.77958, val loss: 0.79616\n",
      "Interaction training epoch: 188, train loss: 0.77794, val loss: 0.78656\n",
      "Interaction training epoch: 189, train loss: 0.77516, val loss: 0.79755\n",
      "Interaction training epoch: 190, train loss: 0.77808, val loss: 0.79000\n",
      "Interaction training epoch: 191, train loss: 0.77803, val loss: 0.79260\n",
      "Interaction training epoch: 192, train loss: 0.77253, val loss: 0.79857\n",
      "Interaction training epoch: 193, train loss: 0.77578, val loss: 0.78493\n",
      "Interaction training epoch: 194, train loss: 0.77891, val loss: 0.80339\n",
      "Interaction training epoch: 195, train loss: 0.78255, val loss: 0.79421\n",
      "Interaction training epoch: 196, train loss: 0.77455, val loss: 0.79916\n",
      "Interaction training epoch: 197, train loss: 0.77575, val loss: 0.79440\n",
      "Interaction training epoch: 198, train loss: 0.77531, val loss: 0.78355\n",
      "Interaction training epoch: 199, train loss: 0.77996, val loss: 0.80998\n",
      "Interaction training epoch: 200, train loss: 0.77984, val loss: 0.78090\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########1 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.77363, val loss: 0.80816\n",
      "Interaction tuning epoch: 2, train loss: 0.77806, val loss: 0.78890\n",
      "Interaction tuning epoch: 3, train loss: 0.77942, val loss: 0.79442\n",
      "Interaction tuning epoch: 4, train loss: 0.78009, val loss: 0.79179\n",
      "Interaction tuning epoch: 5, train loss: 0.78389, val loss: 0.79531\n",
      "Interaction tuning epoch: 6, train loss: 0.77233, val loss: 0.79493\n",
      "Interaction tuning epoch: 7, train loss: 0.77868, val loss: 0.79942\n",
      "Interaction tuning epoch: 8, train loss: 0.77442, val loss: 0.78578\n",
      "Interaction tuning epoch: 9, train loss: 0.77189, val loss: 0.78694\n",
      "Interaction tuning epoch: 10, train loss: 0.77773, val loss: 0.79647\n",
      "Interaction tuning epoch: 11, train loss: 0.77047, val loss: 0.79041\n",
      "Interaction tuning epoch: 12, train loss: 0.77722, val loss: 0.79207\n",
      "Interaction tuning epoch: 13, train loss: 0.77300, val loss: 0.79308\n",
      "Interaction tuning epoch: 14, train loss: 0.77120, val loss: 0.79276\n",
      "Interaction tuning epoch: 15, train loss: 0.77381, val loss: 0.78507\n",
      "Interaction tuning epoch: 16, train loss: 0.77431, val loss: 0.79038\n",
      "Interaction tuning epoch: 17, train loss: 0.77198, val loss: 0.78723\n",
      "Interaction tuning epoch: 18, train loss: 0.77100, val loss: 0.78943\n",
      "Interaction tuning epoch: 19, train loss: 0.76982, val loss: 0.78674\n",
      "Interaction tuning epoch: 20, train loss: 0.77257, val loss: 0.79210\n",
      "Interaction tuning epoch: 21, train loss: 0.76951, val loss: 0.77918\n",
      "Interaction tuning epoch: 22, train loss: 0.77278, val loss: 0.79255\n",
      "Interaction tuning epoch: 23, train loss: 0.77462, val loss: 0.78870\n",
      "Interaction tuning epoch: 24, train loss: 0.77181, val loss: 0.79325\n",
      "Interaction tuning epoch: 25, train loss: 0.77033, val loss: 0.78709\n",
      "Interaction tuning epoch: 26, train loss: 0.77584, val loss: 0.79444\n",
      "Interaction tuning epoch: 27, train loss: 0.77282, val loss: 0.79535\n",
      "Interaction tuning epoch: 28, train loss: 0.77088, val loss: 0.78515\n",
      "Interaction tuning epoch: 29, train loss: 0.76861, val loss: 0.78395\n",
      "Interaction tuning epoch: 30, train loss: 0.77347, val loss: 0.78712\n",
      "Interaction tuning epoch: 31, train loss: 0.77523, val loss: 0.78738\n",
      "Interaction tuning epoch: 32, train loss: 0.77572, val loss: 0.79595\n",
      "Interaction tuning epoch: 33, train loss: 0.77108, val loss: 0.78683\n",
      "Interaction tuning epoch: 34, train loss: 0.77107, val loss: 0.79717\n",
      "Interaction tuning epoch: 35, train loss: 0.77214, val loss: 0.78795\n",
      "Interaction tuning epoch: 36, train loss: 0.77618, val loss: 0.79703\n",
      "Interaction tuning epoch: 37, train loss: 0.77118, val loss: 0.78676\n",
      "Interaction tuning epoch: 38, train loss: 0.77890, val loss: 0.80051\n",
      "Interaction tuning epoch: 39, train loss: 0.77615, val loss: 0.79204\n",
      "Interaction tuning epoch: 40, train loss: 0.77346, val loss: 0.79232\n",
      "Interaction tuning epoch: 41, train loss: 0.77649, val loss: 0.78611\n",
      "Interaction tuning epoch: 42, train loss: 0.77513, val loss: 0.79759\n",
      "Interaction tuning epoch: 43, train loss: 0.77301, val loss: 0.79188\n",
      "Interaction tuning epoch: 44, train loss: 0.77492, val loss: 0.79124\n",
      "Interaction tuning epoch: 45, train loss: 0.77018, val loss: 0.78949\n",
      "Interaction tuning epoch: 46, train loss: 0.77089, val loss: 0.79235\n",
      "Interaction tuning epoch: 47, train loss: 0.77568, val loss: 0.79159\n",
      "Interaction tuning epoch: 48, train loss: 0.77781, val loss: 0.79439\n",
      "Interaction tuning epoch: 49, train loss: 0.78587, val loss: 0.79673\n",
      "Interaction tuning epoch: 50, train loss: 0.76847, val loss: 0.79048\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 38.022746562957764\n",
      "After the gam stage, training error is 0.76847 , validation error is 0.79048\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 18.855941\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.636536 validation MAE=0.746650,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.593229 validation MAE=0.726856,rank=5\n",
      "[SoftImpute] Iter 3: observed MAE=0.556242 validation MAE=0.708145,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.526005 validation MAE=0.691818,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.500872 validation MAE=0.677220,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.478733 validation MAE=0.664132,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.460669 validation MAE=0.652382,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.444100 validation MAE=0.641049,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.430023 validation MAE=0.631198,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.416714 validation MAE=0.621333,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.402815 validation MAE=0.612879,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.391411 validation MAE=0.604353,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.379174 validation MAE=0.596761,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.369607 validation MAE=0.589896,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.360919 validation MAE=0.583868,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.351144 validation MAE=0.577417,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.344005 validation MAE=0.572212,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.336109 validation MAE=0.566978,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.329246 validation MAE=0.562325,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.323058 validation MAE=0.557975,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.316519 validation MAE=0.553694,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.311056 validation MAE=0.548574,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.306090 validation MAE=0.545739,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.300934 validation MAE=0.542299,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.296521 validation MAE=0.539751,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.291750 validation MAE=0.535488,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.287394 validation MAE=0.532628,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.283570 validation MAE=0.530322,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.278855 validation MAE=0.527512,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.275286 validation MAE=0.525112,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.271490 validation MAE=0.522069,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.269249 validation MAE=0.520232,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.265864 validation MAE=0.518252,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.262854 validation MAE=0.516173,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.259880 validation MAE=0.515029,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.256847 validation MAE=0.512518,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.254788 validation MAE=0.511454,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.251586 validation MAE=0.508943,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.249624 validation MAE=0.507339,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.246808 validation MAE=0.504985,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.245213 validation MAE=0.504535,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.242332 validation MAE=0.502261,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.240560 validation MAE=0.500825,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.237727 validation MAE=0.498992,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.236424 validation MAE=0.497900,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.233438 validation MAE=0.496111,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.232164 validation MAE=0.494947,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.230530 validation MAE=0.493702,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.228809 validation MAE=0.491931,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.227038 validation MAE=0.490850,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.225865 validation MAE=0.489769,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.223641 validation MAE=0.488429,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.222860 validation MAE=0.487842,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.221004 validation MAE=0.486010,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.219369 validation MAE=0.484694,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.218412 validation MAE=0.483780,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.217397 validation MAE=0.483405,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.216226 validation MAE=0.482325,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.214913 validation MAE=0.481020,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.213300 validation MAE=0.479624,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.212483 validation MAE=0.479046,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.211050 validation MAE=0.477864,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.210776 validation MAE=0.478061,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.208686 validation MAE=0.476153,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.208158 validation MAE=0.476230,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.207604 validation MAE=0.475200,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.206646 validation MAE=0.473735,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.205615 validation MAE=0.473759,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.205062 validation MAE=0.473069,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.203897 validation MAE=0.471589,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.202690 validation MAE=0.470850,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.202140 validation MAE=0.469746,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.201944 validation MAE=0.469782,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.200362 validation MAE=0.468757,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.200034 validation MAE=0.468253,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.198759 validation MAE=0.467660,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.198359 validation MAE=0.466664,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.197607 validation MAE=0.465885,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.197359 validation MAE=0.465195,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.196091 validation MAE=0.464429,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.196044 validation MAE=0.463697,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.194834 validation MAE=0.462905,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.194098 validation MAE=0.463050,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.193708 validation MAE=0.462055,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.193096 validation MAE=0.461198,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.192288 validation MAE=0.460374,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.191761 validation MAE=0.460229,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.191540 validation MAE=0.459730,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.190503 validation MAE=0.458824,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.190268 validation MAE=0.458234,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.190398 validation MAE=0.457952,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.190373 validation MAE=0.457469,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.189022 validation MAE=0.456252,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.188468 validation MAE=0.456051,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.188339 validation MAE=0.455790,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.188555 validation MAE=0.455386,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.187105 validation MAE=0.454447,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.186286 validation MAE=0.454189,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.186049 validation MAE=0.453075,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.185848 validation MAE=0.452920,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.184988 validation MAE=0.452490,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.184901 validation MAE=0.452268,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.184623 validation MAE=0.451399,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.184310 validation MAE=0.451440,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.184338 validation MAE=0.450304,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.183393 validation MAE=0.449838,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.182648 validation MAE=0.450001,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.182604 validation MAE=0.449312,rank=5\n",
      "[SoftImpute] Iter 109: observed MAE=0.182332 validation MAE=0.449374,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.182468 validation MAE=0.448768,rank=5\n",
      "[SoftImpute] Iter 111: observed MAE=0.181773 validation MAE=0.448018,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.181449 validation MAE=0.447932,rank=5\n",
      "[SoftImpute] Iter 113: observed MAE=0.180751 validation MAE=0.447166,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 114: observed MAE=0.180870 validation MAE=0.447025,rank=5\n",
      "[SoftImpute] Iter 115: observed MAE=0.180332 validation MAE=0.446174,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.179876 validation MAE=0.445742,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.179059 validation MAE=0.445726,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.179506 validation MAE=0.445619,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.179193 validation MAE=0.444946,rank=5\n",
      "[SoftImpute] Iter 120: observed MAE=0.178570 validation MAE=0.444217,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.178472 validation MAE=0.443429,rank=5\n",
      "[SoftImpute] Iter 122: observed MAE=0.178106 validation MAE=0.443226,rank=5\n",
      "[SoftImpute] Iter 123: observed MAE=0.177904 validation MAE=0.442969,rank=5\n",
      "[SoftImpute] Iter 124: observed MAE=0.177713 validation MAE=0.442522,rank=5\n",
      "[SoftImpute] Iter 125: observed MAE=0.176599 validation MAE=0.441967,rank=5\n",
      "[SoftImpute] Iter 126: observed MAE=0.176871 validation MAE=0.442022,rank=5\n",
      "[SoftImpute] Iter 127: observed MAE=0.176318 validation MAE=0.441203,rank=5\n",
      "[SoftImpute] Iter 128: observed MAE=0.177057 validation MAE=0.441809,rank=5\n",
      "[SoftImpute] Iter 129: observed MAE=0.176106 validation MAE=0.440870,rank=5\n",
      "[SoftImpute] Iter 130: observed MAE=0.175858 validation MAE=0.440485,rank=5\n",
      "[SoftImpute] Iter 131: observed MAE=0.175296 validation MAE=0.440001,rank=5\n",
      "[SoftImpute] Iter 132: observed MAE=0.175219 validation MAE=0.439521,rank=5\n",
      "[SoftImpute] Iter 133: observed MAE=0.175143 validation MAE=0.439498,rank=5\n",
      "[SoftImpute] Iter 134: observed MAE=0.174915 validation MAE=0.438983,rank=5\n",
      "[SoftImpute] Iter 135: observed MAE=0.174936 validation MAE=0.438729,rank=5\n",
      "[SoftImpute] Iter 136: observed MAE=0.174117 validation MAE=0.437969,rank=5\n",
      "[SoftImpute] Iter 137: observed MAE=0.173994 validation MAE=0.438216,rank=5\n",
      "[SoftImpute] Iter 138: observed MAE=0.174464 validation MAE=0.438279,rank=5\n",
      "[SoftImpute] Iter 139: observed MAE=0.174041 validation MAE=0.437637,rank=5\n",
      "[SoftImpute] Iter 140: observed MAE=0.173286 validation MAE=0.437692,rank=5\n",
      "[SoftImpute] Iter 141: observed MAE=0.173664 validation MAE=0.436716,rank=5\n",
      "[SoftImpute] Iter 142: observed MAE=0.172730 validation MAE=0.436014,rank=5\n",
      "[SoftImpute] Iter 143: observed MAE=0.172736 validation MAE=0.436313,rank=5\n",
      "[SoftImpute] Iter 144: observed MAE=0.172879 validation MAE=0.436347,rank=5\n",
      "[SoftImpute] Iter 145: observed MAE=0.172363 validation MAE=0.435968,rank=5\n",
      "[SoftImpute] Iter 146: observed MAE=0.172354 validation MAE=0.435868,rank=5\n",
      "[SoftImpute] Iter 147: observed MAE=0.171552 validation MAE=0.435371,rank=5\n",
      "[SoftImpute] Iter 148: observed MAE=0.170615 validation MAE=0.434553,rank=5\n",
      "[SoftImpute] Iter 149: observed MAE=0.171167 validation MAE=0.434809,rank=5\n",
      "[SoftImpute] Iter 150: observed MAE=0.170969 validation MAE=0.434567,rank=5\n",
      "[SoftImpute] Iter 151: observed MAE=0.171059 validation MAE=0.434767,rank=5\n",
      "[SoftImpute] Iter 152: observed MAE=0.170654 validation MAE=0.434613,rank=5\n",
      "[SoftImpute] Iter 153: observed MAE=0.170574 validation MAE=0.434001,rank=5\n",
      "[SoftImpute] Iter 154: observed MAE=0.169849 validation MAE=0.433627,rank=5\n",
      "[SoftImpute] Iter 155: observed MAE=0.169498 validation MAE=0.433791,rank=5\n",
      "[SoftImpute] Iter 156: observed MAE=0.170258 validation MAE=0.433383,rank=5\n",
      "[SoftImpute] Iter 157: observed MAE=0.169684 validation MAE=0.432864,rank=5\n",
      "[SoftImpute] Iter 158: observed MAE=0.169839 validation MAE=0.432976,rank=5\n",
      "[SoftImpute] Iter 159: observed MAE=0.169536 validation MAE=0.432827,rank=5\n",
      "[SoftImpute] Iter 160: observed MAE=0.168797 validation MAE=0.432168,rank=5\n",
      "[SoftImpute] Iter 161: observed MAE=0.169067 validation MAE=0.432083,rank=5\n",
      "[SoftImpute] Iter 162: observed MAE=0.168885 validation MAE=0.432004,rank=5\n",
      "[SoftImpute] Iter 163: observed MAE=0.168524 validation MAE=0.431585,rank=5\n",
      "[SoftImpute] Iter 164: observed MAE=0.167997 validation MAE=0.431457,rank=5\n",
      "[SoftImpute] Iter 165: observed MAE=0.168139 validation MAE=0.431293,rank=5\n",
      "[SoftImpute] Iter 166: observed MAE=0.167882 validation MAE=0.430634,rank=5\n",
      "[SoftImpute] Iter 167: observed MAE=0.167326 validation MAE=0.430673,rank=5\n",
      "[SoftImpute] Iter 168: observed MAE=0.167900 validation MAE=0.431224,rank=5\n",
      "[SoftImpute] Iter 169: observed MAE=0.167291 validation MAE=0.430235,rank=5\n",
      "[SoftImpute] Iter 170: observed MAE=0.167437 validation MAE=0.430046,rank=5\n",
      "[SoftImpute] Iter 171: observed MAE=0.167108 validation MAE=0.430212,rank=5\n",
      "[SoftImpute] Iter 172: observed MAE=0.167167 validation MAE=0.429562,rank=5\n",
      "[SoftImpute] Iter 173: observed MAE=0.166376 validation MAE=0.429577,rank=5\n",
      "[SoftImpute] Iter 174: observed MAE=0.165665 validation MAE=0.429081,rank=5\n",
      "[SoftImpute] Iter 175: observed MAE=0.166420 validation MAE=0.428963,rank=5\n",
      "[SoftImpute] Iter 176: observed MAE=0.165642 validation MAE=0.428605,rank=5\n",
      "[SoftImpute] Iter 177: observed MAE=0.165730 validation MAE=0.428475,rank=5\n",
      "[SoftImpute] Iter 178: observed MAE=0.165150 validation MAE=0.428373,rank=5\n",
      "[SoftImpute] Iter 179: observed MAE=0.165059 validation MAE=0.428755,rank=5\n",
      "[SoftImpute] Iter 180: observed MAE=0.165378 validation MAE=0.428236,rank=5\n",
      "[SoftImpute] Iter 181: observed MAE=0.164594 validation MAE=0.427473,rank=5\n",
      "[SoftImpute] Iter 182: observed MAE=0.164881 validation MAE=0.427561,rank=5\n",
      "[SoftImpute] Iter 183: observed MAE=0.165195 validation MAE=0.428070,rank=5\n",
      "[SoftImpute] Iter 184: observed MAE=0.165271 validation MAE=0.427577,rank=5\n",
      "[SoftImpute] Iter 185: observed MAE=0.164720 validation MAE=0.427521,rank=5\n",
      "[SoftImpute] Iter 186: observed MAE=0.164056 validation MAE=0.426918,rank=5\n",
      "[SoftImpute] Iter 187: observed MAE=0.164110 validation MAE=0.426514,rank=5\n",
      "[SoftImpute] Iter 188: observed MAE=0.163555 validation MAE=0.425888,rank=5\n",
      "[SoftImpute] Iter 189: observed MAE=0.163713 validation MAE=0.426270,rank=5\n",
      "[SoftImpute] Iter 190: observed MAE=0.163912 validation MAE=0.426585,rank=5\n",
      "[SoftImpute] Iter 191: observed MAE=0.163631 validation MAE=0.426339,rank=5\n",
      "[SoftImpute] Iter 192: observed MAE=0.163444 validation MAE=0.425656,rank=5\n",
      "[SoftImpute] Iter 193: observed MAE=0.162895 validation MAE=0.425514,rank=5\n",
      "[SoftImpute] Iter 194: observed MAE=0.163117 validation MAE=0.425666,rank=5\n",
      "[SoftImpute] Iter 195: observed MAE=0.162966 validation MAE=0.425423,rank=5\n",
      "[SoftImpute] Iter 196: observed MAE=0.162641 validation MAE=0.424547,rank=5\n",
      "[SoftImpute] Iter 197: observed MAE=0.162454 validation MAE=0.424956,rank=5\n",
      "[SoftImpute] Iter 198: observed MAE=0.162331 validation MAE=0.424722,rank=5\n",
      "[SoftImpute] Iter 199: observed MAE=0.162883 validation MAE=0.424855,rank=5\n",
      "[SoftImpute] Iter 200: observed MAE=0.162303 validation MAE=0.424126,rank=5\n",
      "[SoftImpute] Stopped after iteration 200 for lambda=0.377119\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 6.12682032585144\n",
      "After the matrix factor stage, training error is 0.16230, validation error is 0.42413\n",
      "8\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.08474, val loss: 3.97421\n",
      "Main effects training epoch: 2, train loss: 3.91219, val loss: 3.81224\n",
      "Main effects training epoch: 3, train loss: 3.75853, val loss: 3.67647\n",
      "Main effects training epoch: 4, train loss: 3.49444, val loss: 3.45187\n",
      "Main effects training epoch: 5, train loss: 3.39134, val loss: 3.37163\n",
      "Main effects training epoch: 6, train loss: 3.32318, val loss: 3.32617\n",
      "Main effects training epoch: 7, train loss: 3.35157, val loss: 3.36231\n",
      "Main effects training epoch: 8, train loss: 3.24127, val loss: 3.23956\n",
      "Main effects training epoch: 9, train loss: 3.16334, val loss: 3.15723\n",
      "Main effects training epoch: 10, train loss: 3.10827, val loss: 3.11385\n",
      "Main effects training epoch: 11, train loss: 3.08823, val loss: 3.10649\n",
      "Main effects training epoch: 12, train loss: 3.05748, val loss: 3.09423\n",
      "Main effects training epoch: 13, train loss: 2.97358, val loss: 3.01147\n",
      "Main effects training epoch: 14, train loss: 2.89278, val loss: 2.95495\n",
      "Main effects training epoch: 15, train loss: 2.80025, val loss: 2.86285\n",
      "Main effects training epoch: 16, train loss: 2.77372, val loss: 2.82689\n",
      "Main effects training epoch: 17, train loss: 2.68861, val loss: 2.73975\n",
      "Main effects training epoch: 18, train loss: 2.64829, val loss: 2.68496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 19, train loss: 2.62151, val loss: 2.65022\n",
      "Main effects training epoch: 20, train loss: 2.55014, val loss: 2.57588\n",
      "Main effects training epoch: 21, train loss: 2.48746, val loss: 2.49961\n",
      "Main effects training epoch: 22, train loss: 2.45745, val loss: 2.48113\n",
      "Main effects training epoch: 23, train loss: 2.41088, val loss: 2.42002\n",
      "Main effects training epoch: 24, train loss: 2.40927, val loss: 2.42063\n",
      "Main effects training epoch: 25, train loss: 2.33381, val loss: 2.34557\n",
      "Main effects training epoch: 26, train loss: 2.35208, val loss: 2.36349\n",
      "Main effects training epoch: 27, train loss: 2.29102, val loss: 2.30529\n",
      "Main effects training epoch: 28, train loss: 2.28279, val loss: 2.29209\n",
      "Main effects training epoch: 29, train loss: 2.22077, val loss: 2.23144\n",
      "Main effects training epoch: 30, train loss: 2.21543, val loss: 2.21940\n",
      "Main effects training epoch: 31, train loss: 2.18742, val loss: 2.19378\n",
      "Main effects training epoch: 32, train loss: 2.16251, val loss: 2.15892\n",
      "Main effects training epoch: 33, train loss: 2.13642, val loss: 2.13632\n",
      "Main effects training epoch: 34, train loss: 2.13181, val loss: 2.12930\n",
      "Main effects training epoch: 35, train loss: 2.09430, val loss: 2.09268\n",
      "Main effects training epoch: 36, train loss: 2.06016, val loss: 2.05339\n",
      "Main effects training epoch: 37, train loss: 2.04616, val loss: 2.03987\n",
      "Main effects training epoch: 38, train loss: 2.02260, val loss: 2.01469\n",
      "Main effects training epoch: 39, train loss: 2.03127, val loss: 2.02380\n",
      "Main effects training epoch: 40, train loss: 1.99122, val loss: 1.98263\n",
      "Main effects training epoch: 41, train loss: 1.98934, val loss: 1.97883\n",
      "Main effects training epoch: 42, train loss: 1.94980, val loss: 1.93760\n",
      "Main effects training epoch: 43, train loss: 1.97350, val loss: 1.96408\n",
      "Main effects training epoch: 44, train loss: 1.93558, val loss: 1.91416\n",
      "Main effects training epoch: 45, train loss: 1.92095, val loss: 1.90982\n",
      "Main effects training epoch: 46, train loss: 1.92044, val loss: 1.90082\n",
      "Main effects training epoch: 47, train loss: 1.89230, val loss: 1.87123\n",
      "Main effects training epoch: 48, train loss: 1.88624, val loss: 1.86917\n",
      "Main effects training epoch: 49, train loss: 1.87359, val loss: 1.85492\n",
      "Main effects training epoch: 50, train loss: 1.86059, val loss: 1.84096\n",
      "Main effects training epoch: 51, train loss: 1.86091, val loss: 1.84700\n",
      "Main effects training epoch: 52, train loss: 1.84389, val loss: 1.82362\n",
      "Main effects training epoch: 53, train loss: 1.83865, val loss: 1.82205\n",
      "Main effects training epoch: 54, train loss: 1.81388, val loss: 1.79284\n",
      "Main effects training epoch: 55, train loss: 1.81919, val loss: 1.80513\n",
      "Main effects training epoch: 56, train loss: 1.81093, val loss: 1.78618\n",
      "Main effects training epoch: 57, train loss: 1.80132, val loss: 1.78011\n",
      "Main effects training epoch: 58, train loss: 1.79474, val loss: 1.77073\n",
      "Main effects training epoch: 59, train loss: 1.80191, val loss: 1.77927\n",
      "Main effects training epoch: 60, train loss: 1.78549, val loss: 1.76043\n",
      "Main effects training epoch: 61, train loss: 1.78799, val loss: 1.76015\n",
      "Main effects training epoch: 62, train loss: 1.77613, val loss: 1.75366\n",
      "Main effects training epoch: 63, train loss: 1.77303, val loss: 1.74433\n",
      "Main effects training epoch: 64, train loss: 1.76558, val loss: 1.73896\n",
      "Main effects training epoch: 65, train loss: 1.77464, val loss: 1.74901\n",
      "Main effects training epoch: 66, train loss: 1.75130, val loss: 1.72468\n",
      "Main effects training epoch: 67, train loss: 1.75796, val loss: 1.72640\n",
      "Main effects training epoch: 68, train loss: 1.75410, val loss: 1.72381\n",
      "Main effects training epoch: 69, train loss: 1.74790, val loss: 1.71646\n",
      "Main effects training epoch: 70, train loss: 1.74405, val loss: 1.71216\n",
      "Main effects training epoch: 71, train loss: 1.74497, val loss: 1.71302\n",
      "Main effects training epoch: 72, train loss: 1.74214, val loss: 1.71285\n",
      "Main effects training epoch: 73, train loss: 1.73308, val loss: 1.69828\n",
      "Main effects training epoch: 74, train loss: 1.73608, val loss: 1.70051\n",
      "Main effects training epoch: 75, train loss: 1.73662, val loss: 1.70198\n",
      "Main effects training epoch: 76, train loss: 1.72947, val loss: 1.69749\n",
      "Main effects training epoch: 77, train loss: 1.72814, val loss: 1.69049\n",
      "Main effects training epoch: 78, train loss: 1.73344, val loss: 1.69949\n",
      "Main effects training epoch: 79, train loss: 1.72560, val loss: 1.68789\n",
      "Main effects training epoch: 80, train loss: 1.72234, val loss: 1.68689\n",
      "Main effects training epoch: 81, train loss: 1.71987, val loss: 1.67483\n",
      "Main effects training epoch: 82, train loss: 1.72227, val loss: 1.68728\n",
      "Main effects training epoch: 83, train loss: 1.71965, val loss: 1.68187\n",
      "Main effects training epoch: 84, train loss: 1.72044, val loss: 1.68639\n",
      "Main effects training epoch: 85, train loss: 1.71611, val loss: 1.67849\n",
      "Main effects training epoch: 86, train loss: 1.71516, val loss: 1.67785\n",
      "Main effects training epoch: 87, train loss: 1.71253, val loss: 1.67160\n",
      "Main effects training epoch: 88, train loss: 1.71083, val loss: 1.67595\n",
      "Main effects training epoch: 89, train loss: 1.70904, val loss: 1.67028\n",
      "Main effects training epoch: 90, train loss: 1.70362, val loss: 1.66509\n",
      "Main effects training epoch: 91, train loss: 1.70718, val loss: 1.66941\n",
      "Main effects training epoch: 92, train loss: 1.70047, val loss: 1.66186\n",
      "Main effects training epoch: 93, train loss: 1.70082, val loss: 1.66534\n",
      "Main effects training epoch: 94, train loss: 1.69663, val loss: 1.65087\n",
      "Main effects training epoch: 95, train loss: 1.69512, val loss: 1.66316\n",
      "Main effects training epoch: 96, train loss: 1.68950, val loss: 1.64639\n",
      "Main effects training epoch: 97, train loss: 1.69065, val loss: 1.65522\n",
      "Main effects training epoch: 98, train loss: 1.68257, val loss: 1.63961\n",
      "Main effects training epoch: 99, train loss: 1.68143, val loss: 1.64129\n",
      "Main effects training epoch: 100, train loss: 1.67104, val loss: 1.63091\n",
      "Main effects training epoch: 101, train loss: 1.66609, val loss: 1.62726\n",
      "Main effects training epoch: 102, train loss: 1.66229, val loss: 1.63524\n",
      "Main effects training epoch: 103, train loss: 1.66053, val loss: 1.62001\n",
      "Main effects training epoch: 104, train loss: 1.65337, val loss: 1.62965\n",
      "Main effects training epoch: 105, train loss: 1.64706, val loss: 1.61485\n",
      "Main effects training epoch: 106, train loss: 1.63901, val loss: 1.61928\n",
      "Main effects training epoch: 107, train loss: 1.63631, val loss: 1.61201\n",
      "Main effects training epoch: 108, train loss: 1.63643, val loss: 1.61731\n",
      "Main effects training epoch: 109, train loss: 1.63589, val loss: 1.62114\n",
      "Main effects training epoch: 110, train loss: 1.63079, val loss: 1.60909\n",
      "Main effects training epoch: 111, train loss: 1.63167, val loss: 1.61478\n",
      "Main effects training epoch: 112, train loss: 1.62366, val loss: 1.60596\n",
      "Main effects training epoch: 113, train loss: 1.62274, val loss: 1.60567\n",
      "Main effects training epoch: 114, train loss: 1.62060, val loss: 1.61379\n",
      "Main effects training epoch: 115, train loss: 1.62791, val loss: 1.61944\n",
      "Main effects training epoch: 116, train loss: 1.61940, val loss: 1.60496\n",
      "Main effects training epoch: 117, train loss: 1.62127, val loss: 1.61014\n",
      "Main effects training epoch: 118, train loss: 1.61721, val loss: 1.61387\n",
      "Main effects training epoch: 119, train loss: 1.61749, val loss: 1.61490\n",
      "Main effects training epoch: 120, train loss: 1.62093, val loss: 1.60627\n",
      "Main effects training epoch: 121, train loss: 1.60998, val loss: 1.60596\n",
      "Main effects training epoch: 122, train loss: 1.61420, val loss: 1.61017\n",
      "Main effects training epoch: 123, train loss: 1.61311, val loss: 1.60138\n",
      "Main effects training epoch: 124, train loss: 1.60829, val loss: 1.60341\n",
      "Main effects training epoch: 125, train loss: 1.61165, val loss: 1.61188\n",
      "Main effects training epoch: 126, train loss: 1.61142, val loss: 1.60332\n",
      "Main effects training epoch: 127, train loss: 1.60579, val loss: 1.59652\n",
      "Main effects training epoch: 128, train loss: 1.60376, val loss: 1.60254\n",
      "Main effects training epoch: 129, train loss: 1.60589, val loss: 1.60470\n",
      "Main effects training epoch: 130, train loss: 1.60038, val loss: 1.59496\n",
      "Main effects training epoch: 131, train loss: 1.59533, val loss: 1.59682\n",
      "Main effects training epoch: 132, train loss: 1.60126, val loss: 1.60225\n",
      "Main effects training epoch: 133, train loss: 1.59780, val loss: 1.59388\n",
      "Main effects training epoch: 134, train loss: 1.59425, val loss: 1.59668\n",
      "Main effects training epoch: 135, train loss: 1.59293, val loss: 1.59595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 136, train loss: 1.59463, val loss: 1.59483\n",
      "Main effects training epoch: 137, train loss: 1.59073, val loss: 1.59224\n",
      "Main effects training epoch: 138, train loss: 1.59297, val loss: 1.60078\n",
      "Main effects training epoch: 139, train loss: 1.59368, val loss: 1.59400\n",
      "Main effects training epoch: 140, train loss: 1.58966, val loss: 1.59011\n",
      "Main effects training epoch: 141, train loss: 1.58847, val loss: 1.59633\n",
      "Main effects training epoch: 142, train loss: 1.59410, val loss: 1.59351\n",
      "Main effects training epoch: 143, train loss: 1.59449, val loss: 1.60195\n",
      "Main effects training epoch: 144, train loss: 1.59454, val loss: 1.59670\n",
      "Main effects training epoch: 145, train loss: 1.59434, val loss: 1.59577\n",
      "Main effects training epoch: 146, train loss: 1.59277, val loss: 1.59829\n",
      "Main effects training epoch: 147, train loss: 1.58640, val loss: 1.58972\n",
      "Main effects training epoch: 148, train loss: 1.58414, val loss: 1.58576\n",
      "Main effects training epoch: 149, train loss: 1.58308, val loss: 1.58879\n",
      "Main effects training epoch: 150, train loss: 1.58047, val loss: 1.58614\n",
      "Main effects training epoch: 151, train loss: 1.58002, val loss: 1.58282\n",
      "Main effects training epoch: 152, train loss: 1.58063, val loss: 1.59010\n",
      "Main effects training epoch: 153, train loss: 1.58361, val loss: 1.58676\n",
      "Main effects training epoch: 154, train loss: 1.57753, val loss: 1.58019\n",
      "Main effects training epoch: 155, train loss: 1.57679, val loss: 1.58517\n",
      "Main effects training epoch: 156, train loss: 1.57561, val loss: 1.57729\n",
      "Main effects training epoch: 157, train loss: 1.57436, val loss: 1.58188\n",
      "Main effects training epoch: 158, train loss: 1.57516, val loss: 1.57635\n",
      "Main effects training epoch: 159, train loss: 1.57608, val loss: 1.58153\n",
      "Main effects training epoch: 160, train loss: 1.57255, val loss: 1.57778\n",
      "Main effects training epoch: 161, train loss: 1.57841, val loss: 1.58736\n",
      "Main effects training epoch: 162, train loss: 1.58091, val loss: 1.58898\n",
      "Main effects training epoch: 163, train loss: 1.57380, val loss: 1.57957\n",
      "Main effects training epoch: 164, train loss: 1.57220, val loss: 1.57507\n",
      "Main effects training epoch: 165, train loss: 1.56816, val loss: 1.57973\n",
      "Main effects training epoch: 166, train loss: 1.56742, val loss: 1.57283\n",
      "Main effects training epoch: 167, train loss: 1.56678, val loss: 1.56960\n",
      "Main effects training epoch: 168, train loss: 1.57149, val loss: 1.58514\n",
      "Main effects training epoch: 169, train loss: 1.57595, val loss: 1.58800\n",
      "Main effects training epoch: 170, train loss: 1.56644, val loss: 1.57619\n",
      "Main effects training epoch: 171, train loss: 1.56539, val loss: 1.57915\n",
      "Main effects training epoch: 172, train loss: 1.57787, val loss: 1.57958\n",
      "Main effects training epoch: 173, train loss: 1.56718, val loss: 1.58087\n",
      "Main effects training epoch: 174, train loss: 1.57746, val loss: 1.58331\n",
      "Main effects training epoch: 175, train loss: 1.56430, val loss: 1.57216\n",
      "Main effects training epoch: 176, train loss: 1.56251, val loss: 1.57894\n",
      "Main effects training epoch: 177, train loss: 1.56277, val loss: 1.56272\n",
      "Main effects training epoch: 178, train loss: 1.56178, val loss: 1.57110\n",
      "Main effects training epoch: 179, train loss: 1.57196, val loss: 1.59060\n",
      "Main effects training epoch: 180, train loss: 1.56732, val loss: 1.57685\n",
      "Main effects training epoch: 181, train loss: 1.55990, val loss: 1.57199\n",
      "Main effects training epoch: 182, train loss: 1.55680, val loss: 1.57244\n",
      "Main effects training epoch: 183, train loss: 1.55735, val loss: 1.56083\n",
      "Main effects training epoch: 184, train loss: 1.55649, val loss: 1.57229\n",
      "Main effects training epoch: 185, train loss: 1.57345, val loss: 1.58061\n",
      "Main effects training epoch: 186, train loss: 1.56457, val loss: 1.58326\n",
      "Main effects training epoch: 187, train loss: 1.56242, val loss: 1.56816\n",
      "Main effects training epoch: 188, train loss: 1.55436, val loss: 1.57107\n",
      "Main effects training epoch: 189, train loss: 1.55329, val loss: 1.56069\n",
      "Main effects training epoch: 190, train loss: 1.55433, val loss: 1.57558\n",
      "Main effects training epoch: 191, train loss: 1.55220, val loss: 1.56248\n",
      "Main effects training epoch: 192, train loss: 1.54650, val loss: 1.55673\n",
      "Main effects training epoch: 193, train loss: 1.54630, val loss: 1.55881\n",
      "Main effects training epoch: 194, train loss: 1.54714, val loss: 1.54913\n",
      "Main effects training epoch: 195, train loss: 1.54154, val loss: 1.55647\n",
      "Main effects training epoch: 196, train loss: 1.54195, val loss: 1.55766\n",
      "Main effects training epoch: 197, train loss: 1.54535, val loss: 1.55535\n",
      "Main effects training epoch: 198, train loss: 1.54832, val loss: 1.56610\n",
      "Main effects training epoch: 199, train loss: 1.54999, val loss: 1.55816\n",
      "Main effects training epoch: 200, train loss: 1.55080, val loss: 1.56574\n",
      "Main effects training epoch: 201, train loss: 1.53551, val loss: 1.54549\n",
      "Main effects training epoch: 202, train loss: 1.53936, val loss: 1.55023\n",
      "Main effects training epoch: 203, train loss: 1.53572, val loss: 1.54498\n",
      "Main effects training epoch: 204, train loss: 1.53424, val loss: 1.53934\n",
      "Main effects training epoch: 205, train loss: 1.54259, val loss: 1.56136\n",
      "Main effects training epoch: 206, train loss: 1.53873, val loss: 1.55282\n",
      "Main effects training epoch: 207, train loss: 1.52891, val loss: 1.53824\n",
      "Main effects training epoch: 208, train loss: 1.52699, val loss: 1.53264\n",
      "Main effects training epoch: 209, train loss: 1.53037, val loss: 1.53794\n",
      "Main effects training epoch: 210, train loss: 1.52608, val loss: 1.54195\n",
      "Main effects training epoch: 211, train loss: 1.53899, val loss: 1.54390\n",
      "Main effects training epoch: 212, train loss: 1.52505, val loss: 1.54353\n",
      "Main effects training epoch: 213, train loss: 1.52252, val loss: 1.53119\n",
      "Main effects training epoch: 214, train loss: 1.51933, val loss: 1.53003\n",
      "Main effects training epoch: 215, train loss: 1.52305, val loss: 1.53004\n",
      "Main effects training epoch: 216, train loss: 1.52512, val loss: 1.54336\n",
      "Main effects training epoch: 217, train loss: 1.51705, val loss: 1.53064\n",
      "Main effects training epoch: 218, train loss: 1.52227, val loss: 1.54093\n",
      "Main effects training epoch: 219, train loss: 1.53783, val loss: 1.54347\n",
      "Main effects training epoch: 220, train loss: 1.51611, val loss: 1.52646\n",
      "Main effects training epoch: 221, train loss: 1.51790, val loss: 1.53076\n",
      "Main effects training epoch: 222, train loss: 1.52242, val loss: 1.53937\n",
      "Main effects training epoch: 223, train loss: 1.51570, val loss: 1.52921\n",
      "Main effects training epoch: 224, train loss: 1.51239, val loss: 1.52409\n",
      "Main effects training epoch: 225, train loss: 1.51335, val loss: 1.52902\n",
      "Main effects training epoch: 226, train loss: 1.51085, val loss: 1.51962\n",
      "Main effects training epoch: 227, train loss: 1.51225, val loss: 1.52541\n",
      "Main effects training epoch: 228, train loss: 1.51902, val loss: 1.52455\n",
      "Main effects training epoch: 229, train loss: 1.51513, val loss: 1.53751\n",
      "Main effects training epoch: 230, train loss: 1.51512, val loss: 1.53309\n",
      "Main effects training epoch: 231, train loss: 1.51548, val loss: 1.53484\n",
      "Main effects training epoch: 232, train loss: 1.50989, val loss: 1.52684\n",
      "Main effects training epoch: 233, train loss: 1.50840, val loss: 1.52382\n",
      "Main effects training epoch: 234, train loss: 1.50625, val loss: 1.52366\n",
      "Main effects training epoch: 235, train loss: 1.50244, val loss: 1.51723\n",
      "Main effects training epoch: 236, train loss: 1.50250, val loss: 1.52025\n",
      "Main effects training epoch: 237, train loss: 1.50364, val loss: 1.52311\n",
      "Main effects training epoch: 238, train loss: 1.50211, val loss: 1.52795\n",
      "Main effects training epoch: 239, train loss: 1.50364, val loss: 1.52041\n",
      "Main effects training epoch: 240, train loss: 1.49976, val loss: 1.51522\n",
      "Main effects training epoch: 241, train loss: 1.50345, val loss: 1.51180\n",
      "Main effects training epoch: 242, train loss: 1.50171, val loss: 1.52759\n",
      "Main effects training epoch: 243, train loss: 1.49931, val loss: 1.50797\n",
      "Main effects training epoch: 244, train loss: 1.49738, val loss: 1.50384\n",
      "Main effects training epoch: 245, train loss: 1.50346, val loss: 1.52637\n",
      "Main effects training epoch: 246, train loss: 1.49689, val loss: 1.50152\n",
      "Main effects training epoch: 247, train loss: 1.49504, val loss: 1.52262\n",
      "Main effects training epoch: 248, train loss: 1.50073, val loss: 1.51325\n",
      "Main effects training epoch: 249, train loss: 1.50698, val loss: 1.51568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 250, train loss: 1.49661, val loss: 1.51474\n",
      "Main effects training epoch: 251, train loss: 1.50255, val loss: 1.50547\n",
      "Main effects training epoch: 252, train loss: 1.49586, val loss: 1.51323\n",
      "Main effects training epoch: 253, train loss: 1.49426, val loss: 1.51134\n",
      "Main effects training epoch: 254, train loss: 1.49092, val loss: 1.49632\n",
      "Main effects training epoch: 255, train loss: 1.49277, val loss: 1.50551\n",
      "Main effects training epoch: 256, train loss: 1.49023, val loss: 1.51111\n",
      "Main effects training epoch: 257, train loss: 1.48879, val loss: 1.50628\n",
      "Main effects training epoch: 258, train loss: 1.49877, val loss: 1.51483\n",
      "Main effects training epoch: 259, train loss: 1.49730, val loss: 1.51754\n",
      "Main effects training epoch: 260, train loss: 1.49374, val loss: 1.50739\n",
      "Main effects training epoch: 261, train loss: 1.50234, val loss: 1.52898\n",
      "Main effects training epoch: 262, train loss: 1.49818, val loss: 1.50738\n",
      "Main effects training epoch: 263, train loss: 1.48628, val loss: 1.50511\n",
      "Main effects training epoch: 264, train loss: 1.48879, val loss: 1.49755\n",
      "Main effects training epoch: 265, train loss: 1.49080, val loss: 1.51938\n",
      "Main effects training epoch: 266, train loss: 1.48740, val loss: 1.50066\n",
      "Main effects training epoch: 267, train loss: 1.48660, val loss: 1.49394\n",
      "Main effects training epoch: 268, train loss: 1.48465, val loss: 1.51148\n",
      "Main effects training epoch: 269, train loss: 1.48528, val loss: 1.48999\n",
      "Main effects training epoch: 270, train loss: 1.47951, val loss: 1.50388\n",
      "Main effects training epoch: 271, train loss: 1.48670, val loss: 1.50528\n",
      "Main effects training epoch: 272, train loss: 1.49603, val loss: 1.50377\n",
      "Main effects training epoch: 273, train loss: 1.48095, val loss: 1.49499\n",
      "Main effects training epoch: 274, train loss: 1.48278, val loss: 1.51562\n",
      "Main effects training epoch: 275, train loss: 1.47605, val loss: 1.48902\n",
      "Main effects training epoch: 276, train loss: 1.48440, val loss: 1.50248\n",
      "Main effects training epoch: 277, train loss: 1.48665, val loss: 1.51104\n",
      "Main effects training epoch: 278, train loss: 1.50729, val loss: 1.50017\n",
      "Main effects training epoch: 279, train loss: 1.48179, val loss: 1.50615\n",
      "Main effects training epoch: 280, train loss: 1.48006, val loss: 1.47896\n",
      "Main effects training epoch: 281, train loss: 1.47599, val loss: 1.49154\n",
      "Main effects training epoch: 282, train loss: 1.47300, val loss: 1.49028\n",
      "Main effects training epoch: 283, train loss: 1.48269, val loss: 1.51140\n",
      "Main effects training epoch: 284, train loss: 1.47555, val loss: 1.47806\n",
      "Main effects training epoch: 285, train loss: 1.47827, val loss: 1.50665\n",
      "Main effects training epoch: 286, train loss: 1.48316, val loss: 1.49224\n",
      "Main effects training epoch: 287, train loss: 1.47461, val loss: 1.49891\n",
      "Main effects training epoch: 288, train loss: 1.47630, val loss: 1.49259\n",
      "Main effects training epoch: 289, train loss: 1.47147, val loss: 1.49016\n",
      "Main effects training epoch: 290, train loss: 1.47005, val loss: 1.48323\n",
      "Main effects training epoch: 291, train loss: 1.47680, val loss: 1.49581\n",
      "Main effects training epoch: 292, train loss: 1.47291, val loss: 1.48535\n",
      "Main effects training epoch: 293, train loss: 1.46571, val loss: 1.48588\n",
      "Main effects training epoch: 294, train loss: 1.47201, val loss: 1.49022\n",
      "Main effects training epoch: 295, train loss: 1.48448, val loss: 1.49076\n",
      "Main effects training epoch: 296, train loss: 1.47591, val loss: 1.49957\n",
      "Main effects training epoch: 297, train loss: 1.46999, val loss: 1.48986\n",
      "Main effects training epoch: 298, train loss: 1.46767, val loss: 1.48752\n",
      "Main effects training epoch: 299, train loss: 1.47881, val loss: 1.49464\n",
      "Main effects training epoch: 300, train loss: 1.47018, val loss: 1.48632\n",
      "##########Stage 1: main effect training stop.##########\n",
      "2 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.47991, val loss: 1.49882\n",
      "Main effects tuning epoch: 2, train loss: 1.47407, val loss: 1.48526\n",
      "Main effects tuning epoch: 3, train loss: 1.47454, val loss: 1.49301\n",
      "Main effects tuning epoch: 4, train loss: 1.47374, val loss: 1.48373\n",
      "Main effects tuning epoch: 5, train loss: 1.47603, val loss: 1.48709\n",
      "Main effects tuning epoch: 6, train loss: 1.47652, val loss: 1.48900\n",
      "Main effects tuning epoch: 7, train loss: 1.47455, val loss: 1.49564\n",
      "Main effects tuning epoch: 8, train loss: 1.46748, val loss: 1.47758\n",
      "Main effects tuning epoch: 9, train loss: 1.47539, val loss: 1.48329\n",
      "Main effects tuning epoch: 10, train loss: 1.46560, val loss: 1.47851\n",
      "Main effects tuning epoch: 11, train loss: 1.47698, val loss: 1.49609\n",
      "Main effects tuning epoch: 12, train loss: 1.47217, val loss: 1.48362\n",
      "Main effects tuning epoch: 13, train loss: 1.47786, val loss: 1.48571\n",
      "Main effects tuning epoch: 14, train loss: 1.46457, val loss: 1.47803\n",
      "Main effects tuning epoch: 15, train loss: 1.46490, val loss: 1.47726\n",
      "Main effects tuning epoch: 16, train loss: 1.46112, val loss: 1.47532\n",
      "Main effects tuning epoch: 17, train loss: 1.46101, val loss: 1.48193\n",
      "Main effects tuning epoch: 18, train loss: 1.45698, val loss: 1.46892\n",
      "Main effects tuning epoch: 19, train loss: 1.46851, val loss: 1.47609\n",
      "Main effects tuning epoch: 20, train loss: 1.46802, val loss: 1.47796\n",
      "Main effects tuning epoch: 21, train loss: 1.46630, val loss: 1.47528\n",
      "Main effects tuning epoch: 22, train loss: 1.46846, val loss: 1.48332\n",
      "Main effects tuning epoch: 23, train loss: 1.46861, val loss: 1.47804\n",
      "Main effects tuning epoch: 24, train loss: 1.46318, val loss: 1.48063\n",
      "Main effects tuning epoch: 25, train loss: 1.45739, val loss: 1.47128\n",
      "Main effects tuning epoch: 26, train loss: 1.46277, val loss: 1.48034\n",
      "Main effects tuning epoch: 27, train loss: 1.45970, val loss: 1.47173\n",
      "Main effects tuning epoch: 28, train loss: 1.45717, val loss: 1.47013\n",
      "Main effects tuning epoch: 29, train loss: 1.45891, val loss: 1.47928\n",
      "Main effects tuning epoch: 30, train loss: 1.45580, val loss: 1.46437\n",
      "Main effects tuning epoch: 31, train loss: 1.46924, val loss: 1.47109\n",
      "Main effects tuning epoch: 32, train loss: 1.46176, val loss: 1.47421\n",
      "Main effects tuning epoch: 33, train loss: 1.45620, val loss: 1.46974\n",
      "Main effects tuning epoch: 34, train loss: 1.45585, val loss: 1.46437\n",
      "Main effects tuning epoch: 35, train loss: 1.45547, val loss: 1.46508\n",
      "Main effects tuning epoch: 36, train loss: 1.45777, val loss: 1.47783\n",
      "Main effects tuning epoch: 37, train loss: 1.45281, val loss: 1.46149\n",
      "Main effects tuning epoch: 38, train loss: 1.45683, val loss: 1.45529\n",
      "Main effects tuning epoch: 39, train loss: 1.45768, val loss: 1.46248\n",
      "Main effects tuning epoch: 40, train loss: 1.44974, val loss: 1.45322\n",
      "Main effects tuning epoch: 41, train loss: 1.45283, val loss: 1.45234\n",
      "Main effects tuning epoch: 42, train loss: 1.45519, val loss: 1.47551\n",
      "Main effects tuning epoch: 43, train loss: 1.46012, val loss: 1.46138\n",
      "Main effects tuning epoch: 44, train loss: 1.45706, val loss: 1.46920\n",
      "Main effects tuning epoch: 45, train loss: 1.45590, val loss: 1.44856\n",
      "Main effects tuning epoch: 46, train loss: 1.45539, val loss: 1.46279\n",
      "Main effects tuning epoch: 47, train loss: 1.44878, val loss: 1.44234\n",
      "Main effects tuning epoch: 48, train loss: 1.44559, val loss: 1.45944\n",
      "Main effects tuning epoch: 49, train loss: 1.44592, val loss: 1.45295\n",
      "Main effects tuning epoch: 50, train loss: 1.45358, val loss: 1.45320\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.34467, val loss: 1.36311\n",
      "Interaction training epoch: 2, train loss: 1.17413, val loss: 1.20671\n",
      "Interaction training epoch: 3, train loss: 1.27030, val loss: 1.30807\n",
      "Interaction training epoch: 4, train loss: 1.05675, val loss: 1.09066\n",
      "Interaction training epoch: 5, train loss: 1.06036, val loss: 1.08885\n",
      "Interaction training epoch: 6, train loss: 1.15464, val loss: 1.18924\n",
      "Interaction training epoch: 7, train loss: 1.03700, val loss: 1.06570\n",
      "Interaction training epoch: 8, train loss: 0.99788, val loss: 1.02914\n",
      "Interaction training epoch: 9, train loss: 1.05014, val loss: 1.08627\n",
      "Interaction training epoch: 10, train loss: 1.01596, val loss: 1.04310\n",
      "Interaction training epoch: 11, train loss: 0.97743, val loss: 1.02177\n",
      "Interaction training epoch: 12, train loss: 0.96811, val loss: 1.01395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 13, train loss: 0.98253, val loss: 1.02374\n",
      "Interaction training epoch: 14, train loss: 0.93519, val loss: 0.98490\n",
      "Interaction training epoch: 15, train loss: 0.95009, val loss: 0.99569\n",
      "Interaction training epoch: 16, train loss: 0.92752, val loss: 0.97034\n",
      "Interaction training epoch: 17, train loss: 1.01015, val loss: 1.03097\n",
      "Interaction training epoch: 18, train loss: 0.92752, val loss: 0.96202\n",
      "Interaction training epoch: 19, train loss: 0.91330, val loss: 0.95873\n",
      "Interaction training epoch: 20, train loss: 0.90974, val loss: 0.96194\n",
      "Interaction training epoch: 21, train loss: 0.90817, val loss: 0.95063\n",
      "Interaction training epoch: 22, train loss: 0.90676, val loss: 0.96281\n",
      "Interaction training epoch: 23, train loss: 0.90798, val loss: 0.95879\n",
      "Interaction training epoch: 24, train loss: 0.89915, val loss: 0.91546\n",
      "Interaction training epoch: 25, train loss: 0.87112, val loss: 0.91913\n",
      "Interaction training epoch: 26, train loss: 0.89515, val loss: 0.93507\n",
      "Interaction training epoch: 27, train loss: 0.89128, val loss: 0.93486\n",
      "Interaction training epoch: 28, train loss: 0.87804, val loss: 0.90657\n",
      "Interaction training epoch: 29, train loss: 0.93852, val loss: 0.96468\n",
      "Interaction training epoch: 30, train loss: 0.87064, val loss: 0.89636\n",
      "Interaction training epoch: 31, train loss: 0.87739, val loss: 0.93007\n",
      "Interaction training epoch: 32, train loss: 0.88622, val loss: 0.90445\n",
      "Interaction training epoch: 33, train loss: 0.89112, val loss: 0.93879\n",
      "Interaction training epoch: 34, train loss: 0.91059, val loss: 0.95098\n",
      "Interaction training epoch: 35, train loss: 0.86169, val loss: 0.88808\n",
      "Interaction training epoch: 36, train loss: 0.84864, val loss: 0.88682\n",
      "Interaction training epoch: 37, train loss: 0.84964, val loss: 0.88105\n",
      "Interaction training epoch: 38, train loss: 0.88602, val loss: 0.93360\n",
      "Interaction training epoch: 39, train loss: 0.85643, val loss: 0.89287\n",
      "Interaction training epoch: 40, train loss: 0.84857, val loss: 0.88433\n",
      "Interaction training epoch: 41, train loss: 0.84201, val loss: 0.88515\n",
      "Interaction training epoch: 42, train loss: 0.83621, val loss: 0.87607\n",
      "Interaction training epoch: 43, train loss: 0.84363, val loss: 0.89273\n",
      "Interaction training epoch: 44, train loss: 0.85890, val loss: 0.88371\n",
      "Interaction training epoch: 45, train loss: 0.84585, val loss: 0.87918\n",
      "Interaction training epoch: 46, train loss: 0.83537, val loss: 0.88445\n",
      "Interaction training epoch: 47, train loss: 0.83868, val loss: 0.88167\n",
      "Interaction training epoch: 48, train loss: 0.82979, val loss: 0.87278\n",
      "Interaction training epoch: 49, train loss: 0.84405, val loss: 0.87878\n",
      "Interaction training epoch: 50, train loss: 0.82586, val loss: 0.87334\n",
      "Interaction training epoch: 51, train loss: 0.82737, val loss: 0.86051\n",
      "Interaction training epoch: 52, train loss: 0.82165, val loss: 0.85475\n",
      "Interaction training epoch: 53, train loss: 0.83069, val loss: 0.87031\n",
      "Interaction training epoch: 54, train loss: 0.84144, val loss: 0.88679\n",
      "Interaction training epoch: 55, train loss: 0.82719, val loss: 0.86068\n",
      "Interaction training epoch: 56, train loss: 0.83289, val loss: 0.88471\n",
      "Interaction training epoch: 57, train loss: 0.82333, val loss: 0.86601\n",
      "Interaction training epoch: 58, train loss: 0.81837, val loss: 0.85370\n",
      "Interaction training epoch: 59, train loss: 0.83124, val loss: 0.87841\n",
      "Interaction training epoch: 60, train loss: 0.83411, val loss: 0.87334\n",
      "Interaction training epoch: 61, train loss: 0.82829, val loss: 0.86841\n",
      "Interaction training epoch: 62, train loss: 0.83145, val loss: 0.87066\n",
      "Interaction training epoch: 63, train loss: 0.81726, val loss: 0.86214\n",
      "Interaction training epoch: 64, train loss: 0.82024, val loss: 0.85882\n",
      "Interaction training epoch: 65, train loss: 0.80574, val loss: 0.84473\n",
      "Interaction training epoch: 66, train loss: 0.83058, val loss: 0.85978\n",
      "Interaction training epoch: 67, train loss: 0.81464, val loss: 0.84883\n",
      "Interaction training epoch: 68, train loss: 0.82131, val loss: 0.86553\n",
      "Interaction training epoch: 69, train loss: 0.82493, val loss: 0.85534\n",
      "Interaction training epoch: 70, train loss: 0.82301, val loss: 0.86255\n",
      "Interaction training epoch: 71, train loss: 0.80834, val loss: 0.84715\n",
      "Interaction training epoch: 72, train loss: 0.82117, val loss: 0.85675\n",
      "Interaction training epoch: 73, train loss: 0.81447, val loss: 0.84487\n",
      "Interaction training epoch: 74, train loss: 0.80942, val loss: 0.84588\n",
      "Interaction training epoch: 75, train loss: 0.82957, val loss: 0.86906\n",
      "Interaction training epoch: 76, train loss: 0.82821, val loss: 0.87187\n",
      "Interaction training epoch: 77, train loss: 0.81342, val loss: 0.85539\n",
      "Interaction training epoch: 78, train loss: 0.80448, val loss: 0.84221\n",
      "Interaction training epoch: 79, train loss: 0.81240, val loss: 0.85378\n",
      "Interaction training epoch: 80, train loss: 0.81627, val loss: 0.85425\n",
      "Interaction training epoch: 81, train loss: 0.81011, val loss: 0.85520\n",
      "Interaction training epoch: 82, train loss: 0.80860, val loss: 0.84546\n",
      "Interaction training epoch: 83, train loss: 0.80930, val loss: 0.84562\n",
      "Interaction training epoch: 84, train loss: 0.82345, val loss: 0.86393\n",
      "Interaction training epoch: 85, train loss: 0.81191, val loss: 0.84849\n",
      "Interaction training epoch: 86, train loss: 0.80488, val loss: 0.84480\n",
      "Interaction training epoch: 87, train loss: 0.81784, val loss: 0.86364\n",
      "Interaction training epoch: 88, train loss: 0.81902, val loss: 0.86871\n",
      "Interaction training epoch: 89, train loss: 0.80272, val loss: 0.84096\n",
      "Interaction training epoch: 90, train loss: 0.81503, val loss: 0.85408\n",
      "Interaction training epoch: 91, train loss: 0.79961, val loss: 0.84404\n",
      "Interaction training epoch: 92, train loss: 0.80278, val loss: 0.84588\n",
      "Interaction training epoch: 93, train loss: 0.81548, val loss: 0.84890\n",
      "Interaction training epoch: 94, train loss: 0.80338, val loss: 0.84040\n",
      "Interaction training epoch: 95, train loss: 0.80954, val loss: 0.85308\n",
      "Interaction training epoch: 96, train loss: 0.80902, val loss: 0.85852\n",
      "Interaction training epoch: 97, train loss: 0.80888, val loss: 0.83806\n",
      "Interaction training epoch: 98, train loss: 0.81339, val loss: 0.85389\n",
      "Interaction training epoch: 99, train loss: 0.80308, val loss: 0.84491\n",
      "Interaction training epoch: 100, train loss: 0.80844, val loss: 0.84957\n",
      "Interaction training epoch: 101, train loss: 0.80770, val loss: 0.85043\n",
      "Interaction training epoch: 102, train loss: 0.79446, val loss: 0.83255\n",
      "Interaction training epoch: 103, train loss: 0.80797, val loss: 0.84788\n",
      "Interaction training epoch: 104, train loss: 0.81002, val loss: 0.85396\n",
      "Interaction training epoch: 105, train loss: 0.80625, val loss: 0.84918\n",
      "Interaction training epoch: 106, train loss: 0.81010, val loss: 0.85374\n",
      "Interaction training epoch: 107, train loss: 0.79593, val loss: 0.84024\n",
      "Interaction training epoch: 108, train loss: 0.80494, val loss: 0.84301\n",
      "Interaction training epoch: 109, train loss: 0.80108, val loss: 0.84211\n",
      "Interaction training epoch: 110, train loss: 0.79831, val loss: 0.83721\n",
      "Interaction training epoch: 111, train loss: 0.79659, val loss: 0.83633\n",
      "Interaction training epoch: 112, train loss: 0.80909, val loss: 0.85505\n",
      "Interaction training epoch: 113, train loss: 0.81925, val loss: 0.85314\n",
      "Interaction training epoch: 114, train loss: 0.79403, val loss: 0.83863\n",
      "Interaction training epoch: 115, train loss: 0.79949, val loss: 0.84115\n",
      "Interaction training epoch: 116, train loss: 0.79979, val loss: 0.83516\n",
      "Interaction training epoch: 117, train loss: 0.80502, val loss: 0.85003\n",
      "Interaction training epoch: 118, train loss: 0.81213, val loss: 0.85208\n",
      "Interaction training epoch: 119, train loss: 0.80217, val loss: 0.84003\n",
      "Interaction training epoch: 120, train loss: 0.79769, val loss: 0.83713\n",
      "Interaction training epoch: 121, train loss: 0.79928, val loss: 0.83642\n",
      "Interaction training epoch: 122, train loss: 0.80296, val loss: 0.84187\n",
      "Interaction training epoch: 123, train loss: 0.81264, val loss: 0.85183\n",
      "Interaction training epoch: 124, train loss: 0.79458, val loss: 0.83286\n",
      "Interaction training epoch: 125, train loss: 0.79408, val loss: 0.84097\n",
      "Interaction training epoch: 126, train loss: 0.79987, val loss: 0.83994\n",
      "Interaction training epoch: 127, train loss: 0.79674, val loss: 0.83777\n",
      "Interaction training epoch: 128, train loss: 0.80044, val loss: 0.83997\n",
      "Interaction training epoch: 129, train loss: 0.80325, val loss: 0.84259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 130, train loss: 0.79272, val loss: 0.83375\n",
      "Interaction training epoch: 131, train loss: 0.79679, val loss: 0.83895\n",
      "Interaction training epoch: 132, train loss: 0.79853, val loss: 0.83734\n",
      "Interaction training epoch: 133, train loss: 0.79660, val loss: 0.83808\n",
      "Interaction training epoch: 134, train loss: 0.79935, val loss: 0.84094\n",
      "Interaction training epoch: 135, train loss: 0.79590, val loss: 0.84162\n",
      "Interaction training epoch: 136, train loss: 0.79456, val loss: 0.83477\n",
      "Interaction training epoch: 137, train loss: 0.80082, val loss: 0.84430\n",
      "Interaction training epoch: 138, train loss: 0.79238, val loss: 0.83345\n",
      "Interaction training epoch: 139, train loss: 0.79721, val loss: 0.84070\n",
      "Interaction training epoch: 140, train loss: 0.79718, val loss: 0.83524\n",
      "Interaction training epoch: 141, train loss: 0.79422, val loss: 0.83553\n",
      "Interaction training epoch: 142, train loss: 0.79345, val loss: 0.83669\n",
      "Interaction training epoch: 143, train loss: 0.79921, val loss: 0.82943\n",
      "Interaction training epoch: 144, train loss: 0.80286, val loss: 0.84109\n",
      "Interaction training epoch: 145, train loss: 0.79884, val loss: 0.83913\n",
      "Interaction training epoch: 146, train loss: 0.80364, val loss: 0.84283\n",
      "Interaction training epoch: 147, train loss: 0.82189, val loss: 0.86515\n",
      "Interaction training epoch: 148, train loss: 0.80161, val loss: 0.83815\n",
      "Interaction training epoch: 149, train loss: 0.79778, val loss: 0.83784\n",
      "Interaction training epoch: 150, train loss: 0.79831, val loss: 0.84445\n",
      "Interaction training epoch: 151, train loss: 0.79691, val loss: 0.83267\n",
      "Interaction training epoch: 152, train loss: 0.79429, val loss: 0.83712\n",
      "Interaction training epoch: 153, train loss: 0.79505, val loss: 0.83271\n",
      "Interaction training epoch: 154, train loss: 0.79517, val loss: 0.84124\n",
      "Interaction training epoch: 155, train loss: 0.79626, val loss: 0.83759\n",
      "Interaction training epoch: 156, train loss: 0.79721, val loss: 0.83542\n",
      "Interaction training epoch: 157, train loss: 0.78976, val loss: 0.83026\n",
      "Interaction training epoch: 158, train loss: 0.81407, val loss: 0.85653\n",
      "Interaction training epoch: 159, train loss: 0.80657, val loss: 0.84196\n",
      "Interaction training epoch: 160, train loss: 0.79407, val loss: 0.83471\n",
      "Interaction training epoch: 161, train loss: 0.79818, val loss: 0.84302\n",
      "Interaction training epoch: 162, train loss: 0.79841, val loss: 0.84042\n",
      "Interaction training epoch: 163, train loss: 0.79547, val loss: 0.84065\n",
      "Interaction training epoch: 164, train loss: 0.80073, val loss: 0.83509\n",
      "Interaction training epoch: 165, train loss: 0.80684, val loss: 0.84476\n",
      "Interaction training epoch: 166, train loss: 0.79331, val loss: 0.83657\n",
      "Interaction training epoch: 167, train loss: 0.79184, val loss: 0.83025\n",
      "Interaction training epoch: 168, train loss: 0.79133, val loss: 0.83439\n",
      "Interaction training epoch: 169, train loss: 0.79993, val loss: 0.83906\n",
      "Interaction training epoch: 170, train loss: 0.80083, val loss: 0.84369\n",
      "Interaction training epoch: 171, train loss: 0.80296, val loss: 0.84730\n",
      "Interaction training epoch: 172, train loss: 0.79541, val loss: 0.83687\n",
      "Interaction training epoch: 173, train loss: 0.79920, val loss: 0.84526\n",
      "Interaction training epoch: 174, train loss: 0.80088, val loss: 0.83141\n",
      "Interaction training epoch: 175, train loss: 0.79364, val loss: 0.83919\n",
      "Interaction training epoch: 176, train loss: 0.79875, val loss: 0.83540\n",
      "Interaction training epoch: 177, train loss: 0.79588, val loss: 0.84308\n",
      "Interaction training epoch: 178, train loss: 0.79596, val loss: 0.83199\n",
      "Interaction training epoch: 179, train loss: 0.79644, val loss: 0.83775\n",
      "Interaction training epoch: 180, train loss: 0.79934, val loss: 0.84277\n",
      "Interaction training epoch: 181, train loss: 0.80009, val loss: 0.84269\n",
      "Interaction training epoch: 182, train loss: 0.79748, val loss: 0.83921\n",
      "Interaction training epoch: 183, train loss: 0.79769, val loss: 0.83715\n",
      "Interaction training epoch: 184, train loss: 0.79538, val loss: 0.83949\n",
      "Interaction training epoch: 185, train loss: 0.80090, val loss: 0.84437\n",
      "Interaction training epoch: 186, train loss: 0.80051, val loss: 0.84054\n",
      "Interaction training epoch: 187, train loss: 0.79346, val loss: 0.83476\n",
      "Interaction training epoch: 188, train loss: 0.79326, val loss: 0.83265\n",
      "Interaction training epoch: 189, train loss: 0.79345, val loss: 0.84040\n",
      "Interaction training epoch: 190, train loss: 0.79454, val loss: 0.83378\n",
      "Interaction training epoch: 191, train loss: 0.79368, val loss: 0.83681\n",
      "Interaction training epoch: 192, train loss: 0.79302, val loss: 0.83751\n",
      "Interaction training epoch: 193, train loss: 0.79203, val loss: 0.82936\n",
      "Interaction training epoch: 194, train loss: 0.80001, val loss: 0.84812\n",
      "Interaction training epoch: 195, train loss: 0.79153, val loss: 0.82876\n",
      "Interaction training epoch: 196, train loss: 0.78968, val loss: 0.83315\n",
      "Interaction training epoch: 197, train loss: 0.78832, val loss: 0.83064\n",
      "Interaction training epoch: 198, train loss: 0.78980, val loss: 0.83056\n",
      "Interaction training epoch: 199, train loss: 0.79171, val loss: 0.83679\n",
      "Interaction training epoch: 200, train loss: 0.79541, val loss: 0.83402\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########No main interaction is pruned, the tuning step is skipped.\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 36.4766845703125\n",
      "After the gam stage, training error is 0.79541 , validation error is 0.83402\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 19.732004\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.656615 validation MAE=0.789040,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.610738 validation MAE=0.770333,rank=5\n",
      "[SoftImpute] Iter 3: observed MAE=0.571910 validation MAE=0.753734,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.539797 validation MAE=0.738915,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.513553 validation MAE=0.725263,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.491514 validation MAE=0.713128,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.473120 validation MAE=0.702182,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.457130 validation MAE=0.691841,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.440527 validation MAE=0.681797,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.425955 validation MAE=0.672275,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.412899 validation MAE=0.663792,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.400054 validation MAE=0.655985,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.389147 validation MAE=0.648306,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.379331 validation MAE=0.641763,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.369946 validation MAE=0.634992,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.360889 validation MAE=0.628901,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.352659 validation MAE=0.623405,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.345060 validation MAE=0.617623,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.338587 validation MAE=0.613075,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.331419 validation MAE=0.608333,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.325679 validation MAE=0.603814,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.320989 validation MAE=0.599919,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.315342 validation MAE=0.595726,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.310042 validation MAE=0.592339,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.305224 validation MAE=0.588793,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.302518 validation MAE=0.586645,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.298887 validation MAE=0.583500,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.294967 validation MAE=0.580456,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.291860 validation MAE=0.578693,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.287050 validation MAE=0.576005,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.284410 validation MAE=0.574213,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.281599 validation MAE=0.571967,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.278809 validation MAE=0.570407,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.276169 validation MAE=0.568915,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.274223 validation MAE=0.566898,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.272023 validation MAE=0.565559,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.269169 validation MAE=0.564039,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.267122 validation MAE=0.563020,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.264463 validation MAE=0.561475,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.263348 validation MAE=0.561018,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.261438 validation MAE=0.559433,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.258596 validation MAE=0.558539,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.258004 validation MAE=0.557797,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.256041 validation MAE=0.557327,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.253062 validation MAE=0.556345,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.251951 validation MAE=0.555510,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.249771 validation MAE=0.554978,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.248160 validation MAE=0.553892,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.247748 validation MAE=0.553695,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.246760 validation MAE=0.552814,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.245585 validation MAE=0.551878,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.244052 validation MAE=0.551533,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.243079 validation MAE=0.550995,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.241570 validation MAE=0.550903,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.239460 validation MAE=0.550897,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.238690 validation MAE=0.550172,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.237630 validation MAE=0.549463,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.236838 validation MAE=0.549001,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.236756 validation MAE=0.548678,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.234745 validation MAE=0.548085,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.233773 validation MAE=0.547368,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.231945 validation MAE=0.547203,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.231754 validation MAE=0.546956,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.230610 validation MAE=0.546157,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.229676 validation MAE=0.546693,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.228023 validation MAE=0.545606,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.227769 validation MAE=0.545281,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.227392 validation MAE=0.544774,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.226301 validation MAE=0.544791,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.225529 validation MAE=0.544050,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.225138 validation MAE=0.543962,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.223255 validation MAE=0.543759,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.222817 validation MAE=0.543233,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.221921 validation MAE=0.542819,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.221356 validation MAE=0.542693,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.220858 validation MAE=0.542021,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.219943 validation MAE=0.541842,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.219006 validation MAE=0.541861,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.218165 validation MAE=0.541514,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.217152 validation MAE=0.540789,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.217242 validation MAE=0.540669,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.216494 validation MAE=0.540612,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.216086 validation MAE=0.540066,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.215864 validation MAE=0.539762,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.215641 validation MAE=0.539510,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.214570 validation MAE=0.539665,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.213799 validation MAE=0.539570,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.212752 validation MAE=0.538836,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.212788 validation MAE=0.538517,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.211921 validation MAE=0.538760,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.211759 validation MAE=0.538452,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.210852 validation MAE=0.537894,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.208963 validation MAE=0.538036,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.210048 validation MAE=0.537737,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.210327 validation MAE=0.537200,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.209168 validation MAE=0.537238,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.208369 validation MAE=0.537115,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.208032 validation MAE=0.536548,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.208127 validation MAE=0.536767,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.207100 validation MAE=0.536091,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.206515 validation MAE=0.536060,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.206097 validation MAE=0.536320,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.204937 validation MAE=0.535729,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.204856 validation MAE=0.535350,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.205098 validation MAE=0.535230,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.203911 validation MAE=0.534580,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.204085 validation MAE=0.534492,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.203755 validation MAE=0.534208,rank=5\n",
      "[SoftImpute] Iter 109: observed MAE=0.203060 validation MAE=0.534038,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.203154 validation MAE=0.533620,rank=5\n",
      "[SoftImpute] Iter 111: observed MAE=0.202369 validation MAE=0.533537,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.201213 validation MAE=0.532919,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 113: observed MAE=0.201589 validation MAE=0.532012,rank=5\n",
      "[SoftImpute] Iter 114: observed MAE=0.201516 validation MAE=0.532360,rank=5\n",
      "[SoftImpute] Iter 115: observed MAE=0.200603 validation MAE=0.532112,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.200949 validation MAE=0.532243,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.199564 validation MAE=0.531813,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.199108 validation MAE=0.531073,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.199215 validation MAE=0.531336,rank=5\n",
      "[SoftImpute] Iter 120: observed MAE=0.200140 validation MAE=0.531529,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.199708 validation MAE=0.530751,rank=5\n",
      "[SoftImpute] Iter 122: observed MAE=0.199347 validation MAE=0.530016,rank=5\n",
      "[SoftImpute] Iter 123: observed MAE=0.199014 validation MAE=0.530205,rank=5\n",
      "[SoftImpute] Iter 124: observed MAE=0.198688 validation MAE=0.529692,rank=5\n",
      "[SoftImpute] Iter 125: observed MAE=0.198056 validation MAE=0.529511,rank=5\n",
      "[SoftImpute] Iter 126: observed MAE=0.197777 validation MAE=0.529827,rank=5\n",
      "[SoftImpute] Iter 127: observed MAE=0.197246 validation MAE=0.529046,rank=5\n",
      "[SoftImpute] Iter 128: observed MAE=0.197134 validation MAE=0.528897,rank=5\n",
      "[SoftImpute] Iter 129: observed MAE=0.196817 validation MAE=0.528821,rank=5\n",
      "[SoftImpute] Iter 130: observed MAE=0.196372 validation MAE=0.528770,rank=5\n",
      "[SoftImpute] Iter 131: observed MAE=0.196388 validation MAE=0.528712,rank=5\n",
      "[SoftImpute] Iter 132: observed MAE=0.195663 validation MAE=0.528254,rank=5\n",
      "[SoftImpute] Iter 133: observed MAE=0.195345 validation MAE=0.528119,rank=5\n",
      "[SoftImpute] Iter 134: observed MAE=0.194828 validation MAE=0.527663,rank=5\n",
      "[SoftImpute] Iter 135: observed MAE=0.194548 validation MAE=0.527889,rank=5\n",
      "[SoftImpute] Iter 136: observed MAE=0.194672 validation MAE=0.527179,rank=5\n",
      "[SoftImpute] Iter 137: observed MAE=0.194381 validation MAE=0.526994,rank=5\n",
      "[SoftImpute] Iter 138: observed MAE=0.194721 validation MAE=0.526754,rank=5\n",
      "[SoftImpute] Iter 139: observed MAE=0.193619 validation MAE=0.526727,rank=5\n",
      "[SoftImpute] Iter 140: observed MAE=0.193019 validation MAE=0.526695,rank=5\n",
      "[SoftImpute] Iter 141: observed MAE=0.191810 validation MAE=0.526536,rank=5\n",
      "[SoftImpute] Iter 142: observed MAE=0.192011 validation MAE=0.526764,rank=5\n",
      "[SoftImpute] Iter 143: observed MAE=0.191681 validation MAE=0.525699,rank=5\n",
      "[SoftImpute] Iter 144: observed MAE=0.191682 validation MAE=0.525677,rank=5\n",
      "[SoftImpute] Iter 145: observed MAE=0.192408 validation MAE=0.525814,rank=5\n",
      "[SoftImpute] Iter 146: observed MAE=0.192292 validation MAE=0.525082,rank=5\n",
      "[SoftImpute] Iter 147: observed MAE=0.190790 validation MAE=0.524870,rank=5\n",
      "[SoftImpute] Iter 148: observed MAE=0.191054 validation MAE=0.524594,rank=5\n",
      "[SoftImpute] Iter 149: observed MAE=0.191294 validation MAE=0.524594,rank=5\n",
      "[SoftImpute] Iter 150: observed MAE=0.191199 validation MAE=0.524668,rank=5\n",
      "[SoftImpute] Iter 151: observed MAE=0.190174 validation MAE=0.524345,rank=5\n",
      "[SoftImpute] Iter 152: observed MAE=0.189597 validation MAE=0.523913,rank=5\n",
      "[SoftImpute] Iter 153: observed MAE=0.189131 validation MAE=0.523653,rank=5\n",
      "[SoftImpute] Iter 154: observed MAE=0.189392 validation MAE=0.523501,rank=5\n",
      "[SoftImpute] Iter 155: observed MAE=0.189727 validation MAE=0.523399,rank=5\n",
      "[SoftImpute] Iter 156: observed MAE=0.188688 validation MAE=0.523248,rank=5\n",
      "[SoftImpute] Iter 157: observed MAE=0.189056 validation MAE=0.522922,rank=5\n",
      "[SoftImpute] Iter 158: observed MAE=0.188318 validation MAE=0.522936,rank=5\n",
      "[SoftImpute] Iter 159: observed MAE=0.188347 validation MAE=0.522486,rank=5\n",
      "[SoftImpute] Iter 160: observed MAE=0.188033 validation MAE=0.522614,rank=5\n",
      "[SoftImpute] Iter 161: observed MAE=0.188553 validation MAE=0.522164,rank=5\n",
      "[SoftImpute] Iter 162: observed MAE=0.186588 validation MAE=0.521948,rank=5\n",
      "[SoftImpute] Iter 163: observed MAE=0.186272 validation MAE=0.522096,rank=5\n",
      "[SoftImpute] Iter 164: observed MAE=0.186157 validation MAE=0.521468,rank=5\n",
      "[SoftImpute] Iter 165: observed MAE=0.185983 validation MAE=0.521684,rank=5\n",
      "[SoftImpute] Iter 166: observed MAE=0.186108 validation MAE=0.521267,rank=5\n",
      "[SoftImpute] Iter 167: observed MAE=0.186479 validation MAE=0.520840,rank=5\n",
      "[SoftImpute] Iter 168: observed MAE=0.186381 validation MAE=0.520854,rank=5\n",
      "[SoftImpute] Iter 169: observed MAE=0.185622 validation MAE=0.520463,rank=5\n",
      "[SoftImpute] Iter 170: observed MAE=0.186602 validation MAE=0.520288,rank=5\n",
      "[SoftImpute] Iter 171: observed MAE=0.185469 validation MAE=0.520301,rank=5\n",
      "[SoftImpute] Iter 172: observed MAE=0.185933 validation MAE=0.520226,rank=5\n",
      "[SoftImpute] Iter 173: observed MAE=0.185053 validation MAE=0.520337,rank=5\n",
      "[SoftImpute] Iter 174: observed MAE=0.184390 validation MAE=0.519922,rank=5\n",
      "[SoftImpute] Iter 175: observed MAE=0.184542 validation MAE=0.519876,rank=5\n",
      "[SoftImpute] Iter 176: observed MAE=0.185152 validation MAE=0.519501,rank=5\n",
      "[SoftImpute] Iter 177: observed MAE=0.184040 validation MAE=0.519318,rank=5\n",
      "[SoftImpute] Iter 178: observed MAE=0.183690 validation MAE=0.519083,rank=5\n",
      "[SoftImpute] Iter 179: observed MAE=0.183546 validation MAE=0.519049,rank=5\n",
      "[SoftImpute] Iter 180: observed MAE=0.182274 validation MAE=0.518820,rank=5\n",
      "[SoftImpute] Iter 181: observed MAE=0.182762 validation MAE=0.518780,rank=5\n",
      "[SoftImpute] Iter 182: observed MAE=0.183257 validation MAE=0.518542,rank=5\n",
      "[SoftImpute] Iter 183: observed MAE=0.182693 validation MAE=0.518174,rank=5\n",
      "[SoftImpute] Iter 184: observed MAE=0.182257 validation MAE=0.518441,rank=5\n",
      "[SoftImpute] Iter 185: observed MAE=0.182782 validation MAE=0.517796,rank=5\n",
      "[SoftImpute] Iter 186: observed MAE=0.181446 validation MAE=0.517955,rank=5\n",
      "[SoftImpute] Iter 187: observed MAE=0.181134 validation MAE=0.517702,rank=5\n",
      "[SoftImpute] Iter 188: observed MAE=0.182037 validation MAE=0.517735,rank=5\n",
      "[SoftImpute] Iter 189: observed MAE=0.182201 validation MAE=0.517482,rank=5\n",
      "[SoftImpute] Iter 190: observed MAE=0.181431 validation MAE=0.517372,rank=5\n",
      "[SoftImpute] Iter 191: observed MAE=0.180553 validation MAE=0.516742,rank=5\n",
      "[SoftImpute] Iter 192: observed MAE=0.180961 validation MAE=0.516414,rank=5\n",
      "[SoftImpute] Iter 193: observed MAE=0.180445 validation MAE=0.516126,rank=5\n",
      "[SoftImpute] Iter 194: observed MAE=0.181404 validation MAE=0.515700,rank=5\n",
      "[SoftImpute] Iter 195: observed MAE=0.181028 validation MAE=0.515681,rank=5\n",
      "[SoftImpute] Iter 196: observed MAE=0.181034 validation MAE=0.515594,rank=5\n",
      "[SoftImpute] Iter 197: observed MAE=0.180365 validation MAE=0.515476,rank=5\n",
      "[SoftImpute] Iter 198: observed MAE=0.180418 validation MAE=0.515209,rank=5\n",
      "[SoftImpute] Iter 199: observed MAE=0.179928 validation MAE=0.514729,rank=5\n",
      "[SoftImpute] Iter 200: observed MAE=0.179895 validation MAE=0.514986,rank=5\n",
      "[SoftImpute] Stopped after iteration 200 for lambda=0.394640\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 6.020781755447388\n",
      "After the matrix factor stage, training error is 0.17990, validation error is 0.51499\n",
      "9\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.04255, val loss: 4.15387\n",
      "Main effects training epoch: 2, train loss: 3.86214, val loss: 3.95015\n",
      "Main effects training epoch: 3, train loss: 3.63275, val loss: 3.69324\n",
      "Main effects training epoch: 4, train loss: 3.44958, val loss: 3.50735\n",
      "Main effects training epoch: 5, train loss: 3.44754, val loss: 3.50975\n",
      "Main effects training epoch: 6, train loss: 3.26437, val loss: 3.31337\n",
      "Main effects training epoch: 7, train loss: 3.30651, val loss: 3.37729\n",
      "Main effects training epoch: 8, train loss: 3.26579, val loss: 3.34316\n",
      "Main effects training epoch: 9, train loss: 3.22463, val loss: 3.30307\n",
      "Main effects training epoch: 10, train loss: 3.15847, val loss: 3.23201\n",
      "Main effects training epoch: 11, train loss: 3.11052, val loss: 3.17994\n",
      "Main effects training epoch: 12, train loss: 3.05621, val loss: 3.13076\n",
      "Main effects training epoch: 13, train loss: 2.99726, val loss: 3.09050\n",
      "Main effects training epoch: 14, train loss: 2.91089, val loss: 3.02710\n",
      "Main effects training epoch: 15, train loss: 2.86638, val loss: 2.98567\n",
      "Main effects training epoch: 16, train loss: 2.82910, val loss: 2.95560\n",
      "Main effects training epoch: 17, train loss: 2.73378, val loss: 2.86662\n",
      "Main effects training epoch: 18, train loss: 2.66687, val loss: 2.80265\n",
      "Main effects training epoch: 19, train loss: 2.57174, val loss: 2.69449\n",
      "Main effects training epoch: 20, train loss: 2.52667, val loss: 2.63886\n",
      "Main effects training epoch: 21, train loss: 2.51737, val loss: 2.63117\n",
      "Main effects training epoch: 22, train loss: 2.45939, val loss: 2.56848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 23, train loss: 2.41466, val loss: 2.52803\n",
      "Main effects training epoch: 24, train loss: 2.37454, val loss: 2.48142\n",
      "Main effects training epoch: 25, train loss: 2.37290, val loss: 2.48585\n",
      "Main effects training epoch: 26, train loss: 2.29809, val loss: 2.41625\n",
      "Main effects training epoch: 27, train loss: 2.29314, val loss: 2.41133\n",
      "Main effects training epoch: 28, train loss: 2.22575, val loss: 2.34577\n",
      "Main effects training epoch: 29, train loss: 2.22338, val loss: 2.35105\n",
      "Main effects training epoch: 30, train loss: 2.21272, val loss: 2.33092\n",
      "Main effects training epoch: 31, train loss: 2.16371, val loss: 2.28780\n",
      "Main effects training epoch: 32, train loss: 2.13626, val loss: 2.26884\n",
      "Main effects training epoch: 33, train loss: 2.13573, val loss: 2.26308\n",
      "Main effects training epoch: 34, train loss: 2.08990, val loss: 2.21951\n",
      "Main effects training epoch: 35, train loss: 2.06684, val loss: 2.20257\n",
      "Main effects training epoch: 36, train loss: 2.05013, val loss: 2.18730\n",
      "Main effects training epoch: 37, train loss: 2.01705, val loss: 2.15815\n",
      "Main effects training epoch: 38, train loss: 2.00298, val loss: 2.13484\n",
      "Main effects training epoch: 39, train loss: 2.00796, val loss: 2.15061\n",
      "Main effects training epoch: 40, train loss: 1.97797, val loss: 2.12415\n",
      "Main effects training epoch: 41, train loss: 1.92085, val loss: 2.05736\n",
      "Main effects training epoch: 42, train loss: 1.94884, val loss: 2.09467\n",
      "Main effects training epoch: 43, train loss: 1.92817, val loss: 2.06021\n",
      "Main effects training epoch: 44, train loss: 1.90767, val loss: 2.04875\n",
      "Main effects training epoch: 45, train loss: 1.91041, val loss: 2.05304\n",
      "Main effects training epoch: 46, train loss: 1.88885, val loss: 2.02385\n",
      "Main effects training epoch: 47, train loss: 1.86189, val loss: 1.99496\n",
      "Main effects training epoch: 48, train loss: 1.86854, val loss: 2.01332\n",
      "Main effects training epoch: 49, train loss: 1.86368, val loss: 2.00139\n",
      "Main effects training epoch: 50, train loss: 1.83394, val loss: 1.97138\n",
      "Main effects training epoch: 51, train loss: 1.82924, val loss: 1.97312\n",
      "Main effects training epoch: 52, train loss: 1.82443, val loss: 1.96050\n",
      "Main effects training epoch: 53, train loss: 1.81424, val loss: 1.94269\n",
      "Main effects training epoch: 54, train loss: 1.80550, val loss: 1.94665\n",
      "Main effects training epoch: 55, train loss: 1.80729, val loss: 1.94131\n",
      "Main effects training epoch: 56, train loss: 1.79603, val loss: 1.92748\n",
      "Main effects training epoch: 57, train loss: 1.78999, val loss: 1.92180\n",
      "Main effects training epoch: 58, train loss: 1.78256, val loss: 1.90862\n",
      "Main effects training epoch: 59, train loss: 1.77153, val loss: 1.90007\n",
      "Main effects training epoch: 60, train loss: 1.77086, val loss: 1.90103\n",
      "Main effects training epoch: 61, train loss: 1.77228, val loss: 1.90394\n",
      "Main effects training epoch: 62, train loss: 1.76703, val loss: 1.88911\n",
      "Main effects training epoch: 63, train loss: 1.75571, val loss: 1.88218\n",
      "Main effects training epoch: 64, train loss: 1.75318, val loss: 1.87277\n",
      "Main effects training epoch: 65, train loss: 1.75527, val loss: 1.87850\n",
      "Main effects training epoch: 66, train loss: 1.74475, val loss: 1.86198\n",
      "Main effects training epoch: 67, train loss: 1.74282, val loss: 1.86598\n",
      "Main effects training epoch: 68, train loss: 1.74284, val loss: 1.86600\n",
      "Main effects training epoch: 69, train loss: 1.73899, val loss: 1.85959\n",
      "Main effects training epoch: 70, train loss: 1.73566, val loss: 1.84929\n",
      "Main effects training epoch: 71, train loss: 1.72782, val loss: 1.84431\n",
      "Main effects training epoch: 72, train loss: 1.73213, val loss: 1.84463\n",
      "Main effects training epoch: 73, train loss: 1.72829, val loss: 1.84620\n",
      "Main effects training epoch: 74, train loss: 1.72503, val loss: 1.83580\n",
      "Main effects training epoch: 75, train loss: 1.71714, val loss: 1.83013\n",
      "Main effects training epoch: 76, train loss: 1.71701, val loss: 1.82756\n",
      "Main effects training epoch: 77, train loss: 1.71652, val loss: 1.83018\n",
      "Main effects training epoch: 78, train loss: 1.71320, val loss: 1.81774\n",
      "Main effects training epoch: 79, train loss: 1.70992, val loss: 1.81812\n",
      "Main effects training epoch: 80, train loss: 1.71226, val loss: 1.82254\n",
      "Main effects training epoch: 81, train loss: 1.71225, val loss: 1.82250\n",
      "Main effects training epoch: 82, train loss: 1.70660, val loss: 1.81483\n",
      "Main effects training epoch: 83, train loss: 1.70529, val loss: 1.80509\n",
      "Main effects training epoch: 84, train loss: 1.70535, val loss: 1.81213\n",
      "Main effects training epoch: 85, train loss: 1.70100, val loss: 1.80385\n",
      "Main effects training epoch: 86, train loss: 1.70004, val loss: 1.80562\n",
      "Main effects training epoch: 87, train loss: 1.69958, val loss: 1.80424\n",
      "Main effects training epoch: 88, train loss: 1.69394, val loss: 1.79897\n",
      "Main effects training epoch: 89, train loss: 1.69423, val loss: 1.79269\n",
      "Main effects training epoch: 90, train loss: 1.69019, val loss: 1.79818\n",
      "Main effects training epoch: 91, train loss: 1.68206, val loss: 1.78579\n",
      "Main effects training epoch: 92, train loss: 1.68488, val loss: 1.79111\n",
      "Main effects training epoch: 93, train loss: 1.68344, val loss: 1.79558\n",
      "Main effects training epoch: 94, train loss: 1.67328, val loss: 1.77897\n",
      "Main effects training epoch: 95, train loss: 1.67178, val loss: 1.76391\n",
      "Main effects training epoch: 96, train loss: 1.66647, val loss: 1.77458\n",
      "Main effects training epoch: 97, train loss: 1.66208, val loss: 1.76232\n",
      "Main effects training epoch: 98, train loss: 1.65745, val loss: 1.76016\n",
      "Main effects training epoch: 99, train loss: 1.65201, val loss: 1.75406\n",
      "Main effects training epoch: 100, train loss: 1.64978, val loss: 1.74214\n",
      "Main effects training epoch: 101, train loss: 1.64794, val loss: 1.75039\n",
      "Main effects training epoch: 102, train loss: 1.64143, val loss: 1.73390\n",
      "Main effects training epoch: 103, train loss: 1.63235, val loss: 1.72200\n",
      "Main effects training epoch: 104, train loss: 1.63051, val loss: 1.72213\n",
      "Main effects training epoch: 105, train loss: 1.62849, val loss: 1.72341\n",
      "Main effects training epoch: 106, train loss: 1.62210, val loss: 1.70416\n",
      "Main effects training epoch: 107, train loss: 1.62109, val loss: 1.71224\n",
      "Main effects training epoch: 108, train loss: 1.61949, val loss: 1.70980\n",
      "Main effects training epoch: 109, train loss: 1.61476, val loss: 1.70385\n",
      "Main effects training epoch: 110, train loss: 1.61213, val loss: 1.70620\n",
      "Main effects training epoch: 111, train loss: 1.61815, val loss: 1.71101\n",
      "Main effects training epoch: 112, train loss: 1.61042, val loss: 1.70237\n",
      "Main effects training epoch: 113, train loss: 1.61257, val loss: 1.69835\n",
      "Main effects training epoch: 114, train loss: 1.61089, val loss: 1.70553\n",
      "Main effects training epoch: 115, train loss: 1.61487, val loss: 1.70313\n",
      "Main effects training epoch: 116, train loss: 1.61673, val loss: 1.70264\n",
      "Main effects training epoch: 117, train loss: 1.61404, val loss: 1.70557\n",
      "Main effects training epoch: 118, train loss: 1.60936, val loss: 1.69712\n",
      "Main effects training epoch: 119, train loss: 1.60879, val loss: 1.68935\n",
      "Main effects training epoch: 120, train loss: 1.60919, val loss: 1.70263\n",
      "Main effects training epoch: 121, train loss: 1.61779, val loss: 1.71621\n",
      "Main effects training epoch: 122, train loss: 1.61080, val loss: 1.69149\n",
      "Main effects training epoch: 123, train loss: 1.60658, val loss: 1.71038\n",
      "Main effects training epoch: 124, train loss: 1.60733, val loss: 1.69877\n",
      "Main effects training epoch: 125, train loss: 1.60695, val loss: 1.68973\n",
      "Main effects training epoch: 126, train loss: 1.60133, val loss: 1.69613\n",
      "Main effects training epoch: 127, train loss: 1.60495, val loss: 1.69935\n",
      "Main effects training epoch: 128, train loss: 1.60545, val loss: 1.69275\n",
      "Main effects training epoch: 129, train loss: 1.60463, val loss: 1.70317\n",
      "Main effects training epoch: 130, train loss: 1.60121, val loss: 1.69748\n",
      "Main effects training epoch: 131, train loss: 1.60137, val loss: 1.69617\n",
      "Main effects training epoch: 132, train loss: 1.59867, val loss: 1.69319\n",
      "Main effects training epoch: 133, train loss: 1.60274, val loss: 1.69828\n",
      "Main effects training epoch: 134, train loss: 1.60095, val loss: 1.69755\n",
      "Main effects training epoch: 135, train loss: 1.59924, val loss: 1.69095\n",
      "Main effects training epoch: 136, train loss: 1.59859, val loss: 1.69844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 137, train loss: 1.59555, val loss: 1.69487\n",
      "Main effects training epoch: 138, train loss: 1.59774, val loss: 1.69355\n",
      "Main effects training epoch: 139, train loss: 1.59422, val loss: 1.69399\n",
      "Main effects training epoch: 140, train loss: 1.59439, val loss: 1.69916\n",
      "Main effects training epoch: 141, train loss: 1.59416, val loss: 1.69023\n",
      "Main effects training epoch: 142, train loss: 1.60124, val loss: 1.69961\n",
      "Main effects training epoch: 143, train loss: 1.59338, val loss: 1.69544\n",
      "Main effects training epoch: 144, train loss: 1.59394, val loss: 1.68894\n",
      "Main effects training epoch: 145, train loss: 1.59272, val loss: 1.69402\n",
      "Main effects training epoch: 146, train loss: 1.59447, val loss: 1.70076\n",
      "Main effects training epoch: 147, train loss: 1.59545, val loss: 1.69745\n",
      "Main effects training epoch: 148, train loss: 1.59102, val loss: 1.69427\n",
      "Main effects training epoch: 149, train loss: 1.59312, val loss: 1.69784\n",
      "Main effects training epoch: 150, train loss: 1.59740, val loss: 1.69898\n",
      "Main effects training epoch: 151, train loss: 1.61280, val loss: 1.72336\n",
      "Main effects training epoch: 152, train loss: 1.59736, val loss: 1.70234\n",
      "Main effects training epoch: 153, train loss: 1.58797, val loss: 1.69233\n",
      "Main effects training epoch: 154, train loss: 1.59346, val loss: 1.69848\n",
      "Main effects training epoch: 155, train loss: 1.59402, val loss: 1.69478\n",
      "Main effects training epoch: 156, train loss: 1.58598, val loss: 1.69031\n",
      "Main effects training epoch: 157, train loss: 1.58685, val loss: 1.69369\n",
      "Main effects training epoch: 158, train loss: 1.58536, val loss: 1.69395\n",
      "Main effects training epoch: 159, train loss: 1.58361, val loss: 1.69028\n",
      "Main effects training epoch: 160, train loss: 1.58246, val loss: 1.69071\n",
      "Main effects training epoch: 161, train loss: 1.58293, val loss: 1.69277\n",
      "Main effects training epoch: 162, train loss: 1.58114, val loss: 1.69076\n",
      "Main effects training epoch: 163, train loss: 1.58347, val loss: 1.69066\n",
      "Main effects training epoch: 164, train loss: 1.58085, val loss: 1.68507\n",
      "Main effects training epoch: 165, train loss: 1.58003, val loss: 1.69117\n",
      "Main effects training epoch: 166, train loss: 1.58251, val loss: 1.68632\n",
      "Main effects training epoch: 167, train loss: 1.57933, val loss: 1.68786\n",
      "Main effects training epoch: 168, train loss: 1.57775, val loss: 1.68970\n",
      "Main effects training epoch: 169, train loss: 1.58348, val loss: 1.68579\n",
      "Main effects training epoch: 170, train loss: 1.57902, val loss: 1.68924\n",
      "Main effects training epoch: 171, train loss: 1.57964, val loss: 1.69368\n",
      "Main effects training epoch: 172, train loss: 1.57743, val loss: 1.68647\n",
      "Main effects training epoch: 173, train loss: 1.57726, val loss: 1.69099\n",
      "Main effects training epoch: 174, train loss: 1.57888, val loss: 1.69030\n",
      "Main effects training epoch: 175, train loss: 1.57769, val loss: 1.68573\n",
      "Main effects training epoch: 176, train loss: 1.57715, val loss: 1.68319\n",
      "Main effects training epoch: 177, train loss: 1.57438, val loss: 1.68807\n",
      "Main effects training epoch: 178, train loss: 1.57897, val loss: 1.68347\n",
      "Main effects training epoch: 179, train loss: 1.57792, val loss: 1.68935\n",
      "Main effects training epoch: 180, train loss: 1.57297, val loss: 1.68057\n",
      "Main effects training epoch: 181, train loss: 1.57135, val loss: 1.68459\n",
      "Main effects training epoch: 182, train loss: 1.57127, val loss: 1.67960\n",
      "Main effects training epoch: 183, train loss: 1.56763, val loss: 1.67858\n",
      "Main effects training epoch: 184, train loss: 1.57550, val loss: 1.68205\n",
      "Main effects training epoch: 185, train loss: 1.56821, val loss: 1.68172\n",
      "Main effects training epoch: 186, train loss: 1.56767, val loss: 1.67038\n",
      "Main effects training epoch: 187, train loss: 1.56623, val loss: 1.66777\n",
      "Main effects training epoch: 188, train loss: 1.56066, val loss: 1.66471\n",
      "Main effects training epoch: 189, train loss: 1.56623, val loss: 1.67967\n",
      "Main effects training epoch: 190, train loss: 1.56567, val loss: 1.67106\n",
      "Main effects training epoch: 191, train loss: 1.56167, val loss: 1.66391\n",
      "Main effects training epoch: 192, train loss: 1.55867, val loss: 1.66896\n",
      "Main effects training epoch: 193, train loss: 1.55886, val loss: 1.66219\n",
      "Main effects training epoch: 194, train loss: 1.55522, val loss: 1.66150\n",
      "Main effects training epoch: 195, train loss: 1.55502, val loss: 1.66006\n",
      "Main effects training epoch: 196, train loss: 1.55587, val loss: 1.65954\n",
      "Main effects training epoch: 197, train loss: 1.55599, val loss: 1.66175\n",
      "Main effects training epoch: 198, train loss: 1.55439, val loss: 1.66095\n",
      "Main effects training epoch: 199, train loss: 1.55166, val loss: 1.65447\n",
      "Main effects training epoch: 200, train loss: 1.54990, val loss: 1.65339\n",
      "Main effects training epoch: 201, train loss: 1.54884, val loss: 1.66146\n",
      "Main effects training epoch: 202, train loss: 1.55767, val loss: 1.65278\n",
      "Main effects training epoch: 203, train loss: 1.55858, val loss: 1.67376\n",
      "Main effects training epoch: 204, train loss: 1.56349, val loss: 1.65360\n",
      "Main effects training epoch: 205, train loss: 1.55483, val loss: 1.66469\n",
      "Main effects training epoch: 206, train loss: 1.55814, val loss: 1.65939\n",
      "Main effects training epoch: 207, train loss: 1.55755, val loss: 1.67075\n",
      "Main effects training epoch: 208, train loss: 1.55092, val loss: 1.65393\n",
      "Main effects training epoch: 209, train loss: 1.55152, val loss: 1.66169\n",
      "Main effects training epoch: 210, train loss: 1.54719, val loss: 1.64984\n",
      "Main effects training epoch: 211, train loss: 1.54826, val loss: 1.65229\n",
      "Main effects training epoch: 212, train loss: 1.55183, val loss: 1.65685\n",
      "Main effects training epoch: 213, train loss: 1.54641, val loss: 1.64872\n",
      "Main effects training epoch: 214, train loss: 1.55968, val loss: 1.66118\n",
      "Main effects training epoch: 215, train loss: 1.55411, val loss: 1.65492\n",
      "Main effects training epoch: 216, train loss: 1.54989, val loss: 1.65211\n",
      "Main effects training epoch: 217, train loss: 1.55166, val loss: 1.65245\n",
      "Main effects training epoch: 218, train loss: 1.55062, val loss: 1.66738\n",
      "Main effects training epoch: 219, train loss: 1.54738, val loss: 1.64531\n",
      "Main effects training epoch: 220, train loss: 1.54556, val loss: 1.65326\n",
      "Main effects training epoch: 221, train loss: 1.55104, val loss: 1.65808\n",
      "Main effects training epoch: 222, train loss: 1.54298, val loss: 1.64935\n",
      "Main effects training epoch: 223, train loss: 1.54414, val loss: 1.65195\n",
      "Main effects training epoch: 224, train loss: 1.54975, val loss: 1.65073\n",
      "Main effects training epoch: 225, train loss: 1.55068, val loss: 1.65336\n",
      "Main effects training epoch: 226, train loss: 1.54835, val loss: 1.66088\n",
      "Main effects training epoch: 227, train loss: 1.54540, val loss: 1.64859\n",
      "Main effects training epoch: 228, train loss: 1.55277, val loss: 1.65893\n",
      "Main effects training epoch: 229, train loss: 1.54764, val loss: 1.65125\n",
      "Main effects training epoch: 230, train loss: 1.54571, val loss: 1.66222\n",
      "Main effects training epoch: 231, train loss: 1.54693, val loss: 1.64774\n",
      "Main effects training epoch: 232, train loss: 1.54362, val loss: 1.65237\n",
      "Main effects training epoch: 233, train loss: 1.54462, val loss: 1.65783\n",
      "Main effects training epoch: 234, train loss: 1.54185, val loss: 1.64349\n",
      "Main effects training epoch: 235, train loss: 1.55088, val loss: 1.65461\n",
      "Main effects training epoch: 236, train loss: 1.54817, val loss: 1.65076\n",
      "Main effects training epoch: 237, train loss: 1.54613, val loss: 1.65590\n",
      "Main effects training epoch: 238, train loss: 1.54111, val loss: 1.64366\n",
      "Main effects training epoch: 239, train loss: 1.54326, val loss: 1.65151\n",
      "Main effects training epoch: 240, train loss: 1.54914, val loss: 1.66278\n",
      "Main effects training epoch: 241, train loss: 1.53850, val loss: 1.64351\n",
      "Main effects training epoch: 242, train loss: 1.55011, val loss: 1.66074\n",
      "Main effects training epoch: 243, train loss: 1.55222, val loss: 1.64485\n",
      "Main effects training epoch: 244, train loss: 1.54557, val loss: 1.66391\n",
      "Main effects training epoch: 245, train loss: 1.54477, val loss: 1.65108\n",
      "Main effects training epoch: 246, train loss: 1.54283, val loss: 1.64267\n",
      "Main effects training epoch: 247, train loss: 1.54514, val loss: 1.65577\n",
      "Main effects training epoch: 248, train loss: 1.53600, val loss: 1.64649\n",
      "Main effects training epoch: 249, train loss: 1.53706, val loss: 1.65184\n",
      "Main effects training epoch: 250, train loss: 1.54231, val loss: 1.64395\n",
      "Main effects training epoch: 251, train loss: 1.53818, val loss: 1.64317\n",
      "Main effects training epoch: 252, train loss: 1.53629, val loss: 1.65604\n",
      "Main effects training epoch: 253, train loss: 1.53836, val loss: 1.63443\n",
      "Main effects training epoch: 254, train loss: 1.53652, val loss: 1.64642\n",
      "Main effects training epoch: 255, train loss: 1.53724, val loss: 1.64140\n",
      "Main effects training epoch: 256, train loss: 1.53860, val loss: 1.64992\n",
      "Main effects training epoch: 257, train loss: 1.53322, val loss: 1.64113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 258, train loss: 1.53446, val loss: 1.64419\n",
      "Main effects training epoch: 259, train loss: 1.53517, val loss: 1.64407\n",
      "Main effects training epoch: 260, train loss: 1.53545, val loss: 1.63616\n",
      "Main effects training epoch: 261, train loss: 1.53571, val loss: 1.64675\n",
      "Main effects training epoch: 262, train loss: 1.53146, val loss: 1.64239\n",
      "Main effects training epoch: 263, train loss: 1.53007, val loss: 1.63536\n",
      "Main effects training epoch: 264, train loss: 1.53160, val loss: 1.64616\n",
      "Main effects training epoch: 265, train loss: 1.53323, val loss: 1.63406\n",
      "Main effects training epoch: 266, train loss: 1.53232, val loss: 1.64812\n",
      "Main effects training epoch: 267, train loss: 1.53206, val loss: 1.63371\n",
      "Main effects training epoch: 268, train loss: 1.52845, val loss: 1.63791\n",
      "Main effects training epoch: 269, train loss: 1.53136, val loss: 1.63658\n",
      "Main effects training epoch: 270, train loss: 1.53756, val loss: 1.63805\n",
      "Main effects training epoch: 271, train loss: 1.53401, val loss: 1.64695\n",
      "Main effects training epoch: 272, train loss: 1.53157, val loss: 1.63349\n",
      "Main effects training epoch: 273, train loss: 1.53013, val loss: 1.64420\n",
      "Main effects training epoch: 274, train loss: 1.52626, val loss: 1.62495\n",
      "Main effects training epoch: 275, train loss: 1.52581, val loss: 1.63900\n",
      "Main effects training epoch: 276, train loss: 1.52587, val loss: 1.63560\n",
      "Main effects training epoch: 277, train loss: 1.52438, val loss: 1.63042\n",
      "Main effects training epoch: 278, train loss: 1.52319, val loss: 1.62645\n",
      "Main effects training epoch: 279, train loss: 1.52359, val loss: 1.63729\n",
      "Main effects training epoch: 280, train loss: 1.52079, val loss: 1.62200\n",
      "Main effects training epoch: 281, train loss: 1.52138, val loss: 1.63486\n",
      "Main effects training epoch: 282, train loss: 1.52432, val loss: 1.63430\n",
      "Main effects training epoch: 283, train loss: 1.51953, val loss: 1.62169\n",
      "Main effects training epoch: 284, train loss: 1.51881, val loss: 1.63464\n",
      "Main effects training epoch: 285, train loss: 1.51801, val loss: 1.62009\n",
      "Main effects training epoch: 286, train loss: 1.51693, val loss: 1.62896\n",
      "Main effects training epoch: 287, train loss: 1.52812, val loss: 1.63370\n",
      "Main effects training epoch: 288, train loss: 1.52375, val loss: 1.61961\n",
      "Main effects training epoch: 289, train loss: 1.51582, val loss: 1.63118\n",
      "Main effects training epoch: 290, train loss: 1.51640, val loss: 1.61915\n",
      "Main effects training epoch: 291, train loss: 1.51513, val loss: 1.62491\n",
      "Main effects training epoch: 292, train loss: 1.51564, val loss: 1.62050\n",
      "Main effects training epoch: 293, train loss: 1.51456, val loss: 1.62204\n",
      "Main effects training epoch: 294, train loss: 1.51701, val loss: 1.62445\n",
      "Main effects training epoch: 295, train loss: 1.51776, val loss: 1.62807\n",
      "Main effects training epoch: 296, train loss: 1.51452, val loss: 1.62752\n",
      "Main effects training epoch: 297, train loss: 1.51410, val loss: 1.61830\n",
      "Main effects training epoch: 298, train loss: 1.51228, val loss: 1.61683\n",
      "Main effects training epoch: 299, train loss: 1.51288, val loss: 1.62774\n",
      "Main effects training epoch: 300, train loss: 1.50919, val loss: 1.62045\n",
      "##########Stage 1: main effect training stop.##########\n",
      "3 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.52552, val loss: 1.62748\n",
      "Main effects tuning epoch: 2, train loss: 1.52835, val loss: 1.63325\n",
      "Main effects tuning epoch: 3, train loss: 1.52782, val loss: 1.63298\n",
      "Main effects tuning epoch: 4, train loss: 1.52547, val loss: 1.62840\n",
      "Main effects tuning epoch: 5, train loss: 1.52467, val loss: 1.62711\n",
      "Main effects tuning epoch: 6, train loss: 1.52253, val loss: 1.62395\n",
      "Main effects tuning epoch: 7, train loss: 1.52282, val loss: 1.62539\n",
      "Main effects tuning epoch: 8, train loss: 1.53122, val loss: 1.63259\n",
      "Main effects tuning epoch: 9, train loss: 1.52394, val loss: 1.63445\n",
      "Main effects tuning epoch: 10, train loss: 1.52392, val loss: 1.61963\n",
      "Main effects tuning epoch: 11, train loss: 1.52377, val loss: 1.63061\n",
      "Main effects tuning epoch: 12, train loss: 1.51984, val loss: 1.62231\n",
      "Main effects tuning epoch: 13, train loss: 1.51996, val loss: 1.61888\n",
      "Main effects tuning epoch: 14, train loss: 1.51863, val loss: 1.62418\n",
      "Main effects tuning epoch: 15, train loss: 1.52112, val loss: 1.62157\n",
      "Main effects tuning epoch: 16, train loss: 1.51662, val loss: 1.61878\n",
      "Main effects tuning epoch: 17, train loss: 1.51777, val loss: 1.61731\n",
      "Main effects tuning epoch: 18, train loss: 1.51614, val loss: 1.61137\n",
      "Main effects tuning epoch: 19, train loss: 1.51461, val loss: 1.61865\n",
      "Main effects tuning epoch: 20, train loss: 1.51492, val loss: 1.61615\n",
      "Main effects tuning epoch: 21, train loss: 1.51278, val loss: 1.61162\n",
      "Main effects tuning epoch: 22, train loss: 1.51140, val loss: 1.62212\n",
      "Main effects tuning epoch: 23, train loss: 1.51129, val loss: 1.60775\n",
      "Main effects tuning epoch: 24, train loss: 1.50997, val loss: 1.61635\n",
      "Main effects tuning epoch: 25, train loss: 1.50834, val loss: 1.61229\n",
      "Main effects tuning epoch: 26, train loss: 1.50748, val loss: 1.60756\n",
      "Main effects tuning epoch: 27, train loss: 1.50786, val loss: 1.61470\n",
      "Main effects tuning epoch: 28, train loss: 1.50658, val loss: 1.60820\n",
      "Main effects tuning epoch: 29, train loss: 1.50826, val loss: 1.61111\n",
      "Main effects tuning epoch: 30, train loss: 1.50556, val loss: 1.60398\n",
      "Main effects tuning epoch: 31, train loss: 1.50426, val loss: 1.61386\n",
      "Main effects tuning epoch: 32, train loss: 1.50444, val loss: 1.60931\n",
      "Main effects tuning epoch: 33, train loss: 1.50295, val loss: 1.60336\n",
      "Main effects tuning epoch: 34, train loss: 1.50429, val loss: 1.61147\n",
      "Main effects tuning epoch: 35, train loss: 1.50284, val loss: 1.60515\n",
      "Main effects tuning epoch: 36, train loss: 1.50201, val loss: 1.60073\n",
      "Main effects tuning epoch: 37, train loss: 1.50037, val loss: 1.60559\n",
      "Main effects tuning epoch: 38, train loss: 1.50062, val loss: 1.60769\n",
      "Main effects tuning epoch: 39, train loss: 1.49896, val loss: 1.59474\n",
      "Main effects tuning epoch: 40, train loss: 1.49923, val loss: 1.61161\n",
      "Main effects tuning epoch: 41, train loss: 1.49461, val loss: 1.59496\n",
      "Main effects tuning epoch: 42, train loss: 1.49737, val loss: 1.60252\n",
      "Main effects tuning epoch: 43, train loss: 1.49716, val loss: 1.60228\n",
      "Main effects tuning epoch: 44, train loss: 1.49558, val loss: 1.59516\n",
      "Main effects tuning epoch: 45, train loss: 1.49462, val loss: 1.59942\n",
      "Main effects tuning epoch: 46, train loss: 1.49761, val loss: 1.59218\n",
      "Main effects tuning epoch: 47, train loss: 1.49582, val loss: 1.59648\n",
      "Main effects tuning epoch: 48, train loss: 1.49559, val loss: 1.59205\n",
      "Main effects tuning epoch: 49, train loss: 1.49876, val loss: 1.59791\n",
      "Main effects tuning epoch: 50, train loss: 1.49216, val loss: 1.58765\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.38244, val loss: 1.45099\n",
      "Interaction training epoch: 2, train loss: 1.08703, val loss: 1.09847\n",
      "Interaction training epoch: 3, train loss: 1.13209, val loss: 1.13870\n",
      "Interaction training epoch: 4, train loss: 1.08676, val loss: 1.10079\n",
      "Interaction training epoch: 5, train loss: 1.03648, val loss: 1.06487\n",
      "Interaction training epoch: 6, train loss: 1.06225, val loss: 1.06416\n",
      "Interaction training epoch: 7, train loss: 1.03610, val loss: 1.05073\n",
      "Interaction training epoch: 8, train loss: 0.99646, val loss: 1.01256\n",
      "Interaction training epoch: 9, train loss: 0.98046, val loss: 1.00497\n",
      "Interaction training epoch: 10, train loss: 0.98676, val loss: 1.00671\n",
      "Interaction training epoch: 11, train loss: 0.97654, val loss: 0.99072\n",
      "Interaction training epoch: 12, train loss: 0.96092, val loss: 0.99209\n",
      "Interaction training epoch: 13, train loss: 0.94882, val loss: 0.97498\n",
      "Interaction training epoch: 14, train loss: 0.94243, val loss: 0.95571\n",
      "Interaction training epoch: 15, train loss: 0.94349, val loss: 0.97253\n",
      "Interaction training epoch: 16, train loss: 0.93896, val loss: 0.95700\n",
      "Interaction training epoch: 17, train loss: 0.92910, val loss: 0.94345\n",
      "Interaction training epoch: 18, train loss: 0.91716, val loss: 0.92971\n",
      "Interaction training epoch: 19, train loss: 0.93855, val loss: 0.94670\n",
      "Interaction training epoch: 20, train loss: 0.91728, val loss: 0.91625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 21, train loss: 0.92173, val loss: 0.94360\n",
      "Interaction training epoch: 22, train loss: 0.92002, val loss: 0.91846\n",
      "Interaction training epoch: 23, train loss: 0.92459, val loss: 0.93582\n",
      "Interaction training epoch: 24, train loss: 0.93998, val loss: 0.95749\n",
      "Interaction training epoch: 25, train loss: 0.90974, val loss: 0.91267\n",
      "Interaction training epoch: 26, train loss: 0.91604, val loss: 0.93331\n",
      "Interaction training epoch: 27, train loss: 0.90228, val loss: 0.91647\n",
      "Interaction training epoch: 28, train loss: 0.90666, val loss: 0.92668\n",
      "Interaction training epoch: 29, train loss: 0.89372, val loss: 0.90615\n",
      "Interaction training epoch: 30, train loss: 0.89112, val loss: 0.90177\n",
      "Interaction training epoch: 31, train loss: 0.90225, val loss: 0.91400\n",
      "Interaction training epoch: 32, train loss: 0.89846, val loss: 0.91656\n",
      "Interaction training epoch: 33, train loss: 0.88375, val loss: 0.90475\n",
      "Interaction training epoch: 34, train loss: 0.89405, val loss: 0.90731\n",
      "Interaction training epoch: 35, train loss: 0.88934, val loss: 0.90461\n",
      "Interaction training epoch: 36, train loss: 0.88236, val loss: 0.90339\n",
      "Interaction training epoch: 37, train loss: 0.90054, val loss: 0.90668\n",
      "Interaction training epoch: 38, train loss: 0.88623, val loss: 0.89725\n",
      "Interaction training epoch: 39, train loss: 0.88169, val loss: 0.88927\n",
      "Interaction training epoch: 40, train loss: 0.87533, val loss: 0.89549\n",
      "Interaction training epoch: 41, train loss: 0.88535, val loss: 0.89984\n",
      "Interaction training epoch: 42, train loss: 0.87708, val loss: 0.88776\n",
      "Interaction training epoch: 43, train loss: 0.89254, val loss: 0.90871\n",
      "Interaction training epoch: 44, train loss: 0.88944, val loss: 0.91645\n",
      "Interaction training epoch: 45, train loss: 0.88571, val loss: 0.89167\n",
      "Interaction training epoch: 46, train loss: 0.88663, val loss: 0.91023\n",
      "Interaction training epoch: 47, train loss: 0.87299, val loss: 0.87852\n",
      "Interaction training epoch: 48, train loss: 0.87705, val loss: 0.89487\n",
      "Interaction training epoch: 49, train loss: 0.87750, val loss: 0.88817\n",
      "Interaction training epoch: 50, train loss: 0.86687, val loss: 0.88689\n",
      "Interaction training epoch: 51, train loss: 0.87756, val loss: 0.89026\n",
      "Interaction training epoch: 52, train loss: 0.87958, val loss: 0.89168\n",
      "Interaction training epoch: 53, train loss: 0.87058, val loss: 0.89085\n",
      "Interaction training epoch: 54, train loss: 0.87008, val loss: 0.88629\n",
      "Interaction training epoch: 55, train loss: 0.87033, val loss: 0.88203\n",
      "Interaction training epoch: 56, train loss: 0.87455, val loss: 0.90007\n",
      "Interaction training epoch: 57, train loss: 0.87576, val loss: 0.89089\n",
      "Interaction training epoch: 58, train loss: 0.86682, val loss: 0.88229\n",
      "Interaction training epoch: 59, train loss: 0.87165, val loss: 0.87773\n",
      "Interaction training epoch: 60, train loss: 0.85524, val loss: 0.87311\n",
      "Interaction training epoch: 61, train loss: 0.85934, val loss: 0.87928\n",
      "Interaction training epoch: 62, train loss: 0.85449, val loss: 0.86284\n",
      "Interaction training epoch: 63, train loss: 0.86287, val loss: 0.87948\n",
      "Interaction training epoch: 64, train loss: 0.85852, val loss: 0.87290\n",
      "Interaction training epoch: 65, train loss: 0.86005, val loss: 0.87906\n",
      "Interaction training epoch: 66, train loss: 0.86628, val loss: 0.87584\n",
      "Interaction training epoch: 67, train loss: 0.86629, val loss: 0.88449\n",
      "Interaction training epoch: 68, train loss: 0.86559, val loss: 0.87451\n",
      "Interaction training epoch: 69, train loss: 0.86049, val loss: 0.87951\n",
      "Interaction training epoch: 70, train loss: 0.86808, val loss: 0.88070\n",
      "Interaction training epoch: 71, train loss: 0.86358, val loss: 0.87569\n",
      "Interaction training epoch: 72, train loss: 0.86553, val loss: 0.88179\n",
      "Interaction training epoch: 73, train loss: 0.85988, val loss: 0.86341\n",
      "Interaction training epoch: 74, train loss: 0.86183, val loss: 0.88101\n",
      "Interaction training epoch: 75, train loss: 0.85733, val loss: 0.87879\n",
      "Interaction training epoch: 76, train loss: 0.85985, val loss: 0.87099\n",
      "Interaction training epoch: 77, train loss: 0.85534, val loss: 0.87059\n",
      "Interaction training epoch: 78, train loss: 0.84958, val loss: 0.85991\n",
      "Interaction training epoch: 79, train loss: 0.84713, val loss: 0.86471\n",
      "Interaction training epoch: 80, train loss: 0.85349, val loss: 0.86499\n",
      "Interaction training epoch: 81, train loss: 0.85260, val loss: 0.87338\n",
      "Interaction training epoch: 82, train loss: 0.85342, val loss: 0.86213\n",
      "Interaction training epoch: 83, train loss: 0.85173, val loss: 0.86430\n",
      "Interaction training epoch: 84, train loss: 0.85379, val loss: 0.86261\n",
      "Interaction training epoch: 85, train loss: 0.85314, val loss: 0.87129\n",
      "Interaction training epoch: 86, train loss: 0.85631, val loss: 0.86579\n",
      "Interaction training epoch: 87, train loss: 0.84727, val loss: 0.85537\n",
      "Interaction training epoch: 88, train loss: 0.85156, val loss: 0.86021\n",
      "Interaction training epoch: 89, train loss: 0.84860, val loss: 0.85588\n",
      "Interaction training epoch: 90, train loss: 0.85551, val loss: 0.85883\n",
      "Interaction training epoch: 91, train loss: 0.85009, val loss: 0.86742\n",
      "Interaction training epoch: 92, train loss: 0.85159, val loss: 0.86052\n",
      "Interaction training epoch: 93, train loss: 0.85083, val loss: 0.85415\n",
      "Interaction training epoch: 94, train loss: 0.84786, val loss: 0.85514\n",
      "Interaction training epoch: 95, train loss: 0.84612, val loss: 0.85133\n",
      "Interaction training epoch: 96, train loss: 0.85478, val loss: 0.87001\n",
      "Interaction training epoch: 97, train loss: 0.84885, val loss: 0.85884\n",
      "Interaction training epoch: 98, train loss: 0.84726, val loss: 0.85115\n",
      "Interaction training epoch: 99, train loss: 0.85253, val loss: 0.87545\n",
      "Interaction training epoch: 100, train loss: 0.84664, val loss: 0.85363\n",
      "Interaction training epoch: 101, train loss: 0.84887, val loss: 0.85045\n",
      "Interaction training epoch: 102, train loss: 0.84533, val loss: 0.85516\n",
      "Interaction training epoch: 103, train loss: 0.84453, val loss: 0.85784\n",
      "Interaction training epoch: 104, train loss: 0.84532, val loss: 0.85147\n",
      "Interaction training epoch: 105, train loss: 0.84565, val loss: 0.85644\n",
      "Interaction training epoch: 106, train loss: 0.84075, val loss: 0.84392\n",
      "Interaction training epoch: 107, train loss: 0.84143, val loss: 0.84932\n",
      "Interaction training epoch: 108, train loss: 0.84421, val loss: 0.85165\n",
      "Interaction training epoch: 109, train loss: 0.84959, val loss: 0.86098\n",
      "Interaction training epoch: 110, train loss: 0.84672, val loss: 0.84922\n",
      "Interaction training epoch: 111, train loss: 0.84559, val loss: 0.86115\n",
      "Interaction training epoch: 112, train loss: 0.84990, val loss: 0.85649\n",
      "Interaction training epoch: 113, train loss: 0.83909, val loss: 0.83701\n",
      "Interaction training epoch: 114, train loss: 0.84380, val loss: 0.85758\n",
      "Interaction training epoch: 115, train loss: 0.85681, val loss: 0.85440\n",
      "Interaction training epoch: 116, train loss: 0.84544, val loss: 0.85235\n",
      "Interaction training epoch: 117, train loss: 0.84336, val loss: 0.84580\n",
      "Interaction training epoch: 118, train loss: 0.84144, val loss: 0.84731\n",
      "Interaction training epoch: 119, train loss: 0.84142, val loss: 0.84515\n",
      "Interaction training epoch: 120, train loss: 0.83932, val loss: 0.84615\n",
      "Interaction training epoch: 121, train loss: 0.83750, val loss: 0.83343\n",
      "Interaction training epoch: 122, train loss: 0.84207, val loss: 0.84555\n",
      "Interaction training epoch: 123, train loss: 0.83809, val loss: 0.84534\n",
      "Interaction training epoch: 124, train loss: 0.84260, val loss: 0.84351\n",
      "Interaction training epoch: 125, train loss: 0.83722, val loss: 0.84535\n",
      "Interaction training epoch: 126, train loss: 0.84274, val loss: 0.84860\n",
      "Interaction training epoch: 127, train loss: 0.84254, val loss: 0.84779\n",
      "Interaction training epoch: 128, train loss: 0.83951, val loss: 0.83304\n",
      "Interaction training epoch: 129, train loss: 0.84195, val loss: 0.85049\n",
      "Interaction training epoch: 130, train loss: 0.83938, val loss: 0.84375\n",
      "Interaction training epoch: 131, train loss: 0.84110, val loss: 0.84626\n",
      "Interaction training epoch: 132, train loss: 0.84238, val loss: 0.84433\n",
      "Interaction training epoch: 133, train loss: 0.84037, val loss: 0.84466\n",
      "Interaction training epoch: 134, train loss: 0.84099, val loss: 0.83961\n",
      "Interaction training epoch: 135, train loss: 0.84511, val loss: 0.85576\n",
      "Interaction training epoch: 136, train loss: 0.84165, val loss: 0.84492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 137, train loss: 0.84116, val loss: 0.84385\n",
      "Interaction training epoch: 138, train loss: 0.83946, val loss: 0.83281\n",
      "Interaction training epoch: 139, train loss: 0.84035, val loss: 0.84692\n",
      "Interaction training epoch: 140, train loss: 0.83705, val loss: 0.83927\n",
      "Interaction training epoch: 141, train loss: 0.84130, val loss: 0.83991\n",
      "Interaction training epoch: 142, train loss: 0.83664, val loss: 0.84483\n",
      "Interaction training epoch: 143, train loss: 0.84289, val loss: 0.84153\n",
      "Interaction training epoch: 144, train loss: 0.83683, val loss: 0.83727\n",
      "Interaction training epoch: 145, train loss: 0.83431, val loss: 0.83317\n",
      "Interaction training epoch: 146, train loss: 0.83517, val loss: 0.83368\n",
      "Interaction training epoch: 147, train loss: 0.83628, val loss: 0.83812\n",
      "Interaction training epoch: 148, train loss: 0.83525, val loss: 0.83556\n",
      "Interaction training epoch: 149, train loss: 0.83409, val loss: 0.83572\n",
      "Interaction training epoch: 150, train loss: 0.83596, val loss: 0.84207\n",
      "Interaction training epoch: 151, train loss: 0.83928, val loss: 0.83880\n",
      "Interaction training epoch: 152, train loss: 0.84559, val loss: 0.85114\n",
      "Interaction training epoch: 153, train loss: 0.83389, val loss: 0.83548\n",
      "Interaction training epoch: 154, train loss: 0.83298, val loss: 0.83684\n",
      "Interaction training epoch: 155, train loss: 0.82991, val loss: 0.82587\n",
      "Interaction training epoch: 156, train loss: 0.83052, val loss: 0.83236\n",
      "Interaction training epoch: 157, train loss: 0.83419, val loss: 0.83582\n",
      "Interaction training epoch: 158, train loss: 0.83529, val loss: 0.83996\n",
      "Interaction training epoch: 159, train loss: 0.83475, val loss: 0.83210\n",
      "Interaction training epoch: 160, train loss: 0.83699, val loss: 0.83590\n",
      "Interaction training epoch: 161, train loss: 0.83822, val loss: 0.83736\n",
      "Interaction training epoch: 162, train loss: 0.83552, val loss: 0.83812\n",
      "Interaction training epoch: 163, train loss: 0.83137, val loss: 0.82691\n",
      "Interaction training epoch: 164, train loss: 0.83273, val loss: 0.82476\n",
      "Interaction training epoch: 165, train loss: 0.83139, val loss: 0.83595\n",
      "Interaction training epoch: 166, train loss: 0.83475, val loss: 0.83593\n",
      "Interaction training epoch: 167, train loss: 0.83795, val loss: 0.83540\n",
      "Interaction training epoch: 168, train loss: 0.83548, val loss: 0.83980\n",
      "Interaction training epoch: 169, train loss: 0.83508, val loss: 0.83743\n",
      "Interaction training epoch: 170, train loss: 0.83661, val loss: 0.83330\n",
      "Interaction training epoch: 171, train loss: 0.82838, val loss: 0.83080\n",
      "Interaction training epoch: 172, train loss: 0.83801, val loss: 0.83703\n",
      "Interaction training epoch: 173, train loss: 0.83243, val loss: 0.83287\n",
      "Interaction training epoch: 174, train loss: 0.83541, val loss: 0.83691\n",
      "Interaction training epoch: 175, train loss: 0.83758, val loss: 0.83863\n",
      "Interaction training epoch: 176, train loss: 0.83169, val loss: 0.82805\n",
      "Interaction training epoch: 177, train loss: 0.82853, val loss: 0.83046\n",
      "Interaction training epoch: 178, train loss: 0.83460, val loss: 0.83127\n",
      "Interaction training epoch: 179, train loss: 0.83591, val loss: 0.84123\n",
      "Interaction training epoch: 180, train loss: 0.83031, val loss: 0.82672\n",
      "Interaction training epoch: 181, train loss: 0.82914, val loss: 0.83141\n",
      "Interaction training epoch: 182, train loss: 0.82484, val loss: 0.82220\n",
      "Interaction training epoch: 183, train loss: 0.82931, val loss: 0.82508\n",
      "Interaction training epoch: 184, train loss: 0.83348, val loss: 0.84238\n",
      "Interaction training epoch: 185, train loss: 0.84021, val loss: 0.83501\n",
      "Interaction training epoch: 186, train loss: 0.83514, val loss: 0.84195\n",
      "Interaction training epoch: 187, train loss: 0.82962, val loss: 0.83133\n",
      "Interaction training epoch: 188, train loss: 0.83245, val loss: 0.83058\n",
      "Interaction training epoch: 189, train loss: 0.82906, val loss: 0.82941\n",
      "Interaction training epoch: 190, train loss: 0.82979, val loss: 0.82969\n",
      "Interaction training epoch: 191, train loss: 0.83225, val loss: 0.83063\n",
      "Interaction training epoch: 192, train loss: 0.83132, val loss: 0.83755\n",
      "Interaction training epoch: 193, train loss: 0.82807, val loss: 0.82237\n",
      "Interaction training epoch: 194, train loss: 0.83030, val loss: 0.82105\n",
      "Interaction training epoch: 195, train loss: 0.83485, val loss: 0.84170\n",
      "Interaction training epoch: 196, train loss: 0.83033, val loss: 0.81952\n",
      "Interaction training epoch: 197, train loss: 0.83278, val loss: 0.83712\n",
      "Interaction training epoch: 198, train loss: 0.83108, val loss: 0.82288\n",
      "Interaction training epoch: 199, train loss: 0.82783, val loss: 0.83319\n",
      "Interaction training epoch: 200, train loss: 0.83160, val loss: 0.82706\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########2 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.82959, val loss: 0.82409\n",
      "Interaction tuning epoch: 2, train loss: 0.83473, val loss: 0.82911\n",
      "Interaction tuning epoch: 3, train loss: 0.83132, val loss: 0.82487\n",
      "Interaction tuning epoch: 4, train loss: 0.83340, val loss: 0.83021\n",
      "Interaction tuning epoch: 5, train loss: 0.83350, val loss: 0.82629\n",
      "Interaction tuning epoch: 6, train loss: 0.83332, val loss: 0.83315\n",
      "Interaction tuning epoch: 7, train loss: 0.83231, val loss: 0.82281\n",
      "Interaction tuning epoch: 8, train loss: 0.83280, val loss: 0.82669\n",
      "Interaction tuning epoch: 9, train loss: 0.83280, val loss: 0.82407\n",
      "Interaction tuning epoch: 10, train loss: 0.83429, val loss: 0.82573\n",
      "Interaction tuning epoch: 11, train loss: 0.83179, val loss: 0.82572\n",
      "Interaction tuning epoch: 12, train loss: 0.83290, val loss: 0.82836\n",
      "Interaction tuning epoch: 13, train loss: 0.83129, val loss: 0.82462\n",
      "Interaction tuning epoch: 14, train loss: 0.83338, val loss: 0.82874\n",
      "Interaction tuning epoch: 15, train loss: 0.83617, val loss: 0.83085\n",
      "Interaction tuning epoch: 16, train loss: 0.83310, val loss: 0.83233\n",
      "Interaction tuning epoch: 17, train loss: 0.83341, val loss: 0.83208\n",
      "Interaction tuning epoch: 18, train loss: 0.83567, val loss: 0.82563\n",
      "Interaction tuning epoch: 19, train loss: 0.83091, val loss: 0.82139\n",
      "Interaction tuning epoch: 20, train loss: 0.83229, val loss: 0.82710\n",
      "Interaction tuning epoch: 21, train loss: 0.83271, val loss: 0.82160\n",
      "Interaction tuning epoch: 22, train loss: 0.83334, val loss: 0.82650\n",
      "Interaction tuning epoch: 23, train loss: 0.83649, val loss: 0.84079\n",
      "Interaction tuning epoch: 24, train loss: 0.83800, val loss: 0.82792\n",
      "Interaction tuning epoch: 25, train loss: 0.83269, val loss: 0.82332\n",
      "Interaction tuning epoch: 26, train loss: 0.83369, val loss: 0.83319\n",
      "Interaction tuning epoch: 27, train loss: 0.83063, val loss: 0.82016\n",
      "Interaction tuning epoch: 28, train loss: 0.83606, val loss: 0.83555\n",
      "Interaction tuning epoch: 29, train loss: 0.83049, val loss: 0.81907\n",
      "Interaction tuning epoch: 30, train loss: 0.83040, val loss: 0.82858\n",
      "Interaction tuning epoch: 31, train loss: 0.83044, val loss: 0.82526\n",
      "Interaction tuning epoch: 32, train loss: 0.82997, val loss: 0.82802\n",
      "Interaction tuning epoch: 33, train loss: 0.83026, val loss: 0.82368\n",
      "Interaction tuning epoch: 34, train loss: 0.82947, val loss: 0.82113\n",
      "Interaction tuning epoch: 35, train loss: 0.83215, val loss: 0.83136\n",
      "Interaction tuning epoch: 36, train loss: 0.84075, val loss: 0.83521\n",
      "Interaction tuning epoch: 37, train loss: 0.84206, val loss: 0.84253\n",
      "Interaction tuning epoch: 38, train loss: 0.83502, val loss: 0.82850\n",
      "Interaction tuning epoch: 39, train loss: 0.83405, val loss: 0.82658\n",
      "Interaction tuning epoch: 40, train loss: 0.83148, val loss: 0.83176\n",
      "Interaction tuning epoch: 41, train loss: 0.82970, val loss: 0.82152\n",
      "Interaction tuning epoch: 42, train loss: 0.83352, val loss: 0.82686\n",
      "Interaction tuning epoch: 43, train loss: 0.83333, val loss: 0.83404\n",
      "Interaction tuning epoch: 44, train loss: 0.82909, val loss: 0.82141\n",
      "Interaction tuning epoch: 45, train loss: 0.83523, val loss: 0.82221\n",
      "Interaction tuning epoch: 46, train loss: 0.83326, val loss: 0.83123\n",
      "Interaction tuning epoch: 47, train loss: 0.83097, val loss: 0.82227\n",
      "Interaction tuning epoch: 48, train loss: 0.83650, val loss: 0.83233\n",
      "Interaction tuning epoch: 49, train loss: 0.83053, val loss: 0.82459\n",
      "Interaction tuning epoch: 50, train loss: 0.82801, val loss: 0.81826\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 37.47904133796692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After the gam stage, training error is 0.82801 , validation error is 0.81826\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 20.485538\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.680814 validation MAE=0.770262,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.631320 validation MAE=0.749580,rank=5\n",
      "[SoftImpute] Iter 3: observed MAE=0.590768 validation MAE=0.729930,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.556844 validation MAE=0.712392,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.527044 validation MAE=0.696451,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.501196 validation MAE=0.681803,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.478794 validation MAE=0.668471,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.459113 validation MAE=0.656522,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 9: observed MAE=0.440899 validation MAE=0.645606,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.426571 validation MAE=0.636994,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.412757 validation MAE=0.629484,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.399561 validation MAE=0.621200,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.388305 validation MAE=0.614498,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.377495 validation MAE=0.607679,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.366723 validation MAE=0.602189,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.356727 validation MAE=0.596069,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.347963 validation MAE=0.592204,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.341449 validation MAE=0.586790,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.333834 validation MAE=0.582629,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.326668 validation MAE=0.578468,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.320485 validation MAE=0.574707,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.314866 validation MAE=0.570990,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.309435 validation MAE=0.567976,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.304563 validation MAE=0.564558,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.300187 validation MAE=0.561641,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.295095 validation MAE=0.559200,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.290933 validation MAE=0.556596,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.287711 validation MAE=0.554748,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.283134 validation MAE=0.552062,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.278666 validation MAE=0.549875,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.276087 validation MAE=0.548418,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.273457 validation MAE=0.546670,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.269820 validation MAE=0.544541,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.266757 validation MAE=0.542433,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.264158 validation MAE=0.541700,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.262123 validation MAE=0.540138,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.259383 validation MAE=0.538704,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.256558 validation MAE=0.537508,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.254114 validation MAE=0.536519,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.251993 validation MAE=0.535435,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.250625 validation MAE=0.535393,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.248639 validation MAE=0.534077,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.247215 validation MAE=0.533041,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.244656 validation MAE=0.532572,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.242392 validation MAE=0.531649,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.241782 validation MAE=0.530669,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.240128 validation MAE=0.530498,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.238640 validation MAE=0.530488,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.237037 validation MAE=0.529038,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.235857 validation MAE=0.528454,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.234213 validation MAE=0.527786,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.232131 validation MAE=0.526804,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.230825 validation MAE=0.526453,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.230840 validation MAE=0.526643,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.228921 validation MAE=0.525791,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.226968 validation MAE=0.525562,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.226172 validation MAE=0.525402,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.226001 validation MAE=0.524259,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.224268 validation MAE=0.523977,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.222855 validation MAE=0.523803,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.221865 validation MAE=0.523343,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.221328 validation MAE=0.522488,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.220417 validation MAE=0.522534,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.219233 validation MAE=0.521731,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.218282 validation MAE=0.522101,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.217017 validation MAE=0.521168,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.216346 validation MAE=0.520547,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.214868 validation MAE=0.520268,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.215173 validation MAE=0.520144,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.214497 validation MAE=0.519454,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.212730 validation MAE=0.519055,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.211462 validation MAE=0.518305,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.211458 validation MAE=0.518355,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.210794 validation MAE=0.518261,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.210347 validation MAE=0.518103,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.209325 validation MAE=0.517269,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.207974 validation MAE=0.517043,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.207383 validation MAE=0.516685,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.207195 validation MAE=0.516507,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.205943 validation MAE=0.516128,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.205420 validation MAE=0.515886,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.204894 validation MAE=0.515226,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.203830 validation MAE=0.515287,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.203041 validation MAE=0.514941,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.203854 validation MAE=0.514602,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.202685 validation MAE=0.514242,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.201606 validation MAE=0.514021,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.201562 validation MAE=0.513136,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.200242 validation MAE=0.512621,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.199553 validation MAE=0.512395,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.199340 validation MAE=0.512125,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.198346 validation MAE=0.511444,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.197546 validation MAE=0.511373,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.198869 validation MAE=0.510945,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.203612 validation MAE=0.511639,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.206406 validation MAE=0.511842,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.207395 validation MAE=0.511756,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.207479 validation MAE=0.512583,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.207149 validation MAE=0.511488,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.206956 validation MAE=0.511508,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.205345 validation MAE=0.511349,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.204968 validation MAE=0.510831,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.205568 validation MAE=0.510353,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.205348 validation MAE=0.510318,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.206150 validation MAE=0.510289,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.204782 validation MAE=0.509866,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.204557 validation MAE=0.509942,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.203916 validation MAE=0.509324,rank=5\n",
      "[SoftImpute] Iter 109: observed MAE=0.203083 validation MAE=0.508754,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.202169 validation MAE=0.508703,rank=5\n",
      "[SoftImpute] Iter 111: observed MAE=0.202277 validation MAE=0.508788,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.203047 validation MAE=0.507901,rank=5\n",
      "[SoftImpute] Iter 113: observed MAE=0.202397 validation MAE=0.507700,rank=5\n",
      "[SoftImpute] Iter 114: observed MAE=0.201842 validation MAE=0.507918,rank=5\n",
      "[SoftImpute] Iter 115: observed MAE=0.202048 validation MAE=0.507073,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.201070 validation MAE=0.507088,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.199885 validation MAE=0.507100,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.200547 validation MAE=0.506377,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.200879 validation MAE=0.505682,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 120: observed MAE=0.199906 validation MAE=0.505403,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.199440 validation MAE=0.504929,rank=5\n",
      "[SoftImpute] Iter 122: observed MAE=0.199488 validation MAE=0.504272,rank=5\n",
      "[SoftImpute] Iter 123: observed MAE=0.198898 validation MAE=0.503893,rank=5\n",
      "[SoftImpute] Iter 124: observed MAE=0.198067 validation MAE=0.504003,rank=5\n",
      "[SoftImpute] Iter 125: observed MAE=0.199049 validation MAE=0.503874,rank=5\n",
      "[SoftImpute] Iter 126: observed MAE=0.197372 validation MAE=0.502987,rank=5\n",
      "[SoftImpute] Iter 127: observed MAE=0.197556 validation MAE=0.503352,rank=5\n",
      "[SoftImpute] Iter 128: observed MAE=0.196680 validation MAE=0.502422,rank=5\n",
      "[SoftImpute] Iter 129: observed MAE=0.197064 validation MAE=0.502367,rank=5\n",
      "[SoftImpute] Iter 130: observed MAE=0.196697 validation MAE=0.501910,rank=5\n",
      "[SoftImpute] Iter 131: observed MAE=0.196079 validation MAE=0.501462,rank=5\n",
      "[SoftImpute] Iter 132: observed MAE=0.196554 validation MAE=0.500517,rank=5\n",
      "[SoftImpute] Iter 133: observed MAE=0.196086 validation MAE=0.500352,rank=5\n",
      "[SoftImpute] Iter 134: observed MAE=0.195769 validation MAE=0.500925,rank=5\n",
      "[SoftImpute] Iter 135: observed MAE=0.195210 validation MAE=0.500532,rank=5\n",
      "[SoftImpute] Iter 136: observed MAE=0.195240 validation MAE=0.499358,rank=5\n",
      "[SoftImpute] Iter 137: observed MAE=0.194549 validation MAE=0.499130,rank=5\n",
      "[SoftImpute] Iter 138: observed MAE=0.193639 validation MAE=0.498855,rank=5\n",
      "[SoftImpute] Iter 139: observed MAE=0.193586 validation MAE=0.498523,rank=5\n",
      "[SoftImpute] Iter 140: observed MAE=0.193672 validation MAE=0.497881,rank=5\n",
      "[SoftImpute] Iter 141: observed MAE=0.193286 validation MAE=0.497875,rank=5\n",
      "[SoftImpute] Iter 142: observed MAE=0.192575 validation MAE=0.497378,rank=5\n",
      "[SoftImpute] Iter 143: observed MAE=0.192547 validation MAE=0.497214,rank=5\n",
      "[SoftImpute] Iter 144: observed MAE=0.193597 validation MAE=0.497180,rank=5\n",
      "[SoftImpute] Iter 145: observed MAE=0.192991 validation MAE=0.496336,rank=5\n",
      "[SoftImpute] Iter 146: observed MAE=0.191788 validation MAE=0.496050,rank=5\n",
      "[SoftImpute] Iter 147: observed MAE=0.191889 validation MAE=0.496425,rank=5\n",
      "[SoftImpute] Iter 148: observed MAE=0.192397 validation MAE=0.494405,rank=5\n",
      "[SoftImpute] Iter 149: observed MAE=0.191306 validation MAE=0.494082,rank=5\n",
      "[SoftImpute] Iter 150: observed MAE=0.190720 validation MAE=0.494085,rank=5\n",
      "[SoftImpute] Iter 151: observed MAE=0.190321 validation MAE=0.493314,rank=5\n",
      "[SoftImpute] Iter 152: observed MAE=0.191329 validation MAE=0.493612,rank=5\n",
      "[SoftImpute] Iter 153: observed MAE=0.190586 validation MAE=0.493589,rank=5\n",
      "[SoftImpute] Iter 154: observed MAE=0.189543 validation MAE=0.492551,rank=5\n",
      "[SoftImpute] Iter 155: observed MAE=0.189606 validation MAE=0.492614,rank=5\n",
      "[SoftImpute] Iter 156: observed MAE=0.189917 validation MAE=0.492019,rank=5\n",
      "[SoftImpute] Iter 157: observed MAE=0.189537 validation MAE=0.491233,rank=5\n",
      "[SoftImpute] Iter 158: observed MAE=0.188622 validation MAE=0.490659,rank=5\n",
      "[SoftImpute] Iter 159: observed MAE=0.188680 validation MAE=0.490980,rank=5\n",
      "[SoftImpute] Iter 160: observed MAE=0.189061 validation MAE=0.490494,rank=5\n",
      "[SoftImpute] Iter 161: observed MAE=0.188824 validation MAE=0.490449,rank=5\n",
      "[SoftImpute] Iter 162: observed MAE=0.188415 validation MAE=0.490074,rank=5\n",
      "[SoftImpute] Iter 163: observed MAE=0.188255 validation MAE=0.489447,rank=5\n",
      "[SoftImpute] Iter 164: observed MAE=0.188249 validation MAE=0.488936,rank=5\n",
      "[SoftImpute] Iter 165: observed MAE=0.188080 validation MAE=0.488900,rank=5\n",
      "[SoftImpute] Iter 166: observed MAE=0.187026 validation MAE=0.488135,rank=5\n",
      "[SoftImpute] Iter 167: observed MAE=0.186748 validation MAE=0.488268,rank=5\n",
      "[SoftImpute] Iter 168: observed MAE=0.187181 validation MAE=0.488385,rank=5\n",
      "[SoftImpute] Iter 169: observed MAE=0.186687 validation MAE=0.487769,rank=5\n",
      "[SoftImpute] Iter 170: observed MAE=0.186180 validation MAE=0.486989,rank=5\n",
      "[SoftImpute] Iter 171: observed MAE=0.186120 validation MAE=0.487302,rank=5\n",
      "[SoftImpute] Iter 172: observed MAE=0.186193 validation MAE=0.487012,rank=5\n",
      "[SoftImpute] Iter 173: observed MAE=0.185666 validation MAE=0.486207,rank=5\n",
      "[SoftImpute] Iter 174: observed MAE=0.185657 validation MAE=0.485292,rank=5\n",
      "[SoftImpute] Iter 175: observed MAE=0.185377 validation MAE=0.485060,rank=5\n",
      "[SoftImpute] Iter 176: observed MAE=0.185110 validation MAE=0.484697,rank=5\n",
      "[SoftImpute] Iter 177: observed MAE=0.184533 validation MAE=0.484680,rank=5\n",
      "[SoftImpute] Iter 178: observed MAE=0.184913 validation MAE=0.484654,rank=5\n",
      "[SoftImpute] Iter 179: observed MAE=0.184488 validation MAE=0.484320,rank=5\n",
      "[SoftImpute] Iter 180: observed MAE=0.184612 validation MAE=0.484021,rank=5\n",
      "[SoftImpute] Iter 181: observed MAE=0.183810 validation MAE=0.483127,rank=5\n",
      "[SoftImpute] Iter 182: observed MAE=0.183620 validation MAE=0.482999,rank=5\n",
      "[SoftImpute] Iter 183: observed MAE=0.184250 validation MAE=0.483037,rank=5\n",
      "[SoftImpute] Iter 184: observed MAE=0.184534 validation MAE=0.482601,rank=5\n",
      "[SoftImpute] Iter 185: observed MAE=0.183924 validation MAE=0.482269,rank=5\n",
      "[SoftImpute] Iter 186: observed MAE=0.183206 validation MAE=0.481849,rank=5\n",
      "[SoftImpute] Iter 187: observed MAE=0.182562 validation MAE=0.481805,rank=5\n",
      "[SoftImpute] Iter 188: observed MAE=0.182401 validation MAE=0.481150,rank=5\n",
      "[SoftImpute] Iter 189: observed MAE=0.182838 validation MAE=0.480969,rank=5\n",
      "[SoftImpute] Iter 190: observed MAE=0.182598 validation MAE=0.480728,rank=5\n",
      "[SoftImpute] Iter 191: observed MAE=0.181656 validation MAE=0.480464,rank=5\n",
      "[SoftImpute] Iter 192: observed MAE=0.182033 validation MAE=0.480877,rank=5\n",
      "[SoftImpute] Iter 193: observed MAE=0.181969 validation MAE=0.479910,rank=5\n",
      "[SoftImpute] Iter 194: observed MAE=0.181426 validation MAE=0.479968,rank=5\n",
      "[SoftImpute] Iter 195: observed MAE=0.181039 validation MAE=0.479631,rank=5\n",
      "[SoftImpute] Iter 196: observed MAE=0.181278 validation MAE=0.479768,rank=5\n",
      "[SoftImpute] Iter 197: observed MAE=0.180650 validation MAE=0.478900,rank=5\n",
      "[SoftImpute] Iter 198: observed MAE=0.181049 validation MAE=0.479163,rank=5\n",
      "[SoftImpute] Iter 199: observed MAE=0.181396 validation MAE=0.478587,rank=5\n",
      "[SoftImpute] Iter 200: observed MAE=0.180725 validation MAE=0.478288,rank=5\n",
      "[SoftImpute] Stopped after iteration 200 for lambda=0.409711\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 6.785805702209473\n",
      "After the matrix factor stage, training error is 0.18072, validation error is 0.47829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 11:37:39.593369 17652 deprecation.py:506] From ../benchmark/deepfm\\DeepFM.py:93: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002854BF072F0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002854A72AA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002854BF072F0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002854A72AA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002854BF072F0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002854A72AA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002854BF072F0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002854A72AA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002854BF072F0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002854A72AA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002854BF072F0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002854A72AA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002854BF072F0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002854A72AA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002854BF072F0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002854A72AA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002854BF072F0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002854A72AA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002854BF072F0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002854A72AA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002854BF072F0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002854A72AA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002854BF072F0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002854A72AA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002854BF072F0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002854A72AA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002854BF072F0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002854A72AA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002854BF072F0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002854A72AA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002854BF072F0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002854A72AA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002854BF072F0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002854A72AA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002854BF072F0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002854A72AA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002854BF072F0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002854A72AA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002854BF072F0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002854A72AA60>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n"
     ]
    }
   ],
   "source": [
    "result_lvxnn = lvxnn('warm',train, test, tr_x, val_x, tr_y, val_y, tr_Xi, val_Xi, tr_idx, val_idx, meta_info, model_info, task_type , val_ratio=0.2, random_state=0, params=lx_params)\n",
    "result_svd = svd('warm',train, test, tr_x, tr_Xi, tr_y , te_x , te_Xi, te_y, meta_info, model_info, task_type, val_ratio=0.2, random_state=0)\n",
    "result_deepfm, result_fm = deepfm_fm('warm',train, test, tr_x, tr_Xi, tr_y , te_x , te_Xi, te_y, meta_info, model_info, task_type, val_ratio=0.2, random_state=0, epochs=300)\n",
    "result_xgb = xgb('warm',train, test, tr_x, tr_Xi, tr_y , te_x , te_Xi, te_y, meta_info, model_info, task_type, val_ratio=0.2, random_state=0)\n",
    "\n",
    "result_sim_re = pd.concat([result_lvxnn,result_svd,result_xgb,result_deepfm,result_fm],0)\n",
    "\n",
    "result_sim_re.to_csv('simulation_regression_result.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0624 15:09:41.781103 18224 deprecation.py:323] From C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "../..\\lvxnn\\DataReader.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.86 MB\n",
      "Memory usage after optimization is: 0.26 MB\n",
      "Decreased by 69.6%\n",
      "Memory usage of dataframe is 0.21 MB\n",
      "Memory usage after optimization is: 0.07 MB\n",
      "Decreased by 69.6%\n",
      "cold start user: 0\n",
      "cold start item: 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error,roc_auc_score,mean_absolute_error,log_loss\n",
    "import sys\n",
    "sys.path.append('../benchmark/')\n",
    "from lvxnn_test import lvxnn\n",
    "from xgb_test import xgb\n",
    "from svd_test import svd\n",
    "from deepfm_fm_test import deepfm_fm\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from lvxnn.LVXNN import LV_XNN\n",
    "from lvxnn.DataReader import data_initialize\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "data= pd.read_csv('../simulation/data/sim_binary_0.9.csv')\n",
    "train , test = train_test_split(data,test_size=0.2,random_state=0)\n",
    "task_type = \"Classification\"\n",
    "\n",
    "meta_info = OrderedDict()\n",
    "\n",
    "meta_info['uf_1']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_2']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_3']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_4']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_5']={'type': 'continues','source':'user'}\n",
    "meta_info['if_1']={'type': 'continues','source':'item'}\n",
    "meta_info['if_2']={'type': 'continues','source':'item'}\n",
    "meta_info['if_3']={'type': 'continues','source':'item'}\n",
    "meta_info['if_4']={'type': 'continues','source':'item'}\n",
    "meta_info['if_5']={'type': 'continues','source':'item'}\n",
    "meta_info['user_id']={\"type\":\"id\",'source':'user'}\n",
    "meta_info['item_id']={\"type\":\"id\",'source':'item'}\n",
    "meta_info['target']={\"type\":\"target\",'source':''}\n",
    "\n",
    "lx_params = {\n",
    "        \"main_effect_epochs\":300,\n",
    "        \"interaction_epochs\" : 200 ,\n",
    "        \"tuning_epochs\" : 50 , \n",
    "        \"mf_training_iters\": 100,\n",
    "        \"u_group_num\":30,\n",
    "        \"i_group_num\":50\n",
    "    }\n",
    "\n",
    "tr_x, tr_Xi, tr_y , te_x , te_Xi, te_y, meta_info, model_info = data_initialize(train,test,meta_info,task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68318, val loss: 0.68192\n",
      "Main effects training epoch: 2, train loss: 0.67724, val loss: 0.67807\n",
      "Main effects training epoch: 3, train loss: 0.67035, val loss: 0.67239\n",
      "Main effects training epoch: 4, train loss: 0.66391, val loss: 0.66654\n",
      "Main effects training epoch: 5, train loss: 0.65041, val loss: 0.65054\n",
      "Main effects training epoch: 6, train loss: 0.62260, val loss: 0.61929\n",
      "Main effects training epoch: 7, train loss: 0.58024, val loss: 0.57432\n",
      "Main effects training epoch: 8, train loss: 0.54720, val loss: 0.53681\n",
      "Main effects training epoch: 9, train loss: 0.53313, val loss: 0.51521\n",
      "Main effects training epoch: 10, train loss: 0.53085, val loss: 0.50585\n",
      "Main effects training epoch: 11, train loss: 0.52583, val loss: 0.50411\n",
      "Main effects training epoch: 12, train loss: 0.52419, val loss: 0.50436\n",
      "Main effects training epoch: 13, train loss: 0.52555, val loss: 0.50572\n",
      "Main effects training epoch: 14, train loss: 0.52540, val loss: 0.50457\n",
      "Main effects training epoch: 15, train loss: 0.52453, val loss: 0.50567\n",
      "Main effects training epoch: 16, train loss: 0.52467, val loss: 0.50372\n",
      "Main effects training epoch: 17, train loss: 0.52324, val loss: 0.50326\n",
      "Main effects training epoch: 18, train loss: 0.52333, val loss: 0.50195\n",
      "Main effects training epoch: 19, train loss: 0.52447, val loss: 0.50358\n",
      "Main effects training epoch: 20, train loss: 0.52310, val loss: 0.50254\n",
      "Main effects training epoch: 21, train loss: 0.52305, val loss: 0.50199\n",
      "Main effects training epoch: 22, train loss: 0.52218, val loss: 0.50175\n",
      "Main effects training epoch: 23, train loss: 0.52230, val loss: 0.50203\n",
      "Main effects training epoch: 24, train loss: 0.52207, val loss: 0.50145\n",
      "Main effects training epoch: 25, train loss: 0.52221, val loss: 0.50219\n",
      "Main effects training epoch: 26, train loss: 0.52249, val loss: 0.50205\n",
      "Main effects training epoch: 27, train loss: 0.52240, val loss: 0.50186\n",
      "Main effects training epoch: 28, train loss: 0.52215, val loss: 0.50118\n",
      "Main effects training epoch: 29, train loss: 0.52167, val loss: 0.50131\n",
      "Main effects training epoch: 30, train loss: 0.52172, val loss: 0.50206\n",
      "Main effects training epoch: 31, train loss: 0.52143, val loss: 0.50126\n",
      "Main effects training epoch: 32, train loss: 0.52173, val loss: 0.50185\n",
      "Main effects training epoch: 33, train loss: 0.52157, val loss: 0.50118\n",
      "Main effects training epoch: 34, train loss: 0.52134, val loss: 0.50190\n",
      "Main effects training epoch: 35, train loss: 0.52160, val loss: 0.50114\n",
      "Main effects training epoch: 36, train loss: 0.52124, val loss: 0.50122\n",
      "Main effects training epoch: 37, train loss: 0.52159, val loss: 0.50182\n",
      "Main effects training epoch: 38, train loss: 0.52178, val loss: 0.50166\n",
      "Main effects training epoch: 39, train loss: 0.52107, val loss: 0.50101\n",
      "Main effects training epoch: 40, train loss: 0.52120, val loss: 0.50207\n",
      "Main effects training epoch: 41, train loss: 0.52137, val loss: 0.50102\n",
      "Main effects training epoch: 42, train loss: 0.52109, val loss: 0.50159\n",
      "Main effects training epoch: 43, train loss: 0.52137, val loss: 0.50169\n",
      "Main effects training epoch: 44, train loss: 0.52140, val loss: 0.50110\n",
      "Main effects training epoch: 45, train loss: 0.52299, val loss: 0.50459\n",
      "Main effects training epoch: 46, train loss: 0.52226, val loss: 0.50272\n",
      "Main effects training epoch: 47, train loss: 0.52101, val loss: 0.50147\n",
      "Main effects training epoch: 48, train loss: 0.52109, val loss: 0.50075\n",
      "Main effects training epoch: 49, train loss: 0.52069, val loss: 0.50215\n",
      "Main effects training epoch: 50, train loss: 0.52045, val loss: 0.50063\n",
      "Main effects training epoch: 51, train loss: 0.52108, val loss: 0.50126\n",
      "Main effects training epoch: 52, train loss: 0.52199, val loss: 0.50324\n",
      "Main effects training epoch: 53, train loss: 0.52131, val loss: 0.50122\n",
      "Main effects training epoch: 54, train loss: 0.52256, val loss: 0.50297\n",
      "Main effects training epoch: 55, train loss: 0.52142, val loss: 0.50258\n",
      "Main effects training epoch: 56, train loss: 0.52045, val loss: 0.50089\n",
      "Main effects training epoch: 57, train loss: 0.52043, val loss: 0.50180\n",
      "Main effects training epoch: 58, train loss: 0.52018, val loss: 0.50121\n",
      "Main effects training epoch: 59, train loss: 0.51996, val loss: 0.50068\n",
      "Main effects training epoch: 60, train loss: 0.52012, val loss: 0.50109\n",
      "Main effects training epoch: 61, train loss: 0.52012, val loss: 0.50024\n",
      "Main effects training epoch: 62, train loss: 0.52057, val loss: 0.50418\n",
      "Main effects training epoch: 63, train loss: 0.52043, val loss: 0.50038\n",
      "Main effects training epoch: 64, train loss: 0.52095, val loss: 0.50364\n",
      "Main effects training epoch: 65, train loss: 0.52038, val loss: 0.50085\n",
      "Main effects training epoch: 66, train loss: 0.52047, val loss: 0.50241\n",
      "Main effects training epoch: 67, train loss: 0.52117, val loss: 0.50295\n",
      "Main effects training epoch: 68, train loss: 0.52073, val loss: 0.50183\n",
      "Main effects training epoch: 69, train loss: 0.52087, val loss: 0.50244\n",
      "Main effects training epoch: 70, train loss: 0.52010, val loss: 0.50147\n",
      "Main effects training epoch: 71, train loss: 0.51994, val loss: 0.50147\n",
      "Main effects training epoch: 72, train loss: 0.51942, val loss: 0.50086\n",
      "Main effects training epoch: 73, train loss: 0.51949, val loss: 0.50028\n",
      "Main effects training epoch: 74, train loss: 0.52005, val loss: 0.50237\n",
      "Main effects training epoch: 75, train loss: 0.51990, val loss: 0.50184\n",
      "Main effects training epoch: 76, train loss: 0.51923, val loss: 0.50236\n",
      "Main effects training epoch: 77, train loss: 0.51910, val loss: 0.50038\n",
      "Main effects training epoch: 78, train loss: 0.51884, val loss: 0.50095\n",
      "Main effects training epoch: 79, train loss: 0.51884, val loss: 0.49983\n",
      "Main effects training epoch: 80, train loss: 0.51866, val loss: 0.50191\n",
      "Main effects training epoch: 81, train loss: 0.51939, val loss: 0.50027\n",
      "Main effects training epoch: 82, train loss: 0.51929, val loss: 0.50319\n",
      "Main effects training epoch: 83, train loss: 0.51868, val loss: 0.50126\n",
      "Main effects training epoch: 84, train loss: 0.51858, val loss: 0.50088\n",
      "Main effects training epoch: 85, train loss: 0.51835, val loss: 0.50068\n",
      "Main effects training epoch: 86, train loss: 0.51834, val loss: 0.50062\n",
      "Main effects training epoch: 87, train loss: 0.51844, val loss: 0.50224\n",
      "Main effects training epoch: 88, train loss: 0.51878, val loss: 0.50098\n",
      "Main effects training epoch: 89, train loss: 0.51903, val loss: 0.50192\n",
      "Main effects training epoch: 90, train loss: 0.51939, val loss: 0.50209\n",
      "Main effects training epoch: 91, train loss: 0.51881, val loss: 0.50201\n",
      "Main effects training epoch: 92, train loss: 0.51849, val loss: 0.50011\n",
      "Main effects training epoch: 93, train loss: 0.51893, val loss: 0.50406\n",
      "Main effects training epoch: 94, train loss: 0.51813, val loss: 0.49975\n",
      "Main effects training epoch: 95, train loss: 0.51804, val loss: 0.50212\n",
      "Main effects training epoch: 96, train loss: 0.51773, val loss: 0.50088\n",
      "Main effects training epoch: 97, train loss: 0.51767, val loss: 0.50090\n",
      "Main effects training epoch: 98, train loss: 0.51759, val loss: 0.50188\n",
      "Main effects training epoch: 99, train loss: 0.51749, val loss: 0.50231\n",
      "Main effects training epoch: 100, train loss: 0.51739, val loss: 0.50044\n",
      "Main effects training epoch: 101, train loss: 0.51733, val loss: 0.50052\n",
      "Main effects training epoch: 102, train loss: 0.51797, val loss: 0.50373\n",
      "Main effects training epoch: 103, train loss: 0.51813, val loss: 0.50085\n",
      "Main effects training epoch: 104, train loss: 0.51776, val loss: 0.50234\n",
      "Main effects training epoch: 105, train loss: 0.51726, val loss: 0.50227\n",
      "Main effects training epoch: 106, train loss: 0.51709, val loss: 0.50129\n",
      "Main effects training epoch: 107, train loss: 0.51781, val loss: 0.50127\n",
      "Main effects training epoch: 108, train loss: 0.51764, val loss: 0.50367\n",
      "Main effects training epoch: 109, train loss: 0.51682, val loss: 0.50096\n",
      "Main effects training epoch: 110, train loss: 0.51693, val loss: 0.50167\n",
      "Main effects training epoch: 111, train loss: 0.51671, val loss: 0.50182\n",
      "Main effects training epoch: 112, train loss: 0.51688, val loss: 0.50161\n",
      "Main effects training epoch: 113, train loss: 0.51698, val loss: 0.50128\n",
      "Main effects training epoch: 114, train loss: 0.51709, val loss: 0.50115\n",
      "Main effects training epoch: 115, train loss: 0.51748, val loss: 0.50244\n",
      "Main effects training epoch: 116, train loss: 0.51737, val loss: 0.50229\n",
      "Main effects training epoch: 117, train loss: 0.51675, val loss: 0.50288\n",
      "Main effects training epoch: 118, train loss: 0.51650, val loss: 0.50089\n",
      "Main effects training epoch: 119, train loss: 0.51679, val loss: 0.50204\n",
      "Main effects training epoch: 120, train loss: 0.51664, val loss: 0.50211\n",
      "Main effects training epoch: 121, train loss: 0.51670, val loss: 0.50227\n",
      "Main effects training epoch: 122, train loss: 0.51695, val loss: 0.50339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 123, train loss: 0.51659, val loss: 0.50129\n",
      "Main effects training epoch: 124, train loss: 0.51633, val loss: 0.50324\n",
      "Main effects training epoch: 125, train loss: 0.51613, val loss: 0.50129\n",
      "Main effects training epoch: 126, train loss: 0.51658, val loss: 0.50223\n",
      "Main effects training epoch: 127, train loss: 0.51636, val loss: 0.50314\n",
      "Main effects training epoch: 128, train loss: 0.51642, val loss: 0.50299\n",
      "Main effects training epoch: 129, train loss: 0.51689, val loss: 0.50193\n",
      "Main effects training epoch: 130, train loss: 0.51717, val loss: 0.50329\n",
      "Main effects training epoch: 131, train loss: 0.51619, val loss: 0.50278\n",
      "Main effects training epoch: 132, train loss: 0.51595, val loss: 0.50213\n",
      "Main effects training epoch: 133, train loss: 0.51591, val loss: 0.50190\n",
      "Main effects training epoch: 134, train loss: 0.51614, val loss: 0.50354\n",
      "Main effects training epoch: 135, train loss: 0.51593, val loss: 0.50167\n",
      "Main effects training epoch: 136, train loss: 0.51683, val loss: 0.50322\n",
      "Main effects training epoch: 137, train loss: 0.51631, val loss: 0.50292\n",
      "Main effects training epoch: 138, train loss: 0.51597, val loss: 0.50196\n",
      "Main effects training epoch: 139, train loss: 0.51594, val loss: 0.50118\n",
      "Main effects training epoch: 140, train loss: 0.51575, val loss: 0.50247\n",
      "Main effects training epoch: 141, train loss: 0.51562, val loss: 0.50226\n",
      "Main effects training epoch: 142, train loss: 0.51583, val loss: 0.50147\n",
      "Main effects training epoch: 143, train loss: 0.51602, val loss: 0.50303\n",
      "Main effects training epoch: 144, train loss: 0.51584, val loss: 0.50264\n",
      "Main effects training epoch: 145, train loss: 0.51575, val loss: 0.50308\n",
      "Main effects training epoch: 146, train loss: 0.51556, val loss: 0.50241\n",
      "Main effects training epoch: 147, train loss: 0.51596, val loss: 0.50159\n",
      "Main effects training epoch: 148, train loss: 0.51555, val loss: 0.50244\n",
      "Main effects training epoch: 149, train loss: 0.51552, val loss: 0.50203\n",
      "Main effects training epoch: 150, train loss: 0.51570, val loss: 0.50205\n",
      "Main effects training epoch: 151, train loss: 0.51556, val loss: 0.50363\n",
      "Main effects training epoch: 152, train loss: 0.51593, val loss: 0.50189\n",
      "Main effects training epoch: 153, train loss: 0.51639, val loss: 0.50286\n",
      "Main effects training epoch: 154, train loss: 0.51624, val loss: 0.50523\n",
      "Main effects training epoch: 155, train loss: 0.51611, val loss: 0.50246\n",
      "Main effects training epoch: 156, train loss: 0.51643, val loss: 0.50459\n",
      "Main effects training epoch: 157, train loss: 0.51592, val loss: 0.50249\n",
      "Main effects training epoch: 158, train loss: 0.51552, val loss: 0.50365\n",
      "Main effects training epoch: 159, train loss: 0.51556, val loss: 0.50177\n",
      "Main effects training epoch: 160, train loss: 0.51540, val loss: 0.50281\n",
      "Main effects training epoch: 161, train loss: 0.51544, val loss: 0.50300\n",
      "Main effects training epoch: 162, train loss: 0.51528, val loss: 0.50196\n",
      "Main effects training epoch: 163, train loss: 0.51547, val loss: 0.50171\n",
      "Main effects training epoch: 164, train loss: 0.51538, val loss: 0.50210\n",
      "Main effects training epoch: 165, train loss: 0.51565, val loss: 0.50469\n",
      "Main effects training epoch: 166, train loss: 0.51521, val loss: 0.50062\n",
      "Main effects training epoch: 167, train loss: 0.51535, val loss: 0.50326\n",
      "Main effects training epoch: 168, train loss: 0.51525, val loss: 0.50222\n",
      "Main effects training epoch: 169, train loss: 0.51514, val loss: 0.50331\n",
      "Main effects training epoch: 170, train loss: 0.51509, val loss: 0.50200\n",
      "Main effects training epoch: 171, train loss: 0.51490, val loss: 0.50324\n",
      "Main effects training epoch: 172, train loss: 0.51508, val loss: 0.50247\n",
      "Main effects training epoch: 173, train loss: 0.51493, val loss: 0.50182\n",
      "Main effects training epoch: 174, train loss: 0.51522, val loss: 0.50212\n",
      "Main effects training epoch: 175, train loss: 0.51508, val loss: 0.50334\n",
      "Main effects training epoch: 176, train loss: 0.51483, val loss: 0.50241\n",
      "Main effects training epoch: 177, train loss: 0.51504, val loss: 0.50039\n",
      "Main effects training epoch: 178, train loss: 0.51532, val loss: 0.50473\n",
      "Main effects training epoch: 179, train loss: 0.51527, val loss: 0.50142\n",
      "Main effects training epoch: 180, train loss: 0.51478, val loss: 0.50288\n",
      "Main effects training epoch: 181, train loss: 0.51468, val loss: 0.50177\n",
      "Main effects training epoch: 182, train loss: 0.51487, val loss: 0.50212\n",
      "Main effects training epoch: 183, train loss: 0.51473, val loss: 0.50130\n",
      "Main effects training epoch: 184, train loss: 0.51443, val loss: 0.50233\n",
      "Main effects training epoch: 185, train loss: 0.51443, val loss: 0.50209\n",
      "Main effects training epoch: 186, train loss: 0.51449, val loss: 0.50105\n",
      "Main effects training epoch: 187, train loss: 0.51500, val loss: 0.50328\n",
      "Main effects training epoch: 188, train loss: 0.51584, val loss: 0.50243\n",
      "Main effects training epoch: 189, train loss: 0.51498, val loss: 0.50282\n",
      "Main effects training epoch: 190, train loss: 0.51476, val loss: 0.50202\n",
      "Main effects training epoch: 191, train loss: 0.51499, val loss: 0.50370\n",
      "Main effects training epoch: 192, train loss: 0.51451, val loss: 0.50061\n",
      "Main effects training epoch: 193, train loss: 0.51427, val loss: 0.50163\n",
      "Main effects training epoch: 194, train loss: 0.51468, val loss: 0.50289\n",
      "Main effects training epoch: 195, train loss: 0.51485, val loss: 0.50152\n",
      "Early stop at epoch 195, with validation loss: 0.50152\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51652, val loss: 0.50334\n",
      "Main effects tuning epoch: 2, train loss: 0.51620, val loss: 0.50298\n",
      "Main effects tuning epoch: 3, train loss: 0.51591, val loss: 0.50243\n",
      "Main effects tuning epoch: 4, train loss: 0.51583, val loss: 0.50178\n",
      "Main effects tuning epoch: 5, train loss: 0.51587, val loss: 0.50310\n",
      "Main effects tuning epoch: 6, train loss: 0.51589, val loss: 0.50141\n",
      "Main effects tuning epoch: 7, train loss: 0.51573, val loss: 0.50312\n",
      "Main effects tuning epoch: 8, train loss: 0.51616, val loss: 0.50270\n",
      "Main effects tuning epoch: 9, train loss: 0.51592, val loss: 0.50204\n",
      "Main effects tuning epoch: 10, train loss: 0.51562, val loss: 0.50183\n",
      "Main effects tuning epoch: 11, train loss: 0.51574, val loss: 0.50319\n",
      "Main effects tuning epoch: 12, train loss: 0.51563, val loss: 0.50186\n",
      "Main effects tuning epoch: 13, train loss: 0.51576, val loss: 0.50178\n",
      "Main effects tuning epoch: 14, train loss: 0.51619, val loss: 0.50417\n",
      "Main effects tuning epoch: 15, train loss: 0.51630, val loss: 0.50174\n",
      "Main effects tuning epoch: 16, train loss: 0.51617, val loss: 0.50504\n",
      "Main effects tuning epoch: 17, train loss: 0.51578, val loss: 0.50114\n",
      "Main effects tuning epoch: 18, train loss: 0.51571, val loss: 0.50339\n",
      "Main effects tuning epoch: 19, train loss: 0.51541, val loss: 0.50297\n",
      "Main effects tuning epoch: 20, train loss: 0.51567, val loss: 0.50233\n",
      "Main effects tuning epoch: 21, train loss: 0.51545, val loss: 0.50194\n",
      "Main effects tuning epoch: 22, train loss: 0.51550, val loss: 0.50202\n",
      "Main effects tuning epoch: 23, train loss: 0.51527, val loss: 0.50183\n",
      "Main effects tuning epoch: 24, train loss: 0.51583, val loss: 0.50227\n",
      "Main effects tuning epoch: 25, train loss: 0.51612, val loss: 0.50429\n",
      "Main effects tuning epoch: 26, train loss: 0.51526, val loss: 0.50222\n",
      "Main effects tuning epoch: 27, train loss: 0.51592, val loss: 0.50259\n",
      "Main effects tuning epoch: 28, train loss: 0.51541, val loss: 0.50208\n",
      "Main effects tuning epoch: 29, train loss: 0.51598, val loss: 0.50367\n",
      "Main effects tuning epoch: 30, train loss: 0.51600, val loss: 0.50229\n",
      "Main effects tuning epoch: 31, train loss: 0.51550, val loss: 0.50323\n",
      "Main effects tuning epoch: 32, train loss: 0.51546, val loss: 0.50104\n",
      "Main effects tuning epoch: 33, train loss: 0.51535, val loss: 0.50236\n",
      "Main effects tuning epoch: 34, train loss: 0.51519, val loss: 0.50118\n",
      "Main effects tuning epoch: 35, train loss: 0.51491, val loss: 0.50257\n",
      "Main effects tuning epoch: 36, train loss: 0.51491, val loss: 0.50182\n",
      "Main effects tuning epoch: 37, train loss: 0.51516, val loss: 0.50071\n",
      "Main effects tuning epoch: 38, train loss: 0.51511, val loss: 0.50194\n",
      "Main effects tuning epoch: 39, train loss: 0.51511, val loss: 0.50230\n",
      "Main effects tuning epoch: 40, train loss: 0.51584, val loss: 0.50414\n",
      "Main effects tuning epoch: 41, train loss: 0.51531, val loss: 0.50065\n",
      "Main effects tuning epoch: 42, train loss: 0.51515, val loss: 0.50337\n",
      "Main effects tuning epoch: 43, train loss: 0.51471, val loss: 0.50169\n",
      "Main effects tuning epoch: 44, train loss: 0.51484, val loss: 0.50097\n",
      "Main effects tuning epoch: 45, train loss: 0.51488, val loss: 0.50235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects tuning epoch: 46, train loss: 0.51479, val loss: 0.50102\n",
      "Main effects tuning epoch: 47, train loss: 0.51487, val loss: 0.50196\n",
      "Main effects tuning epoch: 48, train loss: 0.51463, val loss: 0.50183\n",
      "Main effects tuning epoch: 49, train loss: 0.51469, val loss: 0.50103\n",
      "Main effects tuning epoch: 50, train loss: 0.51472, val loss: 0.50134\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.49630, val loss: 0.48952\n",
      "Interaction training epoch: 2, train loss: 0.33769, val loss: 0.34284\n",
      "Interaction training epoch: 3, train loss: 0.35128, val loss: 0.36496\n",
      "Interaction training epoch: 4, train loss: 0.31659, val loss: 0.31842\n",
      "Interaction training epoch: 5, train loss: 0.30166, val loss: 0.30688\n",
      "Interaction training epoch: 6, train loss: 0.29518, val loss: 0.29846\n",
      "Interaction training epoch: 7, train loss: 0.28798, val loss: 0.28992\n",
      "Interaction training epoch: 8, train loss: 0.28937, val loss: 0.29121\n",
      "Interaction training epoch: 9, train loss: 0.29272, val loss: 0.29817\n",
      "Interaction training epoch: 10, train loss: 0.29134, val loss: 0.29788\n",
      "Interaction training epoch: 11, train loss: 0.28068, val loss: 0.28755\n",
      "Interaction training epoch: 12, train loss: 0.28154, val loss: 0.28695\n",
      "Interaction training epoch: 13, train loss: 0.27857, val loss: 0.28611\n",
      "Interaction training epoch: 14, train loss: 0.28018, val loss: 0.28605\n",
      "Interaction training epoch: 15, train loss: 0.27761, val loss: 0.28852\n",
      "Interaction training epoch: 16, train loss: 0.27799, val loss: 0.28495\n",
      "Interaction training epoch: 17, train loss: 0.27600, val loss: 0.28684\n",
      "Interaction training epoch: 18, train loss: 0.27797, val loss: 0.28446\n",
      "Interaction training epoch: 19, train loss: 0.27427, val loss: 0.28254\n",
      "Interaction training epoch: 20, train loss: 0.27444, val loss: 0.28269\n",
      "Interaction training epoch: 21, train loss: 0.27197, val loss: 0.28017\n",
      "Interaction training epoch: 22, train loss: 0.28111, val loss: 0.29098\n",
      "Interaction training epoch: 23, train loss: 0.27471, val loss: 0.28461\n",
      "Interaction training epoch: 24, train loss: 0.27383, val loss: 0.28217\n",
      "Interaction training epoch: 25, train loss: 0.27561, val loss: 0.28093\n",
      "Interaction training epoch: 26, train loss: 0.27286, val loss: 0.28056\n",
      "Interaction training epoch: 27, train loss: 0.27501, val loss: 0.28302\n",
      "Interaction training epoch: 28, train loss: 0.27465, val loss: 0.28525\n",
      "Interaction training epoch: 29, train loss: 0.27005, val loss: 0.27613\n",
      "Interaction training epoch: 30, train loss: 0.27126, val loss: 0.27893\n",
      "Interaction training epoch: 31, train loss: 0.27173, val loss: 0.28095\n",
      "Interaction training epoch: 32, train loss: 0.27099, val loss: 0.28101\n",
      "Interaction training epoch: 33, train loss: 0.26687, val loss: 0.27472\n",
      "Interaction training epoch: 34, train loss: 0.26881, val loss: 0.27762\n",
      "Interaction training epoch: 35, train loss: 0.26911, val loss: 0.28276\n",
      "Interaction training epoch: 36, train loss: 0.27233, val loss: 0.28279\n",
      "Interaction training epoch: 37, train loss: 0.27192, val loss: 0.28092\n",
      "Interaction training epoch: 38, train loss: 0.26677, val loss: 0.27617\n",
      "Interaction training epoch: 39, train loss: 0.26946, val loss: 0.27759\n",
      "Interaction training epoch: 40, train loss: 0.26705, val loss: 0.28085\n",
      "Interaction training epoch: 41, train loss: 0.26524, val loss: 0.27444\n",
      "Interaction training epoch: 42, train loss: 0.27260, val loss: 0.28364\n",
      "Interaction training epoch: 43, train loss: 0.26587, val loss: 0.27973\n",
      "Interaction training epoch: 44, train loss: 0.26654, val loss: 0.27690\n",
      "Interaction training epoch: 45, train loss: 0.26566, val loss: 0.28007\n",
      "Interaction training epoch: 46, train loss: 0.26457, val loss: 0.27688\n",
      "Interaction training epoch: 47, train loss: 0.26778, val loss: 0.28093\n",
      "Interaction training epoch: 48, train loss: 0.26713, val loss: 0.28041\n",
      "Interaction training epoch: 49, train loss: 0.26385, val loss: 0.27635\n",
      "Interaction training epoch: 50, train loss: 0.26436, val loss: 0.27912\n",
      "Interaction training epoch: 51, train loss: 0.26506, val loss: 0.27701\n",
      "Interaction training epoch: 52, train loss: 0.26332, val loss: 0.27839\n",
      "Interaction training epoch: 53, train loss: 0.26575, val loss: 0.27820\n",
      "Interaction training epoch: 54, train loss: 0.26444, val loss: 0.27399\n",
      "Interaction training epoch: 55, train loss: 0.26446, val loss: 0.28050\n",
      "Interaction training epoch: 56, train loss: 0.26212, val loss: 0.27389\n",
      "Interaction training epoch: 57, train loss: 0.26537, val loss: 0.28100\n",
      "Interaction training epoch: 58, train loss: 0.26231, val loss: 0.27333\n",
      "Interaction training epoch: 59, train loss: 0.26531, val loss: 0.28369\n",
      "Interaction training epoch: 60, train loss: 0.26223, val loss: 0.27409\n",
      "Interaction training epoch: 61, train loss: 0.26070, val loss: 0.27585\n",
      "Interaction training epoch: 62, train loss: 0.26415, val loss: 0.28179\n",
      "Interaction training epoch: 63, train loss: 0.26012, val loss: 0.27531\n",
      "Interaction training epoch: 64, train loss: 0.26214, val loss: 0.27596\n",
      "Interaction training epoch: 65, train loss: 0.26123, val loss: 0.27634\n",
      "Interaction training epoch: 66, train loss: 0.26041, val loss: 0.27562\n",
      "Interaction training epoch: 67, train loss: 0.26073, val loss: 0.27566\n",
      "Interaction training epoch: 68, train loss: 0.26038, val loss: 0.27691\n",
      "Interaction training epoch: 69, train loss: 0.25848, val loss: 0.27646\n",
      "Interaction training epoch: 70, train loss: 0.26296, val loss: 0.27763\n",
      "Interaction training epoch: 71, train loss: 0.26102, val loss: 0.27864\n",
      "Interaction training epoch: 72, train loss: 0.26107, val loss: 0.27708\n",
      "Interaction training epoch: 73, train loss: 0.26414, val loss: 0.27619\n",
      "Interaction training epoch: 74, train loss: 0.25864, val loss: 0.27725\n",
      "Interaction training epoch: 75, train loss: 0.25926, val loss: 0.27525\n",
      "Interaction training epoch: 76, train loss: 0.25686, val loss: 0.27432\n",
      "Interaction training epoch: 77, train loss: 0.25954, val loss: 0.27993\n",
      "Interaction training epoch: 78, train loss: 0.26144, val loss: 0.27545\n",
      "Interaction training epoch: 79, train loss: 0.25751, val loss: 0.27693\n",
      "Interaction training epoch: 80, train loss: 0.25861, val loss: 0.27448\n",
      "Interaction training epoch: 81, train loss: 0.25719, val loss: 0.27631\n",
      "Interaction training epoch: 82, train loss: 0.26048, val loss: 0.27937\n",
      "Interaction training epoch: 83, train loss: 0.25757, val loss: 0.27424\n",
      "Interaction training epoch: 84, train loss: 0.25932, val loss: 0.27815\n",
      "Interaction training epoch: 85, train loss: 0.26140, val loss: 0.27858\n",
      "Interaction training epoch: 86, train loss: 0.25378, val loss: 0.27194\n",
      "Interaction training epoch: 87, train loss: 0.25811, val loss: 0.27779\n",
      "Interaction training epoch: 88, train loss: 0.25630, val loss: 0.27527\n",
      "Interaction training epoch: 89, train loss: 0.25527, val loss: 0.27723\n",
      "Interaction training epoch: 90, train loss: 0.25759, val loss: 0.27676\n",
      "Interaction training epoch: 91, train loss: 0.25457, val loss: 0.27467\n",
      "Interaction training epoch: 92, train loss: 0.25542, val loss: 0.27403\n",
      "Interaction training epoch: 93, train loss: 0.25535, val loss: 0.27589\n",
      "Interaction training epoch: 94, train loss: 0.25407, val loss: 0.27538\n",
      "Interaction training epoch: 95, train loss: 0.25767, val loss: 0.27769\n",
      "Interaction training epoch: 96, train loss: 0.25220, val loss: 0.27444\n",
      "Interaction training epoch: 97, train loss: 0.25584, val loss: 0.27263\n",
      "Interaction training epoch: 98, train loss: 0.25244, val loss: 0.27405\n",
      "Interaction training epoch: 99, train loss: 0.25535, val loss: 0.27697\n",
      "Interaction training epoch: 100, train loss: 0.25545, val loss: 0.27934\n",
      "Interaction training epoch: 101, train loss: 0.25633, val loss: 0.27754\n",
      "Interaction training epoch: 102, train loss: 0.25267, val loss: 0.27028\n",
      "Interaction training epoch: 103, train loss: 0.25493, val loss: 0.27683\n",
      "Interaction training epoch: 104, train loss: 0.25262, val loss: 0.27466\n",
      "Interaction training epoch: 105, train loss: 0.25030, val loss: 0.27110\n",
      "Interaction training epoch: 106, train loss: 0.25358, val loss: 0.27660\n",
      "Interaction training epoch: 107, train loss: 0.25603, val loss: 0.27754\n",
      "Interaction training epoch: 108, train loss: 0.24978, val loss: 0.27290\n",
      "Interaction training epoch: 109, train loss: 0.25282, val loss: 0.27685\n",
      "Interaction training epoch: 110, train loss: 0.25069, val loss: 0.27038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 111, train loss: 0.25313, val loss: 0.27830\n",
      "Interaction training epoch: 112, train loss: 0.25039, val loss: 0.27405\n",
      "Interaction training epoch: 113, train loss: 0.25058, val loss: 0.27310\n",
      "Interaction training epoch: 114, train loss: 0.25279, val loss: 0.27999\n",
      "Interaction training epoch: 115, train loss: 0.24922, val loss: 0.27298\n",
      "Interaction training epoch: 116, train loss: 0.25238, val loss: 0.28017\n",
      "Interaction training epoch: 117, train loss: 0.24869, val loss: 0.27323\n",
      "Interaction training epoch: 118, train loss: 0.24943, val loss: 0.27615\n",
      "Interaction training epoch: 119, train loss: 0.24886, val loss: 0.27248\n",
      "Interaction training epoch: 120, train loss: 0.24706, val loss: 0.27312\n",
      "Interaction training epoch: 121, train loss: 0.24960, val loss: 0.27178\n",
      "Interaction training epoch: 122, train loss: 0.24969, val loss: 0.27520\n",
      "Interaction training epoch: 123, train loss: 0.24595, val loss: 0.27154\n",
      "Interaction training epoch: 124, train loss: 0.24900, val loss: 0.27416\n",
      "Interaction training epoch: 125, train loss: 0.24952, val loss: 0.27728\n",
      "Interaction training epoch: 126, train loss: 0.24514, val loss: 0.27059\n",
      "Interaction training epoch: 127, train loss: 0.24724, val loss: 0.27257\n",
      "Interaction training epoch: 128, train loss: 0.24934, val loss: 0.27673\n",
      "Interaction training epoch: 129, train loss: 0.25057, val loss: 0.27925\n",
      "Interaction training epoch: 130, train loss: 0.24874, val loss: 0.27362\n",
      "Interaction training epoch: 131, train loss: 0.24820, val loss: 0.27789\n",
      "Interaction training epoch: 132, train loss: 0.24936, val loss: 0.27270\n",
      "Interaction training epoch: 133, train loss: 0.24469, val loss: 0.27186\n",
      "Interaction training epoch: 134, train loss: 0.24571, val loss: 0.27370\n",
      "Interaction training epoch: 135, train loss: 0.24431, val loss: 0.27322\n",
      "Interaction training epoch: 136, train loss: 0.24633, val loss: 0.27261\n",
      "Interaction training epoch: 137, train loss: 0.24825, val loss: 0.27969\n",
      "Interaction training epoch: 138, train loss: 0.24335, val loss: 0.27059\n",
      "Interaction training epoch: 139, train loss: 0.24851, val loss: 0.27798\n",
      "Interaction training epoch: 140, train loss: 0.24583, val loss: 0.27031\n",
      "Interaction training epoch: 141, train loss: 0.24941, val loss: 0.28354\n",
      "Interaction training epoch: 142, train loss: 0.24411, val loss: 0.27392\n",
      "Interaction training epoch: 143, train loss: 0.24626, val loss: 0.27780\n",
      "Interaction training epoch: 144, train loss: 0.24308, val loss: 0.27648\n",
      "Interaction training epoch: 145, train loss: 0.24347, val loss: 0.27154\n",
      "Interaction training epoch: 146, train loss: 0.24461, val loss: 0.27730\n",
      "Interaction training epoch: 147, train loss: 0.24531, val loss: 0.27648\n",
      "Interaction training epoch: 148, train loss: 0.24362, val loss: 0.27834\n",
      "Interaction training epoch: 149, train loss: 0.24601, val loss: 0.27461\n",
      "Interaction training epoch: 150, train loss: 0.24428, val loss: 0.27347\n",
      "Interaction training epoch: 151, train loss: 0.24732, val loss: 0.28053\n",
      "Interaction training epoch: 152, train loss: 0.24191, val loss: 0.27360\n",
      "Interaction training epoch: 153, train loss: 0.24221, val loss: 0.27440\n",
      "Interaction training epoch: 154, train loss: 0.24280, val loss: 0.27499\n",
      "Interaction training epoch: 155, train loss: 0.24201, val loss: 0.27586\n",
      "Interaction training epoch: 156, train loss: 0.24477, val loss: 0.27752\n",
      "Interaction training epoch: 157, train loss: 0.24507, val loss: 0.27833\n",
      "Interaction training epoch: 158, train loss: 0.24068, val loss: 0.27333\n",
      "Interaction training epoch: 159, train loss: 0.24391, val loss: 0.27683\n",
      "Interaction training epoch: 160, train loss: 0.24175, val loss: 0.27674\n",
      "Interaction training epoch: 161, train loss: 0.23881, val loss: 0.26956\n",
      "Interaction training epoch: 162, train loss: 0.24359, val loss: 0.27676\n",
      "Interaction training epoch: 163, train loss: 0.23898, val loss: 0.27401\n",
      "Interaction training epoch: 164, train loss: 0.23981, val loss: 0.27359\n",
      "Interaction training epoch: 165, train loss: 0.24112, val loss: 0.27625\n",
      "Interaction training epoch: 166, train loss: 0.23941, val loss: 0.27442\n",
      "Interaction training epoch: 167, train loss: 0.24063, val loss: 0.27651\n",
      "Interaction training epoch: 168, train loss: 0.24010, val loss: 0.27885\n",
      "Interaction training epoch: 169, train loss: 0.24168, val loss: 0.27591\n",
      "Interaction training epoch: 170, train loss: 0.24327, val loss: 0.28124\n",
      "Interaction training epoch: 171, train loss: 0.23941, val loss: 0.27438\n",
      "Interaction training epoch: 172, train loss: 0.24055, val loss: 0.27595\n",
      "Interaction training epoch: 173, train loss: 0.23916, val loss: 0.27534\n",
      "Interaction training epoch: 174, train loss: 0.24062, val loss: 0.28139\n",
      "Interaction training epoch: 175, train loss: 0.23799, val loss: 0.27390\n",
      "Interaction training epoch: 176, train loss: 0.23947, val loss: 0.27684\n",
      "Interaction training epoch: 177, train loss: 0.23886, val loss: 0.27735\n",
      "Interaction training epoch: 178, train loss: 0.23895, val loss: 0.27472\n",
      "Interaction training epoch: 179, train loss: 0.23942, val loss: 0.27651\n",
      "Interaction training epoch: 180, train loss: 0.24049, val loss: 0.28036\n",
      "Interaction training epoch: 181, train loss: 0.23897, val loss: 0.27791\n",
      "Interaction training epoch: 182, train loss: 0.23646, val loss: 0.27363\n",
      "Interaction training epoch: 183, train loss: 0.23810, val loss: 0.27910\n",
      "Interaction training epoch: 184, train loss: 0.23821, val loss: 0.27539\n",
      "Interaction training epoch: 185, train loss: 0.23642, val loss: 0.27411\n",
      "Interaction training epoch: 186, train loss: 0.23809, val loss: 0.27799\n",
      "Interaction training epoch: 187, train loss: 0.23796, val loss: 0.27475\n",
      "Interaction training epoch: 188, train loss: 0.23539, val loss: 0.27566\n",
      "Interaction training epoch: 189, train loss: 0.23735, val loss: 0.27996\n",
      "Interaction training epoch: 190, train loss: 0.23828, val loss: 0.27894\n",
      "Interaction training epoch: 191, train loss: 0.23465, val loss: 0.27510\n",
      "Interaction training epoch: 192, train loss: 0.23838, val loss: 0.27980\n",
      "Interaction training epoch: 193, train loss: 0.23741, val loss: 0.27565\n",
      "Interaction training epoch: 194, train loss: 0.23572, val loss: 0.27452\n",
      "Interaction training epoch: 195, train loss: 0.23802, val loss: 0.28027\n",
      "Interaction training epoch: 196, train loss: 0.23539, val loss: 0.27689\n",
      "Interaction training epoch: 197, train loss: 0.23723, val loss: 0.27746\n",
      "Interaction training epoch: 198, train loss: 0.23395, val loss: 0.27462\n",
      "Interaction training epoch: 199, train loss: 0.23661, val loss: 0.28002\n",
      "Interaction training epoch: 200, train loss: 0.23513, val loss: 0.27434\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########3 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.24968, val loss: 0.27903\n",
      "Interaction tuning epoch: 2, train loss: 0.24776, val loss: 0.28053\n",
      "Interaction tuning epoch: 3, train loss: 0.24547, val loss: 0.27095\n",
      "Interaction tuning epoch: 4, train loss: 0.24990, val loss: 0.28140\n",
      "Interaction tuning epoch: 5, train loss: 0.24715, val loss: 0.27506\n",
      "Interaction tuning epoch: 6, train loss: 0.24635, val loss: 0.27332\n",
      "Interaction tuning epoch: 7, train loss: 0.24591, val loss: 0.27750\n",
      "Interaction tuning epoch: 8, train loss: 0.24374, val loss: 0.27154\n",
      "Interaction tuning epoch: 9, train loss: 0.24867, val loss: 0.27508\n",
      "Interaction tuning epoch: 10, train loss: 0.24609, val loss: 0.27569\n",
      "Interaction tuning epoch: 11, train loss: 0.24703, val loss: 0.27608\n",
      "Interaction tuning epoch: 12, train loss: 0.24513, val loss: 0.27373\n",
      "Interaction tuning epoch: 13, train loss: 0.24529, val loss: 0.27530\n",
      "Interaction tuning epoch: 14, train loss: 0.24515, val loss: 0.27229\n",
      "Interaction tuning epoch: 15, train loss: 0.24666, val loss: 0.27752\n",
      "Interaction tuning epoch: 16, train loss: 0.24396, val loss: 0.27377\n",
      "Interaction tuning epoch: 17, train loss: 0.24588, val loss: 0.27324\n",
      "Interaction tuning epoch: 18, train loss: 0.24525, val loss: 0.27665\n",
      "Interaction tuning epoch: 19, train loss: 0.24285, val loss: 0.27234\n",
      "Interaction tuning epoch: 20, train loss: 0.24430, val loss: 0.27507\n",
      "Interaction tuning epoch: 21, train loss: 0.24167, val loss: 0.27076\n",
      "Interaction tuning epoch: 22, train loss: 0.24412, val loss: 0.27322\n",
      "Interaction tuning epoch: 23, train loss: 0.24488, val loss: 0.27656\n",
      "Interaction tuning epoch: 24, train loss: 0.24215, val loss: 0.27113\n",
      "Interaction tuning epoch: 25, train loss: 0.24397, val loss: 0.27656\n",
      "Interaction tuning epoch: 26, train loss: 0.24370, val loss: 0.27526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 27, train loss: 0.24330, val loss: 0.27308\n",
      "Interaction tuning epoch: 28, train loss: 0.24366, val loss: 0.27189\n",
      "Interaction tuning epoch: 29, train loss: 0.24133, val loss: 0.27163\n",
      "Interaction tuning epoch: 30, train loss: 0.24330, val loss: 0.27379\n",
      "Interaction tuning epoch: 31, train loss: 0.24268, val loss: 0.27507\n",
      "Interaction tuning epoch: 32, train loss: 0.24475, val loss: 0.27470\n",
      "Interaction tuning epoch: 33, train loss: 0.24434, val loss: 0.27429\n",
      "Interaction tuning epoch: 34, train loss: 0.24265, val loss: 0.27108\n",
      "Interaction tuning epoch: 35, train loss: 0.24270, val loss: 0.27413\n",
      "Interaction tuning epoch: 36, train loss: 0.24404, val loss: 0.27374\n",
      "Interaction tuning epoch: 37, train loss: 0.24276, val loss: 0.27324\n",
      "Interaction tuning epoch: 38, train loss: 0.24406, val loss: 0.27388\n",
      "Interaction tuning epoch: 39, train loss: 0.24261, val loss: 0.27577\n",
      "Interaction tuning epoch: 40, train loss: 0.24088, val loss: 0.27160\n",
      "Interaction tuning epoch: 41, train loss: 0.24276, val loss: 0.27220\n",
      "Interaction tuning epoch: 42, train loss: 0.24198, val loss: 0.27404\n",
      "Interaction tuning epoch: 43, train loss: 0.24426, val loss: 0.27793\n",
      "Interaction tuning epoch: 44, train loss: 0.24072, val loss: 0.27060\n",
      "Interaction tuning epoch: 45, train loss: 0.24021, val loss: 0.27217\n",
      "Interaction tuning epoch: 46, train loss: 0.24042, val loss: 0.27170\n",
      "Interaction tuning epoch: 47, train loss: 0.24126, val loss: 0.27376\n",
      "Interaction tuning epoch: 48, train loss: 0.24628, val loss: 0.27823\n",
      "Interaction tuning epoch: 49, train loss: 0.24267, val loss: 0.26952\n",
      "Interaction tuning epoch: 50, train loss: 0.24486, val loss: 0.27854\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 32.85082197189331\n",
      "After the gam stage, training error is 0.24486 , validation error is 0.27854\n",
      "missing value counts: 99148\n",
      "#####start auto_tuning#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best shrinkage is 0.793600\n",
      "the best combination is 0.938664\n",
      "[SoftImpute] Max Singular Value of X_init = 3.614024\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.210213 validation BCE=0.301748,rank=5\n",
      "[SoftImpute] Iter 2: observed BCE=0.205910 validation BCE=0.302484,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.203628 validation BCE=0.300061,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.201417 validation BCE=0.299692,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.199947 validation BCE=0.287628,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.198856 validation BCE=0.297811,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.197819 validation BCE=0.288751,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.197132 validation BCE=0.297311,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.196484 validation BCE=0.285337,rank=5\n",
      "[SoftImpute] Iter 10: observed BCE=0.196001 validation BCE=0.285604,rank=5\n",
      "[SoftImpute] Iter 11: observed BCE=0.195640 validation BCE=0.285478,rank=5\n",
      "[SoftImpute] Iter 12: observed BCE=0.195081 validation BCE=0.286393,rank=5\n",
      "[SoftImpute] Iter 13: observed BCE=0.194486 validation BCE=0.286278,rank=5\n",
      "[SoftImpute] Iter 14: observed BCE=0.194365 validation BCE=0.286792,rank=5\n",
      "[SoftImpute] Iter 15: observed BCE=0.193987 validation BCE=0.288731,rank=5\n",
      "[SoftImpute] Iter 16: observed BCE=0.193837 validation BCE=0.297786,rank=5\n",
      "[SoftImpute] Stopped after iteration 16 for lambda=0.072280\n",
      "final num of user group: 29\n",
      "final num of item group: 48\n",
      "change mode state : True\n",
      "time cost: 583.6314580440521\n",
      "After the matrix factor stage, training error is 0.19384, validation error is 0.29779\n",
      "0\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68318, val loss: 0.68192\n",
      "Main effects training epoch: 2, train loss: 0.67724, val loss: 0.67807\n",
      "Main effects training epoch: 3, train loss: 0.67035, val loss: 0.67239\n",
      "Main effects training epoch: 4, train loss: 0.66391, val loss: 0.66654\n",
      "Main effects training epoch: 5, train loss: 0.65041, val loss: 0.65054\n",
      "Main effects training epoch: 6, train loss: 0.62260, val loss: 0.61929\n",
      "Main effects training epoch: 7, train loss: 0.58024, val loss: 0.57432\n",
      "Main effects training epoch: 8, train loss: 0.54720, val loss: 0.53681\n",
      "Main effects training epoch: 9, train loss: 0.53313, val loss: 0.51521\n",
      "Main effects training epoch: 10, train loss: 0.53085, val loss: 0.50585\n",
      "Main effects training epoch: 11, train loss: 0.52583, val loss: 0.50411\n",
      "Main effects training epoch: 12, train loss: 0.52419, val loss: 0.50436\n",
      "Main effects training epoch: 13, train loss: 0.52555, val loss: 0.50572\n",
      "Main effects training epoch: 14, train loss: 0.52540, val loss: 0.50457\n",
      "Main effects training epoch: 15, train loss: 0.52453, val loss: 0.50567\n",
      "Main effects training epoch: 16, train loss: 0.52467, val loss: 0.50372\n",
      "Main effects training epoch: 17, train loss: 0.52324, val loss: 0.50326\n",
      "Main effects training epoch: 18, train loss: 0.52333, val loss: 0.50195\n",
      "Main effects training epoch: 19, train loss: 0.52447, val loss: 0.50358\n",
      "Main effects training epoch: 20, train loss: 0.52310, val loss: 0.50254\n",
      "Main effects training epoch: 21, train loss: 0.52305, val loss: 0.50199\n",
      "Main effects training epoch: 22, train loss: 0.52218, val loss: 0.50175\n",
      "Main effects training epoch: 23, train loss: 0.52230, val loss: 0.50203\n",
      "Main effects training epoch: 24, train loss: 0.52207, val loss: 0.50145\n",
      "Main effects training epoch: 25, train loss: 0.52221, val loss: 0.50219\n",
      "Main effects training epoch: 26, train loss: 0.52249, val loss: 0.50205\n",
      "Main effects training epoch: 27, train loss: 0.52240, val loss: 0.50186\n",
      "Main effects training epoch: 28, train loss: 0.52215, val loss: 0.50118\n",
      "Main effects training epoch: 29, train loss: 0.52167, val loss: 0.50131\n",
      "Main effects training epoch: 30, train loss: 0.52172, val loss: 0.50206\n",
      "Main effects training epoch: 31, train loss: 0.52143, val loss: 0.50126\n",
      "Main effects training epoch: 32, train loss: 0.52173, val loss: 0.50185\n",
      "Main effects training epoch: 33, train loss: 0.52157, val loss: 0.50118\n",
      "Main effects training epoch: 34, train loss: 0.52134, val loss: 0.50190\n",
      "Main effects training epoch: 35, train loss: 0.52160, val loss: 0.50114\n",
      "Main effects training epoch: 36, train loss: 0.52124, val loss: 0.50122\n",
      "Main effects training epoch: 37, train loss: 0.52159, val loss: 0.50182\n",
      "Main effects training epoch: 38, train loss: 0.52178, val loss: 0.50166\n",
      "Main effects training epoch: 39, train loss: 0.52107, val loss: 0.50101\n",
      "Main effects training epoch: 40, train loss: 0.52120, val loss: 0.50207\n",
      "Main effects training epoch: 41, train loss: 0.52137, val loss: 0.50102\n",
      "Main effects training epoch: 42, train loss: 0.52109, val loss: 0.50159\n",
      "Main effects training epoch: 43, train loss: 0.52137, val loss: 0.50169\n",
      "Main effects training epoch: 44, train loss: 0.52140, val loss: 0.50110\n",
      "Main effects training epoch: 45, train loss: 0.52299, val loss: 0.50459\n",
      "Main effects training epoch: 46, train loss: 0.52226, val loss: 0.50272\n",
      "Main effects training epoch: 47, train loss: 0.52101, val loss: 0.50147\n",
      "Main effects training epoch: 48, train loss: 0.52109, val loss: 0.50075\n",
      "Main effects training epoch: 49, train loss: 0.52069, val loss: 0.50215\n",
      "Main effects training epoch: 50, train loss: 0.52045, val loss: 0.50063\n",
      "Main effects training epoch: 51, train loss: 0.52108, val loss: 0.50126\n",
      "Main effects training epoch: 52, train loss: 0.52199, val loss: 0.50324\n",
      "Main effects training epoch: 53, train loss: 0.52131, val loss: 0.50122\n",
      "Main effects training epoch: 54, train loss: 0.52256, val loss: 0.50297\n",
      "Main effects training epoch: 55, train loss: 0.52142, val loss: 0.50258\n",
      "Main effects training epoch: 56, train loss: 0.52045, val loss: 0.50089\n",
      "Main effects training epoch: 57, train loss: 0.52043, val loss: 0.50180\n",
      "Main effects training epoch: 58, train loss: 0.52018, val loss: 0.50121\n",
      "Main effects training epoch: 59, train loss: 0.51996, val loss: 0.50068\n",
      "Main effects training epoch: 60, train loss: 0.52012, val loss: 0.50109\n",
      "Main effects training epoch: 61, train loss: 0.52012, val loss: 0.50024\n",
      "Main effects training epoch: 62, train loss: 0.52057, val loss: 0.50418\n",
      "Main effects training epoch: 63, train loss: 0.52043, val loss: 0.50038\n",
      "Main effects training epoch: 64, train loss: 0.52095, val loss: 0.50364\n",
      "Main effects training epoch: 65, train loss: 0.52038, val loss: 0.50085\n",
      "Main effects training epoch: 66, train loss: 0.52047, val loss: 0.50241\n",
      "Main effects training epoch: 67, train loss: 0.52117, val loss: 0.50295\n",
      "Main effects training epoch: 68, train loss: 0.52073, val loss: 0.50183\n",
      "Main effects training epoch: 69, train loss: 0.52087, val loss: 0.50244\n",
      "Main effects training epoch: 70, train loss: 0.52010, val loss: 0.50147\n",
      "Main effects training epoch: 71, train loss: 0.51994, val loss: 0.50147\n",
      "Main effects training epoch: 72, train loss: 0.51942, val loss: 0.50086\n",
      "Main effects training epoch: 73, train loss: 0.51949, val loss: 0.50028\n",
      "Main effects training epoch: 74, train loss: 0.52005, val loss: 0.50237\n",
      "Main effects training epoch: 75, train loss: 0.51990, val loss: 0.50184\n",
      "Main effects training epoch: 76, train loss: 0.51923, val loss: 0.50236\n",
      "Main effects training epoch: 77, train loss: 0.51910, val loss: 0.50038\n",
      "Main effects training epoch: 78, train loss: 0.51884, val loss: 0.50095\n",
      "Main effects training epoch: 79, train loss: 0.51884, val loss: 0.49983\n",
      "Main effects training epoch: 80, train loss: 0.51866, val loss: 0.50191\n",
      "Main effects training epoch: 81, train loss: 0.51939, val loss: 0.50027\n",
      "Main effects training epoch: 82, train loss: 0.51929, val loss: 0.50319\n",
      "Main effects training epoch: 83, train loss: 0.51868, val loss: 0.50126\n",
      "Main effects training epoch: 84, train loss: 0.51858, val loss: 0.50088\n",
      "Main effects training epoch: 85, train loss: 0.51835, val loss: 0.50068\n",
      "Main effects training epoch: 86, train loss: 0.51834, val loss: 0.50062\n",
      "Main effects training epoch: 87, train loss: 0.51844, val loss: 0.50224\n",
      "Main effects training epoch: 88, train loss: 0.51878, val loss: 0.50098\n",
      "Main effects training epoch: 89, train loss: 0.51903, val loss: 0.50192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 90, train loss: 0.51939, val loss: 0.50209\n",
      "Main effects training epoch: 91, train loss: 0.51881, val loss: 0.50201\n",
      "Main effects training epoch: 92, train loss: 0.51849, val loss: 0.50011\n",
      "Main effects training epoch: 93, train loss: 0.51893, val loss: 0.50406\n",
      "Main effects training epoch: 94, train loss: 0.51813, val loss: 0.49975\n",
      "Main effects training epoch: 95, train loss: 0.51804, val loss: 0.50212\n",
      "Main effects training epoch: 96, train loss: 0.51773, val loss: 0.50088\n",
      "Main effects training epoch: 97, train loss: 0.51767, val loss: 0.50090\n",
      "Main effects training epoch: 98, train loss: 0.51759, val loss: 0.50188\n",
      "Main effects training epoch: 99, train loss: 0.51749, val loss: 0.50231\n",
      "Main effects training epoch: 100, train loss: 0.51739, val loss: 0.50044\n",
      "Main effects training epoch: 101, train loss: 0.51733, val loss: 0.50052\n",
      "Main effects training epoch: 102, train loss: 0.51797, val loss: 0.50373\n",
      "Main effects training epoch: 103, train loss: 0.51813, val loss: 0.50085\n",
      "Main effects training epoch: 104, train loss: 0.51776, val loss: 0.50234\n",
      "Main effects training epoch: 105, train loss: 0.51726, val loss: 0.50227\n",
      "Main effects training epoch: 106, train loss: 0.51709, val loss: 0.50129\n",
      "Main effects training epoch: 107, train loss: 0.51781, val loss: 0.50127\n",
      "Main effects training epoch: 108, train loss: 0.51764, val loss: 0.50367\n",
      "Main effects training epoch: 109, train loss: 0.51682, val loss: 0.50096\n",
      "Main effects training epoch: 110, train loss: 0.51693, val loss: 0.50167\n",
      "Main effects training epoch: 111, train loss: 0.51671, val loss: 0.50182\n",
      "Main effects training epoch: 112, train loss: 0.51688, val loss: 0.50161\n",
      "Main effects training epoch: 113, train loss: 0.51698, val loss: 0.50128\n",
      "Main effects training epoch: 114, train loss: 0.51709, val loss: 0.50115\n",
      "Main effects training epoch: 115, train loss: 0.51748, val loss: 0.50244\n",
      "Main effects training epoch: 116, train loss: 0.51737, val loss: 0.50229\n",
      "Main effects training epoch: 117, train loss: 0.51675, val loss: 0.50288\n",
      "Main effects training epoch: 118, train loss: 0.51650, val loss: 0.50089\n",
      "Main effects training epoch: 119, train loss: 0.51679, val loss: 0.50204\n",
      "Main effects training epoch: 120, train loss: 0.51664, val loss: 0.50211\n",
      "Main effects training epoch: 121, train loss: 0.51670, val loss: 0.50227\n",
      "Main effects training epoch: 122, train loss: 0.51695, val loss: 0.50339\n",
      "Main effects training epoch: 123, train loss: 0.51659, val loss: 0.50129\n",
      "Main effects training epoch: 124, train loss: 0.51633, val loss: 0.50324\n",
      "Main effects training epoch: 125, train loss: 0.51613, val loss: 0.50129\n",
      "Main effects training epoch: 126, train loss: 0.51658, val loss: 0.50223\n",
      "Main effects training epoch: 127, train loss: 0.51636, val loss: 0.50314\n",
      "Main effects training epoch: 128, train loss: 0.51642, val loss: 0.50299\n",
      "Main effects training epoch: 129, train loss: 0.51689, val loss: 0.50193\n",
      "Main effects training epoch: 130, train loss: 0.51717, val loss: 0.50329\n",
      "Main effects training epoch: 131, train loss: 0.51619, val loss: 0.50278\n",
      "Main effects training epoch: 132, train loss: 0.51595, val loss: 0.50213\n",
      "Main effects training epoch: 133, train loss: 0.51591, val loss: 0.50190\n",
      "Main effects training epoch: 134, train loss: 0.51614, val loss: 0.50354\n",
      "Main effects training epoch: 135, train loss: 0.51593, val loss: 0.50167\n",
      "Main effects training epoch: 136, train loss: 0.51683, val loss: 0.50322\n",
      "Main effects training epoch: 137, train loss: 0.51631, val loss: 0.50292\n",
      "Main effects training epoch: 138, train loss: 0.51597, val loss: 0.50196\n",
      "Main effects training epoch: 139, train loss: 0.51594, val loss: 0.50118\n",
      "Main effects training epoch: 140, train loss: 0.51575, val loss: 0.50247\n",
      "Main effects training epoch: 141, train loss: 0.51562, val loss: 0.50226\n",
      "Main effects training epoch: 142, train loss: 0.51583, val loss: 0.50147\n",
      "Main effects training epoch: 143, train loss: 0.51602, val loss: 0.50303\n",
      "Main effects training epoch: 144, train loss: 0.51584, val loss: 0.50264\n",
      "Main effects training epoch: 145, train loss: 0.51575, val loss: 0.50308\n",
      "Main effects training epoch: 146, train loss: 0.51556, val loss: 0.50241\n",
      "Main effects training epoch: 147, train loss: 0.51596, val loss: 0.50159\n",
      "Main effects training epoch: 148, train loss: 0.51555, val loss: 0.50244\n",
      "Main effects training epoch: 149, train loss: 0.51552, val loss: 0.50203\n",
      "Main effects training epoch: 150, train loss: 0.51570, val loss: 0.50205\n",
      "Main effects training epoch: 151, train loss: 0.51556, val loss: 0.50363\n",
      "Main effects training epoch: 152, train loss: 0.51593, val loss: 0.50189\n",
      "Main effects training epoch: 153, train loss: 0.51639, val loss: 0.50286\n",
      "Main effects training epoch: 154, train loss: 0.51624, val loss: 0.50523\n",
      "Main effects training epoch: 155, train loss: 0.51611, val loss: 0.50246\n",
      "Main effects training epoch: 156, train loss: 0.51643, val loss: 0.50459\n",
      "Main effects training epoch: 157, train loss: 0.51592, val loss: 0.50249\n",
      "Main effects training epoch: 158, train loss: 0.51552, val loss: 0.50365\n",
      "Main effects training epoch: 159, train loss: 0.51556, val loss: 0.50177\n",
      "Main effects training epoch: 160, train loss: 0.51540, val loss: 0.50281\n",
      "Main effects training epoch: 161, train loss: 0.51544, val loss: 0.50300\n",
      "Main effects training epoch: 162, train loss: 0.51528, val loss: 0.50196\n",
      "Main effects training epoch: 163, train loss: 0.51547, val loss: 0.50171\n",
      "Main effects training epoch: 164, train loss: 0.51538, val loss: 0.50210\n",
      "Main effects training epoch: 165, train loss: 0.51565, val loss: 0.50469\n",
      "Main effects training epoch: 166, train loss: 0.51521, val loss: 0.50062\n",
      "Main effects training epoch: 167, train loss: 0.51535, val loss: 0.50326\n",
      "Main effects training epoch: 168, train loss: 0.51525, val loss: 0.50222\n",
      "Main effects training epoch: 169, train loss: 0.51514, val loss: 0.50331\n",
      "Main effects training epoch: 170, train loss: 0.51509, val loss: 0.50200\n",
      "Main effects training epoch: 171, train loss: 0.51490, val loss: 0.50324\n",
      "Main effects training epoch: 172, train loss: 0.51508, val loss: 0.50247\n",
      "Main effects training epoch: 173, train loss: 0.51493, val loss: 0.50182\n",
      "Main effects training epoch: 174, train loss: 0.51522, val loss: 0.50212\n",
      "Main effects training epoch: 175, train loss: 0.51508, val loss: 0.50334\n",
      "Main effects training epoch: 176, train loss: 0.51483, val loss: 0.50241\n",
      "Main effects training epoch: 177, train loss: 0.51504, val loss: 0.50039\n",
      "Main effects training epoch: 178, train loss: 0.51532, val loss: 0.50473\n",
      "Main effects training epoch: 179, train loss: 0.51527, val loss: 0.50142\n",
      "Main effects training epoch: 180, train loss: 0.51478, val loss: 0.50288\n",
      "Main effects training epoch: 181, train loss: 0.51468, val loss: 0.50177\n",
      "Main effects training epoch: 182, train loss: 0.51487, val loss: 0.50212\n",
      "Main effects training epoch: 183, train loss: 0.51473, val loss: 0.50130\n",
      "Main effects training epoch: 184, train loss: 0.51443, val loss: 0.50233\n",
      "Main effects training epoch: 185, train loss: 0.51443, val loss: 0.50209\n",
      "Main effects training epoch: 186, train loss: 0.51449, val loss: 0.50105\n",
      "Main effects training epoch: 187, train loss: 0.51500, val loss: 0.50328\n",
      "Main effects training epoch: 188, train loss: 0.51584, val loss: 0.50243\n",
      "Main effects training epoch: 189, train loss: 0.51498, val loss: 0.50282\n",
      "Main effects training epoch: 190, train loss: 0.51476, val loss: 0.50202\n",
      "Main effects training epoch: 191, train loss: 0.51499, val loss: 0.50370\n",
      "Main effects training epoch: 192, train loss: 0.51451, val loss: 0.50061\n",
      "Main effects training epoch: 193, train loss: 0.51427, val loss: 0.50163\n",
      "Main effects training epoch: 194, train loss: 0.51468, val loss: 0.50289\n",
      "Main effects training epoch: 195, train loss: 0.51485, val loss: 0.50152\n",
      "Early stop at epoch 195, with validation loss: 0.50152\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51652, val loss: 0.50334\n",
      "Main effects tuning epoch: 2, train loss: 0.51620, val loss: 0.50298\n",
      "Main effects tuning epoch: 3, train loss: 0.51591, val loss: 0.50243\n",
      "Main effects tuning epoch: 4, train loss: 0.51583, val loss: 0.50178\n",
      "Main effects tuning epoch: 5, train loss: 0.51587, val loss: 0.50310\n",
      "Main effects tuning epoch: 6, train loss: 0.51589, val loss: 0.50141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects tuning epoch: 7, train loss: 0.51573, val loss: 0.50312\n",
      "Main effects tuning epoch: 8, train loss: 0.51616, val loss: 0.50270\n",
      "Main effects tuning epoch: 9, train loss: 0.51592, val loss: 0.50204\n",
      "Main effects tuning epoch: 10, train loss: 0.51562, val loss: 0.50183\n",
      "Main effects tuning epoch: 11, train loss: 0.51574, val loss: 0.50319\n",
      "Main effects tuning epoch: 12, train loss: 0.51563, val loss: 0.50186\n",
      "Main effects tuning epoch: 13, train loss: 0.51576, val loss: 0.50178\n",
      "Main effects tuning epoch: 14, train loss: 0.51619, val loss: 0.50417\n",
      "Main effects tuning epoch: 15, train loss: 0.51630, val loss: 0.50174\n",
      "Main effects tuning epoch: 16, train loss: 0.51617, val loss: 0.50504\n",
      "Main effects tuning epoch: 17, train loss: 0.51578, val loss: 0.50114\n",
      "Main effects tuning epoch: 18, train loss: 0.51571, val loss: 0.50339\n",
      "Main effects tuning epoch: 19, train loss: 0.51541, val loss: 0.50297\n",
      "Main effects tuning epoch: 20, train loss: 0.51567, val loss: 0.50233\n",
      "Main effects tuning epoch: 21, train loss: 0.51545, val loss: 0.50194\n",
      "Main effects tuning epoch: 22, train loss: 0.51550, val loss: 0.50202\n",
      "Main effects tuning epoch: 23, train loss: 0.51527, val loss: 0.50183\n",
      "Main effects tuning epoch: 24, train loss: 0.51583, val loss: 0.50227\n",
      "Main effects tuning epoch: 25, train loss: 0.51612, val loss: 0.50429\n",
      "Main effects tuning epoch: 26, train loss: 0.51526, val loss: 0.50222\n",
      "Main effects tuning epoch: 27, train loss: 0.51592, val loss: 0.50259\n",
      "Main effects tuning epoch: 28, train loss: 0.51541, val loss: 0.50208\n",
      "Main effects tuning epoch: 29, train loss: 0.51598, val loss: 0.50367\n",
      "Main effects tuning epoch: 30, train loss: 0.51600, val loss: 0.50229\n",
      "Main effects tuning epoch: 31, train loss: 0.51550, val loss: 0.50323\n",
      "Main effects tuning epoch: 32, train loss: 0.51546, val loss: 0.50104\n",
      "Main effects tuning epoch: 33, train loss: 0.51535, val loss: 0.50236\n",
      "Main effects tuning epoch: 34, train loss: 0.51519, val loss: 0.50118\n",
      "Main effects tuning epoch: 35, train loss: 0.51491, val loss: 0.50257\n",
      "Main effects tuning epoch: 36, train loss: 0.51491, val loss: 0.50182\n",
      "Main effects tuning epoch: 37, train loss: 0.51516, val loss: 0.50071\n",
      "Main effects tuning epoch: 38, train loss: 0.51511, val loss: 0.50194\n",
      "Main effects tuning epoch: 39, train loss: 0.51511, val loss: 0.50230\n",
      "Main effects tuning epoch: 40, train loss: 0.51584, val loss: 0.50414\n",
      "Main effects tuning epoch: 41, train loss: 0.51531, val loss: 0.50065\n",
      "Main effects tuning epoch: 42, train loss: 0.51515, val loss: 0.50337\n",
      "Main effects tuning epoch: 43, train loss: 0.51471, val loss: 0.50169\n",
      "Main effects tuning epoch: 44, train loss: 0.51484, val loss: 0.50097\n",
      "Main effects tuning epoch: 45, train loss: 0.51488, val loss: 0.50235\n",
      "Main effects tuning epoch: 46, train loss: 0.51479, val loss: 0.50102\n",
      "Main effects tuning epoch: 47, train loss: 0.51487, val loss: 0.50196\n",
      "Main effects tuning epoch: 48, train loss: 0.51463, val loss: 0.50183\n",
      "Main effects tuning epoch: 49, train loss: 0.51469, val loss: 0.50103\n",
      "Main effects tuning epoch: 50, train loss: 0.51472, val loss: 0.50134\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.49630, val loss: 0.48952\n",
      "Interaction training epoch: 2, train loss: 0.33769, val loss: 0.34284\n",
      "Interaction training epoch: 3, train loss: 0.35128, val loss: 0.36496\n",
      "Interaction training epoch: 4, train loss: 0.31659, val loss: 0.31842\n",
      "Interaction training epoch: 5, train loss: 0.30166, val loss: 0.30688\n",
      "Interaction training epoch: 6, train loss: 0.29518, val loss: 0.29846\n",
      "Interaction training epoch: 7, train loss: 0.28798, val loss: 0.28992\n",
      "Interaction training epoch: 8, train loss: 0.28937, val loss: 0.29121\n",
      "Interaction training epoch: 9, train loss: 0.29272, val loss: 0.29817\n",
      "Interaction training epoch: 10, train loss: 0.29134, val loss: 0.29788\n",
      "Interaction training epoch: 11, train loss: 0.28068, val loss: 0.28755\n",
      "Interaction training epoch: 12, train loss: 0.28154, val loss: 0.28695\n",
      "Interaction training epoch: 13, train loss: 0.27857, val loss: 0.28611\n",
      "Interaction training epoch: 14, train loss: 0.28018, val loss: 0.28605\n",
      "Interaction training epoch: 15, train loss: 0.27761, val loss: 0.28852\n",
      "Interaction training epoch: 16, train loss: 0.27799, val loss: 0.28495\n",
      "Interaction training epoch: 17, train loss: 0.27600, val loss: 0.28684\n",
      "Interaction training epoch: 18, train loss: 0.27797, val loss: 0.28446\n",
      "Interaction training epoch: 19, train loss: 0.27427, val loss: 0.28254\n",
      "Interaction training epoch: 20, train loss: 0.27444, val loss: 0.28269\n",
      "Interaction training epoch: 21, train loss: 0.27197, val loss: 0.28017\n",
      "Interaction training epoch: 22, train loss: 0.28111, val loss: 0.29098\n",
      "Interaction training epoch: 23, train loss: 0.27471, val loss: 0.28461\n",
      "Interaction training epoch: 24, train loss: 0.27383, val loss: 0.28217\n",
      "Interaction training epoch: 25, train loss: 0.27561, val loss: 0.28093\n",
      "Interaction training epoch: 26, train loss: 0.27286, val loss: 0.28056\n",
      "Interaction training epoch: 27, train loss: 0.27501, val loss: 0.28302\n",
      "Interaction training epoch: 28, train loss: 0.27465, val loss: 0.28525\n",
      "Interaction training epoch: 29, train loss: 0.27005, val loss: 0.27613\n",
      "Interaction training epoch: 30, train loss: 0.27126, val loss: 0.27893\n",
      "Interaction training epoch: 31, train loss: 0.27173, val loss: 0.28095\n",
      "Interaction training epoch: 32, train loss: 0.27099, val loss: 0.28101\n",
      "Interaction training epoch: 33, train loss: 0.26687, val loss: 0.27472\n",
      "Interaction training epoch: 34, train loss: 0.26881, val loss: 0.27762\n",
      "Interaction training epoch: 35, train loss: 0.26911, val loss: 0.28276\n",
      "Interaction training epoch: 36, train loss: 0.27233, val loss: 0.28279\n",
      "Interaction training epoch: 37, train loss: 0.27192, val loss: 0.28092\n",
      "Interaction training epoch: 38, train loss: 0.26677, val loss: 0.27617\n",
      "Interaction training epoch: 39, train loss: 0.26946, val loss: 0.27759\n",
      "Interaction training epoch: 40, train loss: 0.26705, val loss: 0.28085\n",
      "Interaction training epoch: 41, train loss: 0.26524, val loss: 0.27444\n",
      "Interaction training epoch: 42, train loss: 0.27260, val loss: 0.28364\n",
      "Interaction training epoch: 43, train loss: 0.26587, val loss: 0.27973\n",
      "Interaction training epoch: 44, train loss: 0.26654, val loss: 0.27690\n",
      "Interaction training epoch: 45, train loss: 0.26566, val loss: 0.28007\n",
      "Interaction training epoch: 46, train loss: 0.26457, val loss: 0.27688\n",
      "Interaction training epoch: 47, train loss: 0.26778, val loss: 0.28093\n",
      "Interaction training epoch: 48, train loss: 0.26713, val loss: 0.28041\n",
      "Interaction training epoch: 49, train loss: 0.26385, val loss: 0.27635\n",
      "Interaction training epoch: 50, train loss: 0.26436, val loss: 0.27912\n",
      "Interaction training epoch: 51, train loss: 0.26506, val loss: 0.27701\n",
      "Interaction training epoch: 52, train loss: 0.26332, val loss: 0.27839\n",
      "Interaction training epoch: 53, train loss: 0.26575, val loss: 0.27820\n",
      "Interaction training epoch: 54, train loss: 0.26444, val loss: 0.27399\n",
      "Interaction training epoch: 55, train loss: 0.26446, val loss: 0.28050\n",
      "Interaction training epoch: 56, train loss: 0.26212, val loss: 0.27389\n",
      "Interaction training epoch: 57, train loss: 0.26537, val loss: 0.28100\n",
      "Interaction training epoch: 58, train loss: 0.26231, val loss: 0.27333\n",
      "Interaction training epoch: 59, train loss: 0.26531, val loss: 0.28369\n",
      "Interaction training epoch: 60, train loss: 0.26223, val loss: 0.27409\n",
      "Interaction training epoch: 61, train loss: 0.26070, val loss: 0.27585\n",
      "Interaction training epoch: 62, train loss: 0.26415, val loss: 0.28179\n",
      "Interaction training epoch: 63, train loss: 0.26012, val loss: 0.27531\n",
      "Interaction training epoch: 64, train loss: 0.26214, val loss: 0.27596\n",
      "Interaction training epoch: 65, train loss: 0.26123, val loss: 0.27634\n",
      "Interaction training epoch: 66, train loss: 0.26041, val loss: 0.27562\n",
      "Interaction training epoch: 67, train loss: 0.26073, val loss: 0.27566\n",
      "Interaction training epoch: 68, train loss: 0.26038, val loss: 0.27691\n",
      "Interaction training epoch: 69, train loss: 0.25848, val loss: 0.27646\n",
      "Interaction training epoch: 70, train loss: 0.26296, val loss: 0.27763\n",
      "Interaction training epoch: 71, train loss: 0.26102, val loss: 0.27864\n",
      "Interaction training epoch: 72, train loss: 0.26107, val loss: 0.27708\n",
      "Interaction training epoch: 73, train loss: 0.26414, val loss: 0.27619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 74, train loss: 0.25864, val loss: 0.27725\n",
      "Interaction training epoch: 75, train loss: 0.25926, val loss: 0.27525\n",
      "Interaction training epoch: 76, train loss: 0.25686, val loss: 0.27432\n",
      "Interaction training epoch: 77, train loss: 0.25954, val loss: 0.27993\n",
      "Interaction training epoch: 78, train loss: 0.26144, val loss: 0.27545\n",
      "Interaction training epoch: 79, train loss: 0.25751, val loss: 0.27693\n",
      "Interaction training epoch: 80, train loss: 0.25861, val loss: 0.27448\n",
      "Interaction training epoch: 81, train loss: 0.25719, val loss: 0.27631\n",
      "Interaction training epoch: 82, train loss: 0.26048, val loss: 0.27937\n",
      "Interaction training epoch: 83, train loss: 0.25757, val loss: 0.27424\n",
      "Interaction training epoch: 84, train loss: 0.25932, val loss: 0.27815\n",
      "Interaction training epoch: 85, train loss: 0.26140, val loss: 0.27858\n",
      "Interaction training epoch: 86, train loss: 0.25378, val loss: 0.27194\n",
      "Interaction training epoch: 87, train loss: 0.25811, val loss: 0.27779\n",
      "Interaction training epoch: 88, train loss: 0.25630, val loss: 0.27527\n",
      "Interaction training epoch: 89, train loss: 0.25527, val loss: 0.27723\n",
      "Interaction training epoch: 90, train loss: 0.25759, val loss: 0.27676\n",
      "Interaction training epoch: 91, train loss: 0.25457, val loss: 0.27467\n",
      "Interaction training epoch: 92, train loss: 0.25542, val loss: 0.27403\n",
      "Interaction training epoch: 93, train loss: 0.25535, val loss: 0.27589\n",
      "Interaction training epoch: 94, train loss: 0.25407, val loss: 0.27538\n",
      "Interaction training epoch: 95, train loss: 0.25767, val loss: 0.27769\n",
      "Interaction training epoch: 96, train loss: 0.25220, val loss: 0.27444\n",
      "Interaction training epoch: 97, train loss: 0.25584, val loss: 0.27263\n",
      "Interaction training epoch: 98, train loss: 0.25244, val loss: 0.27405\n",
      "Interaction training epoch: 99, train loss: 0.25535, val loss: 0.27697\n",
      "Interaction training epoch: 100, train loss: 0.25545, val loss: 0.27934\n",
      "Interaction training epoch: 101, train loss: 0.25633, val loss: 0.27754\n",
      "Interaction training epoch: 102, train loss: 0.25267, val loss: 0.27028\n",
      "Interaction training epoch: 103, train loss: 0.25493, val loss: 0.27683\n",
      "Interaction training epoch: 104, train loss: 0.25262, val loss: 0.27466\n",
      "Interaction training epoch: 105, train loss: 0.25030, val loss: 0.27110\n",
      "Interaction training epoch: 106, train loss: 0.25358, val loss: 0.27660\n",
      "Interaction training epoch: 107, train loss: 0.25603, val loss: 0.27754\n",
      "Interaction training epoch: 108, train loss: 0.24978, val loss: 0.27290\n",
      "Interaction training epoch: 109, train loss: 0.25282, val loss: 0.27685\n",
      "Interaction training epoch: 110, train loss: 0.25069, val loss: 0.27038\n",
      "Interaction training epoch: 111, train loss: 0.25313, val loss: 0.27830\n",
      "Interaction training epoch: 112, train loss: 0.25039, val loss: 0.27405\n",
      "Interaction training epoch: 113, train loss: 0.25058, val loss: 0.27310\n",
      "Interaction training epoch: 114, train loss: 0.25279, val loss: 0.27999\n",
      "Interaction training epoch: 115, train loss: 0.24922, val loss: 0.27298\n",
      "Interaction training epoch: 116, train loss: 0.25238, val loss: 0.28017\n",
      "Interaction training epoch: 117, train loss: 0.24869, val loss: 0.27323\n",
      "Interaction training epoch: 118, train loss: 0.24943, val loss: 0.27615\n",
      "Interaction training epoch: 119, train loss: 0.24886, val loss: 0.27248\n",
      "Interaction training epoch: 120, train loss: 0.24706, val loss: 0.27312\n",
      "Interaction training epoch: 121, train loss: 0.24960, val loss: 0.27178\n",
      "Interaction training epoch: 122, train loss: 0.24969, val loss: 0.27520\n",
      "Interaction training epoch: 123, train loss: 0.24595, val loss: 0.27154\n",
      "Interaction training epoch: 124, train loss: 0.24900, val loss: 0.27416\n",
      "Interaction training epoch: 125, train loss: 0.24952, val loss: 0.27728\n",
      "Interaction training epoch: 126, train loss: 0.24514, val loss: 0.27059\n",
      "Interaction training epoch: 127, train loss: 0.24724, val loss: 0.27257\n",
      "Interaction training epoch: 128, train loss: 0.24934, val loss: 0.27673\n",
      "Interaction training epoch: 129, train loss: 0.25057, val loss: 0.27925\n",
      "Interaction training epoch: 130, train loss: 0.24874, val loss: 0.27362\n",
      "Interaction training epoch: 131, train loss: 0.24820, val loss: 0.27789\n",
      "Interaction training epoch: 132, train loss: 0.24936, val loss: 0.27270\n",
      "Interaction training epoch: 133, train loss: 0.24469, val loss: 0.27186\n",
      "Interaction training epoch: 134, train loss: 0.24571, val loss: 0.27370\n",
      "Interaction training epoch: 135, train loss: 0.24431, val loss: 0.27322\n",
      "Interaction training epoch: 136, train loss: 0.24633, val loss: 0.27261\n",
      "Interaction training epoch: 137, train loss: 0.24825, val loss: 0.27969\n",
      "Interaction training epoch: 138, train loss: 0.24335, val loss: 0.27059\n",
      "Interaction training epoch: 139, train loss: 0.24851, val loss: 0.27798\n",
      "Interaction training epoch: 140, train loss: 0.24583, val loss: 0.27031\n",
      "Interaction training epoch: 141, train loss: 0.24941, val loss: 0.28354\n",
      "Interaction training epoch: 142, train loss: 0.24411, val loss: 0.27392\n",
      "Interaction training epoch: 143, train loss: 0.24626, val loss: 0.27780\n",
      "Interaction training epoch: 144, train loss: 0.24308, val loss: 0.27648\n",
      "Interaction training epoch: 145, train loss: 0.24347, val loss: 0.27154\n",
      "Interaction training epoch: 146, train loss: 0.24461, val loss: 0.27730\n",
      "Interaction training epoch: 147, train loss: 0.24531, val loss: 0.27648\n",
      "Interaction training epoch: 148, train loss: 0.24362, val loss: 0.27834\n",
      "Interaction training epoch: 149, train loss: 0.24601, val loss: 0.27461\n",
      "Interaction training epoch: 150, train loss: 0.24428, val loss: 0.27347\n",
      "Interaction training epoch: 151, train loss: 0.24732, val loss: 0.28053\n",
      "Interaction training epoch: 152, train loss: 0.24191, val loss: 0.27360\n",
      "Interaction training epoch: 153, train loss: 0.24221, val loss: 0.27440\n",
      "Interaction training epoch: 154, train loss: 0.24280, val loss: 0.27499\n",
      "Interaction training epoch: 155, train loss: 0.24201, val loss: 0.27586\n",
      "Interaction training epoch: 156, train loss: 0.24477, val loss: 0.27752\n",
      "Interaction training epoch: 157, train loss: 0.24507, val loss: 0.27833\n",
      "Interaction training epoch: 158, train loss: 0.24068, val loss: 0.27333\n",
      "Interaction training epoch: 159, train loss: 0.24391, val loss: 0.27683\n",
      "Interaction training epoch: 160, train loss: 0.24175, val loss: 0.27674\n",
      "Interaction training epoch: 161, train loss: 0.23881, val loss: 0.26956\n",
      "Interaction training epoch: 162, train loss: 0.24359, val loss: 0.27676\n",
      "Interaction training epoch: 163, train loss: 0.23898, val loss: 0.27401\n",
      "Interaction training epoch: 164, train loss: 0.23981, val loss: 0.27359\n",
      "Interaction training epoch: 165, train loss: 0.24112, val loss: 0.27625\n",
      "Interaction training epoch: 166, train loss: 0.23941, val loss: 0.27442\n",
      "Interaction training epoch: 167, train loss: 0.24063, val loss: 0.27651\n",
      "Interaction training epoch: 168, train loss: 0.24010, val loss: 0.27885\n",
      "Interaction training epoch: 169, train loss: 0.24168, val loss: 0.27591\n",
      "Interaction training epoch: 170, train loss: 0.24327, val loss: 0.28124\n",
      "Interaction training epoch: 171, train loss: 0.23941, val loss: 0.27438\n",
      "Interaction training epoch: 172, train loss: 0.24055, val loss: 0.27595\n",
      "Interaction training epoch: 173, train loss: 0.23916, val loss: 0.27534\n",
      "Interaction training epoch: 174, train loss: 0.24062, val loss: 0.28139\n",
      "Interaction training epoch: 175, train loss: 0.23799, val loss: 0.27390\n",
      "Interaction training epoch: 176, train loss: 0.23947, val loss: 0.27684\n",
      "Interaction training epoch: 177, train loss: 0.23886, val loss: 0.27735\n",
      "Interaction training epoch: 178, train loss: 0.23895, val loss: 0.27472\n",
      "Interaction training epoch: 179, train loss: 0.23942, val loss: 0.27651\n",
      "Interaction training epoch: 180, train loss: 0.24049, val loss: 0.28036\n",
      "Interaction training epoch: 181, train loss: 0.23897, val loss: 0.27791\n",
      "Interaction training epoch: 182, train loss: 0.23646, val loss: 0.27363\n",
      "Interaction training epoch: 183, train loss: 0.23810, val loss: 0.27910\n",
      "Interaction training epoch: 184, train loss: 0.23821, val loss: 0.27539\n",
      "Interaction training epoch: 185, train loss: 0.23642, val loss: 0.27411\n",
      "Interaction training epoch: 186, train loss: 0.23809, val loss: 0.27799\n",
      "Interaction training epoch: 187, train loss: 0.23796, val loss: 0.27475\n",
      "Interaction training epoch: 188, train loss: 0.23539, val loss: 0.27566\n",
      "Interaction training epoch: 189, train loss: 0.23735, val loss: 0.27996\n",
      "Interaction training epoch: 190, train loss: 0.23828, val loss: 0.27894\n",
      "Interaction training epoch: 191, train loss: 0.23465, val loss: 0.27510\n",
      "Interaction training epoch: 192, train loss: 0.23838, val loss: 0.27980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 193, train loss: 0.23741, val loss: 0.27565\n",
      "Interaction training epoch: 194, train loss: 0.23572, val loss: 0.27452\n",
      "Interaction training epoch: 195, train loss: 0.23802, val loss: 0.28027\n",
      "Interaction training epoch: 196, train loss: 0.23539, val loss: 0.27689\n",
      "Interaction training epoch: 197, train loss: 0.23723, val loss: 0.27746\n",
      "Interaction training epoch: 198, train loss: 0.23395, val loss: 0.27462\n",
      "Interaction training epoch: 199, train loss: 0.23661, val loss: 0.28002\n",
      "Interaction training epoch: 200, train loss: 0.23513, val loss: 0.27434\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########3 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.24968, val loss: 0.27903\n",
      "Interaction tuning epoch: 2, train loss: 0.24776, val loss: 0.28053\n",
      "Interaction tuning epoch: 3, train loss: 0.24547, val loss: 0.27095\n",
      "Interaction tuning epoch: 4, train loss: 0.24990, val loss: 0.28140\n",
      "Interaction tuning epoch: 5, train loss: 0.24715, val loss: 0.27506\n",
      "Interaction tuning epoch: 6, train loss: 0.24635, val loss: 0.27332\n",
      "Interaction tuning epoch: 7, train loss: 0.24591, val loss: 0.27750\n",
      "Interaction tuning epoch: 8, train loss: 0.24374, val loss: 0.27154\n",
      "Interaction tuning epoch: 9, train loss: 0.24867, val loss: 0.27508\n",
      "Interaction tuning epoch: 10, train loss: 0.24609, val loss: 0.27569\n",
      "Interaction tuning epoch: 11, train loss: 0.24703, val loss: 0.27608\n",
      "Interaction tuning epoch: 12, train loss: 0.24513, val loss: 0.27373\n",
      "Interaction tuning epoch: 13, train loss: 0.24529, val loss: 0.27530\n",
      "Interaction tuning epoch: 14, train loss: 0.24515, val loss: 0.27229\n",
      "Interaction tuning epoch: 15, train loss: 0.24666, val loss: 0.27752\n",
      "Interaction tuning epoch: 16, train loss: 0.24396, val loss: 0.27377\n",
      "Interaction tuning epoch: 17, train loss: 0.24588, val loss: 0.27324\n",
      "Interaction tuning epoch: 18, train loss: 0.24525, val loss: 0.27665\n",
      "Interaction tuning epoch: 19, train loss: 0.24285, val loss: 0.27234\n",
      "Interaction tuning epoch: 20, train loss: 0.24430, val loss: 0.27507\n",
      "Interaction tuning epoch: 21, train loss: 0.24167, val loss: 0.27076\n",
      "Interaction tuning epoch: 22, train loss: 0.24412, val loss: 0.27322\n",
      "Interaction tuning epoch: 23, train loss: 0.24488, val loss: 0.27656\n",
      "Interaction tuning epoch: 24, train loss: 0.24215, val loss: 0.27113\n",
      "Interaction tuning epoch: 25, train loss: 0.24397, val loss: 0.27656\n",
      "Interaction tuning epoch: 26, train loss: 0.24370, val loss: 0.27526\n",
      "Interaction tuning epoch: 27, train loss: 0.24330, val loss: 0.27308\n",
      "Interaction tuning epoch: 28, train loss: 0.24366, val loss: 0.27189\n",
      "Interaction tuning epoch: 29, train loss: 0.24133, val loss: 0.27163\n",
      "Interaction tuning epoch: 30, train loss: 0.24330, val loss: 0.27379\n",
      "Interaction tuning epoch: 31, train loss: 0.24268, val loss: 0.27507\n",
      "Interaction tuning epoch: 32, train loss: 0.24475, val loss: 0.27470\n",
      "Interaction tuning epoch: 33, train loss: 0.24434, val loss: 0.27429\n",
      "Interaction tuning epoch: 34, train loss: 0.24265, val loss: 0.27108\n",
      "Interaction tuning epoch: 35, train loss: 0.24270, val loss: 0.27413\n",
      "Interaction tuning epoch: 36, train loss: 0.24404, val loss: 0.27374\n",
      "Interaction tuning epoch: 37, train loss: 0.24276, val loss: 0.27324\n",
      "Interaction tuning epoch: 38, train loss: 0.24406, val loss: 0.27388\n",
      "Interaction tuning epoch: 39, train loss: 0.24261, val loss: 0.27577\n",
      "Interaction tuning epoch: 40, train loss: 0.24088, val loss: 0.27160\n",
      "Interaction tuning epoch: 41, train loss: 0.24276, val loss: 0.27220\n",
      "Interaction tuning epoch: 42, train loss: 0.24198, val loss: 0.27404\n",
      "Interaction tuning epoch: 43, train loss: 0.24426, val loss: 0.27793\n",
      "Interaction tuning epoch: 44, train loss: 0.24072, val loss: 0.27060\n",
      "Interaction tuning epoch: 45, train loss: 0.24021, val loss: 0.27217\n",
      "Interaction tuning epoch: 46, train loss: 0.24042, val loss: 0.27170\n",
      "Interaction tuning epoch: 47, train loss: 0.24126, val loss: 0.27376\n",
      "Interaction tuning epoch: 48, train loss: 0.24628, val loss: 0.27823\n",
      "Interaction tuning epoch: 49, train loss: 0.24267, val loss: 0.26952\n",
      "Interaction tuning epoch: 50, train loss: 0.24486, val loss: 0.27854\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 35.5438175201416\n",
      "After the gam stage, training error is 0.24486 , validation error is 0.27854\n",
      "missing value counts: 99148\n",
      "[SoftImpute] Max Singular Value of X_init = 3.614024\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.210263 validation BCE=0.301627,rank=5\n",
      "[SoftImpute] Iter 2: observed BCE=0.206446 validation BCE=0.302406,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.204381 validation BCE=0.310570,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.202812 validation BCE=0.300020,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.201149 validation BCE=0.298051,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.200357 validation BCE=0.286749,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.199236 validation BCE=0.286261,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.198981 validation BCE=0.296666,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.198640 validation BCE=0.285229,rank=5\n",
      "[SoftImpute] Iter 10: observed BCE=0.198339 validation BCE=0.285604,rank=5\n",
      "[SoftImpute] Iter 11: observed BCE=0.197999 validation BCE=0.289727,rank=5\n",
      "[SoftImpute] Iter 12: observed BCE=0.197582 validation BCE=0.296787,rank=5\n",
      "[SoftImpute] Stopped after iteration 12 for lambda=0.072280\n",
      "final num of user group: 25\n",
      "final num of item group: 36\n",
      "change mode state : True\n",
      "time cost: 4.960755825042725\n",
      "After the matrix factor stage, training error is 0.19758, validation error is 0.29679\n",
      "1\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68269, val loss: 0.68055\n",
      "Main effects training epoch: 2, train loss: 0.67712, val loss: 0.67262\n",
      "Main effects training epoch: 3, train loss: 0.67284, val loss: 0.67007\n",
      "Main effects training epoch: 4, train loss: 0.66727, val loss: 0.66702\n",
      "Main effects training epoch: 5, train loss: 0.65915, val loss: 0.66170\n",
      "Main effects training epoch: 6, train loss: 0.64370, val loss: 0.64940\n",
      "Main effects training epoch: 7, train loss: 0.61314, val loss: 0.62222\n",
      "Main effects training epoch: 8, train loss: 0.56548, val loss: 0.57830\n",
      "Main effects training epoch: 9, train loss: 0.52934, val loss: 0.54217\n",
      "Main effects training epoch: 10, train loss: 0.52345, val loss: 0.54130\n",
      "Main effects training epoch: 11, train loss: 0.52295, val loss: 0.54310\n",
      "Main effects training epoch: 12, train loss: 0.52262, val loss: 0.53352\n",
      "Main effects training epoch: 13, train loss: 0.52089, val loss: 0.54216\n",
      "Main effects training epoch: 14, train loss: 0.51944, val loss: 0.53292\n",
      "Main effects training epoch: 15, train loss: 0.51857, val loss: 0.53746\n",
      "Main effects training epoch: 16, train loss: 0.52060, val loss: 0.53102\n",
      "Main effects training epoch: 17, train loss: 0.51943, val loss: 0.54148\n",
      "Main effects training epoch: 18, train loss: 0.51787, val loss: 0.53239\n",
      "Main effects training epoch: 19, train loss: 0.51803, val loss: 0.53805\n",
      "Main effects training epoch: 20, train loss: 0.51823, val loss: 0.53232\n",
      "Main effects training epoch: 21, train loss: 0.51821, val loss: 0.53794\n",
      "Main effects training epoch: 22, train loss: 0.51779, val loss: 0.53479\n",
      "Main effects training epoch: 23, train loss: 0.51733, val loss: 0.53446\n",
      "Main effects training epoch: 24, train loss: 0.51735, val loss: 0.53362\n",
      "Main effects training epoch: 25, train loss: 0.51737, val loss: 0.53335\n",
      "Main effects training epoch: 26, train loss: 0.51776, val loss: 0.53555\n",
      "Main effects training epoch: 27, train loss: 0.51820, val loss: 0.53862\n",
      "Main effects training epoch: 28, train loss: 0.51807, val loss: 0.53280\n",
      "Main effects training epoch: 29, train loss: 0.51798, val loss: 0.53747\n",
      "Main effects training epoch: 30, train loss: 0.51722, val loss: 0.53333\n",
      "Main effects training epoch: 31, train loss: 0.51683, val loss: 0.53484\n",
      "Main effects training epoch: 32, train loss: 0.51694, val loss: 0.53350\n",
      "Main effects training epoch: 33, train loss: 0.51728, val loss: 0.53758\n",
      "Main effects training epoch: 34, train loss: 0.51662, val loss: 0.53353\n",
      "Main effects training epoch: 35, train loss: 0.51677, val loss: 0.53271\n",
      "Main effects training epoch: 36, train loss: 0.51704, val loss: 0.53735\n",
      "Main effects training epoch: 37, train loss: 0.51677, val loss: 0.53342\n",
      "Main effects training epoch: 38, train loss: 0.51696, val loss: 0.53771\n",
      "Main effects training epoch: 39, train loss: 0.51703, val loss: 0.53168\n",
      "Main effects training epoch: 40, train loss: 0.51640, val loss: 0.53564\n",
      "Main effects training epoch: 41, train loss: 0.51617, val loss: 0.53346\n",
      "Main effects training epoch: 42, train loss: 0.51616, val loss: 0.53318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 43, train loss: 0.51650, val loss: 0.53619\n",
      "Main effects training epoch: 44, train loss: 0.51659, val loss: 0.53210\n",
      "Main effects training epoch: 45, train loss: 0.51605, val loss: 0.53490\n",
      "Main effects training epoch: 46, train loss: 0.51604, val loss: 0.53395\n",
      "Main effects training epoch: 47, train loss: 0.51605, val loss: 0.53482\n",
      "Main effects training epoch: 48, train loss: 0.51614, val loss: 0.53439\n",
      "Main effects training epoch: 49, train loss: 0.51621, val loss: 0.53173\n",
      "Main effects training epoch: 50, train loss: 0.51621, val loss: 0.53585\n",
      "Main effects training epoch: 51, train loss: 0.51627, val loss: 0.53249\n",
      "Main effects training epoch: 52, train loss: 0.51586, val loss: 0.53428\n",
      "Main effects training epoch: 53, train loss: 0.51580, val loss: 0.53445\n",
      "Main effects training epoch: 54, train loss: 0.51630, val loss: 0.53459\n",
      "Main effects training epoch: 55, train loss: 0.51612, val loss: 0.53399\n",
      "Main effects training epoch: 56, train loss: 0.51565, val loss: 0.53444\n",
      "Main effects training epoch: 57, train loss: 0.51559, val loss: 0.53434\n",
      "Main effects training epoch: 58, train loss: 0.51549, val loss: 0.53296\n",
      "Main effects training epoch: 59, train loss: 0.51543, val loss: 0.53359\n",
      "Main effects training epoch: 60, train loss: 0.51531, val loss: 0.53362\n",
      "Main effects training epoch: 61, train loss: 0.51564, val loss: 0.53521\n",
      "Main effects training epoch: 62, train loss: 0.51550, val loss: 0.53380\n",
      "Main effects training epoch: 63, train loss: 0.51553, val loss: 0.53393\n",
      "Main effects training epoch: 64, train loss: 0.51523, val loss: 0.53316\n",
      "Main effects training epoch: 65, train loss: 0.51522, val loss: 0.53186\n",
      "Main effects training epoch: 66, train loss: 0.51531, val loss: 0.53406\n",
      "Main effects training epoch: 67, train loss: 0.51558, val loss: 0.53157\n",
      "Main effects training epoch: 68, train loss: 0.51538, val loss: 0.53621\n",
      "Main effects training epoch: 69, train loss: 0.51519, val loss: 0.53261\n",
      "Main effects training epoch: 70, train loss: 0.51499, val loss: 0.53338\n",
      "Main effects training epoch: 71, train loss: 0.51508, val loss: 0.53247\n",
      "Main effects training epoch: 72, train loss: 0.51503, val loss: 0.53409\n",
      "Main effects training epoch: 73, train loss: 0.51487, val loss: 0.53396\n",
      "Main effects training epoch: 74, train loss: 0.51497, val loss: 0.53256\n",
      "Main effects training epoch: 75, train loss: 0.51476, val loss: 0.53458\n",
      "Main effects training epoch: 76, train loss: 0.51467, val loss: 0.53359\n",
      "Main effects training epoch: 77, train loss: 0.51477, val loss: 0.53283\n",
      "Main effects training epoch: 78, train loss: 0.51461, val loss: 0.53353\n",
      "Main effects training epoch: 79, train loss: 0.51492, val loss: 0.53114\n",
      "Main effects training epoch: 80, train loss: 0.51486, val loss: 0.53574\n",
      "Main effects training epoch: 81, train loss: 0.51521, val loss: 0.53052\n",
      "Main effects training epoch: 82, train loss: 0.51487, val loss: 0.53583\n",
      "Main effects training epoch: 83, train loss: 0.51478, val loss: 0.53112\n",
      "Main effects training epoch: 84, train loss: 0.51441, val loss: 0.53383\n",
      "Main effects training epoch: 85, train loss: 0.51470, val loss: 0.53524\n",
      "Main effects training epoch: 86, train loss: 0.51462, val loss: 0.53061\n",
      "Main effects training epoch: 87, train loss: 0.51470, val loss: 0.53558\n",
      "Main effects training epoch: 88, train loss: 0.51434, val loss: 0.53345\n",
      "Main effects training epoch: 89, train loss: 0.51474, val loss: 0.53211\n",
      "Main effects training epoch: 90, train loss: 0.51435, val loss: 0.53462\n",
      "Main effects training epoch: 91, train loss: 0.51429, val loss: 0.53100\n",
      "Main effects training epoch: 92, train loss: 0.51417, val loss: 0.53130\n",
      "Main effects training epoch: 93, train loss: 0.51398, val loss: 0.53148\n",
      "Main effects training epoch: 94, train loss: 0.51378, val loss: 0.53259\n",
      "Main effects training epoch: 95, train loss: 0.51426, val loss: 0.53376\n",
      "Main effects training epoch: 96, train loss: 0.51416, val loss: 0.53162\n",
      "Main effects training epoch: 97, train loss: 0.51390, val loss: 0.53453\n",
      "Main effects training epoch: 98, train loss: 0.51367, val loss: 0.53048\n",
      "Main effects training epoch: 99, train loss: 0.51364, val loss: 0.53155\n",
      "Main effects training epoch: 100, train loss: 0.51356, val loss: 0.53259\n",
      "Main effects training epoch: 101, train loss: 0.51351, val loss: 0.53086\n",
      "Main effects training epoch: 102, train loss: 0.51334, val loss: 0.53222\n",
      "Main effects training epoch: 103, train loss: 0.51323, val loss: 0.53200\n",
      "Main effects training epoch: 104, train loss: 0.51322, val loss: 0.53193\n",
      "Main effects training epoch: 105, train loss: 0.51337, val loss: 0.53234\n",
      "Main effects training epoch: 106, train loss: 0.51342, val loss: 0.52960\n",
      "Main effects training epoch: 107, train loss: 0.51381, val loss: 0.53445\n",
      "Main effects training epoch: 108, train loss: 0.51346, val loss: 0.53081\n",
      "Main effects training epoch: 109, train loss: 0.51312, val loss: 0.53170\n",
      "Main effects training epoch: 110, train loss: 0.51316, val loss: 0.53067\n",
      "Main effects training epoch: 111, train loss: 0.51287, val loss: 0.53258\n",
      "Main effects training epoch: 112, train loss: 0.51310, val loss: 0.53004\n",
      "Main effects training epoch: 113, train loss: 0.51265, val loss: 0.53146\n",
      "Main effects training epoch: 114, train loss: 0.51292, val loss: 0.53033\n",
      "Main effects training epoch: 115, train loss: 0.51309, val loss: 0.53084\n",
      "Main effects training epoch: 116, train loss: 0.51251, val loss: 0.53254\n",
      "Main effects training epoch: 117, train loss: 0.51264, val loss: 0.52905\n",
      "Main effects training epoch: 118, train loss: 0.51227, val loss: 0.53065\n",
      "Main effects training epoch: 119, train loss: 0.51225, val loss: 0.53137\n",
      "Main effects training epoch: 120, train loss: 0.51223, val loss: 0.53147\n",
      "Main effects training epoch: 121, train loss: 0.51289, val loss: 0.52884\n",
      "Main effects training epoch: 122, train loss: 0.51227, val loss: 0.53212\n",
      "Main effects training epoch: 123, train loss: 0.51248, val loss: 0.52867\n",
      "Main effects training epoch: 124, train loss: 0.51212, val loss: 0.53182\n",
      "Main effects training epoch: 125, train loss: 0.51195, val loss: 0.52855\n",
      "Main effects training epoch: 126, train loss: 0.51244, val loss: 0.53224\n",
      "Main effects training epoch: 127, train loss: 0.51241, val loss: 0.52776\n",
      "Main effects training epoch: 128, train loss: 0.51218, val loss: 0.53198\n",
      "Main effects training epoch: 129, train loss: 0.51212, val loss: 0.52994\n",
      "Main effects training epoch: 130, train loss: 0.51199, val loss: 0.53041\n",
      "Main effects training epoch: 131, train loss: 0.51152, val loss: 0.52976\n",
      "Main effects training epoch: 132, train loss: 0.51149, val loss: 0.52938\n",
      "Main effects training epoch: 133, train loss: 0.51162, val loss: 0.52706\n",
      "Main effects training epoch: 134, train loss: 0.51149, val loss: 0.53124\n",
      "Main effects training epoch: 135, train loss: 0.51140, val loss: 0.52826\n",
      "Main effects training epoch: 136, train loss: 0.51163, val loss: 0.53144\n",
      "Main effects training epoch: 137, train loss: 0.51152, val loss: 0.52939\n",
      "Main effects training epoch: 138, train loss: 0.51200, val loss: 0.52959\n",
      "Main effects training epoch: 139, train loss: 0.51112, val loss: 0.52790\n",
      "Main effects training epoch: 140, train loss: 0.51079, val loss: 0.52752\n",
      "Main effects training epoch: 141, train loss: 0.51056, val loss: 0.52917\n",
      "Main effects training epoch: 142, train loss: 0.51050, val loss: 0.52730\n",
      "Main effects training epoch: 143, train loss: 0.51033, val loss: 0.52870\n",
      "Main effects training epoch: 144, train loss: 0.51010, val loss: 0.52697\n",
      "Main effects training epoch: 145, train loss: 0.50992, val loss: 0.52821\n",
      "Main effects training epoch: 146, train loss: 0.51017, val loss: 0.52600\n",
      "Main effects training epoch: 147, train loss: 0.50964, val loss: 0.52947\n",
      "Main effects training epoch: 148, train loss: 0.50940, val loss: 0.52825\n",
      "Main effects training epoch: 149, train loss: 0.50953, val loss: 0.52961\n",
      "Main effects training epoch: 150, train loss: 0.50905, val loss: 0.52725\n",
      "Main effects training epoch: 151, train loss: 0.50870, val loss: 0.52907\n",
      "Main effects training epoch: 152, train loss: 0.50852, val loss: 0.52766\n",
      "Main effects training epoch: 153, train loss: 0.50832, val loss: 0.52878\n",
      "Main effects training epoch: 154, train loss: 0.50889, val loss: 0.52766\n",
      "Main effects training epoch: 155, train loss: 0.50852, val loss: 0.53046\n",
      "Main effects training epoch: 156, train loss: 0.50892, val loss: 0.53047\n",
      "Main effects training epoch: 157, train loss: 0.50789, val loss: 0.52822\n",
      "Main effects training epoch: 158, train loss: 0.50780, val loss: 0.52863\n",
      "Main effects training epoch: 159, train loss: 0.50843, val loss: 0.52636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 160, train loss: 0.50822, val loss: 0.53196\n",
      "Main effects training epoch: 161, train loss: 0.50883, val loss: 0.52561\n",
      "Main effects training epoch: 162, train loss: 0.50762, val loss: 0.52874\n",
      "Main effects training epoch: 163, train loss: 0.50802, val loss: 0.52883\n",
      "Main effects training epoch: 164, train loss: 0.50816, val loss: 0.52867\n",
      "Main effects training epoch: 165, train loss: 0.50784, val loss: 0.53002\n",
      "Main effects training epoch: 166, train loss: 0.50786, val loss: 0.52423\n",
      "Main effects training epoch: 167, train loss: 0.50730, val loss: 0.52882\n",
      "Main effects training epoch: 168, train loss: 0.50731, val loss: 0.52831\n",
      "Main effects training epoch: 169, train loss: 0.50816, val loss: 0.52480\n",
      "Main effects training epoch: 170, train loss: 0.50756, val loss: 0.53042\n",
      "Main effects training epoch: 171, train loss: 0.50728, val loss: 0.52692\n",
      "Main effects training epoch: 172, train loss: 0.50736, val loss: 0.52793\n",
      "Main effects training epoch: 173, train loss: 0.50762, val loss: 0.52543\n",
      "Main effects training epoch: 174, train loss: 0.50749, val loss: 0.52941\n",
      "Main effects training epoch: 175, train loss: 0.50723, val loss: 0.52569\n",
      "Main effects training epoch: 176, train loss: 0.50707, val loss: 0.52928\n",
      "Main effects training epoch: 177, train loss: 0.50683, val loss: 0.52560\n",
      "Main effects training epoch: 178, train loss: 0.50693, val loss: 0.52936\n",
      "Main effects training epoch: 179, train loss: 0.50714, val loss: 0.52842\n",
      "Main effects training epoch: 180, train loss: 0.50726, val loss: 0.52739\n",
      "Main effects training epoch: 181, train loss: 0.50765, val loss: 0.53128\n",
      "Main effects training epoch: 182, train loss: 0.50763, val loss: 0.52407\n",
      "Main effects training epoch: 183, train loss: 0.50851, val loss: 0.53295\n",
      "Main effects training epoch: 184, train loss: 0.50747, val loss: 0.52405\n",
      "Main effects training epoch: 185, train loss: 0.50751, val loss: 0.52919\n",
      "Main effects training epoch: 186, train loss: 0.50766, val loss: 0.52422\n",
      "Main effects training epoch: 187, train loss: 0.50672, val loss: 0.52880\n",
      "Main effects training epoch: 188, train loss: 0.50642, val loss: 0.52489\n",
      "Main effects training epoch: 189, train loss: 0.50628, val loss: 0.52742\n",
      "Main effects training epoch: 190, train loss: 0.50657, val loss: 0.52457\n",
      "Main effects training epoch: 191, train loss: 0.50638, val loss: 0.52789\n",
      "Main effects training epoch: 192, train loss: 0.50610, val loss: 0.52544\n",
      "Main effects training epoch: 193, train loss: 0.50752, val loss: 0.52357\n",
      "Main effects training epoch: 194, train loss: 0.50734, val loss: 0.53054\n",
      "Main effects training epoch: 195, train loss: 0.50611, val loss: 0.52599\n",
      "Main effects training epoch: 196, train loss: 0.50636, val loss: 0.52614\n",
      "Main effects training epoch: 197, train loss: 0.50588, val loss: 0.52555\n",
      "Main effects training epoch: 198, train loss: 0.50611, val loss: 0.52454\n",
      "Main effects training epoch: 199, train loss: 0.50627, val loss: 0.52802\n",
      "Main effects training epoch: 200, train loss: 0.50700, val loss: 0.52400\n",
      "Main effects training epoch: 201, train loss: 0.50642, val loss: 0.52838\n",
      "Main effects training epoch: 202, train loss: 0.50637, val loss: 0.52340\n",
      "Main effects training epoch: 203, train loss: 0.50593, val loss: 0.52638\n",
      "Main effects training epoch: 204, train loss: 0.50574, val loss: 0.52578\n",
      "Main effects training epoch: 205, train loss: 0.50582, val loss: 0.52523\n",
      "Main effects training epoch: 206, train loss: 0.50624, val loss: 0.52521\n",
      "Main effects training epoch: 207, train loss: 0.50602, val loss: 0.52695\n",
      "Main effects training epoch: 208, train loss: 0.50621, val loss: 0.52271\n",
      "Main effects training epoch: 209, train loss: 0.50569, val loss: 0.52561\n",
      "Main effects training epoch: 210, train loss: 0.50570, val loss: 0.52445\n",
      "Main effects training epoch: 211, train loss: 0.50573, val loss: 0.52436\n",
      "Main effects training epoch: 212, train loss: 0.50551, val loss: 0.52423\n",
      "Main effects training epoch: 213, train loss: 0.50569, val loss: 0.52549\n",
      "Main effects training epoch: 214, train loss: 0.50612, val loss: 0.52423\n",
      "Main effects training epoch: 215, train loss: 0.50602, val loss: 0.52725\n",
      "Main effects training epoch: 216, train loss: 0.50580, val loss: 0.52310\n",
      "Main effects training epoch: 217, train loss: 0.50676, val loss: 0.52858\n",
      "Main effects training epoch: 218, train loss: 0.50601, val loss: 0.52551\n",
      "Main effects training epoch: 219, train loss: 0.50579, val loss: 0.52329\n",
      "Main effects training epoch: 220, train loss: 0.50589, val loss: 0.52621\n",
      "Main effects training epoch: 221, train loss: 0.50603, val loss: 0.52261\n",
      "Main effects training epoch: 222, train loss: 0.50584, val loss: 0.52726\n",
      "Main effects training epoch: 223, train loss: 0.50658, val loss: 0.52506\n",
      "Main effects training epoch: 224, train loss: 0.50569, val loss: 0.52437\n",
      "Main effects training epoch: 225, train loss: 0.50548, val loss: 0.52656\n",
      "Main effects training epoch: 226, train loss: 0.50555, val loss: 0.52354\n",
      "Main effects training epoch: 227, train loss: 0.50642, val loss: 0.52621\n",
      "Main effects training epoch: 228, train loss: 0.50595, val loss: 0.52459\n",
      "Main effects training epoch: 229, train loss: 0.50620, val loss: 0.52590\n",
      "Main effects training epoch: 230, train loss: 0.50528, val loss: 0.52521\n",
      "Main effects training epoch: 231, train loss: 0.50567, val loss: 0.52522\n",
      "Main effects training epoch: 232, train loss: 0.50557, val loss: 0.52468\n",
      "Main effects training epoch: 233, train loss: 0.50538, val loss: 0.52567\n",
      "Main effects training epoch: 234, train loss: 0.50516, val loss: 0.52420\n",
      "Main effects training epoch: 235, train loss: 0.50552, val loss: 0.52694\n",
      "Main effects training epoch: 236, train loss: 0.50524, val loss: 0.52336\n",
      "Main effects training epoch: 237, train loss: 0.50535, val loss: 0.52533\n",
      "Main effects training epoch: 238, train loss: 0.50560, val loss: 0.52251\n",
      "Main effects training epoch: 239, train loss: 0.50570, val loss: 0.52766\n",
      "Main effects training epoch: 240, train loss: 0.50554, val loss: 0.52136\n",
      "Main effects training epoch: 241, train loss: 0.50633, val loss: 0.52896\n",
      "Main effects training epoch: 242, train loss: 0.50640, val loss: 0.52311\n",
      "Main effects training epoch: 243, train loss: 0.50535, val loss: 0.52683\n",
      "Main effects training epoch: 244, train loss: 0.50536, val loss: 0.52320\n",
      "Main effects training epoch: 245, train loss: 0.50537, val loss: 0.52546\n",
      "Main effects training epoch: 246, train loss: 0.50514, val loss: 0.52280\n",
      "Main effects training epoch: 247, train loss: 0.50530, val loss: 0.52543\n",
      "Main effects training epoch: 248, train loss: 0.50512, val loss: 0.52396\n",
      "Main effects training epoch: 249, train loss: 0.50536, val loss: 0.52315\n",
      "Main effects training epoch: 250, train loss: 0.50540, val loss: 0.52726\n",
      "Main effects training epoch: 251, train loss: 0.50508, val loss: 0.52303\n",
      "Main effects training epoch: 252, train loss: 0.50513, val loss: 0.52584\n",
      "Main effects training epoch: 253, train loss: 0.50506, val loss: 0.52370\n",
      "Main effects training epoch: 254, train loss: 0.50502, val loss: 0.52501\n",
      "Main effects training epoch: 255, train loss: 0.50553, val loss: 0.52376\n",
      "Main effects training epoch: 256, train loss: 0.50554, val loss: 0.52736\n",
      "Main effects training epoch: 257, train loss: 0.50514, val loss: 0.52584\n",
      "Main effects training epoch: 258, train loss: 0.50506, val loss: 0.52281\n",
      "Main effects training epoch: 259, train loss: 0.50477, val loss: 0.52508\n",
      "Main effects training epoch: 260, train loss: 0.50530, val loss: 0.52383\n",
      "Main effects training epoch: 261, train loss: 0.50504, val loss: 0.52438\n",
      "Main effects training epoch: 262, train loss: 0.50520, val loss: 0.52301\n",
      "Main effects training epoch: 263, train loss: 0.50588, val loss: 0.52823\n",
      "Main effects training epoch: 264, train loss: 0.50480, val loss: 0.52273\n",
      "Main effects training epoch: 265, train loss: 0.50545, val loss: 0.52763\n",
      "Main effects training epoch: 266, train loss: 0.50480, val loss: 0.52291\n",
      "Main effects training epoch: 267, train loss: 0.50576, val loss: 0.52663\n",
      "Main effects training epoch: 268, train loss: 0.50546, val loss: 0.52224\n",
      "Main effects training epoch: 269, train loss: 0.50518, val loss: 0.52558\n",
      "Main effects training epoch: 270, train loss: 0.50534, val loss: 0.52160\n",
      "Main effects training epoch: 271, train loss: 0.50488, val loss: 0.52495\n",
      "Main effects training epoch: 272, train loss: 0.50464, val loss: 0.52229\n",
      "Main effects training epoch: 273, train loss: 0.50459, val loss: 0.52332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 274, train loss: 0.50462, val loss: 0.52555\n",
      "Main effects training epoch: 275, train loss: 0.50463, val loss: 0.52364\n",
      "Main effects training epoch: 276, train loss: 0.50482, val loss: 0.52434\n",
      "Main effects training epoch: 277, train loss: 0.50502, val loss: 0.52532\n",
      "Main effects training epoch: 278, train loss: 0.50478, val loss: 0.52286\n",
      "Main effects training epoch: 279, train loss: 0.50468, val loss: 0.52467\n",
      "Main effects training epoch: 280, train loss: 0.50460, val loss: 0.52334\n",
      "Main effects training epoch: 281, train loss: 0.50452, val loss: 0.52335\n",
      "Main effects training epoch: 282, train loss: 0.50459, val loss: 0.52464\n",
      "Main effects training epoch: 283, train loss: 0.50442, val loss: 0.52400\n",
      "Main effects training epoch: 284, train loss: 0.50438, val loss: 0.52399\n",
      "Main effects training epoch: 285, train loss: 0.50451, val loss: 0.52403\n",
      "Main effects training epoch: 286, train loss: 0.50449, val loss: 0.52389\n",
      "Main effects training epoch: 287, train loss: 0.50438, val loss: 0.52466\n",
      "Main effects training epoch: 288, train loss: 0.50463, val loss: 0.52346\n",
      "Main effects training epoch: 289, train loss: 0.50468, val loss: 0.52570\n",
      "Main effects training epoch: 290, train loss: 0.50439, val loss: 0.52229\n",
      "Main effects training epoch: 291, train loss: 0.50464, val loss: 0.52608\n",
      "Main effects training epoch: 292, train loss: 0.50434, val loss: 0.52362\n",
      "Main effects training epoch: 293, train loss: 0.50418, val loss: 0.52306\n",
      "Main effects training epoch: 294, train loss: 0.50420, val loss: 0.52438\n",
      "Main effects training epoch: 295, train loss: 0.50450, val loss: 0.52367\n",
      "Main effects training epoch: 296, train loss: 0.50448, val loss: 0.52515\n",
      "Main effects training epoch: 297, train loss: 0.50436, val loss: 0.52336\n",
      "Main effects training epoch: 298, train loss: 0.50473, val loss: 0.52485\n",
      "Main effects training epoch: 299, train loss: 0.50437, val loss: 0.52510\n",
      "Main effects training epoch: 300, train loss: 0.50414, val loss: 0.52341\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.50932, val loss: 0.52259\n",
      "Main effects tuning epoch: 2, train loss: 0.50912, val loss: 0.52416\n",
      "Main effects tuning epoch: 3, train loss: 0.50912, val loss: 0.52330\n",
      "Main effects tuning epoch: 4, train loss: 0.50914, val loss: 0.52353\n",
      "Main effects tuning epoch: 5, train loss: 0.50931, val loss: 0.52214\n",
      "Main effects tuning epoch: 6, train loss: 0.50924, val loss: 0.52440\n",
      "Main effects tuning epoch: 7, train loss: 0.50918, val loss: 0.52176\n",
      "Main effects tuning epoch: 8, train loss: 0.50926, val loss: 0.52500\n",
      "Main effects tuning epoch: 9, train loss: 0.50932, val loss: 0.52330\n",
      "Main effects tuning epoch: 10, train loss: 0.50931, val loss: 0.52454\n",
      "Main effects tuning epoch: 11, train loss: 0.50931, val loss: 0.52251\n",
      "Main effects tuning epoch: 12, train loss: 0.50959, val loss: 0.52505\n",
      "Main effects tuning epoch: 13, train loss: 0.50949, val loss: 0.52304\n",
      "Main effects tuning epoch: 14, train loss: 0.50916, val loss: 0.52265\n",
      "Main effects tuning epoch: 15, train loss: 0.50911, val loss: 0.52267\n",
      "Main effects tuning epoch: 16, train loss: 0.50922, val loss: 0.52521\n",
      "Main effects tuning epoch: 17, train loss: 0.50929, val loss: 0.52188\n",
      "Main effects tuning epoch: 18, train loss: 0.50921, val loss: 0.52326\n",
      "Main effects tuning epoch: 19, train loss: 0.50918, val loss: 0.52499\n",
      "Main effects tuning epoch: 20, train loss: 0.50960, val loss: 0.52081\n",
      "Main effects tuning epoch: 21, train loss: 0.50931, val loss: 0.52523\n",
      "Main effects tuning epoch: 22, train loss: 0.50926, val loss: 0.52397\n",
      "Main effects tuning epoch: 23, train loss: 0.50901, val loss: 0.52279\n",
      "Main effects tuning epoch: 24, train loss: 0.50957, val loss: 0.52469\n",
      "Main effects tuning epoch: 25, train loss: 0.50912, val loss: 0.52213\n",
      "Main effects tuning epoch: 26, train loss: 0.50968, val loss: 0.52676\n",
      "Main effects tuning epoch: 27, train loss: 0.50918, val loss: 0.52250\n",
      "Main effects tuning epoch: 28, train loss: 0.50913, val loss: 0.52345\n",
      "Main effects tuning epoch: 29, train loss: 0.50907, val loss: 0.52458\n",
      "Main effects tuning epoch: 30, train loss: 0.50888, val loss: 0.52281\n",
      "Main effects tuning epoch: 31, train loss: 0.50886, val loss: 0.52267\n",
      "Main effects tuning epoch: 32, train loss: 0.50911, val loss: 0.52282\n",
      "Main effects tuning epoch: 33, train loss: 0.50903, val loss: 0.52222\n",
      "Main effects tuning epoch: 34, train loss: 0.50898, val loss: 0.52385\n",
      "Main effects tuning epoch: 35, train loss: 0.50888, val loss: 0.52308\n",
      "Main effects tuning epoch: 36, train loss: 0.50899, val loss: 0.52345\n",
      "Main effects tuning epoch: 37, train loss: 0.50900, val loss: 0.52367\n",
      "Main effects tuning epoch: 38, train loss: 0.50892, val loss: 0.52288\n",
      "Main effects tuning epoch: 39, train loss: 0.50921, val loss: 0.52601\n",
      "Main effects tuning epoch: 40, train loss: 0.50931, val loss: 0.52216\n",
      "Main effects tuning epoch: 41, train loss: 0.50983, val loss: 0.52482\n",
      "Main effects tuning epoch: 42, train loss: 0.50983, val loss: 0.52343\n",
      "Main effects tuning epoch: 43, train loss: 0.50908, val loss: 0.52291\n",
      "Main effects tuning epoch: 44, train loss: 0.50918, val loss: 0.52458\n",
      "Main effects tuning epoch: 45, train loss: 0.51000, val loss: 0.52303\n",
      "Main effects tuning epoch: 46, train loss: 0.50957, val loss: 0.52322\n",
      "Main effects tuning epoch: 47, train loss: 0.50908, val loss: 0.52264\n",
      "Main effects tuning epoch: 48, train loss: 0.50922, val loss: 0.52232\n",
      "Main effects tuning epoch: 49, train loss: 0.50895, val loss: 0.52351\n",
      "Main effects tuning epoch: 50, train loss: 0.50933, val loss: 0.52226\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.46810, val loss: 0.48181\n",
      "Interaction training epoch: 2, train loss: 0.32580, val loss: 0.31275\n",
      "Interaction training epoch: 3, train loss: 0.30975, val loss: 0.29704\n",
      "Interaction training epoch: 4, train loss: 0.31543, val loss: 0.29719\n",
      "Interaction training epoch: 5, train loss: 0.29469, val loss: 0.27638\n",
      "Interaction training epoch: 6, train loss: 0.30250, val loss: 0.30077\n",
      "Interaction training epoch: 7, train loss: 0.30656, val loss: 0.29715\n",
      "Interaction training epoch: 8, train loss: 0.29670, val loss: 0.26823\n",
      "Interaction training epoch: 9, train loss: 0.29289, val loss: 0.28287\n",
      "Interaction training epoch: 10, train loss: 0.29364, val loss: 0.28482\n",
      "Interaction training epoch: 11, train loss: 0.28973, val loss: 0.27342\n",
      "Interaction training epoch: 12, train loss: 0.28659, val loss: 0.27494\n",
      "Interaction training epoch: 13, train loss: 0.28871, val loss: 0.26820\n",
      "Interaction training epoch: 14, train loss: 0.28378, val loss: 0.27095\n",
      "Interaction training epoch: 15, train loss: 0.28717, val loss: 0.27724\n",
      "Interaction training epoch: 16, train loss: 0.28832, val loss: 0.27251\n",
      "Interaction training epoch: 17, train loss: 0.28562, val loss: 0.27752\n",
      "Interaction training epoch: 18, train loss: 0.28356, val loss: 0.27111\n",
      "Interaction training epoch: 19, train loss: 0.28091, val loss: 0.26956\n",
      "Interaction training epoch: 20, train loss: 0.28290, val loss: 0.27819\n",
      "Interaction training epoch: 21, train loss: 0.28157, val loss: 0.26934\n",
      "Interaction training epoch: 22, train loss: 0.27892, val loss: 0.26440\n",
      "Interaction training epoch: 23, train loss: 0.28661, val loss: 0.27929\n",
      "Interaction training epoch: 24, train loss: 0.27775, val loss: 0.26609\n",
      "Interaction training epoch: 25, train loss: 0.28445, val loss: 0.27433\n",
      "Interaction training epoch: 26, train loss: 0.28287, val loss: 0.26658\n",
      "Interaction training epoch: 27, train loss: 0.27811, val loss: 0.26766\n",
      "Interaction training epoch: 28, train loss: 0.27685, val loss: 0.26728\n",
      "Interaction training epoch: 29, train loss: 0.28395, val loss: 0.27524\n",
      "Interaction training epoch: 30, train loss: 0.27584, val loss: 0.26535\n",
      "Interaction training epoch: 31, train loss: 0.27783, val loss: 0.27084\n",
      "Interaction training epoch: 32, train loss: 0.27773, val loss: 0.26304\n",
      "Interaction training epoch: 33, train loss: 0.27596, val loss: 0.26840\n",
      "Interaction training epoch: 34, train loss: 0.28093, val loss: 0.27255\n",
      "Interaction training epoch: 35, train loss: 0.27823, val loss: 0.25934\n",
      "Interaction training epoch: 36, train loss: 0.27484, val loss: 0.26607\n",
      "Interaction training epoch: 37, train loss: 0.27877, val loss: 0.27218\n",
      "Interaction training epoch: 38, train loss: 0.27325, val loss: 0.26050\n",
      "Interaction training epoch: 39, train loss: 0.27583, val loss: 0.26912\n",
      "Interaction training epoch: 40, train loss: 0.27505, val loss: 0.26231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 41, train loss: 0.27377, val loss: 0.26662\n",
      "Interaction training epoch: 42, train loss: 0.27522, val loss: 0.26355\n",
      "Interaction training epoch: 43, train loss: 0.28107, val loss: 0.27058\n",
      "Interaction training epoch: 44, train loss: 0.27315, val loss: 0.26349\n",
      "Interaction training epoch: 45, train loss: 0.27582, val loss: 0.26786\n",
      "Interaction training epoch: 46, train loss: 0.27247, val loss: 0.26201\n",
      "Interaction training epoch: 47, train loss: 0.27252, val loss: 0.26248\n",
      "Interaction training epoch: 48, train loss: 0.27287, val loss: 0.26488\n",
      "Interaction training epoch: 49, train loss: 0.27445, val loss: 0.26154\n",
      "Interaction training epoch: 50, train loss: 0.27236, val loss: 0.26463\n",
      "Interaction training epoch: 51, train loss: 0.27426, val loss: 0.26218\n",
      "Interaction training epoch: 52, train loss: 0.27196, val loss: 0.25931\n",
      "Interaction training epoch: 53, train loss: 0.27403, val loss: 0.26780\n",
      "Interaction training epoch: 54, train loss: 0.27370, val loss: 0.25793\n",
      "Interaction training epoch: 55, train loss: 0.27031, val loss: 0.26178\n",
      "Interaction training epoch: 56, train loss: 0.27091, val loss: 0.26264\n",
      "Interaction training epoch: 57, train loss: 0.26994, val loss: 0.25953\n",
      "Interaction training epoch: 58, train loss: 0.27288, val loss: 0.26354\n",
      "Interaction training epoch: 59, train loss: 0.27031, val loss: 0.25962\n",
      "Interaction training epoch: 60, train loss: 0.27107, val loss: 0.25972\n",
      "Interaction training epoch: 61, train loss: 0.26917, val loss: 0.26059\n",
      "Interaction training epoch: 62, train loss: 0.27459, val loss: 0.26738\n",
      "Interaction training epoch: 63, train loss: 0.27085, val loss: 0.25889\n",
      "Interaction training epoch: 64, train loss: 0.27059, val loss: 0.26235\n",
      "Interaction training epoch: 65, train loss: 0.27263, val loss: 0.26620\n",
      "Interaction training epoch: 66, train loss: 0.27013, val loss: 0.26197\n",
      "Interaction training epoch: 67, train loss: 0.26984, val loss: 0.26065\n",
      "Interaction training epoch: 68, train loss: 0.27245, val loss: 0.26561\n",
      "Interaction training epoch: 69, train loss: 0.26877, val loss: 0.25963\n",
      "Interaction training epoch: 70, train loss: 0.27774, val loss: 0.27227\n",
      "Interaction training epoch: 71, train loss: 0.27016, val loss: 0.25780\n",
      "Interaction training epoch: 72, train loss: 0.27052, val loss: 0.26601\n",
      "Interaction training epoch: 73, train loss: 0.26705, val loss: 0.25619\n",
      "Interaction training epoch: 74, train loss: 0.26949, val loss: 0.26316\n",
      "Interaction training epoch: 75, train loss: 0.26831, val loss: 0.25742\n",
      "Interaction training epoch: 76, train loss: 0.26735, val loss: 0.26141\n",
      "Interaction training epoch: 77, train loss: 0.26845, val loss: 0.26119\n",
      "Interaction training epoch: 78, train loss: 0.26440, val loss: 0.25988\n",
      "Interaction training epoch: 79, train loss: 0.26879, val loss: 0.25882\n",
      "Interaction training epoch: 80, train loss: 0.26545, val loss: 0.26012\n",
      "Interaction training epoch: 81, train loss: 0.26953, val loss: 0.26193\n",
      "Interaction training epoch: 82, train loss: 0.26670, val loss: 0.26053\n",
      "Interaction training epoch: 83, train loss: 0.26811, val loss: 0.26773\n",
      "Interaction training epoch: 84, train loss: 0.26880, val loss: 0.25964\n",
      "Interaction training epoch: 85, train loss: 0.26914, val loss: 0.26617\n",
      "Interaction training epoch: 86, train loss: 0.26805, val loss: 0.25887\n",
      "Interaction training epoch: 87, train loss: 0.26777, val loss: 0.26382\n",
      "Interaction training epoch: 88, train loss: 0.26476, val loss: 0.25785\n",
      "Interaction training epoch: 89, train loss: 0.26328, val loss: 0.25973\n",
      "Interaction training epoch: 90, train loss: 0.26756, val loss: 0.26217\n",
      "Interaction training epoch: 91, train loss: 0.26415, val loss: 0.25786\n",
      "Interaction training epoch: 92, train loss: 0.26351, val loss: 0.25892\n",
      "Interaction training epoch: 93, train loss: 0.26338, val loss: 0.26020\n",
      "Interaction training epoch: 94, train loss: 0.26448, val loss: 0.26136\n",
      "Interaction training epoch: 95, train loss: 0.26212, val loss: 0.25883\n",
      "Interaction training epoch: 96, train loss: 0.26747, val loss: 0.26182\n",
      "Interaction training epoch: 97, train loss: 0.26341, val loss: 0.26476\n",
      "Interaction training epoch: 98, train loss: 0.26457, val loss: 0.25919\n",
      "Interaction training epoch: 99, train loss: 0.26026, val loss: 0.25898\n",
      "Interaction training epoch: 100, train loss: 0.26488, val loss: 0.25836\n",
      "Interaction training epoch: 101, train loss: 0.26249, val loss: 0.25925\n",
      "Interaction training epoch: 102, train loss: 0.26414, val loss: 0.25973\n",
      "Interaction training epoch: 103, train loss: 0.26245, val loss: 0.25893\n",
      "Interaction training epoch: 104, train loss: 0.26070, val loss: 0.26108\n",
      "Interaction training epoch: 105, train loss: 0.25986, val loss: 0.25604\n",
      "Interaction training epoch: 106, train loss: 0.26224, val loss: 0.26287\n",
      "Interaction training epoch: 107, train loss: 0.26129, val loss: 0.25723\n",
      "Interaction training epoch: 108, train loss: 0.26120, val loss: 0.25880\n",
      "Interaction training epoch: 109, train loss: 0.26342, val loss: 0.25726\n",
      "Interaction training epoch: 110, train loss: 0.25961, val loss: 0.25919\n",
      "Interaction training epoch: 111, train loss: 0.26148, val loss: 0.26099\n",
      "Interaction training epoch: 112, train loss: 0.25976, val loss: 0.25741\n",
      "Interaction training epoch: 113, train loss: 0.26025, val loss: 0.26132\n",
      "Interaction training epoch: 114, train loss: 0.26117, val loss: 0.25892\n",
      "Interaction training epoch: 115, train loss: 0.25965, val loss: 0.26109\n",
      "Interaction training epoch: 116, train loss: 0.25790, val loss: 0.26256\n",
      "Interaction training epoch: 117, train loss: 0.26004, val loss: 0.25814\n",
      "Interaction training epoch: 118, train loss: 0.26039, val loss: 0.26195\n",
      "Interaction training epoch: 119, train loss: 0.25633, val loss: 0.25868\n",
      "Interaction training epoch: 120, train loss: 0.25921, val loss: 0.25983\n",
      "Interaction training epoch: 121, train loss: 0.25939, val loss: 0.26217\n",
      "Interaction training epoch: 122, train loss: 0.25894, val loss: 0.26115\n",
      "Interaction training epoch: 123, train loss: 0.26148, val loss: 0.26613\n",
      "Interaction training epoch: 124, train loss: 0.25710, val loss: 0.25671\n",
      "Interaction training epoch: 125, train loss: 0.25721, val loss: 0.26023\n",
      "Interaction training epoch: 126, train loss: 0.25366, val loss: 0.25753\n",
      "Interaction training epoch: 127, train loss: 0.25604, val loss: 0.25826\n",
      "Interaction training epoch: 128, train loss: 0.25799, val loss: 0.25905\n",
      "Interaction training epoch: 129, train loss: 0.25612, val loss: 0.25988\n",
      "Interaction training epoch: 130, train loss: 0.25247, val loss: 0.25649\n",
      "Interaction training epoch: 131, train loss: 0.25670, val loss: 0.25786\n",
      "Interaction training epoch: 132, train loss: 0.25383, val loss: 0.26056\n",
      "Interaction training epoch: 133, train loss: 0.25564, val loss: 0.25696\n",
      "Interaction training epoch: 134, train loss: 0.25460, val loss: 0.26367\n",
      "Interaction training epoch: 135, train loss: 0.25574, val loss: 0.25805\n",
      "Interaction training epoch: 136, train loss: 0.25535, val loss: 0.25946\n",
      "Interaction training epoch: 137, train loss: 0.25357, val loss: 0.25907\n",
      "Interaction training epoch: 138, train loss: 0.25335, val loss: 0.25811\n",
      "Interaction training epoch: 139, train loss: 0.25985, val loss: 0.26465\n",
      "Interaction training epoch: 140, train loss: 0.25420, val loss: 0.25754\n",
      "Interaction training epoch: 141, train loss: 0.25280, val loss: 0.25881\n",
      "Interaction training epoch: 142, train loss: 0.25304, val loss: 0.25752\n",
      "Interaction training epoch: 143, train loss: 0.25318, val loss: 0.26186\n",
      "Interaction training epoch: 144, train loss: 0.25330, val loss: 0.25897\n",
      "Interaction training epoch: 145, train loss: 0.25268, val loss: 0.25882\n",
      "Interaction training epoch: 146, train loss: 0.25517, val loss: 0.26217\n",
      "Interaction training epoch: 147, train loss: 0.25021, val loss: 0.25459\n",
      "Interaction training epoch: 148, train loss: 0.25543, val loss: 0.26742\n",
      "Interaction training epoch: 149, train loss: 0.25039, val loss: 0.25335\n",
      "Interaction training epoch: 150, train loss: 0.25211, val loss: 0.26190\n",
      "Interaction training epoch: 151, train loss: 0.25323, val loss: 0.25952\n",
      "Interaction training epoch: 152, train loss: 0.25154, val loss: 0.26123\n",
      "Interaction training epoch: 153, train loss: 0.25463, val loss: 0.25724\n",
      "Interaction training epoch: 154, train loss: 0.25025, val loss: 0.25858\n",
      "Interaction training epoch: 155, train loss: 0.25265, val loss: 0.25757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 156, train loss: 0.25156, val loss: 0.26370\n",
      "Interaction training epoch: 157, train loss: 0.24960, val loss: 0.25885\n",
      "Interaction training epoch: 158, train loss: 0.25130, val loss: 0.26081\n",
      "Interaction training epoch: 159, train loss: 0.25202, val loss: 0.25738\n",
      "Interaction training epoch: 160, train loss: 0.24989, val loss: 0.26287\n",
      "Interaction training epoch: 161, train loss: 0.24992, val loss: 0.25811\n",
      "Interaction training epoch: 162, train loss: 0.25354, val loss: 0.26498\n",
      "Interaction training epoch: 163, train loss: 0.24906, val loss: 0.25948\n",
      "Interaction training epoch: 164, train loss: 0.24859, val loss: 0.25741\n",
      "Interaction training epoch: 165, train loss: 0.24954, val loss: 0.26230\n",
      "Interaction training epoch: 166, train loss: 0.25029, val loss: 0.26176\n",
      "Interaction training epoch: 167, train loss: 0.25036, val loss: 0.26381\n",
      "Interaction training epoch: 168, train loss: 0.24844, val loss: 0.25941\n",
      "Interaction training epoch: 169, train loss: 0.24988, val loss: 0.26787\n",
      "Interaction training epoch: 170, train loss: 0.24981, val loss: 0.26165\n",
      "Interaction training epoch: 171, train loss: 0.24775, val loss: 0.25818\n",
      "Interaction training epoch: 172, train loss: 0.25013, val loss: 0.26646\n",
      "Interaction training epoch: 173, train loss: 0.24637, val loss: 0.25601\n",
      "Interaction training epoch: 174, train loss: 0.24873, val loss: 0.26478\n",
      "Interaction training epoch: 175, train loss: 0.24699, val loss: 0.26006\n",
      "Interaction training epoch: 176, train loss: 0.24893, val loss: 0.26105\n",
      "Interaction training epoch: 177, train loss: 0.25019, val loss: 0.26865\n",
      "Interaction training epoch: 178, train loss: 0.24799, val loss: 0.25663\n",
      "Interaction training epoch: 179, train loss: 0.24956, val loss: 0.26532\n",
      "Interaction training epoch: 180, train loss: 0.24676, val loss: 0.26163\n",
      "Interaction training epoch: 181, train loss: 0.24771, val loss: 0.26595\n",
      "Interaction training epoch: 182, train loss: 0.24750, val loss: 0.26076\n",
      "Interaction training epoch: 183, train loss: 0.25504, val loss: 0.26916\n",
      "Interaction training epoch: 184, train loss: 0.24498, val loss: 0.26036\n",
      "Interaction training epoch: 185, train loss: 0.25018, val loss: 0.26426\n",
      "Interaction training epoch: 186, train loss: 0.24846, val loss: 0.26636\n",
      "Interaction training epoch: 187, train loss: 0.24784, val loss: 0.26178\n",
      "Interaction training epoch: 188, train loss: 0.25009, val loss: 0.26721\n",
      "Interaction training epoch: 189, train loss: 0.24459, val loss: 0.25778\n",
      "Interaction training epoch: 190, train loss: 0.24667, val loss: 0.26355\n",
      "Interaction training epoch: 191, train loss: 0.24726, val loss: 0.26509\n",
      "Interaction training epoch: 192, train loss: 0.24707, val loss: 0.26174\n",
      "Interaction training epoch: 193, train loss: 0.24858, val loss: 0.26407\n",
      "Interaction training epoch: 194, train loss: 0.24521, val loss: 0.26164\n",
      "Interaction training epoch: 195, train loss: 0.24713, val loss: 0.26921\n",
      "Interaction training epoch: 196, train loss: 0.24576, val loss: 0.26410\n",
      "Interaction training epoch: 197, train loss: 0.24802, val loss: 0.26396\n",
      "Interaction training epoch: 198, train loss: 0.24611, val loss: 0.26480\n",
      "Interaction training epoch: 199, train loss: 0.24384, val loss: 0.26235\n",
      "Interaction training epoch: 200, train loss: 0.24300, val loss: 0.26181\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########2 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.25373, val loss: 0.26258\n",
      "Interaction tuning epoch: 2, train loss: 0.25208, val loss: 0.25933\n",
      "Interaction tuning epoch: 3, train loss: 0.25349, val loss: 0.26071\n",
      "Interaction tuning epoch: 4, train loss: 0.25121, val loss: 0.26254\n",
      "Interaction tuning epoch: 5, train loss: 0.25152, val loss: 0.25888\n",
      "Interaction tuning epoch: 6, train loss: 0.25122, val loss: 0.26307\n",
      "Interaction tuning epoch: 7, train loss: 0.25042, val loss: 0.26359\n",
      "Interaction tuning epoch: 8, train loss: 0.25134, val loss: 0.25957\n",
      "Interaction tuning epoch: 9, train loss: 0.25191, val loss: 0.26143\n",
      "Interaction tuning epoch: 10, train loss: 0.25144, val loss: 0.26504\n",
      "Interaction tuning epoch: 11, train loss: 0.25185, val loss: 0.26326\n",
      "Interaction tuning epoch: 12, train loss: 0.25582, val loss: 0.26596\n",
      "Interaction tuning epoch: 13, train loss: 0.25021, val loss: 0.26313\n",
      "Interaction tuning epoch: 14, train loss: 0.24853, val loss: 0.26297\n",
      "Interaction tuning epoch: 15, train loss: 0.24957, val loss: 0.26461\n",
      "Interaction tuning epoch: 16, train loss: 0.24900, val loss: 0.26264\n",
      "Interaction tuning epoch: 17, train loss: 0.25173, val loss: 0.26281\n",
      "Interaction tuning epoch: 18, train loss: 0.24822, val loss: 0.26469\n",
      "Interaction tuning epoch: 19, train loss: 0.24858, val loss: 0.26296\n",
      "Interaction tuning epoch: 20, train loss: 0.25197, val loss: 0.26823\n",
      "Interaction tuning epoch: 21, train loss: 0.24787, val loss: 0.26438\n",
      "Interaction tuning epoch: 22, train loss: 0.25502, val loss: 0.26806\n",
      "Interaction tuning epoch: 23, train loss: 0.24873, val loss: 0.26552\n",
      "Interaction tuning epoch: 24, train loss: 0.25030, val loss: 0.26559\n",
      "Interaction tuning epoch: 25, train loss: 0.24861, val loss: 0.26270\n",
      "Interaction tuning epoch: 26, train loss: 0.24935, val loss: 0.26526\n",
      "Interaction tuning epoch: 27, train loss: 0.24808, val loss: 0.26451\n",
      "Interaction tuning epoch: 28, train loss: 0.24777, val loss: 0.26568\n",
      "Interaction tuning epoch: 29, train loss: 0.24794, val loss: 0.26585\n",
      "Interaction tuning epoch: 30, train loss: 0.24906, val loss: 0.26546\n",
      "Interaction tuning epoch: 31, train loss: 0.24715, val loss: 0.26560\n",
      "Interaction tuning epoch: 32, train loss: 0.25073, val loss: 0.26630\n",
      "Interaction tuning epoch: 33, train loss: 0.25093, val loss: 0.27112\n",
      "Interaction tuning epoch: 34, train loss: 0.24843, val loss: 0.26260\n",
      "Interaction tuning epoch: 35, train loss: 0.24981, val loss: 0.27111\n",
      "Interaction tuning epoch: 36, train loss: 0.24823, val loss: 0.26879\n",
      "Interaction tuning epoch: 37, train loss: 0.25443, val loss: 0.27418\n",
      "Interaction tuning epoch: 38, train loss: 0.24739, val loss: 0.26256\n",
      "Interaction tuning epoch: 39, train loss: 0.24887, val loss: 0.27154\n",
      "Interaction tuning epoch: 40, train loss: 0.24990, val loss: 0.26488\n",
      "Interaction tuning epoch: 41, train loss: 0.24574, val loss: 0.26443\n",
      "Interaction tuning epoch: 42, train loss: 0.25097, val loss: 0.26620\n",
      "Interaction tuning epoch: 43, train loss: 0.24586, val loss: 0.26306\n",
      "Interaction tuning epoch: 44, train loss: 0.25218, val loss: 0.27663\n",
      "Interaction tuning epoch: 45, train loss: 0.24741, val loss: 0.26614\n",
      "Interaction tuning epoch: 46, train loss: 0.24975, val loss: 0.26836\n",
      "Interaction tuning epoch: 47, train loss: 0.24574, val loss: 0.26548\n",
      "Interaction tuning epoch: 48, train loss: 0.24762, val loss: 0.26633\n",
      "Interaction tuning epoch: 49, train loss: 0.25082, val loss: 0.27150\n",
      "Interaction tuning epoch: 50, train loss: 0.24743, val loss: 0.26844\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 37.46588683128357\n",
      "After the gam stage, training error is 0.24743 , validation error is 0.26844\n",
      "missing value counts: 99102\n",
      "[SoftImpute] Max Singular Value of X_init = 3.883094\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed BCE=0.211138 validation BCE=0.263663,rank=5\n",
      "[SoftImpute] Iter 2: observed BCE=0.206948 validation BCE=0.262882,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.204277 validation BCE=0.262488,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.202401 validation BCE=0.261927,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.200927 validation BCE=0.261900,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.199630 validation BCE=0.261569,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.198911 validation BCE=0.261857,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.198174 validation BCE=0.261389,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.197609 validation BCE=0.262361,rank=5\n",
      "[SoftImpute] Iter 10: observed BCE=0.197497 validation BCE=0.261637,rank=5\n",
      "[SoftImpute] Iter 11: observed BCE=0.196890 validation BCE=0.261419,rank=5\n",
      "[SoftImpute] Iter 12: observed BCE=0.196552 validation BCE=0.262927,rank=5\n",
      "[SoftImpute] Iter 13: observed BCE=0.196382 validation BCE=0.262142,rank=5\n",
      "[SoftImpute] Iter 14: observed BCE=0.196091 validation BCE=0.261782,rank=5\n",
      "[SoftImpute] Iter 15: observed BCE=0.195808 validation BCE=0.264880,rank=5\n",
      "[SoftImpute] Iter 16: observed BCE=0.195431 validation BCE=0.262587,rank=5\n",
      "[SoftImpute] Iter 17: observed BCE=0.195501 validation BCE=0.261984,rank=5\n",
      "[SoftImpute] Iter 18: observed BCE=0.195658 validation BCE=0.262596,rank=5\n",
      "[SoftImpute] Iter 19: observed BCE=0.195231 validation BCE=0.261928,rank=5\n",
      "[SoftImpute] Iter 20: observed BCE=0.195302 validation BCE=0.261971,rank=5\n",
      "[SoftImpute] Iter 21: observed BCE=0.195110 validation BCE=0.261809,rank=5\n",
      "[SoftImpute] Iter 22: observed BCE=0.194993 validation BCE=0.274103,rank=5\n",
      "[SoftImpute] Iter 23: observed BCE=0.194787 validation BCE=0.264029,rank=5\n",
      "[SoftImpute] Iter 24: observed BCE=0.194824 validation BCE=0.262199,rank=5\n",
      "[SoftImpute] Iter 25: observed BCE=0.194676 validation BCE=0.262316,rank=5\n",
      "[SoftImpute] Iter 26: observed BCE=0.194930 validation BCE=0.265352,rank=5\n",
      "[SoftImpute] Iter 27: observed BCE=0.195268 validation BCE=0.264192,rank=5\n",
      "[SoftImpute] Iter 28: observed BCE=0.195253 validation BCE=0.264226,rank=5\n",
      "[SoftImpute] Iter 29: observed BCE=0.195598 validation BCE=0.264411,rank=5\n",
      "[SoftImpute] Iter 30: observed BCE=0.195471 validation BCE=0.264280,rank=5\n",
      "[SoftImpute] Iter 31: observed BCE=0.195401 validation BCE=0.264458,rank=5\n",
      "[SoftImpute] Iter 32: observed BCE=0.195279 validation BCE=0.264508,rank=5\n",
      "[SoftImpute] Iter 33: observed BCE=0.195361 validation BCE=0.276707,rank=5\n",
      "[SoftImpute] Stopped after iteration 33 for lambda=0.077662\n",
      "final num of user group: 26\n",
      "final num of item group: 26\n",
      "change mode state : True\n",
      "time cost: 11.846651792526245\n",
      "After the matrix factor stage, training error is 0.19536, validation error is 0.27671\n",
      "2\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68204, val loss: 0.68054\n",
      "Main effects training epoch: 2, train loss: 0.67624, val loss: 0.67186\n",
      "Main effects training epoch: 3, train loss: 0.67258, val loss: 0.66373\n",
      "Main effects training epoch: 4, train loss: 0.66434, val loss: 0.65757\n",
      "Main effects training epoch: 5, train loss: 0.65335, val loss: 0.64602\n",
      "Main effects training epoch: 6, train loss: 0.63257, val loss: 0.62451\n",
      "Main effects training epoch: 7, train loss: 0.59944, val loss: 0.58877\n",
      "Main effects training epoch: 8, train loss: 0.55739, val loss: 0.55189\n",
      "Main effects training epoch: 9, train loss: 0.52959, val loss: 0.53483\n",
      "Main effects training epoch: 10, train loss: 0.53472, val loss: 0.52064\n",
      "Main effects training epoch: 11, train loss: 0.52478, val loss: 0.53297\n",
      "Main effects training epoch: 12, train loss: 0.52253, val loss: 0.52373\n",
      "Main effects training epoch: 13, train loss: 0.52022, val loss: 0.52803\n",
      "Main effects training epoch: 14, train loss: 0.51920, val loss: 0.52236\n",
      "Main effects training epoch: 15, train loss: 0.51889, val loss: 0.52445\n",
      "Main effects training epoch: 16, train loss: 0.51878, val loss: 0.52355\n",
      "Main effects training epoch: 17, train loss: 0.51855, val loss: 0.52431\n",
      "Main effects training epoch: 18, train loss: 0.51879, val loss: 0.52778\n",
      "Main effects training epoch: 19, train loss: 0.51937, val loss: 0.52371\n",
      "Main effects training epoch: 20, train loss: 0.51930, val loss: 0.52597\n",
      "Main effects training epoch: 21, train loss: 0.51980, val loss: 0.52405\n",
      "Main effects training epoch: 22, train loss: 0.51865, val loss: 0.52746\n",
      "Main effects training epoch: 23, train loss: 0.51927, val loss: 0.52306\n",
      "Main effects training epoch: 24, train loss: 0.51968, val loss: 0.53276\n",
      "Main effects training epoch: 25, train loss: 0.51930, val loss: 0.52284\n",
      "Main effects training epoch: 26, train loss: 0.51905, val loss: 0.53061\n",
      "Main effects training epoch: 27, train loss: 0.51995, val loss: 0.52144\n",
      "Main effects training epoch: 28, train loss: 0.51860, val loss: 0.52811\n",
      "Main effects training epoch: 29, train loss: 0.51819, val loss: 0.52802\n",
      "Main effects training epoch: 30, train loss: 0.51783, val loss: 0.52430\n",
      "Main effects training epoch: 31, train loss: 0.51780, val loss: 0.52665\n",
      "Main effects training epoch: 32, train loss: 0.51765, val loss: 0.52619\n",
      "Main effects training epoch: 33, train loss: 0.51776, val loss: 0.52700\n",
      "Main effects training epoch: 34, train loss: 0.51834, val loss: 0.52636\n",
      "Main effects training epoch: 35, train loss: 0.51823, val loss: 0.52637\n",
      "Main effects training epoch: 36, train loss: 0.51768, val loss: 0.52593\n",
      "Main effects training epoch: 37, train loss: 0.51870, val loss: 0.52265\n",
      "Main effects training epoch: 38, train loss: 0.51794, val loss: 0.52717\n",
      "Main effects training epoch: 39, train loss: 0.51793, val loss: 0.52654\n",
      "Main effects training epoch: 40, train loss: 0.51778, val loss: 0.52601\n",
      "Main effects training epoch: 41, train loss: 0.51816, val loss: 0.52702\n",
      "Main effects training epoch: 42, train loss: 0.51957, val loss: 0.53151\n",
      "Main effects training epoch: 43, train loss: 0.51829, val loss: 0.52504\n",
      "Main effects training epoch: 44, train loss: 0.51784, val loss: 0.52901\n",
      "Main effects training epoch: 45, train loss: 0.51784, val loss: 0.52381\n",
      "Main effects training epoch: 46, train loss: 0.51759, val loss: 0.52767\n",
      "Main effects training epoch: 47, train loss: 0.51749, val loss: 0.52471\n",
      "Main effects training epoch: 48, train loss: 0.51856, val loss: 0.52936\n",
      "Main effects training epoch: 49, train loss: 0.51805, val loss: 0.52400\n",
      "Main effects training epoch: 50, train loss: 0.51755, val loss: 0.52787\n",
      "Main effects training epoch: 51, train loss: 0.51734, val loss: 0.52705\n",
      "Main effects training epoch: 52, train loss: 0.51729, val loss: 0.52543\n",
      "Main effects training epoch: 53, train loss: 0.51780, val loss: 0.52594\n",
      "Main effects training epoch: 54, train loss: 0.51858, val loss: 0.52719\n",
      "Main effects training epoch: 55, train loss: 0.51854, val loss: 0.52568\n",
      "Main effects training epoch: 56, train loss: 0.51768, val loss: 0.52644\n",
      "Main effects training epoch: 57, train loss: 0.51782, val loss: 0.52676\n",
      "Main effects training epoch: 58, train loss: 0.51811, val loss: 0.52328\n",
      "Main effects training epoch: 59, train loss: 0.51759, val loss: 0.52756\n",
      "Main effects training epoch: 60, train loss: 0.51732, val loss: 0.52234\n",
      "Main effects training epoch: 61, train loss: 0.51714, val loss: 0.52689\n",
      "Main effects training epoch: 62, train loss: 0.51779, val loss: 0.52764\n",
      "Main effects training epoch: 63, train loss: 0.51710, val loss: 0.52464\n",
      "Main effects training epoch: 64, train loss: 0.51708, val loss: 0.52596\n",
      "Main effects training epoch: 65, train loss: 0.51694, val loss: 0.52503\n",
      "Main effects training epoch: 66, train loss: 0.51778, val loss: 0.52886\n",
      "Main effects training epoch: 67, train loss: 0.51707, val loss: 0.52306\n",
      "Main effects training epoch: 68, train loss: 0.51677, val loss: 0.52492\n",
      "Main effects training epoch: 69, train loss: 0.51682, val loss: 0.52681\n",
      "Main effects training epoch: 70, train loss: 0.51682, val loss: 0.52416\n",
      "Main effects training epoch: 71, train loss: 0.51696, val loss: 0.52729\n",
      "Main effects training epoch: 72, train loss: 0.51675, val loss: 0.52361\n",
      "Main effects training epoch: 73, train loss: 0.51673, val loss: 0.52691\n",
      "Main effects training epoch: 74, train loss: 0.51751, val loss: 0.52523\n",
      "Main effects training epoch: 75, train loss: 0.51752, val loss: 0.52639\n",
      "Main effects training epoch: 76, train loss: 0.51694, val loss: 0.52377\n",
      "Main effects training epoch: 77, train loss: 0.51711, val loss: 0.52487\n",
      "Main effects training epoch: 78, train loss: 0.51674, val loss: 0.52405\n",
      "Main effects training epoch: 79, train loss: 0.51656, val loss: 0.52437\n",
      "Main effects training epoch: 80, train loss: 0.51656, val loss: 0.52694\n",
      "Main effects training epoch: 81, train loss: 0.51646, val loss: 0.52299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 82, train loss: 0.51633, val loss: 0.52390\n",
      "Main effects training epoch: 83, train loss: 0.51671, val loss: 0.52857\n",
      "Main effects training epoch: 84, train loss: 0.51638, val loss: 0.52219\n",
      "Main effects training epoch: 85, train loss: 0.51645, val loss: 0.52678\n",
      "Main effects training epoch: 86, train loss: 0.51631, val loss: 0.52300\n",
      "Main effects training epoch: 87, train loss: 0.51698, val loss: 0.52759\n",
      "Main effects training epoch: 88, train loss: 0.51651, val loss: 0.52171\n",
      "Main effects training epoch: 89, train loss: 0.51608, val loss: 0.52671\n",
      "Main effects training epoch: 90, train loss: 0.51629, val loss: 0.52348\n",
      "Main effects training epoch: 91, train loss: 0.51594, val loss: 0.52531\n",
      "Main effects training epoch: 92, train loss: 0.51671, val loss: 0.52356\n",
      "Main effects training epoch: 93, train loss: 0.51604, val loss: 0.52576\n",
      "Main effects training epoch: 94, train loss: 0.51611, val loss: 0.52123\n",
      "Main effects training epoch: 95, train loss: 0.51595, val loss: 0.52717\n",
      "Main effects training epoch: 96, train loss: 0.51581, val loss: 0.52229\n",
      "Main effects training epoch: 97, train loss: 0.51619, val loss: 0.52409\n",
      "Main effects training epoch: 98, train loss: 0.51578, val loss: 0.52401\n",
      "Main effects training epoch: 99, train loss: 0.51567, val loss: 0.52416\n",
      "Main effects training epoch: 100, train loss: 0.51563, val loss: 0.52308\n",
      "Main effects training epoch: 101, train loss: 0.51554, val loss: 0.52214\n",
      "Main effects training epoch: 102, train loss: 0.51527, val loss: 0.52326\n",
      "Main effects training epoch: 103, train loss: 0.51534, val loss: 0.52384\n",
      "Main effects training epoch: 104, train loss: 0.51508, val loss: 0.52258\n",
      "Main effects training epoch: 105, train loss: 0.51507, val loss: 0.52255\n",
      "Main effects training epoch: 106, train loss: 0.51519, val loss: 0.52075\n",
      "Main effects training epoch: 107, train loss: 0.51491, val loss: 0.52353\n",
      "Main effects training epoch: 108, train loss: 0.51507, val loss: 0.52082\n",
      "Main effects training epoch: 109, train loss: 0.51519, val loss: 0.52046\n",
      "Main effects training epoch: 110, train loss: 0.51565, val loss: 0.52187\n",
      "Main effects training epoch: 111, train loss: 0.51476, val loss: 0.52166\n",
      "Main effects training epoch: 112, train loss: 0.51456, val loss: 0.52224\n",
      "Main effects training epoch: 113, train loss: 0.51459, val loss: 0.52227\n",
      "Main effects training epoch: 114, train loss: 0.51528, val loss: 0.52167\n",
      "Main effects training epoch: 115, train loss: 0.51477, val loss: 0.52054\n",
      "Main effects training epoch: 116, train loss: 0.51451, val loss: 0.52046\n",
      "Main effects training epoch: 117, train loss: 0.51418, val loss: 0.52102\n",
      "Main effects training epoch: 118, train loss: 0.51395, val loss: 0.52051\n",
      "Main effects training epoch: 119, train loss: 0.51514, val loss: 0.51831\n",
      "Main effects training epoch: 120, train loss: 0.51434, val loss: 0.52315\n",
      "Main effects training epoch: 121, train loss: 0.51441, val loss: 0.51822\n",
      "Main effects training epoch: 122, train loss: 0.51399, val loss: 0.52266\n",
      "Main effects training epoch: 123, train loss: 0.51365, val loss: 0.51966\n",
      "Main effects training epoch: 124, train loss: 0.51369, val loss: 0.51955\n",
      "Main effects training epoch: 125, train loss: 0.51367, val loss: 0.52076\n",
      "Main effects training epoch: 126, train loss: 0.51342, val loss: 0.52019\n",
      "Main effects training epoch: 127, train loss: 0.51334, val loss: 0.51893\n",
      "Main effects training epoch: 128, train loss: 0.51359, val loss: 0.52034\n",
      "Main effects training epoch: 129, train loss: 0.51359, val loss: 0.52152\n",
      "Main effects training epoch: 130, train loss: 0.51343, val loss: 0.51847\n",
      "Main effects training epoch: 131, train loss: 0.51369, val loss: 0.51745\n",
      "Main effects training epoch: 132, train loss: 0.51360, val loss: 0.51983\n",
      "Main effects training epoch: 133, train loss: 0.51366, val loss: 0.51862\n",
      "Main effects training epoch: 134, train loss: 0.51321, val loss: 0.52087\n",
      "Main effects training epoch: 135, train loss: 0.51340, val loss: 0.51716\n",
      "Main effects training epoch: 136, train loss: 0.51354, val loss: 0.52240\n",
      "Main effects training epoch: 137, train loss: 0.51429, val loss: 0.51627\n",
      "Main effects training epoch: 138, train loss: 0.51346, val loss: 0.52077\n",
      "Main effects training epoch: 139, train loss: 0.51351, val loss: 0.51723\n",
      "Main effects training epoch: 140, train loss: 0.51339, val loss: 0.51963\n",
      "Main effects training epoch: 141, train loss: 0.51411, val loss: 0.51722\n",
      "Main effects training epoch: 142, train loss: 0.51310, val loss: 0.51958\n",
      "Main effects training epoch: 143, train loss: 0.51275, val loss: 0.51748\n",
      "Main effects training epoch: 144, train loss: 0.51332, val loss: 0.52187\n",
      "Main effects training epoch: 145, train loss: 0.51270, val loss: 0.51712\n",
      "Main effects training epoch: 146, train loss: 0.51282, val loss: 0.51976\n",
      "Main effects training epoch: 147, train loss: 0.51277, val loss: 0.51718\n",
      "Main effects training epoch: 148, train loss: 0.51288, val loss: 0.51853\n",
      "Main effects training epoch: 149, train loss: 0.51231, val loss: 0.51835\n",
      "Main effects training epoch: 150, train loss: 0.51232, val loss: 0.51838\n",
      "Main effects training epoch: 151, train loss: 0.51271, val loss: 0.51871\n",
      "Main effects training epoch: 152, train loss: 0.51237, val loss: 0.51744\n",
      "Main effects training epoch: 153, train loss: 0.51233, val loss: 0.51928\n",
      "Main effects training epoch: 154, train loss: 0.51246, val loss: 0.51742\n",
      "Main effects training epoch: 155, train loss: 0.51240, val loss: 0.51695\n",
      "Main effects training epoch: 156, train loss: 0.51254, val loss: 0.52022\n",
      "Main effects training epoch: 157, train loss: 0.51247, val loss: 0.51781\n",
      "Main effects training epoch: 158, train loss: 0.51257, val loss: 0.52030\n",
      "Main effects training epoch: 159, train loss: 0.51246, val loss: 0.51716\n",
      "Main effects training epoch: 160, train loss: 0.51244, val loss: 0.51936\n",
      "Main effects training epoch: 161, train loss: 0.51217, val loss: 0.51730\n",
      "Main effects training epoch: 162, train loss: 0.51224, val loss: 0.51720\n",
      "Main effects training epoch: 163, train loss: 0.51265, val loss: 0.51489\n",
      "Main effects training epoch: 164, train loss: 0.51224, val loss: 0.51891\n",
      "Main effects training epoch: 165, train loss: 0.51204, val loss: 0.51765\n",
      "Main effects training epoch: 166, train loss: 0.51255, val loss: 0.51680\n",
      "Main effects training epoch: 167, train loss: 0.51250, val loss: 0.51884\n",
      "Main effects training epoch: 168, train loss: 0.51252, val loss: 0.51623\n",
      "Main effects training epoch: 169, train loss: 0.51241, val loss: 0.51755\n",
      "Main effects training epoch: 170, train loss: 0.51228, val loss: 0.51618\n",
      "Main effects training epoch: 171, train loss: 0.51238, val loss: 0.52088\n",
      "Main effects training epoch: 172, train loss: 0.51199, val loss: 0.51607\n",
      "Main effects training epoch: 173, train loss: 0.51216, val loss: 0.51520\n",
      "Main effects training epoch: 174, train loss: 0.51209, val loss: 0.51871\n",
      "Main effects training epoch: 175, train loss: 0.51216, val loss: 0.52022\n",
      "Main effects training epoch: 176, train loss: 0.51238, val loss: 0.51448\n",
      "Main effects training epoch: 177, train loss: 0.51223, val loss: 0.52003\n",
      "Main effects training epoch: 178, train loss: 0.51202, val loss: 0.51594\n",
      "Main effects training epoch: 179, train loss: 0.51197, val loss: 0.51839\n",
      "Main effects training epoch: 180, train loss: 0.51184, val loss: 0.51707\n",
      "Main effects training epoch: 181, train loss: 0.51162, val loss: 0.51686\n",
      "Main effects training epoch: 182, train loss: 0.51186, val loss: 0.51929\n",
      "Main effects training epoch: 183, train loss: 0.51204, val loss: 0.51532\n",
      "Main effects training epoch: 184, train loss: 0.51195, val loss: 0.51880\n",
      "Main effects training epoch: 185, train loss: 0.51168, val loss: 0.51616\n",
      "Main effects training epoch: 186, train loss: 0.51174, val loss: 0.51727\n",
      "Main effects training epoch: 187, train loss: 0.51186, val loss: 0.51724\n",
      "Main effects training epoch: 188, train loss: 0.51217, val loss: 0.51807\n",
      "Main effects training epoch: 189, train loss: 0.51160, val loss: 0.51717\n",
      "Main effects training epoch: 190, train loss: 0.51172, val loss: 0.51901\n",
      "Main effects training epoch: 191, train loss: 0.51199, val loss: 0.51621\n",
      "Main effects training epoch: 192, train loss: 0.51191, val loss: 0.51762\n",
      "Main effects training epoch: 193, train loss: 0.51155, val loss: 0.51675\n",
      "Main effects training epoch: 194, train loss: 0.51204, val loss: 0.51984\n",
      "Main effects training epoch: 195, train loss: 0.51158, val loss: 0.51411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 196, train loss: 0.51189, val loss: 0.51866\n",
      "Main effects training epoch: 197, train loss: 0.51155, val loss: 0.51406\n",
      "Main effects training epoch: 198, train loss: 0.51236, val loss: 0.51976\n",
      "Main effects training epoch: 199, train loss: 0.51237, val loss: 0.51520\n",
      "Main effects training epoch: 200, train loss: 0.51174, val loss: 0.51725\n",
      "Main effects training epoch: 201, train loss: 0.51157, val loss: 0.51542\n",
      "Main effects training epoch: 202, train loss: 0.51111, val loss: 0.51574\n",
      "Main effects training epoch: 203, train loss: 0.51119, val loss: 0.51681\n",
      "Main effects training epoch: 204, train loss: 0.51133, val loss: 0.51531\n",
      "Main effects training epoch: 205, train loss: 0.51115, val loss: 0.51781\n",
      "Main effects training epoch: 206, train loss: 0.51102, val loss: 0.51409\n",
      "Main effects training epoch: 207, train loss: 0.51131, val loss: 0.51852\n",
      "Main effects training epoch: 208, train loss: 0.51120, val loss: 0.51422\n",
      "Main effects training epoch: 209, train loss: 0.51113, val loss: 0.51656\n",
      "Main effects training epoch: 210, train loss: 0.51143, val loss: 0.51539\n",
      "Main effects training epoch: 211, train loss: 0.51192, val loss: 0.52043\n",
      "Main effects training epoch: 212, train loss: 0.51164, val loss: 0.51320\n",
      "Main effects training epoch: 213, train loss: 0.51105, val loss: 0.51481\n",
      "Main effects training epoch: 214, train loss: 0.51098, val loss: 0.51685\n",
      "Main effects training epoch: 215, train loss: 0.51080, val loss: 0.51429\n",
      "Main effects training epoch: 216, train loss: 0.51107, val loss: 0.51513\n",
      "Main effects training epoch: 217, train loss: 0.51128, val loss: 0.51322\n",
      "Main effects training epoch: 218, train loss: 0.51093, val loss: 0.51658\n",
      "Main effects training epoch: 219, train loss: 0.51085, val loss: 0.51590\n",
      "Main effects training epoch: 220, train loss: 0.51128, val loss: 0.51424\n",
      "Main effects training epoch: 221, train loss: 0.51133, val loss: 0.51739\n",
      "Main effects training epoch: 222, train loss: 0.51088, val loss: 0.51520\n",
      "Main effects training epoch: 223, train loss: 0.51057, val loss: 0.51559\n",
      "Main effects training epoch: 224, train loss: 0.51083, val loss: 0.51633\n",
      "Main effects training epoch: 225, train loss: 0.51051, val loss: 0.51472\n",
      "Main effects training epoch: 226, train loss: 0.51049, val loss: 0.51667\n",
      "Main effects training epoch: 227, train loss: 0.51084, val loss: 0.51535\n",
      "Main effects training epoch: 228, train loss: 0.51061, val loss: 0.51556\n",
      "Main effects training epoch: 229, train loss: 0.51033, val loss: 0.51509\n",
      "Main effects training epoch: 230, train loss: 0.51032, val loss: 0.51566\n",
      "Main effects training epoch: 231, train loss: 0.51037, val loss: 0.51500\n",
      "Main effects training epoch: 232, train loss: 0.51112, val loss: 0.51558\n",
      "Main effects training epoch: 233, train loss: 0.51059, val loss: 0.51396\n",
      "Main effects training epoch: 234, train loss: 0.51042, val loss: 0.51610\n",
      "Main effects training epoch: 235, train loss: 0.51018, val loss: 0.51438\n",
      "Main effects training epoch: 236, train loss: 0.51030, val loss: 0.51357\n",
      "Main effects training epoch: 237, train loss: 0.51018, val loss: 0.51595\n",
      "Main effects training epoch: 238, train loss: 0.50999, val loss: 0.51451\n",
      "Main effects training epoch: 239, train loss: 0.51032, val loss: 0.51508\n",
      "Main effects training epoch: 240, train loss: 0.51010, val loss: 0.51500\n",
      "Main effects training epoch: 241, train loss: 0.50998, val loss: 0.51379\n",
      "Main effects training epoch: 242, train loss: 0.50986, val loss: 0.51518\n",
      "Main effects training epoch: 243, train loss: 0.51006, val loss: 0.51251\n",
      "Main effects training epoch: 244, train loss: 0.51040, val loss: 0.51602\n",
      "Main effects training epoch: 245, train loss: 0.51016, val loss: 0.51569\n",
      "Main effects training epoch: 246, train loss: 0.51003, val loss: 0.51343\n",
      "Main effects training epoch: 247, train loss: 0.51008, val loss: 0.51401\n",
      "Main effects training epoch: 248, train loss: 0.51026, val loss: 0.51462\n",
      "Main effects training epoch: 249, train loss: 0.50991, val loss: 0.51366\n",
      "Main effects training epoch: 250, train loss: 0.50997, val loss: 0.51537\n",
      "Main effects training epoch: 251, train loss: 0.50988, val loss: 0.51394\n",
      "Main effects training epoch: 252, train loss: 0.50963, val loss: 0.51441\n",
      "Main effects training epoch: 253, train loss: 0.50993, val loss: 0.51556\n",
      "Main effects training epoch: 254, train loss: 0.50969, val loss: 0.51520\n",
      "Main effects training epoch: 255, train loss: 0.50954, val loss: 0.51513\n",
      "Main effects training epoch: 256, train loss: 0.50996, val loss: 0.51336\n",
      "Main effects training epoch: 257, train loss: 0.51057, val loss: 0.51691\n",
      "Main effects training epoch: 258, train loss: 0.51041, val loss: 0.51189\n",
      "Main effects training epoch: 259, train loss: 0.51004, val loss: 0.51729\n",
      "Main effects training epoch: 260, train loss: 0.50942, val loss: 0.51601\n",
      "Main effects training epoch: 261, train loss: 0.50946, val loss: 0.51269\n",
      "Main effects training epoch: 262, train loss: 0.50945, val loss: 0.51452\n",
      "Main effects training epoch: 263, train loss: 0.50936, val loss: 0.51315\n",
      "Main effects training epoch: 264, train loss: 0.50958, val loss: 0.51574\n",
      "Main effects training epoch: 265, train loss: 0.50925, val loss: 0.51281\n",
      "Main effects training epoch: 266, train loss: 0.51028, val loss: 0.51804\n",
      "Main effects training epoch: 267, train loss: 0.50945, val loss: 0.51123\n",
      "Main effects training epoch: 268, train loss: 0.50919, val loss: 0.51382\n",
      "Main effects training epoch: 269, train loss: 0.50941, val loss: 0.51301\n",
      "Main effects training epoch: 270, train loss: 0.50940, val loss: 0.51496\n",
      "Main effects training epoch: 271, train loss: 0.50937, val loss: 0.51348\n",
      "Main effects training epoch: 272, train loss: 0.50923, val loss: 0.51332\n",
      "Main effects training epoch: 273, train loss: 0.50926, val loss: 0.51317\n",
      "Main effects training epoch: 274, train loss: 0.50939, val loss: 0.51614\n",
      "Main effects training epoch: 275, train loss: 0.50977, val loss: 0.50965\n",
      "Main effects training epoch: 276, train loss: 0.50948, val loss: 0.51550\n",
      "Main effects training epoch: 277, train loss: 0.50977, val loss: 0.51312\n",
      "Main effects training epoch: 278, train loss: 0.50892, val loss: 0.51418\n",
      "Main effects training epoch: 279, train loss: 0.50866, val loss: 0.51454\n",
      "Main effects training epoch: 280, train loss: 0.50838, val loss: 0.51234\n",
      "Main effects training epoch: 281, train loss: 0.50865, val loss: 0.51503\n",
      "Main effects training epoch: 282, train loss: 0.50855, val loss: 0.51109\n",
      "Main effects training epoch: 283, train loss: 0.50867, val loss: 0.51395\n",
      "Main effects training epoch: 284, train loss: 0.50885, val loss: 0.51460\n",
      "Main effects training epoch: 285, train loss: 0.50833, val loss: 0.51370\n",
      "Main effects training epoch: 286, train loss: 0.50822, val loss: 0.51219\n",
      "Main effects training epoch: 287, train loss: 0.50790, val loss: 0.51385\n",
      "Main effects training epoch: 288, train loss: 0.50824, val loss: 0.51057\n",
      "Main effects training epoch: 289, train loss: 0.50849, val loss: 0.51474\n",
      "Main effects training epoch: 290, train loss: 0.50791, val loss: 0.51052\n",
      "Main effects training epoch: 291, train loss: 0.50783, val loss: 0.51061\n",
      "Main effects training epoch: 292, train loss: 0.50756, val loss: 0.51196\n",
      "Main effects training epoch: 293, train loss: 0.50749, val loss: 0.51129\n",
      "Main effects training epoch: 294, train loss: 0.50796, val loss: 0.51465\n",
      "Main effects training epoch: 295, train loss: 0.50721, val loss: 0.51044\n",
      "Main effects training epoch: 296, train loss: 0.50704, val loss: 0.51155\n",
      "Main effects training epoch: 297, train loss: 0.50692, val loss: 0.51055\n",
      "Main effects training epoch: 298, train loss: 0.50680, val loss: 0.51105\n",
      "Main effects training epoch: 299, train loss: 0.50681, val loss: 0.51314\n",
      "Main effects training epoch: 300, train loss: 0.50671, val loss: 0.51059\n",
      "##########Stage 1: main effect training stop.##########\n",
      "6 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.50801, val loss: 0.51075\n",
      "Main effects tuning epoch: 2, train loss: 0.50820, val loss: 0.51027\n",
      "Main effects tuning epoch: 3, train loss: 0.50807, val loss: 0.51232\n",
      "Main effects tuning epoch: 4, train loss: 0.50803, val loss: 0.50828\n",
      "Main effects tuning epoch: 5, train loss: 0.50789, val loss: 0.51101\n",
      "Main effects tuning epoch: 6, train loss: 0.50740, val loss: 0.50819\n",
      "Main effects tuning epoch: 7, train loss: 0.50773, val loss: 0.50798\n",
      "Main effects tuning epoch: 8, train loss: 0.50769, val loss: 0.51195\n",
      "Main effects tuning epoch: 9, train loss: 0.50752, val loss: 0.50688\n",
      "Main effects tuning epoch: 10, train loss: 0.50761, val loss: 0.51250\n",
      "Main effects tuning epoch: 11, train loss: 0.50727, val loss: 0.50637\n",
      "Main effects tuning epoch: 12, train loss: 0.50706, val loss: 0.51306\n",
      "Main effects tuning epoch: 13, train loss: 0.50674, val loss: 0.50912\n",
      "Main effects tuning epoch: 14, train loss: 0.50664, val loss: 0.51126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects tuning epoch: 15, train loss: 0.50683, val loss: 0.50671\n",
      "Main effects tuning epoch: 16, train loss: 0.50713, val loss: 0.51426\n",
      "Main effects tuning epoch: 17, train loss: 0.50705, val loss: 0.50882\n",
      "Main effects tuning epoch: 18, train loss: 0.50693, val loss: 0.51271\n",
      "Main effects tuning epoch: 19, train loss: 0.50669, val loss: 0.50852\n",
      "Main effects tuning epoch: 20, train loss: 0.50622, val loss: 0.51090\n",
      "Main effects tuning epoch: 21, train loss: 0.50688, val loss: 0.50937\n",
      "Main effects tuning epoch: 22, train loss: 0.50681, val loss: 0.51087\n",
      "Main effects tuning epoch: 23, train loss: 0.50660, val loss: 0.50878\n",
      "Main effects tuning epoch: 24, train loss: 0.50673, val loss: 0.51212\n",
      "Main effects tuning epoch: 25, train loss: 0.50595, val loss: 0.51174\n",
      "Main effects tuning epoch: 26, train loss: 0.50602, val loss: 0.50903\n",
      "Main effects tuning epoch: 27, train loss: 0.50583, val loss: 0.51147\n",
      "Main effects tuning epoch: 28, train loss: 0.50549, val loss: 0.51020\n",
      "Main effects tuning epoch: 29, train loss: 0.50597, val loss: 0.50682\n",
      "Main effects tuning epoch: 30, train loss: 0.50625, val loss: 0.51253\n",
      "Main effects tuning epoch: 31, train loss: 0.50600, val loss: 0.50939\n",
      "Main effects tuning epoch: 32, train loss: 0.50666, val loss: 0.51312\n",
      "Main effects tuning epoch: 33, train loss: 0.50572, val loss: 0.50968\n",
      "Main effects tuning epoch: 34, train loss: 0.50513, val loss: 0.50922\n",
      "Main effects tuning epoch: 35, train loss: 0.50504, val loss: 0.51019\n",
      "Main effects tuning epoch: 36, train loss: 0.50510, val loss: 0.51247\n",
      "Main effects tuning epoch: 37, train loss: 0.50520, val loss: 0.51051\n",
      "Main effects tuning epoch: 38, train loss: 0.50528, val loss: 0.51042\n",
      "Main effects tuning epoch: 39, train loss: 0.50536, val loss: 0.50947\n",
      "Main effects tuning epoch: 40, train loss: 0.50518, val loss: 0.51107\n",
      "Main effects tuning epoch: 41, train loss: 0.50480, val loss: 0.51108\n",
      "Main effects tuning epoch: 42, train loss: 0.50509, val loss: 0.50976\n",
      "Main effects tuning epoch: 43, train loss: 0.50487, val loss: 0.51146\n",
      "Main effects tuning epoch: 44, train loss: 0.50494, val loss: 0.50884\n",
      "Main effects tuning epoch: 45, train loss: 0.50529, val loss: 0.51483\n",
      "Main effects tuning epoch: 46, train loss: 0.50503, val loss: 0.50804\n",
      "Main effects tuning epoch: 47, train loss: 0.50492, val loss: 0.51541\n",
      "Main effects tuning epoch: 48, train loss: 0.50507, val loss: 0.50926\n",
      "Main effects tuning epoch: 49, train loss: 0.50445, val loss: 0.51236\n",
      "Main effects tuning epoch: 50, train loss: 0.50433, val loss: 0.51077\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.50369, val loss: 0.50745\n",
      "Interaction training epoch: 2, train loss: 0.45602, val loss: 0.47486\n",
      "Interaction training epoch: 3, train loss: 0.39119, val loss: 0.38336\n",
      "Interaction training epoch: 4, train loss: 0.30713, val loss: 0.31493\n",
      "Interaction training epoch: 5, train loss: 0.29030, val loss: 0.29210\n",
      "Interaction training epoch: 6, train loss: 0.31227, val loss: 0.30923\n",
      "Interaction training epoch: 7, train loss: 0.29728, val loss: 0.30057\n",
      "Interaction training epoch: 8, train loss: 0.29322, val loss: 0.29496\n",
      "Interaction training epoch: 9, train loss: 0.28662, val loss: 0.29234\n",
      "Interaction training epoch: 10, train loss: 0.28274, val loss: 0.28644\n",
      "Interaction training epoch: 11, train loss: 0.28188, val loss: 0.28710\n",
      "Interaction training epoch: 12, train loss: 0.27981, val loss: 0.28447\n",
      "Interaction training epoch: 13, train loss: 0.28246, val loss: 0.28728\n",
      "Interaction training epoch: 14, train loss: 0.28609, val loss: 0.29161\n",
      "Interaction training epoch: 15, train loss: 0.28292, val loss: 0.28737\n",
      "Interaction training epoch: 16, train loss: 0.27709, val loss: 0.28508\n",
      "Interaction training epoch: 17, train loss: 0.28048, val loss: 0.28560\n",
      "Interaction training epoch: 18, train loss: 0.27914, val loss: 0.28812\n",
      "Interaction training epoch: 19, train loss: 0.27759, val loss: 0.28496\n",
      "Interaction training epoch: 20, train loss: 0.27523, val loss: 0.28101\n",
      "Interaction training epoch: 21, train loss: 0.27675, val loss: 0.28407\n",
      "Interaction training epoch: 22, train loss: 0.27582, val loss: 0.28149\n",
      "Interaction training epoch: 23, train loss: 0.27712, val loss: 0.28482\n",
      "Interaction training epoch: 24, train loss: 0.27422, val loss: 0.27980\n",
      "Interaction training epoch: 25, train loss: 0.27673, val loss: 0.28309\n",
      "Interaction training epoch: 26, train loss: 0.28422, val loss: 0.28950\n",
      "Interaction training epoch: 27, train loss: 0.28102, val loss: 0.28860\n",
      "Interaction training epoch: 28, train loss: 0.27272, val loss: 0.28115\n",
      "Interaction training epoch: 29, train loss: 0.27865, val loss: 0.28360\n",
      "Interaction training epoch: 30, train loss: 0.27804, val loss: 0.28456\n",
      "Interaction training epoch: 31, train loss: 0.27496, val loss: 0.28208\n",
      "Interaction training epoch: 32, train loss: 0.27179, val loss: 0.28017\n",
      "Interaction training epoch: 33, train loss: 0.27287, val loss: 0.28067\n",
      "Interaction training epoch: 34, train loss: 0.28117, val loss: 0.28606\n",
      "Interaction training epoch: 35, train loss: 0.27549, val loss: 0.28514\n",
      "Interaction training epoch: 36, train loss: 0.26960, val loss: 0.27684\n",
      "Interaction training epoch: 37, train loss: 0.27367, val loss: 0.28068\n",
      "Interaction training epoch: 38, train loss: 0.27685, val loss: 0.28366\n",
      "Interaction training epoch: 39, train loss: 0.27157, val loss: 0.27834\n",
      "Interaction training epoch: 40, train loss: 0.27467, val loss: 0.28466\n",
      "Interaction training epoch: 41, train loss: 0.27404, val loss: 0.28423\n",
      "Interaction training epoch: 42, train loss: 0.27276, val loss: 0.27810\n",
      "Interaction training epoch: 43, train loss: 0.27448, val loss: 0.28483\n",
      "Interaction training epoch: 44, train loss: 0.27079, val loss: 0.27770\n",
      "Interaction training epoch: 45, train loss: 0.27197, val loss: 0.28114\n",
      "Interaction training epoch: 46, train loss: 0.27239, val loss: 0.28267\n",
      "Interaction training epoch: 47, train loss: 0.27232, val loss: 0.27995\n",
      "Interaction training epoch: 48, train loss: 0.27125, val loss: 0.28094\n",
      "Interaction training epoch: 49, train loss: 0.26921, val loss: 0.27695\n",
      "Interaction training epoch: 50, train loss: 0.27023, val loss: 0.27731\n",
      "Interaction training epoch: 51, train loss: 0.27183, val loss: 0.28097\n",
      "Interaction training epoch: 52, train loss: 0.27094, val loss: 0.28046\n",
      "Interaction training epoch: 53, train loss: 0.26753, val loss: 0.27566\n",
      "Interaction training epoch: 54, train loss: 0.27322, val loss: 0.28493\n",
      "Interaction training epoch: 55, train loss: 0.26795, val loss: 0.27773\n",
      "Interaction training epoch: 56, train loss: 0.27527, val loss: 0.28178\n",
      "Interaction training epoch: 57, train loss: 0.27005, val loss: 0.28282\n",
      "Interaction training epoch: 58, train loss: 0.26562, val loss: 0.27303\n",
      "Interaction training epoch: 59, train loss: 0.27085, val loss: 0.27922\n",
      "Interaction training epoch: 60, train loss: 0.26894, val loss: 0.27789\n",
      "Interaction training epoch: 61, train loss: 0.26753, val loss: 0.27750\n",
      "Interaction training epoch: 62, train loss: 0.26973, val loss: 0.27829\n",
      "Interaction training epoch: 63, train loss: 0.26593, val loss: 0.27631\n",
      "Interaction training epoch: 64, train loss: 0.26805, val loss: 0.27798\n",
      "Interaction training epoch: 65, train loss: 0.26674, val loss: 0.27667\n",
      "Interaction training epoch: 66, train loss: 0.26987, val loss: 0.27802\n",
      "Interaction training epoch: 67, train loss: 0.27135, val loss: 0.28194\n",
      "Interaction training epoch: 68, train loss: 0.27081, val loss: 0.28018\n",
      "Interaction training epoch: 69, train loss: 0.26877, val loss: 0.27790\n",
      "Interaction training epoch: 70, train loss: 0.26567, val loss: 0.27622\n",
      "Interaction training epoch: 71, train loss: 0.26862, val loss: 0.27575\n",
      "Interaction training epoch: 72, train loss: 0.26746, val loss: 0.27663\n",
      "Interaction training epoch: 73, train loss: 0.26458, val loss: 0.27191\n",
      "Interaction training epoch: 74, train loss: 0.26973, val loss: 0.28019\n",
      "Interaction training epoch: 75, train loss: 0.26718, val loss: 0.27937\n",
      "Interaction training epoch: 76, train loss: 0.26470, val loss: 0.27691\n",
      "Interaction training epoch: 77, train loss: 0.26582, val loss: 0.27826\n",
      "Interaction training epoch: 78, train loss: 0.26657, val loss: 0.27586\n",
      "Interaction training epoch: 79, train loss: 0.26500, val loss: 0.27548\n",
      "Interaction training epoch: 80, train loss: 0.26531, val loss: 0.27802\n",
      "Interaction training epoch: 81, train loss: 0.26293, val loss: 0.27526\n",
      "Interaction training epoch: 82, train loss: 0.26371, val loss: 0.27069\n",
      "Interaction training epoch: 83, train loss: 0.26638, val loss: 0.28044\n",
      "Interaction training epoch: 84, train loss: 0.26667, val loss: 0.27945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 85, train loss: 0.26605, val loss: 0.27743\n",
      "Interaction training epoch: 86, train loss: 0.26717, val loss: 0.27990\n",
      "Interaction training epoch: 87, train loss: 0.26466, val loss: 0.27476\n",
      "Interaction training epoch: 88, train loss: 0.26354, val loss: 0.27351\n",
      "Interaction training epoch: 89, train loss: 0.26173, val loss: 0.27493\n",
      "Interaction training epoch: 90, train loss: 0.26435, val loss: 0.27589\n",
      "Interaction training epoch: 91, train loss: 0.25899, val loss: 0.27253\n",
      "Interaction training epoch: 92, train loss: 0.26596, val loss: 0.27984\n",
      "Interaction training epoch: 93, train loss: 0.25732, val loss: 0.26962\n",
      "Interaction training epoch: 94, train loss: 0.26437, val loss: 0.27846\n",
      "Interaction training epoch: 95, train loss: 0.25979, val loss: 0.27064\n",
      "Interaction training epoch: 96, train loss: 0.26007, val loss: 0.27450\n",
      "Interaction training epoch: 97, train loss: 0.26633, val loss: 0.27944\n",
      "Interaction training epoch: 98, train loss: 0.26084, val loss: 0.27158\n",
      "Interaction training epoch: 99, train loss: 0.25946, val loss: 0.27453\n",
      "Interaction training epoch: 100, train loss: 0.26277, val loss: 0.27717\n",
      "Interaction training epoch: 101, train loss: 0.25611, val loss: 0.26901\n",
      "Interaction training epoch: 102, train loss: 0.25747, val loss: 0.27125\n",
      "Interaction training epoch: 103, train loss: 0.26177, val loss: 0.27765\n",
      "Interaction training epoch: 104, train loss: 0.26021, val loss: 0.27348\n",
      "Interaction training epoch: 105, train loss: 0.25655, val loss: 0.27021\n",
      "Interaction training epoch: 106, train loss: 0.26272, val loss: 0.27871\n",
      "Interaction training epoch: 107, train loss: 0.25795, val loss: 0.27695\n",
      "Interaction training epoch: 108, train loss: 0.25853, val loss: 0.27189\n",
      "Interaction training epoch: 109, train loss: 0.25865, val loss: 0.27176\n",
      "Interaction training epoch: 110, train loss: 0.25551, val loss: 0.27194\n",
      "Interaction training epoch: 111, train loss: 0.25695, val loss: 0.26957\n",
      "Interaction training epoch: 112, train loss: 0.25638, val loss: 0.27440\n",
      "Interaction training epoch: 113, train loss: 0.25868, val loss: 0.27499\n",
      "Interaction training epoch: 114, train loss: 0.25492, val loss: 0.26905\n",
      "Interaction training epoch: 115, train loss: 0.25978, val loss: 0.27306\n",
      "Interaction training epoch: 116, train loss: 0.25893, val loss: 0.27296\n",
      "Interaction training epoch: 117, train loss: 0.25596, val loss: 0.27179\n",
      "Interaction training epoch: 118, train loss: 0.25351, val loss: 0.26654\n",
      "Interaction training epoch: 119, train loss: 0.25799, val loss: 0.27529\n",
      "Interaction training epoch: 120, train loss: 0.25520, val loss: 0.26984\n",
      "Interaction training epoch: 121, train loss: 0.25352, val loss: 0.27188\n",
      "Interaction training epoch: 122, train loss: 0.25323, val loss: 0.26725\n",
      "Interaction training epoch: 123, train loss: 0.25952, val loss: 0.27734\n",
      "Interaction training epoch: 124, train loss: 0.25886, val loss: 0.27726\n",
      "Interaction training epoch: 125, train loss: 0.25556, val loss: 0.27338\n",
      "Interaction training epoch: 126, train loss: 0.25452, val loss: 0.27285\n",
      "Interaction training epoch: 127, train loss: 0.25406, val loss: 0.27208\n",
      "Interaction training epoch: 128, train loss: 0.25296, val loss: 0.27270\n",
      "Interaction training epoch: 129, train loss: 0.25634, val loss: 0.27676\n",
      "Interaction training epoch: 130, train loss: 0.25731, val loss: 0.27483\n",
      "Interaction training epoch: 131, train loss: 0.25160, val loss: 0.27198\n",
      "Interaction training epoch: 132, train loss: 0.25705, val loss: 0.27568\n",
      "Interaction training epoch: 133, train loss: 0.25310, val loss: 0.27274\n",
      "Interaction training epoch: 134, train loss: 0.25182, val loss: 0.27004\n",
      "Interaction training epoch: 135, train loss: 0.25213, val loss: 0.26825\n",
      "Interaction training epoch: 136, train loss: 0.25691, val loss: 0.27566\n",
      "Interaction training epoch: 137, train loss: 0.25745, val loss: 0.27819\n",
      "Interaction training epoch: 138, train loss: 0.24992, val loss: 0.26791\n",
      "Interaction training epoch: 139, train loss: 0.25398, val loss: 0.27616\n",
      "Interaction training epoch: 140, train loss: 0.25209, val loss: 0.27622\n",
      "Interaction training epoch: 141, train loss: 0.25015, val loss: 0.26787\n",
      "Interaction training epoch: 142, train loss: 0.25524, val loss: 0.27883\n",
      "Interaction training epoch: 143, train loss: 0.25228, val loss: 0.27547\n",
      "Interaction training epoch: 144, train loss: 0.24997, val loss: 0.26844\n",
      "Interaction training epoch: 145, train loss: 0.25338, val loss: 0.27592\n",
      "Interaction training epoch: 146, train loss: 0.25305, val loss: 0.27616\n",
      "Interaction training epoch: 147, train loss: 0.25500, val loss: 0.27482\n",
      "Interaction training epoch: 148, train loss: 0.25213, val loss: 0.27299\n",
      "Interaction training epoch: 149, train loss: 0.25202, val loss: 0.27559\n",
      "Interaction training epoch: 150, train loss: 0.24938, val loss: 0.27439\n",
      "Interaction training epoch: 151, train loss: 0.25096, val loss: 0.27406\n",
      "Interaction training epoch: 152, train loss: 0.24714, val loss: 0.26853\n",
      "Interaction training epoch: 153, train loss: 0.24857, val loss: 0.27422\n",
      "Interaction training epoch: 154, train loss: 0.25001, val loss: 0.27558\n",
      "Interaction training epoch: 155, train loss: 0.25019, val loss: 0.27165\n",
      "Interaction training epoch: 156, train loss: 0.24972, val loss: 0.27882\n",
      "Interaction training epoch: 157, train loss: 0.24887, val loss: 0.27666\n",
      "Interaction training epoch: 158, train loss: 0.24871, val loss: 0.27518\n",
      "Interaction training epoch: 159, train loss: 0.24890, val loss: 0.27700\n",
      "Interaction training epoch: 160, train loss: 0.24708, val loss: 0.27679\n",
      "Interaction training epoch: 161, train loss: 0.24913, val loss: 0.27381\n",
      "Interaction training epoch: 162, train loss: 0.25115, val loss: 0.28182\n",
      "Interaction training epoch: 163, train loss: 0.24972, val loss: 0.27266\n",
      "Interaction training epoch: 164, train loss: 0.25134, val loss: 0.28271\n",
      "Interaction training epoch: 165, train loss: 0.24662, val loss: 0.27263\n",
      "Interaction training epoch: 166, train loss: 0.25066, val loss: 0.28036\n",
      "Interaction training epoch: 167, train loss: 0.24637, val loss: 0.27702\n",
      "Interaction training epoch: 168, train loss: 0.24569, val loss: 0.27020\n",
      "Interaction training epoch: 169, train loss: 0.24845, val loss: 0.28004\n",
      "Interaction training epoch: 170, train loss: 0.24433, val loss: 0.27165\n",
      "Interaction training epoch: 171, train loss: 0.24999, val loss: 0.27956\n",
      "Interaction training epoch: 172, train loss: 0.24967, val loss: 0.28265\n",
      "Interaction training epoch: 173, train loss: 0.24406, val loss: 0.27162\n",
      "Interaction training epoch: 174, train loss: 0.24559, val loss: 0.27483\n",
      "Interaction training epoch: 175, train loss: 0.25149, val loss: 0.28414\n",
      "Interaction training epoch: 176, train loss: 0.24435, val loss: 0.27355\n",
      "Interaction training epoch: 177, train loss: 0.24514, val loss: 0.27815\n",
      "Interaction training epoch: 178, train loss: 0.24822, val loss: 0.28090\n",
      "Interaction training epoch: 179, train loss: 0.24581, val loss: 0.28071\n",
      "Interaction training epoch: 180, train loss: 0.24715, val loss: 0.27596\n",
      "Interaction training epoch: 181, train loss: 0.24687, val loss: 0.27536\n",
      "Interaction training epoch: 182, train loss: 0.24580, val loss: 0.27719\n",
      "Interaction training epoch: 183, train loss: 0.24610, val loss: 0.27810\n",
      "Interaction training epoch: 184, train loss: 0.24532, val loss: 0.27422\n",
      "Interaction training epoch: 185, train loss: 0.24548, val loss: 0.27645\n",
      "Interaction training epoch: 186, train loss: 0.24330, val loss: 0.27302\n",
      "Interaction training epoch: 187, train loss: 0.24686, val loss: 0.27917\n",
      "Interaction training epoch: 188, train loss: 0.24294, val loss: 0.27199\n",
      "Interaction training epoch: 189, train loss: 0.24522, val loss: 0.27998\n",
      "Interaction training epoch: 190, train loss: 0.24275, val loss: 0.27176\n",
      "Interaction training epoch: 191, train loss: 0.24174, val loss: 0.27310\n",
      "Interaction training epoch: 192, train loss: 0.24435, val loss: 0.27555\n",
      "Interaction training epoch: 193, train loss: 0.24325, val loss: 0.27705\n",
      "Interaction training epoch: 194, train loss: 0.24695, val loss: 0.28143\n",
      "Interaction training epoch: 195, train loss: 0.24677, val loss: 0.28126\n",
      "Interaction training epoch: 196, train loss: 0.24455, val loss: 0.27404\n",
      "Interaction training epoch: 197, train loss: 0.24122, val loss: 0.27640\n",
      "Interaction training epoch: 198, train loss: 0.24549, val loss: 0.27827\n",
      "Interaction training epoch: 199, train loss: 0.24357, val loss: 0.27302\n",
      "Interaction training epoch: 200, train loss: 0.24260, val loss: 0.27734\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########3 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.24881, val loss: 0.27891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 2, train loss: 0.24688, val loss: 0.27332\n",
      "Interaction tuning epoch: 3, train loss: 0.24548, val loss: 0.27205\n",
      "Interaction tuning epoch: 4, train loss: 0.24895, val loss: 0.28301\n",
      "Interaction tuning epoch: 5, train loss: 0.24416, val loss: 0.27138\n",
      "Interaction tuning epoch: 6, train loss: 0.24399, val loss: 0.26980\n",
      "Interaction tuning epoch: 7, train loss: 0.25281, val loss: 0.28251\n",
      "Interaction tuning epoch: 8, train loss: 0.24259, val loss: 0.27397\n",
      "Interaction tuning epoch: 9, train loss: 0.24799, val loss: 0.27800\n",
      "Interaction tuning epoch: 10, train loss: 0.24365, val loss: 0.26857\n",
      "Interaction tuning epoch: 11, train loss: 0.24549, val loss: 0.27980\n",
      "Interaction tuning epoch: 12, train loss: 0.24285, val loss: 0.26819\n",
      "Interaction tuning epoch: 13, train loss: 0.24450, val loss: 0.27050\n",
      "Interaction tuning epoch: 14, train loss: 0.24231, val loss: 0.27541\n",
      "Interaction tuning epoch: 15, train loss: 0.24167, val loss: 0.27004\n",
      "Interaction tuning epoch: 16, train loss: 0.24303, val loss: 0.26758\n",
      "Interaction tuning epoch: 17, train loss: 0.24538, val loss: 0.28050\n",
      "Interaction tuning epoch: 18, train loss: 0.24736, val loss: 0.27290\n",
      "Interaction tuning epoch: 19, train loss: 0.24292, val loss: 0.27399\n",
      "Interaction tuning epoch: 20, train loss: 0.23900, val loss: 0.26852\n",
      "Interaction tuning epoch: 21, train loss: 0.24209, val loss: 0.26937\n",
      "Interaction tuning epoch: 22, train loss: 0.24328, val loss: 0.27450\n",
      "Interaction tuning epoch: 23, train loss: 0.24206, val loss: 0.27023\n",
      "Interaction tuning epoch: 24, train loss: 0.24493, val loss: 0.27346\n",
      "Interaction tuning epoch: 25, train loss: 0.23827, val loss: 0.26873\n",
      "Interaction tuning epoch: 26, train loss: 0.24684, val loss: 0.27809\n",
      "Interaction tuning epoch: 27, train loss: 0.23898, val loss: 0.26875\n",
      "Interaction tuning epoch: 28, train loss: 0.24679, val loss: 0.27360\n",
      "Interaction tuning epoch: 29, train loss: 0.24053, val loss: 0.27178\n",
      "Interaction tuning epoch: 30, train loss: 0.24644, val loss: 0.27554\n",
      "Interaction tuning epoch: 31, train loss: 0.24014, val loss: 0.27189\n",
      "Interaction tuning epoch: 32, train loss: 0.24343, val loss: 0.27157\n",
      "Interaction tuning epoch: 33, train loss: 0.23908, val loss: 0.26958\n",
      "Interaction tuning epoch: 34, train loss: 0.24826, val loss: 0.28193\n",
      "Interaction tuning epoch: 35, train loss: 0.24027, val loss: 0.27002\n",
      "Interaction tuning epoch: 36, train loss: 0.24065, val loss: 0.27077\n",
      "Interaction tuning epoch: 37, train loss: 0.23959, val loss: 0.26968\n",
      "Interaction tuning epoch: 38, train loss: 0.24392, val loss: 0.27976\n",
      "Interaction tuning epoch: 39, train loss: 0.23999, val loss: 0.26714\n",
      "Interaction tuning epoch: 40, train loss: 0.24198, val loss: 0.27385\n",
      "Interaction tuning epoch: 41, train loss: 0.24328, val loss: 0.26787\n",
      "Interaction tuning epoch: 42, train loss: 0.23839, val loss: 0.26844\n",
      "Interaction tuning epoch: 43, train loss: 0.24126, val loss: 0.27283\n",
      "Interaction tuning epoch: 44, train loss: 0.24077, val loss: 0.26982\n",
      "Interaction tuning epoch: 45, train loss: 0.24479, val loss: 0.27430\n",
      "Interaction tuning epoch: 46, train loss: 0.23665, val loss: 0.26156\n",
      "Interaction tuning epoch: 47, train loss: 0.24015, val loss: 0.27323\n",
      "Interaction tuning epoch: 48, train loss: 0.23814, val loss: 0.27011\n",
      "Interaction tuning epoch: 49, train loss: 0.23848, val loss: 0.26671\n",
      "Interaction tuning epoch: 50, train loss: 0.23827, val loss: 0.26929\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 36.81663990020752\n",
      "After the gam stage, training error is 0.23827 , validation error is 0.26929\n",
      "missing value counts: 99179\n",
      "[SoftImpute] Max Singular Value of X_init = 3.609985\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.201024 validation BCE=0.265367,rank=5\n",
      "[SoftImpute] Iter 2: observed BCE=0.196452 validation BCE=0.265199,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.193510 validation BCE=0.275349,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.191158 validation BCE=0.274172,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.189731 validation BCE=0.273581,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.188447 validation BCE=0.273237,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.187655 validation BCE=0.272304,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.186847 validation BCE=0.272132,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.186378 validation BCE=0.271700,rank=5\n",
      "[SoftImpute] Iter 10: observed BCE=0.186086 validation BCE=0.262821,rank=5\n",
      "[SoftImpute] Iter 11: observed BCE=0.185924 validation BCE=0.260639,rank=5\n",
      "[SoftImpute] Iter 12: observed BCE=0.185738 validation BCE=0.259285,rank=5\n",
      "[SoftImpute] Iter 13: observed BCE=0.185810 validation BCE=0.258732,rank=5\n",
      "[SoftImpute] Iter 14: observed BCE=0.185970 validation BCE=0.258042,rank=5\n",
      "[SoftImpute] Iter 15: observed BCE=0.185972 validation BCE=0.258312,rank=5\n",
      "[SoftImpute] Iter 16: observed BCE=0.186093 validation BCE=0.258154,rank=5\n",
      "[SoftImpute] Iter 17: observed BCE=0.185849 validation BCE=0.257871,rank=5\n",
      "[SoftImpute] Iter 18: observed BCE=0.186132 validation BCE=0.257560,rank=5\n",
      "[SoftImpute] Iter 19: observed BCE=0.185939 validation BCE=0.257404,rank=5\n",
      "[SoftImpute] Iter 20: observed BCE=0.185953 validation BCE=0.257141,rank=5\n",
      "[SoftImpute] Iter 21: observed BCE=0.186114 validation BCE=0.257119,rank=5\n",
      "[SoftImpute] Iter 22: observed BCE=0.186326 validation BCE=0.257091,rank=5\n",
      "[SoftImpute] Iter 23: observed BCE=0.186280 validation BCE=0.257476,rank=5\n",
      "[SoftImpute] Iter 24: observed BCE=0.186314 validation BCE=0.257045,rank=5\n",
      "[SoftImpute] Iter 25: observed BCE=0.185965 validation BCE=0.257291,rank=5\n",
      "[SoftImpute] Iter 26: observed BCE=0.185933 validation BCE=0.257372,rank=5\n",
      "[SoftImpute] Iter 27: observed BCE=0.186342 validation BCE=0.257363,rank=5\n",
      "[SoftImpute] Iter 28: observed BCE=0.186035 validation BCE=0.257485,rank=5\n",
      "[SoftImpute] Iter 29: observed BCE=0.185890 validation BCE=0.257382,rank=5\n",
      "[SoftImpute] Iter 30: observed BCE=0.185953 validation BCE=0.256747,rank=5\n",
      "[SoftImpute] Iter 31: observed BCE=0.185877 validation BCE=0.257259,rank=5\n",
      "[SoftImpute] Iter 32: observed BCE=0.185798 validation BCE=0.256894,rank=5\n",
      "[SoftImpute] Iter 33: observed BCE=0.185829 validation BCE=0.257029,rank=5\n",
      "[SoftImpute] Iter 34: observed BCE=0.185743 validation BCE=0.256672,rank=5\n",
      "[SoftImpute] Iter 35: observed BCE=0.185724 validation BCE=0.257045,rank=5\n",
      "[SoftImpute] Iter 36: observed BCE=0.185659 validation BCE=0.256971,rank=5\n",
      "[SoftImpute] Iter 37: observed BCE=0.185457 validation BCE=0.256682,rank=5\n",
      "[SoftImpute] Iter 38: observed BCE=0.185271 validation BCE=0.257279,rank=5\n",
      "[SoftImpute] Iter 39: observed BCE=0.185336 validation BCE=0.257390,rank=5\n",
      "[SoftImpute] Iter 40: observed BCE=0.185211 validation BCE=0.257191,rank=5\n",
      "[SoftImpute] Iter 41: observed BCE=0.185296 validation BCE=0.257505,rank=5\n",
      "[SoftImpute] Iter 42: observed BCE=0.185140 validation BCE=0.256930,rank=5\n",
      "[SoftImpute] Iter 43: observed BCE=0.185216 validation BCE=0.257677,rank=5\n",
      "[SoftImpute] Iter 44: observed BCE=0.185236 validation BCE=0.257719,rank=5\n",
      "[SoftImpute] Iter 45: observed BCE=0.185154 validation BCE=0.257683,rank=5\n",
      "[SoftImpute] Iter 46: observed BCE=0.185207 validation BCE=0.257870,rank=5\n",
      "[SoftImpute] Iter 47: observed BCE=0.185014 validation BCE=0.257682,rank=5\n",
      "[SoftImpute] Iter 48: observed BCE=0.184949 validation BCE=0.257777,rank=5\n",
      "[SoftImpute] Iter 49: observed BCE=0.184773 validation BCE=0.258292,rank=5\n",
      "[SoftImpute] Iter 50: observed BCE=0.184567 validation BCE=0.258339,rank=5\n",
      "[SoftImpute] Stopped after iteration 50 for lambda=0.072200\n",
      "final num of user group: 23\n",
      "final num of item group: 32\n",
      "change mode state : True\n",
      "time cost: 18.642451524734497\n",
      "After the matrix factor stage, training error is 0.18457, validation error is 0.25834\n",
      "3\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68418, val loss: 0.68455\n",
      "Main effects training epoch: 2, train loss: 0.67784, val loss: 0.67956\n",
      "Main effects training epoch: 3, train loss: 0.67268, val loss: 0.67589\n",
      "Main effects training epoch: 4, train loss: 0.66928, val loss: 0.67281\n",
      "Main effects training epoch: 5, train loss: 0.66440, val loss: 0.66785\n",
      "Main effects training epoch: 6, train loss: 0.65336, val loss: 0.65715\n",
      "Main effects training epoch: 7, train loss: 0.62792, val loss: 0.63225\n",
      "Main effects training epoch: 8, train loss: 0.58662, val loss: 0.58959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 9, train loss: 0.54239, val loss: 0.54107\n",
      "Main effects training epoch: 10, train loss: 0.53158, val loss: 0.52133\n",
      "Main effects training epoch: 11, train loss: 0.52856, val loss: 0.51610\n",
      "Main effects training epoch: 12, train loss: 0.52431, val loss: 0.51244\n",
      "Main effects training epoch: 13, train loss: 0.52230, val loss: 0.51132\n",
      "Main effects training epoch: 14, train loss: 0.52214, val loss: 0.51203\n",
      "Main effects training epoch: 15, train loss: 0.52201, val loss: 0.51056\n",
      "Main effects training epoch: 16, train loss: 0.52079, val loss: 0.50904\n",
      "Main effects training epoch: 17, train loss: 0.52125, val loss: 0.50975\n",
      "Main effects training epoch: 18, train loss: 0.52104, val loss: 0.50928\n",
      "Main effects training epoch: 19, train loss: 0.52069, val loss: 0.50875\n",
      "Main effects training epoch: 20, train loss: 0.52144, val loss: 0.51001\n",
      "Main effects training epoch: 21, train loss: 0.52255, val loss: 0.51029\n",
      "Main effects training epoch: 22, train loss: 0.52171, val loss: 0.51137\n",
      "Main effects training epoch: 23, train loss: 0.52160, val loss: 0.50990\n",
      "Main effects training epoch: 24, train loss: 0.52080, val loss: 0.50941\n",
      "Main effects training epoch: 25, train loss: 0.52050, val loss: 0.50881\n",
      "Main effects training epoch: 26, train loss: 0.52073, val loss: 0.50938\n",
      "Main effects training epoch: 27, train loss: 0.52052, val loss: 0.50865\n",
      "Main effects training epoch: 28, train loss: 0.52072, val loss: 0.50876\n",
      "Main effects training epoch: 29, train loss: 0.52194, val loss: 0.51126\n",
      "Main effects training epoch: 30, train loss: 0.52092, val loss: 0.50913\n",
      "Main effects training epoch: 31, train loss: 0.52149, val loss: 0.51070\n",
      "Main effects training epoch: 32, train loss: 0.52047, val loss: 0.50874\n",
      "Main effects training epoch: 33, train loss: 0.52049, val loss: 0.50902\n",
      "Main effects training epoch: 34, train loss: 0.52061, val loss: 0.50931\n",
      "Main effects training epoch: 35, train loss: 0.52036, val loss: 0.50987\n",
      "Main effects training epoch: 36, train loss: 0.52016, val loss: 0.50870\n",
      "Main effects training epoch: 37, train loss: 0.52024, val loss: 0.50909\n",
      "Main effects training epoch: 38, train loss: 0.52006, val loss: 0.50856\n",
      "Main effects training epoch: 39, train loss: 0.52059, val loss: 0.50890\n",
      "Main effects training epoch: 40, train loss: 0.52162, val loss: 0.51142\n",
      "Main effects training epoch: 41, train loss: 0.52059, val loss: 0.50964\n",
      "Main effects training epoch: 42, train loss: 0.52068, val loss: 0.51031\n",
      "Main effects training epoch: 43, train loss: 0.52179, val loss: 0.51051\n",
      "Main effects training epoch: 44, train loss: 0.52237, val loss: 0.51116\n",
      "Main effects training epoch: 45, train loss: 0.52242, val loss: 0.51299\n",
      "Main effects training epoch: 46, train loss: 0.52146, val loss: 0.51039\n",
      "Main effects training epoch: 47, train loss: 0.52021, val loss: 0.51038\n",
      "Main effects training epoch: 48, train loss: 0.52085, val loss: 0.50932\n",
      "Main effects training epoch: 49, train loss: 0.52142, val loss: 0.51076\n",
      "Main effects training epoch: 50, train loss: 0.52086, val loss: 0.50955\n",
      "Main effects training epoch: 51, train loss: 0.51995, val loss: 0.50987\n",
      "Main effects training epoch: 52, train loss: 0.51972, val loss: 0.50953\n",
      "Main effects training epoch: 53, train loss: 0.51965, val loss: 0.50985\n",
      "Main effects training epoch: 54, train loss: 0.51977, val loss: 0.50895\n",
      "Main effects training epoch: 55, train loss: 0.52030, val loss: 0.50981\n",
      "Main effects training epoch: 56, train loss: 0.52032, val loss: 0.51060\n",
      "Main effects training epoch: 57, train loss: 0.51972, val loss: 0.50979\n",
      "Main effects training epoch: 58, train loss: 0.51964, val loss: 0.50911\n",
      "Main effects training epoch: 59, train loss: 0.52085, val loss: 0.50960\n",
      "Main effects training epoch: 60, train loss: 0.51983, val loss: 0.51072\n",
      "Main effects training epoch: 61, train loss: 0.51942, val loss: 0.50966\n",
      "Main effects training epoch: 62, train loss: 0.51980, val loss: 0.51040\n",
      "Main effects training epoch: 63, train loss: 0.51938, val loss: 0.50915\n",
      "Main effects training epoch: 64, train loss: 0.51892, val loss: 0.50939\n",
      "Main effects training epoch: 65, train loss: 0.51940, val loss: 0.51015\n",
      "Main effects training epoch: 66, train loss: 0.51908, val loss: 0.51044\n",
      "Main effects training epoch: 67, train loss: 0.51938, val loss: 0.50939\n",
      "Main effects training epoch: 68, train loss: 0.51884, val loss: 0.50948\n",
      "Main effects training epoch: 69, train loss: 0.52014, val loss: 0.51147\n",
      "Main effects training epoch: 70, train loss: 0.51945, val loss: 0.51065\n",
      "Main effects training epoch: 71, train loss: 0.51915, val loss: 0.51031\n",
      "Main effects training epoch: 72, train loss: 0.51876, val loss: 0.51026\n",
      "Main effects training epoch: 73, train loss: 0.51867, val loss: 0.50927\n",
      "Main effects training epoch: 74, train loss: 0.51831, val loss: 0.50970\n",
      "Main effects training epoch: 75, train loss: 0.51856, val loss: 0.50970\n",
      "Main effects training epoch: 76, train loss: 0.51849, val loss: 0.50955\n",
      "Main effects training epoch: 77, train loss: 0.51842, val loss: 0.51060\n",
      "Main effects training epoch: 78, train loss: 0.51824, val loss: 0.50874\n",
      "Main effects training epoch: 79, train loss: 0.51790, val loss: 0.50908\n",
      "Main effects training epoch: 80, train loss: 0.51821, val loss: 0.51024\n",
      "Main effects training epoch: 81, train loss: 0.51802, val loss: 0.50943\n",
      "Main effects training epoch: 82, train loss: 0.51788, val loss: 0.50987\n",
      "Main effects training epoch: 83, train loss: 0.51798, val loss: 0.50946\n",
      "Main effects training epoch: 84, train loss: 0.51768, val loss: 0.50900\n",
      "Main effects training epoch: 85, train loss: 0.51770, val loss: 0.50970\n",
      "Main effects training epoch: 86, train loss: 0.51734, val loss: 0.50970\n",
      "Main effects training epoch: 87, train loss: 0.51732, val loss: 0.50900\n",
      "Main effects training epoch: 88, train loss: 0.51774, val loss: 0.50958\n",
      "Main effects training epoch: 89, train loss: 0.51748, val loss: 0.50923\n",
      "Main effects training epoch: 90, train loss: 0.51709, val loss: 0.50966\n",
      "Main effects training epoch: 91, train loss: 0.51713, val loss: 0.50917\n",
      "Main effects training epoch: 92, train loss: 0.51694, val loss: 0.50979\n",
      "Main effects training epoch: 93, train loss: 0.51661, val loss: 0.50794\n",
      "Main effects training epoch: 94, train loss: 0.51673, val loss: 0.51015\n",
      "Main effects training epoch: 95, train loss: 0.51642, val loss: 0.50918\n",
      "Main effects training epoch: 96, train loss: 0.51670, val loss: 0.50884\n",
      "Main effects training epoch: 97, train loss: 0.51614, val loss: 0.50872\n",
      "Main effects training epoch: 98, train loss: 0.51653, val loss: 0.50972\n",
      "Main effects training epoch: 99, train loss: 0.51648, val loss: 0.50953\n",
      "Main effects training epoch: 100, train loss: 0.51668, val loss: 0.50977\n",
      "Main effects training epoch: 101, train loss: 0.51749, val loss: 0.51051\n",
      "Main effects training epoch: 102, train loss: 0.51699, val loss: 0.51070\n",
      "Main effects training epoch: 103, train loss: 0.51627, val loss: 0.50969\n",
      "Main effects training epoch: 104, train loss: 0.51608, val loss: 0.50960\n",
      "Main effects training epoch: 105, train loss: 0.51740, val loss: 0.51090\n",
      "Main effects training epoch: 106, train loss: 0.51656, val loss: 0.51025\n",
      "Main effects training epoch: 107, train loss: 0.51718, val loss: 0.51090\n",
      "Main effects training epoch: 108, train loss: 0.51635, val loss: 0.50984\n",
      "Main effects training epoch: 109, train loss: 0.51688, val loss: 0.51063\n",
      "Main effects training epoch: 110, train loss: 0.51694, val loss: 0.51177\n",
      "Main effects training epoch: 111, train loss: 0.51589, val loss: 0.50937\n",
      "Main effects training epoch: 112, train loss: 0.51523, val loss: 0.50937\n",
      "Main effects training epoch: 113, train loss: 0.51511, val loss: 0.50934\n",
      "Main effects training epoch: 114, train loss: 0.51512, val loss: 0.50899\n",
      "Main effects training epoch: 115, train loss: 0.51519, val loss: 0.50930\n",
      "Main effects training epoch: 116, train loss: 0.51560, val loss: 0.50963\n",
      "Main effects training epoch: 117, train loss: 0.51537, val loss: 0.50977\n",
      "Main effects training epoch: 118, train loss: 0.51533, val loss: 0.50955\n",
      "Main effects training epoch: 119, train loss: 0.51556, val loss: 0.51056\n",
      "Main effects training epoch: 120, train loss: 0.51554, val loss: 0.50950\n",
      "Main effects training epoch: 121, train loss: 0.51536, val loss: 0.50875\n",
      "Main effects training epoch: 122, train loss: 0.51547, val loss: 0.51155\n",
      "Main effects training epoch: 123, train loss: 0.51623, val loss: 0.51146\n",
      "Main effects training epoch: 124, train loss: 0.51539, val loss: 0.51083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 125, train loss: 0.51524, val loss: 0.50952\n",
      "Main effects training epoch: 126, train loss: 0.51555, val loss: 0.51175\n",
      "Main effects training epoch: 127, train loss: 0.51484, val loss: 0.51054\n",
      "Main effects training epoch: 128, train loss: 0.51494, val loss: 0.51073\n",
      "Main effects training epoch: 129, train loss: 0.51471, val loss: 0.50957\n",
      "Main effects training epoch: 130, train loss: 0.51449, val loss: 0.50997\n",
      "Main effects training epoch: 131, train loss: 0.51454, val loss: 0.51010\n",
      "Main effects training epoch: 132, train loss: 0.51492, val loss: 0.51117\n",
      "Main effects training epoch: 133, train loss: 0.51449, val loss: 0.51108\n",
      "Main effects training epoch: 134, train loss: 0.51415, val loss: 0.50875\n",
      "Main effects training epoch: 135, train loss: 0.51428, val loss: 0.51065\n",
      "Main effects training epoch: 136, train loss: 0.51394, val loss: 0.51133\n",
      "Main effects training epoch: 137, train loss: 0.51382, val loss: 0.50993\n",
      "Main effects training epoch: 138, train loss: 0.51394, val loss: 0.50924\n",
      "Main effects training epoch: 139, train loss: 0.51371, val loss: 0.50991\n",
      "Main effects training epoch: 140, train loss: 0.51428, val loss: 0.51142\n",
      "Main effects training epoch: 141, train loss: 0.51408, val loss: 0.51079\n",
      "Main effects training epoch: 142, train loss: 0.51374, val loss: 0.51095\n",
      "Main effects training epoch: 143, train loss: 0.51350, val loss: 0.51029\n",
      "Main effects training epoch: 144, train loss: 0.51372, val loss: 0.51087\n",
      "Main effects training epoch: 145, train loss: 0.51395, val loss: 0.51138\n",
      "Main effects training epoch: 146, train loss: 0.51351, val loss: 0.51026\n",
      "Main effects training epoch: 147, train loss: 0.51350, val loss: 0.50986\n",
      "Main effects training epoch: 148, train loss: 0.51358, val loss: 0.51128\n",
      "Main effects training epoch: 149, train loss: 0.51367, val loss: 0.51214\n",
      "Main effects training epoch: 150, train loss: 0.51378, val loss: 0.51231\n",
      "Main effects training epoch: 151, train loss: 0.51364, val loss: 0.51063\n",
      "Main effects training epoch: 152, train loss: 0.51354, val loss: 0.50997\n",
      "Main effects training epoch: 153, train loss: 0.51357, val loss: 0.51181\n",
      "Main effects training epoch: 154, train loss: 0.51347, val loss: 0.51191\n",
      "Main effects training epoch: 155, train loss: 0.51305, val loss: 0.51026\n",
      "Main effects training epoch: 156, train loss: 0.51316, val loss: 0.51046\n",
      "Main effects training epoch: 157, train loss: 0.51311, val loss: 0.51182\n",
      "Main effects training epoch: 158, train loss: 0.51288, val loss: 0.51097\n",
      "Main effects training epoch: 159, train loss: 0.51414, val loss: 0.51243\n",
      "Main effects training epoch: 160, train loss: 0.51433, val loss: 0.51264\n",
      "Main effects training epoch: 161, train loss: 0.51478, val loss: 0.51344\n",
      "Main effects training epoch: 162, train loss: 0.51368, val loss: 0.51238\n",
      "Main effects training epoch: 163, train loss: 0.51322, val loss: 0.51200\n",
      "Main effects training epoch: 164, train loss: 0.51262, val loss: 0.51130\n",
      "Main effects training epoch: 165, train loss: 0.51267, val loss: 0.51048\n",
      "Main effects training epoch: 166, train loss: 0.51267, val loss: 0.50998\n",
      "Main effects training epoch: 167, train loss: 0.51294, val loss: 0.51203\n",
      "Main effects training epoch: 168, train loss: 0.51228, val loss: 0.51153\n",
      "Main effects training epoch: 169, train loss: 0.51251, val loss: 0.51083\n",
      "Main effects training epoch: 170, train loss: 0.51276, val loss: 0.51157\n",
      "Main effects training epoch: 171, train loss: 0.51219, val loss: 0.51124\n",
      "Main effects training epoch: 172, train loss: 0.51247, val loss: 0.51130\n",
      "Main effects training epoch: 173, train loss: 0.51245, val loss: 0.51278\n",
      "Main effects training epoch: 174, train loss: 0.51223, val loss: 0.51184\n",
      "Main effects training epoch: 175, train loss: 0.51221, val loss: 0.51227\n",
      "Main effects training epoch: 176, train loss: 0.51198, val loss: 0.51060\n",
      "Main effects training epoch: 177, train loss: 0.51223, val loss: 0.51207\n",
      "Main effects training epoch: 178, train loss: 0.51213, val loss: 0.51173\n",
      "Main effects training epoch: 179, train loss: 0.51278, val loss: 0.51306\n",
      "Main effects training epoch: 180, train loss: 0.51234, val loss: 0.51257\n",
      "Main effects training epoch: 181, train loss: 0.51195, val loss: 0.51212\n",
      "Main effects training epoch: 182, train loss: 0.51199, val loss: 0.51234\n",
      "Main effects training epoch: 183, train loss: 0.51174, val loss: 0.51185\n",
      "Main effects training epoch: 184, train loss: 0.51169, val loss: 0.51264\n",
      "Main effects training epoch: 185, train loss: 0.51167, val loss: 0.51245\n",
      "Main effects training epoch: 186, train loss: 0.51178, val loss: 0.51176\n",
      "Main effects training epoch: 187, train loss: 0.51179, val loss: 0.51301\n",
      "Main effects training epoch: 188, train loss: 0.51140, val loss: 0.51217\n",
      "Main effects training epoch: 189, train loss: 0.51187, val loss: 0.51202\n",
      "Main effects training epoch: 190, train loss: 0.51180, val loss: 0.51350\n",
      "Main effects training epoch: 191, train loss: 0.51118, val loss: 0.51307\n",
      "Main effects training epoch: 192, train loss: 0.51142, val loss: 0.51150\n",
      "Main effects training epoch: 193, train loss: 0.51135, val loss: 0.51185\n",
      "Main effects training epoch: 194, train loss: 0.51129, val loss: 0.51230\n",
      "Early stop at epoch 194, with validation loss: 0.51230\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51549, val loss: 0.51187\n",
      "Main effects tuning epoch: 2, train loss: 0.51546, val loss: 0.51203\n",
      "Main effects tuning epoch: 3, train loss: 0.51590, val loss: 0.51200\n",
      "Main effects tuning epoch: 4, train loss: 0.51562, val loss: 0.51254\n",
      "Main effects tuning epoch: 5, train loss: 0.51627, val loss: 0.51327\n",
      "Main effects tuning epoch: 6, train loss: 0.51575, val loss: 0.51258\n",
      "Main effects tuning epoch: 7, train loss: 0.51521, val loss: 0.51121\n",
      "Main effects tuning epoch: 8, train loss: 0.51500, val loss: 0.51162\n",
      "Main effects tuning epoch: 9, train loss: 0.51496, val loss: 0.51242\n",
      "Main effects tuning epoch: 10, train loss: 0.51518, val loss: 0.51273\n",
      "Main effects tuning epoch: 11, train loss: 0.51544, val loss: 0.51234\n",
      "Main effects tuning epoch: 12, train loss: 0.51487, val loss: 0.51232\n",
      "Main effects tuning epoch: 13, train loss: 0.51485, val loss: 0.51199\n",
      "Main effects tuning epoch: 14, train loss: 0.51513, val loss: 0.51276\n",
      "Main effects tuning epoch: 15, train loss: 0.51544, val loss: 0.51255\n",
      "Main effects tuning epoch: 16, train loss: 0.51519, val loss: 0.51281\n",
      "Main effects tuning epoch: 17, train loss: 0.51495, val loss: 0.51241\n",
      "Main effects tuning epoch: 18, train loss: 0.51541, val loss: 0.51348\n",
      "Main effects tuning epoch: 19, train loss: 0.51504, val loss: 0.51288\n",
      "Main effects tuning epoch: 20, train loss: 0.51503, val loss: 0.51214\n",
      "Main effects tuning epoch: 21, train loss: 0.51533, val loss: 0.51297\n",
      "Main effects tuning epoch: 22, train loss: 0.51499, val loss: 0.51321\n",
      "Main effects tuning epoch: 23, train loss: 0.51458, val loss: 0.51235\n",
      "Main effects tuning epoch: 24, train loss: 0.51443, val loss: 0.51219\n",
      "Main effects tuning epoch: 25, train loss: 0.51445, val loss: 0.51235\n",
      "Main effects tuning epoch: 26, train loss: 0.51468, val loss: 0.51281\n",
      "Main effects tuning epoch: 27, train loss: 0.51521, val loss: 0.51245\n",
      "Main effects tuning epoch: 28, train loss: 0.51431, val loss: 0.51326\n",
      "Main effects tuning epoch: 29, train loss: 0.51465, val loss: 0.51176\n",
      "Main effects tuning epoch: 30, train loss: 0.51431, val loss: 0.51284\n",
      "Main effects tuning epoch: 31, train loss: 0.51418, val loss: 0.51263\n",
      "Main effects tuning epoch: 32, train loss: 0.51410, val loss: 0.51267\n",
      "Main effects tuning epoch: 33, train loss: 0.51459, val loss: 0.51253\n",
      "Main effects tuning epoch: 34, train loss: 0.51446, val loss: 0.51250\n",
      "Main effects tuning epoch: 35, train loss: 0.51387, val loss: 0.51228\n",
      "Main effects tuning epoch: 36, train loss: 0.51459, val loss: 0.51313\n",
      "Main effects tuning epoch: 37, train loss: 0.51443, val loss: 0.51302\n",
      "Main effects tuning epoch: 38, train loss: 0.51397, val loss: 0.51284\n",
      "Main effects tuning epoch: 39, train loss: 0.51381, val loss: 0.51161\n",
      "Main effects tuning epoch: 40, train loss: 0.51421, val loss: 0.51318\n",
      "Main effects tuning epoch: 41, train loss: 0.51390, val loss: 0.51279\n",
      "Main effects tuning epoch: 42, train loss: 0.51366, val loss: 0.51245\n",
      "Main effects tuning epoch: 43, train loss: 0.51376, val loss: 0.51213\n",
      "Main effects tuning epoch: 44, train loss: 0.51384, val loss: 0.51300\n",
      "Main effects tuning epoch: 45, train loss: 0.51408, val loss: 0.51242\n",
      "Main effects tuning epoch: 46, train loss: 0.51390, val loss: 0.51293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects tuning epoch: 47, train loss: 0.51350, val loss: 0.51211\n",
      "Main effects tuning epoch: 48, train loss: 0.51373, val loss: 0.51201\n",
      "Main effects tuning epoch: 49, train loss: 0.51379, val loss: 0.51282\n",
      "Main effects tuning epoch: 50, train loss: 0.51359, val loss: 0.51280\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.46531, val loss: 0.46967\n",
      "Interaction training epoch: 2, train loss: 0.50247, val loss: 0.52476\n",
      "Interaction training epoch: 3, train loss: 0.32265, val loss: 0.34170\n",
      "Interaction training epoch: 4, train loss: 0.33224, val loss: 0.35409\n",
      "Interaction training epoch: 5, train loss: 0.29555, val loss: 0.32809\n",
      "Interaction training epoch: 6, train loss: 0.28850, val loss: 0.31154\n",
      "Interaction training epoch: 7, train loss: 0.28512, val loss: 0.30920\n",
      "Interaction training epoch: 8, train loss: 0.28415, val loss: 0.31281\n",
      "Interaction training epoch: 9, train loss: 0.31852, val loss: 0.33740\n",
      "Interaction training epoch: 10, train loss: 0.28548, val loss: 0.31361\n",
      "Interaction training epoch: 11, train loss: 0.28734, val loss: 0.31611\n",
      "Interaction training epoch: 12, train loss: 0.28267, val loss: 0.31323\n",
      "Interaction training epoch: 13, train loss: 0.28043, val loss: 0.30828\n",
      "Interaction training epoch: 14, train loss: 0.27894, val loss: 0.30754\n",
      "Interaction training epoch: 15, train loss: 0.27436, val loss: 0.30311\n",
      "Interaction training epoch: 16, train loss: 0.27576, val loss: 0.30873\n",
      "Interaction training epoch: 17, train loss: 0.27281, val loss: 0.30153\n",
      "Interaction training epoch: 18, train loss: 0.27323, val loss: 0.30293\n",
      "Interaction training epoch: 19, train loss: 0.27293, val loss: 0.30216\n",
      "Interaction training epoch: 20, train loss: 0.27475, val loss: 0.30670\n",
      "Interaction training epoch: 21, train loss: 0.27777, val loss: 0.31416\n",
      "Interaction training epoch: 22, train loss: 0.27125, val loss: 0.30557\n",
      "Interaction training epoch: 23, train loss: 0.26997, val loss: 0.30485\n",
      "Interaction training epoch: 24, train loss: 0.27243, val loss: 0.30422\n",
      "Interaction training epoch: 25, train loss: 0.27428, val loss: 0.31147\n",
      "Interaction training epoch: 26, train loss: 0.27172, val loss: 0.30639\n",
      "Interaction training epoch: 27, train loss: 0.27980, val loss: 0.30815\n",
      "Interaction training epoch: 28, train loss: 0.27627, val loss: 0.31298\n",
      "Interaction training epoch: 29, train loss: 0.26963, val loss: 0.30473\n",
      "Interaction training epoch: 30, train loss: 0.27478, val loss: 0.30685\n",
      "Interaction training epoch: 31, train loss: 0.27152, val loss: 0.30542\n",
      "Interaction training epoch: 32, train loss: 0.26877, val loss: 0.30490\n",
      "Interaction training epoch: 33, train loss: 0.26920, val loss: 0.30484\n",
      "Interaction training epoch: 34, train loss: 0.26867, val loss: 0.30490\n",
      "Interaction training epoch: 35, train loss: 0.27279, val loss: 0.30720\n",
      "Interaction training epoch: 36, train loss: 0.27121, val loss: 0.30741\n",
      "Interaction training epoch: 37, train loss: 0.27035, val loss: 0.30654\n",
      "Interaction training epoch: 38, train loss: 0.26761, val loss: 0.30408\n",
      "Interaction training epoch: 39, train loss: 0.26492, val loss: 0.30026\n",
      "Interaction training epoch: 40, train loss: 0.27162, val loss: 0.30902\n",
      "Interaction training epoch: 41, train loss: 0.27526, val loss: 0.31259\n",
      "Interaction training epoch: 42, train loss: 0.26862, val loss: 0.30646\n",
      "Interaction training epoch: 43, train loss: 0.26315, val loss: 0.29932\n",
      "Interaction training epoch: 44, train loss: 0.26421, val loss: 0.29871\n",
      "Interaction training epoch: 45, train loss: 0.26302, val loss: 0.30053\n",
      "Interaction training epoch: 46, train loss: 0.26557, val loss: 0.30435\n",
      "Interaction training epoch: 47, train loss: 0.26467, val loss: 0.29501\n",
      "Interaction training epoch: 48, train loss: 0.26624, val loss: 0.30446\n",
      "Interaction training epoch: 49, train loss: 0.26215, val loss: 0.29867\n",
      "Interaction training epoch: 50, train loss: 0.26057, val loss: 0.30026\n",
      "Interaction training epoch: 51, train loss: 0.26425, val loss: 0.29771\n",
      "Interaction training epoch: 52, train loss: 0.26585, val loss: 0.30354\n",
      "Interaction training epoch: 53, train loss: 0.26085, val loss: 0.29767\n",
      "Interaction training epoch: 54, train loss: 0.26164, val loss: 0.29820\n",
      "Interaction training epoch: 55, train loss: 0.26219, val loss: 0.29499\n",
      "Interaction training epoch: 56, train loss: 0.25849, val loss: 0.29808\n",
      "Interaction training epoch: 57, train loss: 0.25877, val loss: 0.29728\n",
      "Interaction training epoch: 58, train loss: 0.26028, val loss: 0.29403\n",
      "Interaction training epoch: 59, train loss: 0.26286, val loss: 0.30465\n",
      "Interaction training epoch: 60, train loss: 0.25703, val loss: 0.29529\n",
      "Interaction training epoch: 61, train loss: 0.25869, val loss: 0.29591\n",
      "Interaction training epoch: 62, train loss: 0.25813, val loss: 0.29651\n",
      "Interaction training epoch: 63, train loss: 0.25530, val loss: 0.29491\n",
      "Interaction training epoch: 64, train loss: 0.25626, val loss: 0.29335\n",
      "Interaction training epoch: 65, train loss: 0.25881, val loss: 0.29843\n",
      "Interaction training epoch: 66, train loss: 0.25577, val loss: 0.29595\n",
      "Interaction training epoch: 67, train loss: 0.25597, val loss: 0.29849\n",
      "Interaction training epoch: 68, train loss: 0.25344, val loss: 0.28700\n",
      "Interaction training epoch: 69, train loss: 0.25793, val loss: 0.30056\n",
      "Interaction training epoch: 70, train loss: 0.25160, val loss: 0.29047\n",
      "Interaction training epoch: 71, train loss: 0.25583, val loss: 0.29667\n",
      "Interaction training epoch: 72, train loss: 0.26861, val loss: 0.31010\n",
      "Interaction training epoch: 73, train loss: 0.25361, val loss: 0.29146\n",
      "Interaction training epoch: 74, train loss: 0.25792, val loss: 0.29370\n",
      "Interaction training epoch: 75, train loss: 0.25367, val loss: 0.30102\n",
      "Interaction training epoch: 76, train loss: 0.24904, val loss: 0.28801\n",
      "Interaction training epoch: 77, train loss: 0.25627, val loss: 0.29669\n",
      "Interaction training epoch: 78, train loss: 0.25227, val loss: 0.29474\n",
      "Interaction training epoch: 79, train loss: 0.25125, val loss: 0.29426\n",
      "Interaction training epoch: 80, train loss: 0.25007, val loss: 0.28775\n",
      "Interaction training epoch: 81, train loss: 0.24988, val loss: 0.29389\n",
      "Interaction training epoch: 82, train loss: 0.25200, val loss: 0.29864\n",
      "Interaction training epoch: 83, train loss: 0.24863, val loss: 0.28945\n",
      "Interaction training epoch: 84, train loss: 0.25034, val loss: 0.29664\n",
      "Interaction training epoch: 85, train loss: 0.24957, val loss: 0.28866\n",
      "Interaction training epoch: 86, train loss: 0.24582, val loss: 0.29055\n",
      "Interaction training epoch: 87, train loss: 0.25068, val loss: 0.29043\n",
      "Interaction training epoch: 88, train loss: 0.24668, val loss: 0.29095\n",
      "Interaction training epoch: 89, train loss: 0.24616, val loss: 0.28878\n",
      "Interaction training epoch: 90, train loss: 0.25127, val loss: 0.29941\n",
      "Interaction training epoch: 91, train loss: 0.24357, val loss: 0.28837\n",
      "Interaction training epoch: 92, train loss: 0.25051, val loss: 0.29832\n",
      "Interaction training epoch: 93, train loss: 0.24550, val loss: 0.28780\n",
      "Interaction training epoch: 94, train loss: 0.24872, val loss: 0.29365\n",
      "Interaction training epoch: 95, train loss: 0.24830, val loss: 0.29323\n",
      "Interaction training epoch: 96, train loss: 0.24541, val loss: 0.29143\n",
      "Interaction training epoch: 97, train loss: 0.24948, val loss: 0.29884\n",
      "Interaction training epoch: 98, train loss: 0.25441, val loss: 0.29637\n",
      "Interaction training epoch: 99, train loss: 0.24645, val loss: 0.29650\n",
      "Interaction training epoch: 100, train loss: 0.24307, val loss: 0.29428\n",
      "Interaction training epoch: 101, train loss: 0.24114, val loss: 0.29214\n",
      "Interaction training epoch: 102, train loss: 0.24553, val loss: 0.28839\n",
      "Interaction training epoch: 103, train loss: 0.24059, val loss: 0.28981\n",
      "Interaction training epoch: 104, train loss: 0.24642, val loss: 0.29363\n",
      "Interaction training epoch: 105, train loss: 0.23978, val loss: 0.28613\n",
      "Interaction training epoch: 106, train loss: 0.23945, val loss: 0.28971\n",
      "Interaction training epoch: 107, train loss: 0.23771, val loss: 0.28794\n",
      "Interaction training epoch: 108, train loss: 0.24066, val loss: 0.28970\n",
      "Interaction training epoch: 109, train loss: 0.24192, val loss: 0.29103\n",
      "Interaction training epoch: 110, train loss: 0.24268, val loss: 0.29186\n",
      "Interaction training epoch: 111, train loss: 0.24496, val loss: 0.29820\n",
      "Interaction training epoch: 112, train loss: 0.23994, val loss: 0.29157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 113, train loss: 0.24852, val loss: 0.29615\n",
      "Interaction training epoch: 114, train loss: 0.23995, val loss: 0.29348\n",
      "Interaction training epoch: 115, train loss: 0.23594, val loss: 0.29075\n",
      "Interaction training epoch: 116, train loss: 0.23883, val loss: 0.28977\n",
      "Interaction training epoch: 117, train loss: 0.23811, val loss: 0.28984\n",
      "Interaction training epoch: 118, train loss: 0.23587, val loss: 0.28964\n",
      "Interaction training epoch: 119, train loss: 0.23777, val loss: 0.29264\n",
      "Interaction training epoch: 120, train loss: 0.23497, val loss: 0.29270\n",
      "Interaction training epoch: 121, train loss: 0.23671, val loss: 0.29148\n",
      "Interaction training epoch: 122, train loss: 0.23079, val loss: 0.28090\n",
      "Interaction training epoch: 123, train loss: 0.23394, val loss: 0.29219\n",
      "Interaction training epoch: 124, train loss: 0.23340, val loss: 0.28779\n",
      "Interaction training epoch: 125, train loss: 0.23296, val loss: 0.28805\n",
      "Interaction training epoch: 126, train loss: 0.23313, val loss: 0.29296\n",
      "Interaction training epoch: 127, train loss: 0.23077, val loss: 0.28620\n",
      "Interaction training epoch: 128, train loss: 0.23268, val loss: 0.28896\n",
      "Interaction training epoch: 129, train loss: 0.22999, val loss: 0.29529\n",
      "Interaction training epoch: 130, train loss: 0.23398, val loss: 0.29010\n",
      "Interaction training epoch: 131, train loss: 0.23227, val loss: 0.28528\n",
      "Interaction training epoch: 132, train loss: 0.23435, val loss: 0.29087\n",
      "Interaction training epoch: 133, train loss: 0.23072, val loss: 0.28637\n",
      "Interaction training epoch: 134, train loss: 0.23514, val loss: 0.29348\n",
      "Interaction training epoch: 135, train loss: 0.23072, val loss: 0.29137\n",
      "Interaction training epoch: 136, train loss: 0.22986, val loss: 0.28682\n",
      "Interaction training epoch: 137, train loss: 0.22711, val loss: 0.28269\n",
      "Interaction training epoch: 138, train loss: 0.23877, val loss: 0.29885\n",
      "Interaction training epoch: 139, train loss: 0.23057, val loss: 0.29326\n",
      "Interaction training epoch: 140, train loss: 0.23845, val loss: 0.29806\n",
      "Interaction training epoch: 141, train loss: 0.23005, val loss: 0.29289\n",
      "Interaction training epoch: 142, train loss: 0.22683, val loss: 0.28470\n",
      "Interaction training epoch: 143, train loss: 0.23093, val loss: 0.29454\n",
      "Interaction training epoch: 144, train loss: 0.22835, val loss: 0.28818\n",
      "Interaction training epoch: 145, train loss: 0.23155, val loss: 0.29425\n",
      "Interaction training epoch: 146, train loss: 0.22530, val loss: 0.28970\n",
      "Interaction training epoch: 147, train loss: 0.22866, val loss: 0.28645\n",
      "Interaction training epoch: 148, train loss: 0.22715, val loss: 0.29598\n",
      "Interaction training epoch: 149, train loss: 0.22594, val loss: 0.28596\n",
      "Interaction training epoch: 150, train loss: 0.23136, val loss: 0.29487\n",
      "Interaction training epoch: 151, train loss: 0.22718, val loss: 0.28466\n",
      "Interaction training epoch: 152, train loss: 0.22781, val loss: 0.29443\n",
      "Interaction training epoch: 153, train loss: 0.22893, val loss: 0.29417\n",
      "Interaction training epoch: 154, train loss: 0.22522, val loss: 0.28544\n",
      "Interaction training epoch: 155, train loss: 0.22357, val loss: 0.28816\n",
      "Interaction training epoch: 156, train loss: 0.22568, val loss: 0.29025\n",
      "Interaction training epoch: 157, train loss: 0.22656, val loss: 0.28711\n",
      "Interaction training epoch: 158, train loss: 0.22607, val loss: 0.29570\n",
      "Interaction training epoch: 159, train loss: 0.22951, val loss: 0.29491\n",
      "Interaction training epoch: 160, train loss: 0.22631, val loss: 0.29390\n",
      "Interaction training epoch: 161, train loss: 0.22311, val loss: 0.28889\n",
      "Interaction training epoch: 162, train loss: 0.22306, val loss: 0.28764\n",
      "Interaction training epoch: 163, train loss: 0.23429, val loss: 0.30290\n",
      "Interaction training epoch: 164, train loss: 0.22193, val loss: 0.28811\n",
      "Interaction training epoch: 165, train loss: 0.23234, val loss: 0.29615\n",
      "Interaction training epoch: 166, train loss: 0.22638, val loss: 0.29401\n",
      "Interaction training epoch: 167, train loss: 0.22162, val loss: 0.28531\n",
      "Interaction training epoch: 168, train loss: 0.22350, val loss: 0.29085\n",
      "Interaction training epoch: 169, train loss: 0.22372, val loss: 0.28895\n",
      "Interaction training epoch: 170, train loss: 0.22550, val loss: 0.29260\n",
      "Interaction training epoch: 171, train loss: 0.22067, val loss: 0.29236\n",
      "Interaction training epoch: 172, train loss: 0.22758, val loss: 0.28891\n",
      "Interaction training epoch: 173, train loss: 0.23027, val loss: 0.29973\n",
      "Interaction training epoch: 174, train loss: 0.22178, val loss: 0.29117\n",
      "Interaction training epoch: 175, train loss: 0.21939, val loss: 0.28726\n",
      "Interaction training epoch: 176, train loss: 0.22067, val loss: 0.28266\n",
      "Interaction training epoch: 177, train loss: 0.22680, val loss: 0.29741\n",
      "Interaction training epoch: 178, train loss: 0.22471, val loss: 0.29835\n",
      "Interaction training epoch: 179, train loss: 0.22419, val loss: 0.28361\n",
      "Interaction training epoch: 180, train loss: 0.22287, val loss: 0.28803\n",
      "Interaction training epoch: 181, train loss: 0.22017, val loss: 0.28531\n",
      "Interaction training epoch: 182, train loss: 0.22163, val loss: 0.29167\n",
      "Interaction training epoch: 183, train loss: 0.22228, val loss: 0.28816\n",
      "Interaction training epoch: 184, train loss: 0.22764, val loss: 0.29701\n",
      "Interaction training epoch: 185, train loss: 0.22111, val loss: 0.28581\n",
      "Interaction training epoch: 186, train loss: 0.22312, val loss: 0.29228\n",
      "Interaction training epoch: 187, train loss: 0.21829, val loss: 0.28835\n",
      "Interaction training epoch: 188, train loss: 0.22105, val loss: 0.29145\n",
      "Interaction training epoch: 189, train loss: 0.22025, val loss: 0.28648\n",
      "Interaction training epoch: 190, train loss: 0.22564, val loss: 0.29504\n",
      "Interaction training epoch: 191, train loss: 0.22413, val loss: 0.29446\n",
      "Interaction training epoch: 192, train loss: 0.21841, val loss: 0.28774\n",
      "Interaction training epoch: 193, train loss: 0.22286, val loss: 0.29057\n",
      "Interaction training epoch: 194, train loss: 0.21838, val loss: 0.28669\n",
      "Interaction training epoch: 195, train loss: 0.22148, val loss: 0.28828\n",
      "Interaction training epoch: 196, train loss: 0.21879, val loss: 0.29213\n",
      "Interaction training epoch: 197, train loss: 0.22192, val loss: 0.28790\n",
      "Interaction training epoch: 198, train loss: 0.21835, val loss: 0.28981\n",
      "Interaction training epoch: 199, train loss: 0.21889, val loss: 0.28684\n",
      "Interaction training epoch: 200, train loss: 0.22262, val loss: 0.29057\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########1 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.22387, val loss: 0.29716\n",
      "Interaction tuning epoch: 2, train loss: 0.22277, val loss: 0.28946\n",
      "Interaction tuning epoch: 3, train loss: 0.22785, val loss: 0.29736\n",
      "Interaction tuning epoch: 4, train loss: 0.22071, val loss: 0.29180\n",
      "Interaction tuning epoch: 5, train loss: 0.22133, val loss: 0.29446\n",
      "Interaction tuning epoch: 6, train loss: 0.22035, val loss: 0.28830\n",
      "Interaction tuning epoch: 7, train loss: 0.22015, val loss: 0.29050\n",
      "Interaction tuning epoch: 8, train loss: 0.22498, val loss: 0.28950\n",
      "Interaction tuning epoch: 9, train loss: 0.21877, val loss: 0.29227\n",
      "Interaction tuning epoch: 10, train loss: 0.22099, val loss: 0.29109\n",
      "Interaction tuning epoch: 11, train loss: 0.22347, val loss: 0.29621\n",
      "Interaction tuning epoch: 12, train loss: 0.22008, val loss: 0.28599\n",
      "Interaction tuning epoch: 13, train loss: 0.21924, val loss: 0.29153\n",
      "Interaction tuning epoch: 14, train loss: 0.22110, val loss: 0.29552\n",
      "Interaction tuning epoch: 15, train loss: 0.22199, val loss: 0.29254\n",
      "Interaction tuning epoch: 16, train loss: 0.21966, val loss: 0.29624\n",
      "Interaction tuning epoch: 17, train loss: 0.21893, val loss: 0.29153\n",
      "Interaction tuning epoch: 18, train loss: 0.21848, val loss: 0.28685\n",
      "Interaction tuning epoch: 19, train loss: 0.21758, val loss: 0.29206\n",
      "Interaction tuning epoch: 20, train loss: 0.22185, val loss: 0.29081\n",
      "Interaction tuning epoch: 21, train loss: 0.22140, val loss: 0.29493\n",
      "Interaction tuning epoch: 22, train loss: 0.22195, val loss: 0.29418\n",
      "Interaction tuning epoch: 23, train loss: 0.22046, val loss: 0.29188\n",
      "Interaction tuning epoch: 24, train loss: 0.21681, val loss: 0.29416\n",
      "Interaction tuning epoch: 25, train loss: 0.21778, val loss: 0.28232\n",
      "Interaction tuning epoch: 26, train loss: 0.22006, val loss: 0.29694\n",
      "Interaction tuning epoch: 27, train loss: 0.21568, val loss: 0.28582\n",
      "Interaction tuning epoch: 28, train loss: 0.21973, val loss: 0.29331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 29, train loss: 0.21812, val loss: 0.29252\n",
      "Interaction tuning epoch: 30, train loss: 0.21923, val loss: 0.29332\n",
      "Interaction tuning epoch: 31, train loss: 0.22897, val loss: 0.29717\n",
      "Interaction tuning epoch: 32, train loss: 0.22371, val loss: 0.29640\n",
      "Interaction tuning epoch: 33, train loss: 0.21848, val loss: 0.28992\n",
      "Interaction tuning epoch: 34, train loss: 0.22100, val loss: 0.29427\n",
      "Interaction tuning epoch: 35, train loss: 0.21802, val loss: 0.29542\n",
      "Interaction tuning epoch: 36, train loss: 0.21705, val loss: 0.28795\n",
      "Interaction tuning epoch: 37, train loss: 0.21841, val loss: 0.29698\n",
      "Interaction tuning epoch: 38, train loss: 0.21410, val loss: 0.28378\n",
      "Interaction tuning epoch: 39, train loss: 0.21906, val loss: 0.29615\n",
      "Interaction tuning epoch: 40, train loss: 0.21632, val loss: 0.29040\n",
      "Interaction tuning epoch: 41, train loss: 0.21667, val loss: 0.29061\n",
      "Interaction tuning epoch: 42, train loss: 0.21733, val loss: 0.29101\n",
      "Interaction tuning epoch: 43, train loss: 0.21528, val loss: 0.29095\n",
      "Interaction tuning epoch: 44, train loss: 0.21717, val loss: 0.29499\n",
      "Interaction tuning epoch: 45, train loss: 0.21655, val loss: 0.28930\n",
      "Interaction tuning epoch: 46, train loss: 0.21691, val loss: 0.29102\n",
      "Interaction tuning epoch: 47, train loss: 0.21601, val loss: 0.29355\n",
      "Interaction tuning epoch: 48, train loss: 0.21476, val loss: 0.28778\n",
      "Interaction tuning epoch: 49, train loss: 0.21711, val loss: 0.28826\n",
      "Interaction tuning epoch: 50, train loss: 0.21579, val loss: 0.29397\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 35.15442490577698\n",
      "After the gam stage, training error is 0.21579 , validation error is 0.29397\n",
      "missing value counts: 99198\n",
      "[SoftImpute] Max Singular Value of X_init = 3.506122\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.180630 validation BCE=0.301996,rank=5\n",
      "[SoftImpute] Iter 2: observed BCE=0.176632 validation BCE=0.299407,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.174446 validation BCE=0.286410,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.172685 validation BCE=0.284821,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.171419 validation BCE=0.282416,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.170549 validation BCE=0.281377,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.169798 validation BCE=0.280326,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.169560 validation BCE=0.279489,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.169257 validation BCE=0.278666,rank=5\n",
      "[SoftImpute] Iter 10: observed BCE=0.168998 validation BCE=0.278771,rank=5\n",
      "[SoftImpute] Iter 11: observed BCE=0.169136 validation BCE=0.278380,rank=5\n",
      "[SoftImpute] Iter 12: observed BCE=0.168785 validation BCE=0.277668,rank=5\n",
      "[SoftImpute] Iter 13: observed BCE=0.168938 validation BCE=0.278022,rank=5\n",
      "[SoftImpute] Iter 14: observed BCE=0.169048 validation BCE=0.266497,rank=5\n",
      "[SoftImpute] Iter 15: observed BCE=0.169160 validation BCE=0.266984,rank=5\n",
      "[SoftImpute] Iter 16: observed BCE=0.169115 validation BCE=0.266711,rank=5\n",
      "[SoftImpute] Iter 17: observed BCE=0.169589 validation BCE=0.266881,rank=5\n",
      "[SoftImpute] Iter 18: observed BCE=0.169720 validation BCE=0.266549,rank=5\n",
      "[SoftImpute] Iter 19: observed BCE=0.170080 validation BCE=0.266348,rank=5\n",
      "[SoftImpute] Iter 20: observed BCE=0.170205 validation BCE=0.266449,rank=5\n",
      "[SoftImpute] Iter 21: observed BCE=0.170386 validation BCE=0.266302,rank=5\n",
      "[SoftImpute] Iter 22: observed BCE=0.169969 validation BCE=0.265937,rank=5\n",
      "[SoftImpute] Iter 23: observed BCE=0.170148 validation BCE=0.265785,rank=5\n",
      "[SoftImpute] Iter 24: observed BCE=0.170236 validation BCE=0.265745,rank=5\n",
      "[SoftImpute] Iter 25: observed BCE=0.170314 validation BCE=0.265580,rank=5\n",
      "[SoftImpute] Iter 26: observed BCE=0.170138 validation BCE=0.264901,rank=5\n",
      "[SoftImpute] Iter 27: observed BCE=0.169945 validation BCE=0.265068,rank=5\n",
      "[SoftImpute] Iter 28: observed BCE=0.170010 validation BCE=0.265264,rank=5\n",
      "[SoftImpute] Iter 29: observed BCE=0.170047 validation BCE=0.265278,rank=5\n",
      "[SoftImpute] Stopped after iteration 29 for lambda=0.070122\n",
      "final num of user group: 20\n",
      "final num of item group: 32\n",
      "change mode state : True\n",
      "time cost: 10.665212869644165\n",
      "After the matrix factor stage, training error is 0.17005, validation error is 0.26528\n",
      "4\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68097, val loss: 0.68285\n",
      "Main effects training epoch: 2, train loss: 0.67671, val loss: 0.67981\n",
      "Main effects training epoch: 3, train loss: 0.67097, val loss: 0.67492\n",
      "Main effects training epoch: 4, train loss: 0.66430, val loss: 0.66851\n",
      "Main effects training epoch: 5, train loss: 0.65039, val loss: 0.65549\n",
      "Main effects training epoch: 6, train loss: 0.62025, val loss: 0.62604\n",
      "Main effects training epoch: 7, train loss: 0.57136, val loss: 0.57789\n",
      "Main effects training epoch: 8, train loss: 0.53454, val loss: 0.54517\n",
      "Main effects training epoch: 9, train loss: 0.52874, val loss: 0.53775\n",
      "Main effects training epoch: 10, train loss: 0.52391, val loss: 0.53079\n",
      "Main effects training epoch: 11, train loss: 0.52080, val loss: 0.52868\n",
      "Main effects training epoch: 12, train loss: 0.52149, val loss: 0.52782\n",
      "Main effects training epoch: 13, train loss: 0.52095, val loss: 0.52979\n",
      "Main effects training epoch: 14, train loss: 0.52032, val loss: 0.52569\n",
      "Main effects training epoch: 15, train loss: 0.51967, val loss: 0.52889\n",
      "Main effects training epoch: 16, train loss: 0.51974, val loss: 0.52613\n",
      "Main effects training epoch: 17, train loss: 0.51977, val loss: 0.53040\n",
      "Main effects training epoch: 18, train loss: 0.51906, val loss: 0.52542\n",
      "Main effects training epoch: 19, train loss: 0.51930, val loss: 0.52878\n",
      "Main effects training epoch: 20, train loss: 0.51880, val loss: 0.52605\n",
      "Main effects training epoch: 21, train loss: 0.51886, val loss: 0.52831\n",
      "Main effects training epoch: 22, train loss: 0.51942, val loss: 0.52672\n",
      "Main effects training epoch: 23, train loss: 0.51887, val loss: 0.52636\n",
      "Main effects training epoch: 24, train loss: 0.51877, val loss: 0.52770\n",
      "Main effects training epoch: 25, train loss: 0.51881, val loss: 0.52665\n",
      "Main effects training epoch: 26, train loss: 0.51898, val loss: 0.52707\n",
      "Main effects training epoch: 27, train loss: 0.51910, val loss: 0.52758\n",
      "Main effects training epoch: 28, train loss: 0.51894, val loss: 0.52656\n",
      "Main effects training epoch: 29, train loss: 0.51837, val loss: 0.52717\n",
      "Main effects training epoch: 30, train loss: 0.51843, val loss: 0.52797\n",
      "Main effects training epoch: 31, train loss: 0.51819, val loss: 0.52639\n",
      "Main effects training epoch: 32, train loss: 0.51838, val loss: 0.52681\n",
      "Main effects training epoch: 33, train loss: 0.51832, val loss: 0.52845\n",
      "Main effects training epoch: 34, train loss: 0.51850, val loss: 0.52592\n",
      "Main effects training epoch: 35, train loss: 0.51804, val loss: 0.52757\n",
      "Main effects training epoch: 36, train loss: 0.51784, val loss: 0.52623\n",
      "Main effects training epoch: 37, train loss: 0.51787, val loss: 0.52702\n",
      "Main effects training epoch: 38, train loss: 0.51783, val loss: 0.52636\n",
      "Main effects training epoch: 39, train loss: 0.51783, val loss: 0.52629\n",
      "Main effects training epoch: 40, train loss: 0.51774, val loss: 0.52793\n",
      "Main effects training epoch: 41, train loss: 0.51780, val loss: 0.52587\n",
      "Main effects training epoch: 42, train loss: 0.51796, val loss: 0.52747\n",
      "Main effects training epoch: 43, train loss: 0.51767, val loss: 0.52589\n",
      "Main effects training epoch: 44, train loss: 0.51790, val loss: 0.52857\n",
      "Main effects training epoch: 45, train loss: 0.51765, val loss: 0.52815\n",
      "Main effects training epoch: 46, train loss: 0.51869, val loss: 0.52542\n",
      "Main effects training epoch: 47, train loss: 0.51827, val loss: 0.53002\n",
      "Main effects training epoch: 48, train loss: 0.51786, val loss: 0.52618\n",
      "Main effects training epoch: 49, train loss: 0.51755, val loss: 0.52746\n",
      "Main effects training epoch: 50, train loss: 0.51772, val loss: 0.52660\n",
      "Main effects training epoch: 51, train loss: 0.51768, val loss: 0.52746\n",
      "Main effects training epoch: 52, train loss: 0.51722, val loss: 0.52693\n",
      "Main effects training epoch: 53, train loss: 0.51721, val loss: 0.52609\n",
      "Main effects training epoch: 54, train loss: 0.51727, val loss: 0.52862\n",
      "Main effects training epoch: 55, train loss: 0.51737, val loss: 0.52616\n",
      "Main effects training epoch: 56, train loss: 0.51720, val loss: 0.52770\n",
      "Main effects training epoch: 57, train loss: 0.51710, val loss: 0.52675\n",
      "Main effects training epoch: 58, train loss: 0.51704, val loss: 0.52800\n",
      "Main effects training epoch: 59, train loss: 0.51701, val loss: 0.52743\n",
      "Main effects training epoch: 60, train loss: 0.51689, val loss: 0.52683\n",
      "Main effects training epoch: 61, train loss: 0.51717, val loss: 0.52866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 62, train loss: 0.51677, val loss: 0.52647\n",
      "Main effects training epoch: 63, train loss: 0.51683, val loss: 0.52838\n",
      "Main effects training epoch: 64, train loss: 0.51681, val loss: 0.52627\n",
      "Main effects training epoch: 65, train loss: 0.51676, val loss: 0.52844\n",
      "Main effects training epoch: 66, train loss: 0.51650, val loss: 0.52738\n",
      "Main effects training epoch: 67, train loss: 0.51648, val loss: 0.52733\n",
      "Main effects training epoch: 68, train loss: 0.51728, val loss: 0.52605\n",
      "Main effects training epoch: 69, train loss: 0.51694, val loss: 0.52877\n",
      "Main effects training epoch: 70, train loss: 0.51700, val loss: 0.52690\n",
      "Main effects training epoch: 71, train loss: 0.51669, val loss: 0.52759\n",
      "Main effects training epoch: 72, train loss: 0.51678, val loss: 0.52764\n",
      "Main effects training epoch: 73, train loss: 0.51664, val loss: 0.52615\n",
      "Main effects training epoch: 74, train loss: 0.51695, val loss: 0.53049\n",
      "Main effects training epoch: 75, train loss: 0.51605, val loss: 0.52661\n",
      "Main effects training epoch: 76, train loss: 0.51596, val loss: 0.52692\n",
      "Main effects training epoch: 77, train loss: 0.51642, val loss: 0.52638\n",
      "Main effects training epoch: 78, train loss: 0.51620, val loss: 0.52862\n",
      "Main effects training epoch: 79, train loss: 0.51586, val loss: 0.52726\n",
      "Main effects training epoch: 80, train loss: 0.51568, val loss: 0.52704\n",
      "Main effects training epoch: 81, train loss: 0.51556, val loss: 0.52618\n",
      "Main effects training epoch: 82, train loss: 0.51580, val loss: 0.52866\n",
      "Main effects training epoch: 83, train loss: 0.51551, val loss: 0.52753\n",
      "Main effects training epoch: 84, train loss: 0.51526, val loss: 0.52765\n",
      "Main effects training epoch: 85, train loss: 0.51501, val loss: 0.52668\n",
      "Main effects training epoch: 86, train loss: 0.51503, val loss: 0.52621\n",
      "Main effects training epoch: 87, train loss: 0.51471, val loss: 0.52824\n",
      "Main effects training epoch: 88, train loss: 0.51452, val loss: 0.52801\n",
      "Main effects training epoch: 89, train loss: 0.51438, val loss: 0.52655\n",
      "Main effects training epoch: 90, train loss: 0.51448, val loss: 0.52694\n",
      "Main effects training epoch: 91, train loss: 0.51424, val loss: 0.52867\n",
      "Main effects training epoch: 92, train loss: 0.51419, val loss: 0.52569\n",
      "Main effects training epoch: 93, train loss: 0.51399, val loss: 0.52795\n",
      "Main effects training epoch: 94, train loss: 0.51487, val loss: 0.52600\n",
      "Main effects training epoch: 95, train loss: 0.51559, val loss: 0.53202\n",
      "Main effects training epoch: 96, train loss: 0.51424, val loss: 0.52488\n",
      "Main effects training epoch: 97, train loss: 0.51455, val loss: 0.52745\n",
      "Main effects training epoch: 98, train loss: 0.51447, val loss: 0.52859\n",
      "Main effects training epoch: 99, train loss: 0.51358, val loss: 0.52630\n",
      "Main effects training epoch: 100, train loss: 0.51382, val loss: 0.52802\n",
      "Main effects training epoch: 101, train loss: 0.51385, val loss: 0.52635\n",
      "Main effects training epoch: 102, train loss: 0.51336, val loss: 0.52901\n",
      "Main effects training epoch: 103, train loss: 0.51295, val loss: 0.52602\n",
      "Main effects training epoch: 104, train loss: 0.51271, val loss: 0.52579\n",
      "Main effects training epoch: 105, train loss: 0.51270, val loss: 0.52711\n",
      "Main effects training epoch: 106, train loss: 0.51286, val loss: 0.52622\n",
      "Main effects training epoch: 107, train loss: 0.51282, val loss: 0.52737\n",
      "Main effects training epoch: 108, train loss: 0.51261, val loss: 0.52609\n",
      "Main effects training epoch: 109, train loss: 0.51273, val loss: 0.52724\n",
      "Main effects training epoch: 110, train loss: 0.51317, val loss: 0.52690\n",
      "Main effects training epoch: 111, train loss: 0.51280, val loss: 0.52668\n",
      "Main effects training epoch: 112, train loss: 0.51247, val loss: 0.52716\n",
      "Main effects training epoch: 113, train loss: 0.51303, val loss: 0.52721\n",
      "Main effects training epoch: 114, train loss: 0.51330, val loss: 0.52677\n",
      "Main effects training epoch: 115, train loss: 0.51254, val loss: 0.52616\n",
      "Main effects training epoch: 116, train loss: 0.51241, val loss: 0.52717\n",
      "Main effects training epoch: 117, train loss: 0.51267, val loss: 0.52701\n",
      "Main effects training epoch: 118, train loss: 0.51244, val loss: 0.52525\n",
      "Main effects training epoch: 119, train loss: 0.51237, val loss: 0.52748\n",
      "Main effects training epoch: 120, train loss: 0.51281, val loss: 0.52815\n",
      "Main effects training epoch: 121, train loss: 0.51350, val loss: 0.52811\n",
      "Main effects training epoch: 122, train loss: 0.51219, val loss: 0.52711\n",
      "Main effects training epoch: 123, train loss: 0.51272, val loss: 0.52553\n",
      "Main effects training epoch: 124, train loss: 0.51240, val loss: 0.52840\n",
      "Main effects training epoch: 125, train loss: 0.51249, val loss: 0.52713\n",
      "Main effects training epoch: 126, train loss: 0.51215, val loss: 0.52667\n",
      "Main effects training epoch: 127, train loss: 0.51277, val loss: 0.52605\n",
      "Main effects training epoch: 128, train loss: 0.51208, val loss: 0.52693\n",
      "Main effects training epoch: 129, train loss: 0.51221, val loss: 0.52499\n",
      "Main effects training epoch: 130, train loss: 0.51280, val loss: 0.52851\n",
      "Main effects training epoch: 131, train loss: 0.51240, val loss: 0.52627\n",
      "Main effects training epoch: 132, train loss: 0.51190, val loss: 0.52666\n",
      "Main effects training epoch: 133, train loss: 0.51213, val loss: 0.52582\n",
      "Main effects training epoch: 134, train loss: 0.51214, val loss: 0.52698\n",
      "Main effects training epoch: 135, train loss: 0.51204, val loss: 0.52623\n",
      "Main effects training epoch: 136, train loss: 0.51211, val loss: 0.52798\n",
      "Main effects training epoch: 137, train loss: 0.51252, val loss: 0.52616\n",
      "Main effects training epoch: 138, train loss: 0.51174, val loss: 0.52657\n",
      "Main effects training epoch: 139, train loss: 0.51133, val loss: 0.52623\n",
      "Main effects training epoch: 140, train loss: 0.51179, val loss: 0.52674\n",
      "Main effects training epoch: 141, train loss: 0.51208, val loss: 0.52622\n",
      "Main effects training epoch: 142, train loss: 0.51200, val loss: 0.52742\n",
      "Main effects training epoch: 143, train loss: 0.51210, val loss: 0.52686\n",
      "Main effects training epoch: 144, train loss: 0.51125, val loss: 0.52640\n",
      "Main effects training epoch: 145, train loss: 0.51187, val loss: 0.52648\n",
      "Main effects training epoch: 146, train loss: 0.51181, val loss: 0.52666\n",
      "Main effects training epoch: 147, train loss: 0.51146, val loss: 0.52699\n",
      "Main effects training epoch: 148, train loss: 0.51173, val loss: 0.52640\n",
      "Main effects training epoch: 149, train loss: 0.51145, val loss: 0.52779\n",
      "Main effects training epoch: 150, train loss: 0.51147, val loss: 0.52561\n",
      "Main effects training epoch: 151, train loss: 0.51122, val loss: 0.52599\n",
      "Main effects training epoch: 152, train loss: 0.51091, val loss: 0.52674\n",
      "Main effects training epoch: 153, train loss: 0.51073, val loss: 0.52522\n",
      "Main effects training epoch: 154, train loss: 0.51067, val loss: 0.52496\n",
      "Main effects training epoch: 155, train loss: 0.51077, val loss: 0.52622\n",
      "Main effects training epoch: 156, train loss: 0.51063, val loss: 0.52536\n",
      "Main effects training epoch: 157, train loss: 0.51089, val loss: 0.52743\n",
      "Main effects training epoch: 158, train loss: 0.51052, val loss: 0.52668\n",
      "Main effects training epoch: 159, train loss: 0.51039, val loss: 0.52516\n",
      "Main effects training epoch: 160, train loss: 0.51072, val loss: 0.52535\n",
      "Main effects training epoch: 161, train loss: 0.51047, val loss: 0.52592\n",
      "Main effects training epoch: 162, train loss: 0.51034, val loss: 0.52554\n",
      "Main effects training epoch: 163, train loss: 0.51052, val loss: 0.52519\n",
      "Main effects training epoch: 164, train loss: 0.51013, val loss: 0.52531\n",
      "Main effects training epoch: 165, train loss: 0.51021, val loss: 0.52634\n",
      "Main effects training epoch: 166, train loss: 0.51086, val loss: 0.52594\n",
      "Main effects training epoch: 167, train loss: 0.51046, val loss: 0.52674\n",
      "Main effects training epoch: 168, train loss: 0.51019, val loss: 0.52545\n",
      "Main effects training epoch: 169, train loss: 0.51013, val loss: 0.52613\n",
      "Main effects training epoch: 170, train loss: 0.51015, val loss: 0.52542\n",
      "Main effects training epoch: 171, train loss: 0.50988, val loss: 0.52518\n",
      "Main effects training epoch: 172, train loss: 0.50964, val loss: 0.52552\n",
      "Main effects training epoch: 173, train loss: 0.50956, val loss: 0.52502\n",
      "Main effects training epoch: 174, train loss: 0.50966, val loss: 0.52608\n",
      "Main effects training epoch: 175, train loss: 0.51049, val loss: 0.52538\n",
      "Main effects training epoch: 176, train loss: 0.50957, val loss: 0.52530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 177, train loss: 0.50937, val loss: 0.52525\n",
      "Main effects training epoch: 178, train loss: 0.50940, val loss: 0.52498\n",
      "Main effects training epoch: 179, train loss: 0.51006, val loss: 0.52799\n",
      "Main effects training epoch: 180, train loss: 0.50937, val loss: 0.52423\n",
      "Main effects training epoch: 181, train loss: 0.50937, val loss: 0.52596\n",
      "Main effects training epoch: 182, train loss: 0.50921, val loss: 0.52484\n",
      "Main effects training epoch: 183, train loss: 0.50948, val loss: 0.52696\n",
      "Main effects training epoch: 184, train loss: 0.50928, val loss: 0.52456\n",
      "Main effects training epoch: 185, train loss: 0.50933, val loss: 0.52566\n",
      "Main effects training epoch: 186, train loss: 0.50934, val loss: 0.52613\n",
      "Main effects training epoch: 187, train loss: 0.50925, val loss: 0.52496\n",
      "Main effects training epoch: 188, train loss: 0.50885, val loss: 0.52528\n",
      "Main effects training epoch: 189, train loss: 0.50940, val loss: 0.52472\n",
      "Main effects training epoch: 190, train loss: 0.50879, val loss: 0.52567\n",
      "Main effects training epoch: 191, train loss: 0.50889, val loss: 0.52519\n",
      "Main effects training epoch: 192, train loss: 0.50905, val loss: 0.52629\n",
      "Main effects training epoch: 193, train loss: 0.50872, val loss: 0.52475\n",
      "Main effects training epoch: 194, train loss: 0.50885, val loss: 0.52598\n",
      "Main effects training epoch: 195, train loss: 0.50889, val loss: 0.52545\n",
      "Main effects training epoch: 196, train loss: 0.50885, val loss: 0.52416\n",
      "Main effects training epoch: 197, train loss: 0.50858, val loss: 0.52498\n",
      "Main effects training epoch: 198, train loss: 0.50880, val loss: 0.52636\n",
      "Main effects training epoch: 199, train loss: 0.50866, val loss: 0.52459\n",
      "Main effects training epoch: 200, train loss: 0.50859, val loss: 0.52468\n",
      "Main effects training epoch: 201, train loss: 0.50896, val loss: 0.52615\n",
      "Main effects training epoch: 202, train loss: 0.50877, val loss: 0.52534\n",
      "Main effects training epoch: 203, train loss: 0.50899, val loss: 0.52474\n",
      "Main effects training epoch: 204, train loss: 0.50893, val loss: 0.52504\n",
      "Main effects training epoch: 205, train loss: 0.50904, val loss: 0.52500\n",
      "Main effects training epoch: 206, train loss: 0.50869, val loss: 0.52707\n",
      "Main effects training epoch: 207, train loss: 0.50886, val loss: 0.52538\n",
      "Main effects training epoch: 208, train loss: 0.50880, val loss: 0.52386\n",
      "Main effects training epoch: 209, train loss: 0.50891, val loss: 0.52663\n",
      "Main effects training epoch: 210, train loss: 0.50836, val loss: 0.52336\n",
      "Main effects training epoch: 211, train loss: 0.50829, val loss: 0.52526\n",
      "Main effects training epoch: 212, train loss: 0.50836, val loss: 0.52623\n",
      "Main effects training epoch: 213, train loss: 0.50829, val loss: 0.52371\n",
      "Main effects training epoch: 214, train loss: 0.50834, val loss: 0.52306\n",
      "Main effects training epoch: 215, train loss: 0.50819, val loss: 0.52597\n",
      "Main effects training epoch: 216, train loss: 0.50815, val loss: 0.52454\n",
      "Main effects training epoch: 217, train loss: 0.50826, val loss: 0.52458\n",
      "Main effects training epoch: 218, train loss: 0.50822, val loss: 0.52589\n",
      "Main effects training epoch: 219, train loss: 0.50817, val loss: 0.52396\n",
      "Main effects training epoch: 220, train loss: 0.50831, val loss: 0.52368\n",
      "Main effects training epoch: 221, train loss: 0.50822, val loss: 0.52452\n",
      "Main effects training epoch: 222, train loss: 0.50793, val loss: 0.52435\n",
      "Main effects training epoch: 223, train loss: 0.50821, val loss: 0.52395\n",
      "Main effects training epoch: 224, train loss: 0.50853, val loss: 0.52542\n",
      "Main effects training epoch: 225, train loss: 0.50911, val loss: 0.52611\n",
      "Main effects training epoch: 226, train loss: 0.50795, val loss: 0.52433\n",
      "Main effects training epoch: 227, train loss: 0.50812, val loss: 0.52443\n",
      "Main effects training epoch: 228, train loss: 0.50775, val loss: 0.52493\n",
      "Main effects training epoch: 229, train loss: 0.50794, val loss: 0.52308\n",
      "Main effects training epoch: 230, train loss: 0.50779, val loss: 0.52525\n",
      "Main effects training epoch: 231, train loss: 0.50831, val loss: 0.52466\n",
      "Main effects training epoch: 232, train loss: 0.50831, val loss: 0.52501\n",
      "Main effects training epoch: 233, train loss: 0.50841, val loss: 0.52352\n",
      "Main effects training epoch: 234, train loss: 0.50777, val loss: 0.52401\n",
      "Main effects training epoch: 235, train loss: 0.50761, val loss: 0.52497\n",
      "Main effects training epoch: 236, train loss: 0.50775, val loss: 0.52371\n",
      "Main effects training epoch: 237, train loss: 0.50746, val loss: 0.52380\n",
      "Main effects training epoch: 238, train loss: 0.50736, val loss: 0.52442\n",
      "Main effects training epoch: 239, train loss: 0.50762, val loss: 0.52270\n",
      "Main effects training epoch: 240, train loss: 0.50739, val loss: 0.52468\n",
      "Main effects training epoch: 241, train loss: 0.51009, val loss: 0.52455\n",
      "Main effects training epoch: 242, train loss: 0.50807, val loss: 0.52465\n",
      "Main effects training epoch: 243, train loss: 0.50770, val loss: 0.52311\n",
      "Main effects training epoch: 244, train loss: 0.50748, val loss: 0.52489\n",
      "Main effects training epoch: 245, train loss: 0.50755, val loss: 0.52342\n",
      "Main effects training epoch: 246, train loss: 0.50767, val loss: 0.52443\n",
      "Main effects training epoch: 247, train loss: 0.50755, val loss: 0.52256\n",
      "Main effects training epoch: 248, train loss: 0.50722, val loss: 0.52530\n",
      "Main effects training epoch: 249, train loss: 0.50759, val loss: 0.52300\n",
      "Main effects training epoch: 250, train loss: 0.50743, val loss: 0.52459\n",
      "Main effects training epoch: 251, train loss: 0.50752, val loss: 0.52429\n",
      "Main effects training epoch: 252, train loss: 0.50766, val loss: 0.52375\n",
      "Main effects training epoch: 253, train loss: 0.50747, val loss: 0.52298\n",
      "Main effects training epoch: 254, train loss: 0.50726, val loss: 0.52553\n",
      "Main effects training epoch: 255, train loss: 0.50712, val loss: 0.52301\n",
      "Main effects training epoch: 256, train loss: 0.50743, val loss: 0.52257\n",
      "Main effects training epoch: 257, train loss: 0.50772, val loss: 0.52541\n",
      "Main effects training epoch: 258, train loss: 0.50716, val loss: 0.52412\n",
      "Main effects training epoch: 259, train loss: 0.50717, val loss: 0.52240\n",
      "Main effects training epoch: 260, train loss: 0.50746, val loss: 0.52494\n",
      "Main effects training epoch: 261, train loss: 0.50754, val loss: 0.52366\n",
      "Main effects training epoch: 262, train loss: 0.50713, val loss: 0.52406\n",
      "Main effects training epoch: 263, train loss: 0.50710, val loss: 0.52191\n",
      "Main effects training epoch: 264, train loss: 0.50716, val loss: 0.52561\n",
      "Main effects training epoch: 265, train loss: 0.50689, val loss: 0.52459\n",
      "Main effects training epoch: 266, train loss: 0.50716, val loss: 0.52212\n",
      "Main effects training epoch: 267, train loss: 0.50704, val loss: 0.52475\n",
      "Main effects training epoch: 268, train loss: 0.50686, val loss: 0.52187\n",
      "Main effects training epoch: 269, train loss: 0.50665, val loss: 0.52218\n",
      "Main effects training epoch: 270, train loss: 0.50673, val loss: 0.52505\n",
      "Main effects training epoch: 271, train loss: 0.50673, val loss: 0.52305\n",
      "Main effects training epoch: 272, train loss: 0.50631, val loss: 0.52311\n",
      "Main effects training epoch: 273, train loss: 0.50637, val loss: 0.52340\n",
      "Main effects training epoch: 274, train loss: 0.50645, val loss: 0.52162\n",
      "Main effects training epoch: 275, train loss: 0.50636, val loss: 0.52436\n",
      "Main effects training epoch: 276, train loss: 0.50632, val loss: 0.52285\n",
      "Main effects training epoch: 277, train loss: 0.50631, val loss: 0.52308\n",
      "Main effects training epoch: 278, train loss: 0.50631, val loss: 0.52293\n",
      "Main effects training epoch: 279, train loss: 0.50645, val loss: 0.52333\n",
      "Main effects training epoch: 280, train loss: 0.50601, val loss: 0.52349\n",
      "Main effects training epoch: 281, train loss: 0.50721, val loss: 0.52388\n",
      "Main effects training epoch: 282, train loss: 0.50649, val loss: 0.52414\n",
      "Main effects training epoch: 283, train loss: 0.50619, val loss: 0.52226\n",
      "Main effects training epoch: 284, train loss: 0.50633, val loss: 0.52381\n",
      "Main effects training epoch: 285, train loss: 0.50595, val loss: 0.52306\n",
      "Main effects training epoch: 286, train loss: 0.50632, val loss: 0.52397\n",
      "Main effects training epoch: 287, train loss: 0.50621, val loss: 0.52211\n",
      "Main effects training epoch: 288, train loss: 0.50582, val loss: 0.52323\n",
      "Main effects training epoch: 289, train loss: 0.50595, val loss: 0.52367\n",
      "Main effects training epoch: 290, train loss: 0.50616, val loss: 0.52365\n",
      "Main effects training epoch: 291, train loss: 0.50576, val loss: 0.52233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 292, train loss: 0.50588, val loss: 0.52189\n",
      "Main effects training epoch: 293, train loss: 0.50576, val loss: 0.52405\n",
      "Main effects training epoch: 294, train loss: 0.50562, val loss: 0.52325\n",
      "Main effects training epoch: 295, train loss: 0.50588, val loss: 0.52147\n",
      "Main effects training epoch: 296, train loss: 0.50555, val loss: 0.52393\n",
      "Main effects training epoch: 297, train loss: 0.50562, val loss: 0.52310\n",
      "Main effects training epoch: 298, train loss: 0.50615, val loss: 0.52259\n",
      "Main effects training epoch: 299, train loss: 0.50555, val loss: 0.52216\n",
      "Main effects training epoch: 300, train loss: 0.50542, val loss: 0.52333\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51071, val loss: 0.52315\n",
      "Main effects tuning epoch: 2, train loss: 0.51064, val loss: 0.52550\n",
      "Main effects tuning epoch: 3, train loss: 0.51044, val loss: 0.52514\n",
      "Main effects tuning epoch: 4, train loss: 0.51073, val loss: 0.52466\n",
      "Main effects tuning epoch: 5, train loss: 0.51010, val loss: 0.52485\n",
      "Main effects tuning epoch: 6, train loss: 0.51006, val loss: 0.52403\n",
      "Main effects tuning epoch: 7, train loss: 0.51007, val loss: 0.52476\n",
      "Main effects tuning epoch: 8, train loss: 0.51042, val loss: 0.52418\n",
      "Main effects tuning epoch: 9, train loss: 0.51006, val loss: 0.52423\n",
      "Main effects tuning epoch: 10, train loss: 0.50986, val loss: 0.52449\n",
      "Main effects tuning epoch: 11, train loss: 0.50994, val loss: 0.52322\n",
      "Main effects tuning epoch: 12, train loss: 0.51000, val loss: 0.52588\n",
      "Main effects tuning epoch: 13, train loss: 0.51057, val loss: 0.52471\n",
      "Main effects tuning epoch: 14, train loss: 0.51009, val loss: 0.52399\n",
      "Main effects tuning epoch: 15, train loss: 0.50980, val loss: 0.52401\n",
      "Main effects tuning epoch: 16, train loss: 0.50993, val loss: 0.52539\n",
      "Main effects tuning epoch: 17, train loss: 0.51027, val loss: 0.52484\n",
      "Main effects tuning epoch: 18, train loss: 0.51010, val loss: 0.52291\n",
      "Main effects tuning epoch: 19, train loss: 0.50974, val loss: 0.52509\n",
      "Main effects tuning epoch: 20, train loss: 0.51003, val loss: 0.52388\n",
      "Main effects tuning epoch: 21, train loss: 0.51011, val loss: 0.52549\n",
      "Main effects tuning epoch: 22, train loss: 0.50989, val loss: 0.52403\n",
      "Main effects tuning epoch: 23, train loss: 0.50982, val loss: 0.52507\n",
      "Main effects tuning epoch: 24, train loss: 0.51033, val loss: 0.52259\n",
      "Main effects tuning epoch: 25, train loss: 0.51021, val loss: 0.52621\n",
      "Main effects tuning epoch: 26, train loss: 0.50972, val loss: 0.52468\n",
      "Main effects tuning epoch: 27, train loss: 0.50984, val loss: 0.52510\n",
      "Main effects tuning epoch: 28, train loss: 0.50990, val loss: 0.52343\n",
      "Main effects tuning epoch: 29, train loss: 0.50958, val loss: 0.52349\n",
      "Main effects tuning epoch: 30, train loss: 0.51038, val loss: 0.52459\n",
      "Main effects tuning epoch: 31, train loss: 0.50946, val loss: 0.52543\n",
      "Main effects tuning epoch: 32, train loss: 0.50946, val loss: 0.52361\n",
      "Main effects tuning epoch: 33, train loss: 0.50968, val loss: 0.52488\n",
      "Main effects tuning epoch: 34, train loss: 0.50957, val loss: 0.52423\n",
      "Main effects tuning epoch: 35, train loss: 0.50954, val loss: 0.52406\n",
      "Main effects tuning epoch: 36, train loss: 0.50951, val loss: 0.52439\n",
      "Main effects tuning epoch: 37, train loss: 0.50977, val loss: 0.52620\n",
      "Main effects tuning epoch: 38, train loss: 0.50984, val loss: 0.52211\n",
      "Main effects tuning epoch: 39, train loss: 0.50963, val loss: 0.52551\n",
      "Main effects tuning epoch: 40, train loss: 0.50941, val loss: 0.52502\n",
      "Main effects tuning epoch: 41, train loss: 0.50939, val loss: 0.52332\n",
      "Main effects tuning epoch: 42, train loss: 0.50928, val loss: 0.52498\n",
      "Main effects tuning epoch: 43, train loss: 0.50914, val loss: 0.52463\n",
      "Main effects tuning epoch: 44, train loss: 0.50927, val loss: 0.52422\n",
      "Main effects tuning epoch: 45, train loss: 0.50942, val loss: 0.52250\n",
      "Main effects tuning epoch: 46, train loss: 0.50935, val loss: 0.52666\n",
      "Main effects tuning epoch: 47, train loss: 0.50935, val loss: 0.52477\n",
      "Main effects tuning epoch: 48, train loss: 0.50915, val loss: 0.52388\n",
      "Main effects tuning epoch: 49, train loss: 0.50928, val loss: 0.52337\n",
      "Main effects tuning epoch: 50, train loss: 0.50997, val loss: 0.52552\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.48364, val loss: 0.49858\n",
      "Interaction training epoch: 2, train loss: 0.36122, val loss: 0.37175\n",
      "Interaction training epoch: 3, train loss: 0.32320, val loss: 0.33355\n",
      "Interaction training epoch: 4, train loss: 0.31364, val loss: 0.32616\n",
      "Interaction training epoch: 5, train loss: 0.30978, val loss: 0.31567\n",
      "Interaction training epoch: 6, train loss: 0.28972, val loss: 0.29929\n",
      "Interaction training epoch: 7, train loss: 0.29319, val loss: 0.30259\n",
      "Interaction training epoch: 8, train loss: 0.29167, val loss: 0.30681\n",
      "Interaction training epoch: 9, train loss: 0.28950, val loss: 0.29984\n",
      "Interaction training epoch: 10, train loss: 0.28972, val loss: 0.30442\n",
      "Interaction training epoch: 11, train loss: 0.28133, val loss: 0.30048\n",
      "Interaction training epoch: 12, train loss: 0.28264, val loss: 0.29792\n",
      "Interaction training epoch: 13, train loss: 0.28311, val loss: 0.29845\n",
      "Interaction training epoch: 14, train loss: 0.28058, val loss: 0.29082\n",
      "Interaction training epoch: 15, train loss: 0.28240, val loss: 0.29507\n",
      "Interaction training epoch: 16, train loss: 0.27692, val loss: 0.29493\n",
      "Interaction training epoch: 17, train loss: 0.28201, val loss: 0.30114\n",
      "Interaction training epoch: 18, train loss: 0.27937, val loss: 0.29673\n",
      "Interaction training epoch: 19, train loss: 0.28043, val loss: 0.29203\n",
      "Interaction training epoch: 20, train loss: 0.27979, val loss: 0.29558\n",
      "Interaction training epoch: 21, train loss: 0.27993, val loss: 0.29376\n",
      "Interaction training epoch: 22, train loss: 0.27525, val loss: 0.29259\n",
      "Interaction training epoch: 23, train loss: 0.27952, val loss: 0.29371\n",
      "Interaction training epoch: 24, train loss: 0.28104, val loss: 0.29962\n",
      "Interaction training epoch: 25, train loss: 0.27679, val loss: 0.29289\n",
      "Interaction training epoch: 26, train loss: 0.27833, val loss: 0.29297\n",
      "Interaction training epoch: 27, train loss: 0.27598, val loss: 0.29305\n",
      "Interaction training epoch: 28, train loss: 0.27281, val loss: 0.28747\n",
      "Interaction training epoch: 29, train loss: 0.27895, val loss: 0.29884\n",
      "Interaction training epoch: 30, train loss: 0.27683, val loss: 0.29292\n",
      "Interaction training epoch: 31, train loss: 0.27225, val loss: 0.28423\n",
      "Interaction training epoch: 32, train loss: 0.27445, val loss: 0.29142\n",
      "Interaction training epoch: 33, train loss: 0.27478, val loss: 0.29039\n",
      "Interaction training epoch: 34, train loss: 0.27189, val loss: 0.28761\n",
      "Interaction training epoch: 35, train loss: 0.27136, val loss: 0.28521\n",
      "Interaction training epoch: 36, train loss: 0.27150, val loss: 0.28942\n",
      "Interaction training epoch: 37, train loss: 0.27307, val loss: 0.28752\n",
      "Interaction training epoch: 38, train loss: 0.27179, val loss: 0.28608\n",
      "Interaction training epoch: 39, train loss: 0.26920, val loss: 0.28491\n",
      "Interaction training epoch: 40, train loss: 0.27115, val loss: 0.28870\n",
      "Interaction training epoch: 41, train loss: 0.27102, val loss: 0.28454\n",
      "Interaction training epoch: 42, train loss: 0.27046, val loss: 0.28855\n",
      "Interaction training epoch: 43, train loss: 0.26767, val loss: 0.28454\n",
      "Interaction training epoch: 44, train loss: 0.27216, val loss: 0.28875\n",
      "Interaction training epoch: 45, train loss: 0.26842, val loss: 0.28232\n",
      "Interaction training epoch: 46, train loss: 0.26805, val loss: 0.28040\n",
      "Interaction training epoch: 47, train loss: 0.26800, val loss: 0.28496\n",
      "Interaction training epoch: 48, train loss: 0.27076, val loss: 0.28556\n",
      "Interaction training epoch: 49, train loss: 0.27119, val loss: 0.28502\n",
      "Interaction training epoch: 50, train loss: 0.26606, val loss: 0.28367\n",
      "Interaction training epoch: 51, train loss: 0.26832, val loss: 0.28219\n",
      "Interaction training epoch: 52, train loss: 0.26510, val loss: 0.28199\n",
      "Interaction training epoch: 53, train loss: 0.26764, val loss: 0.28274\n",
      "Interaction training epoch: 54, train loss: 0.26720, val loss: 0.28139\n",
      "Interaction training epoch: 55, train loss: 0.26660, val loss: 0.28051\n",
      "Interaction training epoch: 56, train loss: 0.26767, val loss: 0.28652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 57, train loss: 0.26480, val loss: 0.27401\n",
      "Interaction training epoch: 58, train loss: 0.26840, val loss: 0.28848\n",
      "Interaction training epoch: 59, train loss: 0.26518, val loss: 0.27732\n",
      "Interaction training epoch: 60, train loss: 0.26870, val loss: 0.28187\n",
      "Interaction training epoch: 61, train loss: 0.26893, val loss: 0.28603\n",
      "Interaction training epoch: 62, train loss: 0.26615, val loss: 0.27985\n",
      "Interaction training epoch: 63, train loss: 0.26620, val loss: 0.27748\n",
      "Interaction training epoch: 64, train loss: 0.26419, val loss: 0.28342\n",
      "Interaction training epoch: 65, train loss: 0.26207, val loss: 0.27790\n",
      "Interaction training epoch: 66, train loss: 0.26367, val loss: 0.27896\n",
      "Interaction training epoch: 67, train loss: 0.26138, val loss: 0.27834\n",
      "Interaction training epoch: 68, train loss: 0.26511, val loss: 0.27792\n",
      "Interaction training epoch: 69, train loss: 0.26168, val loss: 0.27680\n",
      "Interaction training epoch: 70, train loss: 0.26410, val loss: 0.28071\n",
      "Interaction training epoch: 71, train loss: 0.26000, val loss: 0.27616\n",
      "Interaction training epoch: 72, train loss: 0.26021, val loss: 0.27689\n",
      "Interaction training epoch: 73, train loss: 0.26139, val loss: 0.27695\n",
      "Interaction training epoch: 74, train loss: 0.26133, val loss: 0.27821\n",
      "Interaction training epoch: 75, train loss: 0.26205, val loss: 0.27426\n",
      "Interaction training epoch: 76, train loss: 0.26422, val loss: 0.28524\n",
      "Interaction training epoch: 77, train loss: 0.26198, val loss: 0.27827\n",
      "Interaction training epoch: 78, train loss: 0.25975, val loss: 0.27639\n",
      "Interaction training epoch: 79, train loss: 0.26183, val loss: 0.27972\n",
      "Interaction training epoch: 80, train loss: 0.26527, val loss: 0.28525\n",
      "Interaction training epoch: 81, train loss: 0.26089, val loss: 0.27581\n",
      "Interaction training epoch: 82, train loss: 0.26245, val loss: 0.28281\n",
      "Interaction training epoch: 83, train loss: 0.26096, val loss: 0.27852\n",
      "Interaction training epoch: 84, train loss: 0.26204, val loss: 0.28119\n",
      "Interaction training epoch: 85, train loss: 0.25839, val loss: 0.27647\n",
      "Interaction training epoch: 86, train loss: 0.26055, val loss: 0.28004\n",
      "Interaction training epoch: 87, train loss: 0.26252, val loss: 0.27772\n",
      "Interaction training epoch: 88, train loss: 0.25851, val loss: 0.27903\n",
      "Interaction training epoch: 89, train loss: 0.26079, val loss: 0.28004\n",
      "Interaction training epoch: 90, train loss: 0.25834, val loss: 0.27595\n",
      "Interaction training epoch: 91, train loss: 0.25800, val loss: 0.27642\n",
      "Interaction training epoch: 92, train loss: 0.25740, val loss: 0.27405\n",
      "Interaction training epoch: 93, train loss: 0.25995, val loss: 0.28396\n",
      "Interaction training epoch: 94, train loss: 0.25847, val loss: 0.27628\n",
      "Interaction training epoch: 95, train loss: 0.25638, val loss: 0.27619\n",
      "Interaction training epoch: 96, train loss: 0.26059, val loss: 0.28026\n",
      "Interaction training epoch: 97, train loss: 0.25843, val loss: 0.27429\n",
      "Interaction training epoch: 98, train loss: 0.26087, val loss: 0.28197\n",
      "Interaction training epoch: 99, train loss: 0.25712, val loss: 0.27619\n",
      "Interaction training epoch: 100, train loss: 0.25762, val loss: 0.28007\n",
      "Interaction training epoch: 101, train loss: 0.25757, val loss: 0.27615\n",
      "Interaction training epoch: 102, train loss: 0.25759, val loss: 0.28116\n",
      "Interaction training epoch: 103, train loss: 0.25602, val loss: 0.27265\n",
      "Interaction training epoch: 104, train loss: 0.25921, val loss: 0.28210\n",
      "Interaction training epoch: 105, train loss: 0.25588, val loss: 0.27712\n",
      "Interaction training epoch: 106, train loss: 0.25682, val loss: 0.28012\n",
      "Interaction training epoch: 107, train loss: 0.26044, val loss: 0.27883\n",
      "Interaction training epoch: 108, train loss: 0.25473, val loss: 0.27841\n",
      "Interaction training epoch: 109, train loss: 0.25635, val loss: 0.27638\n",
      "Interaction training epoch: 110, train loss: 0.25313, val loss: 0.27427\n",
      "Interaction training epoch: 111, train loss: 0.25541, val loss: 0.27997\n",
      "Interaction training epoch: 112, train loss: 0.25237, val loss: 0.27394\n",
      "Interaction training epoch: 113, train loss: 0.25245, val loss: 0.27539\n",
      "Interaction training epoch: 114, train loss: 0.25142, val loss: 0.27460\n",
      "Interaction training epoch: 115, train loss: 0.25383, val loss: 0.27824\n",
      "Interaction training epoch: 116, train loss: 0.24988, val loss: 0.26971\n",
      "Interaction training epoch: 117, train loss: 0.25132, val loss: 0.27554\n",
      "Interaction training epoch: 118, train loss: 0.25175, val loss: 0.27427\n",
      "Interaction training epoch: 119, train loss: 0.25003, val loss: 0.27542\n",
      "Interaction training epoch: 120, train loss: 0.24870, val loss: 0.27514\n",
      "Interaction training epoch: 121, train loss: 0.25166, val loss: 0.27042\n",
      "Interaction training epoch: 122, train loss: 0.25832, val loss: 0.28875\n",
      "Interaction training epoch: 123, train loss: 0.25870, val loss: 0.27728\n",
      "Interaction training epoch: 124, train loss: 0.25335, val loss: 0.28110\n",
      "Interaction training epoch: 125, train loss: 0.25099, val loss: 0.27801\n",
      "Interaction training epoch: 126, train loss: 0.25155, val loss: 0.27144\n",
      "Interaction training epoch: 127, train loss: 0.25027, val loss: 0.27762\n",
      "Interaction training epoch: 128, train loss: 0.25541, val loss: 0.27944\n",
      "Interaction training epoch: 129, train loss: 0.24855, val loss: 0.27529\n",
      "Interaction training epoch: 130, train loss: 0.24403, val loss: 0.26696\n",
      "Interaction training epoch: 131, train loss: 0.24509, val loss: 0.26932\n",
      "Interaction training epoch: 132, train loss: 0.24357, val loss: 0.27379\n",
      "Interaction training epoch: 133, train loss: 0.24369, val loss: 0.27203\n",
      "Interaction training epoch: 134, train loss: 0.24267, val loss: 0.26981\n",
      "Interaction training epoch: 135, train loss: 0.24293, val loss: 0.26533\n",
      "Interaction training epoch: 136, train loss: 0.24558, val loss: 0.26901\n",
      "Interaction training epoch: 137, train loss: 0.23973, val loss: 0.26572\n",
      "Interaction training epoch: 138, train loss: 0.24137, val loss: 0.26647\n",
      "Interaction training epoch: 139, train loss: 0.24092, val loss: 0.26885\n",
      "Interaction training epoch: 140, train loss: 0.24010, val loss: 0.27178\n",
      "Interaction training epoch: 141, train loss: 0.23803, val loss: 0.26453\n",
      "Interaction training epoch: 142, train loss: 0.24100, val loss: 0.27200\n",
      "Interaction training epoch: 143, train loss: 0.23959, val loss: 0.26787\n",
      "Interaction training epoch: 144, train loss: 0.23932, val loss: 0.26931\n",
      "Interaction training epoch: 145, train loss: 0.23860, val loss: 0.26825\n",
      "Interaction training epoch: 146, train loss: 0.24088, val loss: 0.26713\n",
      "Interaction training epoch: 147, train loss: 0.24274, val loss: 0.26637\n",
      "Interaction training epoch: 148, train loss: 0.23933, val loss: 0.27463\n",
      "Interaction training epoch: 149, train loss: 0.25493, val loss: 0.29218\n",
      "Interaction training epoch: 150, train loss: 0.23863, val loss: 0.26888\n",
      "Interaction training epoch: 151, train loss: 0.24491, val loss: 0.26572\n",
      "Interaction training epoch: 152, train loss: 0.24275, val loss: 0.27772\n",
      "Interaction training epoch: 153, train loss: 0.23868, val loss: 0.26807\n",
      "Interaction training epoch: 154, train loss: 0.23748, val loss: 0.26486\n",
      "Interaction training epoch: 155, train loss: 0.23742, val loss: 0.26799\n",
      "Interaction training epoch: 156, train loss: 0.23584, val loss: 0.27213\n",
      "Interaction training epoch: 157, train loss: 0.24468, val loss: 0.27807\n",
      "Interaction training epoch: 158, train loss: 0.23731, val loss: 0.26425\n",
      "Interaction training epoch: 159, train loss: 0.23826, val loss: 0.26509\n",
      "Interaction training epoch: 160, train loss: 0.23677, val loss: 0.27225\n",
      "Interaction training epoch: 161, train loss: 0.23763, val loss: 0.27376\n",
      "Interaction training epoch: 162, train loss: 0.23428, val loss: 0.26504\n",
      "Interaction training epoch: 163, train loss: 0.23301, val loss: 0.26722\n",
      "Interaction training epoch: 164, train loss: 0.23386, val loss: 0.26970\n",
      "Interaction training epoch: 165, train loss: 0.23473, val loss: 0.26521\n",
      "Interaction training epoch: 166, train loss: 0.23495, val loss: 0.26435\n",
      "Interaction training epoch: 167, train loss: 0.24660, val loss: 0.29391\n",
      "Interaction training epoch: 168, train loss: 0.23573, val loss: 0.26707\n",
      "Interaction training epoch: 169, train loss: 0.23828, val loss: 0.26509\n",
      "Interaction training epoch: 170, train loss: 0.23605, val loss: 0.27058\n",
      "Interaction training epoch: 171, train loss: 0.23780, val loss: 0.27742\n",
      "Interaction training epoch: 172, train loss: 0.23690, val loss: 0.26307\n",
      "Interaction training epoch: 173, train loss: 0.23469, val loss: 0.27211\n",
      "Interaction training epoch: 174, train loss: 0.23142, val loss: 0.26655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 175, train loss: 0.23119, val loss: 0.26754\n",
      "Interaction training epoch: 176, train loss: 0.23210, val loss: 0.26475\n",
      "Interaction training epoch: 177, train loss: 0.23157, val loss: 0.27006\n",
      "Interaction training epoch: 178, train loss: 0.23066, val loss: 0.26743\n",
      "Interaction training epoch: 179, train loss: 0.23023, val loss: 0.26442\n",
      "Interaction training epoch: 180, train loss: 0.22949, val loss: 0.26793\n",
      "Interaction training epoch: 181, train loss: 0.23038, val loss: 0.26510\n",
      "Interaction training epoch: 182, train loss: 0.23059, val loss: 0.26974\n",
      "Interaction training epoch: 183, train loss: 0.23136, val loss: 0.27102\n",
      "Interaction training epoch: 184, train loss: 0.23094, val loss: 0.26745\n",
      "Interaction training epoch: 185, train loss: 0.23274, val loss: 0.26397\n",
      "Interaction training epoch: 186, train loss: 0.23044, val loss: 0.26953\n",
      "Interaction training epoch: 187, train loss: 0.23609, val loss: 0.27794\n",
      "Interaction training epoch: 188, train loss: 0.22894, val loss: 0.26329\n",
      "Interaction training epoch: 189, train loss: 0.22890, val loss: 0.26857\n",
      "Interaction training epoch: 190, train loss: 0.22828, val loss: 0.26733\n",
      "Interaction training epoch: 191, train loss: 0.22997, val loss: 0.26556\n",
      "Interaction training epoch: 192, train loss: 0.22867, val loss: 0.27089\n",
      "Interaction training epoch: 193, train loss: 0.23183, val loss: 0.26633\n",
      "Interaction training epoch: 194, train loss: 0.23172, val loss: 0.27152\n",
      "Interaction training epoch: 195, train loss: 0.23146, val loss: 0.26138\n",
      "Interaction training epoch: 196, train loss: 0.23615, val loss: 0.28318\n",
      "Interaction training epoch: 197, train loss: 0.22912, val loss: 0.26769\n",
      "Interaction training epoch: 198, train loss: 0.23224, val loss: 0.26230\n",
      "Interaction training epoch: 199, train loss: 0.22862, val loss: 0.27042\n",
      "Interaction training epoch: 200, train loss: 0.23069, val loss: 0.27190\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########2 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.24150, val loss: 0.26602\n",
      "Interaction tuning epoch: 2, train loss: 0.23637, val loss: 0.26259\n",
      "Interaction tuning epoch: 3, train loss: 0.24169, val loss: 0.27598\n",
      "Interaction tuning epoch: 4, train loss: 0.23770, val loss: 0.26141\n",
      "Interaction tuning epoch: 5, train loss: 0.23470, val loss: 0.26290\n",
      "Interaction tuning epoch: 6, train loss: 0.23813, val loss: 0.27084\n",
      "Interaction tuning epoch: 7, train loss: 0.23704, val loss: 0.26649\n",
      "Interaction tuning epoch: 8, train loss: 0.23485, val loss: 0.25943\n",
      "Interaction tuning epoch: 9, train loss: 0.23558, val loss: 0.26308\n",
      "Interaction tuning epoch: 10, train loss: 0.23329, val loss: 0.26071\n",
      "Interaction tuning epoch: 11, train loss: 0.23374, val loss: 0.26366\n",
      "Interaction tuning epoch: 12, train loss: 0.24171, val loss: 0.27760\n",
      "Interaction tuning epoch: 13, train loss: 0.23864, val loss: 0.26195\n",
      "Interaction tuning epoch: 14, train loss: 0.23356, val loss: 0.26253\n",
      "Interaction tuning epoch: 15, train loss: 0.23656, val loss: 0.26989\n",
      "Interaction tuning epoch: 16, train loss: 0.23867, val loss: 0.25881\n",
      "Interaction tuning epoch: 17, train loss: 0.23388, val loss: 0.26552\n",
      "Interaction tuning epoch: 18, train loss: 0.23490, val loss: 0.26201\n",
      "Interaction tuning epoch: 19, train loss: 0.23301, val loss: 0.26129\n",
      "Interaction tuning epoch: 20, train loss: 0.23270, val loss: 0.26222\n",
      "Interaction tuning epoch: 21, train loss: 0.23686, val loss: 0.26567\n",
      "Interaction tuning epoch: 22, train loss: 0.23386, val loss: 0.26345\n",
      "Interaction tuning epoch: 23, train loss: 0.23490, val loss: 0.26285\n",
      "Interaction tuning epoch: 24, train loss: 0.23498, val loss: 0.26880\n",
      "Interaction tuning epoch: 25, train loss: 0.23343, val loss: 0.26095\n",
      "Interaction tuning epoch: 26, train loss: 0.23186, val loss: 0.25940\n",
      "Interaction tuning epoch: 27, train loss: 0.22951, val loss: 0.25637\n",
      "Interaction tuning epoch: 28, train loss: 0.23366, val loss: 0.26728\n",
      "Interaction tuning epoch: 29, train loss: 0.23030, val loss: 0.25544\n",
      "Interaction tuning epoch: 30, train loss: 0.23475, val loss: 0.26435\n",
      "Interaction tuning epoch: 31, train loss: 0.23021, val loss: 0.26254\n",
      "Interaction tuning epoch: 32, train loss: 0.23428, val loss: 0.26668\n",
      "Interaction tuning epoch: 33, train loss: 0.23001, val loss: 0.25869\n",
      "Interaction tuning epoch: 34, train loss: 0.23002, val loss: 0.26189\n",
      "Interaction tuning epoch: 35, train loss: 0.23073, val loss: 0.26451\n",
      "Interaction tuning epoch: 36, train loss: 0.23166, val loss: 0.26262\n",
      "Interaction tuning epoch: 37, train loss: 0.22902, val loss: 0.25691\n",
      "Interaction tuning epoch: 38, train loss: 0.23046, val loss: 0.26751\n",
      "Interaction tuning epoch: 39, train loss: 0.23207, val loss: 0.26167\n",
      "Interaction tuning epoch: 40, train loss: 0.23161, val loss: 0.26099\n",
      "Interaction tuning epoch: 41, train loss: 0.22828, val loss: 0.25991\n",
      "Interaction tuning epoch: 42, train loss: 0.23051, val loss: 0.26092\n",
      "Interaction tuning epoch: 43, train loss: 0.23044, val loss: 0.25767\n",
      "Interaction tuning epoch: 44, train loss: 0.22874, val loss: 0.26052\n",
      "Interaction tuning epoch: 45, train loss: 0.22967, val loss: 0.26122\n",
      "Interaction tuning epoch: 46, train loss: 0.22683, val loss: 0.25690\n",
      "Interaction tuning epoch: 47, train loss: 0.22996, val loss: 0.26550\n",
      "Interaction tuning epoch: 48, train loss: 0.22732, val loss: 0.25566\n",
      "Interaction tuning epoch: 49, train loss: 0.22741, val loss: 0.26139\n",
      "Interaction tuning epoch: 50, train loss: 0.22719, val loss: 0.25841\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 39.657283544540405\n",
      "After the gam stage, training error is 0.22719 , validation error is 0.25841\n",
      "missing value counts: 99145\n",
      "[SoftImpute] Max Singular Value of X_init = 3.674031\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.192455 validation BCE=0.266912,rank=5\n",
      "[SoftImpute] Iter 2: observed BCE=0.188564 validation BCE=0.255635,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.185950 validation BCE=0.256503,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.184265 validation BCE=0.266476,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.183107 validation BCE=0.266594,rank=5\n",
      "[SoftImpute] Stopped after iteration 5 for lambda=0.073481\n",
      "final num of user group: 26\n",
      "final num of item group: 43\n",
      "change mode state : True\n",
      "time cost: 2.4644978046417236\n",
      "After the matrix factor stage, training error is 0.18311, validation error is 0.26659\n",
      "5\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68112, val loss: 0.68331\n",
      "Main effects training epoch: 2, train loss: 0.67573, val loss: 0.68003\n",
      "Main effects training epoch: 3, train loss: 0.67311, val loss: 0.67822\n",
      "Main effects training epoch: 4, train loss: 0.66742, val loss: 0.67044\n",
      "Main effects training epoch: 5, train loss: 0.65826, val loss: 0.65978\n",
      "Main effects training epoch: 6, train loss: 0.63739, val loss: 0.63839\n",
      "Main effects training epoch: 7, train loss: 0.59708, val loss: 0.59593\n",
      "Main effects training epoch: 8, train loss: 0.54937, val loss: 0.54657\n",
      "Main effects training epoch: 9, train loss: 0.52711, val loss: 0.51912\n",
      "Main effects training epoch: 10, train loss: 0.52733, val loss: 0.51704\n",
      "Main effects training epoch: 11, train loss: 0.52477, val loss: 0.52191\n",
      "Main effects training epoch: 12, train loss: 0.52557, val loss: 0.51324\n",
      "Main effects training epoch: 13, train loss: 0.52409, val loss: 0.52200\n",
      "Main effects training epoch: 14, train loss: 0.52341, val loss: 0.51383\n",
      "Main effects training epoch: 15, train loss: 0.52200, val loss: 0.51682\n",
      "Main effects training epoch: 16, train loss: 0.52095, val loss: 0.51147\n",
      "Main effects training epoch: 17, train loss: 0.52046, val loss: 0.51426\n",
      "Main effects training epoch: 18, train loss: 0.52081, val loss: 0.51212\n",
      "Main effects training epoch: 19, train loss: 0.52202, val loss: 0.51474\n",
      "Main effects training epoch: 20, train loss: 0.52138, val loss: 0.51446\n",
      "Main effects training epoch: 21, train loss: 0.52146, val loss: 0.51475\n",
      "Main effects training epoch: 22, train loss: 0.52284, val loss: 0.51515\n",
      "Main effects training epoch: 23, train loss: 0.52124, val loss: 0.51462\n",
      "Main effects training epoch: 24, train loss: 0.52047, val loss: 0.51387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 25, train loss: 0.52094, val loss: 0.51444\n",
      "Main effects training epoch: 26, train loss: 0.52039, val loss: 0.51219\n",
      "Main effects training epoch: 27, train loss: 0.52064, val loss: 0.51544\n",
      "Main effects training epoch: 28, train loss: 0.52070, val loss: 0.51221\n",
      "Main effects training epoch: 29, train loss: 0.52129, val loss: 0.51553\n",
      "Main effects training epoch: 30, train loss: 0.52020, val loss: 0.51339\n",
      "Main effects training epoch: 31, train loss: 0.52015, val loss: 0.51324\n",
      "Main effects training epoch: 32, train loss: 0.52039, val loss: 0.51289\n",
      "Main effects training epoch: 33, train loss: 0.52073, val loss: 0.51261\n",
      "Main effects training epoch: 34, train loss: 0.51978, val loss: 0.51259\n",
      "Main effects training epoch: 35, train loss: 0.51991, val loss: 0.51383\n",
      "Main effects training epoch: 36, train loss: 0.52011, val loss: 0.51308\n",
      "Main effects training epoch: 37, train loss: 0.52016, val loss: 0.51175\n",
      "Main effects training epoch: 38, train loss: 0.52100, val loss: 0.51626\n",
      "Main effects training epoch: 39, train loss: 0.52007, val loss: 0.51323\n",
      "Main effects training epoch: 40, train loss: 0.51982, val loss: 0.51339\n",
      "Main effects training epoch: 41, train loss: 0.52010, val loss: 0.51138\n",
      "Main effects training epoch: 42, train loss: 0.52004, val loss: 0.51523\n",
      "Main effects training epoch: 43, train loss: 0.51996, val loss: 0.51256\n",
      "Main effects training epoch: 44, train loss: 0.51979, val loss: 0.51381\n",
      "Main effects training epoch: 45, train loss: 0.51949, val loss: 0.51298\n",
      "Main effects training epoch: 46, train loss: 0.51976, val loss: 0.51163\n",
      "Main effects training epoch: 47, train loss: 0.51939, val loss: 0.51372\n",
      "Main effects training epoch: 48, train loss: 0.51924, val loss: 0.51269\n",
      "Main effects training epoch: 49, train loss: 0.51922, val loss: 0.51264\n",
      "Main effects training epoch: 50, train loss: 0.51936, val loss: 0.51295\n",
      "Main effects training epoch: 51, train loss: 0.51927, val loss: 0.51199\n",
      "Main effects training epoch: 52, train loss: 0.51948, val loss: 0.51375\n",
      "Main effects training epoch: 53, train loss: 0.51949, val loss: 0.51273\n",
      "Main effects training epoch: 54, train loss: 0.51939, val loss: 0.51200\n",
      "Main effects training epoch: 55, train loss: 0.51945, val loss: 0.51427\n",
      "Main effects training epoch: 56, train loss: 0.51897, val loss: 0.51255\n",
      "Main effects training epoch: 57, train loss: 0.51907, val loss: 0.51251\n",
      "Main effects training epoch: 58, train loss: 0.51968, val loss: 0.51088\n",
      "Main effects training epoch: 59, train loss: 0.51949, val loss: 0.51471\n",
      "Main effects training epoch: 60, train loss: 0.51901, val loss: 0.51270\n",
      "Main effects training epoch: 61, train loss: 0.51883, val loss: 0.51264\n",
      "Main effects training epoch: 62, train loss: 0.51892, val loss: 0.51206\n",
      "Main effects training epoch: 63, train loss: 0.51892, val loss: 0.51262\n",
      "Main effects training epoch: 64, train loss: 0.51874, val loss: 0.51290\n",
      "Main effects training epoch: 65, train loss: 0.51934, val loss: 0.51198\n",
      "Main effects training epoch: 66, train loss: 0.51894, val loss: 0.51403\n",
      "Main effects training epoch: 67, train loss: 0.52004, val loss: 0.51091\n",
      "Main effects training epoch: 68, train loss: 0.52043, val loss: 0.51804\n",
      "Main effects training epoch: 69, train loss: 0.51909, val loss: 0.51148\n",
      "Main effects training epoch: 70, train loss: 0.51860, val loss: 0.51279\n",
      "Main effects training epoch: 71, train loss: 0.51860, val loss: 0.51234\n",
      "Main effects training epoch: 72, train loss: 0.51880, val loss: 0.51415\n",
      "Main effects training epoch: 73, train loss: 0.51882, val loss: 0.51146\n",
      "Main effects training epoch: 74, train loss: 0.51906, val loss: 0.51470\n",
      "Main effects training epoch: 75, train loss: 0.51842, val loss: 0.51282\n",
      "Main effects training epoch: 76, train loss: 0.51882, val loss: 0.51326\n",
      "Main effects training epoch: 77, train loss: 0.51929, val loss: 0.51300\n",
      "Main effects training epoch: 78, train loss: 0.51916, val loss: 0.51159\n",
      "Main effects training epoch: 79, train loss: 0.51848, val loss: 0.51301\n",
      "Main effects training epoch: 80, train loss: 0.51824, val loss: 0.51312\n",
      "Main effects training epoch: 81, train loss: 0.51841, val loss: 0.51314\n",
      "Main effects training epoch: 82, train loss: 0.51832, val loss: 0.51140\n",
      "Main effects training epoch: 83, train loss: 0.51817, val loss: 0.51265\n",
      "Main effects training epoch: 84, train loss: 0.51849, val loss: 0.51138\n",
      "Main effects training epoch: 85, train loss: 0.51860, val loss: 0.51403\n",
      "Main effects training epoch: 86, train loss: 0.51816, val loss: 0.51060\n",
      "Main effects training epoch: 87, train loss: 0.51830, val loss: 0.51420\n",
      "Main effects training epoch: 88, train loss: 0.51811, val loss: 0.51181\n",
      "Main effects training epoch: 89, train loss: 0.51789, val loss: 0.51153\n",
      "Main effects training epoch: 90, train loss: 0.51803, val loss: 0.51125\n",
      "Main effects training epoch: 91, train loss: 0.51785, val loss: 0.51248\n",
      "Main effects training epoch: 92, train loss: 0.51800, val loss: 0.51339\n",
      "Main effects training epoch: 93, train loss: 0.51768, val loss: 0.51184\n",
      "Main effects training epoch: 94, train loss: 0.51785, val loss: 0.51309\n",
      "Main effects training epoch: 95, train loss: 0.51756, val loss: 0.51178\n",
      "Main effects training epoch: 96, train loss: 0.51788, val loss: 0.51333\n",
      "Main effects training epoch: 97, train loss: 0.51798, val loss: 0.51009\n",
      "Main effects training epoch: 98, train loss: 0.51792, val loss: 0.51437\n",
      "Main effects training epoch: 99, train loss: 0.51744, val loss: 0.51215\n",
      "Main effects training epoch: 100, train loss: 0.51794, val loss: 0.51234\n",
      "Main effects training epoch: 101, train loss: 0.51764, val loss: 0.51140\n",
      "Main effects training epoch: 102, train loss: 0.51743, val loss: 0.51251\n",
      "Main effects training epoch: 103, train loss: 0.51729, val loss: 0.51150\n",
      "Main effects training epoch: 104, train loss: 0.51744, val loss: 0.51164\n",
      "Main effects training epoch: 105, train loss: 0.51708, val loss: 0.51256\n",
      "Main effects training epoch: 106, train loss: 0.51697, val loss: 0.51084\n",
      "Main effects training epoch: 107, train loss: 0.51714, val loss: 0.51232\n",
      "Main effects training epoch: 108, train loss: 0.51728, val loss: 0.51145\n",
      "Main effects training epoch: 109, train loss: 0.51749, val loss: 0.51073\n",
      "Main effects training epoch: 110, train loss: 0.51732, val loss: 0.51336\n",
      "Main effects training epoch: 111, train loss: 0.51751, val loss: 0.51095\n",
      "Main effects training epoch: 112, train loss: 0.51711, val loss: 0.51197\n",
      "Main effects training epoch: 113, train loss: 0.51671, val loss: 0.51211\n",
      "Main effects training epoch: 114, train loss: 0.51653, val loss: 0.51231\n",
      "Main effects training epoch: 115, train loss: 0.51643, val loss: 0.51165\n",
      "Main effects training epoch: 116, train loss: 0.51745, val loss: 0.51163\n",
      "Main effects training epoch: 117, train loss: 0.51694, val loss: 0.51128\n",
      "Main effects training epoch: 118, train loss: 0.51580, val loss: 0.51098\n",
      "Main effects training epoch: 119, train loss: 0.51567, val loss: 0.51054\n",
      "Main effects training epoch: 120, train loss: 0.51554, val loss: 0.51155\n",
      "Main effects training epoch: 121, train loss: 0.51548, val loss: 0.50920\n",
      "Main effects training epoch: 122, train loss: 0.51542, val loss: 0.51144\n",
      "Main effects training epoch: 123, train loss: 0.51640, val loss: 0.51117\n",
      "Main effects training epoch: 124, train loss: 0.51621, val loss: 0.51162\n",
      "Main effects training epoch: 125, train loss: 0.51512, val loss: 0.50932\n",
      "Main effects training epoch: 126, train loss: 0.51472, val loss: 0.51049\n",
      "Main effects training epoch: 127, train loss: 0.51451, val loss: 0.50898\n",
      "Main effects training epoch: 128, train loss: 0.51506, val loss: 0.51144\n",
      "Main effects training epoch: 129, train loss: 0.51476, val loss: 0.50903\n",
      "Main effects training epoch: 130, train loss: 0.51450, val loss: 0.50955\n",
      "Main effects training epoch: 131, train loss: 0.51390, val loss: 0.50894\n",
      "Main effects training epoch: 132, train loss: 0.51430, val loss: 0.51079\n",
      "Main effects training epoch: 133, train loss: 0.51417, val loss: 0.51005\n",
      "Main effects training epoch: 134, train loss: 0.51481, val loss: 0.50951\n",
      "Main effects training epoch: 135, train loss: 0.51386, val loss: 0.50994\n",
      "Main effects training epoch: 136, train loss: 0.51442, val loss: 0.50905\n",
      "Main effects training epoch: 137, train loss: 0.51485, val loss: 0.50926\n",
      "Main effects training epoch: 138, train loss: 0.51413, val loss: 0.51047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 139, train loss: 0.51360, val loss: 0.50842\n",
      "Main effects training epoch: 140, train loss: 0.51371, val loss: 0.50983\n",
      "Main effects training epoch: 141, train loss: 0.51373, val loss: 0.50961\n",
      "Main effects training epoch: 142, train loss: 0.51383, val loss: 0.50875\n",
      "Main effects training epoch: 143, train loss: 0.51414, val loss: 0.50978\n",
      "Main effects training epoch: 144, train loss: 0.51367, val loss: 0.50883\n",
      "Main effects training epoch: 145, train loss: 0.51351, val loss: 0.50935\n",
      "Main effects training epoch: 146, train loss: 0.51377, val loss: 0.50909\n",
      "Main effects training epoch: 147, train loss: 0.51389, val loss: 0.50782\n",
      "Main effects training epoch: 148, train loss: 0.51395, val loss: 0.50892\n",
      "Main effects training epoch: 149, train loss: 0.51404, val loss: 0.50847\n",
      "Main effects training epoch: 150, train loss: 0.51372, val loss: 0.51017\n",
      "Main effects training epoch: 151, train loss: 0.51358, val loss: 0.50824\n",
      "Main effects training epoch: 152, train loss: 0.51319, val loss: 0.50730\n",
      "Main effects training epoch: 153, train loss: 0.51337, val loss: 0.50945\n",
      "Main effects training epoch: 154, train loss: 0.51347, val loss: 0.50709\n",
      "Main effects training epoch: 155, train loss: 0.51317, val loss: 0.50763\n",
      "Main effects training epoch: 156, train loss: 0.51340, val loss: 0.50739\n",
      "Main effects training epoch: 157, train loss: 0.51297, val loss: 0.50779\n",
      "Main effects training epoch: 158, train loss: 0.51323, val loss: 0.50760\n",
      "Main effects training epoch: 159, train loss: 0.51310, val loss: 0.50716\n",
      "Main effects training epoch: 160, train loss: 0.51300, val loss: 0.50748\n",
      "Main effects training epoch: 161, train loss: 0.51284, val loss: 0.50782\n",
      "Main effects training epoch: 162, train loss: 0.51273, val loss: 0.50740\n",
      "Main effects training epoch: 163, train loss: 0.51284, val loss: 0.50648\n",
      "Main effects training epoch: 164, train loss: 0.51308, val loss: 0.50893\n",
      "Main effects training epoch: 165, train loss: 0.51305, val loss: 0.50684\n",
      "Main effects training epoch: 166, train loss: 0.51378, val loss: 0.50911\n",
      "Main effects training epoch: 167, train loss: 0.51361, val loss: 0.50716\n",
      "Main effects training epoch: 168, train loss: 0.51292, val loss: 0.50837\n",
      "Main effects training epoch: 169, train loss: 0.51332, val loss: 0.50743\n",
      "Main effects training epoch: 170, train loss: 0.51307, val loss: 0.50818\n",
      "Main effects training epoch: 171, train loss: 0.51270, val loss: 0.50665\n",
      "Main effects training epoch: 172, train loss: 0.51252, val loss: 0.50687\n",
      "Main effects training epoch: 173, train loss: 0.51254, val loss: 0.50608\n",
      "Main effects training epoch: 174, train loss: 0.51280, val loss: 0.50897\n",
      "Main effects training epoch: 175, train loss: 0.51290, val loss: 0.50646\n",
      "Main effects training epoch: 176, train loss: 0.51281, val loss: 0.50850\n",
      "Main effects training epoch: 177, train loss: 0.51285, val loss: 0.50592\n",
      "Main effects training epoch: 178, train loss: 0.51338, val loss: 0.50831\n",
      "Main effects training epoch: 179, train loss: 0.51308, val loss: 0.50757\n",
      "Main effects training epoch: 180, train loss: 0.51257, val loss: 0.50689\n",
      "Main effects training epoch: 181, train loss: 0.51263, val loss: 0.50549\n",
      "Main effects training epoch: 182, train loss: 0.51252, val loss: 0.50716\n",
      "Main effects training epoch: 183, train loss: 0.51320, val loss: 0.50576\n",
      "Main effects training epoch: 184, train loss: 0.51344, val loss: 0.50890\n",
      "Main effects training epoch: 185, train loss: 0.51306, val loss: 0.50664\n",
      "Main effects training epoch: 186, train loss: 0.51253, val loss: 0.50798\n",
      "Main effects training epoch: 187, train loss: 0.51243, val loss: 0.50509\n",
      "Main effects training epoch: 188, train loss: 0.51233, val loss: 0.50692\n",
      "Main effects training epoch: 189, train loss: 0.51222, val loss: 0.50584\n",
      "Main effects training epoch: 190, train loss: 0.51199, val loss: 0.50623\n",
      "Main effects training epoch: 191, train loss: 0.51187, val loss: 0.50536\n",
      "Main effects training epoch: 192, train loss: 0.51199, val loss: 0.50486\n",
      "Main effects training epoch: 193, train loss: 0.51229, val loss: 0.50700\n",
      "Main effects training epoch: 194, train loss: 0.51223, val loss: 0.50542\n",
      "Main effects training epoch: 195, train loss: 0.51200, val loss: 0.50635\n",
      "Main effects training epoch: 196, train loss: 0.51176, val loss: 0.50601\n",
      "Main effects training epoch: 197, train loss: 0.51178, val loss: 0.50483\n",
      "Main effects training epoch: 198, train loss: 0.51171, val loss: 0.50605\n",
      "Main effects training epoch: 199, train loss: 0.51196, val loss: 0.50661\n",
      "Main effects training epoch: 200, train loss: 0.51174, val loss: 0.50528\n",
      "Main effects training epoch: 201, train loss: 0.51218, val loss: 0.50497\n",
      "Main effects training epoch: 202, train loss: 0.51232, val loss: 0.50785\n",
      "Main effects training epoch: 203, train loss: 0.51214, val loss: 0.50540\n",
      "Main effects training epoch: 204, train loss: 0.51201, val loss: 0.50721\n",
      "Main effects training epoch: 205, train loss: 0.51173, val loss: 0.50501\n",
      "Main effects training epoch: 206, train loss: 0.51164, val loss: 0.50470\n",
      "Main effects training epoch: 207, train loss: 0.51143, val loss: 0.50556\n",
      "Main effects training epoch: 208, train loss: 0.51172, val loss: 0.50624\n",
      "Main effects training epoch: 209, train loss: 0.51170, val loss: 0.50412\n",
      "Main effects training epoch: 210, train loss: 0.51149, val loss: 0.50609\n",
      "Main effects training epoch: 211, train loss: 0.51157, val loss: 0.50375\n",
      "Main effects training epoch: 212, train loss: 0.51132, val loss: 0.50516\n",
      "Main effects training epoch: 213, train loss: 0.51189, val loss: 0.50532\n",
      "Main effects training epoch: 214, train loss: 0.51157, val loss: 0.50494\n",
      "Main effects training epoch: 215, train loss: 0.51172, val loss: 0.50640\n",
      "Main effects training epoch: 216, train loss: 0.51192, val loss: 0.50473\n",
      "Main effects training epoch: 217, train loss: 0.51152, val loss: 0.50648\n",
      "Main effects training epoch: 218, train loss: 0.51116, val loss: 0.50380\n",
      "Main effects training epoch: 219, train loss: 0.51167, val loss: 0.50387\n",
      "Main effects training epoch: 220, train loss: 0.51175, val loss: 0.50757\n",
      "Main effects training epoch: 221, train loss: 0.51127, val loss: 0.50316\n",
      "Main effects training epoch: 222, train loss: 0.51142, val loss: 0.50639\n",
      "Main effects training epoch: 223, train loss: 0.51108, val loss: 0.50367\n",
      "Main effects training epoch: 224, train loss: 0.51143, val loss: 0.50547\n",
      "Main effects training epoch: 225, train loss: 0.51139, val loss: 0.50365\n",
      "Main effects training epoch: 226, train loss: 0.51145, val loss: 0.50631\n",
      "Main effects training epoch: 227, train loss: 0.51160, val loss: 0.50303\n",
      "Main effects training epoch: 228, train loss: 0.51274, val loss: 0.50726\n",
      "Main effects training epoch: 229, train loss: 0.51155, val loss: 0.50487\n",
      "Main effects training epoch: 230, train loss: 0.51103, val loss: 0.50348\n",
      "Main effects training epoch: 231, train loss: 0.51093, val loss: 0.50463\n",
      "Main effects training epoch: 232, train loss: 0.51096, val loss: 0.50441\n",
      "Main effects training epoch: 233, train loss: 0.51116, val loss: 0.50322\n",
      "Main effects training epoch: 234, train loss: 0.51110, val loss: 0.50441\n",
      "Main effects training epoch: 235, train loss: 0.51101, val loss: 0.50403\n",
      "Main effects training epoch: 236, train loss: 0.51106, val loss: 0.50472\n",
      "Main effects training epoch: 237, train loss: 0.51189, val loss: 0.50404\n",
      "Main effects training epoch: 238, train loss: 0.51175, val loss: 0.50579\n",
      "Main effects training epoch: 239, train loss: 0.51236, val loss: 0.50364\n",
      "Main effects training epoch: 240, train loss: 0.51238, val loss: 0.50589\n",
      "Main effects training epoch: 241, train loss: 0.51115, val loss: 0.50463\n",
      "Main effects training epoch: 242, train loss: 0.51101, val loss: 0.50302\n",
      "Main effects training epoch: 243, train loss: 0.51096, val loss: 0.50333\n",
      "Main effects training epoch: 244, train loss: 0.51113, val loss: 0.50426\n",
      "Main effects training epoch: 245, train loss: 0.51094, val loss: 0.50338\n",
      "Main effects training epoch: 246, train loss: 0.51094, val loss: 0.50451\n",
      "Main effects training epoch: 247, train loss: 0.51078, val loss: 0.50396\n",
      "Main effects training epoch: 248, train loss: 0.51069, val loss: 0.50436\n",
      "Main effects training epoch: 249, train loss: 0.51048, val loss: 0.50305\n",
      "Main effects training epoch: 250, train loss: 0.51048, val loss: 0.50446\n",
      "Main effects training epoch: 251, train loss: 0.51057, val loss: 0.50218\n",
      "Main effects training epoch: 252, train loss: 0.51049, val loss: 0.50462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 253, train loss: 0.51038, val loss: 0.50388\n",
      "Main effects training epoch: 254, train loss: 0.51030, val loss: 0.50225\n",
      "Main effects training epoch: 255, train loss: 0.51045, val loss: 0.50448\n",
      "Main effects training epoch: 256, train loss: 0.51038, val loss: 0.50290\n",
      "Main effects training epoch: 257, train loss: 0.51118, val loss: 0.50347\n",
      "Main effects training epoch: 258, train loss: 0.51029, val loss: 0.50385\n",
      "Main effects training epoch: 259, train loss: 0.51026, val loss: 0.50256\n",
      "Main effects training epoch: 260, train loss: 0.51009, val loss: 0.50286\n",
      "Main effects training epoch: 261, train loss: 0.51024, val loss: 0.50312\n",
      "Main effects training epoch: 262, train loss: 0.51037, val loss: 0.50313\n",
      "Main effects training epoch: 263, train loss: 0.51068, val loss: 0.50404\n",
      "Main effects training epoch: 264, train loss: 0.51035, val loss: 0.50213\n",
      "Main effects training epoch: 265, train loss: 0.51023, val loss: 0.50239\n",
      "Main effects training epoch: 266, train loss: 0.51040, val loss: 0.50401\n",
      "Main effects training epoch: 267, train loss: 0.51023, val loss: 0.50233\n",
      "Main effects training epoch: 268, train loss: 0.51011, val loss: 0.50429\n",
      "Main effects training epoch: 269, train loss: 0.50988, val loss: 0.50242\n",
      "Main effects training epoch: 270, train loss: 0.50986, val loss: 0.50315\n",
      "Main effects training epoch: 271, train loss: 0.50994, val loss: 0.50339\n",
      "Main effects training epoch: 272, train loss: 0.50991, val loss: 0.50234\n",
      "Main effects training epoch: 273, train loss: 0.50988, val loss: 0.50255\n",
      "Main effects training epoch: 274, train loss: 0.50970, val loss: 0.50209\n",
      "Main effects training epoch: 275, train loss: 0.50957, val loss: 0.50263\n",
      "Main effects training epoch: 276, train loss: 0.50955, val loss: 0.50254\n",
      "Main effects training epoch: 277, train loss: 0.50980, val loss: 0.50421\n",
      "Main effects training epoch: 278, train loss: 0.50970, val loss: 0.50268\n",
      "Main effects training epoch: 279, train loss: 0.50979, val loss: 0.50257\n",
      "Main effects training epoch: 280, train loss: 0.50979, val loss: 0.50276\n",
      "Main effects training epoch: 281, train loss: 0.50947, val loss: 0.50236\n",
      "Main effects training epoch: 282, train loss: 0.50934, val loss: 0.50194\n",
      "Main effects training epoch: 283, train loss: 0.50934, val loss: 0.50374\n",
      "Main effects training epoch: 284, train loss: 0.50928, val loss: 0.50311\n",
      "Main effects training epoch: 285, train loss: 0.50947, val loss: 0.50312\n",
      "Main effects training epoch: 286, train loss: 0.50939, val loss: 0.50162\n",
      "Main effects training epoch: 287, train loss: 0.50922, val loss: 0.50363\n",
      "Main effects training epoch: 288, train loss: 0.50914, val loss: 0.50226\n",
      "Main effects training epoch: 289, train loss: 0.50959, val loss: 0.50330\n",
      "Main effects training epoch: 290, train loss: 0.50931, val loss: 0.50438\n",
      "Main effects training epoch: 291, train loss: 0.50895, val loss: 0.50228\n",
      "Main effects training epoch: 292, train loss: 0.50877, val loss: 0.50104\n",
      "Main effects training epoch: 293, train loss: 0.50877, val loss: 0.50215\n",
      "Main effects training epoch: 294, train loss: 0.50910, val loss: 0.50220\n",
      "Main effects training epoch: 295, train loss: 0.50884, val loss: 0.50377\n",
      "Main effects training epoch: 296, train loss: 0.50871, val loss: 0.50134\n",
      "Main effects training epoch: 297, train loss: 0.50848, val loss: 0.50254\n",
      "Main effects training epoch: 298, train loss: 0.50837, val loss: 0.50219\n",
      "Main effects training epoch: 299, train loss: 0.50832, val loss: 0.50251\n",
      "Main effects training epoch: 300, train loss: 0.50859, val loss: 0.50200\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51389, val loss: 0.50211\n",
      "Main effects tuning epoch: 2, train loss: 0.51351, val loss: 0.50270\n",
      "Main effects tuning epoch: 3, train loss: 0.51330, val loss: 0.50139\n",
      "Main effects tuning epoch: 4, train loss: 0.51337, val loss: 0.50223\n",
      "Main effects tuning epoch: 5, train loss: 0.51305, val loss: 0.50237\n",
      "Main effects tuning epoch: 6, train loss: 0.51308, val loss: 0.50074\n",
      "Main effects tuning epoch: 7, train loss: 0.51315, val loss: 0.50381\n",
      "Main effects tuning epoch: 8, train loss: 0.51312, val loss: 0.50050\n",
      "Main effects tuning epoch: 9, train loss: 0.51316, val loss: 0.50344\n",
      "Main effects tuning epoch: 10, train loss: 0.51313, val loss: 0.50182\n",
      "Main effects tuning epoch: 11, train loss: 0.51316, val loss: 0.50200\n",
      "Main effects tuning epoch: 12, train loss: 0.51316, val loss: 0.50325\n",
      "Main effects tuning epoch: 13, train loss: 0.51283, val loss: 0.50251\n",
      "Main effects tuning epoch: 14, train loss: 0.51276, val loss: 0.50174\n",
      "Main effects tuning epoch: 15, train loss: 0.51256, val loss: 0.50180\n",
      "Main effects tuning epoch: 16, train loss: 0.51275, val loss: 0.50268\n",
      "Main effects tuning epoch: 17, train loss: 0.51254, val loss: 0.50211\n",
      "Main effects tuning epoch: 18, train loss: 0.51271, val loss: 0.50225\n",
      "Main effects tuning epoch: 19, train loss: 0.51341, val loss: 0.50308\n",
      "Main effects tuning epoch: 20, train loss: 0.51291, val loss: 0.50288\n",
      "Main effects tuning epoch: 21, train loss: 0.51256, val loss: 0.50175\n",
      "Main effects tuning epoch: 22, train loss: 0.51278, val loss: 0.50372\n",
      "Main effects tuning epoch: 23, train loss: 0.51245, val loss: 0.50125\n",
      "Main effects tuning epoch: 24, train loss: 0.51247, val loss: 0.50340\n",
      "Main effects tuning epoch: 25, train loss: 0.51236, val loss: 0.50115\n",
      "Main effects tuning epoch: 26, train loss: 0.51224, val loss: 0.50280\n",
      "Main effects tuning epoch: 27, train loss: 0.51216, val loss: 0.50112\n",
      "Main effects tuning epoch: 28, train loss: 0.51231, val loss: 0.50273\n",
      "Main effects tuning epoch: 29, train loss: 0.51221, val loss: 0.50304\n",
      "Main effects tuning epoch: 30, train loss: 0.51228, val loss: 0.50221\n",
      "Main effects tuning epoch: 31, train loss: 0.51222, val loss: 0.50066\n",
      "Main effects tuning epoch: 32, train loss: 0.51257, val loss: 0.50305\n",
      "Main effects tuning epoch: 33, train loss: 0.51225, val loss: 0.50210\n",
      "Main effects tuning epoch: 34, train loss: 0.51213, val loss: 0.50280\n",
      "Main effects tuning epoch: 35, train loss: 0.51217, val loss: 0.50151\n",
      "Main effects tuning epoch: 36, train loss: 0.51185, val loss: 0.50219\n",
      "Main effects tuning epoch: 37, train loss: 0.51197, val loss: 0.50208\n",
      "Main effects tuning epoch: 38, train loss: 0.51202, val loss: 0.50165\n",
      "Main effects tuning epoch: 39, train loss: 0.51181, val loss: 0.50093\n",
      "Main effects tuning epoch: 40, train loss: 0.51169, val loss: 0.50194\n",
      "Main effects tuning epoch: 41, train loss: 0.51180, val loss: 0.50104\n",
      "Main effects tuning epoch: 42, train loss: 0.51168, val loss: 0.50210\n",
      "Main effects tuning epoch: 43, train loss: 0.51197, val loss: 0.50068\n",
      "Main effects tuning epoch: 44, train loss: 0.51181, val loss: 0.50308\n",
      "Main effects tuning epoch: 45, train loss: 0.51200, val loss: 0.49980\n",
      "Main effects tuning epoch: 46, train loss: 0.51201, val loss: 0.50318\n",
      "Main effects tuning epoch: 47, train loss: 0.51197, val loss: 0.49986\n",
      "Main effects tuning epoch: 48, train loss: 0.51213, val loss: 0.50343\n",
      "Main effects tuning epoch: 49, train loss: 0.51167, val loss: 0.50039\n",
      "Main effects tuning epoch: 50, train loss: 0.51155, val loss: 0.50206\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.49963, val loss: 0.49256\n",
      "Interaction training epoch: 2, train loss: 0.33109, val loss: 0.31683\n",
      "Interaction training epoch: 3, train loss: 0.33204, val loss: 0.32190\n",
      "Interaction training epoch: 4, train loss: 0.31507, val loss: 0.29808\n",
      "Interaction training epoch: 5, train loss: 0.30615, val loss: 0.29734\n",
      "Interaction training epoch: 6, train loss: 0.28727, val loss: 0.27792\n",
      "Interaction training epoch: 7, train loss: 0.29299, val loss: 0.28927\n",
      "Interaction training epoch: 8, train loss: 0.28635, val loss: 0.28300\n",
      "Interaction training epoch: 9, train loss: 0.27907, val loss: 0.27369\n",
      "Interaction training epoch: 10, train loss: 0.27603, val loss: 0.26895\n",
      "Interaction training epoch: 11, train loss: 0.27120, val loss: 0.26949\n",
      "Interaction training epoch: 12, train loss: 0.26616, val loss: 0.25899\n",
      "Interaction training epoch: 13, train loss: 0.26350, val loss: 0.26516\n",
      "Interaction training epoch: 14, train loss: 0.26147, val loss: 0.25669\n",
      "Interaction training epoch: 15, train loss: 0.26207, val loss: 0.26006\n",
      "Interaction training epoch: 16, train loss: 0.26007, val loss: 0.25734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 17, train loss: 0.26325, val loss: 0.25960\n",
      "Interaction training epoch: 18, train loss: 0.26371, val loss: 0.26036\n",
      "Interaction training epoch: 19, train loss: 0.26038, val loss: 0.25448\n",
      "Interaction training epoch: 20, train loss: 0.26066, val loss: 0.26124\n",
      "Interaction training epoch: 21, train loss: 0.25998, val loss: 0.25560\n",
      "Interaction training epoch: 22, train loss: 0.25758, val loss: 0.25298\n",
      "Interaction training epoch: 23, train loss: 0.25844, val loss: 0.25655\n",
      "Interaction training epoch: 24, train loss: 0.25629, val loss: 0.25457\n",
      "Interaction training epoch: 25, train loss: 0.25486, val loss: 0.25267\n",
      "Interaction training epoch: 26, train loss: 0.25549, val loss: 0.25249\n",
      "Interaction training epoch: 27, train loss: 0.25719, val loss: 0.25209\n",
      "Interaction training epoch: 28, train loss: 0.26044, val loss: 0.25871\n",
      "Interaction training epoch: 29, train loss: 0.25623, val loss: 0.25217\n",
      "Interaction training epoch: 30, train loss: 0.25604, val loss: 0.25488\n",
      "Interaction training epoch: 31, train loss: 0.25823, val loss: 0.25750\n",
      "Interaction training epoch: 32, train loss: 0.25851, val loss: 0.25669\n",
      "Interaction training epoch: 33, train loss: 0.25009, val loss: 0.24891\n",
      "Interaction training epoch: 34, train loss: 0.25298, val loss: 0.25083\n",
      "Interaction training epoch: 35, train loss: 0.25611, val loss: 0.25120\n",
      "Interaction training epoch: 36, train loss: 0.25425, val loss: 0.25140\n",
      "Interaction training epoch: 37, train loss: 0.25199, val loss: 0.24996\n",
      "Interaction training epoch: 38, train loss: 0.25127, val loss: 0.24899\n",
      "Interaction training epoch: 39, train loss: 0.25441, val loss: 0.24902\n",
      "Interaction training epoch: 40, train loss: 0.25073, val loss: 0.24854\n",
      "Interaction training epoch: 41, train loss: 0.25237, val loss: 0.24886\n",
      "Interaction training epoch: 42, train loss: 0.25070, val loss: 0.25016\n",
      "Interaction training epoch: 43, train loss: 0.25086, val loss: 0.24955\n",
      "Interaction training epoch: 44, train loss: 0.25195, val loss: 0.25067\n",
      "Interaction training epoch: 45, train loss: 0.25202, val loss: 0.24692\n",
      "Interaction training epoch: 46, train loss: 0.25357, val loss: 0.25540\n",
      "Interaction training epoch: 47, train loss: 0.24915, val loss: 0.24644\n",
      "Interaction training epoch: 48, train loss: 0.24784, val loss: 0.24582\n",
      "Interaction training epoch: 49, train loss: 0.24853, val loss: 0.24611\n",
      "Interaction training epoch: 50, train loss: 0.24970, val loss: 0.24858\n",
      "Interaction training epoch: 51, train loss: 0.24924, val loss: 0.24538\n",
      "Interaction training epoch: 52, train loss: 0.24744, val loss: 0.24768\n",
      "Interaction training epoch: 53, train loss: 0.24968, val loss: 0.24663\n",
      "Interaction training epoch: 54, train loss: 0.24788, val loss: 0.24666\n",
      "Interaction training epoch: 55, train loss: 0.25039, val loss: 0.25173\n",
      "Interaction training epoch: 56, train loss: 0.24848, val loss: 0.24521\n",
      "Interaction training epoch: 57, train loss: 0.25138, val loss: 0.25038\n",
      "Interaction training epoch: 58, train loss: 0.25048, val loss: 0.25156\n",
      "Interaction training epoch: 59, train loss: 0.24557, val loss: 0.24772\n",
      "Interaction training epoch: 60, train loss: 0.24780, val loss: 0.24719\n",
      "Interaction training epoch: 61, train loss: 0.24699, val loss: 0.24580\n",
      "Interaction training epoch: 62, train loss: 0.24739, val loss: 0.24607\n",
      "Interaction training epoch: 63, train loss: 0.24578, val loss: 0.24729\n",
      "Interaction training epoch: 64, train loss: 0.25109, val loss: 0.24625\n",
      "Interaction training epoch: 65, train loss: 0.24572, val loss: 0.24436\n",
      "Interaction training epoch: 66, train loss: 0.24516, val loss: 0.24382\n",
      "Interaction training epoch: 67, train loss: 0.24672, val loss: 0.24475\n",
      "Interaction training epoch: 68, train loss: 0.24584, val loss: 0.24761\n",
      "Interaction training epoch: 69, train loss: 0.24488, val loss: 0.24575\n",
      "Interaction training epoch: 70, train loss: 0.24484, val loss: 0.24696\n",
      "Interaction training epoch: 71, train loss: 0.24403, val loss: 0.24235\n",
      "Interaction training epoch: 72, train loss: 0.24438, val loss: 0.24581\n",
      "Interaction training epoch: 73, train loss: 0.24433, val loss: 0.24593\n",
      "Interaction training epoch: 74, train loss: 0.24525, val loss: 0.24468\n",
      "Interaction training epoch: 75, train loss: 0.24394, val loss: 0.24374\n",
      "Interaction training epoch: 76, train loss: 0.24476, val loss: 0.24298\n",
      "Interaction training epoch: 77, train loss: 0.24571, val loss: 0.24356\n",
      "Interaction training epoch: 78, train loss: 0.24351, val loss: 0.24547\n",
      "Interaction training epoch: 79, train loss: 0.24386, val loss: 0.24362\n",
      "Interaction training epoch: 80, train loss: 0.24235, val loss: 0.24386\n",
      "Interaction training epoch: 81, train loss: 0.24656, val loss: 0.24644\n",
      "Interaction training epoch: 82, train loss: 0.24280, val loss: 0.24190\n",
      "Interaction training epoch: 83, train loss: 0.24308, val loss: 0.24630\n",
      "Interaction training epoch: 84, train loss: 0.24299, val loss: 0.24173\n",
      "Interaction training epoch: 85, train loss: 0.24152, val loss: 0.24453\n",
      "Interaction training epoch: 86, train loss: 0.24060, val loss: 0.24289\n",
      "Interaction training epoch: 87, train loss: 0.24175, val loss: 0.24074\n",
      "Interaction training epoch: 88, train loss: 0.24195, val loss: 0.24367\n",
      "Interaction training epoch: 89, train loss: 0.24121, val loss: 0.24367\n",
      "Interaction training epoch: 90, train loss: 0.24025, val loss: 0.24199\n",
      "Interaction training epoch: 91, train loss: 0.24173, val loss: 0.24581\n",
      "Interaction training epoch: 92, train loss: 0.24142, val loss: 0.24263\n",
      "Interaction training epoch: 93, train loss: 0.23894, val loss: 0.24403\n",
      "Interaction training epoch: 94, train loss: 0.24109, val loss: 0.24087\n",
      "Interaction training epoch: 95, train loss: 0.24071, val loss: 0.24190\n",
      "Interaction training epoch: 96, train loss: 0.24147, val loss: 0.24511\n",
      "Interaction training epoch: 97, train loss: 0.24062, val loss: 0.24456\n",
      "Interaction training epoch: 98, train loss: 0.24063, val loss: 0.24740\n",
      "Interaction training epoch: 99, train loss: 0.23903, val loss: 0.24121\n",
      "Interaction training epoch: 100, train loss: 0.24043, val loss: 0.24585\n",
      "Interaction training epoch: 101, train loss: 0.23894, val loss: 0.24335\n",
      "Interaction training epoch: 102, train loss: 0.23998, val loss: 0.24320\n",
      "Interaction training epoch: 103, train loss: 0.23937, val loss: 0.24698\n",
      "Interaction training epoch: 104, train loss: 0.23941, val loss: 0.23988\n",
      "Interaction training epoch: 105, train loss: 0.23746, val loss: 0.24098\n",
      "Interaction training epoch: 106, train loss: 0.23966, val loss: 0.24561\n",
      "Interaction training epoch: 107, train loss: 0.23897, val loss: 0.24114\n",
      "Interaction training epoch: 108, train loss: 0.23967, val loss: 0.24589\n",
      "Interaction training epoch: 109, train loss: 0.24026, val loss: 0.24303\n",
      "Interaction training epoch: 110, train loss: 0.23919, val loss: 0.24565\n",
      "Interaction training epoch: 111, train loss: 0.23935, val loss: 0.24568\n",
      "Interaction training epoch: 112, train loss: 0.23510, val loss: 0.24313\n",
      "Interaction training epoch: 113, train loss: 0.23706, val loss: 0.24327\n",
      "Interaction training epoch: 114, train loss: 0.23695, val loss: 0.24212\n",
      "Interaction training epoch: 115, train loss: 0.23583, val loss: 0.24067\n",
      "Interaction training epoch: 116, train loss: 0.23680, val loss: 0.24462\n",
      "Interaction training epoch: 117, train loss: 0.23659, val loss: 0.24219\n",
      "Interaction training epoch: 118, train loss: 0.23666, val loss: 0.24311\n",
      "Interaction training epoch: 119, train loss: 0.23566, val loss: 0.24072\n",
      "Interaction training epoch: 120, train loss: 0.23788, val loss: 0.24737\n",
      "Interaction training epoch: 121, train loss: 0.23428, val loss: 0.24078\n",
      "Interaction training epoch: 122, train loss: 0.23842, val loss: 0.24443\n",
      "Interaction training epoch: 123, train loss: 0.23845, val loss: 0.24468\n",
      "Interaction training epoch: 124, train loss: 0.23572, val loss: 0.24593\n",
      "Interaction training epoch: 125, train loss: 0.23548, val loss: 0.24321\n",
      "Interaction training epoch: 126, train loss: 0.23423, val loss: 0.23941\n",
      "Interaction training epoch: 127, train loss: 0.23801, val loss: 0.24262\n",
      "Interaction training epoch: 128, train loss: 0.23696, val loss: 0.24433\n",
      "Interaction training epoch: 129, train loss: 0.23657, val loss: 0.24195\n",
      "Interaction training epoch: 130, train loss: 0.23512, val loss: 0.24270\n",
      "Interaction training epoch: 131, train loss: 0.23472, val loss: 0.24261\n",
      "Interaction training epoch: 132, train loss: 0.23460, val loss: 0.23962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 133, train loss: 0.23412, val loss: 0.24060\n",
      "Interaction training epoch: 134, train loss: 0.23348, val loss: 0.23968\n",
      "Interaction training epoch: 135, train loss: 0.23561, val loss: 0.24302\n",
      "Interaction training epoch: 136, train loss: 0.23507, val loss: 0.24282\n",
      "Interaction training epoch: 137, train loss: 0.23336, val loss: 0.24215\n",
      "Interaction training epoch: 138, train loss: 0.23490, val loss: 0.24335\n",
      "Interaction training epoch: 139, train loss: 0.23433, val loss: 0.24089\n",
      "Interaction training epoch: 140, train loss: 0.23369, val loss: 0.24246\n",
      "Interaction training epoch: 141, train loss: 0.23410, val loss: 0.24115\n",
      "Interaction training epoch: 142, train loss: 0.23323, val loss: 0.24246\n",
      "Interaction training epoch: 143, train loss: 0.23207, val loss: 0.23777\n",
      "Interaction training epoch: 144, train loss: 0.23586, val loss: 0.24257\n",
      "Interaction training epoch: 145, train loss: 0.23242, val loss: 0.23983\n",
      "Interaction training epoch: 146, train loss: 0.23366, val loss: 0.24437\n",
      "Interaction training epoch: 147, train loss: 0.23182, val loss: 0.24178\n",
      "Interaction training epoch: 148, train loss: 0.23236, val loss: 0.23739\n",
      "Interaction training epoch: 149, train loss: 0.23397, val loss: 0.24726\n",
      "Interaction training epoch: 150, train loss: 0.23330, val loss: 0.23574\n",
      "Interaction training epoch: 151, train loss: 0.23225, val loss: 0.24513\n",
      "Interaction training epoch: 152, train loss: 0.23082, val loss: 0.23966\n",
      "Interaction training epoch: 153, train loss: 0.23229, val loss: 0.24060\n",
      "Interaction training epoch: 154, train loss: 0.23294, val loss: 0.24680\n",
      "Interaction training epoch: 155, train loss: 0.23088, val loss: 0.23790\n",
      "Interaction training epoch: 156, train loss: 0.23106, val loss: 0.24559\n",
      "Interaction training epoch: 157, train loss: 0.23165, val loss: 0.23984\n",
      "Interaction training epoch: 158, train loss: 0.23275, val loss: 0.24351\n",
      "Interaction training epoch: 159, train loss: 0.22992, val loss: 0.23990\n",
      "Interaction training epoch: 160, train loss: 0.23107, val loss: 0.24335\n",
      "Interaction training epoch: 161, train loss: 0.23199, val loss: 0.24232\n",
      "Interaction training epoch: 162, train loss: 0.22937, val loss: 0.24343\n",
      "Interaction training epoch: 163, train loss: 0.23051, val loss: 0.23902\n",
      "Interaction training epoch: 164, train loss: 0.23101, val loss: 0.24656\n",
      "Interaction training epoch: 165, train loss: 0.22952, val loss: 0.24065\n",
      "Interaction training epoch: 166, train loss: 0.23069, val loss: 0.24404\n",
      "Interaction training epoch: 167, train loss: 0.22885, val loss: 0.23677\n",
      "Interaction training epoch: 168, train loss: 0.23006, val loss: 0.24721\n",
      "Interaction training epoch: 169, train loss: 0.22866, val loss: 0.24152\n",
      "Interaction training epoch: 170, train loss: 0.22703, val loss: 0.24041\n",
      "Interaction training epoch: 171, train loss: 0.22977, val loss: 0.24358\n",
      "Interaction training epoch: 172, train loss: 0.22965, val loss: 0.23985\n",
      "Interaction training epoch: 173, train loss: 0.22815, val loss: 0.24288\n",
      "Interaction training epoch: 174, train loss: 0.22783, val loss: 0.23943\n",
      "Interaction training epoch: 175, train loss: 0.22813, val loss: 0.23916\n",
      "Interaction training epoch: 176, train loss: 0.22829, val loss: 0.24228\n",
      "Interaction training epoch: 177, train loss: 0.22855, val loss: 0.24316\n",
      "Interaction training epoch: 178, train loss: 0.22713, val loss: 0.24211\n",
      "Interaction training epoch: 179, train loss: 0.22952, val loss: 0.24167\n",
      "Interaction training epoch: 180, train loss: 0.22881, val loss: 0.24454\n",
      "Interaction training epoch: 181, train loss: 0.22656, val loss: 0.24283\n",
      "Interaction training epoch: 182, train loss: 0.22786, val loss: 0.24087\n",
      "Interaction training epoch: 183, train loss: 0.22632, val loss: 0.23899\n",
      "Interaction training epoch: 184, train loss: 0.22614, val loss: 0.24360\n",
      "Interaction training epoch: 185, train loss: 0.22567, val loss: 0.24225\n",
      "Interaction training epoch: 186, train loss: 0.22652, val loss: 0.24173\n",
      "Interaction training epoch: 187, train loss: 0.22697, val loss: 0.24372\n",
      "Interaction training epoch: 188, train loss: 0.22751, val loss: 0.24272\n",
      "Interaction training epoch: 189, train loss: 0.22634, val loss: 0.23982\n",
      "Interaction training epoch: 190, train loss: 0.22618, val loss: 0.24448\n",
      "Interaction training epoch: 191, train loss: 0.22481, val loss: 0.24014\n",
      "Interaction training epoch: 192, train loss: 0.22541, val loss: 0.24134\n",
      "Interaction training epoch: 193, train loss: 0.22644, val loss: 0.24243\n",
      "Interaction training epoch: 194, train loss: 0.22481, val loss: 0.24216\n",
      "Interaction training epoch: 195, train loss: 0.22480, val loss: 0.24522\n",
      "Interaction training epoch: 196, train loss: 0.22518, val loss: 0.24324\n",
      "Interaction training epoch: 197, train loss: 0.22511, val loss: 0.24287\n",
      "Interaction training epoch: 198, train loss: 0.22556, val loss: 0.24718\n",
      "Interaction training epoch: 199, train loss: 0.22274, val loss: 0.23850\n",
      "Interaction training epoch: 200, train loss: 0.22627, val loss: 0.24738\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########6 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.25265, val loss: 0.23765\n",
      "Interaction tuning epoch: 2, train loss: 0.25177, val loss: 0.24207\n",
      "Interaction tuning epoch: 3, train loss: 0.25144, val loss: 0.24295\n",
      "Interaction tuning epoch: 4, train loss: 0.25252, val loss: 0.24046\n",
      "Interaction tuning epoch: 5, train loss: 0.25086, val loss: 0.24137\n",
      "Interaction tuning epoch: 6, train loss: 0.25037, val loss: 0.23991\n",
      "Interaction tuning epoch: 7, train loss: 0.25191, val loss: 0.24014\n",
      "Interaction tuning epoch: 8, train loss: 0.25109, val loss: 0.24066\n",
      "Interaction tuning epoch: 9, train loss: 0.25032, val loss: 0.23880\n",
      "Interaction tuning epoch: 10, train loss: 0.24980, val loss: 0.23998\n",
      "Interaction tuning epoch: 11, train loss: 0.25033, val loss: 0.23996\n",
      "Interaction tuning epoch: 12, train loss: 0.24876, val loss: 0.23864\n",
      "Interaction tuning epoch: 13, train loss: 0.25038, val loss: 0.24152\n",
      "Interaction tuning epoch: 14, train loss: 0.24972, val loss: 0.23900\n",
      "Interaction tuning epoch: 15, train loss: 0.24809, val loss: 0.24072\n",
      "Interaction tuning epoch: 16, train loss: 0.24945, val loss: 0.24071\n",
      "Interaction tuning epoch: 17, train loss: 0.24800, val loss: 0.23647\n",
      "Interaction tuning epoch: 18, train loss: 0.25083, val loss: 0.24184\n",
      "Interaction tuning epoch: 19, train loss: 0.25097, val loss: 0.24155\n",
      "Interaction tuning epoch: 20, train loss: 0.24803, val loss: 0.24128\n",
      "Interaction tuning epoch: 21, train loss: 0.24967, val loss: 0.24146\n",
      "Interaction tuning epoch: 22, train loss: 0.24785, val loss: 0.23859\n",
      "Interaction tuning epoch: 23, train loss: 0.24780, val loss: 0.23934\n",
      "Interaction tuning epoch: 24, train loss: 0.24973, val loss: 0.24086\n",
      "Interaction tuning epoch: 25, train loss: 0.24861, val loss: 0.24113\n",
      "Interaction tuning epoch: 26, train loss: 0.24820, val loss: 0.23967\n",
      "Interaction tuning epoch: 27, train loss: 0.24897, val loss: 0.23942\n",
      "Interaction tuning epoch: 28, train loss: 0.24805, val loss: 0.23990\n",
      "Interaction tuning epoch: 29, train loss: 0.24818, val loss: 0.24132\n",
      "Interaction tuning epoch: 30, train loss: 0.25339, val loss: 0.24419\n",
      "Interaction tuning epoch: 31, train loss: 0.25028, val loss: 0.23937\n",
      "Interaction tuning epoch: 32, train loss: 0.24921, val loss: 0.24162\n",
      "Interaction tuning epoch: 33, train loss: 0.24665, val loss: 0.23897\n",
      "Interaction tuning epoch: 34, train loss: 0.24722, val loss: 0.24271\n",
      "Interaction tuning epoch: 35, train loss: 0.24608, val loss: 0.23682\n",
      "Interaction tuning epoch: 36, train loss: 0.24703, val loss: 0.23931\n",
      "Interaction tuning epoch: 37, train loss: 0.24599, val loss: 0.23895\n",
      "Interaction tuning epoch: 38, train loss: 0.24734, val loss: 0.24159\n",
      "Interaction tuning epoch: 39, train loss: 0.24810, val loss: 0.24174\n",
      "Interaction tuning epoch: 40, train loss: 0.25007, val loss: 0.24010\n",
      "Interaction tuning epoch: 41, train loss: 0.24805, val loss: 0.23876\n",
      "Interaction tuning epoch: 42, train loss: 0.24916, val loss: 0.24012\n",
      "Interaction tuning epoch: 43, train loss: 0.24858, val loss: 0.24021\n",
      "Interaction tuning epoch: 44, train loss: 0.24775, val loss: 0.23786\n",
      "Interaction tuning epoch: 45, train loss: 0.24829, val loss: 0.24150\n",
      "Interaction tuning epoch: 46, train loss: 0.25051, val loss: 0.24100\n",
      "Interaction tuning epoch: 47, train loss: 0.24806, val loss: 0.24055\n",
      "Interaction tuning epoch: 48, train loss: 0.24834, val loss: 0.23997\n",
      "Interaction tuning epoch: 49, train loss: 0.24866, val loss: 0.24025\n",
      "Interaction tuning epoch: 50, train loss: 0.24731, val loss: 0.23841\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 38.397878885269165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After the gam stage, training error is 0.24731 , validation error is 0.23841\n",
      "missing value counts: 99112\n",
      "[SoftImpute] Max Singular Value of X_init = 4.140870\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed BCE=0.206352 validation BCE=0.249860,rank=5\n",
      "[SoftImpute] Iter 2: observed BCE=0.201972 validation BCE=0.248892,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.199368 validation BCE=0.247759,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.197821 validation BCE=0.247037,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.196663 validation BCE=0.246599,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.195873 validation BCE=0.235750,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.195491 validation BCE=0.245547,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.195220 validation BCE=0.233229,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.195267 validation BCE=0.233650,rank=5\n",
      "[SoftImpute] Iter 10: observed BCE=0.195367 validation BCE=0.232748,rank=5\n",
      "[SoftImpute] Iter 11: observed BCE=0.195401 validation BCE=0.233191,rank=5\n",
      "[SoftImpute] Iter 12: observed BCE=0.195448 validation BCE=0.232403,rank=5\n",
      "[SoftImpute] Iter 13: observed BCE=0.195392 validation BCE=0.232510,rank=5\n",
      "[SoftImpute] Iter 14: observed BCE=0.195446 validation BCE=0.233052,rank=5\n",
      "[SoftImpute] Iter 15: observed BCE=0.195553 validation BCE=0.232845,rank=5\n",
      "[SoftImpute] Iter 16: observed BCE=0.195562 validation BCE=0.232462,rank=5\n",
      "[SoftImpute] Iter 17: observed BCE=0.195945 validation BCE=0.232938,rank=5\n",
      "[SoftImpute] Iter 18: observed BCE=0.195820 validation BCE=0.232791,rank=5\n",
      "[SoftImpute] Iter 19: observed BCE=0.197190 validation BCE=0.233049,rank=5\n",
      "[SoftImpute] Iter 20: observed BCE=0.197387 validation BCE=0.232957,rank=5\n",
      "[SoftImpute] Iter 21: observed BCE=0.197720 validation BCE=0.233054,rank=5\n",
      "[SoftImpute] Iter 22: observed BCE=0.198151 validation BCE=0.233599,rank=5\n",
      "[SoftImpute] Iter 23: observed BCE=0.198285 validation BCE=0.233340,rank=5\n",
      "[SoftImpute] Iter 24: observed BCE=0.198186 validation BCE=0.233835,rank=5\n",
      "[SoftImpute] Iter 25: observed BCE=0.196375 validation BCE=0.233519,rank=5\n",
      "[SoftImpute] Iter 26: observed BCE=0.195810 validation BCE=0.233883,rank=5\n",
      "[SoftImpute] Iter 27: observed BCE=0.195872 validation BCE=0.233941,rank=5\n",
      "[SoftImpute] Iter 28: observed BCE=0.195876 validation BCE=0.234140,rank=5\n",
      "[SoftImpute] Stopped after iteration 28 for lambda=0.082817\n",
      "final num of user group: 18\n",
      "final num of item group: 35\n",
      "change mode state : True\n",
      "time cost: 10.485646486282349\n",
      "After the matrix factor stage, training error is 0.19588, validation error is 0.23414\n",
      "6\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68066, val loss: 0.68541\n",
      "Main effects training epoch: 2, train loss: 0.67685, val loss: 0.68407\n",
      "Main effects training epoch: 3, train loss: 0.67060, val loss: 0.67758\n",
      "Main effects training epoch: 4, train loss: 0.66402, val loss: 0.67124\n",
      "Main effects training epoch: 5, train loss: 0.65273, val loss: 0.65923\n",
      "Main effects training epoch: 6, train loss: 0.63250, val loss: 0.63794\n",
      "Main effects training epoch: 7, train loss: 0.59923, val loss: 0.60078\n",
      "Main effects training epoch: 8, train loss: 0.56077, val loss: 0.55903\n",
      "Main effects training epoch: 9, train loss: 0.53198, val loss: 0.52265\n",
      "Main effects training epoch: 10, train loss: 0.53149, val loss: 0.51786\n",
      "Main effects training epoch: 11, train loss: 0.52514, val loss: 0.51703\n",
      "Main effects training epoch: 12, train loss: 0.52217, val loss: 0.51051\n",
      "Main effects training epoch: 13, train loss: 0.52116, val loss: 0.51168\n",
      "Main effects training epoch: 14, train loss: 0.52146, val loss: 0.51272\n",
      "Main effects training epoch: 15, train loss: 0.52319, val loss: 0.51309\n",
      "Main effects training epoch: 16, train loss: 0.52244, val loss: 0.51626\n",
      "Main effects training epoch: 17, train loss: 0.52277, val loss: 0.50941\n",
      "Main effects training epoch: 18, train loss: 0.52144, val loss: 0.51565\n",
      "Main effects training epoch: 19, train loss: 0.52045, val loss: 0.51207\n",
      "Main effects training epoch: 20, train loss: 0.52127, val loss: 0.51125\n",
      "Main effects training epoch: 21, train loss: 0.52060, val loss: 0.51352\n",
      "Main effects training epoch: 22, train loss: 0.52041, val loss: 0.51016\n",
      "Main effects training epoch: 23, train loss: 0.52024, val loss: 0.51152\n",
      "Main effects training epoch: 24, train loss: 0.52031, val loss: 0.51221\n",
      "Main effects training epoch: 25, train loss: 0.52074, val loss: 0.50987\n",
      "Main effects training epoch: 26, train loss: 0.52031, val loss: 0.51284\n",
      "Main effects training epoch: 27, train loss: 0.51984, val loss: 0.51163\n",
      "Main effects training epoch: 28, train loss: 0.52004, val loss: 0.51171\n",
      "Main effects training epoch: 29, train loss: 0.52030, val loss: 0.51289\n",
      "Main effects training epoch: 30, train loss: 0.52111, val loss: 0.51035\n",
      "Main effects training epoch: 31, train loss: 0.52078, val loss: 0.51311\n",
      "Main effects training epoch: 32, train loss: 0.51985, val loss: 0.51236\n",
      "Main effects training epoch: 33, train loss: 0.52016, val loss: 0.50964\n",
      "Main effects training epoch: 34, train loss: 0.52122, val loss: 0.51702\n",
      "Main effects training epoch: 35, train loss: 0.52006, val loss: 0.50915\n",
      "Main effects training epoch: 36, train loss: 0.51992, val loss: 0.51428\n",
      "Main effects training epoch: 37, train loss: 0.51947, val loss: 0.51061\n",
      "Main effects training epoch: 38, train loss: 0.51962, val loss: 0.51254\n",
      "Main effects training epoch: 39, train loss: 0.51955, val loss: 0.50942\n",
      "Main effects training epoch: 40, train loss: 0.51921, val loss: 0.51086\n",
      "Main effects training epoch: 41, train loss: 0.52028, val loss: 0.50921\n",
      "Main effects training epoch: 42, train loss: 0.52108, val loss: 0.51780\n",
      "Main effects training epoch: 43, train loss: 0.52036, val loss: 0.50911\n",
      "Main effects training epoch: 44, train loss: 0.51969, val loss: 0.51427\n",
      "Main effects training epoch: 45, train loss: 0.51933, val loss: 0.50895\n",
      "Main effects training epoch: 46, train loss: 0.51938, val loss: 0.51231\n",
      "Main effects training epoch: 47, train loss: 0.51936, val loss: 0.51082\n",
      "Main effects training epoch: 48, train loss: 0.51954, val loss: 0.51211\n",
      "Main effects training epoch: 49, train loss: 0.52045, val loss: 0.51182\n",
      "Main effects training epoch: 50, train loss: 0.52051, val loss: 0.51389\n",
      "Main effects training epoch: 51, train loss: 0.52055, val loss: 0.51081\n",
      "Main effects training epoch: 52, train loss: 0.52018, val loss: 0.51570\n",
      "Main effects training epoch: 53, train loss: 0.52005, val loss: 0.50956\n",
      "Main effects training epoch: 54, train loss: 0.52179, val loss: 0.51822\n",
      "Main effects training epoch: 55, train loss: 0.52004, val loss: 0.51027\n",
      "Main effects training epoch: 56, train loss: 0.51932, val loss: 0.51344\n",
      "Main effects training epoch: 57, train loss: 0.51881, val loss: 0.51190\n",
      "Main effects training epoch: 58, train loss: 0.51838, val loss: 0.51098\n",
      "Main effects training epoch: 59, train loss: 0.51812, val loss: 0.51215\n",
      "Main effects training epoch: 60, train loss: 0.51829, val loss: 0.51011\n",
      "Main effects training epoch: 61, train loss: 0.51802, val loss: 0.51189\n",
      "Main effects training epoch: 62, train loss: 0.51844, val loss: 0.51108\n",
      "Main effects training epoch: 63, train loss: 0.51835, val loss: 0.51136\n",
      "Main effects training epoch: 64, train loss: 0.51942, val loss: 0.51198\n",
      "Main effects training epoch: 65, train loss: 0.51829, val loss: 0.51186\n",
      "Main effects training epoch: 66, train loss: 0.51725, val loss: 0.51201\n",
      "Main effects training epoch: 67, train loss: 0.51786, val loss: 0.50906\n",
      "Main effects training epoch: 68, train loss: 0.51743, val loss: 0.51104\n",
      "Main effects training epoch: 69, train loss: 0.51679, val loss: 0.51116\n",
      "Main effects training epoch: 70, train loss: 0.51682, val loss: 0.51082\n",
      "Main effects training epoch: 71, train loss: 0.51670, val loss: 0.51013\n",
      "Main effects training epoch: 72, train loss: 0.51686, val loss: 0.51270\n",
      "Main effects training epoch: 73, train loss: 0.51789, val loss: 0.50962\n",
      "Main effects training epoch: 74, train loss: 0.51683, val loss: 0.51246\n",
      "Main effects training epoch: 75, train loss: 0.51674, val loss: 0.51039\n",
      "Main effects training epoch: 76, train loss: 0.51657, val loss: 0.51086\n",
      "Main effects training epoch: 77, train loss: 0.51673, val loss: 0.50829\n",
      "Main effects training epoch: 78, train loss: 0.51707, val loss: 0.51119\n",
      "Main effects training epoch: 79, train loss: 0.51637, val loss: 0.51190\n",
      "Main effects training epoch: 80, train loss: 0.51651, val loss: 0.51119\n",
      "Main effects training epoch: 81, train loss: 0.51639, val loss: 0.50797\n",
      "Main effects training epoch: 82, train loss: 0.51720, val loss: 0.51434\n",
      "Main effects training epoch: 83, train loss: 0.51609, val loss: 0.50917\n",
      "Main effects training epoch: 84, train loss: 0.51684, val loss: 0.51255\n",
      "Main effects training epoch: 85, train loss: 0.51617, val loss: 0.50931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 86, train loss: 0.51595, val loss: 0.50857\n",
      "Main effects training epoch: 87, train loss: 0.51591, val loss: 0.50846\n",
      "Main effects training epoch: 88, train loss: 0.51583, val loss: 0.50954\n",
      "Main effects training epoch: 89, train loss: 0.51605, val loss: 0.51085\n",
      "Main effects training epoch: 90, train loss: 0.51601, val loss: 0.50903\n",
      "Main effects training epoch: 91, train loss: 0.51578, val loss: 0.51105\n",
      "Main effects training epoch: 92, train loss: 0.51657, val loss: 0.50650\n",
      "Main effects training epoch: 93, train loss: 0.51616, val loss: 0.51200\n",
      "Main effects training epoch: 94, train loss: 0.51582, val loss: 0.50965\n",
      "Main effects training epoch: 95, train loss: 0.51646, val loss: 0.50975\n",
      "Main effects training epoch: 96, train loss: 0.51565, val loss: 0.50976\n",
      "Main effects training epoch: 97, train loss: 0.51579, val loss: 0.50864\n",
      "Main effects training epoch: 98, train loss: 0.51570, val loss: 0.51030\n",
      "Main effects training epoch: 99, train loss: 0.51564, val loss: 0.50767\n",
      "Main effects training epoch: 100, train loss: 0.51573, val loss: 0.51215\n",
      "Main effects training epoch: 101, train loss: 0.51538, val loss: 0.50739\n",
      "Main effects training epoch: 102, train loss: 0.51537, val loss: 0.50746\n",
      "Main effects training epoch: 103, train loss: 0.51564, val loss: 0.50890\n",
      "Main effects training epoch: 104, train loss: 0.51547, val loss: 0.50963\n",
      "Main effects training epoch: 105, train loss: 0.51547, val loss: 0.50716\n",
      "Main effects training epoch: 106, train loss: 0.51537, val loss: 0.51131\n",
      "Main effects training epoch: 107, train loss: 0.51559, val loss: 0.50644\n",
      "Main effects training epoch: 108, train loss: 0.51568, val loss: 0.51166\n",
      "Main effects training epoch: 109, train loss: 0.51532, val loss: 0.50856\n",
      "Main effects training epoch: 110, train loss: 0.51525, val loss: 0.50939\n",
      "Main effects training epoch: 111, train loss: 0.51514, val loss: 0.50856\n",
      "Main effects training epoch: 112, train loss: 0.51624, val loss: 0.51385\n",
      "Main effects training epoch: 113, train loss: 0.51515, val loss: 0.50689\n",
      "Main effects training epoch: 114, train loss: 0.51507, val loss: 0.51146\n",
      "Main effects training epoch: 115, train loss: 0.51556, val loss: 0.50640\n",
      "Main effects training epoch: 116, train loss: 0.51512, val loss: 0.51039\n",
      "Main effects training epoch: 117, train loss: 0.51472, val loss: 0.50788\n",
      "Main effects training epoch: 118, train loss: 0.51484, val loss: 0.50906\n",
      "Main effects training epoch: 119, train loss: 0.51496, val loss: 0.50603\n",
      "Main effects training epoch: 120, train loss: 0.51493, val loss: 0.51100\n",
      "Main effects training epoch: 121, train loss: 0.51485, val loss: 0.50960\n",
      "Main effects training epoch: 122, train loss: 0.51446, val loss: 0.50788\n",
      "Main effects training epoch: 123, train loss: 0.51512, val loss: 0.50814\n",
      "Main effects training epoch: 124, train loss: 0.51462, val loss: 0.50848\n",
      "Main effects training epoch: 125, train loss: 0.51512, val loss: 0.50792\n",
      "Main effects training epoch: 126, train loss: 0.51482, val loss: 0.51161\n",
      "Main effects training epoch: 127, train loss: 0.51509, val loss: 0.50519\n",
      "Main effects training epoch: 128, train loss: 0.51460, val loss: 0.50945\n",
      "Main effects training epoch: 129, train loss: 0.51481, val loss: 0.50720\n",
      "Main effects training epoch: 130, train loss: 0.51448, val loss: 0.50931\n",
      "Main effects training epoch: 131, train loss: 0.51484, val loss: 0.50672\n",
      "Main effects training epoch: 132, train loss: 0.51471, val loss: 0.51036\n",
      "Main effects training epoch: 133, train loss: 0.51460, val loss: 0.50668\n",
      "Main effects training epoch: 134, train loss: 0.51401, val loss: 0.50899\n",
      "Main effects training epoch: 135, train loss: 0.51386, val loss: 0.50687\n",
      "Main effects training epoch: 136, train loss: 0.51382, val loss: 0.50726\n",
      "Main effects training epoch: 137, train loss: 0.51389, val loss: 0.50770\n",
      "Main effects training epoch: 138, train loss: 0.51382, val loss: 0.50628\n",
      "Main effects training epoch: 139, train loss: 0.51355, val loss: 0.50696\n",
      "Main effects training epoch: 140, train loss: 0.51386, val loss: 0.50808\n",
      "Main effects training epoch: 141, train loss: 0.51348, val loss: 0.50679\n",
      "Main effects training epoch: 142, train loss: 0.51354, val loss: 0.50854\n",
      "Main effects training epoch: 143, train loss: 0.51330, val loss: 0.50601\n",
      "Main effects training epoch: 144, train loss: 0.51373, val loss: 0.50596\n",
      "Main effects training epoch: 145, train loss: 0.51355, val loss: 0.50836\n",
      "Main effects training epoch: 146, train loss: 0.51329, val loss: 0.50620\n",
      "Main effects training epoch: 147, train loss: 0.51324, val loss: 0.50782\n",
      "Main effects training epoch: 148, train loss: 0.51312, val loss: 0.50479\n",
      "Main effects training epoch: 149, train loss: 0.51317, val loss: 0.50669\n",
      "Main effects training epoch: 150, train loss: 0.51308, val loss: 0.50873\n",
      "Main effects training epoch: 151, train loss: 0.51295, val loss: 0.50566\n",
      "Main effects training epoch: 152, train loss: 0.51269, val loss: 0.50741\n",
      "Main effects training epoch: 153, train loss: 0.51272, val loss: 0.50449\n",
      "Main effects training epoch: 154, train loss: 0.51294, val loss: 0.50883\n",
      "Main effects training epoch: 155, train loss: 0.51321, val loss: 0.50589\n",
      "Main effects training epoch: 156, train loss: 0.51300, val loss: 0.50715\n",
      "Main effects training epoch: 157, train loss: 0.51214, val loss: 0.50535\n",
      "Main effects training epoch: 158, train loss: 0.51204, val loss: 0.50482\n",
      "Main effects training epoch: 159, train loss: 0.51238, val loss: 0.50682\n",
      "Main effects training epoch: 160, train loss: 0.51333, val loss: 0.50512\n",
      "Main effects training epoch: 161, train loss: 0.51192, val loss: 0.50596\n",
      "Main effects training epoch: 162, train loss: 0.51217, val loss: 0.50496\n",
      "Main effects training epoch: 163, train loss: 0.51242, val loss: 0.50618\n",
      "Main effects training epoch: 164, train loss: 0.51234, val loss: 0.50521\n",
      "Main effects training epoch: 165, train loss: 0.51221, val loss: 0.50652\n",
      "Main effects training epoch: 166, train loss: 0.51227, val loss: 0.50619\n",
      "Main effects training epoch: 167, train loss: 0.51180, val loss: 0.50576\n",
      "Main effects training epoch: 168, train loss: 0.51191, val loss: 0.50323\n",
      "Main effects training epoch: 169, train loss: 0.51116, val loss: 0.50605\n",
      "Main effects training epoch: 170, train loss: 0.51101, val loss: 0.50400\n",
      "Main effects training epoch: 171, train loss: 0.51106, val loss: 0.50517\n",
      "Main effects training epoch: 172, train loss: 0.51136, val loss: 0.50594\n",
      "Main effects training epoch: 173, train loss: 0.51124, val loss: 0.50377\n",
      "Main effects training epoch: 174, train loss: 0.51141, val loss: 0.50564\n",
      "Main effects training epoch: 175, train loss: 0.51097, val loss: 0.50410\n",
      "Main effects training epoch: 176, train loss: 0.51076, val loss: 0.50524\n",
      "Main effects training epoch: 177, train loss: 0.51057, val loss: 0.50450\n",
      "Main effects training epoch: 178, train loss: 0.51096, val loss: 0.50551\n",
      "Main effects training epoch: 179, train loss: 0.51055, val loss: 0.50366\n",
      "Main effects training epoch: 180, train loss: 0.51076, val loss: 0.50605\n",
      "Main effects training epoch: 181, train loss: 0.51021, val loss: 0.50419\n",
      "Main effects training epoch: 182, train loss: 0.51027, val loss: 0.50394\n",
      "Main effects training epoch: 183, train loss: 0.51024, val loss: 0.50444\n",
      "Main effects training epoch: 184, train loss: 0.50994, val loss: 0.50365\n",
      "Main effects training epoch: 185, train loss: 0.51042, val loss: 0.50335\n",
      "Main effects training epoch: 186, train loss: 0.51103, val loss: 0.50606\n",
      "Main effects training epoch: 187, train loss: 0.51099, val loss: 0.50527\n",
      "Main effects training epoch: 188, train loss: 0.50995, val loss: 0.50561\n",
      "Main effects training epoch: 189, train loss: 0.50992, val loss: 0.50275\n",
      "Main effects training epoch: 190, train loss: 0.51078, val loss: 0.50884\n",
      "Main effects training epoch: 191, train loss: 0.51055, val loss: 0.50148\n",
      "Main effects training epoch: 192, train loss: 0.50974, val loss: 0.50604\n",
      "Main effects training epoch: 193, train loss: 0.50948, val loss: 0.50246\n",
      "Main effects training epoch: 194, train loss: 0.50938, val loss: 0.50540\n",
      "Main effects training epoch: 195, train loss: 0.50947, val loss: 0.50261\n",
      "Main effects training epoch: 196, train loss: 0.50930, val loss: 0.50293\n",
      "Main effects training epoch: 197, train loss: 0.50933, val loss: 0.50406\n",
      "Main effects training epoch: 198, train loss: 0.50919, val loss: 0.50371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 199, train loss: 0.50894, val loss: 0.50289\n",
      "Main effects training epoch: 200, train loss: 0.50904, val loss: 0.50319\n",
      "Main effects training epoch: 201, train loss: 0.50953, val loss: 0.50674\n",
      "Main effects training epoch: 202, train loss: 0.50903, val loss: 0.50254\n",
      "Main effects training epoch: 203, train loss: 0.50976, val loss: 0.50630\n",
      "Main effects training epoch: 204, train loss: 0.51172, val loss: 0.50325\n",
      "Main effects training epoch: 205, train loss: 0.51098, val loss: 0.50974\n",
      "Main effects training epoch: 206, train loss: 0.50964, val loss: 0.50176\n",
      "Main effects training epoch: 207, train loss: 0.50943, val loss: 0.50332\n",
      "Main effects training epoch: 208, train loss: 0.50919, val loss: 0.50403\n",
      "Main effects training epoch: 209, train loss: 0.50879, val loss: 0.50322\n",
      "Main effects training epoch: 210, train loss: 0.50856, val loss: 0.50460\n",
      "Main effects training epoch: 211, train loss: 0.50863, val loss: 0.50099\n",
      "Main effects training epoch: 212, train loss: 0.50845, val loss: 0.50455\n",
      "Main effects training epoch: 213, train loss: 0.50826, val loss: 0.50278\n",
      "Main effects training epoch: 214, train loss: 0.50839, val loss: 0.50413\n",
      "Main effects training epoch: 215, train loss: 0.50852, val loss: 0.50318\n",
      "Main effects training epoch: 216, train loss: 0.50868, val loss: 0.50456\n",
      "Main effects training epoch: 217, train loss: 0.50822, val loss: 0.50171\n",
      "Main effects training epoch: 218, train loss: 0.50858, val loss: 0.50558\n",
      "Main effects training epoch: 219, train loss: 0.50843, val loss: 0.50532\n",
      "Main effects training epoch: 220, train loss: 0.50825, val loss: 0.50422\n",
      "Main effects training epoch: 221, train loss: 0.50815, val loss: 0.50174\n",
      "Main effects training epoch: 222, train loss: 0.50840, val loss: 0.50422\n",
      "Main effects training epoch: 223, train loss: 0.50829, val loss: 0.50446\n",
      "Main effects training epoch: 224, train loss: 0.50814, val loss: 0.50287\n",
      "Main effects training epoch: 225, train loss: 0.50794, val loss: 0.50371\n",
      "Main effects training epoch: 226, train loss: 0.50804, val loss: 0.50178\n",
      "Main effects training epoch: 227, train loss: 0.50788, val loss: 0.50386\n",
      "Main effects training epoch: 228, train loss: 0.50811, val loss: 0.50214\n",
      "Main effects training epoch: 229, train loss: 0.50801, val loss: 0.50345\n",
      "Main effects training epoch: 230, train loss: 0.50782, val loss: 0.50250\n",
      "Main effects training epoch: 231, train loss: 0.50807, val loss: 0.50482\n",
      "Main effects training epoch: 232, train loss: 0.50804, val loss: 0.50187\n",
      "Main effects training epoch: 233, train loss: 0.50789, val loss: 0.50213\n",
      "Main effects training epoch: 234, train loss: 0.50812, val loss: 0.50472\n",
      "Main effects training epoch: 235, train loss: 0.50835, val loss: 0.50340\n",
      "Main effects training epoch: 236, train loss: 0.50868, val loss: 0.50171\n",
      "Main effects training epoch: 237, train loss: 0.50800, val loss: 0.50525\n",
      "Main effects training epoch: 238, train loss: 0.50860, val loss: 0.50105\n",
      "Main effects training epoch: 239, train loss: 0.50765, val loss: 0.50408\n",
      "Main effects training epoch: 240, train loss: 0.50836, val loss: 0.49939\n",
      "Main effects training epoch: 241, train loss: 0.50778, val loss: 0.50468\n",
      "Main effects training epoch: 242, train loss: 0.50799, val loss: 0.50230\n",
      "Main effects training epoch: 243, train loss: 0.50783, val loss: 0.50422\n",
      "Main effects training epoch: 244, train loss: 0.50763, val loss: 0.50155\n",
      "Main effects training epoch: 245, train loss: 0.50797, val loss: 0.50360\n",
      "Main effects training epoch: 246, train loss: 0.50751, val loss: 0.50323\n",
      "Main effects training epoch: 247, train loss: 0.50746, val loss: 0.50430\n",
      "Main effects training epoch: 248, train loss: 0.50721, val loss: 0.50164\n",
      "Main effects training epoch: 249, train loss: 0.50767, val loss: 0.50377\n",
      "Main effects training epoch: 250, train loss: 0.50755, val loss: 0.50408\n",
      "Main effects training epoch: 251, train loss: 0.50731, val loss: 0.50348\n",
      "Main effects training epoch: 252, train loss: 0.50795, val loss: 0.50049\n",
      "Main effects training epoch: 253, train loss: 0.50744, val loss: 0.50324\n",
      "Main effects training epoch: 254, train loss: 0.50734, val loss: 0.50438\n",
      "Main effects training epoch: 255, train loss: 0.50750, val loss: 0.50408\n",
      "Main effects training epoch: 256, train loss: 0.50732, val loss: 0.50194\n",
      "Main effects training epoch: 257, train loss: 0.50740, val loss: 0.50328\n",
      "Main effects training epoch: 258, train loss: 0.50705, val loss: 0.50316\n",
      "Main effects training epoch: 259, train loss: 0.50715, val loss: 0.50115\n",
      "Main effects training epoch: 260, train loss: 0.50747, val loss: 0.50208\n",
      "Main effects training epoch: 261, train loss: 0.50766, val loss: 0.50554\n",
      "Main effects training epoch: 262, train loss: 0.50734, val loss: 0.50286\n",
      "Main effects training epoch: 263, train loss: 0.50724, val loss: 0.50371\n",
      "Main effects training epoch: 264, train loss: 0.50692, val loss: 0.50149\n",
      "Main effects training epoch: 265, train loss: 0.50705, val loss: 0.50371\n",
      "Main effects training epoch: 266, train loss: 0.50725, val loss: 0.50322\n",
      "Main effects training epoch: 267, train loss: 0.50696, val loss: 0.50456\n",
      "Main effects training epoch: 268, train loss: 0.50719, val loss: 0.50158\n",
      "Main effects training epoch: 269, train loss: 0.50679, val loss: 0.50310\n",
      "Main effects training epoch: 270, train loss: 0.50667, val loss: 0.50218\n",
      "Main effects training epoch: 271, train loss: 0.50723, val loss: 0.50624\n",
      "Main effects training epoch: 272, train loss: 0.50722, val loss: 0.50183\n",
      "Main effects training epoch: 273, train loss: 0.50690, val loss: 0.50270\n",
      "Main effects training epoch: 274, train loss: 0.50718, val loss: 0.50258\n",
      "Main effects training epoch: 275, train loss: 0.50704, val loss: 0.50367\n",
      "Main effects training epoch: 276, train loss: 0.50678, val loss: 0.50162\n",
      "Main effects training epoch: 277, train loss: 0.50697, val loss: 0.50346\n",
      "Main effects training epoch: 278, train loss: 0.50786, val loss: 0.50224\n",
      "Main effects training epoch: 279, train loss: 0.50803, val loss: 0.50613\n",
      "Main effects training epoch: 280, train loss: 0.50714, val loss: 0.50253\n",
      "Main effects training epoch: 281, train loss: 0.50688, val loss: 0.50376\n",
      "Main effects training epoch: 282, train loss: 0.50679, val loss: 0.50353\n",
      "Main effects training epoch: 283, train loss: 0.50737, val loss: 0.50387\n",
      "Main effects training epoch: 284, train loss: 0.50698, val loss: 0.50196\n",
      "Main effects training epoch: 285, train loss: 0.50676, val loss: 0.50351\n",
      "Main effects training epoch: 286, train loss: 0.50678, val loss: 0.50049\n",
      "Main effects training epoch: 287, train loss: 0.50669, val loss: 0.50377\n",
      "Main effects training epoch: 288, train loss: 0.50636, val loss: 0.50219\n",
      "Main effects training epoch: 289, train loss: 0.50626, val loss: 0.50178\n",
      "Main effects training epoch: 290, train loss: 0.50623, val loss: 0.50361\n",
      "Main effects training epoch: 291, train loss: 0.50619, val loss: 0.50328\n",
      "Main effects training epoch: 292, train loss: 0.50644, val loss: 0.50388\n",
      "Main effects training epoch: 293, train loss: 0.50633, val loss: 0.50288\n",
      "Main effects training epoch: 294, train loss: 0.50654, val loss: 0.50063\n",
      "Main effects training epoch: 295, train loss: 0.50691, val loss: 0.50352\n",
      "Main effects training epoch: 296, train loss: 0.50654, val loss: 0.50264\n",
      "Main effects training epoch: 297, train loss: 0.50606, val loss: 0.50291\n",
      "Main effects training epoch: 298, train loss: 0.50629, val loss: 0.50339\n",
      "Main effects training epoch: 299, train loss: 0.50645, val loss: 0.50460\n",
      "Main effects training epoch: 300, train loss: 0.50599, val loss: 0.50165\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51334, val loss: 0.50336\n",
      "Main effects tuning epoch: 2, train loss: 0.51329, val loss: 0.50389\n",
      "Main effects tuning epoch: 3, train loss: 0.51340, val loss: 0.50522\n",
      "Main effects tuning epoch: 4, train loss: 0.51353, val loss: 0.50177\n",
      "Main effects tuning epoch: 5, train loss: 0.51331, val loss: 0.50577\n",
      "Main effects tuning epoch: 6, train loss: 0.51333, val loss: 0.50256\n",
      "Main effects tuning epoch: 7, train loss: 0.51311, val loss: 0.50231\n",
      "Main effects tuning epoch: 8, train loss: 0.51292, val loss: 0.50376\n",
      "Main effects tuning epoch: 9, train loss: 0.51340, val loss: 0.50396\n",
      "Main effects tuning epoch: 10, train loss: 0.51343, val loss: 0.50412\n",
      "Main effects tuning epoch: 11, train loss: 0.51363, val loss: 0.50359\n",
      "Main effects tuning epoch: 12, train loss: 0.51380, val loss: 0.50549\n",
      "Main effects tuning epoch: 13, train loss: 0.51348, val loss: 0.50252\n",
      "Main effects tuning epoch: 14, train loss: 0.51298, val loss: 0.50366\n",
      "Main effects tuning epoch: 15, train loss: 0.51306, val loss: 0.50427\n",
      "Main effects tuning epoch: 16, train loss: 0.51289, val loss: 0.50347\n",
      "Main effects tuning epoch: 17, train loss: 0.51298, val loss: 0.50345\n",
      "Main effects tuning epoch: 18, train loss: 0.51298, val loss: 0.50294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects tuning epoch: 19, train loss: 0.51274, val loss: 0.50464\n",
      "Main effects tuning epoch: 20, train loss: 0.51276, val loss: 0.50280\n",
      "Main effects tuning epoch: 21, train loss: 0.51268, val loss: 0.50222\n",
      "Main effects tuning epoch: 22, train loss: 0.51266, val loss: 0.50264\n",
      "Main effects tuning epoch: 23, train loss: 0.51282, val loss: 0.50547\n",
      "Main effects tuning epoch: 24, train loss: 0.51328, val loss: 0.50114\n",
      "Main effects tuning epoch: 25, train loss: 0.51297, val loss: 0.50408\n",
      "Main effects tuning epoch: 26, train loss: 0.51302, val loss: 0.50229\n",
      "Main effects tuning epoch: 27, train loss: 0.51259, val loss: 0.50272\n",
      "Main effects tuning epoch: 28, train loss: 0.51275, val loss: 0.50446\n",
      "Main effects tuning epoch: 29, train loss: 0.51257, val loss: 0.50358\n",
      "Main effects tuning epoch: 30, train loss: 0.51269, val loss: 0.50274\n",
      "Main effects tuning epoch: 31, train loss: 0.51245, val loss: 0.50305\n",
      "Main effects tuning epoch: 32, train loss: 0.51261, val loss: 0.50278\n",
      "Main effects tuning epoch: 33, train loss: 0.51240, val loss: 0.50365\n",
      "Main effects tuning epoch: 34, train loss: 0.51253, val loss: 0.50344\n",
      "Main effects tuning epoch: 35, train loss: 0.51243, val loss: 0.50308\n",
      "Main effects tuning epoch: 36, train loss: 0.51258, val loss: 0.50332\n",
      "Main effects tuning epoch: 37, train loss: 0.51257, val loss: 0.50317\n",
      "Main effects tuning epoch: 38, train loss: 0.51240, val loss: 0.50253\n",
      "Main effects tuning epoch: 39, train loss: 0.51225, val loss: 0.50266\n",
      "Main effects tuning epoch: 40, train loss: 0.51251, val loss: 0.50268\n",
      "Main effects tuning epoch: 41, train loss: 0.51289, val loss: 0.50306\n",
      "Main effects tuning epoch: 42, train loss: 0.51284, val loss: 0.50182\n",
      "Main effects tuning epoch: 43, train loss: 0.51287, val loss: 0.50530\n",
      "Main effects tuning epoch: 44, train loss: 0.51288, val loss: 0.50302\n",
      "Main effects tuning epoch: 45, train loss: 0.51234, val loss: 0.50275\n",
      "Main effects tuning epoch: 46, train loss: 0.51259, val loss: 0.50534\n",
      "Main effects tuning epoch: 47, train loss: 0.51255, val loss: 0.50170\n",
      "Main effects tuning epoch: 48, train loss: 0.51255, val loss: 0.50308\n",
      "Main effects tuning epoch: 49, train loss: 0.51239, val loss: 0.50401\n",
      "Main effects tuning epoch: 50, train loss: 0.51226, val loss: 0.50318\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.46129, val loss: 0.44895\n",
      "Interaction training epoch: 2, train loss: 0.36063, val loss: 0.34013\n",
      "Interaction training epoch: 3, train loss: 0.30898, val loss: 0.29814\n",
      "Interaction training epoch: 4, train loss: 0.32677, val loss: 0.32171\n",
      "Interaction training epoch: 5, train loss: 0.29573, val loss: 0.28566\n",
      "Interaction training epoch: 6, train loss: 0.29615, val loss: 0.28632\n",
      "Interaction training epoch: 7, train loss: 0.30742, val loss: 0.30614\n",
      "Interaction training epoch: 8, train loss: 0.28599, val loss: 0.27701\n",
      "Interaction training epoch: 9, train loss: 0.28786, val loss: 0.28143\n",
      "Interaction training epoch: 10, train loss: 0.28092, val loss: 0.27354\n",
      "Interaction training epoch: 11, train loss: 0.28604, val loss: 0.27681\n",
      "Interaction training epoch: 12, train loss: 0.27871, val loss: 0.27252\n",
      "Interaction training epoch: 13, train loss: 0.28110, val loss: 0.27945\n",
      "Interaction training epoch: 14, train loss: 0.28876, val loss: 0.28650\n",
      "Interaction training epoch: 15, train loss: 0.27731, val loss: 0.26946\n",
      "Interaction training epoch: 16, train loss: 0.28005, val loss: 0.27299\n",
      "Interaction training epoch: 17, train loss: 0.27736, val loss: 0.27544\n",
      "Interaction training epoch: 18, train loss: 0.27681, val loss: 0.27268\n",
      "Interaction training epoch: 19, train loss: 0.27660, val loss: 0.26872\n",
      "Interaction training epoch: 20, train loss: 0.27681, val loss: 0.27437\n",
      "Interaction training epoch: 21, train loss: 0.28145, val loss: 0.28224\n",
      "Interaction training epoch: 22, train loss: 0.27722, val loss: 0.27216\n",
      "Interaction training epoch: 23, train loss: 0.27561, val loss: 0.27252\n",
      "Interaction training epoch: 24, train loss: 0.28000, val loss: 0.27941\n",
      "Interaction training epoch: 25, train loss: 0.27705, val loss: 0.27655\n",
      "Interaction training epoch: 26, train loss: 0.27532, val loss: 0.26913\n",
      "Interaction training epoch: 27, train loss: 0.27896, val loss: 0.27696\n",
      "Interaction training epoch: 28, train loss: 0.27325, val loss: 0.27270\n",
      "Interaction training epoch: 29, train loss: 0.27366, val loss: 0.27041\n",
      "Interaction training epoch: 30, train loss: 0.27245, val loss: 0.27369\n",
      "Interaction training epoch: 31, train loss: 0.27582, val loss: 0.27243\n",
      "Interaction training epoch: 32, train loss: 0.27379, val loss: 0.26837\n",
      "Interaction training epoch: 33, train loss: 0.27031, val loss: 0.26867\n",
      "Interaction training epoch: 34, train loss: 0.27074, val loss: 0.26972\n",
      "Interaction training epoch: 35, train loss: 0.27177, val loss: 0.26852\n",
      "Interaction training epoch: 36, train loss: 0.27124, val loss: 0.27337\n",
      "Interaction training epoch: 37, train loss: 0.26898, val loss: 0.26701\n",
      "Interaction training epoch: 38, train loss: 0.27039, val loss: 0.26773\n",
      "Interaction training epoch: 39, train loss: 0.27165, val loss: 0.27114\n",
      "Interaction training epoch: 40, train loss: 0.26873, val loss: 0.26876\n",
      "Interaction training epoch: 41, train loss: 0.27152, val loss: 0.27344\n",
      "Interaction training epoch: 42, train loss: 0.26957, val loss: 0.26809\n",
      "Interaction training epoch: 43, train loss: 0.26955, val loss: 0.26966\n",
      "Interaction training epoch: 44, train loss: 0.26914, val loss: 0.26852\n",
      "Interaction training epoch: 45, train loss: 0.26764, val loss: 0.26757\n",
      "Interaction training epoch: 46, train loss: 0.26768, val loss: 0.26854\n",
      "Interaction training epoch: 47, train loss: 0.26914, val loss: 0.27032\n",
      "Interaction training epoch: 48, train loss: 0.26982, val loss: 0.26878\n",
      "Interaction training epoch: 49, train loss: 0.26812, val loss: 0.26929\n",
      "Interaction training epoch: 50, train loss: 0.26682, val loss: 0.26827\n",
      "Interaction training epoch: 51, train loss: 0.27541, val loss: 0.27761\n",
      "Interaction training epoch: 52, train loss: 0.26949, val loss: 0.27172\n",
      "Interaction training epoch: 53, train loss: 0.26800, val loss: 0.26828\n",
      "Interaction training epoch: 54, train loss: 0.26601, val loss: 0.26808\n",
      "Interaction training epoch: 55, train loss: 0.26660, val loss: 0.27000\n",
      "Interaction training epoch: 56, train loss: 0.26638, val loss: 0.26983\n",
      "Interaction training epoch: 57, train loss: 0.26757, val loss: 0.26875\n",
      "Interaction training epoch: 58, train loss: 0.26912, val loss: 0.27475\n",
      "Interaction training epoch: 59, train loss: 0.26488, val loss: 0.26651\n",
      "Interaction training epoch: 60, train loss: 0.26825, val loss: 0.27262\n",
      "Interaction training epoch: 61, train loss: 0.26724, val loss: 0.27289\n",
      "Interaction training epoch: 62, train loss: 0.26535, val loss: 0.26610\n",
      "Interaction training epoch: 63, train loss: 0.26737, val loss: 0.27418\n",
      "Interaction training epoch: 64, train loss: 0.26549, val loss: 0.26745\n",
      "Interaction training epoch: 65, train loss: 0.26583, val loss: 0.26950\n",
      "Interaction training epoch: 66, train loss: 0.26258, val loss: 0.26389\n",
      "Interaction training epoch: 67, train loss: 0.26365, val loss: 0.27105\n",
      "Interaction training epoch: 68, train loss: 0.26690, val loss: 0.27382\n",
      "Interaction training epoch: 69, train loss: 0.26540, val loss: 0.26910\n",
      "Interaction training epoch: 70, train loss: 0.26196, val loss: 0.26907\n",
      "Interaction training epoch: 71, train loss: 0.26219, val loss: 0.26717\n",
      "Interaction training epoch: 72, train loss: 0.26904, val loss: 0.27586\n",
      "Interaction training epoch: 73, train loss: 0.26709, val loss: 0.27261\n",
      "Interaction training epoch: 74, train loss: 0.26590, val loss: 0.27479\n",
      "Interaction training epoch: 75, train loss: 0.26274, val loss: 0.27062\n",
      "Interaction training epoch: 76, train loss: 0.26204, val loss: 0.26817\n",
      "Interaction training epoch: 77, train loss: 0.26809, val loss: 0.27928\n",
      "Interaction training epoch: 78, train loss: 0.26284, val loss: 0.26851\n",
      "Interaction training epoch: 79, train loss: 0.26191, val loss: 0.26829\n",
      "Interaction training epoch: 80, train loss: 0.26392, val loss: 0.27145\n",
      "Interaction training epoch: 81, train loss: 0.25936, val loss: 0.26718\n",
      "Interaction training epoch: 82, train loss: 0.26456, val loss: 0.26985\n",
      "Interaction training epoch: 83, train loss: 0.26453, val loss: 0.27051\n",
      "Interaction training epoch: 84, train loss: 0.26359, val loss: 0.27022\n",
      "Interaction training epoch: 85, train loss: 0.26334, val loss: 0.27125\n",
      "Interaction training epoch: 86, train loss: 0.26111, val loss: 0.27041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 87, train loss: 0.26026, val loss: 0.26850\n",
      "Interaction training epoch: 88, train loss: 0.26508, val loss: 0.27633\n",
      "Interaction training epoch: 89, train loss: 0.26252, val loss: 0.27121\n",
      "Interaction training epoch: 90, train loss: 0.26226, val loss: 0.26845\n",
      "Interaction training epoch: 91, train loss: 0.26086, val loss: 0.26798\n",
      "Interaction training epoch: 92, train loss: 0.25985, val loss: 0.26865\n",
      "Interaction training epoch: 93, train loss: 0.26079, val loss: 0.27080\n",
      "Interaction training epoch: 94, train loss: 0.26169, val loss: 0.27179\n",
      "Interaction training epoch: 95, train loss: 0.26026, val loss: 0.26870\n",
      "Interaction training epoch: 96, train loss: 0.25796, val loss: 0.26671\n",
      "Interaction training epoch: 97, train loss: 0.25783, val loss: 0.26508\n",
      "Interaction training epoch: 98, train loss: 0.25907, val loss: 0.26832\n",
      "Interaction training epoch: 99, train loss: 0.25983, val loss: 0.27146\n",
      "Interaction training epoch: 100, train loss: 0.25672, val loss: 0.26683\n",
      "Interaction training epoch: 101, train loss: 0.25767, val loss: 0.26516\n",
      "Interaction training epoch: 102, train loss: 0.25823, val loss: 0.26588\n",
      "Interaction training epoch: 103, train loss: 0.25840, val loss: 0.26778\n",
      "Interaction training epoch: 104, train loss: 0.25867, val loss: 0.26893\n",
      "Interaction training epoch: 105, train loss: 0.25838, val loss: 0.26969\n",
      "Interaction training epoch: 106, train loss: 0.25942, val loss: 0.26669\n",
      "Interaction training epoch: 107, train loss: 0.25998, val loss: 0.26782\n",
      "Interaction training epoch: 108, train loss: 0.25804, val loss: 0.26735\n",
      "Interaction training epoch: 109, train loss: 0.25890, val loss: 0.27270\n",
      "Interaction training epoch: 110, train loss: 0.25634, val loss: 0.26661\n",
      "Interaction training epoch: 111, train loss: 0.25580, val loss: 0.26312\n",
      "Interaction training epoch: 112, train loss: 0.25632, val loss: 0.26974\n",
      "Interaction training epoch: 113, train loss: 0.25446, val loss: 0.26681\n",
      "Interaction training epoch: 114, train loss: 0.25620, val loss: 0.26582\n",
      "Interaction training epoch: 115, train loss: 0.26718, val loss: 0.28110\n",
      "Interaction training epoch: 116, train loss: 0.25449, val loss: 0.26538\n",
      "Interaction training epoch: 117, train loss: 0.26073, val loss: 0.27022\n",
      "Interaction training epoch: 118, train loss: 0.25603, val loss: 0.26598\n",
      "Interaction training epoch: 119, train loss: 0.25733, val loss: 0.27130\n",
      "Interaction training epoch: 120, train loss: 0.25462, val loss: 0.26586\n",
      "Interaction training epoch: 121, train loss: 0.25324, val loss: 0.25953\n",
      "Interaction training epoch: 122, train loss: 0.25810, val loss: 0.26999\n",
      "Interaction training epoch: 123, train loss: 0.25213, val loss: 0.26543\n",
      "Interaction training epoch: 124, train loss: 0.25802, val loss: 0.27013\n",
      "Interaction training epoch: 125, train loss: 0.25207, val loss: 0.26173\n",
      "Interaction training epoch: 126, train loss: 0.25437, val loss: 0.26610\n",
      "Interaction training epoch: 127, train loss: 0.25971, val loss: 0.27378\n",
      "Interaction training epoch: 128, train loss: 0.25098, val loss: 0.26266\n",
      "Interaction training epoch: 129, train loss: 0.25449, val loss: 0.26708\n",
      "Interaction training epoch: 130, train loss: 0.25066, val loss: 0.26195\n",
      "Interaction training epoch: 131, train loss: 0.25222, val loss: 0.26431\n",
      "Interaction training epoch: 132, train loss: 0.25298, val loss: 0.26544\n",
      "Interaction training epoch: 133, train loss: 0.25898, val loss: 0.27357\n",
      "Interaction training epoch: 134, train loss: 0.25451, val loss: 0.26619\n",
      "Interaction training epoch: 135, train loss: 0.25121, val loss: 0.26402\n",
      "Interaction training epoch: 136, train loss: 0.25196, val loss: 0.26857\n",
      "Interaction training epoch: 137, train loss: 0.25577, val loss: 0.27055\n",
      "Interaction training epoch: 138, train loss: 0.25187, val loss: 0.26577\n",
      "Interaction training epoch: 139, train loss: 0.24915, val loss: 0.26403\n",
      "Interaction training epoch: 140, train loss: 0.24989, val loss: 0.26235\n",
      "Interaction training epoch: 141, train loss: 0.25328, val loss: 0.26804\n",
      "Interaction training epoch: 142, train loss: 0.25011, val loss: 0.26523\n",
      "Interaction training epoch: 143, train loss: 0.24982, val loss: 0.26445\n",
      "Interaction training epoch: 144, train loss: 0.25669, val loss: 0.27116\n",
      "Interaction training epoch: 145, train loss: 0.24822, val loss: 0.26105\n",
      "Interaction training epoch: 146, train loss: 0.25121, val loss: 0.26864\n",
      "Interaction training epoch: 147, train loss: 0.24777, val loss: 0.25954\n",
      "Interaction training epoch: 148, train loss: 0.25202, val loss: 0.26699\n",
      "Interaction training epoch: 149, train loss: 0.25173, val loss: 0.26570\n",
      "Interaction training epoch: 150, train loss: 0.24768, val loss: 0.26430\n",
      "Interaction training epoch: 151, train loss: 0.24810, val loss: 0.26275\n",
      "Interaction training epoch: 152, train loss: 0.25055, val loss: 0.26604\n",
      "Interaction training epoch: 153, train loss: 0.24717, val loss: 0.26365\n",
      "Interaction training epoch: 154, train loss: 0.24640, val loss: 0.25909\n",
      "Interaction training epoch: 155, train loss: 0.25180, val loss: 0.26607\n",
      "Interaction training epoch: 156, train loss: 0.25021, val loss: 0.26639\n",
      "Interaction training epoch: 157, train loss: 0.24884, val loss: 0.26230\n",
      "Interaction training epoch: 158, train loss: 0.25060, val loss: 0.26732\n",
      "Interaction training epoch: 159, train loss: 0.24901, val loss: 0.26066\n",
      "Interaction training epoch: 160, train loss: 0.25282, val loss: 0.26692\n",
      "Interaction training epoch: 161, train loss: 0.25178, val loss: 0.26526\n",
      "Interaction training epoch: 162, train loss: 0.24740, val loss: 0.26490\n",
      "Interaction training epoch: 163, train loss: 0.25180, val loss: 0.26975\n",
      "Interaction training epoch: 164, train loss: 0.25020, val loss: 0.26267\n",
      "Interaction training epoch: 165, train loss: 0.24870, val loss: 0.26856\n",
      "Interaction training epoch: 166, train loss: 0.24954, val loss: 0.26429\n",
      "Interaction training epoch: 167, train loss: 0.24898, val loss: 0.26424\n",
      "Interaction training epoch: 168, train loss: 0.24798, val loss: 0.26251\n",
      "Interaction training epoch: 169, train loss: 0.25148, val loss: 0.26806\n",
      "Interaction training epoch: 170, train loss: 0.24906, val loss: 0.26533\n",
      "Interaction training epoch: 171, train loss: 0.24575, val loss: 0.26034\n",
      "Interaction training epoch: 172, train loss: 0.25078, val loss: 0.27048\n",
      "Interaction training epoch: 173, train loss: 0.25287, val loss: 0.26917\n",
      "Interaction training epoch: 174, train loss: 0.24772, val loss: 0.26522\n",
      "Interaction training epoch: 175, train loss: 0.25232, val loss: 0.27062\n",
      "Interaction training epoch: 176, train loss: 0.24968, val loss: 0.26237\n",
      "Interaction training epoch: 177, train loss: 0.24653, val loss: 0.26448\n",
      "Interaction training epoch: 178, train loss: 0.24960, val loss: 0.26757\n",
      "Interaction training epoch: 179, train loss: 0.24664, val loss: 0.26175\n",
      "Interaction training epoch: 180, train loss: 0.24770, val loss: 0.26645\n",
      "Interaction training epoch: 181, train loss: 0.24609, val loss: 0.26377\n",
      "Interaction training epoch: 182, train loss: 0.24494, val loss: 0.26220\n",
      "Interaction training epoch: 183, train loss: 0.25121, val loss: 0.27181\n",
      "Interaction training epoch: 184, train loss: 0.24882, val loss: 0.26743\n",
      "Interaction training epoch: 185, train loss: 0.24692, val loss: 0.26354\n",
      "Interaction training epoch: 186, train loss: 0.24429, val loss: 0.26048\n",
      "Interaction training epoch: 187, train loss: 0.24712, val loss: 0.26391\n",
      "Interaction training epoch: 188, train loss: 0.24941, val loss: 0.26661\n",
      "Interaction training epoch: 189, train loss: 0.24658, val loss: 0.26414\n",
      "Interaction training epoch: 190, train loss: 0.24458, val loss: 0.26281\n",
      "Interaction training epoch: 191, train loss: 0.24527, val loss: 0.26241\n",
      "Interaction training epoch: 192, train loss: 0.24416, val loss: 0.26214\n",
      "Interaction training epoch: 193, train loss: 0.24552, val loss: 0.26211\n",
      "Interaction training epoch: 194, train loss: 0.24339, val loss: 0.26175\n",
      "Interaction training epoch: 195, train loss: 0.24345, val loss: 0.25972\n",
      "Interaction training epoch: 196, train loss: 0.24446, val loss: 0.26355\n",
      "Interaction training epoch: 197, train loss: 0.24502, val loss: 0.26440\n",
      "Interaction training epoch: 198, train loss: 0.24748, val loss: 0.26792\n",
      "Interaction training epoch: 199, train loss: 0.24721, val loss: 0.26339\n",
      "Interaction training epoch: 200, train loss: 0.24468, val loss: 0.26344\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########4 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.25619, val loss: 0.26587\n",
      "Interaction tuning epoch: 2, train loss: 0.25457, val loss: 0.26410\n",
      "Interaction tuning epoch: 3, train loss: 0.25272, val loss: 0.26093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 4, train loss: 0.25852, val loss: 0.27170\n",
      "Interaction tuning epoch: 5, train loss: 0.25555, val loss: 0.26341\n",
      "Interaction tuning epoch: 6, train loss: 0.25464, val loss: 0.26645\n",
      "Interaction tuning epoch: 7, train loss: 0.25412, val loss: 0.26416\n",
      "Interaction tuning epoch: 8, train loss: 0.25094, val loss: 0.26088\n",
      "Interaction tuning epoch: 9, train loss: 0.25420, val loss: 0.26679\n",
      "Interaction tuning epoch: 10, train loss: 0.25612, val loss: 0.26986\n",
      "Interaction tuning epoch: 11, train loss: 0.25204, val loss: 0.26173\n",
      "Interaction tuning epoch: 12, train loss: 0.25603, val loss: 0.26429\n",
      "Interaction tuning epoch: 13, train loss: 0.25844, val loss: 0.27033\n",
      "Interaction tuning epoch: 14, train loss: 0.25256, val loss: 0.26353\n",
      "Interaction tuning epoch: 15, train loss: 0.25306, val loss: 0.26514\n",
      "Interaction tuning epoch: 16, train loss: 0.25246, val loss: 0.26517\n",
      "Interaction tuning epoch: 17, train loss: 0.25641, val loss: 0.26874\n",
      "Interaction tuning epoch: 18, train loss: 0.26109, val loss: 0.27622\n",
      "Interaction tuning epoch: 19, train loss: 0.25283, val loss: 0.26125\n",
      "Interaction tuning epoch: 20, train loss: 0.25503, val loss: 0.26913\n",
      "Interaction tuning epoch: 21, train loss: 0.25052, val loss: 0.26176\n",
      "Interaction tuning epoch: 22, train loss: 0.25111, val loss: 0.26060\n",
      "Interaction tuning epoch: 23, train loss: 0.24968, val loss: 0.26420\n",
      "Interaction tuning epoch: 24, train loss: 0.24952, val loss: 0.26033\n",
      "Interaction tuning epoch: 25, train loss: 0.24895, val loss: 0.26092\n",
      "Interaction tuning epoch: 26, train loss: 0.25199, val loss: 0.26572\n",
      "Interaction tuning epoch: 27, train loss: 0.25246, val loss: 0.26659\n",
      "Interaction tuning epoch: 28, train loss: 0.25835, val loss: 0.27734\n",
      "Interaction tuning epoch: 29, train loss: 0.25869, val loss: 0.26826\n",
      "Interaction tuning epoch: 30, train loss: 0.25649, val loss: 0.26814\n",
      "Interaction tuning epoch: 31, train loss: 0.25954, val loss: 0.27367\n",
      "Interaction tuning epoch: 32, train loss: 0.25410, val loss: 0.26683\n",
      "Interaction tuning epoch: 33, train loss: 0.25564, val loss: 0.27249\n",
      "Interaction tuning epoch: 34, train loss: 0.25137, val loss: 0.26450\n",
      "Interaction tuning epoch: 35, train loss: 0.25038, val loss: 0.26624\n",
      "Interaction tuning epoch: 36, train loss: 0.25350, val loss: 0.26820\n",
      "Interaction tuning epoch: 37, train loss: 0.24994, val loss: 0.26254\n",
      "Interaction tuning epoch: 38, train loss: 0.24894, val loss: 0.26420\n",
      "Interaction tuning epoch: 39, train loss: 0.25043, val loss: 0.26433\n",
      "Interaction tuning epoch: 40, train loss: 0.25156, val loss: 0.26502\n",
      "Interaction tuning epoch: 41, train loss: 0.24819, val loss: 0.26616\n",
      "Interaction tuning epoch: 42, train loss: 0.24863, val loss: 0.26398\n",
      "Interaction tuning epoch: 43, train loss: 0.24879, val loss: 0.26580\n",
      "Interaction tuning epoch: 44, train loss: 0.24757, val loss: 0.25895\n",
      "Interaction tuning epoch: 45, train loss: 0.24653, val loss: 0.26264\n",
      "Interaction tuning epoch: 46, train loss: 0.24793, val loss: 0.26584\n",
      "Interaction tuning epoch: 47, train loss: 0.24884, val loss: 0.26472\n",
      "Interaction tuning epoch: 48, train loss: 0.24667, val loss: 0.25916\n",
      "Interaction tuning epoch: 49, train loss: 0.24890, val loss: 0.26652\n",
      "Interaction tuning epoch: 50, train loss: 0.24635, val loss: 0.26243\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 39.62427639961243\n",
      "After the gam stage, training error is 0.24635 , validation error is 0.26243\n",
      "missing value counts: 99115\n",
      "[SoftImpute] Max Singular Value of X_init = 3.879887\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.207176 validation BCE=0.269276,rank=5\n",
      "[SoftImpute] Iter 2: observed BCE=0.202826 validation BCE=0.267533,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.200182 validation BCE=0.266438,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.198053 validation BCE=0.265909,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.197192 validation BCE=0.265689,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.196776 validation BCE=0.265118,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.196202 validation BCE=0.264955,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.195950 validation BCE=0.264623,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.195797 validation BCE=0.264127,rank=5\n",
      "[SoftImpute] Iter 10: observed BCE=0.195909 validation BCE=0.263691,rank=5\n",
      "[SoftImpute] Iter 11: observed BCE=0.195756 validation BCE=0.263785,rank=5\n",
      "[SoftImpute] Iter 12: observed BCE=0.195763 validation BCE=0.263179,rank=5\n",
      "[SoftImpute] Iter 13: observed BCE=0.196246 validation BCE=0.252012,rank=5\n",
      "[SoftImpute] Iter 14: observed BCE=0.197030 validation BCE=0.251672,rank=5\n",
      "[SoftImpute] Iter 15: observed BCE=0.197273 validation BCE=0.251452,rank=5\n",
      "[SoftImpute] Iter 16: observed BCE=0.197400 validation BCE=0.250820,rank=5\n",
      "[SoftImpute] Iter 17: observed BCE=0.197529 validation BCE=0.251516,rank=5\n",
      "[SoftImpute] Iter 18: observed BCE=0.197364 validation BCE=0.250661,rank=5\n",
      "[SoftImpute] Iter 19: observed BCE=0.197483 validation BCE=0.250867,rank=5\n",
      "[SoftImpute] Iter 20: observed BCE=0.197506 validation BCE=0.250836,rank=5\n",
      "[SoftImpute] Iter 21: observed BCE=0.197176 validation BCE=0.250834,rank=5\n",
      "[SoftImpute] Iter 22: observed BCE=0.197185 validation BCE=0.250630,rank=5\n",
      "[SoftImpute] Iter 23: observed BCE=0.197096 validation BCE=0.250845,rank=5\n",
      "[SoftImpute] Iter 24: observed BCE=0.197206 validation BCE=0.250223,rank=5\n",
      "[SoftImpute] Iter 25: observed BCE=0.197469 validation BCE=0.250899,rank=5\n",
      "[SoftImpute] Iter 26: observed BCE=0.197591 validation BCE=0.250502,rank=5\n",
      "[SoftImpute] Iter 27: observed BCE=0.197287 validation BCE=0.250882,rank=5\n",
      "[SoftImpute] Iter 28: observed BCE=0.197148 validation BCE=0.250680,rank=5\n",
      "[SoftImpute] Iter 29: observed BCE=0.196886 validation BCE=0.250376,rank=5\n",
      "[SoftImpute] Iter 30: observed BCE=0.197284 validation BCE=0.250506,rank=5\n",
      "[SoftImpute] Iter 31: observed BCE=0.197168 validation BCE=0.250544,rank=5\n",
      "[SoftImpute] Iter 32: observed BCE=0.197271 validation BCE=0.250546,rank=5\n",
      "[SoftImpute] Stopped after iteration 32 for lambda=0.077598\n",
      "final num of user group: 20\n",
      "final num of item group: 28\n",
      "change mode state : True\n",
      "time cost: 10.901796579360962\n",
      "After the matrix factor stage, training error is 0.19727, validation error is 0.25055\n",
      "7\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.67922, val loss: 0.68029\n",
      "Main effects training epoch: 2, train loss: 0.67294, val loss: 0.67344\n",
      "Main effects training epoch: 3, train loss: 0.66720, val loss: 0.66633\n",
      "Main effects training epoch: 4, train loss: 0.65676, val loss: 0.65650\n",
      "Main effects training epoch: 5, train loss: 0.63670, val loss: 0.63391\n",
      "Main effects training epoch: 6, train loss: 0.60375, val loss: 0.59726\n",
      "Main effects training epoch: 7, train loss: 0.56392, val loss: 0.55025\n",
      "Main effects training epoch: 8, train loss: 0.54257, val loss: 0.51955\n",
      "Main effects training epoch: 9, train loss: 0.53334, val loss: 0.50041\n",
      "Main effects training epoch: 10, train loss: 0.52907, val loss: 0.49515\n",
      "Main effects training epoch: 11, train loss: 0.52633, val loss: 0.49005\n",
      "Main effects training epoch: 12, train loss: 0.52728, val loss: 0.49026\n",
      "Main effects training epoch: 13, train loss: 0.52856, val loss: 0.49712\n",
      "Main effects training epoch: 14, train loss: 0.52679, val loss: 0.48911\n",
      "Main effects training epoch: 15, train loss: 0.52669, val loss: 0.49279\n",
      "Main effects training epoch: 16, train loss: 0.52829, val loss: 0.49260\n",
      "Main effects training epoch: 17, train loss: 0.52648, val loss: 0.49369\n",
      "Main effects training epoch: 18, train loss: 0.52588, val loss: 0.49047\n",
      "Main effects training epoch: 19, train loss: 0.52512, val loss: 0.49125\n",
      "Main effects training epoch: 20, train loss: 0.52484, val loss: 0.48941\n",
      "Main effects training epoch: 21, train loss: 0.52537, val loss: 0.49309\n",
      "Main effects training epoch: 22, train loss: 0.52516, val loss: 0.48872\n",
      "Main effects training epoch: 23, train loss: 0.52448, val loss: 0.49132\n",
      "Main effects training epoch: 24, train loss: 0.52420, val loss: 0.49011\n",
      "Main effects training epoch: 25, train loss: 0.52416, val loss: 0.49058\n",
      "Main effects training epoch: 26, train loss: 0.52421, val loss: 0.48987\n",
      "Main effects training epoch: 27, train loss: 0.52427, val loss: 0.49103\n",
      "Main effects training epoch: 28, train loss: 0.52392, val loss: 0.48981\n",
      "Main effects training epoch: 29, train loss: 0.52416, val loss: 0.49075\n",
      "Main effects training epoch: 30, train loss: 0.52504, val loss: 0.49021\n",
      "Main effects training epoch: 31, train loss: 0.52478, val loss: 0.49080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 32, train loss: 0.52444, val loss: 0.48994\n",
      "Main effects training epoch: 33, train loss: 0.52419, val loss: 0.49070\n",
      "Main effects training epoch: 34, train loss: 0.52442, val loss: 0.48873\n",
      "Main effects training epoch: 35, train loss: 0.52416, val loss: 0.49133\n",
      "Main effects training epoch: 36, train loss: 0.52386, val loss: 0.48926\n",
      "Main effects training epoch: 37, train loss: 0.52359, val loss: 0.48943\n",
      "Main effects training epoch: 38, train loss: 0.52401, val loss: 0.48909\n",
      "Main effects training epoch: 39, train loss: 0.52380, val loss: 0.49057\n",
      "Main effects training epoch: 40, train loss: 0.52352, val loss: 0.48934\n",
      "Main effects training epoch: 41, train loss: 0.52393, val loss: 0.48984\n",
      "Main effects training epoch: 42, train loss: 0.52413, val loss: 0.48969\n",
      "Main effects training epoch: 43, train loss: 0.52368, val loss: 0.48952\n",
      "Main effects training epoch: 44, train loss: 0.52354, val loss: 0.48988\n",
      "Main effects training epoch: 45, train loss: 0.52383, val loss: 0.48856\n",
      "Main effects training epoch: 46, train loss: 0.52348, val loss: 0.49063\n",
      "Main effects training epoch: 47, train loss: 0.52365, val loss: 0.48929\n",
      "Main effects training epoch: 48, train loss: 0.52360, val loss: 0.48815\n",
      "Main effects training epoch: 49, train loss: 0.52411, val loss: 0.49174\n",
      "Main effects training epoch: 50, train loss: 0.52389, val loss: 0.48877\n",
      "Main effects training epoch: 51, train loss: 0.52365, val loss: 0.49077\n",
      "Main effects training epoch: 52, train loss: 0.52327, val loss: 0.48883\n",
      "Main effects training epoch: 53, train loss: 0.52376, val loss: 0.49106\n",
      "Main effects training epoch: 54, train loss: 0.52328, val loss: 0.48884\n",
      "Main effects training epoch: 55, train loss: 0.52367, val loss: 0.49067\n",
      "Main effects training epoch: 56, train loss: 0.52344, val loss: 0.48844\n",
      "Main effects training epoch: 57, train loss: 0.52314, val loss: 0.48924\n",
      "Main effects training epoch: 58, train loss: 0.52333, val loss: 0.48907\n",
      "Main effects training epoch: 59, train loss: 0.52301, val loss: 0.48916\n",
      "Main effects training epoch: 60, train loss: 0.52322, val loss: 0.48932\n",
      "Main effects training epoch: 61, train loss: 0.52285, val loss: 0.48959\n",
      "Main effects training epoch: 62, train loss: 0.52293, val loss: 0.48972\n",
      "Main effects training epoch: 63, train loss: 0.52323, val loss: 0.48876\n",
      "Main effects training epoch: 64, train loss: 0.52283, val loss: 0.48906\n",
      "Main effects training epoch: 65, train loss: 0.52300, val loss: 0.49068\n",
      "Main effects training epoch: 66, train loss: 0.52274, val loss: 0.48865\n",
      "Main effects training epoch: 67, train loss: 0.52306, val loss: 0.49022\n",
      "Main effects training epoch: 68, train loss: 0.52308, val loss: 0.48814\n",
      "Main effects training epoch: 69, train loss: 0.52306, val loss: 0.48987\n",
      "Main effects training epoch: 70, train loss: 0.52305, val loss: 0.48811\n",
      "Main effects training epoch: 71, train loss: 0.52326, val loss: 0.49165\n",
      "Main effects training epoch: 72, train loss: 0.52260, val loss: 0.48838\n",
      "Main effects training epoch: 73, train loss: 0.52237, val loss: 0.48804\n",
      "Main effects training epoch: 74, train loss: 0.52257, val loss: 0.48869\n",
      "Main effects training epoch: 75, train loss: 0.52249, val loss: 0.48941\n",
      "Main effects training epoch: 76, train loss: 0.52232, val loss: 0.48871\n",
      "Main effects training epoch: 77, train loss: 0.52230, val loss: 0.48831\n",
      "Main effects training epoch: 78, train loss: 0.52252, val loss: 0.48928\n",
      "Main effects training epoch: 79, train loss: 0.52230, val loss: 0.48800\n",
      "Main effects training epoch: 80, train loss: 0.52230, val loss: 0.48763\n",
      "Main effects training epoch: 81, train loss: 0.52232, val loss: 0.48834\n",
      "Main effects training epoch: 82, train loss: 0.52253, val loss: 0.48965\n",
      "Main effects training epoch: 83, train loss: 0.52244, val loss: 0.48830\n",
      "Main effects training epoch: 84, train loss: 0.52213, val loss: 0.48787\n",
      "Main effects training epoch: 85, train loss: 0.52246, val loss: 0.48787\n",
      "Main effects training epoch: 86, train loss: 0.52260, val loss: 0.48985\n",
      "Main effects training epoch: 87, train loss: 0.52346, val loss: 0.48846\n",
      "Main effects training epoch: 88, train loss: 0.52327, val loss: 0.49228\n",
      "Main effects training epoch: 89, train loss: 0.52357, val loss: 0.48662\n",
      "Main effects training epoch: 90, train loss: 0.52399, val loss: 0.49264\n",
      "Main effects training epoch: 91, train loss: 0.52301, val loss: 0.48757\n",
      "Main effects training epoch: 92, train loss: 0.52239, val loss: 0.48981\n",
      "Main effects training epoch: 93, train loss: 0.52209, val loss: 0.48853\n",
      "Main effects training epoch: 94, train loss: 0.52342, val loss: 0.48939\n",
      "Main effects training epoch: 95, train loss: 0.52325, val loss: 0.49039\n",
      "Main effects training epoch: 96, train loss: 0.52235, val loss: 0.48743\n",
      "Main effects training epoch: 97, train loss: 0.52236, val loss: 0.49007\n",
      "Main effects training epoch: 98, train loss: 0.52168, val loss: 0.48775\n",
      "Main effects training epoch: 99, train loss: 0.52164, val loss: 0.48790\n",
      "Main effects training epoch: 100, train loss: 0.52157, val loss: 0.48752\n",
      "Main effects training epoch: 101, train loss: 0.52181, val loss: 0.48778\n",
      "Main effects training epoch: 102, train loss: 0.52235, val loss: 0.48748\n",
      "Main effects training epoch: 103, train loss: 0.52163, val loss: 0.48902\n",
      "Main effects training epoch: 104, train loss: 0.52172, val loss: 0.48812\n",
      "Main effects training epoch: 105, train loss: 0.52143, val loss: 0.48666\n",
      "Main effects training epoch: 106, train loss: 0.52150, val loss: 0.48876\n",
      "Main effects training epoch: 107, train loss: 0.52194, val loss: 0.48663\n",
      "Main effects training epoch: 108, train loss: 0.52166, val loss: 0.48807\n",
      "Main effects training epoch: 109, train loss: 0.52296, val loss: 0.48814\n",
      "Main effects training epoch: 110, train loss: 0.52257, val loss: 0.49023\n",
      "Main effects training epoch: 111, train loss: 0.52181, val loss: 0.48573\n",
      "Main effects training epoch: 112, train loss: 0.52160, val loss: 0.48755\n",
      "Main effects training epoch: 113, train loss: 0.52167, val loss: 0.48666\n",
      "Main effects training epoch: 114, train loss: 0.52228, val loss: 0.48934\n",
      "Main effects training epoch: 115, train loss: 0.52302, val loss: 0.48620\n",
      "Main effects training epoch: 116, train loss: 0.52217, val loss: 0.48973\n",
      "Main effects training epoch: 117, train loss: 0.52174, val loss: 0.48696\n",
      "Main effects training epoch: 118, train loss: 0.52162, val loss: 0.48726\n",
      "Main effects training epoch: 119, train loss: 0.52128, val loss: 0.48665\n",
      "Main effects training epoch: 120, train loss: 0.52127, val loss: 0.48750\n",
      "Main effects training epoch: 121, train loss: 0.52096, val loss: 0.48753\n",
      "Main effects training epoch: 122, train loss: 0.52140, val loss: 0.48660\n",
      "Main effects training epoch: 123, train loss: 0.52134, val loss: 0.48822\n",
      "Main effects training epoch: 124, train loss: 0.52130, val loss: 0.48615\n",
      "Main effects training epoch: 125, train loss: 0.52105, val loss: 0.48623\n",
      "Main effects training epoch: 126, train loss: 0.52091, val loss: 0.48680\n",
      "Main effects training epoch: 127, train loss: 0.52106, val loss: 0.48647\n",
      "Main effects training epoch: 128, train loss: 0.52085, val loss: 0.48665\n",
      "Main effects training epoch: 129, train loss: 0.52095, val loss: 0.48685\n",
      "Main effects training epoch: 130, train loss: 0.52134, val loss: 0.48651\n",
      "Main effects training epoch: 131, train loss: 0.52110, val loss: 0.48792\n",
      "Main effects training epoch: 132, train loss: 0.52115, val loss: 0.48554\n",
      "Main effects training epoch: 133, train loss: 0.52080, val loss: 0.48795\n",
      "Main effects training epoch: 134, train loss: 0.52078, val loss: 0.48656\n",
      "Main effects training epoch: 135, train loss: 0.52042, val loss: 0.48593\n",
      "Main effects training epoch: 136, train loss: 0.52042, val loss: 0.48695\n",
      "Main effects training epoch: 137, train loss: 0.52037, val loss: 0.48638\n",
      "Main effects training epoch: 138, train loss: 0.52031, val loss: 0.48637\n",
      "Main effects training epoch: 139, train loss: 0.52039, val loss: 0.48540\n",
      "Main effects training epoch: 140, train loss: 0.52091, val loss: 0.48862\n",
      "Main effects training epoch: 141, train loss: 0.52158, val loss: 0.48547\n",
      "Main effects training epoch: 142, train loss: 0.52057, val loss: 0.48626\n",
      "Main effects training epoch: 143, train loss: 0.52077, val loss: 0.48774\n",
      "Main effects training epoch: 144, train loss: 0.52141, val loss: 0.48640\n",
      "Main effects training epoch: 145, train loss: 0.52028, val loss: 0.48678\n",
      "Main effects training epoch: 146, train loss: 0.51999, val loss: 0.48555\n",
      "Main effects training epoch: 147, train loss: 0.52004, val loss: 0.48513\n",
      "Main effects training epoch: 148, train loss: 0.52053, val loss: 0.48672\n",
      "Main effects training epoch: 149, train loss: 0.52025, val loss: 0.48656\n",
      "Main effects training epoch: 150, train loss: 0.51926, val loss: 0.48464\n",
      "Main effects training epoch: 151, train loss: 0.51949, val loss: 0.48608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 152, train loss: 0.51958, val loss: 0.48511\n",
      "Main effects training epoch: 153, train loss: 0.51973, val loss: 0.48773\n",
      "Main effects training epoch: 154, train loss: 0.51917, val loss: 0.48415\n",
      "Main effects training epoch: 155, train loss: 0.51881, val loss: 0.48607\n",
      "Main effects training epoch: 156, train loss: 0.51844, val loss: 0.48353\n",
      "Main effects training epoch: 157, train loss: 0.51934, val loss: 0.48763\n",
      "Main effects training epoch: 158, train loss: 0.51867, val loss: 0.48415\n",
      "Main effects training epoch: 159, train loss: 0.51836, val loss: 0.48418\n",
      "Main effects training epoch: 160, train loss: 0.51825, val loss: 0.48407\n",
      "Main effects training epoch: 161, train loss: 0.51821, val loss: 0.48502\n",
      "Main effects training epoch: 162, train loss: 0.51842, val loss: 0.48395\n",
      "Main effects training epoch: 163, train loss: 0.51928, val loss: 0.48687\n",
      "Main effects training epoch: 164, train loss: 0.51814, val loss: 0.48415\n",
      "Main effects training epoch: 165, train loss: 0.51829, val loss: 0.48363\n",
      "Main effects training epoch: 166, train loss: 0.51808, val loss: 0.48509\n",
      "Main effects training epoch: 167, train loss: 0.51829, val loss: 0.48420\n",
      "Main effects training epoch: 168, train loss: 0.51822, val loss: 0.48610\n",
      "Main effects training epoch: 169, train loss: 0.51802, val loss: 0.48283\n",
      "Main effects training epoch: 170, train loss: 0.51796, val loss: 0.48473\n",
      "Main effects training epoch: 171, train loss: 0.51772, val loss: 0.48310\n",
      "Main effects training epoch: 172, train loss: 0.51781, val loss: 0.48480\n",
      "Main effects training epoch: 173, train loss: 0.51778, val loss: 0.48310\n",
      "Main effects training epoch: 174, train loss: 0.51772, val loss: 0.48427\n",
      "Main effects training epoch: 175, train loss: 0.51811, val loss: 0.48328\n",
      "Main effects training epoch: 176, train loss: 0.51784, val loss: 0.48538\n",
      "Main effects training epoch: 177, train loss: 0.51753, val loss: 0.48346\n",
      "Main effects training epoch: 178, train loss: 0.51764, val loss: 0.48437\n",
      "Main effects training epoch: 179, train loss: 0.51753, val loss: 0.48328\n",
      "Main effects training epoch: 180, train loss: 0.51730, val loss: 0.48318\n",
      "Main effects training epoch: 181, train loss: 0.51732, val loss: 0.48406\n",
      "Main effects training epoch: 182, train loss: 0.51741, val loss: 0.48220\n",
      "Main effects training epoch: 183, train loss: 0.51683, val loss: 0.48435\n",
      "Main effects training epoch: 184, train loss: 0.51691, val loss: 0.48342\n",
      "Main effects training epoch: 185, train loss: 0.51722, val loss: 0.48383\n",
      "Main effects training epoch: 186, train loss: 0.51701, val loss: 0.48408\n",
      "Main effects training epoch: 187, train loss: 0.51675, val loss: 0.48321\n",
      "Main effects training epoch: 188, train loss: 0.51667, val loss: 0.48421\n",
      "Main effects training epoch: 189, train loss: 0.51750, val loss: 0.48408\n",
      "Main effects training epoch: 190, train loss: 0.51696, val loss: 0.48372\n",
      "Main effects training epoch: 191, train loss: 0.51677, val loss: 0.48353\n",
      "Main effects training epoch: 192, train loss: 0.51651, val loss: 0.48337\n",
      "Main effects training epoch: 193, train loss: 0.51675, val loss: 0.48438\n",
      "Main effects training epoch: 194, train loss: 0.51660, val loss: 0.48265\n",
      "Main effects training epoch: 195, train loss: 0.51682, val loss: 0.48490\n",
      "Main effects training epoch: 196, train loss: 0.51681, val loss: 0.48215\n",
      "Main effects training epoch: 197, train loss: 0.51701, val loss: 0.48528\n",
      "Main effects training epoch: 198, train loss: 0.51652, val loss: 0.48292\n",
      "Main effects training epoch: 199, train loss: 0.51621, val loss: 0.48302\n",
      "Main effects training epoch: 200, train loss: 0.51615, val loss: 0.48296\n",
      "Main effects training epoch: 201, train loss: 0.51607, val loss: 0.48196\n",
      "Main effects training epoch: 202, train loss: 0.51618, val loss: 0.48215\n",
      "Main effects training epoch: 203, train loss: 0.51626, val loss: 0.48301\n",
      "Main effects training epoch: 204, train loss: 0.51612, val loss: 0.48390\n",
      "Main effects training epoch: 205, train loss: 0.51587, val loss: 0.48311\n",
      "Main effects training epoch: 206, train loss: 0.51619, val loss: 0.48397\n",
      "Main effects training epoch: 207, train loss: 0.51597, val loss: 0.48146\n",
      "Main effects training epoch: 208, train loss: 0.51588, val loss: 0.48148\n",
      "Main effects training epoch: 209, train loss: 0.51591, val loss: 0.48454\n",
      "Main effects training epoch: 210, train loss: 0.51574, val loss: 0.48289\n",
      "Main effects training epoch: 211, train loss: 0.51569, val loss: 0.48306\n",
      "Main effects training epoch: 212, train loss: 0.51554, val loss: 0.48140\n",
      "Main effects training epoch: 213, train loss: 0.51576, val loss: 0.48414\n",
      "Main effects training epoch: 214, train loss: 0.51590, val loss: 0.48157\n",
      "Main effects training epoch: 215, train loss: 0.51605, val loss: 0.48505\n",
      "Main effects training epoch: 216, train loss: 0.51642, val loss: 0.48211\n",
      "Main effects training epoch: 217, train loss: 0.51593, val loss: 0.48413\n",
      "Main effects training epoch: 218, train loss: 0.51557, val loss: 0.48228\n",
      "Main effects training epoch: 219, train loss: 0.51554, val loss: 0.48224\n",
      "Main effects training epoch: 220, train loss: 0.51547, val loss: 0.48437\n",
      "Main effects training epoch: 221, train loss: 0.51568, val loss: 0.48103\n",
      "Main effects training epoch: 222, train loss: 0.51585, val loss: 0.48538\n",
      "Main effects training epoch: 223, train loss: 0.51531, val loss: 0.48149\n",
      "Main effects training epoch: 224, train loss: 0.51523, val loss: 0.48282\n",
      "Main effects training epoch: 225, train loss: 0.51489, val loss: 0.48128\n",
      "Main effects training epoch: 226, train loss: 0.51492, val loss: 0.48256\n",
      "Main effects training epoch: 227, train loss: 0.51477, val loss: 0.48230\n",
      "Main effects training epoch: 228, train loss: 0.51481, val loss: 0.48313\n",
      "Main effects training epoch: 229, train loss: 0.51455, val loss: 0.48165\n",
      "Main effects training epoch: 230, train loss: 0.51505, val loss: 0.48547\n",
      "Main effects training epoch: 231, train loss: 0.51444, val loss: 0.48119\n",
      "Main effects training epoch: 232, train loss: 0.51416, val loss: 0.48237\n",
      "Main effects training epoch: 233, train loss: 0.51433, val loss: 0.48221\n",
      "Main effects training epoch: 234, train loss: 0.51412, val loss: 0.48214\n",
      "Main effects training epoch: 235, train loss: 0.51403, val loss: 0.48361\n",
      "Main effects training epoch: 236, train loss: 0.51435, val loss: 0.48108\n",
      "Main effects training epoch: 237, train loss: 0.51390, val loss: 0.48265\n",
      "Main effects training epoch: 238, train loss: 0.51369, val loss: 0.48233\n",
      "Main effects training epoch: 239, train loss: 0.51375, val loss: 0.48151\n",
      "Main effects training epoch: 240, train loss: 0.51368, val loss: 0.48197\n",
      "Main effects training epoch: 241, train loss: 0.51358, val loss: 0.48285\n",
      "Main effects training epoch: 242, train loss: 0.51384, val loss: 0.48358\n",
      "Main effects training epoch: 243, train loss: 0.51327, val loss: 0.48239\n",
      "Main effects training epoch: 244, train loss: 0.51337, val loss: 0.48263\n",
      "Main effects training epoch: 245, train loss: 0.51342, val loss: 0.48305\n",
      "Main effects training epoch: 246, train loss: 0.51350, val loss: 0.48209\n",
      "Main effects training epoch: 247, train loss: 0.51322, val loss: 0.48375\n",
      "Main effects training epoch: 248, train loss: 0.51277, val loss: 0.48310\n",
      "Main effects training epoch: 249, train loss: 0.51298, val loss: 0.48345\n",
      "Main effects training epoch: 250, train loss: 0.51274, val loss: 0.48297\n",
      "Main effects training epoch: 251, train loss: 0.51231, val loss: 0.48201\n",
      "Main effects training epoch: 252, train loss: 0.51216, val loss: 0.48179\n",
      "Main effects training epoch: 253, train loss: 0.51268, val loss: 0.48371\n",
      "Main effects training epoch: 254, train loss: 0.51219, val loss: 0.48211\n",
      "Main effects training epoch: 255, train loss: 0.51175, val loss: 0.48264\n",
      "Main effects training epoch: 256, train loss: 0.51172, val loss: 0.48307\n",
      "Main effects training epoch: 257, train loss: 0.51205, val loss: 0.48180\n",
      "Main effects training epoch: 258, train loss: 0.51181, val loss: 0.48297\n",
      "Main effects training epoch: 259, train loss: 0.51151, val loss: 0.48305\n",
      "Main effects training epoch: 260, train loss: 0.51123, val loss: 0.48273\n",
      "Main effects training epoch: 261, train loss: 0.51151, val loss: 0.48258\n",
      "Main effects training epoch: 262, train loss: 0.51154, val loss: 0.48348\n",
      "Main effects training epoch: 263, train loss: 0.51101, val loss: 0.48386\n",
      "Main effects training epoch: 264, train loss: 0.51099, val loss: 0.48187\n",
      "Main effects training epoch: 265, train loss: 0.51144, val loss: 0.48394\n",
      "Main effects training epoch: 266, train loss: 0.51140, val loss: 0.48221\n",
      "Main effects training epoch: 267, train loss: 0.51091, val loss: 0.48207\n",
      "Main effects training epoch: 268, train loss: 0.51081, val loss: 0.48456\n",
      "Main effects training epoch: 269, train loss: 0.51114, val loss: 0.48318\n",
      "Main effects training epoch: 270, train loss: 0.51336, val loss: 0.48389\n",
      "Main effects training epoch: 271, train loss: 0.51194, val loss: 0.48338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 272, train loss: 0.51092, val loss: 0.48248\n",
      "Main effects training epoch: 273, train loss: 0.51092, val loss: 0.48113\n",
      "Main effects training epoch: 274, train loss: 0.51092, val loss: 0.48519\n",
      "Main effects training epoch: 275, train loss: 0.51114, val loss: 0.48163\n",
      "Main effects training epoch: 276, train loss: 0.51025, val loss: 0.48330\n",
      "Main effects training epoch: 277, train loss: 0.51034, val loss: 0.48166\n",
      "Main effects training epoch: 278, train loss: 0.51029, val loss: 0.48433\n",
      "Main effects training epoch: 279, train loss: 0.50990, val loss: 0.48299\n",
      "Main effects training epoch: 280, train loss: 0.50994, val loss: 0.48332\n",
      "Main effects training epoch: 281, train loss: 0.51047, val loss: 0.48201\n",
      "Main effects training epoch: 282, train loss: 0.51022, val loss: 0.48305\n",
      "Main effects training epoch: 283, train loss: 0.51080, val loss: 0.48348\n",
      "Main effects training epoch: 284, train loss: 0.51061, val loss: 0.48402\n",
      "Main effects training epoch: 285, train loss: 0.51009, val loss: 0.48281\n",
      "Main effects training epoch: 286, train loss: 0.50974, val loss: 0.48300\n",
      "Main effects training epoch: 287, train loss: 0.50950, val loss: 0.48358\n",
      "Main effects training epoch: 288, train loss: 0.50988, val loss: 0.48224\n",
      "Main effects training epoch: 289, train loss: 0.50965, val loss: 0.48318\n",
      "Main effects training epoch: 290, train loss: 0.50971, val loss: 0.48409\n",
      "Main effects training epoch: 291, train loss: 0.50960, val loss: 0.48384\n",
      "Main effects training epoch: 292, train loss: 0.50992, val loss: 0.48237\n",
      "Main effects training epoch: 293, train loss: 0.50985, val loss: 0.48617\n",
      "Main effects training epoch: 294, train loss: 0.50955, val loss: 0.48244\n",
      "Main effects training epoch: 295, train loss: 0.50941, val loss: 0.48347\n",
      "Main effects training epoch: 296, train loss: 0.50948, val loss: 0.48503\n",
      "Main effects training epoch: 297, train loss: 0.50950, val loss: 0.48314\n",
      "Main effects training epoch: 298, train loss: 0.50922, val loss: 0.48358\n",
      "Main effects training epoch: 299, train loss: 0.50898, val loss: 0.48358\n",
      "Main effects training epoch: 300, train loss: 0.50922, val loss: 0.48267\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51731, val loss: 0.48393\n",
      "Main effects tuning epoch: 2, train loss: 0.51713, val loss: 0.48716\n",
      "Main effects tuning epoch: 3, train loss: 0.51655, val loss: 0.48501\n",
      "Main effects tuning epoch: 4, train loss: 0.51648, val loss: 0.48381\n",
      "Main effects tuning epoch: 5, train loss: 0.51655, val loss: 0.48318\n",
      "Main effects tuning epoch: 6, train loss: 0.51640, val loss: 0.48509\n",
      "Main effects tuning epoch: 7, train loss: 0.51642, val loss: 0.48470\n",
      "Main effects tuning epoch: 8, train loss: 0.51644, val loss: 0.48367\n",
      "Main effects tuning epoch: 9, train loss: 0.51633, val loss: 0.48448\n",
      "Main effects tuning epoch: 10, train loss: 0.51623, val loss: 0.48378\n",
      "Main effects tuning epoch: 11, train loss: 0.51661, val loss: 0.48434\n",
      "Main effects tuning epoch: 12, train loss: 0.51632, val loss: 0.48417\n",
      "Main effects tuning epoch: 13, train loss: 0.51613, val loss: 0.48388\n",
      "Main effects tuning epoch: 14, train loss: 0.51643, val loss: 0.48460\n",
      "Main effects tuning epoch: 15, train loss: 0.51625, val loss: 0.48511\n",
      "Main effects tuning epoch: 16, train loss: 0.51606, val loss: 0.48427\n",
      "Main effects tuning epoch: 17, train loss: 0.51649, val loss: 0.48339\n",
      "Main effects tuning epoch: 18, train loss: 0.51629, val loss: 0.48589\n",
      "Main effects tuning epoch: 19, train loss: 0.51639, val loss: 0.48358\n",
      "Main effects tuning epoch: 20, train loss: 0.51644, val loss: 0.48503\n",
      "Main effects tuning epoch: 21, train loss: 0.51612, val loss: 0.48408\n",
      "Main effects tuning epoch: 22, train loss: 0.51614, val loss: 0.48547\n",
      "Main effects tuning epoch: 23, train loss: 0.51603, val loss: 0.48324\n",
      "Main effects tuning epoch: 24, train loss: 0.51604, val loss: 0.48511\n",
      "Main effects tuning epoch: 25, train loss: 0.51627, val loss: 0.48519\n",
      "Main effects tuning epoch: 26, train loss: 0.51594, val loss: 0.48325\n",
      "Main effects tuning epoch: 27, train loss: 0.51597, val loss: 0.48531\n",
      "Main effects tuning epoch: 28, train loss: 0.51576, val loss: 0.48385\n",
      "Main effects tuning epoch: 29, train loss: 0.51605, val loss: 0.48558\n",
      "Main effects tuning epoch: 30, train loss: 0.51602, val loss: 0.48416\n",
      "Main effects tuning epoch: 31, train loss: 0.51582, val loss: 0.48283\n",
      "Main effects tuning epoch: 32, train loss: 0.51590, val loss: 0.48425\n",
      "Main effects tuning epoch: 33, train loss: 0.51587, val loss: 0.48464\n",
      "Main effects tuning epoch: 34, train loss: 0.51564, val loss: 0.48338\n",
      "Main effects tuning epoch: 35, train loss: 0.51562, val loss: 0.48511\n",
      "Main effects tuning epoch: 36, train loss: 0.51562, val loss: 0.48504\n",
      "Main effects tuning epoch: 37, train loss: 0.51604, val loss: 0.48239\n",
      "Main effects tuning epoch: 38, train loss: 0.51573, val loss: 0.48596\n",
      "Main effects tuning epoch: 39, train loss: 0.51540, val loss: 0.48399\n",
      "Main effects tuning epoch: 40, train loss: 0.51553, val loss: 0.48389\n",
      "Main effects tuning epoch: 41, train loss: 0.51544, val loss: 0.48404\n",
      "Main effects tuning epoch: 42, train loss: 0.51604, val loss: 0.48605\n",
      "Main effects tuning epoch: 43, train loss: 0.51588, val loss: 0.48369\n",
      "Main effects tuning epoch: 44, train loss: 0.51598, val loss: 0.48606\n",
      "Main effects tuning epoch: 45, train loss: 0.51554, val loss: 0.48472\n",
      "Main effects tuning epoch: 46, train loss: 0.51542, val loss: 0.48359\n",
      "Main effects tuning epoch: 47, train loss: 0.51543, val loss: 0.48386\n",
      "Main effects tuning epoch: 48, train loss: 0.51544, val loss: 0.48577\n",
      "Main effects tuning epoch: 49, train loss: 0.51562, val loss: 0.48271\n",
      "Main effects tuning epoch: 50, train loss: 0.51550, val loss: 0.48498\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.45842, val loss: 0.43181\n",
      "Interaction training epoch: 2, train loss: 0.33235, val loss: 0.33249\n",
      "Interaction training epoch: 3, train loss: 0.31791, val loss: 0.30849\n",
      "Interaction training epoch: 4, train loss: 0.30777, val loss: 0.28528\n",
      "Interaction training epoch: 5, train loss: 0.29645, val loss: 0.28078\n",
      "Interaction training epoch: 6, train loss: 0.28821, val loss: 0.26771\n",
      "Interaction training epoch: 7, train loss: 0.28867, val loss: 0.27312\n",
      "Interaction training epoch: 8, train loss: 0.28990, val loss: 0.27188\n",
      "Interaction training epoch: 9, train loss: 0.28624, val loss: 0.26719\n",
      "Interaction training epoch: 10, train loss: 0.29710, val loss: 0.28070\n",
      "Interaction training epoch: 11, train loss: 0.29247, val loss: 0.27961\n",
      "Interaction training epoch: 12, train loss: 0.28488, val loss: 0.27849\n",
      "Interaction training epoch: 13, train loss: 0.27892, val loss: 0.26549\n",
      "Interaction training epoch: 14, train loss: 0.28100, val loss: 0.26520\n",
      "Interaction training epoch: 15, train loss: 0.27681, val loss: 0.26611\n",
      "Interaction training epoch: 16, train loss: 0.28420, val loss: 0.27186\n",
      "Interaction training epoch: 17, train loss: 0.27776, val loss: 0.26603\n",
      "Interaction training epoch: 18, train loss: 0.27802, val loss: 0.26805\n",
      "Interaction training epoch: 19, train loss: 0.27613, val loss: 0.26608\n",
      "Interaction training epoch: 20, train loss: 0.28084, val loss: 0.26621\n",
      "Interaction training epoch: 21, train loss: 0.28652, val loss: 0.27126\n",
      "Interaction training epoch: 22, train loss: 0.28123, val loss: 0.27246\n",
      "Interaction training epoch: 23, train loss: 0.27629, val loss: 0.26635\n",
      "Interaction training epoch: 24, train loss: 0.27732, val loss: 0.26740\n",
      "Interaction training epoch: 25, train loss: 0.27371, val loss: 0.26279\n",
      "Interaction training epoch: 26, train loss: 0.27607, val loss: 0.26559\n",
      "Interaction training epoch: 27, train loss: 0.27520, val loss: 0.26485\n",
      "Interaction training epoch: 28, train loss: 0.27642, val loss: 0.26785\n",
      "Interaction training epoch: 29, train loss: 0.27362, val loss: 0.26602\n",
      "Interaction training epoch: 30, train loss: 0.27481, val loss: 0.26375\n",
      "Interaction training epoch: 31, train loss: 0.27410, val loss: 0.26413\n",
      "Interaction training epoch: 32, train loss: 0.27348, val loss: 0.26343\n",
      "Interaction training epoch: 33, train loss: 0.27552, val loss: 0.26341\n",
      "Interaction training epoch: 34, train loss: 0.27516, val loss: 0.26937\n",
      "Interaction training epoch: 35, train loss: 0.27268, val loss: 0.26112\n",
      "Interaction training epoch: 36, train loss: 0.27221, val loss: 0.26288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 37, train loss: 0.27761, val loss: 0.26819\n",
      "Interaction training epoch: 38, train loss: 0.27416, val loss: 0.26728\n",
      "Interaction training epoch: 39, train loss: 0.27100, val loss: 0.25844\n",
      "Interaction training epoch: 40, train loss: 0.27497, val loss: 0.26587\n",
      "Interaction training epoch: 41, train loss: 0.27345, val loss: 0.26544\n",
      "Interaction training epoch: 42, train loss: 0.27016, val loss: 0.26094\n",
      "Interaction training epoch: 43, train loss: 0.27376, val loss: 0.26319\n",
      "Interaction training epoch: 44, train loss: 0.27018, val loss: 0.26463\n",
      "Interaction training epoch: 45, train loss: 0.27187, val loss: 0.26101\n",
      "Interaction training epoch: 46, train loss: 0.27398, val loss: 0.26377\n",
      "Interaction training epoch: 47, train loss: 0.27177, val loss: 0.26239\n",
      "Interaction training epoch: 48, train loss: 0.26840, val loss: 0.25926\n",
      "Interaction training epoch: 49, train loss: 0.27284, val loss: 0.26232\n",
      "Interaction training epoch: 50, train loss: 0.27726, val loss: 0.26826\n",
      "Interaction training epoch: 51, train loss: 0.27318, val loss: 0.26526\n",
      "Interaction training epoch: 52, train loss: 0.26819, val loss: 0.26048\n",
      "Interaction training epoch: 53, train loss: 0.26925, val loss: 0.26199\n",
      "Interaction training epoch: 54, train loss: 0.26804, val loss: 0.25512\n",
      "Interaction training epoch: 55, train loss: 0.27251, val loss: 0.26093\n",
      "Interaction training epoch: 56, train loss: 0.26677, val loss: 0.26116\n",
      "Interaction training epoch: 57, train loss: 0.26901, val loss: 0.25702\n",
      "Interaction training epoch: 58, train loss: 0.27552, val loss: 0.26878\n",
      "Interaction training epoch: 59, train loss: 0.26597, val loss: 0.25861\n",
      "Interaction training epoch: 60, train loss: 0.26586, val loss: 0.25827\n",
      "Interaction training epoch: 61, train loss: 0.27933, val loss: 0.26729\n",
      "Interaction training epoch: 62, train loss: 0.26647, val loss: 0.26104\n",
      "Interaction training epoch: 63, train loss: 0.27589, val loss: 0.26184\n",
      "Interaction training epoch: 64, train loss: 0.27035, val loss: 0.26695\n",
      "Interaction training epoch: 65, train loss: 0.26162, val loss: 0.25526\n",
      "Interaction training epoch: 66, train loss: 0.26986, val loss: 0.26445\n",
      "Interaction training epoch: 67, train loss: 0.26299, val loss: 0.25782\n",
      "Interaction training epoch: 68, train loss: 0.26465, val loss: 0.25374\n",
      "Interaction training epoch: 69, train loss: 0.26610, val loss: 0.25952\n",
      "Interaction training epoch: 70, train loss: 0.26484, val loss: 0.25957\n",
      "Interaction training epoch: 71, train loss: 0.26527, val loss: 0.25516\n",
      "Interaction training epoch: 72, train loss: 0.26375, val loss: 0.25591\n",
      "Interaction training epoch: 73, train loss: 0.26150, val loss: 0.25514\n",
      "Interaction training epoch: 74, train loss: 0.26330, val loss: 0.25588\n",
      "Interaction training epoch: 75, train loss: 0.26942, val loss: 0.26039\n",
      "Interaction training epoch: 76, train loss: 0.25955, val loss: 0.25165\n",
      "Interaction training epoch: 77, train loss: 0.26327, val loss: 0.25282\n",
      "Interaction training epoch: 78, train loss: 0.26891, val loss: 0.26347\n",
      "Interaction training epoch: 79, train loss: 0.25851, val loss: 0.24937\n",
      "Interaction training epoch: 80, train loss: 0.26212, val loss: 0.25206\n",
      "Interaction training epoch: 81, train loss: 0.26500, val loss: 0.25852\n",
      "Interaction training epoch: 82, train loss: 0.25800, val loss: 0.25025\n",
      "Interaction training epoch: 83, train loss: 0.26114, val loss: 0.25189\n",
      "Interaction training epoch: 84, train loss: 0.25959, val loss: 0.25126\n",
      "Interaction training epoch: 85, train loss: 0.25912, val loss: 0.25212\n",
      "Interaction training epoch: 86, train loss: 0.26240, val loss: 0.25261\n",
      "Interaction training epoch: 87, train loss: 0.26048, val loss: 0.25538\n",
      "Interaction training epoch: 88, train loss: 0.25859, val loss: 0.24780\n",
      "Interaction training epoch: 89, train loss: 0.26229, val loss: 0.25395\n",
      "Interaction training epoch: 90, train loss: 0.25885, val loss: 0.24878\n",
      "Interaction training epoch: 91, train loss: 0.25922, val loss: 0.25445\n",
      "Interaction training epoch: 92, train loss: 0.26207, val loss: 0.24968\n",
      "Interaction training epoch: 93, train loss: 0.25931, val loss: 0.25386\n",
      "Interaction training epoch: 94, train loss: 0.25748, val loss: 0.24548\n",
      "Interaction training epoch: 95, train loss: 0.26151, val loss: 0.25343\n",
      "Interaction training epoch: 96, train loss: 0.25547, val loss: 0.24555\n",
      "Interaction training epoch: 97, train loss: 0.25890, val loss: 0.25383\n",
      "Interaction training epoch: 98, train loss: 0.25776, val loss: 0.24966\n",
      "Interaction training epoch: 99, train loss: 0.25446, val loss: 0.24462\n",
      "Interaction training epoch: 100, train loss: 0.25511, val loss: 0.24740\n",
      "Interaction training epoch: 101, train loss: 0.26033, val loss: 0.25379\n",
      "Interaction training epoch: 102, train loss: 0.25444, val loss: 0.24217\n",
      "Interaction training epoch: 103, train loss: 0.25770, val loss: 0.24698\n",
      "Interaction training epoch: 104, train loss: 0.25592, val loss: 0.24984\n",
      "Interaction training epoch: 105, train loss: 0.25545, val loss: 0.24883\n",
      "Interaction training epoch: 106, train loss: 0.25570, val loss: 0.24563\n",
      "Interaction training epoch: 107, train loss: 0.25423, val loss: 0.24388\n",
      "Interaction training epoch: 108, train loss: 0.25422, val loss: 0.24476\n",
      "Interaction training epoch: 109, train loss: 0.25431, val loss: 0.24585\n",
      "Interaction training epoch: 110, train loss: 0.25102, val loss: 0.24306\n",
      "Interaction training epoch: 111, train loss: 0.25882, val loss: 0.25014\n",
      "Interaction training epoch: 112, train loss: 0.25526, val loss: 0.24677\n",
      "Interaction training epoch: 113, train loss: 0.25324, val loss: 0.24504\n",
      "Interaction training epoch: 114, train loss: 0.25558, val loss: 0.24713\n",
      "Interaction training epoch: 115, train loss: 0.25202, val loss: 0.24452\n",
      "Interaction training epoch: 116, train loss: 0.25779, val loss: 0.24427\n",
      "Interaction training epoch: 117, train loss: 0.25181, val loss: 0.24704\n",
      "Interaction training epoch: 118, train loss: 0.25310, val loss: 0.24106\n",
      "Interaction training epoch: 119, train loss: 0.25247, val loss: 0.24494\n",
      "Interaction training epoch: 120, train loss: 0.25658, val loss: 0.25225\n",
      "Interaction training epoch: 121, train loss: 0.25168, val loss: 0.23896\n",
      "Interaction training epoch: 122, train loss: 0.25599, val loss: 0.24683\n",
      "Interaction training epoch: 123, train loss: 0.24899, val loss: 0.24454\n",
      "Interaction training epoch: 124, train loss: 0.25172, val loss: 0.24281\n",
      "Interaction training epoch: 125, train loss: 0.25259, val loss: 0.24278\n",
      "Interaction training epoch: 126, train loss: 0.24862, val loss: 0.24272\n",
      "Interaction training epoch: 127, train loss: 0.25167, val loss: 0.24243\n",
      "Interaction training epoch: 128, train loss: 0.24893, val loss: 0.24217\n",
      "Interaction training epoch: 129, train loss: 0.25352, val loss: 0.24058\n",
      "Interaction training epoch: 130, train loss: 0.24922, val loss: 0.24623\n",
      "Interaction training epoch: 131, train loss: 0.25032, val loss: 0.24269\n",
      "Interaction training epoch: 132, train loss: 0.25192, val loss: 0.24464\n",
      "Interaction training epoch: 133, train loss: 0.24741, val loss: 0.24312\n",
      "Interaction training epoch: 134, train loss: 0.24910, val loss: 0.23799\n",
      "Interaction training epoch: 135, train loss: 0.24780, val loss: 0.24505\n",
      "Interaction training epoch: 136, train loss: 0.24896, val loss: 0.24343\n",
      "Interaction training epoch: 137, train loss: 0.24611, val loss: 0.23949\n",
      "Interaction training epoch: 138, train loss: 0.24982, val loss: 0.24373\n",
      "Interaction training epoch: 139, train loss: 0.24559, val loss: 0.24084\n",
      "Interaction training epoch: 140, train loss: 0.25310, val loss: 0.24724\n",
      "Interaction training epoch: 141, train loss: 0.24509, val loss: 0.24048\n",
      "Interaction training epoch: 142, train loss: 0.24807, val loss: 0.24428\n",
      "Interaction training epoch: 143, train loss: 0.24870, val loss: 0.24756\n",
      "Interaction training epoch: 144, train loss: 0.24754, val loss: 0.23642\n",
      "Interaction training epoch: 145, train loss: 0.25314, val loss: 0.25176\n",
      "Interaction training epoch: 146, train loss: 0.24407, val loss: 0.23908\n",
      "Interaction training epoch: 147, train loss: 0.24774, val loss: 0.24846\n",
      "Interaction training epoch: 148, train loss: 0.24584, val loss: 0.23863\n",
      "Interaction training epoch: 149, train loss: 0.25070, val loss: 0.24951\n",
      "Interaction training epoch: 150, train loss: 0.24767, val loss: 0.24229\n",
      "Interaction training epoch: 151, train loss: 0.24711, val loss: 0.24482\n",
      "Interaction training epoch: 152, train loss: 0.24423, val loss: 0.24300\n",
      "Interaction training epoch: 153, train loss: 0.24726, val loss: 0.24378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 154, train loss: 0.24490, val loss: 0.24592\n",
      "Interaction training epoch: 155, train loss: 0.24434, val loss: 0.24167\n",
      "Interaction training epoch: 156, train loss: 0.24349, val loss: 0.23879\n",
      "Interaction training epoch: 157, train loss: 0.24632, val loss: 0.24751\n",
      "Interaction training epoch: 158, train loss: 0.24262, val loss: 0.24048\n",
      "Interaction training epoch: 159, train loss: 0.24387, val loss: 0.23856\n",
      "Interaction training epoch: 160, train loss: 0.24314, val loss: 0.24172\n",
      "Interaction training epoch: 161, train loss: 0.24540, val loss: 0.24204\n",
      "Interaction training epoch: 162, train loss: 0.24437, val loss: 0.24683\n",
      "Interaction training epoch: 163, train loss: 0.24146, val loss: 0.24018\n",
      "Interaction training epoch: 164, train loss: 0.24655, val loss: 0.24366\n",
      "Interaction training epoch: 165, train loss: 0.24138, val loss: 0.24285\n",
      "Interaction training epoch: 166, train loss: 0.25252, val loss: 0.24911\n",
      "Interaction training epoch: 167, train loss: 0.24153, val loss: 0.24404\n",
      "Interaction training epoch: 168, train loss: 0.24294, val loss: 0.24484\n",
      "Interaction training epoch: 169, train loss: 0.24537, val loss: 0.23835\n",
      "Interaction training epoch: 170, train loss: 0.24261, val loss: 0.24877\n",
      "Interaction training epoch: 171, train loss: 0.24008, val loss: 0.23535\n",
      "Interaction training epoch: 172, train loss: 0.24621, val loss: 0.24906\n",
      "Interaction training epoch: 173, train loss: 0.24042, val loss: 0.23990\n",
      "Interaction training epoch: 174, train loss: 0.24401, val loss: 0.24610\n",
      "Interaction training epoch: 175, train loss: 0.24119, val loss: 0.24549\n",
      "Interaction training epoch: 176, train loss: 0.24043, val loss: 0.24072\n",
      "Interaction training epoch: 177, train loss: 0.23927, val loss: 0.24278\n",
      "Interaction training epoch: 178, train loss: 0.24133, val loss: 0.24262\n",
      "Interaction training epoch: 179, train loss: 0.23940, val loss: 0.24181\n",
      "Interaction training epoch: 180, train loss: 0.24099, val loss: 0.24312\n",
      "Interaction training epoch: 181, train loss: 0.23980, val loss: 0.24053\n",
      "Interaction training epoch: 182, train loss: 0.24029, val loss: 0.24519\n",
      "Interaction training epoch: 183, train loss: 0.24115, val loss: 0.24150\n",
      "Interaction training epoch: 184, train loss: 0.24037, val loss: 0.24620\n",
      "Interaction training epoch: 185, train loss: 0.24170, val loss: 0.24204\n",
      "Interaction training epoch: 186, train loss: 0.23846, val loss: 0.24240\n",
      "Interaction training epoch: 187, train loss: 0.23959, val loss: 0.24174\n",
      "Interaction training epoch: 188, train loss: 0.23937, val loss: 0.24361\n",
      "Interaction training epoch: 189, train loss: 0.23940, val loss: 0.23885\n",
      "Interaction training epoch: 190, train loss: 0.23844, val loss: 0.24053\n",
      "Interaction training epoch: 191, train loss: 0.24053, val loss: 0.24525\n",
      "Interaction training epoch: 192, train loss: 0.23771, val loss: 0.24002\n",
      "Interaction training epoch: 193, train loss: 0.23741, val loss: 0.24056\n",
      "Interaction training epoch: 194, train loss: 0.23761, val loss: 0.24086\n",
      "Interaction training epoch: 195, train loss: 0.23848, val loss: 0.24497\n",
      "Interaction training epoch: 196, train loss: 0.23925, val loss: 0.23959\n",
      "Interaction training epoch: 197, train loss: 0.23767, val loss: 0.24487\n",
      "Interaction training epoch: 198, train loss: 0.23994, val loss: 0.23997\n",
      "Interaction training epoch: 199, train loss: 0.24284, val loss: 0.24718\n",
      "Interaction training epoch: 200, train loss: 0.23545, val loss: 0.23786\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########2 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.24691, val loss: 0.24834\n",
      "Interaction tuning epoch: 2, train loss: 0.24488, val loss: 0.24369\n",
      "Interaction tuning epoch: 3, train loss: 0.24550, val loss: 0.24111\n",
      "Interaction tuning epoch: 4, train loss: 0.24655, val loss: 0.24587\n",
      "Interaction tuning epoch: 5, train loss: 0.24084, val loss: 0.23617\n",
      "Interaction tuning epoch: 6, train loss: 0.24383, val loss: 0.24354\n",
      "Interaction tuning epoch: 7, train loss: 0.24044, val loss: 0.23882\n",
      "Interaction tuning epoch: 8, train loss: 0.24412, val loss: 0.24046\n",
      "Interaction tuning epoch: 9, train loss: 0.24097, val loss: 0.23989\n",
      "Interaction tuning epoch: 10, train loss: 0.24673, val loss: 0.24073\n",
      "Interaction tuning epoch: 11, train loss: 0.24146, val loss: 0.24286\n",
      "Interaction tuning epoch: 12, train loss: 0.24454, val loss: 0.24335\n",
      "Interaction tuning epoch: 13, train loss: 0.24307, val loss: 0.23753\n",
      "Interaction tuning epoch: 14, train loss: 0.24343, val loss: 0.24281\n",
      "Interaction tuning epoch: 15, train loss: 0.24609, val loss: 0.24514\n",
      "Interaction tuning epoch: 16, train loss: 0.24353, val loss: 0.24387\n",
      "Interaction tuning epoch: 17, train loss: 0.24198, val loss: 0.23823\n",
      "Interaction tuning epoch: 18, train loss: 0.24735, val loss: 0.25015\n",
      "Interaction tuning epoch: 19, train loss: 0.24222, val loss: 0.23811\n",
      "Interaction tuning epoch: 20, train loss: 0.24030, val loss: 0.23719\n",
      "Interaction tuning epoch: 21, train loss: 0.24290, val loss: 0.24444\n",
      "Interaction tuning epoch: 22, train loss: 0.24074, val loss: 0.23685\n",
      "Interaction tuning epoch: 23, train loss: 0.24070, val loss: 0.23852\n",
      "Interaction tuning epoch: 24, train loss: 0.24190, val loss: 0.24382\n",
      "Interaction tuning epoch: 25, train loss: 0.24101, val loss: 0.23738\n",
      "Interaction tuning epoch: 26, train loss: 0.24357, val loss: 0.24491\n",
      "Interaction tuning epoch: 27, train loss: 0.24015, val loss: 0.24078\n",
      "Interaction tuning epoch: 28, train loss: 0.23950, val loss: 0.23673\n",
      "Interaction tuning epoch: 29, train loss: 0.23925, val loss: 0.23713\n",
      "Interaction tuning epoch: 30, train loss: 0.24060, val loss: 0.24109\n",
      "Interaction tuning epoch: 31, train loss: 0.24009, val loss: 0.24092\n",
      "Interaction tuning epoch: 32, train loss: 0.24093, val loss: 0.23980\n",
      "Interaction tuning epoch: 33, train loss: 0.24065, val loss: 0.23998\n",
      "Interaction tuning epoch: 34, train loss: 0.24050, val loss: 0.24010\n",
      "Interaction tuning epoch: 35, train loss: 0.24379, val loss: 0.24085\n",
      "Interaction tuning epoch: 36, train loss: 0.23738, val loss: 0.23894\n",
      "Interaction tuning epoch: 37, train loss: 0.24516, val loss: 0.24277\n",
      "Interaction tuning epoch: 38, train loss: 0.23874, val loss: 0.24017\n",
      "Interaction tuning epoch: 39, train loss: 0.23770, val loss: 0.23589\n",
      "Interaction tuning epoch: 40, train loss: 0.24178, val loss: 0.23947\n",
      "Interaction tuning epoch: 41, train loss: 0.23782, val loss: 0.24075\n",
      "Interaction tuning epoch: 42, train loss: 0.23854, val loss: 0.23691\n",
      "Interaction tuning epoch: 43, train loss: 0.23762, val loss: 0.23460\n",
      "Interaction tuning epoch: 44, train loss: 0.24149, val loss: 0.24139\n",
      "Interaction tuning epoch: 45, train loss: 0.23799, val loss: 0.23971\n",
      "Interaction tuning epoch: 46, train loss: 0.23805, val loss: 0.23705\n",
      "Interaction tuning epoch: 47, train loss: 0.23902, val loss: 0.23873\n",
      "Interaction tuning epoch: 48, train loss: 0.23910, val loss: 0.24082\n",
      "Interaction tuning epoch: 49, train loss: 0.23711, val loss: 0.23645\n",
      "Interaction tuning epoch: 50, train loss: 0.24235, val loss: 0.24239\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 40.52944374084473\n",
      "After the gam stage, training error is 0.24235 , validation error is 0.24239\n",
      "missing value counts: 99148\n",
      "[SoftImpute] Max Singular Value of X_init = 3.629384\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.206299 validation BCE=0.238933,rank=5\n",
      "[SoftImpute] Iter 2: observed BCE=0.202738 validation BCE=0.237865,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.200211 validation BCE=0.237088,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.198521 validation BCE=0.236449,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.197183 validation BCE=0.236054,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.196592 validation BCE=0.235531,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.195484 validation BCE=0.234882,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.195528 validation BCE=0.234837,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.195221 validation BCE=0.234580,rank=5\n",
      "[SoftImpute] Iter 10: observed BCE=0.195569 validation BCE=0.234350,rank=5\n",
      "[SoftImpute] Iter 11: observed BCE=0.195572 validation BCE=0.234491,rank=5\n",
      "[SoftImpute] Iter 12: observed BCE=0.195458 validation BCE=0.234302,rank=5\n",
      "[SoftImpute] Iter 13: observed BCE=0.195448 validation BCE=0.234485,rank=5\n",
      "[SoftImpute] Iter 14: observed BCE=0.195633 validation BCE=0.234453,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 15: observed BCE=0.195240 validation BCE=0.234658,rank=5\n",
      "[SoftImpute] Iter 16: observed BCE=0.195177 validation BCE=0.234516,rank=5\n",
      "[SoftImpute] Iter 17: observed BCE=0.195168 validation BCE=0.234626,rank=5\n",
      "[SoftImpute] Iter 18: observed BCE=0.194767 validation BCE=0.234802,rank=5\n",
      "[SoftImpute] Iter 19: observed BCE=0.194954 validation BCE=0.234525,rank=5\n",
      "[SoftImpute] Iter 20: observed BCE=0.194955 validation BCE=0.234947,rank=5\n",
      "[SoftImpute] Iter 21: observed BCE=0.194577 validation BCE=0.234978,rank=5\n",
      "[SoftImpute] Iter 22: observed BCE=0.194705 validation BCE=0.235350,rank=5\n",
      "[SoftImpute] Stopped after iteration 22 for lambda=0.072588\n",
      "final num of user group: 22\n",
      "final num of item group: 29\n",
      "change mode state : True\n",
      "time cost: 7.678015232086182\n",
      "After the matrix factor stage, training error is 0.19470, validation error is 0.23535\n",
      "8\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68435, val loss: 0.68138\n",
      "Main effects training epoch: 2, train loss: 0.67709, val loss: 0.67262\n",
      "Main effects training epoch: 3, train loss: 0.67331, val loss: 0.66804\n",
      "Main effects training epoch: 4, train loss: 0.66781, val loss: 0.66322\n",
      "Main effects training epoch: 5, train loss: 0.65733, val loss: 0.65355\n",
      "Main effects training epoch: 6, train loss: 0.63200, val loss: 0.63169\n",
      "Main effects training epoch: 7, train loss: 0.58258, val loss: 0.59373\n",
      "Main effects training epoch: 8, train loss: 0.53599, val loss: 0.56840\n",
      "Main effects training epoch: 9, train loss: 0.51958, val loss: 0.57413\n",
      "Main effects training epoch: 10, train loss: 0.52130, val loss: 0.57879\n",
      "Main effects training epoch: 11, train loss: 0.52130, val loss: 0.58132\n",
      "Main effects training epoch: 12, train loss: 0.51818, val loss: 0.56438\n",
      "Main effects training epoch: 13, train loss: 0.51719, val loss: 0.56721\n",
      "Main effects training epoch: 14, train loss: 0.51454, val loss: 0.56128\n",
      "Main effects training epoch: 15, train loss: 0.51342, val loss: 0.56485\n",
      "Main effects training epoch: 16, train loss: 0.51346, val loss: 0.56435\n",
      "Main effects training epoch: 17, train loss: 0.51331, val loss: 0.56741\n",
      "Main effects training epoch: 18, train loss: 0.51316, val loss: 0.56499\n",
      "Main effects training epoch: 19, train loss: 0.51321, val loss: 0.56560\n",
      "Main effects training epoch: 20, train loss: 0.51353, val loss: 0.56509\n",
      "Main effects training epoch: 21, train loss: 0.51367, val loss: 0.56357\n",
      "Main effects training epoch: 22, train loss: 0.51410, val loss: 0.56542\n",
      "Main effects training epoch: 23, train loss: 0.51297, val loss: 0.56546\n",
      "Main effects training epoch: 24, train loss: 0.51375, val loss: 0.56521\n",
      "Main effects training epoch: 25, train loss: 0.51272, val loss: 0.56517\n",
      "Main effects training epoch: 26, train loss: 0.51273, val loss: 0.56505\n",
      "Main effects training epoch: 27, train loss: 0.51249, val loss: 0.56339\n",
      "Main effects training epoch: 28, train loss: 0.51244, val loss: 0.56375\n",
      "Main effects training epoch: 29, train loss: 0.51245, val loss: 0.56575\n",
      "Main effects training epoch: 30, train loss: 0.51254, val loss: 0.56434\n",
      "Main effects training epoch: 31, train loss: 0.51256, val loss: 0.56277\n",
      "Main effects training epoch: 32, train loss: 0.51254, val loss: 0.56407\n",
      "Main effects training epoch: 33, train loss: 0.51267, val loss: 0.56407\n",
      "Main effects training epoch: 34, train loss: 0.51237, val loss: 0.56394\n",
      "Main effects training epoch: 35, train loss: 0.51254, val loss: 0.56488\n",
      "Main effects training epoch: 36, train loss: 0.51294, val loss: 0.56551\n",
      "Main effects training epoch: 37, train loss: 0.51224, val loss: 0.56351\n",
      "Main effects training epoch: 38, train loss: 0.51234, val loss: 0.56476\n",
      "Main effects training epoch: 39, train loss: 0.51218, val loss: 0.56369\n",
      "Main effects training epoch: 40, train loss: 0.51220, val loss: 0.56369\n",
      "Main effects training epoch: 41, train loss: 0.51286, val loss: 0.56385\n",
      "Main effects training epoch: 42, train loss: 0.51240, val loss: 0.56330\n",
      "Main effects training epoch: 43, train loss: 0.51247, val loss: 0.56324\n",
      "Main effects training epoch: 44, train loss: 0.51345, val loss: 0.56604\n",
      "Main effects training epoch: 45, train loss: 0.51343, val loss: 0.56573\n",
      "Main effects training epoch: 46, train loss: 0.51258, val loss: 0.56283\n",
      "Main effects training epoch: 47, train loss: 0.51203, val loss: 0.56325\n",
      "Main effects training epoch: 48, train loss: 0.51198, val loss: 0.56387\n",
      "Main effects training epoch: 49, train loss: 0.51246, val loss: 0.56564\n",
      "Main effects training epoch: 50, train loss: 0.51188, val loss: 0.56476\n",
      "Main effects training epoch: 51, train loss: 0.51194, val loss: 0.56303\n",
      "Main effects training epoch: 52, train loss: 0.51226, val loss: 0.56319\n",
      "Main effects training epoch: 53, train loss: 0.51254, val loss: 0.56431\n",
      "Main effects training epoch: 54, train loss: 0.51207, val loss: 0.56466\n",
      "Main effects training epoch: 55, train loss: 0.51277, val loss: 0.56352\n",
      "Main effects training epoch: 56, train loss: 0.51201, val loss: 0.56457\n",
      "Main effects training epoch: 57, train loss: 0.51184, val loss: 0.56370\n",
      "Main effects training epoch: 58, train loss: 0.51224, val loss: 0.56344\n",
      "Main effects training epoch: 59, train loss: 0.51240, val loss: 0.56464\n",
      "Main effects training epoch: 60, train loss: 0.51203, val loss: 0.56256\n",
      "Main effects training epoch: 61, train loss: 0.51182, val loss: 0.56515\n",
      "Main effects training epoch: 62, train loss: 0.51155, val loss: 0.56366\n",
      "Main effects training epoch: 63, train loss: 0.51203, val loss: 0.56450\n",
      "Main effects training epoch: 64, train loss: 0.51225, val loss: 0.56411\n",
      "Main effects training epoch: 65, train loss: 0.51203, val loss: 0.56299\n",
      "Main effects training epoch: 66, train loss: 0.51163, val loss: 0.56405\n",
      "Main effects training epoch: 67, train loss: 0.51254, val loss: 0.56403\n",
      "Main effects training epoch: 68, train loss: 0.51260, val loss: 0.56341\n",
      "Main effects training epoch: 69, train loss: 0.51183, val loss: 0.56390\n",
      "Main effects training epoch: 70, train loss: 0.51222, val loss: 0.56237\n",
      "Main effects training epoch: 71, train loss: 0.51163, val loss: 0.56319\n",
      "Main effects training epoch: 72, train loss: 0.51171, val loss: 0.56407\n",
      "Main effects training epoch: 73, train loss: 0.51209, val loss: 0.56292\n",
      "Main effects training epoch: 74, train loss: 0.51165, val loss: 0.56282\n",
      "Main effects training epoch: 75, train loss: 0.51189, val loss: 0.56520\n",
      "Main effects training epoch: 76, train loss: 0.51219, val loss: 0.56393\n",
      "Main effects training epoch: 77, train loss: 0.51229, val loss: 0.56254\n",
      "Main effects training epoch: 78, train loss: 0.51204, val loss: 0.56473\n",
      "Main effects training epoch: 79, train loss: 0.51124, val loss: 0.56290\n",
      "Main effects training epoch: 80, train loss: 0.51212, val loss: 0.56417\n",
      "Main effects training epoch: 81, train loss: 0.51240, val loss: 0.56501\n",
      "Main effects training epoch: 82, train loss: 0.51122, val loss: 0.56256\n",
      "Main effects training epoch: 83, train loss: 0.51090, val loss: 0.56240\n",
      "Main effects training epoch: 84, train loss: 0.51102, val loss: 0.56264\n",
      "Main effects training epoch: 85, train loss: 0.51101, val loss: 0.56427\n",
      "Main effects training epoch: 86, train loss: 0.51097, val loss: 0.56266\n",
      "Main effects training epoch: 87, train loss: 0.51124, val loss: 0.56339\n",
      "Main effects training epoch: 88, train loss: 0.51034, val loss: 0.56147\n",
      "Main effects training epoch: 89, train loss: 0.51049, val loss: 0.56358\n",
      "Main effects training epoch: 90, train loss: 0.51060, val loss: 0.56268\n",
      "Main effects training epoch: 91, train loss: 0.51003, val loss: 0.56219\n",
      "Main effects training epoch: 92, train loss: 0.51084, val loss: 0.56304\n",
      "Main effects training epoch: 93, train loss: 0.51056, val loss: 0.56296\n",
      "Main effects training epoch: 94, train loss: 0.50989, val loss: 0.56083\n",
      "Main effects training epoch: 95, train loss: 0.51004, val loss: 0.56218\n",
      "Main effects training epoch: 96, train loss: 0.50927, val loss: 0.56047\n",
      "Main effects training epoch: 97, train loss: 0.50939, val loss: 0.56207\n",
      "Main effects training epoch: 98, train loss: 0.50944, val loss: 0.56047\n",
      "Main effects training epoch: 99, train loss: 0.50924, val loss: 0.56055\n",
      "Main effects training epoch: 100, train loss: 0.50937, val loss: 0.56135\n",
      "Main effects training epoch: 101, train loss: 0.50959, val loss: 0.55969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 102, train loss: 0.51126, val loss: 0.56437\n",
      "Main effects training epoch: 103, train loss: 0.51039, val loss: 0.55845\n",
      "Main effects training epoch: 104, train loss: 0.50933, val loss: 0.56228\n",
      "Main effects training epoch: 105, train loss: 0.50890, val loss: 0.55918\n",
      "Main effects training epoch: 106, train loss: 0.50876, val loss: 0.56027\n",
      "Main effects training epoch: 107, train loss: 0.50880, val loss: 0.56188\n",
      "Main effects training epoch: 108, train loss: 0.50894, val loss: 0.55940\n",
      "Main effects training epoch: 109, train loss: 0.50985, val loss: 0.56239\n",
      "Main effects training epoch: 110, train loss: 0.50904, val loss: 0.55912\n",
      "Main effects training epoch: 111, train loss: 0.50886, val loss: 0.56105\n",
      "Main effects training epoch: 112, train loss: 0.51034, val loss: 0.56234\n",
      "Main effects training epoch: 113, train loss: 0.51024, val loss: 0.56188\n",
      "Main effects training epoch: 114, train loss: 0.50949, val loss: 0.56023\n",
      "Main effects training epoch: 115, train loss: 0.50942, val loss: 0.56243\n",
      "Main effects training epoch: 116, train loss: 0.50877, val loss: 0.55910\n",
      "Main effects training epoch: 117, train loss: 0.50886, val loss: 0.56076\n",
      "Main effects training epoch: 118, train loss: 0.50880, val loss: 0.56109\n",
      "Main effects training epoch: 119, train loss: 0.50898, val loss: 0.55983\n",
      "Main effects training epoch: 120, train loss: 0.50891, val loss: 0.55871\n",
      "Main effects training epoch: 121, train loss: 0.50863, val loss: 0.56016\n",
      "Main effects training epoch: 122, train loss: 0.50861, val loss: 0.56152\n",
      "Main effects training epoch: 123, train loss: 0.50843, val loss: 0.55874\n",
      "Main effects training epoch: 124, train loss: 0.50932, val loss: 0.56208\n",
      "Main effects training epoch: 125, train loss: 0.50939, val loss: 0.55873\n",
      "Main effects training epoch: 126, train loss: 0.50856, val loss: 0.55988\n",
      "Main effects training epoch: 127, train loss: 0.50851, val loss: 0.55996\n",
      "Main effects training epoch: 128, train loss: 0.50838, val loss: 0.56079\n",
      "Main effects training epoch: 129, train loss: 0.50858, val loss: 0.55895\n",
      "Main effects training epoch: 130, train loss: 0.50829, val loss: 0.55993\n",
      "Main effects training epoch: 131, train loss: 0.50842, val loss: 0.55901\n",
      "Main effects training epoch: 132, train loss: 0.50831, val loss: 0.56014\n",
      "Main effects training epoch: 133, train loss: 0.50859, val loss: 0.56058\n",
      "Main effects training epoch: 134, train loss: 0.50837, val loss: 0.55901\n",
      "Main effects training epoch: 135, train loss: 0.50912, val loss: 0.56080\n",
      "Main effects training epoch: 136, train loss: 0.50895, val loss: 0.55935\n",
      "Main effects training epoch: 137, train loss: 0.50825, val loss: 0.55933\n",
      "Main effects training epoch: 138, train loss: 0.50822, val loss: 0.55956\n",
      "Main effects training epoch: 139, train loss: 0.51002, val loss: 0.56171\n",
      "Main effects training epoch: 140, train loss: 0.50915, val loss: 0.55944\n",
      "Main effects training epoch: 141, train loss: 0.50844, val loss: 0.56063\n",
      "Main effects training epoch: 142, train loss: 0.50848, val loss: 0.55895\n",
      "Main effects training epoch: 143, train loss: 0.50962, val loss: 0.56214\n",
      "Main effects training epoch: 144, train loss: 0.50966, val loss: 0.56065\n",
      "Main effects training epoch: 145, train loss: 0.50882, val loss: 0.56000\n",
      "Main effects training epoch: 146, train loss: 0.50775, val loss: 0.55882\n",
      "Main effects training epoch: 147, train loss: 0.50775, val loss: 0.55979\n",
      "Main effects training epoch: 148, train loss: 0.50825, val loss: 0.56016\n",
      "Main effects training epoch: 149, train loss: 0.50856, val loss: 0.56106\n",
      "Main effects training epoch: 150, train loss: 0.50869, val loss: 0.55808\n",
      "Main effects training epoch: 151, train loss: 0.50804, val loss: 0.55884\n",
      "Main effects training epoch: 152, train loss: 0.50793, val loss: 0.55854\n",
      "Main effects training epoch: 153, train loss: 0.50775, val loss: 0.55970\n",
      "Main effects training epoch: 154, train loss: 0.50825, val loss: 0.56004\n",
      "Main effects training epoch: 155, train loss: 0.50785, val loss: 0.55783\n",
      "Main effects training epoch: 156, train loss: 0.50782, val loss: 0.56115\n",
      "Main effects training epoch: 157, train loss: 0.50739, val loss: 0.55789\n",
      "Main effects training epoch: 158, train loss: 0.50729, val loss: 0.55861\n",
      "Main effects training epoch: 159, train loss: 0.50742, val loss: 0.55944\n",
      "Main effects training epoch: 160, train loss: 0.50767, val loss: 0.55874\n",
      "Main effects training epoch: 161, train loss: 0.50745, val loss: 0.55814\n",
      "Main effects training epoch: 162, train loss: 0.50795, val loss: 0.55810\n",
      "Main effects training epoch: 163, train loss: 0.50755, val loss: 0.55931\n",
      "Main effects training epoch: 164, train loss: 0.50794, val loss: 0.55765\n",
      "Main effects training epoch: 165, train loss: 0.50762, val loss: 0.56005\n",
      "Main effects training epoch: 166, train loss: 0.50825, val loss: 0.56036\n",
      "Main effects training epoch: 167, train loss: 0.50783, val loss: 0.55690\n",
      "Main effects training epoch: 168, train loss: 0.50807, val loss: 0.55836\n",
      "Main effects training epoch: 169, train loss: 0.50755, val loss: 0.56011\n",
      "Main effects training epoch: 170, train loss: 0.50734, val loss: 0.55737\n",
      "Main effects training epoch: 171, train loss: 0.50831, val loss: 0.55789\n",
      "Main effects training epoch: 172, train loss: 0.50725, val loss: 0.55749\n",
      "Main effects training epoch: 173, train loss: 0.50713, val loss: 0.55783\n",
      "Main effects training epoch: 174, train loss: 0.50737, val loss: 0.55881\n",
      "Main effects training epoch: 175, train loss: 0.50750, val loss: 0.55812\n",
      "Main effects training epoch: 176, train loss: 0.50716, val loss: 0.55778\n",
      "Main effects training epoch: 177, train loss: 0.50673, val loss: 0.55758\n",
      "Main effects training epoch: 178, train loss: 0.50696, val loss: 0.55786\n",
      "Main effects training epoch: 179, train loss: 0.50695, val loss: 0.55792\n",
      "Main effects training epoch: 180, train loss: 0.50661, val loss: 0.55725\n",
      "Main effects training epoch: 181, train loss: 0.50720, val loss: 0.55778\n",
      "Main effects training epoch: 182, train loss: 0.50694, val loss: 0.55812\n",
      "Main effects training epoch: 183, train loss: 0.50655, val loss: 0.55692\n",
      "Main effects training epoch: 184, train loss: 0.50656, val loss: 0.55744\n",
      "Main effects training epoch: 185, train loss: 0.50647, val loss: 0.55612\n",
      "Main effects training epoch: 186, train loss: 0.50660, val loss: 0.55816\n",
      "Main effects training epoch: 187, train loss: 0.50670, val loss: 0.55651\n",
      "Main effects training epoch: 188, train loss: 0.50662, val loss: 0.55719\n",
      "Main effects training epoch: 189, train loss: 0.50653, val loss: 0.55841\n",
      "Main effects training epoch: 190, train loss: 0.50653, val loss: 0.55624\n",
      "Main effects training epoch: 191, train loss: 0.50679, val loss: 0.55661\n",
      "Main effects training epoch: 192, train loss: 0.50689, val loss: 0.55895\n",
      "Main effects training epoch: 193, train loss: 0.50681, val loss: 0.55607\n",
      "Main effects training epoch: 194, train loss: 0.50609, val loss: 0.55642\n",
      "Main effects training epoch: 195, train loss: 0.50626, val loss: 0.55875\n",
      "Main effects training epoch: 196, train loss: 0.50609, val loss: 0.55621\n",
      "Main effects training epoch: 197, train loss: 0.50618, val loss: 0.55673\n",
      "Main effects training epoch: 198, train loss: 0.50607, val loss: 0.55497\n",
      "Main effects training epoch: 199, train loss: 0.50600, val loss: 0.55658\n",
      "Main effects training epoch: 200, train loss: 0.50622, val loss: 0.55742\n",
      "Main effects training epoch: 201, train loss: 0.50621, val loss: 0.55684\n",
      "Main effects training epoch: 202, train loss: 0.50585, val loss: 0.55702\n",
      "Main effects training epoch: 203, train loss: 0.50604, val loss: 0.55681\n",
      "Main effects training epoch: 204, train loss: 0.50591, val loss: 0.55712\n",
      "Main effects training epoch: 205, train loss: 0.50609, val loss: 0.55661\n",
      "Main effects training epoch: 206, train loss: 0.50585, val loss: 0.55633\n",
      "Main effects training epoch: 207, train loss: 0.50604, val loss: 0.55635\n",
      "Main effects training epoch: 208, train loss: 0.50645, val loss: 0.55622\n",
      "Main effects training epoch: 209, train loss: 0.50627, val loss: 0.55767\n",
      "Main effects training epoch: 210, train loss: 0.50615, val loss: 0.55692\n",
      "Main effects training epoch: 211, train loss: 0.50688, val loss: 0.55743\n",
      "Main effects training epoch: 212, train loss: 0.50653, val loss: 0.55678\n",
      "Main effects training epoch: 213, train loss: 0.50660, val loss: 0.55690\n",
      "Main effects training epoch: 214, train loss: 0.50570, val loss: 0.55532\n",
      "Main effects training epoch: 215, train loss: 0.50562, val loss: 0.55720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 216, train loss: 0.50555, val loss: 0.55601\n",
      "Main effects training epoch: 217, train loss: 0.50604, val loss: 0.55599\n",
      "Main effects training epoch: 218, train loss: 0.50599, val loss: 0.55567\n",
      "Main effects training epoch: 219, train loss: 0.50600, val loss: 0.55786\n",
      "Main effects training epoch: 220, train loss: 0.50657, val loss: 0.55625\n",
      "Main effects training epoch: 221, train loss: 0.50615, val loss: 0.55695\n",
      "Main effects training epoch: 222, train loss: 0.50587, val loss: 0.55512\n",
      "Main effects training epoch: 223, train loss: 0.50625, val loss: 0.55768\n",
      "Main effects training epoch: 224, train loss: 0.50614, val loss: 0.55513\n",
      "Main effects training epoch: 225, train loss: 0.50558, val loss: 0.55722\n",
      "Main effects training epoch: 226, train loss: 0.50529, val loss: 0.55603\n",
      "Main effects training epoch: 227, train loss: 0.50541, val loss: 0.55535\n",
      "Main effects training epoch: 228, train loss: 0.50556, val loss: 0.55627\n",
      "Main effects training epoch: 229, train loss: 0.50609, val loss: 0.55559\n",
      "Main effects training epoch: 230, train loss: 0.50539, val loss: 0.55698\n",
      "Main effects training epoch: 231, train loss: 0.50526, val loss: 0.55550\n",
      "Main effects training epoch: 232, train loss: 0.50508, val loss: 0.55573\n",
      "Main effects training epoch: 233, train loss: 0.50505, val loss: 0.55569\n",
      "Main effects training epoch: 234, train loss: 0.50525, val loss: 0.55457\n",
      "Main effects training epoch: 235, train loss: 0.50512, val loss: 0.55571\n",
      "Main effects training epoch: 236, train loss: 0.50514, val loss: 0.55609\n",
      "Main effects training epoch: 237, train loss: 0.50513, val loss: 0.55554\n",
      "Main effects training epoch: 238, train loss: 0.50499, val loss: 0.55653\n",
      "Main effects training epoch: 239, train loss: 0.50504, val loss: 0.55337\n",
      "Main effects training epoch: 240, train loss: 0.50514, val loss: 0.55535\n",
      "Main effects training epoch: 241, train loss: 0.50529, val loss: 0.55467\n",
      "Main effects training epoch: 242, train loss: 0.50493, val loss: 0.55640\n",
      "Main effects training epoch: 243, train loss: 0.50483, val loss: 0.55502\n",
      "Main effects training epoch: 244, train loss: 0.50503, val loss: 0.55571\n",
      "Main effects training epoch: 245, train loss: 0.50531, val loss: 0.55376\n",
      "Main effects training epoch: 246, train loss: 0.50498, val loss: 0.55623\n",
      "Main effects training epoch: 247, train loss: 0.50500, val loss: 0.55547\n",
      "Main effects training epoch: 248, train loss: 0.50465, val loss: 0.55429\n",
      "Main effects training epoch: 249, train loss: 0.50472, val loss: 0.55445\n",
      "Main effects training epoch: 250, train loss: 0.50460, val loss: 0.55515\n",
      "Main effects training epoch: 251, train loss: 0.50462, val loss: 0.55659\n",
      "Main effects training epoch: 252, train loss: 0.50508, val loss: 0.55535\n",
      "Main effects training epoch: 253, train loss: 0.50479, val loss: 0.55571\n",
      "Main effects training epoch: 254, train loss: 0.50462, val loss: 0.55376\n",
      "Main effects training epoch: 255, train loss: 0.50456, val loss: 0.55395\n",
      "Main effects training epoch: 256, train loss: 0.50491, val loss: 0.55709\n",
      "Main effects training epoch: 257, train loss: 0.50460, val loss: 0.55450\n",
      "Main effects training epoch: 258, train loss: 0.50447, val loss: 0.55564\n",
      "Main effects training epoch: 259, train loss: 0.50434, val loss: 0.55429\n",
      "Main effects training epoch: 260, train loss: 0.50443, val loss: 0.55553\n",
      "Main effects training epoch: 261, train loss: 0.50462, val loss: 0.55241\n",
      "Main effects training epoch: 262, train loss: 0.50404, val loss: 0.55524\n",
      "Main effects training epoch: 263, train loss: 0.50413, val loss: 0.55487\n",
      "Main effects training epoch: 264, train loss: 0.50414, val loss: 0.55589\n",
      "Main effects training epoch: 265, train loss: 0.50425, val loss: 0.55508\n",
      "Main effects training epoch: 266, train loss: 0.50426, val loss: 0.55307\n",
      "Main effects training epoch: 267, train loss: 0.50410, val loss: 0.55467\n",
      "Main effects training epoch: 268, train loss: 0.50408, val loss: 0.55375\n",
      "Main effects training epoch: 269, train loss: 0.50383, val loss: 0.55427\n",
      "Main effects training epoch: 270, train loss: 0.50385, val loss: 0.55483\n",
      "Main effects training epoch: 271, train loss: 0.50390, val loss: 0.55425\n",
      "Main effects training epoch: 272, train loss: 0.50422, val loss: 0.55299\n",
      "Main effects training epoch: 273, train loss: 0.50443, val loss: 0.55524\n",
      "Main effects training epoch: 274, train loss: 0.50391, val loss: 0.55352\n",
      "Main effects training epoch: 275, train loss: 0.50363, val loss: 0.55456\n",
      "Main effects training epoch: 276, train loss: 0.50365, val loss: 0.55387\n",
      "Main effects training epoch: 277, train loss: 0.50392, val loss: 0.55502\n",
      "Main effects training epoch: 278, train loss: 0.50362, val loss: 0.55598\n",
      "Main effects training epoch: 279, train loss: 0.50352, val loss: 0.55209\n",
      "Main effects training epoch: 280, train loss: 0.50393, val loss: 0.55421\n",
      "Main effects training epoch: 281, train loss: 0.50400, val loss: 0.55529\n",
      "Main effects training epoch: 282, train loss: 0.50359, val loss: 0.55491\n",
      "Main effects training epoch: 283, train loss: 0.50338, val loss: 0.55435\n",
      "Main effects training epoch: 284, train loss: 0.50351, val loss: 0.55471\n",
      "Main effects training epoch: 285, train loss: 0.50360, val loss: 0.55174\n",
      "Main effects training epoch: 286, train loss: 0.50324, val loss: 0.55474\n",
      "Main effects training epoch: 287, train loss: 0.50321, val loss: 0.55450\n",
      "Main effects training epoch: 288, train loss: 0.50308, val loss: 0.55325\n",
      "Main effects training epoch: 289, train loss: 0.50342, val loss: 0.55534\n",
      "Main effects training epoch: 290, train loss: 0.50332, val loss: 0.55328\n",
      "Main effects training epoch: 291, train loss: 0.50348, val loss: 0.55314\n",
      "Main effects training epoch: 292, train loss: 0.50366, val loss: 0.55582\n",
      "Main effects training epoch: 293, train loss: 0.50303, val loss: 0.55298\n",
      "Main effects training epoch: 294, train loss: 0.50357, val loss: 0.55530\n",
      "Main effects training epoch: 295, train loss: 0.50362, val loss: 0.55374\n",
      "Main effects training epoch: 296, train loss: 0.50330, val loss: 0.55509\n",
      "Main effects training epoch: 297, train loss: 0.50280, val loss: 0.55155\n",
      "Main effects training epoch: 298, train loss: 0.50283, val loss: 0.55351\n",
      "Main effects training epoch: 299, train loss: 0.50297, val loss: 0.55200\n",
      "Main effects training epoch: 300, train loss: 0.50319, val loss: 0.55415\n",
      "##########Stage 1: main effect training stop.##########\n",
      "6 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.50374, val loss: 0.55460\n",
      "Main effects tuning epoch: 2, train loss: 0.50366, val loss: 0.55350\n",
      "Main effects tuning epoch: 3, train loss: 0.50405, val loss: 0.55689\n",
      "Main effects tuning epoch: 4, train loss: 0.50376, val loss: 0.55374\n",
      "Main effects tuning epoch: 5, train loss: 0.50349, val loss: 0.55343\n",
      "Main effects tuning epoch: 6, train loss: 0.50356, val loss: 0.55485\n",
      "Main effects tuning epoch: 7, train loss: 0.50363, val loss: 0.55399\n",
      "Main effects tuning epoch: 8, train loss: 0.50355, val loss: 0.55403\n",
      "Main effects tuning epoch: 9, train loss: 0.50334, val loss: 0.55444\n",
      "Main effects tuning epoch: 10, train loss: 0.50355, val loss: 0.55400\n",
      "Main effects tuning epoch: 11, train loss: 0.50350, val loss: 0.55574\n",
      "Main effects tuning epoch: 12, train loss: 0.50342, val loss: 0.55301\n",
      "Main effects tuning epoch: 13, train loss: 0.50328, val loss: 0.55326\n",
      "Main effects tuning epoch: 14, train loss: 0.50376, val loss: 0.55350\n",
      "Main effects tuning epoch: 15, train loss: 0.50345, val loss: 0.55465\n",
      "Main effects tuning epoch: 16, train loss: 0.50380, val loss: 0.55426\n",
      "Main effects tuning epoch: 17, train loss: 0.50367, val loss: 0.55535\n",
      "Main effects tuning epoch: 18, train loss: 0.50362, val loss: 0.55425\n",
      "Main effects tuning epoch: 19, train loss: 0.50343, val loss: 0.55310\n",
      "Main effects tuning epoch: 20, train loss: 0.50313, val loss: 0.55599\n",
      "Main effects tuning epoch: 21, train loss: 0.50312, val loss: 0.55260\n",
      "Main effects tuning epoch: 22, train loss: 0.50329, val loss: 0.55298\n",
      "Main effects tuning epoch: 23, train loss: 0.50312, val loss: 0.55482\n",
      "Main effects tuning epoch: 24, train loss: 0.50326, val loss: 0.55374\n",
      "Main effects tuning epoch: 25, train loss: 0.50319, val loss: 0.55336\n",
      "Main effects tuning epoch: 26, train loss: 0.50318, val loss: 0.55537\n",
      "Main effects tuning epoch: 27, train loss: 0.50345, val loss: 0.55223\n",
      "Main effects tuning epoch: 28, train loss: 0.50356, val loss: 0.55643\n",
      "Main effects tuning epoch: 29, train loss: 0.50298, val loss: 0.55447\n",
      "Main effects tuning epoch: 30, train loss: 0.50297, val loss: 0.55325\n",
      "Main effects tuning epoch: 31, train loss: 0.50301, val loss: 0.55432\n",
      "Main effects tuning epoch: 32, train loss: 0.50322, val loss: 0.55338\n",
      "Main effects tuning epoch: 33, train loss: 0.50310, val loss: 0.55250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects tuning epoch: 34, train loss: 0.50290, val loss: 0.55539\n",
      "Main effects tuning epoch: 35, train loss: 0.50269, val loss: 0.55343\n",
      "Main effects tuning epoch: 36, train loss: 0.50283, val loss: 0.55249\n",
      "Main effects tuning epoch: 37, train loss: 0.50272, val loss: 0.55353\n",
      "Main effects tuning epoch: 38, train loss: 0.50288, val loss: 0.55286\n",
      "Main effects tuning epoch: 39, train loss: 0.50332, val loss: 0.55418\n",
      "Main effects tuning epoch: 40, train loss: 0.50281, val loss: 0.55400\n",
      "Main effects tuning epoch: 41, train loss: 0.50268, val loss: 0.55360\n",
      "Main effects tuning epoch: 42, train loss: 0.50277, val loss: 0.55389\n",
      "Main effects tuning epoch: 43, train loss: 0.50265, val loss: 0.55352\n",
      "Main effects tuning epoch: 44, train loss: 0.50266, val loss: 0.55295\n",
      "Main effects tuning epoch: 45, train loss: 0.50289, val loss: 0.55410\n",
      "Main effects tuning epoch: 46, train loss: 0.50256, val loss: 0.55215\n",
      "Main effects tuning epoch: 47, train loss: 0.50269, val loss: 0.55325\n",
      "Main effects tuning epoch: 48, train loss: 0.50336, val loss: 0.55636\n",
      "Main effects tuning epoch: 49, train loss: 0.50266, val loss: 0.55105\n",
      "Main effects tuning epoch: 50, train loss: 0.50272, val loss: 0.55423\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.45761, val loss: 0.49294\n",
      "Interaction training epoch: 2, train loss: 0.34727, val loss: 0.35556\n",
      "Interaction training epoch: 3, train loss: 0.30681, val loss: 0.32380\n",
      "Interaction training epoch: 4, train loss: 0.31084, val loss: 0.31978\n",
      "Interaction training epoch: 5, train loss: 0.29506, val loss: 0.31046\n",
      "Interaction training epoch: 6, train loss: 0.28978, val loss: 0.30362\n",
      "Interaction training epoch: 7, train loss: 0.28417, val loss: 0.30303\n",
      "Interaction training epoch: 8, train loss: 0.28658, val loss: 0.30220\n",
      "Interaction training epoch: 9, train loss: 0.28409, val loss: 0.30018\n",
      "Interaction training epoch: 10, train loss: 0.30195, val loss: 0.32200\n",
      "Interaction training epoch: 11, train loss: 0.28283, val loss: 0.29994\n",
      "Interaction training epoch: 12, train loss: 0.27840, val loss: 0.30055\n",
      "Interaction training epoch: 13, train loss: 0.28178, val loss: 0.30049\n",
      "Interaction training epoch: 14, train loss: 0.27772, val loss: 0.29858\n",
      "Interaction training epoch: 15, train loss: 0.27466, val loss: 0.29541\n",
      "Interaction training epoch: 16, train loss: 0.28477, val loss: 0.30856\n",
      "Interaction training epoch: 17, train loss: 0.27550, val loss: 0.29772\n",
      "Interaction training epoch: 18, train loss: 0.27314, val loss: 0.29767\n",
      "Interaction training epoch: 19, train loss: 0.27338, val loss: 0.29612\n",
      "Interaction training epoch: 20, train loss: 0.27086, val loss: 0.29847\n",
      "Interaction training epoch: 21, train loss: 0.28216, val loss: 0.30366\n",
      "Interaction training epoch: 22, train loss: 0.27189, val loss: 0.30154\n",
      "Interaction training epoch: 23, train loss: 0.26778, val loss: 0.29424\n",
      "Interaction training epoch: 24, train loss: 0.26884, val loss: 0.29702\n",
      "Interaction training epoch: 25, train loss: 0.27040, val loss: 0.29804\n",
      "Interaction training epoch: 26, train loss: 0.27527, val loss: 0.30011\n",
      "Interaction training epoch: 27, train loss: 0.26772, val loss: 0.29784\n",
      "Interaction training epoch: 28, train loss: 0.27047, val loss: 0.29814\n",
      "Interaction training epoch: 29, train loss: 0.26588, val loss: 0.29554\n",
      "Interaction training epoch: 30, train loss: 0.26842, val loss: 0.29953\n",
      "Interaction training epoch: 31, train loss: 0.26794, val loss: 0.29964\n",
      "Interaction training epoch: 32, train loss: 0.27270, val loss: 0.30184\n",
      "Interaction training epoch: 33, train loss: 0.26872, val loss: 0.30064\n",
      "Interaction training epoch: 34, train loss: 0.27100, val loss: 0.30327\n",
      "Interaction training epoch: 35, train loss: 0.27116, val loss: 0.30204\n",
      "Interaction training epoch: 36, train loss: 0.26665, val loss: 0.29774\n",
      "Interaction training epoch: 37, train loss: 0.26492, val loss: 0.29750\n",
      "Interaction training epoch: 38, train loss: 0.26557, val loss: 0.29749\n",
      "Interaction training epoch: 39, train loss: 0.26521, val loss: 0.29981\n",
      "Interaction training epoch: 40, train loss: 0.26419, val loss: 0.29870\n",
      "Interaction training epoch: 41, train loss: 0.26603, val loss: 0.30179\n",
      "Interaction training epoch: 42, train loss: 0.26477, val loss: 0.30072\n",
      "Interaction training epoch: 43, train loss: 0.26167, val loss: 0.29804\n",
      "Interaction training epoch: 44, train loss: 0.26229, val loss: 0.30041\n",
      "Interaction training epoch: 45, train loss: 0.26127, val loss: 0.30027\n",
      "Interaction training epoch: 46, train loss: 0.26574, val loss: 0.29997\n",
      "Interaction training epoch: 47, train loss: 0.26287, val loss: 0.30305\n",
      "Interaction training epoch: 48, train loss: 0.25893, val loss: 0.29991\n",
      "Interaction training epoch: 49, train loss: 0.26409, val loss: 0.29926\n",
      "Interaction training epoch: 50, train loss: 0.26184, val loss: 0.29961\n",
      "Interaction training epoch: 51, train loss: 0.25906, val loss: 0.30087\n",
      "Interaction training epoch: 52, train loss: 0.26086, val loss: 0.30047\n",
      "Interaction training epoch: 53, train loss: 0.25918, val loss: 0.29791\n",
      "Interaction training epoch: 54, train loss: 0.25938, val loss: 0.30051\n",
      "Interaction training epoch: 55, train loss: 0.26038, val loss: 0.30168\n",
      "Interaction training epoch: 56, train loss: 0.25740, val loss: 0.29441\n",
      "Interaction training epoch: 57, train loss: 0.26240, val loss: 0.30335\n",
      "Interaction training epoch: 58, train loss: 0.25998, val loss: 0.29848\n",
      "Interaction training epoch: 59, train loss: 0.26117, val loss: 0.30308\n",
      "Interaction training epoch: 60, train loss: 0.25994, val loss: 0.30051\n",
      "Interaction training epoch: 61, train loss: 0.25756, val loss: 0.29787\n",
      "Interaction training epoch: 62, train loss: 0.26094, val loss: 0.30140\n",
      "Interaction training epoch: 63, train loss: 0.25743, val loss: 0.29677\n",
      "Interaction training epoch: 64, train loss: 0.25778, val loss: 0.30063\n",
      "Interaction training epoch: 65, train loss: 0.25957, val loss: 0.29903\n",
      "Interaction training epoch: 66, train loss: 0.25649, val loss: 0.29840\n",
      "Interaction training epoch: 67, train loss: 0.25805, val loss: 0.30291\n",
      "Interaction training epoch: 68, train loss: 0.25867, val loss: 0.29918\n",
      "Interaction training epoch: 69, train loss: 0.25845, val loss: 0.30040\n",
      "Interaction training epoch: 70, train loss: 0.25817, val loss: 0.30180\n",
      "Interaction training epoch: 71, train loss: 0.25732, val loss: 0.30076\n",
      "Interaction training epoch: 72, train loss: 0.25728, val loss: 0.29960\n",
      "Interaction training epoch: 73, train loss: 0.25505, val loss: 0.29697\n",
      "Interaction training epoch: 74, train loss: 0.25639, val loss: 0.29838\n",
      "Interaction training epoch: 75, train loss: 0.25516, val loss: 0.30010\n",
      "Interaction training epoch: 76, train loss: 0.25674, val loss: 0.29802\n",
      "Interaction training epoch: 77, train loss: 0.25881, val loss: 0.30122\n",
      "Interaction training epoch: 78, train loss: 0.25385, val loss: 0.29665\n",
      "Interaction training epoch: 79, train loss: 0.25398, val loss: 0.29624\n",
      "Interaction training epoch: 80, train loss: 0.25696, val loss: 0.29715\n",
      "Interaction training epoch: 81, train loss: 0.25390, val loss: 0.29772\n",
      "Interaction training epoch: 82, train loss: 0.25257, val loss: 0.29771\n",
      "Interaction training epoch: 83, train loss: 0.25513, val loss: 0.29862\n",
      "Interaction training epoch: 84, train loss: 0.25361, val loss: 0.29970\n",
      "Interaction training epoch: 85, train loss: 0.25906, val loss: 0.29982\n",
      "Interaction training epoch: 86, train loss: 0.25576, val loss: 0.29910\n",
      "Interaction training epoch: 87, train loss: 0.25494, val loss: 0.29807\n",
      "Interaction training epoch: 88, train loss: 0.25522, val loss: 0.30214\n",
      "Interaction training epoch: 89, train loss: 0.25265, val loss: 0.29735\n",
      "Interaction training epoch: 90, train loss: 0.25444, val loss: 0.29720\n",
      "Interaction training epoch: 91, train loss: 0.25423, val loss: 0.30074\n",
      "Interaction training epoch: 92, train loss: 0.25638, val loss: 0.29688\n",
      "Interaction training epoch: 93, train loss: 0.25380, val loss: 0.29712\n",
      "Interaction training epoch: 94, train loss: 0.25510, val loss: 0.29949\n",
      "Interaction training epoch: 95, train loss: 0.25377, val loss: 0.29739\n",
      "Interaction training epoch: 96, train loss: 0.25128, val loss: 0.29646\n",
      "Interaction training epoch: 97, train loss: 0.25420, val loss: 0.29908\n",
      "Interaction training epoch: 98, train loss: 0.25218, val loss: 0.29722\n",
      "Interaction training epoch: 99, train loss: 0.25351, val loss: 0.29459\n",
      "Interaction training epoch: 100, train loss: 0.25464, val loss: 0.29773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 101, train loss: 0.25483, val loss: 0.29532\n",
      "Interaction training epoch: 102, train loss: 0.25068, val loss: 0.29432\n",
      "Interaction training epoch: 103, train loss: 0.25391, val loss: 0.29717\n",
      "Interaction training epoch: 104, train loss: 0.25235, val loss: 0.29815\n",
      "Interaction training epoch: 105, train loss: 0.25138, val loss: 0.30082\n",
      "Interaction training epoch: 106, train loss: 0.25151, val loss: 0.29955\n",
      "Interaction training epoch: 107, train loss: 0.25134, val loss: 0.29493\n",
      "Interaction training epoch: 108, train loss: 0.25165, val loss: 0.29599\n",
      "Interaction training epoch: 109, train loss: 0.25247, val loss: 0.30013\n",
      "Interaction training epoch: 110, train loss: 0.25126, val loss: 0.29454\n",
      "Interaction training epoch: 111, train loss: 0.25183, val loss: 0.29596\n",
      "Interaction training epoch: 112, train loss: 0.24907, val loss: 0.29628\n",
      "Interaction training epoch: 113, train loss: 0.25030, val loss: 0.29543\n",
      "Interaction training epoch: 114, train loss: 0.24995, val loss: 0.29928\n",
      "Interaction training epoch: 115, train loss: 0.24953, val loss: 0.29385\n",
      "Interaction training epoch: 116, train loss: 0.25199, val loss: 0.29751\n",
      "Interaction training epoch: 117, train loss: 0.24890, val loss: 0.29343\n",
      "Interaction training epoch: 118, train loss: 0.24993, val loss: 0.29569\n",
      "Interaction training epoch: 119, train loss: 0.25074, val loss: 0.29736\n",
      "Interaction training epoch: 120, train loss: 0.25088, val loss: 0.30050\n",
      "Interaction training epoch: 121, train loss: 0.25124, val loss: 0.29495\n",
      "Interaction training epoch: 122, train loss: 0.24946, val loss: 0.29557\n",
      "Interaction training epoch: 123, train loss: 0.24995, val loss: 0.29738\n",
      "Interaction training epoch: 124, train loss: 0.24669, val loss: 0.29701\n",
      "Interaction training epoch: 125, train loss: 0.25529, val loss: 0.29808\n",
      "Interaction training epoch: 126, train loss: 0.25103, val loss: 0.29995\n",
      "Interaction training epoch: 127, train loss: 0.25237, val loss: 0.29631\n",
      "Interaction training epoch: 128, train loss: 0.25139, val loss: 0.29806\n",
      "Interaction training epoch: 129, train loss: 0.24674, val loss: 0.29534\n",
      "Interaction training epoch: 130, train loss: 0.25173, val loss: 0.29466\n",
      "Interaction training epoch: 131, train loss: 0.24868, val loss: 0.29852\n",
      "Interaction training epoch: 132, train loss: 0.24873, val loss: 0.29616\n",
      "Interaction training epoch: 133, train loss: 0.24752, val loss: 0.29535\n",
      "Interaction training epoch: 134, train loss: 0.24503, val loss: 0.29264\n",
      "Interaction training epoch: 135, train loss: 0.24771, val loss: 0.29578\n",
      "Interaction training epoch: 136, train loss: 0.24930, val loss: 0.29584\n",
      "Interaction training epoch: 137, train loss: 0.24626, val loss: 0.29440\n",
      "Interaction training epoch: 138, train loss: 0.24840, val loss: 0.29585\n",
      "Interaction training epoch: 139, train loss: 0.24624, val loss: 0.29229\n",
      "Interaction training epoch: 140, train loss: 0.24472, val loss: 0.29314\n",
      "Interaction training epoch: 141, train loss: 0.24939, val loss: 0.29494\n",
      "Interaction training epoch: 142, train loss: 0.24544, val loss: 0.29458\n",
      "Interaction training epoch: 143, train loss: 0.24677, val loss: 0.29251\n",
      "Interaction training epoch: 144, train loss: 0.24731, val loss: 0.29823\n",
      "Interaction training epoch: 145, train loss: 0.24754, val loss: 0.29631\n",
      "Interaction training epoch: 146, train loss: 0.24565, val loss: 0.29261\n",
      "Interaction training epoch: 147, train loss: 0.24528, val loss: 0.29419\n",
      "Interaction training epoch: 148, train loss: 0.24469, val loss: 0.29634\n",
      "Interaction training epoch: 149, train loss: 0.24454, val loss: 0.29264\n",
      "Interaction training epoch: 150, train loss: 0.24583, val loss: 0.29577\n",
      "Interaction training epoch: 151, train loss: 0.24368, val loss: 0.29355\n",
      "Interaction training epoch: 152, train loss: 0.24367, val loss: 0.29342\n",
      "Interaction training epoch: 153, train loss: 0.24583, val loss: 0.29563\n",
      "Interaction training epoch: 154, train loss: 0.24238, val loss: 0.29488\n",
      "Interaction training epoch: 155, train loss: 0.24226, val loss: 0.29050\n",
      "Interaction training epoch: 156, train loss: 0.24242, val loss: 0.29342\n",
      "Interaction training epoch: 157, train loss: 0.24201, val loss: 0.29008\n",
      "Interaction training epoch: 158, train loss: 0.24538, val loss: 0.29473\n",
      "Interaction training epoch: 159, train loss: 0.24172, val loss: 0.28985\n",
      "Interaction training epoch: 160, train loss: 0.24177, val loss: 0.29561\n",
      "Interaction training epoch: 161, train loss: 0.24058, val loss: 0.29166\n",
      "Interaction training epoch: 162, train loss: 0.23935, val loss: 0.29111\n",
      "Interaction training epoch: 163, train loss: 0.24247, val loss: 0.29488\n",
      "Interaction training epoch: 164, train loss: 0.24057, val loss: 0.29347\n",
      "Interaction training epoch: 165, train loss: 0.24373, val loss: 0.29507\n",
      "Interaction training epoch: 166, train loss: 0.24113, val loss: 0.29250\n",
      "Interaction training epoch: 167, train loss: 0.24038, val loss: 0.29515\n",
      "Interaction training epoch: 168, train loss: 0.24200, val loss: 0.29629\n",
      "Interaction training epoch: 169, train loss: 0.24438, val loss: 0.29633\n",
      "Interaction training epoch: 170, train loss: 0.24129, val loss: 0.29023\n",
      "Interaction training epoch: 171, train loss: 0.23595, val loss: 0.29411\n",
      "Interaction training epoch: 172, train loss: 0.24008, val loss: 0.29739\n",
      "Interaction training epoch: 173, train loss: 0.24291, val loss: 0.29154\n",
      "Interaction training epoch: 174, train loss: 0.23644, val loss: 0.29440\n",
      "Interaction training epoch: 175, train loss: 0.23932, val loss: 0.29475\n",
      "Interaction training epoch: 176, train loss: 0.24521, val loss: 0.29602\n",
      "Interaction training epoch: 177, train loss: 0.23960, val loss: 0.29785\n",
      "Interaction training epoch: 178, train loss: 0.24133, val loss: 0.29045\n",
      "Interaction training epoch: 179, train loss: 0.24182, val loss: 0.30755\n",
      "Interaction training epoch: 180, train loss: 0.24265, val loss: 0.29080\n",
      "Interaction training epoch: 181, train loss: 0.24125, val loss: 0.29832\n",
      "Interaction training epoch: 182, train loss: 0.23894, val loss: 0.29301\n",
      "Interaction training epoch: 183, train loss: 0.23933, val loss: 0.29531\n",
      "Interaction training epoch: 184, train loss: 0.23692, val loss: 0.29616\n",
      "Interaction training epoch: 185, train loss: 0.24195, val loss: 0.29753\n",
      "Interaction training epoch: 186, train loss: 0.23800, val loss: 0.29484\n",
      "Interaction training epoch: 187, train loss: 0.24243, val loss: 0.29580\n",
      "Interaction training epoch: 188, train loss: 0.23572, val loss: 0.29179\n",
      "Interaction training epoch: 189, train loss: 0.23799, val loss: 0.29748\n",
      "Interaction training epoch: 190, train loss: 0.23536, val loss: 0.29540\n",
      "Interaction training epoch: 191, train loss: 0.24204, val loss: 0.30143\n",
      "Interaction training epoch: 192, train loss: 0.23596, val loss: 0.29371\n",
      "Interaction training epoch: 193, train loss: 0.23760, val loss: 0.29570\n",
      "Interaction training epoch: 194, train loss: 0.24014, val loss: 0.29122\n",
      "Interaction training epoch: 195, train loss: 0.23657, val loss: 0.29438\n",
      "Interaction training epoch: 196, train loss: 0.24030, val loss: 0.29151\n",
      "Interaction training epoch: 197, train loss: 0.23753, val loss: 0.30151\n",
      "Interaction training epoch: 198, train loss: 0.23552, val loss: 0.29151\n",
      "Interaction training epoch: 199, train loss: 0.23912, val loss: 0.29502\n",
      "Interaction training epoch: 200, train loss: 0.23474, val loss: 0.29830\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########2 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.24125, val loss: 0.29511\n",
      "Interaction tuning epoch: 2, train loss: 0.23748, val loss: 0.29956\n",
      "Interaction tuning epoch: 3, train loss: 0.23827, val loss: 0.29429\n",
      "Interaction tuning epoch: 4, train loss: 0.23567, val loss: 0.29248\n",
      "Interaction tuning epoch: 5, train loss: 0.23796, val loss: 0.29587\n",
      "Interaction tuning epoch: 6, train loss: 0.23396, val loss: 0.29347\n",
      "Interaction tuning epoch: 7, train loss: 0.23616, val loss: 0.29684\n",
      "Interaction tuning epoch: 8, train loss: 0.23798, val loss: 0.28988\n",
      "Interaction tuning epoch: 9, train loss: 0.23677, val loss: 0.29907\n",
      "Interaction tuning epoch: 10, train loss: 0.23378, val loss: 0.29128\n",
      "Interaction tuning epoch: 11, train loss: 0.23507, val loss: 0.29248\n",
      "Interaction tuning epoch: 12, train loss: 0.23687, val loss: 0.29673\n",
      "Interaction tuning epoch: 13, train loss: 0.23288, val loss: 0.29264\n",
      "Interaction tuning epoch: 14, train loss: 0.23566, val loss: 0.29634\n",
      "Interaction tuning epoch: 15, train loss: 0.23319, val loss: 0.29188\n",
      "Interaction tuning epoch: 16, train loss: 0.23689, val loss: 0.29445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 17, train loss: 0.23528, val loss: 0.29292\n",
      "Interaction tuning epoch: 18, train loss: 0.23576, val loss: 0.29525\n",
      "Interaction tuning epoch: 19, train loss: 0.23616, val loss: 0.29103\n",
      "Interaction tuning epoch: 20, train loss: 0.23470, val loss: 0.29258\n",
      "Interaction tuning epoch: 21, train loss: 0.23577, val loss: 0.29362\n",
      "Interaction tuning epoch: 22, train loss: 0.23685, val loss: 0.30027\n",
      "Interaction tuning epoch: 23, train loss: 0.23917, val loss: 0.29554\n",
      "Interaction tuning epoch: 24, train loss: 0.23392, val loss: 0.29409\n",
      "Interaction tuning epoch: 25, train loss: 0.23540, val loss: 0.29790\n",
      "Interaction tuning epoch: 26, train loss: 0.23678, val loss: 0.29537\n",
      "Interaction tuning epoch: 27, train loss: 0.23473, val loss: 0.29322\n",
      "Interaction tuning epoch: 28, train loss: 0.23657, val loss: 0.29510\n",
      "Interaction tuning epoch: 29, train loss: 0.23651, val loss: 0.29532\n",
      "Interaction tuning epoch: 30, train loss: 0.23266, val loss: 0.29144\n",
      "Interaction tuning epoch: 31, train loss: 0.23324, val loss: 0.29773\n",
      "Interaction tuning epoch: 32, train loss: 0.23349, val loss: 0.29296\n",
      "Interaction tuning epoch: 33, train loss: 0.23456, val loss: 0.29277\n",
      "Interaction tuning epoch: 34, train loss: 0.23519, val loss: 0.29422\n",
      "Interaction tuning epoch: 35, train loss: 0.23535, val loss: 0.29867\n",
      "Interaction tuning epoch: 36, train loss: 0.23145, val loss: 0.29335\n",
      "Interaction tuning epoch: 37, train loss: 0.23554, val loss: 0.29114\n",
      "Interaction tuning epoch: 38, train loss: 0.23292, val loss: 0.28936\n",
      "Interaction tuning epoch: 39, train loss: 0.23445, val loss: 0.29688\n",
      "Interaction tuning epoch: 40, train loss: 0.23050, val loss: 0.29185\n",
      "Interaction tuning epoch: 41, train loss: 0.23329, val loss: 0.29349\n",
      "Interaction tuning epoch: 42, train loss: 0.23370, val loss: 0.29101\n",
      "Interaction tuning epoch: 43, train loss: 0.23416, val loss: 0.29723\n",
      "Interaction tuning epoch: 44, train loss: 0.23030, val loss: 0.28969\n",
      "Interaction tuning epoch: 45, train loss: 0.23296, val loss: 0.29450\n",
      "Interaction tuning epoch: 46, train loss: 0.23091, val loss: 0.29372\n",
      "Interaction tuning epoch: 47, train loss: 0.23408, val loss: 0.29383\n",
      "Interaction tuning epoch: 48, train loss: 0.23113, val loss: 0.29259\n",
      "Interaction tuning epoch: 49, train loss: 0.23612, val loss: 0.29738\n",
      "Interaction tuning epoch: 50, train loss: 0.22825, val loss: 0.29070\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 38.42477035522461\n",
      "After the gam stage, training error is 0.22825 , validation error is 0.29070\n",
      "missing value counts: 99190\n",
      "[SoftImpute] Max Singular Value of X_init = 3.558150\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.194291 validation BCE=0.287146,rank=5\n",
      "[SoftImpute] Iter 2: observed BCE=0.190755 validation BCE=0.285706,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.188549 validation BCE=0.285094,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.187010 validation BCE=0.283604,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.186090 validation BCE=0.283186,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.185580 validation BCE=0.282672,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.185255 validation BCE=0.282280,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.184816 validation BCE=0.281597,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.184488 validation BCE=0.281477,rank=5\n",
      "[SoftImpute] Iter 10: observed BCE=0.184220 validation BCE=0.280867,rank=5\n",
      "[SoftImpute] Iter 11: observed BCE=0.184173 validation BCE=0.280715,rank=5\n",
      "[SoftImpute] Iter 12: observed BCE=0.184406 validation BCE=0.281590,rank=5\n",
      "[SoftImpute] Iter 13: observed BCE=0.183874 validation BCE=0.299735,rank=5\n",
      "[SoftImpute] Iter 14: observed BCE=0.184293 validation BCE=0.299554,rank=5\n",
      "[SoftImpute] Iter 15: observed BCE=0.184163 validation BCE=0.299762,rank=5\n",
      "[SoftImpute] Iter 16: observed BCE=0.183876 validation BCE=0.299502,rank=5\n",
      "[SoftImpute] Iter 17: observed BCE=0.184266 validation BCE=0.287981,rank=5\n",
      "[SoftImpute] Iter 18: observed BCE=0.184210 validation BCE=0.288636,rank=5\n",
      "[SoftImpute] Iter 19: observed BCE=0.184059 validation BCE=0.299123,rank=5\n",
      "[SoftImpute] Iter 20: observed BCE=0.184231 validation BCE=0.299137,rank=5\n",
      "[SoftImpute] Stopped after iteration 20 for lambda=0.071163\n",
      "final num of user group: 24\n",
      "final num of item group: 31\n",
      "change mode state : True\n",
      "time cost: 7.472515106201172\n",
      "After the matrix factor stage, training error is 0.18423, validation error is 0.29914\n",
      "9\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68389, val loss: 0.68243\n",
      "Main effects training epoch: 2, train loss: 0.67576, val loss: 0.67667\n",
      "Main effects training epoch: 3, train loss: 0.67173, val loss: 0.67415\n",
      "Main effects training epoch: 4, train loss: 0.66557, val loss: 0.66765\n",
      "Main effects training epoch: 5, train loss: 0.65270, val loss: 0.65346\n",
      "Main effects training epoch: 6, train loss: 0.62146, val loss: 0.62022\n",
      "Main effects training epoch: 7, train loss: 0.57504, val loss: 0.57221\n",
      "Main effects training epoch: 8, train loss: 0.54112, val loss: 0.53436\n",
      "Main effects training epoch: 9, train loss: 0.52824, val loss: 0.52187\n",
      "Main effects training epoch: 10, train loss: 0.52412, val loss: 0.51726\n",
      "Main effects training epoch: 11, train loss: 0.52198, val loss: 0.51378\n",
      "Main effects training epoch: 12, train loss: 0.52157, val loss: 0.51369\n",
      "Main effects training epoch: 13, train loss: 0.52133, val loss: 0.51446\n",
      "Main effects training epoch: 14, train loss: 0.52115, val loss: 0.51371\n",
      "Main effects training epoch: 15, train loss: 0.52178, val loss: 0.51500\n",
      "Main effects training epoch: 16, train loss: 0.52182, val loss: 0.51506\n",
      "Main effects training epoch: 17, train loss: 0.52118, val loss: 0.51558\n",
      "Main effects training epoch: 18, train loss: 0.52059, val loss: 0.51445\n",
      "Main effects training epoch: 19, train loss: 0.52018, val loss: 0.51414\n",
      "Main effects training epoch: 20, train loss: 0.52030, val loss: 0.51416\n",
      "Main effects training epoch: 21, train loss: 0.52033, val loss: 0.51381\n",
      "Main effects training epoch: 22, train loss: 0.52040, val loss: 0.51324\n",
      "Main effects training epoch: 23, train loss: 0.52013, val loss: 0.51489\n",
      "Main effects training epoch: 24, train loss: 0.51997, val loss: 0.51381\n",
      "Main effects training epoch: 25, train loss: 0.52014, val loss: 0.51420\n",
      "Main effects training epoch: 26, train loss: 0.52084, val loss: 0.51349\n",
      "Main effects training epoch: 27, train loss: 0.52022, val loss: 0.51509\n",
      "Main effects training epoch: 28, train loss: 0.52024, val loss: 0.51447\n",
      "Main effects training epoch: 29, train loss: 0.52061, val loss: 0.51511\n",
      "Main effects training epoch: 30, train loss: 0.51980, val loss: 0.51360\n",
      "Main effects training epoch: 31, train loss: 0.51974, val loss: 0.51426\n",
      "Main effects training epoch: 32, train loss: 0.51968, val loss: 0.51411\n",
      "Main effects training epoch: 33, train loss: 0.52087, val loss: 0.51592\n",
      "Main effects training epoch: 34, train loss: 0.52197, val loss: 0.51417\n",
      "Main effects training epoch: 35, train loss: 0.52029, val loss: 0.51528\n",
      "Main effects training epoch: 36, train loss: 0.52039, val loss: 0.51459\n",
      "Main effects training epoch: 37, train loss: 0.52064, val loss: 0.51349\n",
      "Main effects training epoch: 38, train loss: 0.52128, val loss: 0.51811\n",
      "Main effects training epoch: 39, train loss: 0.52048, val loss: 0.51330\n",
      "Main effects training epoch: 40, train loss: 0.52013, val loss: 0.51472\n",
      "Main effects training epoch: 41, train loss: 0.52085, val loss: 0.51369\n",
      "Main effects training epoch: 42, train loss: 0.51998, val loss: 0.51394\n",
      "Main effects training epoch: 43, train loss: 0.51961, val loss: 0.51474\n",
      "Main effects training epoch: 44, train loss: 0.52000, val loss: 0.51356\n",
      "Main effects training epoch: 45, train loss: 0.52021, val loss: 0.51330\n",
      "Main effects training epoch: 46, train loss: 0.52030, val loss: 0.51619\n",
      "Main effects training epoch: 47, train loss: 0.52232, val loss: 0.51368\n",
      "Main effects training epoch: 48, train loss: 0.52004, val loss: 0.51589\n",
      "Main effects training epoch: 49, train loss: 0.51933, val loss: 0.51287\n",
      "Main effects training epoch: 50, train loss: 0.51914, val loss: 0.51303\n",
      "Main effects training epoch: 51, train loss: 0.51922, val loss: 0.51315\n",
      "Main effects training epoch: 52, train loss: 0.52032, val loss: 0.51433\n",
      "Main effects training epoch: 53, train loss: 0.52073, val loss: 0.51649\n",
      "Main effects training epoch: 54, train loss: 0.51946, val loss: 0.51281\n",
      "Main effects training epoch: 55, train loss: 0.51942, val loss: 0.51494\n",
      "Main effects training epoch: 56, train loss: 0.51990, val loss: 0.51384\n",
      "Main effects training epoch: 57, train loss: 0.51962, val loss: 0.51487\n",
      "Main effects training epoch: 58, train loss: 0.51978, val loss: 0.51256\n",
      "Main effects training epoch: 59, train loss: 0.51948, val loss: 0.51504\n",
      "Main effects training epoch: 60, train loss: 0.51923, val loss: 0.51376\n",
      "Main effects training epoch: 61, train loss: 0.51895, val loss: 0.51341\n",
      "Main effects training epoch: 62, train loss: 0.51913, val loss: 0.51255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 63, train loss: 0.51887, val loss: 0.51395\n",
      "Main effects training epoch: 64, train loss: 0.51883, val loss: 0.51430\n",
      "Main effects training epoch: 65, train loss: 0.51912, val loss: 0.51254\n",
      "Main effects training epoch: 66, train loss: 0.51939, val loss: 0.51457\n",
      "Main effects training epoch: 67, train loss: 0.51885, val loss: 0.51292\n",
      "Main effects training epoch: 68, train loss: 0.51876, val loss: 0.51278\n",
      "Main effects training epoch: 69, train loss: 0.51870, val loss: 0.51409\n",
      "Main effects training epoch: 70, train loss: 0.51858, val loss: 0.51281\n",
      "Main effects training epoch: 71, train loss: 0.51868, val loss: 0.51314\n",
      "Main effects training epoch: 72, train loss: 0.51868, val loss: 0.51386\n",
      "Main effects training epoch: 73, train loss: 0.51906, val loss: 0.51368\n",
      "Main effects training epoch: 74, train loss: 0.51878, val loss: 0.51404\n",
      "Main effects training epoch: 75, train loss: 0.51864, val loss: 0.51283\n",
      "Main effects training epoch: 76, train loss: 0.51855, val loss: 0.51395\n",
      "Main effects training epoch: 77, train loss: 0.51878, val loss: 0.51436\n",
      "Main effects training epoch: 78, train loss: 0.51860, val loss: 0.51262\n",
      "Main effects training epoch: 79, train loss: 0.51836, val loss: 0.51268\n",
      "Main effects training epoch: 80, train loss: 0.51846, val loss: 0.51348\n",
      "Main effects training epoch: 81, train loss: 0.51853, val loss: 0.51351\n",
      "Main effects training epoch: 82, train loss: 0.51876, val loss: 0.51402\n",
      "Main effects training epoch: 83, train loss: 0.51841, val loss: 0.51439\n",
      "Main effects training epoch: 84, train loss: 0.51817, val loss: 0.51230\n",
      "Main effects training epoch: 85, train loss: 0.51816, val loss: 0.51272\n",
      "Main effects training epoch: 86, train loss: 0.51810, val loss: 0.51342\n",
      "Main effects training epoch: 87, train loss: 0.51819, val loss: 0.51275\n",
      "Main effects training epoch: 88, train loss: 0.51832, val loss: 0.51414\n",
      "Main effects training epoch: 89, train loss: 0.51843, val loss: 0.51244\n",
      "Main effects training epoch: 90, train loss: 0.51845, val loss: 0.51441\n",
      "Main effects training epoch: 91, train loss: 0.51847, val loss: 0.51419\n",
      "Main effects training epoch: 92, train loss: 0.51842, val loss: 0.51332\n",
      "Main effects training epoch: 93, train loss: 0.51837, val loss: 0.51370\n",
      "Main effects training epoch: 94, train loss: 0.51812, val loss: 0.51212\n",
      "Main effects training epoch: 95, train loss: 0.51808, val loss: 0.51201\n",
      "Main effects training epoch: 96, train loss: 0.51830, val loss: 0.51308\n",
      "Main effects training epoch: 97, train loss: 0.51813, val loss: 0.51296\n",
      "Main effects training epoch: 98, train loss: 0.51834, val loss: 0.51227\n",
      "Main effects training epoch: 99, train loss: 0.51811, val loss: 0.51351\n",
      "Main effects training epoch: 100, train loss: 0.51808, val loss: 0.51338\n",
      "Main effects training epoch: 101, train loss: 0.51804, val loss: 0.51277\n",
      "Main effects training epoch: 102, train loss: 0.51793, val loss: 0.51264\n",
      "Main effects training epoch: 103, train loss: 0.51785, val loss: 0.51297\n",
      "Main effects training epoch: 104, train loss: 0.51787, val loss: 0.51282\n",
      "Main effects training epoch: 105, train loss: 0.51850, val loss: 0.51354\n",
      "Main effects training epoch: 106, train loss: 0.51861, val loss: 0.51283\n",
      "Main effects training epoch: 107, train loss: 0.51821, val loss: 0.51396\n",
      "Main effects training epoch: 108, train loss: 0.51892, val loss: 0.51189\n",
      "Main effects training epoch: 109, train loss: 0.51849, val loss: 0.51470\n",
      "Main effects training epoch: 110, train loss: 0.51811, val loss: 0.51220\n",
      "Main effects training epoch: 111, train loss: 0.51806, val loss: 0.51393\n",
      "Main effects training epoch: 112, train loss: 0.51805, val loss: 0.51210\n",
      "Main effects training epoch: 113, train loss: 0.51785, val loss: 0.51387\n",
      "Main effects training epoch: 114, train loss: 0.51817, val loss: 0.51123\n",
      "Main effects training epoch: 115, train loss: 0.51766, val loss: 0.51257\n",
      "Main effects training epoch: 116, train loss: 0.51800, val loss: 0.51310\n",
      "Main effects training epoch: 117, train loss: 0.51765, val loss: 0.51364\n",
      "Main effects training epoch: 118, train loss: 0.51754, val loss: 0.51290\n",
      "Main effects training epoch: 119, train loss: 0.51739, val loss: 0.51291\n",
      "Main effects training epoch: 120, train loss: 0.51735, val loss: 0.51235\n",
      "Main effects training epoch: 121, train loss: 0.51746, val loss: 0.51194\n",
      "Main effects training epoch: 122, train loss: 0.51723, val loss: 0.51239\n",
      "Main effects training epoch: 123, train loss: 0.51782, val loss: 0.51169\n",
      "Main effects training epoch: 124, train loss: 0.51878, val loss: 0.51571\n",
      "Main effects training epoch: 125, train loss: 0.51826, val loss: 0.51230\n",
      "Main effects training epoch: 126, train loss: 0.51806, val loss: 0.51469\n",
      "Main effects training epoch: 127, train loss: 0.51767, val loss: 0.51226\n",
      "Main effects training epoch: 128, train loss: 0.51754, val loss: 0.51240\n",
      "Main effects training epoch: 129, train loss: 0.51721, val loss: 0.51243\n",
      "Main effects training epoch: 130, train loss: 0.51701, val loss: 0.51198\n",
      "Main effects training epoch: 131, train loss: 0.51726, val loss: 0.51164\n",
      "Main effects training epoch: 132, train loss: 0.51749, val loss: 0.51413\n",
      "Main effects training epoch: 133, train loss: 0.51694, val loss: 0.51177\n",
      "Main effects training epoch: 134, train loss: 0.51697, val loss: 0.51261\n",
      "Main effects training epoch: 135, train loss: 0.51696, val loss: 0.51225\n",
      "Main effects training epoch: 136, train loss: 0.51720, val loss: 0.51265\n",
      "Main effects training epoch: 137, train loss: 0.51708, val loss: 0.51236\n",
      "Main effects training epoch: 138, train loss: 0.51729, val loss: 0.51358\n",
      "Main effects training epoch: 139, train loss: 0.51768, val loss: 0.51130\n",
      "Main effects training epoch: 140, train loss: 0.51700, val loss: 0.51230\n",
      "Main effects training epoch: 141, train loss: 0.51740, val loss: 0.51104\n",
      "Main effects training epoch: 142, train loss: 0.51681, val loss: 0.51271\n",
      "Main effects training epoch: 143, train loss: 0.51657, val loss: 0.51159\n",
      "Main effects training epoch: 144, train loss: 0.51654, val loss: 0.51114\n",
      "Main effects training epoch: 145, train loss: 0.51650, val loss: 0.51195\n",
      "Main effects training epoch: 146, train loss: 0.51642, val loss: 0.51117\n",
      "Main effects training epoch: 147, train loss: 0.51647, val loss: 0.51162\n",
      "Main effects training epoch: 148, train loss: 0.51761, val loss: 0.51145\n",
      "Main effects training epoch: 149, train loss: 0.51726, val loss: 0.51416\n",
      "Main effects training epoch: 150, train loss: 0.51722, val loss: 0.51075\n",
      "Main effects training epoch: 151, train loss: 0.51623, val loss: 0.51121\n",
      "Main effects training epoch: 152, train loss: 0.51626, val loss: 0.51170\n",
      "Main effects training epoch: 153, train loss: 0.51647, val loss: 0.51172\n",
      "Main effects training epoch: 154, train loss: 0.51690, val loss: 0.51155\n",
      "Main effects training epoch: 155, train loss: 0.51679, val loss: 0.51348\n",
      "Main effects training epoch: 156, train loss: 0.51755, val loss: 0.51183\n",
      "Main effects training epoch: 157, train loss: 0.51638, val loss: 0.51239\n",
      "Main effects training epoch: 158, train loss: 0.51595, val loss: 0.51088\n",
      "Main effects training epoch: 159, train loss: 0.51596, val loss: 0.51121\n",
      "Main effects training epoch: 160, train loss: 0.51611, val loss: 0.50977\n",
      "Main effects training epoch: 161, train loss: 0.51616, val loss: 0.51247\n",
      "Main effects training epoch: 162, train loss: 0.51694, val loss: 0.51096\n",
      "Main effects training epoch: 163, train loss: 0.51638, val loss: 0.51386\n",
      "Main effects training epoch: 164, train loss: 0.51631, val loss: 0.51079\n",
      "Main effects training epoch: 165, train loss: 0.51633, val loss: 0.51274\n",
      "Main effects training epoch: 166, train loss: 0.51604, val loss: 0.51085\n",
      "Main effects training epoch: 167, train loss: 0.51571, val loss: 0.51131\n",
      "Main effects training epoch: 168, train loss: 0.51571, val loss: 0.51082\n",
      "Main effects training epoch: 169, train loss: 0.51646, val loss: 0.51313\n",
      "Main effects training epoch: 170, train loss: 0.51540, val loss: 0.51092\n",
      "Main effects training epoch: 171, train loss: 0.51576, val loss: 0.51062\n",
      "Main effects training epoch: 172, train loss: 0.51538, val loss: 0.51096\n",
      "Main effects training epoch: 173, train loss: 0.51596, val loss: 0.51043\n",
      "Main effects training epoch: 174, train loss: 0.51616, val loss: 0.51385\n",
      "Main effects training epoch: 175, train loss: 0.51573, val loss: 0.51038\n",
      "Main effects training epoch: 176, train loss: 0.51542, val loss: 0.51227\n",
      "Main effects training epoch: 177, train loss: 0.51516, val loss: 0.51009\n",
      "Main effects training epoch: 178, train loss: 0.51538, val loss: 0.51018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 179, train loss: 0.51522, val loss: 0.51190\n",
      "Main effects training epoch: 180, train loss: 0.51518, val loss: 0.51023\n",
      "Main effects training epoch: 181, train loss: 0.51532, val loss: 0.51202\n",
      "Main effects training epoch: 182, train loss: 0.51500, val loss: 0.51000\n",
      "Main effects training epoch: 183, train loss: 0.51497, val loss: 0.51057\n",
      "Main effects training epoch: 184, train loss: 0.51532, val loss: 0.51116\n",
      "Main effects training epoch: 185, train loss: 0.51494, val loss: 0.50990\n",
      "Main effects training epoch: 186, train loss: 0.51470, val loss: 0.51063\n",
      "Main effects training epoch: 187, train loss: 0.51501, val loss: 0.51230\n",
      "Main effects training epoch: 188, train loss: 0.51488, val loss: 0.51066\n",
      "Main effects training epoch: 189, train loss: 0.51485, val loss: 0.51088\n",
      "Main effects training epoch: 190, train loss: 0.51504, val loss: 0.51221\n",
      "Main effects training epoch: 191, train loss: 0.51481, val loss: 0.50981\n",
      "Main effects training epoch: 192, train loss: 0.51461, val loss: 0.51115\n",
      "Main effects training epoch: 193, train loss: 0.51495, val loss: 0.51027\n",
      "Main effects training epoch: 194, train loss: 0.51537, val loss: 0.51086\n",
      "Main effects training epoch: 195, train loss: 0.51476, val loss: 0.51141\n",
      "Main effects training epoch: 196, train loss: 0.51456, val loss: 0.51111\n",
      "Main effects training epoch: 197, train loss: 0.51463, val loss: 0.51030\n",
      "Main effects training epoch: 198, train loss: 0.51469, val loss: 0.51044\n",
      "Main effects training epoch: 199, train loss: 0.51442, val loss: 0.51091\n",
      "Main effects training epoch: 200, train loss: 0.51444, val loss: 0.50942\n",
      "Main effects training epoch: 201, train loss: 0.51444, val loss: 0.51211\n",
      "Main effects training epoch: 202, train loss: 0.51458, val loss: 0.50941\n",
      "Main effects training epoch: 203, train loss: 0.51456, val loss: 0.51242\n",
      "Main effects training epoch: 204, train loss: 0.51416, val loss: 0.50933\n",
      "Main effects training epoch: 205, train loss: 0.51394, val loss: 0.51027\n",
      "Main effects training epoch: 206, train loss: 0.51401, val loss: 0.50906\n",
      "Main effects training epoch: 207, train loss: 0.51386, val loss: 0.50951\n",
      "Main effects training epoch: 208, train loss: 0.51410, val loss: 0.51151\n",
      "Main effects training epoch: 209, train loss: 0.51462, val loss: 0.50821\n",
      "Main effects training epoch: 210, train loss: 0.51434, val loss: 0.51227\n",
      "Main effects training epoch: 211, train loss: 0.51382, val loss: 0.51005\n",
      "Main effects training epoch: 212, train loss: 0.51419, val loss: 0.50919\n",
      "Main effects training epoch: 213, train loss: 0.51363, val loss: 0.51056\n",
      "Main effects training epoch: 214, train loss: 0.51357, val loss: 0.50911\n",
      "Main effects training epoch: 215, train loss: 0.51359, val loss: 0.50926\n",
      "Main effects training epoch: 216, train loss: 0.51436, val loss: 0.51217\n",
      "Main effects training epoch: 217, train loss: 0.51444, val loss: 0.50861\n",
      "Main effects training epoch: 218, train loss: 0.51401, val loss: 0.51209\n",
      "Main effects training epoch: 219, train loss: 0.51389, val loss: 0.50841\n",
      "Main effects training epoch: 220, train loss: 0.51316, val loss: 0.50869\n",
      "Main effects training epoch: 221, train loss: 0.51327, val loss: 0.50971\n",
      "Main effects training epoch: 222, train loss: 0.51308, val loss: 0.50844\n",
      "Main effects training epoch: 223, train loss: 0.51305, val loss: 0.51017\n",
      "Main effects training epoch: 224, train loss: 0.51382, val loss: 0.50857\n",
      "Main effects training epoch: 225, train loss: 0.51337, val loss: 0.51016\n",
      "Main effects training epoch: 226, train loss: 0.51313, val loss: 0.50827\n",
      "Main effects training epoch: 227, train loss: 0.51278, val loss: 0.50878\n",
      "Main effects training epoch: 228, train loss: 0.51304, val loss: 0.50872\n",
      "Main effects training epoch: 229, train loss: 0.51269, val loss: 0.50765\n",
      "Main effects training epoch: 230, train loss: 0.51251, val loss: 0.50909\n",
      "Main effects training epoch: 231, train loss: 0.51258, val loss: 0.50750\n",
      "Main effects training epoch: 232, train loss: 0.51294, val loss: 0.50996\n",
      "Main effects training epoch: 233, train loss: 0.51254, val loss: 0.50708\n",
      "Main effects training epoch: 234, train loss: 0.51224, val loss: 0.50919\n",
      "Main effects training epoch: 235, train loss: 0.51238, val loss: 0.50659\n",
      "Main effects training epoch: 236, train loss: 0.51215, val loss: 0.50904\n",
      "Main effects training epoch: 237, train loss: 0.51146, val loss: 0.50679\n",
      "Main effects training epoch: 238, train loss: 0.51177, val loss: 0.50801\n",
      "Main effects training epoch: 239, train loss: 0.51156, val loss: 0.50638\n",
      "Main effects training epoch: 240, train loss: 0.51146, val loss: 0.50793\n",
      "Main effects training epoch: 241, train loss: 0.51147, val loss: 0.50700\n",
      "Main effects training epoch: 242, train loss: 0.51122, val loss: 0.50708\n",
      "Main effects training epoch: 243, train loss: 0.51135, val loss: 0.50506\n",
      "Main effects training epoch: 244, train loss: 0.51148, val loss: 0.50829\n",
      "Main effects training epoch: 245, train loss: 0.51115, val loss: 0.50669\n",
      "Main effects training epoch: 246, train loss: 0.51107, val loss: 0.50574\n",
      "Main effects training epoch: 247, train loss: 0.51107, val loss: 0.50769\n",
      "Main effects training epoch: 248, train loss: 0.51109, val loss: 0.50622\n",
      "Main effects training epoch: 249, train loss: 0.51080, val loss: 0.50594\n",
      "Main effects training epoch: 250, train loss: 0.51112, val loss: 0.50506\n",
      "Main effects training epoch: 251, train loss: 0.51151, val loss: 0.50997\n",
      "Main effects training epoch: 252, train loss: 0.51159, val loss: 0.50481\n",
      "Main effects training epoch: 253, train loss: 0.51069, val loss: 0.50594\n",
      "Main effects training epoch: 254, train loss: 0.51060, val loss: 0.50529\n",
      "Main effects training epoch: 255, train loss: 0.51129, val loss: 0.50883\n",
      "Main effects training epoch: 256, train loss: 0.51097, val loss: 0.50594\n",
      "Main effects training epoch: 257, train loss: 0.51035, val loss: 0.50622\n",
      "Main effects training epoch: 258, train loss: 0.51054, val loss: 0.50694\n",
      "Main effects training epoch: 259, train loss: 0.51043, val loss: 0.50670\n",
      "Main effects training epoch: 260, train loss: 0.51037, val loss: 0.50704\n",
      "Main effects training epoch: 261, train loss: 0.51037, val loss: 0.50638\n",
      "Main effects training epoch: 262, train loss: 0.51033, val loss: 0.50520\n",
      "Main effects training epoch: 263, train loss: 0.51023, val loss: 0.50627\n",
      "Main effects training epoch: 264, train loss: 0.51072, val loss: 0.50609\n",
      "Main effects training epoch: 265, train loss: 0.51027, val loss: 0.50707\n",
      "Main effects training epoch: 266, train loss: 0.51051, val loss: 0.50708\n",
      "Main effects training epoch: 267, train loss: 0.50994, val loss: 0.50502\n",
      "Main effects training epoch: 268, train loss: 0.51000, val loss: 0.50550\n",
      "Main effects training epoch: 269, train loss: 0.51059, val loss: 0.50630\n",
      "Main effects training epoch: 270, train loss: 0.51026, val loss: 0.50555\n",
      "Main effects training epoch: 271, train loss: 0.50991, val loss: 0.50610\n",
      "Main effects training epoch: 272, train loss: 0.51057, val loss: 0.50413\n",
      "Main effects training epoch: 273, train loss: 0.50953, val loss: 0.50562\n",
      "Main effects training epoch: 274, train loss: 0.50997, val loss: 0.50799\n",
      "Main effects training epoch: 275, train loss: 0.51002, val loss: 0.50437\n",
      "Main effects training epoch: 276, train loss: 0.51176, val loss: 0.50862\n",
      "Main effects training epoch: 277, train loss: 0.51203, val loss: 0.50703\n",
      "Main effects training epoch: 278, train loss: 0.51110, val loss: 0.50737\n",
      "Main effects training epoch: 279, train loss: 0.50998, val loss: 0.50489\n",
      "Main effects training epoch: 280, train loss: 0.51040, val loss: 0.50831\n",
      "Main effects training epoch: 281, train loss: 0.51053, val loss: 0.50378\n",
      "Main effects training epoch: 282, train loss: 0.50955, val loss: 0.50699\n",
      "Main effects training epoch: 283, train loss: 0.50946, val loss: 0.50362\n",
      "Main effects training epoch: 284, train loss: 0.50982, val loss: 0.50639\n",
      "Main effects training epoch: 285, train loss: 0.50927, val loss: 0.50455\n",
      "Main effects training epoch: 286, train loss: 0.50903, val loss: 0.50562\n",
      "Main effects training epoch: 287, train loss: 0.50897, val loss: 0.50317\n",
      "Main effects training epoch: 288, train loss: 0.50841, val loss: 0.50419\n",
      "Main effects training epoch: 289, train loss: 0.50866, val loss: 0.50466\n",
      "Main effects training epoch: 290, train loss: 0.50856, val loss: 0.50428\n",
      "Main effects training epoch: 291, train loss: 0.50856, val loss: 0.50428\n",
      "Main effects training epoch: 292, train loss: 0.50835, val loss: 0.50485\n",
      "Main effects training epoch: 293, train loss: 0.50857, val loss: 0.50400\n",
      "Main effects training epoch: 294, train loss: 0.50863, val loss: 0.50362\n",
      "Main effects training epoch: 295, train loss: 0.50879, val loss: 0.50558\n",
      "Main effects training epoch: 296, train loss: 0.50849, val loss: 0.50298\n",
      "Main effects training epoch: 297, train loss: 0.50776, val loss: 0.50475\n",
      "Main effects training epoch: 298, train loss: 0.50761, val loss: 0.50333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 299, train loss: 0.50788, val loss: 0.50367\n",
      "Main effects training epoch: 300, train loss: 0.50785, val loss: 0.50353\n",
      "##########Stage 1: main effect training stop.##########\n",
      "6 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51062, val loss: 0.50555\n",
      "Main effects tuning epoch: 2, train loss: 0.51021, val loss: 0.50699\n",
      "Main effects tuning epoch: 3, train loss: 0.51079, val loss: 0.50514\n",
      "Main effects tuning epoch: 4, train loss: 0.51019, val loss: 0.50632\n",
      "Main effects tuning epoch: 5, train loss: 0.50984, val loss: 0.50450\n",
      "Main effects tuning epoch: 6, train loss: 0.50973, val loss: 0.50655\n",
      "Main effects tuning epoch: 7, train loss: 0.50969, val loss: 0.50364\n",
      "Main effects tuning epoch: 8, train loss: 0.50966, val loss: 0.50552\n",
      "Main effects tuning epoch: 9, train loss: 0.50945, val loss: 0.50590\n",
      "Main effects tuning epoch: 10, train loss: 0.50940, val loss: 0.50382\n",
      "Main effects tuning epoch: 11, train loss: 0.50917, val loss: 0.50550\n",
      "Main effects tuning epoch: 12, train loss: 0.50899, val loss: 0.50380\n",
      "Main effects tuning epoch: 13, train loss: 0.50861, val loss: 0.50436\n",
      "Main effects tuning epoch: 14, train loss: 0.50856, val loss: 0.50383\n",
      "Main effects tuning epoch: 15, train loss: 0.50871, val loss: 0.50277\n",
      "Main effects tuning epoch: 16, train loss: 0.50847, val loss: 0.50462\n",
      "Main effects tuning epoch: 17, train loss: 0.50803, val loss: 0.50376\n",
      "Main effects tuning epoch: 18, train loss: 0.50807, val loss: 0.50397\n",
      "Main effects tuning epoch: 19, train loss: 0.50852, val loss: 0.50288\n",
      "Main effects tuning epoch: 20, train loss: 0.50882, val loss: 0.50602\n",
      "Main effects tuning epoch: 21, train loss: 0.50809, val loss: 0.50335\n",
      "Main effects tuning epoch: 22, train loss: 0.50791, val loss: 0.50456\n",
      "Main effects tuning epoch: 23, train loss: 0.50802, val loss: 0.50221\n",
      "Main effects tuning epoch: 24, train loss: 0.50768, val loss: 0.50406\n",
      "Main effects tuning epoch: 25, train loss: 0.50848, val loss: 0.50253\n",
      "Main effects tuning epoch: 26, train loss: 0.50928, val loss: 0.50877\n",
      "Main effects tuning epoch: 27, train loss: 0.50888, val loss: 0.50085\n",
      "Main effects tuning epoch: 28, train loss: 0.50879, val loss: 0.50494\n",
      "Main effects tuning epoch: 29, train loss: 0.50751, val loss: 0.50218\n",
      "Main effects tuning epoch: 30, train loss: 0.50724, val loss: 0.50440\n",
      "Main effects tuning epoch: 31, train loss: 0.50705, val loss: 0.50286\n",
      "Main effects tuning epoch: 32, train loss: 0.50702, val loss: 0.50255\n",
      "Main effects tuning epoch: 33, train loss: 0.50685, val loss: 0.50360\n",
      "Main effects tuning epoch: 34, train loss: 0.50668, val loss: 0.50246\n",
      "Main effects tuning epoch: 35, train loss: 0.50697, val loss: 0.50386\n",
      "Main effects tuning epoch: 36, train loss: 0.50675, val loss: 0.50379\n",
      "Main effects tuning epoch: 37, train loss: 0.50695, val loss: 0.50331\n",
      "Main effects tuning epoch: 38, train loss: 0.50657, val loss: 0.50400\n",
      "Main effects tuning epoch: 39, train loss: 0.50659, val loss: 0.50256\n",
      "Main effects tuning epoch: 40, train loss: 0.50632, val loss: 0.50253\n",
      "Main effects tuning epoch: 41, train loss: 0.50603, val loss: 0.50329\n",
      "Main effects tuning epoch: 42, train loss: 0.50622, val loss: 0.50349\n",
      "Main effects tuning epoch: 43, train loss: 0.50646, val loss: 0.50175\n",
      "Main effects tuning epoch: 44, train loss: 0.50628, val loss: 0.50337\n",
      "Main effects tuning epoch: 45, train loss: 0.50623, val loss: 0.50398\n",
      "Main effects tuning epoch: 46, train loss: 0.50598, val loss: 0.50309\n",
      "Main effects tuning epoch: 47, train loss: 0.50625, val loss: 0.50238\n",
      "Main effects tuning epoch: 48, train loss: 0.50597, val loss: 0.50462\n",
      "Main effects tuning epoch: 49, train loss: 0.50541, val loss: 0.50240\n",
      "Main effects tuning epoch: 50, train loss: 0.50550, val loss: 0.50238\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.50580, val loss: 0.50801\n",
      "Interaction training epoch: 2, train loss: 0.34656, val loss: 0.39379\n",
      "Interaction training epoch: 3, train loss: 0.29999, val loss: 0.33691\n",
      "Interaction training epoch: 4, train loss: 0.28770, val loss: 0.32530\n",
      "Interaction training epoch: 5, train loss: 0.28299, val loss: 0.32418\n",
      "Interaction training epoch: 6, train loss: 0.28379, val loss: 0.32729\n",
      "Interaction training epoch: 7, train loss: 0.27357, val loss: 0.31389\n",
      "Interaction training epoch: 8, train loss: 0.27754, val loss: 0.31979\n",
      "Interaction training epoch: 9, train loss: 0.29029, val loss: 0.33285\n",
      "Interaction training epoch: 10, train loss: 0.28556, val loss: 0.32375\n",
      "Interaction training epoch: 11, train loss: 0.28311, val loss: 0.32327\n",
      "Interaction training epoch: 12, train loss: 0.27458, val loss: 0.32112\n",
      "Interaction training epoch: 13, train loss: 0.26988, val loss: 0.31259\n",
      "Interaction training epoch: 14, train loss: 0.27124, val loss: 0.31641\n",
      "Interaction training epoch: 15, train loss: 0.27299, val loss: 0.31679\n",
      "Interaction training epoch: 16, train loss: 0.26703, val loss: 0.31132\n",
      "Interaction training epoch: 17, train loss: 0.26694, val loss: 0.30903\n",
      "Interaction training epoch: 18, train loss: 0.26685, val loss: 0.30861\n",
      "Interaction training epoch: 19, train loss: 0.27183, val loss: 0.31801\n",
      "Interaction training epoch: 20, train loss: 0.26692, val loss: 0.31376\n",
      "Interaction training epoch: 21, train loss: 0.26519, val loss: 0.31202\n",
      "Interaction training epoch: 22, train loss: 0.26267, val loss: 0.30618\n",
      "Interaction training epoch: 23, train loss: 0.26370, val loss: 0.30542\n",
      "Interaction training epoch: 24, train loss: 0.26475, val loss: 0.30630\n",
      "Interaction training epoch: 25, train loss: 0.27164, val loss: 0.32278\n",
      "Interaction training epoch: 26, train loss: 0.26176, val loss: 0.30448\n",
      "Interaction training epoch: 27, train loss: 0.26113, val loss: 0.30408\n",
      "Interaction training epoch: 28, train loss: 0.26016, val loss: 0.30552\n",
      "Interaction training epoch: 29, train loss: 0.26269, val loss: 0.31028\n",
      "Interaction training epoch: 30, train loss: 0.26000, val loss: 0.30555\n",
      "Interaction training epoch: 31, train loss: 0.26747, val loss: 0.31338\n",
      "Interaction training epoch: 32, train loss: 0.25839, val loss: 0.30641\n",
      "Interaction training epoch: 33, train loss: 0.25543, val loss: 0.30234\n",
      "Interaction training epoch: 34, train loss: 0.25768, val loss: 0.30462\n",
      "Interaction training epoch: 35, train loss: 0.26649, val loss: 0.31374\n",
      "Interaction training epoch: 36, train loss: 0.25585, val loss: 0.30895\n",
      "Interaction training epoch: 37, train loss: 0.25782, val loss: 0.29983\n",
      "Interaction training epoch: 38, train loss: 0.26261, val loss: 0.31868\n",
      "Interaction training epoch: 39, train loss: 0.25275, val loss: 0.30158\n",
      "Interaction training epoch: 40, train loss: 0.26383, val loss: 0.31293\n",
      "Interaction training epoch: 41, train loss: 0.26336, val loss: 0.31193\n",
      "Interaction training epoch: 42, train loss: 0.25663, val loss: 0.30529\n",
      "Interaction training epoch: 43, train loss: 0.25437, val loss: 0.30280\n",
      "Interaction training epoch: 44, train loss: 0.25441, val loss: 0.30275\n",
      "Interaction training epoch: 45, train loss: 0.25410, val loss: 0.30605\n",
      "Interaction training epoch: 46, train loss: 0.25173, val loss: 0.30335\n",
      "Interaction training epoch: 47, train loss: 0.25331, val loss: 0.30385\n",
      "Interaction training epoch: 48, train loss: 0.25174, val loss: 0.29903\n",
      "Interaction training epoch: 49, train loss: 0.25557, val loss: 0.30494\n",
      "Interaction training epoch: 50, train loss: 0.25638, val loss: 0.30963\n",
      "Interaction training epoch: 51, train loss: 0.25257, val loss: 0.30435\n",
      "Interaction training epoch: 52, train loss: 0.25014, val loss: 0.30301\n",
      "Interaction training epoch: 53, train loss: 0.24977, val loss: 0.30287\n",
      "Interaction training epoch: 54, train loss: 0.25441, val loss: 0.31209\n",
      "Interaction training epoch: 55, train loss: 0.24687, val loss: 0.29983\n",
      "Interaction training epoch: 56, train loss: 0.24953, val loss: 0.29925\n",
      "Interaction training epoch: 57, train loss: 0.24778, val loss: 0.30181\n",
      "Interaction training epoch: 58, train loss: 0.24699, val loss: 0.30298\n",
      "Interaction training epoch: 59, train loss: 0.24889, val loss: 0.30129\n",
      "Interaction training epoch: 60, train loss: 0.25205, val loss: 0.30823\n",
      "Interaction training epoch: 61, train loss: 0.24520, val loss: 0.29412\n",
      "Interaction training epoch: 62, train loss: 0.25322, val loss: 0.30735\n",
      "Interaction training epoch: 63, train loss: 0.24565, val loss: 0.29623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 64, train loss: 0.24660, val loss: 0.29969\n",
      "Interaction training epoch: 65, train loss: 0.24540, val loss: 0.30491\n",
      "Interaction training epoch: 66, train loss: 0.24559, val loss: 0.29591\n",
      "Interaction training epoch: 67, train loss: 0.24852, val loss: 0.30734\n",
      "Interaction training epoch: 68, train loss: 0.24377, val loss: 0.29573\n",
      "Interaction training epoch: 69, train loss: 0.24821, val loss: 0.30075\n",
      "Interaction training epoch: 70, train loss: 0.24742, val loss: 0.30412\n",
      "Interaction training epoch: 71, train loss: 0.24427, val loss: 0.29924\n",
      "Interaction training epoch: 72, train loss: 0.24429, val loss: 0.29884\n",
      "Interaction training epoch: 73, train loss: 0.24638, val loss: 0.29742\n",
      "Interaction training epoch: 74, train loss: 0.24486, val loss: 0.30404\n",
      "Interaction training epoch: 75, train loss: 0.24115, val loss: 0.29343\n",
      "Interaction training epoch: 76, train loss: 0.24412, val loss: 0.29766\n",
      "Interaction training epoch: 77, train loss: 0.24124, val loss: 0.29895\n",
      "Interaction training epoch: 78, train loss: 0.24070, val loss: 0.29917\n",
      "Interaction training epoch: 79, train loss: 0.24339, val loss: 0.29865\n",
      "Interaction training epoch: 80, train loss: 0.24094, val loss: 0.29583\n",
      "Interaction training epoch: 81, train loss: 0.24309, val loss: 0.30035\n",
      "Interaction training epoch: 82, train loss: 0.24769, val loss: 0.30599\n",
      "Interaction training epoch: 83, train loss: 0.24209, val loss: 0.29959\n",
      "Interaction training epoch: 84, train loss: 0.24369, val loss: 0.30160\n",
      "Interaction training epoch: 85, train loss: 0.24060, val loss: 0.29935\n",
      "Interaction training epoch: 86, train loss: 0.23943, val loss: 0.29646\n",
      "Interaction training epoch: 87, train loss: 0.24208, val loss: 0.30186\n",
      "Interaction training epoch: 88, train loss: 0.24341, val loss: 0.29839\n",
      "Interaction training epoch: 89, train loss: 0.24235, val loss: 0.30510\n",
      "Interaction training epoch: 90, train loss: 0.24281, val loss: 0.29807\n",
      "Interaction training epoch: 91, train loss: 0.23938, val loss: 0.29484\n",
      "Interaction training epoch: 92, train loss: 0.24091, val loss: 0.30139\n",
      "Interaction training epoch: 93, train loss: 0.24318, val loss: 0.29547\n",
      "Interaction training epoch: 94, train loss: 0.23906, val loss: 0.30236\n",
      "Interaction training epoch: 95, train loss: 0.24073, val loss: 0.29539\n",
      "Interaction training epoch: 96, train loss: 0.23837, val loss: 0.29922\n",
      "Interaction training epoch: 97, train loss: 0.23751, val loss: 0.29531\n",
      "Interaction training epoch: 98, train loss: 0.23764, val loss: 0.30228\n",
      "Interaction training epoch: 99, train loss: 0.23515, val loss: 0.29102\n",
      "Interaction training epoch: 100, train loss: 0.23863, val loss: 0.29484\n",
      "Interaction training epoch: 101, train loss: 0.23579, val loss: 0.29580\n",
      "Interaction training epoch: 102, train loss: 0.23780, val loss: 0.29415\n",
      "Interaction training epoch: 103, train loss: 0.23469, val loss: 0.29298\n",
      "Interaction training epoch: 104, train loss: 0.23612, val loss: 0.29454\n",
      "Interaction training epoch: 105, train loss: 0.23845, val loss: 0.29815\n",
      "Interaction training epoch: 106, train loss: 0.23848, val loss: 0.30360\n",
      "Interaction training epoch: 107, train loss: 0.23617, val loss: 0.29343\n",
      "Interaction training epoch: 108, train loss: 0.23898, val loss: 0.30126\n",
      "Interaction training epoch: 109, train loss: 0.23805, val loss: 0.30199\n",
      "Interaction training epoch: 110, train loss: 0.23853, val loss: 0.29364\n",
      "Interaction training epoch: 111, train loss: 0.23500, val loss: 0.29722\n",
      "Interaction training epoch: 112, train loss: 0.24232, val loss: 0.30874\n",
      "Interaction training epoch: 113, train loss: 0.23490, val loss: 0.29137\n",
      "Interaction training epoch: 114, train loss: 0.23576, val loss: 0.29757\n",
      "Interaction training epoch: 115, train loss: 0.23460, val loss: 0.29390\n",
      "Interaction training epoch: 116, train loss: 0.23795, val loss: 0.30198\n",
      "Interaction training epoch: 117, train loss: 0.23241, val loss: 0.28800\n",
      "Interaction training epoch: 118, train loss: 0.24199, val loss: 0.30748\n",
      "Interaction training epoch: 119, train loss: 0.23135, val loss: 0.29144\n",
      "Interaction training epoch: 120, train loss: 0.23445, val loss: 0.29689\n",
      "Interaction training epoch: 121, train loss: 0.23188, val loss: 0.29187\n",
      "Interaction training epoch: 122, train loss: 0.23281, val loss: 0.29739\n",
      "Interaction training epoch: 123, train loss: 0.23300, val loss: 0.29392\n",
      "Interaction training epoch: 124, train loss: 0.23484, val loss: 0.29686\n",
      "Interaction training epoch: 125, train loss: 0.23115, val loss: 0.29331\n",
      "Interaction training epoch: 126, train loss: 0.23350, val loss: 0.29713\n",
      "Interaction training epoch: 127, train loss: 0.23149, val loss: 0.29289\n",
      "Interaction training epoch: 128, train loss: 0.23202, val loss: 0.29505\n",
      "Interaction training epoch: 129, train loss: 0.23298, val loss: 0.29774\n",
      "Interaction training epoch: 130, train loss: 0.23157, val loss: 0.29577\n",
      "Interaction training epoch: 131, train loss: 0.23337, val loss: 0.28962\n",
      "Interaction training epoch: 132, train loss: 0.23494, val loss: 0.30323\n",
      "Interaction training epoch: 133, train loss: 0.22883, val loss: 0.29340\n",
      "Interaction training epoch: 134, train loss: 0.23366, val loss: 0.29245\n",
      "Interaction training epoch: 135, train loss: 0.23040, val loss: 0.29398\n",
      "Interaction training epoch: 136, train loss: 0.23082, val loss: 0.29391\n",
      "Interaction training epoch: 137, train loss: 0.22843, val loss: 0.29077\n",
      "Interaction training epoch: 138, train loss: 0.23185, val loss: 0.29879\n",
      "Interaction training epoch: 139, train loss: 0.23106, val loss: 0.29535\n",
      "Interaction training epoch: 140, train loss: 0.23663, val loss: 0.29488\n",
      "Interaction training epoch: 141, train loss: 0.23725, val loss: 0.31145\n",
      "Interaction training epoch: 142, train loss: 0.23658, val loss: 0.29807\n",
      "Interaction training epoch: 143, train loss: 0.22871, val loss: 0.29071\n",
      "Interaction training epoch: 144, train loss: 0.22831, val loss: 0.28938\n",
      "Interaction training epoch: 145, train loss: 0.23027, val loss: 0.29908\n",
      "Interaction training epoch: 146, train loss: 0.22858, val loss: 0.28964\n",
      "Interaction training epoch: 147, train loss: 0.23073, val loss: 0.29526\n",
      "Interaction training epoch: 148, train loss: 0.22687, val loss: 0.29358\n",
      "Interaction training epoch: 149, train loss: 0.22846, val loss: 0.29240\n",
      "Interaction training epoch: 150, train loss: 0.22715, val loss: 0.28664\n",
      "Interaction training epoch: 151, train loss: 0.23035, val loss: 0.30206\n",
      "Interaction training epoch: 152, train loss: 0.23237, val loss: 0.29130\n",
      "Interaction training epoch: 153, train loss: 0.22804, val loss: 0.29298\n",
      "Interaction training epoch: 154, train loss: 0.22793, val loss: 0.28891\n",
      "Interaction training epoch: 155, train loss: 0.22848, val loss: 0.28747\n",
      "Interaction training epoch: 156, train loss: 0.22574, val loss: 0.29213\n",
      "Interaction training epoch: 157, train loss: 0.22651, val loss: 0.29595\n",
      "Interaction training epoch: 158, train loss: 0.22552, val loss: 0.28698\n",
      "Interaction training epoch: 159, train loss: 0.22737, val loss: 0.29431\n",
      "Interaction training epoch: 160, train loss: 0.22536, val loss: 0.28874\n",
      "Interaction training epoch: 161, train loss: 0.22730, val loss: 0.29113\n",
      "Interaction training epoch: 162, train loss: 0.22541, val loss: 0.29153\n",
      "Interaction training epoch: 163, train loss: 0.22806, val loss: 0.29075\n",
      "Interaction training epoch: 164, train loss: 0.22404, val loss: 0.29066\n",
      "Interaction training epoch: 165, train loss: 0.22759, val loss: 0.28958\n",
      "Interaction training epoch: 166, train loss: 0.22866, val loss: 0.29264\n",
      "Interaction training epoch: 167, train loss: 0.22495, val loss: 0.29264\n",
      "Interaction training epoch: 168, train loss: 0.22417, val loss: 0.28617\n",
      "Interaction training epoch: 169, train loss: 0.22610, val loss: 0.29390\n",
      "Interaction training epoch: 170, train loss: 0.22733, val loss: 0.28856\n",
      "Interaction training epoch: 171, train loss: 0.23014, val loss: 0.30113\n",
      "Interaction training epoch: 172, train loss: 0.22205, val loss: 0.28163\n",
      "Interaction training epoch: 173, train loss: 0.22928, val loss: 0.29936\n",
      "Interaction training epoch: 174, train loss: 0.22576, val loss: 0.28850\n",
      "Interaction training epoch: 175, train loss: 0.22910, val loss: 0.29666\n",
      "Interaction training epoch: 176, train loss: 0.22956, val loss: 0.29006\n",
      "Interaction training epoch: 177, train loss: 0.22545, val loss: 0.29321\n",
      "Interaction training epoch: 178, train loss: 0.22914, val loss: 0.29071\n",
      "Interaction training epoch: 179, train loss: 0.22453, val loss: 0.29079\n",
      "Interaction training epoch: 180, train loss: 0.22396, val loss: 0.29007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 181, train loss: 0.22264, val loss: 0.29131\n",
      "Interaction training epoch: 182, train loss: 0.22576, val loss: 0.28749\n",
      "Interaction training epoch: 183, train loss: 0.22537, val loss: 0.29781\n",
      "Interaction training epoch: 184, train loss: 0.22537, val loss: 0.28747\n",
      "Interaction training epoch: 185, train loss: 0.22236, val loss: 0.28802\n",
      "Interaction training epoch: 186, train loss: 0.22547, val loss: 0.28981\n",
      "Interaction training epoch: 187, train loss: 0.22661, val loss: 0.29056\n",
      "Interaction training epoch: 188, train loss: 0.23080, val loss: 0.30164\n",
      "Interaction training epoch: 189, train loss: 0.22038, val loss: 0.28479\n",
      "Interaction training epoch: 190, train loss: 0.22638, val loss: 0.29376\n",
      "Interaction training epoch: 191, train loss: 0.22482, val loss: 0.28472\n",
      "Interaction training epoch: 192, train loss: 0.22470, val loss: 0.29453\n",
      "Interaction training epoch: 193, train loss: 0.22504, val loss: 0.28936\n",
      "Interaction training epoch: 194, train loss: 0.22130, val loss: 0.28942\n",
      "Interaction training epoch: 195, train loss: 0.22154, val loss: 0.28426\n",
      "Interaction training epoch: 196, train loss: 0.22168, val loss: 0.29193\n",
      "Interaction training epoch: 197, train loss: 0.21950, val loss: 0.28586\n",
      "Interaction training epoch: 198, train loss: 0.22612, val loss: 0.29071\n",
      "Interaction training epoch: 199, train loss: 0.22341, val loss: 0.29090\n",
      "Interaction training epoch: 200, train loss: 0.22376, val loss: 0.29467\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########2 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.22629, val loss: 0.28210\n",
      "Interaction tuning epoch: 2, train loss: 0.23218, val loss: 0.28865\n",
      "Interaction tuning epoch: 3, train loss: 0.23027, val loss: 0.28684\n",
      "Interaction tuning epoch: 4, train loss: 0.22551, val loss: 0.28637\n",
      "Interaction tuning epoch: 5, train loss: 0.23198, val loss: 0.28529\n",
      "Interaction tuning epoch: 6, train loss: 0.22615, val loss: 0.28333\n",
      "Interaction tuning epoch: 7, train loss: 0.22712, val loss: 0.28630\n",
      "Interaction tuning epoch: 8, train loss: 0.22799, val loss: 0.28472\n",
      "Interaction tuning epoch: 9, train loss: 0.22489, val loss: 0.28128\n",
      "Interaction tuning epoch: 10, train loss: 0.22541, val loss: 0.28244\n",
      "Interaction tuning epoch: 11, train loss: 0.22602, val loss: 0.27925\n",
      "Interaction tuning epoch: 12, train loss: 0.22670, val loss: 0.29076\n",
      "Interaction tuning epoch: 13, train loss: 0.22426, val loss: 0.28111\n",
      "Interaction tuning epoch: 14, train loss: 0.22582, val loss: 0.28042\n",
      "Interaction tuning epoch: 15, train loss: 0.22589, val loss: 0.28594\n",
      "Interaction tuning epoch: 16, train loss: 0.22606, val loss: 0.28283\n",
      "Interaction tuning epoch: 17, train loss: 0.22481, val loss: 0.28786\n",
      "Interaction tuning epoch: 18, train loss: 0.22462, val loss: 0.28037\n",
      "Interaction tuning epoch: 19, train loss: 0.22566, val loss: 0.28416\n",
      "Interaction tuning epoch: 20, train loss: 0.22378, val loss: 0.28258\n",
      "Interaction tuning epoch: 21, train loss: 0.22375, val loss: 0.28170\n",
      "Interaction tuning epoch: 22, train loss: 0.22561, val loss: 0.28319\n",
      "Interaction tuning epoch: 23, train loss: 0.22920, val loss: 0.28950\n",
      "Interaction tuning epoch: 24, train loss: 0.22236, val loss: 0.28384\n",
      "Interaction tuning epoch: 25, train loss: 0.23033, val loss: 0.28819\n",
      "Interaction tuning epoch: 26, train loss: 0.22553, val loss: 0.28864\n",
      "Interaction tuning epoch: 27, train loss: 0.22641, val loss: 0.28500\n",
      "Interaction tuning epoch: 28, train loss: 0.22460, val loss: 0.28029\n",
      "Interaction tuning epoch: 29, train loss: 0.22403, val loss: 0.28690\n",
      "Interaction tuning epoch: 30, train loss: 0.22708, val loss: 0.28584\n",
      "Interaction tuning epoch: 31, train loss: 0.22387, val loss: 0.28496\n",
      "Interaction tuning epoch: 32, train loss: 0.22281, val loss: 0.27892\n",
      "Interaction tuning epoch: 33, train loss: 0.22474, val loss: 0.28373\n",
      "Interaction tuning epoch: 34, train loss: 0.22002, val loss: 0.27833\n",
      "Interaction tuning epoch: 35, train loss: 0.22384, val loss: 0.28530\n",
      "Interaction tuning epoch: 36, train loss: 0.22596, val loss: 0.28605\n",
      "Interaction tuning epoch: 37, train loss: 0.22703, val loss: 0.28666\n",
      "Interaction tuning epoch: 38, train loss: 0.22724, val loss: 0.28528\n",
      "Interaction tuning epoch: 39, train loss: 0.22436, val loss: 0.28874\n",
      "Interaction tuning epoch: 40, train loss: 0.22319, val loss: 0.27974\n",
      "Interaction tuning epoch: 41, train loss: 0.22896, val loss: 0.29446\n",
      "Interaction tuning epoch: 42, train loss: 0.22749, val loss: 0.28418\n",
      "Interaction tuning epoch: 43, train loss: 0.22348, val loss: 0.28656\n",
      "Interaction tuning epoch: 44, train loss: 0.22128, val loss: 0.27838\n",
      "Interaction tuning epoch: 45, train loss: 0.23052, val loss: 0.29354\n",
      "Interaction tuning epoch: 46, train loss: 0.22250, val loss: 0.28240\n",
      "Interaction tuning epoch: 47, train loss: 0.22486, val loss: 0.28409\n",
      "Interaction tuning epoch: 48, train loss: 0.22435, val loss: 0.28467\n",
      "Interaction tuning epoch: 49, train loss: 0.21889, val loss: 0.28132\n",
      "Interaction tuning epoch: 50, train loss: 0.22415, val loss: 0.28047\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 37.63217282295227\n",
      "After the gam stage, training error is 0.22415 , validation error is 0.28047\n",
      "missing value counts: 99174\n",
      "[SoftImpute] Max Singular Value of X_init = 3.497793\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.191369 validation BCE=0.285229,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 2: observed BCE=0.187333 validation BCE=0.273448,rank=5\n",
      "[SoftImpute] Iter 3: observed BCE=0.184637 validation BCE=0.270969,rank=5\n",
      "[SoftImpute] Iter 4: observed BCE=0.183213 validation BCE=0.269065,rank=5\n",
      "[SoftImpute] Iter 5: observed BCE=0.181971 validation BCE=0.267795,rank=5\n",
      "[SoftImpute] Iter 6: observed BCE=0.181136 validation BCE=0.266742,rank=5\n",
      "[SoftImpute] Iter 7: observed BCE=0.180673 validation BCE=0.265993,rank=5\n",
      "[SoftImpute] Iter 8: observed BCE=0.180399 validation BCE=0.265429,rank=5\n",
      "[SoftImpute] Iter 9: observed BCE=0.180201 validation BCE=0.264957,rank=5\n",
      "[SoftImpute] Iter 10: observed BCE=0.180111 validation BCE=0.264663,rank=5\n",
      "[SoftImpute] Iter 11: observed BCE=0.180068 validation BCE=0.264166,rank=5\n",
      "[SoftImpute] Iter 12: observed BCE=0.180053 validation BCE=0.264152,rank=5\n",
      "[SoftImpute] Iter 13: observed BCE=0.179727 validation BCE=0.263595,rank=5\n",
      "[SoftImpute] Iter 14: observed BCE=0.179857 validation BCE=0.263588,rank=5\n",
      "[SoftImpute] Iter 15: observed BCE=0.179479 validation BCE=0.263257,rank=5\n",
      "[SoftImpute] Iter 16: observed BCE=0.179671 validation BCE=0.263453,rank=5\n",
      "[SoftImpute] Iter 17: observed BCE=0.179421 validation BCE=0.262956,rank=5\n",
      "[SoftImpute] Iter 18: observed BCE=0.179346 validation BCE=0.263044,rank=5\n",
      "[SoftImpute] Iter 19: observed BCE=0.178671 validation BCE=0.262924,rank=5\n",
      "[SoftImpute] Iter 20: observed BCE=0.178931 validation BCE=0.262718,rank=5\n",
      "[SoftImpute] Iter 21: observed BCE=0.179095 validation BCE=0.262895,rank=5\n",
      "[SoftImpute] Iter 22: observed BCE=0.178852 validation BCE=0.263214,rank=5\n",
      "[SoftImpute] Iter 23: observed BCE=0.178754 validation BCE=0.262952,rank=5\n",
      "[SoftImpute] Iter 24: observed BCE=0.178993 validation BCE=0.263398,rank=5\n",
      "[SoftImpute] Iter 25: observed BCE=0.178828 validation BCE=0.263102,rank=5\n",
      "[SoftImpute] Iter 26: observed BCE=0.178750 validation BCE=0.263235,rank=5\n",
      "[SoftImpute] Iter 27: observed BCE=0.178598 validation BCE=0.263437,rank=5\n",
      "[SoftImpute] Iter 28: observed BCE=0.178516 validation BCE=0.263345,rank=5\n",
      "[SoftImpute] Iter 29: observed BCE=0.178556 validation BCE=0.263305,rank=5\n",
      "[SoftImpute] Iter 30: observed BCE=0.178564 validation BCE=0.263535,rank=5\n",
      "[SoftImpute] Iter 31: observed BCE=0.178287 validation BCE=0.263319,rank=5\n",
      "[SoftImpute] Iter 32: observed BCE=0.178413 validation BCE=0.263229,rank=5\n",
      "[SoftImpute] Iter 33: observed BCE=0.178521 validation BCE=0.264011,rank=5\n",
      "[SoftImpute] Iter 34: observed BCE=0.178857 validation BCE=0.264073,rank=5\n",
      "[SoftImpute] Iter 35: observed BCE=0.179062 validation BCE=0.263888,rank=5\n",
      "[SoftImpute] Iter 36: observed BCE=0.179482 validation BCE=0.264663,rank=5\n",
      "[SoftImpute] Iter 37: observed BCE=0.179162 validation BCE=0.264333,rank=5\n",
      "[SoftImpute] Iter 38: observed BCE=0.179250 validation BCE=0.264724,rank=5\n",
      "[SoftImpute] Iter 39: observed BCE=0.178973 validation BCE=0.264415,rank=5\n",
      "[SoftImpute] Iter 40: observed BCE=0.178993 validation BCE=0.264681,rank=5\n",
      "[SoftImpute] Iter 41: observed BCE=0.178816 validation BCE=0.264560,rank=5\n",
      "[SoftImpute] Iter 42: observed BCE=0.179102 validation BCE=0.264290,rank=5\n",
      "[SoftImpute] Iter 43: observed BCE=0.178916 validation BCE=0.264752,rank=5\n",
      "[SoftImpute] Iter 44: observed BCE=0.178788 validation BCE=0.264581,rank=5\n",
      "[SoftImpute] Iter 45: observed BCE=0.178802 validation BCE=0.264927,rank=5\n",
      "[SoftImpute] Iter 46: observed BCE=0.178843 validation BCE=0.265053,rank=5\n",
      "[SoftImpute] Iter 47: observed BCE=0.178823 validation BCE=0.264722,rank=5\n",
      "[SoftImpute] Iter 48: observed BCE=0.178906 validation BCE=0.265048,rank=5\n",
      "[SoftImpute] Iter 49: observed BCE=0.178678 validation BCE=0.264890,rank=5\n",
      "[SoftImpute] Iter 50: observed BCE=0.178541 validation BCE=0.264872,rank=5\n",
      "[SoftImpute] Iter 51: observed BCE=0.178586 validation BCE=0.264803,rank=5\n",
      "[SoftImpute] Iter 52: observed BCE=0.178472 validation BCE=0.264780,rank=5\n",
      "[SoftImpute] Iter 53: observed BCE=0.178479 validation BCE=0.265161,rank=5\n",
      "[SoftImpute] Iter 54: observed BCE=0.178408 validation BCE=0.265159,rank=5\n",
      "[SoftImpute] Iter 55: observed BCE=0.178262 validation BCE=0.265120,rank=5\n",
      "[SoftImpute] Iter 56: observed BCE=0.178074 validation BCE=0.264882,rank=5\n",
      "[SoftImpute] Iter 57: observed BCE=0.178481 validation BCE=0.265030,rank=5\n",
      "[SoftImpute] Iter 58: observed BCE=0.178415 validation BCE=0.265256,rank=5\n",
      "[SoftImpute] Iter 59: observed BCE=0.178210 validation BCE=0.265269,rank=5\n",
      "[SoftImpute] Stopped after iteration 59 for lambda=0.069956\n",
      "final num of user group: 21\n",
      "final num of item group: 26\n",
      "change mode state : True\n",
      "time cost: 17.403693675994873\n",
      "After the matrix factor stage, training error is 0.17821, validation error is 0.26527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 15:28:47.629149 18224 deprecation.py:506] From ../benchmark/deepfm\\DeepFM.py:93: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002051F0461E0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002051D867950>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002051F0461E0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002051D867950>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002051F0461E0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002051D867950>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002051F0461E0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002051D867950>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002051F0461E0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002051D867950>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002051F0461E0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002051D867950>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002051F0461E0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002051D867950>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002051F0461E0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002051D867950>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002051F0461E0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002051D867950>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002051F0461E0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002051D867950>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002051F0461E0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002051D867950>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002051F0461E0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002051D867950>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002051F0461E0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002051D867950>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002051F0461E0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002051D867950>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002051F0461E0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002051D867950>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002051F0461E0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002051D867950>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002051F0461E0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002051D867950>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002051F0461E0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002051D867950>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002051F0461E0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002051D867950>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_size': 5, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002051F0461E0>, 'loss_type': 'mse', 'epoch': 300, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function mean_absolute_error at 0x000002051D867950>, 'random_seed': 0, 'feature_size': 1110, 'field_size': 12}\n"
     ]
    }
   ],
   "source": [
    "result_lvxnn = lvxnn('warm',train, test, tr_x, tr_Xi, tr_y , te_x , te_Xi, te_y, meta_info, model_info, task_type , val_ratio=0.2, random_state=0,params=lx_params)\n",
    "result_svd = svd('warm',train, test, tr_x, tr_Xi, tr_y , te_x , te_Xi, te_y, meta_info, model_info, task_type, val_ratio=0.2, random_state=0)\n",
    "result_deepfm, result_fm = deepfm_fm('warm',train, test, tr_x, tr_Xi, tr_y , te_x , te_Xi, te_y, meta_info, model_info, task_type, val_ratio=0.2, random_state=0,epochs=300)\n",
    "result_xgb = xgb('warm',train, test, tr_x, tr_Xi, tr_y , te_x , te_Xi, te_y, meta_info, model_info, task_type, val_ratio=0.2, random_state=0)\n",
    "\n",
    "result_sim_re = pd.concat([result_lvxnn,result_svd,result_xgb,result_deepfm,result_fm],0)\n",
    "\n",
    "result_sim_re.to_csv('simulation_classification_result.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>cold_auc</th>\n",
       "      <th>cold_logloss</th>\n",
       "      <th>std_cold_auc</th>\n",
       "      <th>std_cold_logloss</th>\n",
       "      <th>warm_auc</th>\n",
       "      <th>warm_logloss</th>\n",
       "      <th>std_warm_auc</th>\n",
       "      <th>std_warm_logloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVD</td>\n",
       "      <td>0.9547525787763178</td>\n",
       "      <td>0.3933188969768967</td>\n",
       "      <td>0.0008141953854002451</td>\n",
       "      <td>0.0007472300121341417</td>\n",
       "      <td>0.9620068086592178</td>\n",
       "      <td>0.36631430476200233</td>\n",
       "      <td>0.001050847744088807</td>\n",
       "      <td>0.0008315278510012899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model            cold_auc        cold_logloss           std_cold_auc  \\\n",
       "0   SVD  0.9547525787763178  0.3933188969768967  0.0008141953854002451   \n",
       "\n",
       "        std_cold_logloss            warm_auc         warm_logloss  \\\n",
       "0  0.0007472300121341417  0.9620068086592178  0.36631430476200233   \n",
       "\n",
       "           std_warm_auc       std_warm_logloss  \n",
       "0  0.001050847744088807  0.0008315278510012899  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_svd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
