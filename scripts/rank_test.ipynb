{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0808 15:10:08.683853 28208 deprecation.py:323] From C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error,roc_auc_score,mean_absolute_error,log_loss\n",
    "import sys\n",
    "sys.path.append('benchmark/')\n",
    "from gammli_test import gammli\n",
    "from xgb_test import xgb\n",
    "from svd_test import svd\n",
    "from deepfm_fm_test import deepfm_fm\n",
    "from rank_test import rtest\n",
    "sys.path.append('../')\n",
    "from gammli.GAMMLI import GAMMLI\n",
    "from gammli.DataReader import data_initialize\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "random_state = 0\n",
    "data= pd.read_csv('data/simulation/simulation_regression.csv')\n",
    "task_type = \"Regression\"\n",
    "\n",
    "meta_info = OrderedDict()\n",
    "\n",
    "meta_info['uf_1']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_2']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_3']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_4']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_5']={'type': 'continues','source':'user'}\n",
    "meta_info['if_1']={'type': 'continues','source':'item'}\n",
    "meta_info['if_2']={'type': 'continues','source':'item'}\n",
    "meta_info['if_3']={'type': 'continues','source':'item'}\n",
    "meta_info['if_4']={'type': 'continues','source':'item'}\n",
    "meta_info['if_5']={'type': 'continues','source':'item'}\n",
    "meta_info['user_id']={\"type\":\"id\",'source':'user'}\n",
    "meta_info['item_id']={\"type\":\"id\",'source':'item'}\n",
    "meta_info['target']={\"type\":\"target\",'source':''}\n",
    "#the best shrinkage is 0.917120\n",
    "#the best combination is 0.600000\n",
    "lx_params = {\n",
    "        \"rank\":3,\n",
    "        \"main_effect_epochs\":300,\n",
    "        \"interaction_epochs\" : 200 ,\n",
    "        \"tuning_epochs\" : 50 , \n",
    "        \"mf_training_iters\": 500,\n",
    "        \"u_group_num\":30,\n",
    "        \"i_group_num\":50,\n",
    "        \"auto_tune\":False,\n",
    "        \"best_shrinkage\":1,\n",
    "        \"best_combination\":0.4,\n",
    "        \"verbose\":True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.86 MB\n",
      "Memory usage after optimization is: 0.26 MB\n",
      "Decreased by 69.6%\n",
      "Memory usage of dataframe is 0.21 MB\n",
      "Memory usage after optimization is: 0.07 MB\n",
      "Decreased by 69.6%\n",
      "test cold start user: 0\n",
      "test cold start item: 3\n",
      "validation cold start user: 0\n",
      "validation cold start item: 1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.30185, val loss: 0.30478\n",
      "Main effects training epoch: 2, train loss: 0.23228, val loss: 0.23617\n",
      "Main effects training epoch: 3, train loss: 0.17997, val loss: 0.18315\n",
      "Main effects training epoch: 4, train loss: 0.14845, val loss: 0.15215\n",
      "Main effects training epoch: 5, train loss: 0.13484, val loss: 0.13695\n",
      "Main effects training epoch: 6, train loss: 0.13033, val loss: 0.13155\n",
      "Main effects training epoch: 7, train loss: 0.13055, val loss: 0.13149\n",
      "Main effects training epoch: 8, train loss: 0.12971, val loss: 0.12988\n",
      "Main effects training epoch: 9, train loss: 0.12862, val loss: 0.12919\n",
      "Main effects training epoch: 10, train loss: 0.12828, val loss: 0.12938\n",
      "Main effects training epoch: 11, train loss: 0.12732, val loss: 0.12789\n",
      "Main effects training epoch: 12, train loss: 0.12659, val loss: 0.12743\n",
      "Main effects training epoch: 13, train loss: 0.12563, val loss: 0.12654\n",
      "Main effects training epoch: 14, train loss: 0.12325, val loss: 0.12456\n",
      "Main effects training epoch: 15, train loss: 0.11949, val loss: 0.12161\n",
      "Main effects training epoch: 16, train loss: 0.11686, val loss: 0.11922\n",
      "Main effects training epoch: 17, train loss: 0.11423, val loss: 0.11562\n",
      "Main effects training epoch: 18, train loss: 0.11484, val loss: 0.11687\n",
      "Main effects training epoch: 19, train loss: 0.11188, val loss: 0.11473\n",
      "Main effects training epoch: 20, train loss: 0.11089, val loss: 0.11324\n",
      "Main effects training epoch: 21, train loss: 0.10849, val loss: 0.11127\n",
      "Main effects training epoch: 22, train loss: 0.10778, val loss: 0.11002\n",
      "Main effects training epoch: 23, train loss: 0.10761, val loss: 0.10934\n",
      "Main effects training epoch: 24, train loss: 0.10695, val loss: 0.10962\n",
      "Main effects training epoch: 25, train loss: 0.10869, val loss: 0.11093\n",
      "Main effects training epoch: 26, train loss: 0.10721, val loss: 0.11016\n",
      "Main effects training epoch: 27, train loss: 0.10692, val loss: 0.10911\n",
      "Main effects training epoch: 28, train loss: 0.10656, val loss: 0.10921\n",
      "Main effects training epoch: 29, train loss: 0.10666, val loss: 0.10915\n",
      "Main effects training epoch: 30, train loss: 0.10688, val loss: 0.10958\n",
      "Main effects training epoch: 31, train loss: 0.10689, val loss: 0.10943\n",
      "Main effects training epoch: 32, train loss: 0.10622, val loss: 0.10918\n",
      "Main effects training epoch: 33, train loss: 0.10631, val loss: 0.10928\n",
      "Main effects training epoch: 34, train loss: 0.10605, val loss: 0.10868\n",
      "Main effects training epoch: 35, train loss: 0.10599, val loss: 0.10879\n",
      "Main effects training epoch: 36, train loss: 0.10605, val loss: 0.10911\n",
      "Main effects training epoch: 37, train loss: 0.10704, val loss: 0.10981\n",
      "Main effects training epoch: 38, train loss: 0.10655, val loss: 0.10926\n",
      "Main effects training epoch: 39, train loss: 0.10607, val loss: 0.10888\n",
      "Main effects training epoch: 40, train loss: 0.10588, val loss: 0.10859\n",
      "Main effects training epoch: 41, train loss: 0.10600, val loss: 0.10889\n",
      "Main effects training epoch: 42, train loss: 0.10616, val loss: 0.10903\n",
      "Main effects training epoch: 43, train loss: 0.10609, val loss: 0.10883\n",
      "Main effects training epoch: 44, train loss: 0.10630, val loss: 0.10899\n",
      "Main effects training epoch: 45, train loss: 0.10686, val loss: 0.10928\n",
      "Main effects training epoch: 46, train loss: 0.10599, val loss: 0.10904\n",
      "Main effects training epoch: 47, train loss: 0.10577, val loss: 0.10887\n",
      "Main effects training epoch: 48, train loss: 0.10584, val loss: 0.10888\n",
      "Main effects training epoch: 49, train loss: 0.10576, val loss: 0.10866\n",
      "Main effects training epoch: 50, train loss: 0.10617, val loss: 0.10903\n",
      "Main effects training epoch: 51, train loss: 0.10592, val loss: 0.10893\n",
      "Main effects training epoch: 52, train loss: 0.10606, val loss: 0.10892\n",
      "Main effects training epoch: 53, train loss: 0.10577, val loss: 0.10888\n",
      "Main effects training epoch: 54, train loss: 0.10586, val loss: 0.10875\n",
      "Main effects training epoch: 55, train loss: 0.10635, val loss: 0.10929\n",
      "Main effects training epoch: 56, train loss: 0.10600, val loss: 0.10895\n",
      "Main effects training epoch: 57, train loss: 0.10614, val loss: 0.10942\n",
      "Main effects training epoch: 58, train loss: 0.10591, val loss: 0.10905\n",
      "Main effects training epoch: 59, train loss: 0.10587, val loss: 0.10866\n",
      "Main effects training epoch: 60, train loss: 0.10580, val loss: 0.10885\n",
      "Main effects training epoch: 61, train loss: 0.10590, val loss: 0.10868\n",
      "Main effects training epoch: 62, train loss: 0.10611, val loss: 0.10911\n",
      "Main effects training epoch: 63, train loss: 0.10587, val loss: 0.10877\n",
      "Main effects training epoch: 64, train loss: 0.10611, val loss: 0.10908\n",
      "Main effects training epoch: 65, train loss: 0.10597, val loss: 0.10888\n",
      "Main effects training epoch: 66, train loss: 0.10579, val loss: 0.10886\n",
      "Main effects training epoch: 67, train loss: 0.10588, val loss: 0.10853\n",
      "Main effects training epoch: 68, train loss: 0.10601, val loss: 0.10926\n",
      "Main effects training epoch: 69, train loss: 0.10583, val loss: 0.10866\n",
      "Main effects training epoch: 70, train loss: 0.10585, val loss: 0.10893\n",
      "Main effects training epoch: 71, train loss: 0.10592, val loss: 0.10879\n",
      "Main effects training epoch: 72, train loss: 0.10582, val loss: 0.10901\n",
      "Main effects training epoch: 73, train loss: 0.10586, val loss: 0.10890\n",
      "Main effects training epoch: 74, train loss: 0.10596, val loss: 0.10893\n",
      "Main effects training epoch: 75, train loss: 0.10614, val loss: 0.10884\n",
      "Main effects training epoch: 76, train loss: 0.10583, val loss: 0.10904\n",
      "Main effects training epoch: 77, train loss: 0.10592, val loss: 0.10890\n",
      "Main effects training epoch: 78, train loss: 0.10580, val loss: 0.10881\n",
      "Main effects training epoch: 79, train loss: 0.10594, val loss: 0.10889\n",
      "Main effects training epoch: 80, train loss: 0.10607, val loss: 0.10888\n",
      "Main effects training epoch: 81, train loss: 0.10581, val loss: 0.10891\n",
      "Main effects training epoch: 82, train loss: 0.10593, val loss: 0.10922\n",
      "Main effects training epoch: 83, train loss: 0.10574, val loss: 0.10873\n",
      "Main effects training epoch: 84, train loss: 0.10573, val loss: 0.10877\n",
      "Main effects training epoch: 85, train loss: 0.10636, val loss: 0.10929\n",
      "Main effects training epoch: 86, train loss: 0.10583, val loss: 0.10874\n",
      "Main effects training epoch: 87, train loss: 0.10614, val loss: 0.10928\n",
      "Main effects training epoch: 88, train loss: 0.10616, val loss: 0.10949\n",
      "Main effects training epoch: 89, train loss: 0.10589, val loss: 0.10865\n",
      "Main effects training epoch: 90, train loss: 0.10604, val loss: 0.10903\n",
      "Main effects training epoch: 91, train loss: 0.10588, val loss: 0.10892\n",
      "Main effects training epoch: 92, train loss: 0.10612, val loss: 0.10925\n",
      "Main effects training epoch: 93, train loss: 0.10583, val loss: 0.10911\n",
      "Main effects training epoch: 94, train loss: 0.10604, val loss: 0.10904\n",
      "Main effects training epoch: 95, train loss: 0.10576, val loss: 0.10903\n",
      "Main effects training epoch: 96, train loss: 0.10568, val loss: 0.10869\n",
      "Main effects training epoch: 97, train loss: 0.10588, val loss: 0.10926\n",
      "Main effects training epoch: 98, train loss: 0.10606, val loss: 0.10910\n",
      "Main effects training epoch: 99, train loss: 0.10649, val loss: 0.10930\n",
      "Main effects training epoch: 100, train loss: 0.10628, val loss: 0.10948\n",
      "Main effects training epoch: 101, train loss: 0.10586, val loss: 0.10912\n",
      "Main effects training epoch: 102, train loss: 0.10570, val loss: 0.10869\n",
      "Main effects training epoch: 103, train loss: 0.10643, val loss: 0.10951\n",
      "Main effects training epoch: 104, train loss: 0.10650, val loss: 0.10946\n",
      "Main effects training epoch: 105, train loss: 0.10626, val loss: 0.10954\n",
      "Main effects training epoch: 106, train loss: 0.10594, val loss: 0.10896\n",
      "Main effects training epoch: 107, train loss: 0.10579, val loss: 0.10878\n",
      "Main effects training epoch: 108, train loss: 0.10607, val loss: 0.10922\n",
      "Main effects training epoch: 109, train loss: 0.10581, val loss: 0.10902\n",
      "Main effects training epoch: 110, train loss: 0.10590, val loss: 0.10892\n",
      "Main effects training epoch: 111, train loss: 0.10605, val loss: 0.10924\n",
      "Main effects training epoch: 112, train loss: 0.10580, val loss: 0.10875\n",
      "Main effects training epoch: 113, train loss: 0.10587, val loss: 0.10900\n",
      "Main effects training epoch: 114, train loss: 0.10581, val loss: 0.10883\n",
      "Main effects training epoch: 115, train loss: 0.10618, val loss: 0.10915\n",
      "Main effects training epoch: 116, train loss: 0.10615, val loss: 0.10949\n",
      "Main effects training epoch: 117, train loss: 0.10602, val loss: 0.10894\n",
      "Main effects training epoch: 118, train loss: 0.10595, val loss: 0.10914\n",
      "Main effects training epoch: 119, train loss: 0.10618, val loss: 0.10895\n",
      "Main effects training epoch: 120, train loss: 0.10601, val loss: 0.10941\n",
      "Main effects training epoch: 121, train loss: 0.10588, val loss: 0.10869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 122, train loss: 0.10576, val loss: 0.10885\n",
      "Main effects training epoch: 123, train loss: 0.10578, val loss: 0.10899\n",
      "Main effects training epoch: 124, train loss: 0.10583, val loss: 0.10934\n",
      "Main effects training epoch: 125, train loss: 0.10610, val loss: 0.10904\n",
      "Main effects training epoch: 126, train loss: 0.10593, val loss: 0.10917\n",
      "Main effects training epoch: 127, train loss: 0.10579, val loss: 0.10863\n",
      "Main effects training epoch: 128, train loss: 0.10583, val loss: 0.10933\n",
      "Main effects training epoch: 129, train loss: 0.10580, val loss: 0.10919\n",
      "Main effects training epoch: 130, train loss: 0.10573, val loss: 0.10874\n",
      "Main effects training epoch: 131, train loss: 0.10601, val loss: 0.10951\n",
      "Main effects training epoch: 132, train loss: 0.10600, val loss: 0.10905\n",
      "Main effects training epoch: 133, train loss: 0.10601, val loss: 0.10931\n",
      "Main effects training epoch: 134, train loss: 0.10566, val loss: 0.10882\n",
      "Main effects training epoch: 135, train loss: 0.10590, val loss: 0.10909\n",
      "Main effects training epoch: 136, train loss: 0.10621, val loss: 0.10930\n",
      "Main effects training epoch: 137, train loss: 0.10597, val loss: 0.10906\n",
      "Main effects training epoch: 138, train loss: 0.10583, val loss: 0.10895\n",
      "Main effects training epoch: 139, train loss: 0.10580, val loss: 0.10894\n",
      "Main effects training epoch: 140, train loss: 0.10591, val loss: 0.10891\n",
      "Main effects training epoch: 141, train loss: 0.10595, val loss: 0.10903\n",
      "Main effects training epoch: 142, train loss: 0.10601, val loss: 0.10932\n",
      "Main effects training epoch: 143, train loss: 0.10596, val loss: 0.10892\n",
      "Main effects training epoch: 144, train loss: 0.10583, val loss: 0.10925\n",
      "Main effects training epoch: 145, train loss: 0.10572, val loss: 0.10887\n",
      "Main effects training epoch: 146, train loss: 0.10574, val loss: 0.10897\n",
      "Main effects training epoch: 147, train loss: 0.10575, val loss: 0.10903\n",
      "Main effects training epoch: 148, train loss: 0.10589, val loss: 0.10894\n",
      "Main effects training epoch: 149, train loss: 0.10577, val loss: 0.10889\n",
      "Main effects training epoch: 150, train loss: 0.10569, val loss: 0.10895\n",
      "Main effects training epoch: 151, train loss: 0.10575, val loss: 0.10884\n",
      "Main effects training epoch: 152, train loss: 0.10606, val loss: 0.10908\n",
      "Main effects training epoch: 153, train loss: 0.10639, val loss: 0.10965\n",
      "Main effects training epoch: 154, train loss: 0.10596, val loss: 0.10920\n",
      "Main effects training epoch: 155, train loss: 0.10578, val loss: 0.10908\n",
      "Main effects training epoch: 156, train loss: 0.10569, val loss: 0.10881\n",
      "Main effects training epoch: 157, train loss: 0.10570, val loss: 0.10898\n",
      "Main effects training epoch: 158, train loss: 0.10595, val loss: 0.10931\n",
      "Main effects training epoch: 159, train loss: 0.10577, val loss: 0.10920\n",
      "Main effects training epoch: 160, train loss: 0.10578, val loss: 0.10935\n",
      "Main effects training epoch: 161, train loss: 0.10579, val loss: 0.10872\n",
      "Main effects training epoch: 162, train loss: 0.10575, val loss: 0.10904\n",
      "Main effects training epoch: 163, train loss: 0.10597, val loss: 0.10913\n",
      "Main effects training epoch: 164, train loss: 0.10605, val loss: 0.10923\n",
      "Main effects training epoch: 165, train loss: 0.10604, val loss: 0.10954\n",
      "Main effects training epoch: 166, train loss: 0.10575, val loss: 0.10883\n",
      "Main effects training epoch: 167, train loss: 0.10578, val loss: 0.10918\n",
      "Main effects training epoch: 168, train loss: 0.10576, val loss: 0.10888\n",
      "Early stop at epoch 168, with validation loss: 0.10888\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.10587, val loss: 0.10915\n",
      "Main effects tuning epoch: 2, train loss: 0.10583, val loss: 0.10863\n",
      "Main effects tuning epoch: 3, train loss: 0.10593, val loss: 0.10918\n",
      "Main effects tuning epoch: 4, train loss: 0.10580, val loss: 0.10863\n",
      "Main effects tuning epoch: 5, train loss: 0.10595, val loss: 0.10896\n",
      "Main effects tuning epoch: 6, train loss: 0.10614, val loss: 0.10907\n",
      "Main effects tuning epoch: 7, train loss: 0.10617, val loss: 0.10891\n",
      "Main effects tuning epoch: 8, train loss: 0.10639, val loss: 0.10952\n",
      "Main effects tuning epoch: 9, train loss: 0.10631, val loss: 0.10952\n",
      "Main effects tuning epoch: 10, train loss: 0.10598, val loss: 0.10913\n",
      "Main effects tuning epoch: 11, train loss: 0.10616, val loss: 0.10872\n",
      "Main effects tuning epoch: 12, train loss: 0.10613, val loss: 0.10940\n",
      "Main effects tuning epoch: 13, train loss: 0.10649, val loss: 0.10913\n",
      "Main effects tuning epoch: 14, train loss: 0.10595, val loss: 0.10906\n",
      "Main effects tuning epoch: 15, train loss: 0.10576, val loss: 0.10901\n",
      "Main effects tuning epoch: 16, train loss: 0.10591, val loss: 0.10867\n",
      "Main effects tuning epoch: 17, train loss: 0.10614, val loss: 0.10929\n",
      "Main effects tuning epoch: 18, train loss: 0.10616, val loss: 0.10914\n",
      "Main effects tuning epoch: 19, train loss: 0.10591, val loss: 0.10901\n",
      "Main effects tuning epoch: 20, train loss: 0.10585, val loss: 0.10893\n",
      "Main effects tuning epoch: 21, train loss: 0.10582, val loss: 0.10851\n",
      "Main effects tuning epoch: 22, train loss: 0.10595, val loss: 0.10917\n",
      "Main effects tuning epoch: 23, train loss: 0.10589, val loss: 0.10893\n",
      "Main effects tuning epoch: 24, train loss: 0.10579, val loss: 0.10899\n",
      "Main effects tuning epoch: 25, train loss: 0.10580, val loss: 0.10907\n",
      "Main effects tuning epoch: 26, train loss: 0.10612, val loss: 0.10929\n",
      "Main effects tuning epoch: 27, train loss: 0.10613, val loss: 0.10892\n",
      "Main effects tuning epoch: 28, train loss: 0.10608, val loss: 0.10898\n",
      "Main effects tuning epoch: 29, train loss: 0.10588, val loss: 0.10904\n",
      "Main effects tuning epoch: 30, train loss: 0.10603, val loss: 0.10875\n",
      "Main effects tuning epoch: 31, train loss: 0.10580, val loss: 0.10885\n",
      "Main effects tuning epoch: 32, train loss: 0.10586, val loss: 0.10892\n",
      "Main effects tuning epoch: 33, train loss: 0.10584, val loss: 0.10877\n",
      "Main effects tuning epoch: 34, train loss: 0.10610, val loss: 0.10890\n",
      "Main effects tuning epoch: 35, train loss: 0.10602, val loss: 0.10950\n",
      "Main effects tuning epoch: 36, train loss: 0.10577, val loss: 0.10870\n",
      "Main effects tuning epoch: 37, train loss: 0.10576, val loss: 0.10890\n",
      "Main effects tuning epoch: 38, train loss: 0.10590, val loss: 0.10894\n",
      "Main effects tuning epoch: 39, train loss: 0.10597, val loss: 0.10915\n",
      "Main effects tuning epoch: 40, train loss: 0.10575, val loss: 0.10856\n",
      "Main effects tuning epoch: 41, train loss: 0.10577, val loss: 0.10899\n",
      "Main effects tuning epoch: 42, train loss: 0.10600, val loss: 0.10888\n",
      "Main effects tuning epoch: 43, train loss: 0.10624, val loss: 0.10938\n",
      "Main effects tuning epoch: 44, train loss: 0.10606, val loss: 0.10863\n",
      "Main effects tuning epoch: 45, train loss: 0.10614, val loss: 0.10961\n",
      "Main effects tuning epoch: 46, train loss: 0.10598, val loss: 0.10874\n",
      "Main effects tuning epoch: 47, train loss: 0.10623, val loss: 0.10916\n",
      "Main effects tuning epoch: 48, train loss: 0.10628, val loss: 0.10910\n",
      "Main effects tuning epoch: 49, train loss: 0.10612, val loss: 0.10902\n",
      "Main effects tuning epoch: 50, train loss: 0.10600, val loss: 0.10922\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.16656, val loss: 0.16504\n",
      "Interaction training epoch: 2, train loss: 0.19799, val loss: 0.19730\n",
      "Interaction training epoch: 3, train loss: 0.07919, val loss: 0.08135\n",
      "Interaction training epoch: 4, train loss: 0.06592, val loss: 0.06695\n",
      "Interaction training epoch: 5, train loss: 0.06719, val loss: 0.06623\n",
      "Interaction training epoch: 6, train loss: 0.07116, val loss: 0.06992\n",
      "Interaction training epoch: 7, train loss: 0.05765, val loss: 0.05869\n",
      "Interaction training epoch: 8, train loss: 0.05791, val loss: 0.05822\n",
      "Interaction training epoch: 9, train loss: 0.06091, val loss: 0.06046\n",
      "Interaction training epoch: 10, train loss: 0.05754, val loss: 0.05788\n",
      "Interaction training epoch: 11, train loss: 0.05624, val loss: 0.05624\n",
      "Interaction training epoch: 12, train loss: 0.05651, val loss: 0.05600\n",
      "Interaction training epoch: 13, train loss: 0.06101, val loss: 0.06080\n",
      "Interaction training epoch: 14, train loss: 0.05709, val loss: 0.05641\n",
      "Interaction training epoch: 15, train loss: 0.06077, val loss: 0.06101\n",
      "Interaction training epoch: 16, train loss: 0.05239, val loss: 0.05197\n",
      "Interaction training epoch: 17, train loss: 0.05618, val loss: 0.05592\n",
      "Interaction training epoch: 18, train loss: 0.05477, val loss: 0.05468\n",
      "Interaction training epoch: 19, train loss: 0.05152, val loss: 0.05113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 20, train loss: 0.05519, val loss: 0.05482\n",
      "Interaction training epoch: 21, train loss: 0.05818, val loss: 0.05633\n",
      "Interaction training epoch: 22, train loss: 0.05614, val loss: 0.05550\n",
      "Interaction training epoch: 23, train loss: 0.05384, val loss: 0.05252\n",
      "Interaction training epoch: 24, train loss: 0.05143, val loss: 0.05136\n",
      "Interaction training epoch: 25, train loss: 0.05185, val loss: 0.05118\n",
      "Interaction training epoch: 26, train loss: 0.05409, val loss: 0.05515\n",
      "Interaction training epoch: 27, train loss: 0.05171, val loss: 0.05189\n",
      "Interaction training epoch: 28, train loss: 0.05284, val loss: 0.05307\n",
      "Interaction training epoch: 29, train loss: 0.05251, val loss: 0.05306\n",
      "Interaction training epoch: 30, train loss: 0.05654, val loss: 0.05667\n",
      "Interaction training epoch: 31, train loss: 0.05008, val loss: 0.05013\n",
      "Interaction training epoch: 32, train loss: 0.05165, val loss: 0.05175\n",
      "Interaction training epoch: 33, train loss: 0.05094, val loss: 0.05147\n",
      "Interaction training epoch: 34, train loss: 0.05154, val loss: 0.05255\n",
      "Interaction training epoch: 35, train loss: 0.05178, val loss: 0.05163\n",
      "Interaction training epoch: 36, train loss: 0.05189, val loss: 0.05137\n",
      "Interaction training epoch: 37, train loss: 0.05261, val loss: 0.05262\n",
      "Interaction training epoch: 38, train loss: 0.05395, val loss: 0.05444\n",
      "Interaction training epoch: 39, train loss: 0.05053, val loss: 0.05093\n",
      "Interaction training epoch: 40, train loss: 0.05186, val loss: 0.05165\n",
      "Interaction training epoch: 41, train loss: 0.05174, val loss: 0.05183\n",
      "Interaction training epoch: 42, train loss: 0.05645, val loss: 0.05707\n",
      "Interaction training epoch: 43, train loss: 0.05130, val loss: 0.05143\n",
      "Interaction training epoch: 44, train loss: 0.05233, val loss: 0.05213\n",
      "Interaction training epoch: 45, train loss: 0.05242, val loss: 0.05266\n",
      "Interaction training epoch: 46, train loss: 0.05234, val loss: 0.05256\n",
      "Interaction training epoch: 47, train loss: 0.05096, val loss: 0.05121\n",
      "Interaction training epoch: 48, train loss: 0.05324, val loss: 0.05317\n",
      "Interaction training epoch: 49, train loss: 0.05143, val loss: 0.05175\n",
      "Interaction training epoch: 50, train loss: 0.05906, val loss: 0.05866\n",
      "Interaction training epoch: 51, train loss: 0.05278, val loss: 0.05220\n",
      "Interaction training epoch: 52, train loss: 0.05429, val loss: 0.05321\n",
      "Interaction training epoch: 53, train loss: 0.05247, val loss: 0.05225\n",
      "Interaction training epoch: 54, train loss: 0.05061, val loss: 0.05129\n",
      "Interaction training epoch: 55, train loss: 0.05239, val loss: 0.05237\n",
      "Interaction training epoch: 56, train loss: 0.05174, val loss: 0.05287\n",
      "Interaction training epoch: 57, train loss: 0.05160, val loss: 0.05111\n",
      "Interaction training epoch: 58, train loss: 0.05180, val loss: 0.05292\n",
      "Interaction training epoch: 59, train loss: 0.04982, val loss: 0.05009\n",
      "Interaction training epoch: 60, train loss: 0.05227, val loss: 0.05368\n",
      "Interaction training epoch: 61, train loss: 0.05217, val loss: 0.05210\n",
      "Interaction training epoch: 62, train loss: 0.05023, val loss: 0.05074\n",
      "Interaction training epoch: 63, train loss: 0.05179, val loss: 0.05284\n",
      "Interaction training epoch: 64, train loss: 0.04959, val loss: 0.05129\n",
      "Interaction training epoch: 65, train loss: 0.05320, val loss: 0.05225\n",
      "Interaction training epoch: 66, train loss: 0.05295, val loss: 0.05326\n",
      "Interaction training epoch: 67, train loss: 0.05028, val loss: 0.05144\n",
      "Interaction training epoch: 68, train loss: 0.05112, val loss: 0.05151\n",
      "Interaction training epoch: 69, train loss: 0.05228, val loss: 0.05248\n",
      "Interaction training epoch: 70, train loss: 0.04994, val loss: 0.05062\n",
      "Interaction training epoch: 71, train loss: 0.05225, val loss: 0.05286\n",
      "Interaction training epoch: 72, train loss: 0.05069, val loss: 0.05180\n",
      "Interaction training epoch: 73, train loss: 0.05077, val loss: 0.05196\n",
      "Interaction training epoch: 74, train loss: 0.04967, val loss: 0.05076\n",
      "Interaction training epoch: 75, train loss: 0.05064, val loss: 0.05071\n",
      "Interaction training epoch: 76, train loss: 0.04886, val loss: 0.05004\n",
      "Interaction training epoch: 77, train loss: 0.05097, val loss: 0.05075\n",
      "Interaction training epoch: 78, train loss: 0.05171, val loss: 0.05128\n",
      "Interaction training epoch: 79, train loss: 0.05178, val loss: 0.05168\n",
      "Interaction training epoch: 80, train loss: 0.04972, val loss: 0.05111\n",
      "Interaction training epoch: 81, train loss: 0.05307, val loss: 0.05407\n",
      "Interaction training epoch: 82, train loss: 0.05287, val loss: 0.05331\n",
      "Interaction training epoch: 83, train loss: 0.05019, val loss: 0.05065\n",
      "Interaction training epoch: 84, train loss: 0.05109, val loss: 0.05177\n",
      "Interaction training epoch: 85, train loss: 0.05241, val loss: 0.05395\n",
      "Interaction training epoch: 86, train loss: 0.05169, val loss: 0.05187\n",
      "Interaction training epoch: 87, train loss: 0.05013, val loss: 0.05031\n",
      "Interaction training epoch: 88, train loss: 0.05249, val loss: 0.05324\n",
      "Interaction training epoch: 89, train loss: 0.04905, val loss: 0.04955\n",
      "Interaction training epoch: 90, train loss: 0.05048, val loss: 0.05104\n",
      "Interaction training epoch: 91, train loss: 0.05291, val loss: 0.05320\n",
      "Interaction training epoch: 92, train loss: 0.05194, val loss: 0.05184\n",
      "Interaction training epoch: 93, train loss: 0.04813, val loss: 0.04857\n",
      "Interaction training epoch: 94, train loss: 0.05434, val loss: 0.05466\n",
      "Interaction training epoch: 95, train loss: 0.05394, val loss: 0.05306\n",
      "Interaction training epoch: 96, train loss: 0.04954, val loss: 0.04995\n",
      "Interaction training epoch: 97, train loss: 0.05003, val loss: 0.05058\n",
      "Interaction training epoch: 98, train loss: 0.05197, val loss: 0.05219\n",
      "Interaction training epoch: 99, train loss: 0.05436, val loss: 0.05280\n",
      "Interaction training epoch: 100, train loss: 0.05350, val loss: 0.05437\n",
      "Interaction training epoch: 101, train loss: 0.04964, val loss: 0.04980\n",
      "Interaction training epoch: 102, train loss: 0.05136, val loss: 0.05087\n",
      "Interaction training epoch: 103, train loss: 0.05162, val loss: 0.05191\n",
      "Interaction training epoch: 104, train loss: 0.05118, val loss: 0.05150\n",
      "Interaction training epoch: 105, train loss: 0.04993, val loss: 0.04937\n",
      "Interaction training epoch: 106, train loss: 0.05171, val loss: 0.05164\n",
      "Interaction training epoch: 107, train loss: 0.05193, val loss: 0.05075\n",
      "Interaction training epoch: 108, train loss: 0.05102, val loss: 0.05183\n",
      "Interaction training epoch: 109, train loss: 0.05261, val loss: 0.05324\n",
      "Interaction training epoch: 110, train loss: 0.05047, val loss: 0.05100\n",
      "Interaction training epoch: 111, train loss: 0.05046, val loss: 0.05063\n",
      "Interaction training epoch: 112, train loss: 0.04811, val loss: 0.04809\n",
      "Interaction training epoch: 113, train loss: 0.05250, val loss: 0.05298\n",
      "Interaction training epoch: 114, train loss: 0.05036, val loss: 0.05112\n",
      "Interaction training epoch: 115, train loss: 0.05119, val loss: 0.05140\n",
      "Interaction training epoch: 116, train loss: 0.04975, val loss: 0.05112\n",
      "Interaction training epoch: 117, train loss: 0.04911, val loss: 0.04994\n",
      "Interaction training epoch: 118, train loss: 0.04990, val loss: 0.05012\n",
      "Interaction training epoch: 119, train loss: 0.04854, val loss: 0.04775\n",
      "Interaction training epoch: 120, train loss: 0.04985, val loss: 0.04987\n",
      "Interaction training epoch: 121, train loss: 0.04779, val loss: 0.04856\n",
      "Interaction training epoch: 122, train loss: 0.04712, val loss: 0.04773\n",
      "Interaction training epoch: 123, train loss: 0.05164, val loss: 0.05255\n",
      "Interaction training epoch: 124, train loss: 0.05327, val loss: 0.05365\n",
      "Interaction training epoch: 125, train loss: 0.05348, val loss: 0.05414\n",
      "Interaction training epoch: 126, train loss: 0.04873, val loss: 0.04978\n",
      "Interaction training epoch: 127, train loss: 0.04866, val loss: 0.04894\n",
      "Interaction training epoch: 128, train loss: 0.04947, val loss: 0.05037\n",
      "Interaction training epoch: 129, train loss: 0.04701, val loss: 0.04711\n",
      "Interaction training epoch: 130, train loss: 0.04867, val loss: 0.04878\n",
      "Interaction training epoch: 131, train loss: 0.04995, val loss: 0.05004\n",
      "Interaction training epoch: 132, train loss: 0.05004, val loss: 0.05008\n",
      "Interaction training epoch: 133, train loss: 0.04970, val loss: 0.05023\n",
      "Interaction training epoch: 134, train loss: 0.04969, val loss: 0.05041\n",
      "Interaction training epoch: 135, train loss: 0.04826, val loss: 0.04935\n",
      "Interaction training epoch: 136, train loss: 0.04956, val loss: 0.05077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 137, train loss: 0.04937, val loss: 0.05116\n",
      "Interaction training epoch: 138, train loss: 0.05042, val loss: 0.05126\n",
      "Interaction training epoch: 139, train loss: 0.04910, val loss: 0.04944\n",
      "Interaction training epoch: 140, train loss: 0.05235, val loss: 0.05329\n",
      "Interaction training epoch: 141, train loss: 0.05151, val loss: 0.05199\n",
      "Interaction training epoch: 142, train loss: 0.04935, val loss: 0.04965\n",
      "Interaction training epoch: 143, train loss: 0.05663, val loss: 0.05667\n",
      "Interaction training epoch: 144, train loss: 0.05217, val loss: 0.05297\n",
      "Interaction training epoch: 145, train loss: 0.05139, val loss: 0.05143\n",
      "Interaction training epoch: 146, train loss: 0.05350, val loss: 0.05388\n",
      "Interaction training epoch: 147, train loss: 0.06195, val loss: 0.06171\n",
      "Interaction training epoch: 148, train loss: 0.05923, val loss: 0.05958\n",
      "Interaction training epoch: 149, train loss: 0.05897, val loss: 0.05842\n",
      "Interaction training epoch: 150, train loss: 0.05688, val loss: 0.05663\n",
      "Interaction training epoch: 151, train loss: 0.05292, val loss: 0.05339\n",
      "Interaction training epoch: 152, train loss: 0.04638, val loss: 0.04708\n",
      "Interaction training epoch: 153, train loss: 0.04830, val loss: 0.05070\n",
      "Interaction training epoch: 154, train loss: 0.04798, val loss: 0.04952\n",
      "Interaction training epoch: 155, train loss: 0.04659, val loss: 0.04786\n",
      "Interaction training epoch: 156, train loss: 0.04767, val loss: 0.04876\n",
      "Interaction training epoch: 157, train loss: 0.04853, val loss: 0.04917\n",
      "Interaction training epoch: 158, train loss: 0.04677, val loss: 0.04803\n",
      "Interaction training epoch: 159, train loss: 0.04901, val loss: 0.04931\n",
      "Interaction training epoch: 160, train loss: 0.04754, val loss: 0.04893\n",
      "Interaction training epoch: 161, train loss: 0.04874, val loss: 0.04950\n",
      "Interaction training epoch: 162, train loss: 0.04595, val loss: 0.04721\n",
      "Interaction training epoch: 163, train loss: 0.04732, val loss: 0.04956\n",
      "Interaction training epoch: 164, train loss: 0.04567, val loss: 0.04749\n",
      "Interaction training epoch: 165, train loss: 0.05013, val loss: 0.05115\n",
      "Interaction training epoch: 166, train loss: 0.04663, val loss: 0.04790\n",
      "Interaction training epoch: 167, train loss: 0.04602, val loss: 0.04784\n",
      "Interaction training epoch: 168, train loss: 0.04789, val loss: 0.04800\n",
      "Interaction training epoch: 169, train loss: 0.04480, val loss: 0.04615\n",
      "Interaction training epoch: 170, train loss: 0.04657, val loss: 0.04712\n",
      "Interaction training epoch: 171, train loss: 0.04515, val loss: 0.04688\n",
      "Interaction training epoch: 172, train loss: 0.04745, val loss: 0.04904\n",
      "Interaction training epoch: 173, train loss: 0.04801, val loss: 0.05050\n",
      "Interaction training epoch: 174, train loss: 0.05297, val loss: 0.05389\n",
      "Interaction training epoch: 175, train loss: 0.04522, val loss: 0.04687\n",
      "Interaction training epoch: 176, train loss: 0.04654, val loss: 0.04709\n",
      "Interaction training epoch: 177, train loss: 0.04462, val loss: 0.04576\n",
      "Interaction training epoch: 178, train loss: 0.04715, val loss: 0.04880\n",
      "Interaction training epoch: 179, train loss: 0.04515, val loss: 0.04673\n",
      "Interaction training epoch: 180, train loss: 0.04652, val loss: 0.04824\n",
      "Interaction training epoch: 181, train loss: 0.04757, val loss: 0.04936\n",
      "Interaction training epoch: 182, train loss: 0.04771, val loss: 0.04909\n",
      "Interaction training epoch: 183, train loss: 0.04999, val loss: 0.05186\n",
      "Interaction training epoch: 184, train loss: 0.04691, val loss: 0.04877\n",
      "Interaction training epoch: 185, train loss: 0.04599, val loss: 0.04763\n",
      "Interaction training epoch: 186, train loss: 0.04839, val loss: 0.04969\n",
      "Interaction training epoch: 187, train loss: 0.04634, val loss: 0.04846\n",
      "Interaction training epoch: 188, train loss: 0.04686, val loss: 0.04828\n",
      "Interaction training epoch: 189, train loss: 0.04527, val loss: 0.04603\n",
      "Interaction training epoch: 190, train loss: 0.04811, val loss: 0.05046\n",
      "Interaction training epoch: 191, train loss: 0.04668, val loss: 0.04794\n",
      "Interaction training epoch: 192, train loss: 0.04580, val loss: 0.04673\n",
      "Interaction training epoch: 193, train loss: 0.04953, val loss: 0.05077\n",
      "Interaction training epoch: 194, train loss: 0.04637, val loss: 0.04745\n",
      "Interaction training epoch: 195, train loss: 0.05291, val loss: 0.05606\n",
      "Interaction training epoch: 196, train loss: 0.04552, val loss: 0.04706\n",
      "Interaction training epoch: 197, train loss: 0.04954, val loss: 0.05175\n",
      "Interaction training epoch: 198, train loss: 0.04504, val loss: 0.04659\n",
      "Interaction training epoch: 199, train loss: 0.04939, val loss: 0.05068\n",
      "Interaction training epoch: 200, train loss: 0.04824, val loss: 0.04969\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########7 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.04624, val loss: 0.04648\n",
      "Interaction tuning epoch: 2, train loss: 0.04654, val loss: 0.04675\n",
      "Interaction tuning epoch: 3, train loss: 0.04688, val loss: 0.04770\n",
      "Interaction tuning epoch: 4, train loss: 0.04703, val loss: 0.04714\n",
      "Interaction tuning epoch: 5, train loss: 0.04871, val loss: 0.04870\n",
      "Interaction tuning epoch: 6, train loss: 0.04715, val loss: 0.04685\n",
      "Interaction tuning epoch: 7, train loss: 0.04753, val loss: 0.04769\n",
      "Interaction tuning epoch: 8, train loss: 0.04804, val loss: 0.04839\n",
      "Interaction tuning epoch: 9, train loss: 0.04656, val loss: 0.04746\n",
      "Interaction tuning epoch: 10, train loss: 0.04708, val loss: 0.04792\n",
      "Interaction tuning epoch: 11, train loss: 0.04593, val loss: 0.04607\n",
      "Interaction tuning epoch: 12, train loss: 0.04889, val loss: 0.05019\n",
      "Interaction tuning epoch: 13, train loss: 0.04756, val loss: 0.04795\n",
      "Interaction tuning epoch: 14, train loss: 0.05056, val loss: 0.04891\n",
      "Interaction tuning epoch: 15, train loss: 0.05649, val loss: 0.05700\n",
      "Interaction tuning epoch: 16, train loss: 0.04770, val loss: 0.04777\n",
      "Interaction tuning epoch: 17, train loss: 0.04665, val loss: 0.04734\n",
      "Interaction tuning epoch: 18, train loss: 0.04709, val loss: 0.04763\n",
      "Interaction tuning epoch: 19, train loss: 0.04762, val loss: 0.04775\n",
      "Interaction tuning epoch: 20, train loss: 0.04647, val loss: 0.04691\n",
      "Interaction tuning epoch: 21, train loss: 0.04858, val loss: 0.04904\n",
      "Interaction tuning epoch: 22, train loss: 0.04763, val loss: 0.04743\n",
      "Interaction tuning epoch: 23, train loss: 0.04883, val loss: 0.04879\n",
      "Interaction tuning epoch: 24, train loss: 0.04608, val loss: 0.04728\n",
      "Interaction tuning epoch: 25, train loss: 0.04999, val loss: 0.04940\n",
      "Interaction tuning epoch: 26, train loss: 0.04916, val loss: 0.04857\n",
      "Interaction tuning epoch: 27, train loss: 0.04784, val loss: 0.04855\n",
      "Interaction tuning epoch: 28, train loss: 0.04702, val loss: 0.04758\n",
      "Interaction tuning epoch: 29, train loss: 0.04685, val loss: 0.04720\n",
      "Interaction tuning epoch: 30, train loss: 0.04688, val loss: 0.04800\n",
      "Interaction tuning epoch: 31, train loss: 0.04656, val loss: 0.04725\n",
      "Interaction tuning epoch: 32, train loss: 0.04698, val loss: 0.04718\n",
      "Interaction tuning epoch: 33, train loss: 0.04648, val loss: 0.04674\n",
      "Interaction tuning epoch: 34, train loss: 0.04622, val loss: 0.04627\n",
      "Interaction tuning epoch: 35, train loss: 0.04695, val loss: 0.04653\n",
      "Interaction tuning epoch: 36, train loss: 0.04638, val loss: 0.04708\n",
      "Interaction tuning epoch: 37, train loss: 0.04517, val loss: 0.04599\n",
      "Interaction tuning epoch: 38, train loss: 0.04594, val loss: 0.04626\n",
      "Interaction tuning epoch: 39, train loss: 0.04806, val loss: 0.04884\n",
      "Interaction tuning epoch: 40, train loss: 0.04762, val loss: 0.04781\n",
      "Interaction tuning epoch: 41, train loss: 0.04762, val loss: 0.04909\n",
      "Interaction tuning epoch: 42, train loss: 0.04746, val loss: 0.04805\n",
      "Interaction tuning epoch: 43, train loss: 0.04604, val loss: 0.04612\n",
      "Interaction tuning epoch: 44, train loss: 0.04636, val loss: 0.04673\n",
      "Interaction tuning epoch: 45, train loss: 0.04643, val loss: 0.04712\n",
      "Interaction tuning epoch: 46, train loss: 0.04656, val loss: 0.04678\n",
      "Interaction tuning epoch: 47, train loss: 0.04635, val loss: 0.04667\n",
      "Interaction tuning epoch: 48, train loss: 0.04816, val loss: 0.04882\n",
      "Interaction tuning epoch: 49, train loss: 0.04683, val loss: 0.04711\n",
      "Interaction tuning epoch: 50, train loss: 0.04845, val loss: 0.04905\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 33.36054491996765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After the gam stage, training error is 0.04845 , validation error is 0.04905\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 1.232198\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed MAE=0.047043 validation MAE=0.048920,rank=1\n",
      "[SoftImpute] Iter 2: observed MAE=0.046359 validation MAE=0.048786,rank=1\n",
      "[SoftImpute] Iter 3: observed MAE=0.045684 validation MAE=0.048613,rank=1\n",
      "[SoftImpute] Iter 4: observed MAE=0.045016 validation MAE=0.048386,rank=1\n",
      "[SoftImpute] Iter 5: observed MAE=0.044358 validation MAE=0.048124,rank=1\n",
      "[SoftImpute] Iter 6: observed MAE=0.043725 validation MAE=0.047835,rank=1\n",
      "[SoftImpute] Iter 7: observed MAE=0.043112 validation MAE=0.047521,rank=1\n",
      "[SoftImpute] Iter 8: observed MAE=0.042516 validation MAE=0.047176,rank=1\n",
      "[SoftImpute] Iter 9: observed MAE=0.041937 validation MAE=0.046820,rank=1\n",
      "[SoftImpute] Iter 10: observed MAE=0.041380 validation MAE=0.046464,rank=1\n",
      "[SoftImpute] Iter 11: observed MAE=0.040844 validation MAE=0.046101,rank=1\n",
      "[SoftImpute] Iter 12: observed MAE=0.040330 validation MAE=0.045735,rank=1\n",
      "[SoftImpute] Iter 13: observed MAE=0.039839 validation MAE=0.045359,rank=1\n",
      "[SoftImpute] Iter 14: observed MAE=0.039368 validation MAE=0.044985,rank=1\n",
      "[SoftImpute] Iter 15: observed MAE=0.038920 validation MAE=0.044607,rank=1\n",
      "[SoftImpute] Iter 16: observed MAE=0.038492 validation MAE=0.044239,rank=1\n",
      "[SoftImpute] Iter 17: observed MAE=0.038083 validation MAE=0.043881,rank=1\n",
      "[SoftImpute] Iter 18: observed MAE=0.037688 validation MAE=0.043533,rank=1\n",
      "[SoftImpute] Iter 19: observed MAE=0.037309 validation MAE=0.043194,rank=1\n",
      "[SoftImpute] Iter 20: observed MAE=0.036943 validation MAE=0.042853,rank=1\n",
      "[SoftImpute] Iter 21: observed MAE=0.036591 validation MAE=0.042515,rank=1\n",
      "[SoftImpute] Iter 22: observed MAE=0.036254 validation MAE=0.042185,rank=1\n",
      "[SoftImpute] Iter 23: observed MAE=0.035930 validation MAE=0.041861,rank=1\n",
      "[SoftImpute] Iter 24: observed MAE=0.035616 validation MAE=0.041542,rank=1\n",
      "[SoftImpute] Iter 25: observed MAE=0.035315 validation MAE=0.041242,rank=1\n",
      "[SoftImpute] Iter 26: observed MAE=0.035028 validation MAE=0.040970,rank=1\n",
      "[SoftImpute] Iter 27: observed MAE=0.034754 validation MAE=0.040704,rank=1\n",
      "[SoftImpute] Iter 28: observed MAE=0.034489 validation MAE=0.040442,rank=1\n",
      "[SoftImpute] Iter 29: observed MAE=0.034239 validation MAE=0.040189,rank=1\n",
      "[SoftImpute] Iter 30: observed MAE=0.034003 validation MAE=0.039940,rank=1\n",
      "[SoftImpute] Iter 31: observed MAE=0.033780 validation MAE=0.039700,rank=1\n",
      "[SoftImpute] Iter 32: observed MAE=0.033568 validation MAE=0.039471,rank=1\n",
      "[SoftImpute] Iter 33: observed MAE=0.033362 validation MAE=0.039248,rank=1\n",
      "[SoftImpute] Iter 34: observed MAE=0.033163 validation MAE=0.039036,rank=1\n",
      "[SoftImpute] Iter 35: observed MAE=0.032972 validation MAE=0.038847,rank=1\n",
      "[SoftImpute] Iter 36: observed MAE=0.032787 validation MAE=0.038674,rank=1\n",
      "[SoftImpute] Iter 37: observed MAE=0.032609 validation MAE=0.038512,rank=1\n",
      "[SoftImpute] Iter 38: observed MAE=0.032436 validation MAE=0.038353,rank=1\n",
      "[SoftImpute] Iter 39: observed MAE=0.032268 validation MAE=0.038197,rank=1\n",
      "[SoftImpute] Iter 40: observed MAE=0.032107 validation MAE=0.038044,rank=1\n",
      "[SoftImpute] Iter 41: observed MAE=0.031952 validation MAE=0.037895,rank=1\n",
      "[SoftImpute] Iter 42: observed MAE=0.031802 validation MAE=0.037752,rank=1\n",
      "[SoftImpute] Iter 43: observed MAE=0.031660 validation MAE=0.037616,rank=1\n",
      "[SoftImpute] Iter 44: observed MAE=0.031523 validation MAE=0.037487,rank=1\n",
      "[SoftImpute] Iter 45: observed MAE=0.031393 validation MAE=0.037360,rank=1\n",
      "[SoftImpute] Iter 46: observed MAE=0.031271 validation MAE=0.037232,rank=1\n",
      "[SoftImpute] Iter 47: observed MAE=0.031157 validation MAE=0.037108,rank=1\n",
      "[SoftImpute] Iter 48: observed MAE=0.031048 validation MAE=0.036986,rank=1\n",
      "[SoftImpute] Iter 49: observed MAE=0.030944 validation MAE=0.036871,rank=1\n",
      "[SoftImpute] Iter 50: observed MAE=0.030846 validation MAE=0.036762,rank=1\n",
      "[SoftImpute] Iter 51: observed MAE=0.030753 validation MAE=0.036665,rank=1\n",
      "[SoftImpute] Iter 52: observed MAE=0.030665 validation MAE=0.036574,rank=1\n",
      "[SoftImpute] Iter 53: observed MAE=0.030583 validation MAE=0.036484,rank=1\n",
      "[SoftImpute] Iter 54: observed MAE=0.030505 validation MAE=0.036397,rank=1\n",
      "[SoftImpute] Iter 55: observed MAE=0.030431 validation MAE=0.036311,rank=1\n",
      "[SoftImpute] Iter 56: observed MAE=0.030361 validation MAE=0.036228,rank=1\n",
      "[SoftImpute] Iter 57: observed MAE=0.030296 validation MAE=0.036148,rank=1\n",
      "[SoftImpute] Iter 58: observed MAE=0.030235 validation MAE=0.036071,rank=1\n",
      "[SoftImpute] Iter 59: observed MAE=0.030178 validation MAE=0.035998,rank=1\n",
      "[SoftImpute] Iter 60: observed MAE=0.030124 validation MAE=0.035926,rank=1\n",
      "[SoftImpute] Iter 61: observed MAE=0.030073 validation MAE=0.035858,rank=1\n",
      "[SoftImpute] Iter 62: observed MAE=0.030025 validation MAE=0.035796,rank=1\n",
      "[SoftImpute] Iter 63: observed MAE=0.029979 validation MAE=0.035739,rank=1\n",
      "[SoftImpute] Iter 64: observed MAE=0.029937 validation MAE=0.035688,rank=1\n",
      "[SoftImpute] Iter 65: observed MAE=0.029898 validation MAE=0.035642,rank=1\n",
      "[SoftImpute] Iter 66: observed MAE=0.029861 validation MAE=0.035600,rank=1\n",
      "[SoftImpute] Iter 67: observed MAE=0.029829 validation MAE=0.035562,rank=1\n",
      "[SoftImpute] Iter 68: observed MAE=0.029799 validation MAE=0.035526,rank=1\n",
      "[SoftImpute] Iter 69: observed MAE=0.029771 validation MAE=0.035492,rank=1\n",
      "[SoftImpute] Iter 70: observed MAE=0.029744 validation MAE=0.035460,rank=1\n",
      "[SoftImpute] Iter 71: observed MAE=0.029718 validation MAE=0.035433,rank=1\n",
      "[SoftImpute] Iter 72: observed MAE=0.029694 validation MAE=0.035411,rank=1\n",
      "[SoftImpute] Iter 73: observed MAE=0.029670 validation MAE=0.035390,rank=1\n",
      "[SoftImpute] Iter 74: observed MAE=0.029648 validation MAE=0.035369,rank=1\n",
      "[SoftImpute] Iter 75: observed MAE=0.029627 validation MAE=0.035350,rank=1\n",
      "[SoftImpute] Iter 76: observed MAE=0.029607 validation MAE=0.035332,rank=1\n",
      "[SoftImpute] Iter 77: observed MAE=0.029588 validation MAE=0.035317,rank=1\n",
      "[SoftImpute] Iter 78: observed MAE=0.029569 validation MAE=0.035303,rank=1\n",
      "[SoftImpute] Iter 79: observed MAE=0.029552 validation MAE=0.035291,rank=1\n",
      "[SoftImpute] Iter 80: observed MAE=0.029537 validation MAE=0.035280,rank=1\n",
      "[SoftImpute] Iter 81: observed MAE=0.029522 validation MAE=0.035269,rank=1\n",
      "[SoftImpute] Iter 82: observed MAE=0.029508 validation MAE=0.035260,rank=1\n",
      "[SoftImpute] Iter 83: observed MAE=0.029495 validation MAE=0.035253,rank=1\n",
      "[SoftImpute] Iter 84: observed MAE=0.029482 validation MAE=0.035245,rank=1\n",
      "[SoftImpute] Iter 85: observed MAE=0.029471 validation MAE=0.035239,rank=1\n",
      "[SoftImpute] Iter 86: observed MAE=0.029460 validation MAE=0.035233,rank=1\n",
      "[SoftImpute] Iter 87: observed MAE=0.029449 validation MAE=0.035229,rank=1\n",
      "[SoftImpute] Iter 88: observed MAE=0.029439 validation MAE=0.035224,rank=1\n",
      "[SoftImpute] Iter 89: observed MAE=0.029429 validation MAE=0.035220,rank=1\n",
      "[SoftImpute] Iter 90: observed MAE=0.029420 validation MAE=0.035217,rank=1\n",
      "[SoftImpute] Iter 91: observed MAE=0.029412 validation MAE=0.035214,rank=1\n",
      "[SoftImpute] Iter 92: observed MAE=0.029404 validation MAE=0.035210,rank=1\n",
      "[SoftImpute] Iter 93: observed MAE=0.029397 validation MAE=0.035207,rank=1\n",
      "[SoftImpute] Iter 94: observed MAE=0.029390 validation MAE=0.035204,rank=1\n",
      "[SoftImpute] Iter 95: observed MAE=0.029383 validation MAE=0.035201,rank=1\n",
      "[SoftImpute] Iter 96: observed MAE=0.029377 validation MAE=0.035198,rank=1\n",
      "[SoftImpute] Iter 97: observed MAE=0.029371 validation MAE=0.035196,rank=1\n",
      "[SoftImpute] Iter 98: observed MAE=0.029365 validation MAE=0.035195,rank=1\n",
      "[SoftImpute] Iter 99: observed MAE=0.029359 validation MAE=0.035194,rank=1\n",
      "[SoftImpute] Iter 100: observed MAE=0.029353 validation MAE=0.035194,rank=1\n",
      "[SoftImpute] Iter 101: observed MAE=0.029348 validation MAE=0.035193,rank=1\n",
      "[SoftImpute] Iter 102: observed MAE=0.029343 validation MAE=0.035192,rank=1\n",
      "[SoftImpute] Iter 103: observed MAE=0.029338 validation MAE=0.035192,rank=1\n",
      "[SoftImpute] Iter 104: observed MAE=0.029333 validation MAE=0.035191,rank=1\n",
      "[SoftImpute] Iter 105: observed MAE=0.029329 validation MAE=0.035191,rank=1\n",
      "[SoftImpute] Iter 106: observed MAE=0.029325 validation MAE=0.035190,rank=1\n",
      "[SoftImpute] Iter 107: observed MAE=0.029320 validation MAE=0.035190,rank=1\n",
      "[SoftImpute] Iter 108: observed MAE=0.029316 validation MAE=0.035189,rank=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 109: observed MAE=0.029313 validation MAE=0.035189,rank=1\n",
      "[SoftImpute] Iter 110: observed MAE=0.029309 validation MAE=0.035188,rank=1\n",
      "[SoftImpute] Iter 111: observed MAE=0.029305 validation MAE=0.035188,rank=1\n",
      "[SoftImpute] Iter 112: observed MAE=0.029302 validation MAE=0.035188,rank=1\n",
      "[SoftImpute] Iter 113: observed MAE=0.029299 validation MAE=0.035187,rank=1\n",
      "[SoftImpute] Iter 114: observed MAE=0.029295 validation MAE=0.035187,rank=1\n",
      "[SoftImpute] Iter 115: observed MAE=0.029292 validation MAE=0.035187,rank=1\n",
      "[SoftImpute] Iter 116: observed MAE=0.029289 validation MAE=0.035186,rank=1\n",
      "[SoftImpute] Iter 117: observed MAE=0.029286 validation MAE=0.035186,rank=1\n",
      "[SoftImpute] Iter 118: observed MAE=0.029284 validation MAE=0.035186,rank=1\n",
      "[SoftImpute] Iter 119: observed MAE=0.029281 validation MAE=0.035185,rank=1\n",
      "[SoftImpute] Iter 120: observed MAE=0.029278 validation MAE=0.035186,rank=1\n",
      "[SoftImpute] Iter 121: observed MAE=0.029276 validation MAE=0.035186,rank=1\n",
      "[SoftImpute] Iter 122: observed MAE=0.029273 validation MAE=0.035187,rank=1\n",
      "[SoftImpute] Stopped after iteration 122 for lambda=0.024644\n",
      "final num of user group: 2\n",
      "final num of item group: 2\n",
      "change mode state : True\n",
      "time cost: 5.532535076141357\n",
      "After the matrix factor stage, training error is 0.02927, validation error is 0.03519\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.30185, val loss: 0.30478\n",
      "Main effects training epoch: 2, train loss: 0.23228, val loss: 0.23617\n",
      "Main effects training epoch: 3, train loss: 0.17997, val loss: 0.18315\n",
      "Main effects training epoch: 4, train loss: 0.14845, val loss: 0.15215\n",
      "Main effects training epoch: 5, train loss: 0.13484, val loss: 0.13695\n",
      "Main effects training epoch: 6, train loss: 0.13033, val loss: 0.13155\n",
      "Main effects training epoch: 7, train loss: 0.13055, val loss: 0.13149\n",
      "Main effects training epoch: 8, train loss: 0.12971, val loss: 0.12988\n",
      "Main effects training epoch: 9, train loss: 0.12862, val loss: 0.12919\n",
      "Main effects training epoch: 10, train loss: 0.12828, val loss: 0.12938\n",
      "Main effects training epoch: 11, train loss: 0.12732, val loss: 0.12789\n",
      "Main effects training epoch: 12, train loss: 0.12659, val loss: 0.12743\n",
      "Main effects training epoch: 13, train loss: 0.12563, val loss: 0.12654\n",
      "Main effects training epoch: 14, train loss: 0.12325, val loss: 0.12456\n",
      "Main effects training epoch: 15, train loss: 0.11949, val loss: 0.12161\n",
      "Main effects training epoch: 16, train loss: 0.11686, val loss: 0.11922\n",
      "Main effects training epoch: 17, train loss: 0.11423, val loss: 0.11562\n",
      "Main effects training epoch: 18, train loss: 0.11484, val loss: 0.11687\n",
      "Main effects training epoch: 19, train loss: 0.11188, val loss: 0.11473\n",
      "Main effects training epoch: 20, train loss: 0.11089, val loss: 0.11324\n",
      "Main effects training epoch: 21, train loss: 0.10849, val loss: 0.11127\n",
      "Main effects training epoch: 22, train loss: 0.10778, val loss: 0.11002\n",
      "Main effects training epoch: 23, train loss: 0.10761, val loss: 0.10934\n",
      "Main effects training epoch: 24, train loss: 0.10695, val loss: 0.10962\n",
      "Main effects training epoch: 25, train loss: 0.10869, val loss: 0.11093\n",
      "Main effects training epoch: 26, train loss: 0.10721, val loss: 0.11016\n",
      "Main effects training epoch: 27, train loss: 0.10692, val loss: 0.10911\n",
      "Main effects training epoch: 28, train loss: 0.10656, val loss: 0.10921\n",
      "Main effects training epoch: 29, train loss: 0.10666, val loss: 0.10915\n",
      "Main effects training epoch: 30, train loss: 0.10688, val loss: 0.10958\n",
      "Main effects training epoch: 31, train loss: 0.10689, val loss: 0.10943\n",
      "Main effects training epoch: 32, train loss: 0.10622, val loss: 0.10918\n",
      "Main effects training epoch: 33, train loss: 0.10631, val loss: 0.10928\n",
      "Main effects training epoch: 34, train loss: 0.10605, val loss: 0.10868\n",
      "Main effects training epoch: 35, train loss: 0.10599, val loss: 0.10879\n",
      "Main effects training epoch: 36, train loss: 0.10605, val loss: 0.10911\n",
      "Main effects training epoch: 37, train loss: 0.10704, val loss: 0.10981\n",
      "Main effects training epoch: 38, train loss: 0.10655, val loss: 0.10926\n",
      "Main effects training epoch: 39, train loss: 0.10607, val loss: 0.10888\n",
      "Main effects training epoch: 40, train loss: 0.10588, val loss: 0.10859\n",
      "Main effects training epoch: 41, train loss: 0.10600, val loss: 0.10889\n",
      "Main effects training epoch: 42, train loss: 0.10616, val loss: 0.10903\n",
      "Main effects training epoch: 43, train loss: 0.10609, val loss: 0.10883\n",
      "Main effects training epoch: 44, train loss: 0.10630, val loss: 0.10899\n",
      "Main effects training epoch: 45, train loss: 0.10686, val loss: 0.10928\n",
      "Main effects training epoch: 46, train loss: 0.10599, val loss: 0.10904\n",
      "Main effects training epoch: 47, train loss: 0.10577, val loss: 0.10887\n",
      "Main effects training epoch: 48, train loss: 0.10584, val loss: 0.10888\n",
      "Main effects training epoch: 49, train loss: 0.10576, val loss: 0.10866\n",
      "Main effects training epoch: 50, train loss: 0.10617, val loss: 0.10903\n",
      "Main effects training epoch: 51, train loss: 0.10592, val loss: 0.10893\n",
      "Main effects training epoch: 52, train loss: 0.10606, val loss: 0.10892\n",
      "Main effects training epoch: 53, train loss: 0.10577, val loss: 0.10888\n",
      "Main effects training epoch: 54, train loss: 0.10586, val loss: 0.10875\n",
      "Main effects training epoch: 55, train loss: 0.10635, val loss: 0.10929\n",
      "Main effects training epoch: 56, train loss: 0.10600, val loss: 0.10895\n",
      "Main effects training epoch: 57, train loss: 0.10614, val loss: 0.10942\n",
      "Main effects training epoch: 58, train loss: 0.10591, val loss: 0.10905\n",
      "Main effects training epoch: 59, train loss: 0.10587, val loss: 0.10866\n",
      "Main effects training epoch: 60, train loss: 0.10580, val loss: 0.10885\n",
      "Main effects training epoch: 61, train loss: 0.10590, val loss: 0.10868\n",
      "Main effects training epoch: 62, train loss: 0.10611, val loss: 0.10911\n",
      "Main effects training epoch: 63, train loss: 0.10587, val loss: 0.10877\n",
      "Main effects training epoch: 64, train loss: 0.10611, val loss: 0.10908\n",
      "Main effects training epoch: 65, train loss: 0.10597, val loss: 0.10888\n",
      "Main effects training epoch: 66, train loss: 0.10579, val loss: 0.10886\n",
      "Main effects training epoch: 67, train loss: 0.10588, val loss: 0.10853\n",
      "Main effects training epoch: 68, train loss: 0.10601, val loss: 0.10926\n",
      "Main effects training epoch: 69, train loss: 0.10583, val loss: 0.10866\n",
      "Main effects training epoch: 70, train loss: 0.10585, val loss: 0.10893\n",
      "Main effects training epoch: 71, train loss: 0.10592, val loss: 0.10879\n",
      "Main effects training epoch: 72, train loss: 0.10582, val loss: 0.10901\n",
      "Main effects training epoch: 73, train loss: 0.10586, val loss: 0.10890\n",
      "Main effects training epoch: 74, train loss: 0.10596, val loss: 0.10893\n",
      "Main effects training epoch: 75, train loss: 0.10614, val loss: 0.10884\n",
      "Main effects training epoch: 76, train loss: 0.10583, val loss: 0.10904\n",
      "Main effects training epoch: 77, train loss: 0.10592, val loss: 0.10890\n",
      "Main effects training epoch: 78, train loss: 0.10580, val loss: 0.10881\n",
      "Main effects training epoch: 79, train loss: 0.10594, val loss: 0.10889\n",
      "Main effects training epoch: 80, train loss: 0.10607, val loss: 0.10888\n",
      "Main effects training epoch: 81, train loss: 0.10581, val loss: 0.10891\n",
      "Main effects training epoch: 82, train loss: 0.10593, val loss: 0.10922\n",
      "Main effects training epoch: 83, train loss: 0.10574, val loss: 0.10873\n",
      "Main effects training epoch: 84, train loss: 0.10573, val loss: 0.10877\n",
      "Main effects training epoch: 85, train loss: 0.10636, val loss: 0.10929\n",
      "Main effects training epoch: 86, train loss: 0.10583, val loss: 0.10874\n",
      "Main effects training epoch: 87, train loss: 0.10614, val loss: 0.10928\n",
      "Main effects training epoch: 88, train loss: 0.10616, val loss: 0.10949\n",
      "Main effects training epoch: 89, train loss: 0.10589, val loss: 0.10865\n",
      "Main effects training epoch: 90, train loss: 0.10604, val loss: 0.10903\n",
      "Main effects training epoch: 91, train loss: 0.10588, val loss: 0.10892\n",
      "Main effects training epoch: 92, train loss: 0.10612, val loss: 0.10925\n",
      "Main effects training epoch: 93, train loss: 0.10583, val loss: 0.10911\n",
      "Main effects training epoch: 94, train loss: 0.10604, val loss: 0.10904\n",
      "Main effects training epoch: 95, train loss: 0.10576, val loss: 0.10903\n",
      "Main effects training epoch: 96, train loss: 0.10568, val loss: 0.10869\n",
      "Main effects training epoch: 97, train loss: 0.10588, val loss: 0.10926\n",
      "Main effects training epoch: 98, train loss: 0.10606, val loss: 0.10910\n",
      "Main effects training epoch: 99, train loss: 0.10649, val loss: 0.10930\n",
      "Main effects training epoch: 100, train loss: 0.10628, val loss: 0.10948\n",
      "Main effects training epoch: 101, train loss: 0.10586, val loss: 0.10912\n",
      "Main effects training epoch: 102, train loss: 0.10570, val loss: 0.10869\n",
      "Main effects training epoch: 103, train loss: 0.10643, val loss: 0.10951\n",
      "Main effects training epoch: 104, train loss: 0.10650, val loss: 0.10946\n",
      "Main effects training epoch: 105, train loss: 0.10626, val loss: 0.10954\n",
      "Main effects training epoch: 106, train loss: 0.10594, val loss: 0.10896\n",
      "Main effects training epoch: 107, train loss: 0.10579, val loss: 0.10878\n",
      "Main effects training epoch: 108, train loss: 0.10607, val loss: 0.10922\n",
      "Main effects training epoch: 109, train loss: 0.10581, val loss: 0.10902\n",
      "Main effects training epoch: 110, train loss: 0.10590, val loss: 0.10892\n",
      "Main effects training epoch: 111, train loss: 0.10605, val loss: 0.10924\n",
      "Main effects training epoch: 112, train loss: 0.10580, val loss: 0.10875\n",
      "Main effects training epoch: 113, train loss: 0.10587, val loss: 0.10900\n",
      "Main effects training epoch: 114, train loss: 0.10581, val loss: 0.10883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 115, train loss: 0.10618, val loss: 0.10915\n",
      "Main effects training epoch: 116, train loss: 0.10615, val loss: 0.10949\n",
      "Main effects training epoch: 117, train loss: 0.10602, val loss: 0.10894\n",
      "Main effects training epoch: 118, train loss: 0.10595, val loss: 0.10914\n",
      "Main effects training epoch: 119, train loss: 0.10618, val loss: 0.10895\n",
      "Main effects training epoch: 120, train loss: 0.10601, val loss: 0.10941\n",
      "Main effects training epoch: 121, train loss: 0.10588, val loss: 0.10869\n",
      "Main effects training epoch: 122, train loss: 0.10576, val loss: 0.10885\n",
      "Main effects training epoch: 123, train loss: 0.10578, val loss: 0.10899\n",
      "Main effects training epoch: 124, train loss: 0.10583, val loss: 0.10934\n",
      "Main effects training epoch: 125, train loss: 0.10610, val loss: 0.10904\n",
      "Main effects training epoch: 126, train loss: 0.10593, val loss: 0.10917\n",
      "Main effects training epoch: 127, train loss: 0.10579, val loss: 0.10863\n",
      "Main effects training epoch: 128, train loss: 0.10583, val loss: 0.10933\n",
      "Main effects training epoch: 129, train loss: 0.10580, val loss: 0.10919\n",
      "Main effects training epoch: 130, train loss: 0.10573, val loss: 0.10874\n",
      "Main effects training epoch: 131, train loss: 0.10601, val loss: 0.10951\n",
      "Main effects training epoch: 132, train loss: 0.10600, val loss: 0.10905\n",
      "Main effects training epoch: 133, train loss: 0.10601, val loss: 0.10931\n",
      "Main effects training epoch: 134, train loss: 0.10566, val loss: 0.10882\n",
      "Main effects training epoch: 135, train loss: 0.10590, val loss: 0.10909\n",
      "Main effects training epoch: 136, train loss: 0.10621, val loss: 0.10930\n",
      "Main effects training epoch: 137, train loss: 0.10597, val loss: 0.10906\n",
      "Main effects training epoch: 138, train loss: 0.10583, val loss: 0.10895\n",
      "Main effects training epoch: 139, train loss: 0.10580, val loss: 0.10894\n",
      "Main effects training epoch: 140, train loss: 0.10591, val loss: 0.10891\n",
      "Main effects training epoch: 141, train loss: 0.10595, val loss: 0.10903\n",
      "Main effects training epoch: 142, train loss: 0.10601, val loss: 0.10932\n",
      "Main effects training epoch: 143, train loss: 0.10596, val loss: 0.10892\n",
      "Main effects training epoch: 144, train loss: 0.10583, val loss: 0.10925\n",
      "Main effects training epoch: 145, train loss: 0.10572, val loss: 0.10887\n",
      "Main effects training epoch: 146, train loss: 0.10574, val loss: 0.10897\n",
      "Main effects training epoch: 147, train loss: 0.10575, val loss: 0.10903\n",
      "Main effects training epoch: 148, train loss: 0.10589, val loss: 0.10894\n",
      "Main effects training epoch: 149, train loss: 0.10577, val loss: 0.10889\n",
      "Main effects training epoch: 150, train loss: 0.10569, val loss: 0.10895\n",
      "Main effects training epoch: 151, train loss: 0.10575, val loss: 0.10884\n",
      "Main effects training epoch: 152, train loss: 0.10606, val loss: 0.10908\n",
      "Main effects training epoch: 153, train loss: 0.10639, val loss: 0.10965\n",
      "Main effects training epoch: 154, train loss: 0.10596, val loss: 0.10920\n",
      "Main effects training epoch: 155, train loss: 0.10578, val loss: 0.10908\n",
      "Main effects training epoch: 156, train loss: 0.10569, val loss: 0.10881\n",
      "Main effects training epoch: 157, train loss: 0.10570, val loss: 0.10898\n",
      "Main effects training epoch: 158, train loss: 0.10595, val loss: 0.10931\n",
      "Main effects training epoch: 159, train loss: 0.10577, val loss: 0.10920\n",
      "Main effects training epoch: 160, train loss: 0.10578, val loss: 0.10935\n",
      "Main effects training epoch: 161, train loss: 0.10579, val loss: 0.10872\n",
      "Main effects training epoch: 162, train loss: 0.10575, val loss: 0.10904\n",
      "Main effects training epoch: 163, train loss: 0.10597, val loss: 0.10913\n",
      "Main effects training epoch: 164, train loss: 0.10605, val loss: 0.10923\n",
      "Main effects training epoch: 165, train loss: 0.10604, val loss: 0.10954\n",
      "Main effects training epoch: 166, train loss: 0.10575, val loss: 0.10883\n",
      "Main effects training epoch: 167, train loss: 0.10578, val loss: 0.10918\n",
      "Main effects training epoch: 168, train loss: 0.10576, val loss: 0.10888\n",
      "Early stop at epoch 168, with validation loss: 0.10888\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.10587, val loss: 0.10915\n",
      "Main effects tuning epoch: 2, train loss: 0.10583, val loss: 0.10863\n",
      "Main effects tuning epoch: 3, train loss: 0.10593, val loss: 0.10918\n",
      "Main effects tuning epoch: 4, train loss: 0.10580, val loss: 0.10863\n",
      "Main effects tuning epoch: 5, train loss: 0.10595, val loss: 0.10896\n",
      "Main effects tuning epoch: 6, train loss: 0.10614, val loss: 0.10907\n",
      "Main effects tuning epoch: 7, train loss: 0.10617, val loss: 0.10891\n",
      "Main effects tuning epoch: 8, train loss: 0.10639, val loss: 0.10952\n",
      "Main effects tuning epoch: 9, train loss: 0.10631, val loss: 0.10952\n",
      "Main effects tuning epoch: 10, train loss: 0.10598, val loss: 0.10913\n",
      "Main effects tuning epoch: 11, train loss: 0.10616, val loss: 0.10872\n",
      "Main effects tuning epoch: 12, train loss: 0.10613, val loss: 0.10940\n",
      "Main effects tuning epoch: 13, train loss: 0.10649, val loss: 0.10913\n",
      "Main effects tuning epoch: 14, train loss: 0.10595, val loss: 0.10906\n",
      "Main effects tuning epoch: 15, train loss: 0.10576, val loss: 0.10901\n",
      "Main effects tuning epoch: 16, train loss: 0.10591, val loss: 0.10867\n",
      "Main effects tuning epoch: 17, train loss: 0.10614, val loss: 0.10929\n",
      "Main effects tuning epoch: 18, train loss: 0.10616, val loss: 0.10914\n",
      "Main effects tuning epoch: 19, train loss: 0.10591, val loss: 0.10901\n",
      "Main effects tuning epoch: 20, train loss: 0.10585, val loss: 0.10893\n",
      "Main effects tuning epoch: 21, train loss: 0.10582, val loss: 0.10851\n",
      "Main effects tuning epoch: 22, train loss: 0.10595, val loss: 0.10917\n",
      "Main effects tuning epoch: 23, train loss: 0.10589, val loss: 0.10893\n",
      "Main effects tuning epoch: 24, train loss: 0.10579, val loss: 0.10899\n",
      "Main effects tuning epoch: 25, train loss: 0.10580, val loss: 0.10907\n",
      "Main effects tuning epoch: 26, train loss: 0.10612, val loss: 0.10929\n",
      "Main effects tuning epoch: 27, train loss: 0.10613, val loss: 0.10892\n",
      "Main effects tuning epoch: 28, train loss: 0.10608, val loss: 0.10898\n",
      "Main effects tuning epoch: 29, train loss: 0.10588, val loss: 0.10904\n",
      "Main effects tuning epoch: 30, train loss: 0.10603, val loss: 0.10875\n",
      "Main effects tuning epoch: 31, train loss: 0.10580, val loss: 0.10885\n",
      "Main effects tuning epoch: 32, train loss: 0.10586, val loss: 0.10892\n",
      "Main effects tuning epoch: 33, train loss: 0.10584, val loss: 0.10877\n",
      "Main effects tuning epoch: 34, train loss: 0.10610, val loss: 0.10890\n",
      "Main effects tuning epoch: 35, train loss: 0.10602, val loss: 0.10950\n",
      "Main effects tuning epoch: 36, train loss: 0.10577, val loss: 0.10870\n",
      "Main effects tuning epoch: 37, train loss: 0.10576, val loss: 0.10890\n",
      "Main effects tuning epoch: 38, train loss: 0.10590, val loss: 0.10894\n",
      "Main effects tuning epoch: 39, train loss: 0.10597, val loss: 0.10915\n",
      "Main effects tuning epoch: 40, train loss: 0.10575, val loss: 0.10856\n",
      "Main effects tuning epoch: 41, train loss: 0.10577, val loss: 0.10899\n",
      "Main effects tuning epoch: 42, train loss: 0.10600, val loss: 0.10888\n",
      "Main effects tuning epoch: 43, train loss: 0.10624, val loss: 0.10938\n",
      "Main effects tuning epoch: 44, train loss: 0.10606, val loss: 0.10863\n",
      "Main effects tuning epoch: 45, train loss: 0.10614, val loss: 0.10961\n",
      "Main effects tuning epoch: 46, train loss: 0.10598, val loss: 0.10874\n",
      "Main effects tuning epoch: 47, train loss: 0.10623, val loss: 0.10916\n",
      "Main effects tuning epoch: 48, train loss: 0.10628, val loss: 0.10910\n",
      "Main effects tuning epoch: 49, train loss: 0.10612, val loss: 0.10902\n",
      "Main effects tuning epoch: 50, train loss: 0.10600, val loss: 0.10922\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.16656, val loss: 0.16504\n",
      "Interaction training epoch: 2, train loss: 0.19799, val loss: 0.19730\n",
      "Interaction training epoch: 3, train loss: 0.07919, val loss: 0.08135\n",
      "Interaction training epoch: 4, train loss: 0.06592, val loss: 0.06695\n",
      "Interaction training epoch: 5, train loss: 0.06719, val loss: 0.06623\n",
      "Interaction training epoch: 6, train loss: 0.07116, val loss: 0.06992\n",
      "Interaction training epoch: 7, train loss: 0.05765, val loss: 0.05869\n",
      "Interaction training epoch: 8, train loss: 0.05791, val loss: 0.05822\n",
      "Interaction training epoch: 9, train loss: 0.06091, val loss: 0.06046\n",
      "Interaction training epoch: 10, train loss: 0.05754, val loss: 0.05788\n",
      "Interaction training epoch: 11, train loss: 0.05624, val loss: 0.05624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 12, train loss: 0.05651, val loss: 0.05600\n",
      "Interaction training epoch: 13, train loss: 0.06101, val loss: 0.06080\n",
      "Interaction training epoch: 14, train loss: 0.05709, val loss: 0.05641\n",
      "Interaction training epoch: 15, train loss: 0.06077, val loss: 0.06101\n",
      "Interaction training epoch: 16, train loss: 0.05239, val loss: 0.05197\n",
      "Interaction training epoch: 17, train loss: 0.05618, val loss: 0.05592\n",
      "Interaction training epoch: 18, train loss: 0.05477, val loss: 0.05468\n",
      "Interaction training epoch: 19, train loss: 0.05152, val loss: 0.05113\n",
      "Interaction training epoch: 20, train loss: 0.05519, val loss: 0.05482\n",
      "Interaction training epoch: 21, train loss: 0.05818, val loss: 0.05633\n",
      "Interaction training epoch: 22, train loss: 0.05614, val loss: 0.05550\n",
      "Interaction training epoch: 23, train loss: 0.05384, val loss: 0.05252\n",
      "Interaction training epoch: 24, train loss: 0.05143, val loss: 0.05136\n",
      "Interaction training epoch: 25, train loss: 0.05185, val loss: 0.05118\n",
      "Interaction training epoch: 26, train loss: 0.05409, val loss: 0.05515\n",
      "Interaction training epoch: 27, train loss: 0.05171, val loss: 0.05189\n",
      "Interaction training epoch: 28, train loss: 0.05284, val loss: 0.05307\n",
      "Interaction training epoch: 29, train loss: 0.05251, val loss: 0.05306\n",
      "Interaction training epoch: 30, train loss: 0.05654, val loss: 0.05667\n",
      "Interaction training epoch: 31, train loss: 0.05008, val loss: 0.05013\n",
      "Interaction training epoch: 32, train loss: 0.05165, val loss: 0.05175\n",
      "Interaction training epoch: 33, train loss: 0.05094, val loss: 0.05147\n",
      "Interaction training epoch: 34, train loss: 0.05154, val loss: 0.05255\n",
      "Interaction training epoch: 35, train loss: 0.05178, val loss: 0.05163\n",
      "Interaction training epoch: 36, train loss: 0.05189, val loss: 0.05137\n",
      "Interaction training epoch: 37, train loss: 0.05261, val loss: 0.05262\n",
      "Interaction training epoch: 38, train loss: 0.05395, val loss: 0.05444\n",
      "Interaction training epoch: 39, train loss: 0.05053, val loss: 0.05093\n",
      "Interaction training epoch: 40, train loss: 0.05186, val loss: 0.05165\n",
      "Interaction training epoch: 41, train loss: 0.05174, val loss: 0.05183\n",
      "Interaction training epoch: 42, train loss: 0.05645, val loss: 0.05707\n",
      "Interaction training epoch: 43, train loss: 0.05130, val loss: 0.05143\n",
      "Interaction training epoch: 44, train loss: 0.05233, val loss: 0.05213\n",
      "Interaction training epoch: 45, train loss: 0.05242, val loss: 0.05266\n",
      "Interaction training epoch: 46, train loss: 0.05234, val loss: 0.05256\n",
      "Interaction training epoch: 47, train loss: 0.05096, val loss: 0.05121\n",
      "Interaction training epoch: 48, train loss: 0.05324, val loss: 0.05317\n",
      "Interaction training epoch: 49, train loss: 0.05143, val loss: 0.05175\n",
      "Interaction training epoch: 50, train loss: 0.05906, val loss: 0.05866\n",
      "Interaction training epoch: 51, train loss: 0.05278, val loss: 0.05220\n",
      "Interaction training epoch: 52, train loss: 0.05429, val loss: 0.05321\n",
      "Interaction training epoch: 53, train loss: 0.05247, val loss: 0.05225\n",
      "Interaction training epoch: 54, train loss: 0.05061, val loss: 0.05129\n",
      "Interaction training epoch: 55, train loss: 0.05239, val loss: 0.05237\n",
      "Interaction training epoch: 56, train loss: 0.05174, val loss: 0.05287\n",
      "Interaction training epoch: 57, train loss: 0.05160, val loss: 0.05111\n",
      "Interaction training epoch: 58, train loss: 0.05180, val loss: 0.05292\n",
      "Interaction training epoch: 59, train loss: 0.04982, val loss: 0.05009\n",
      "Interaction training epoch: 60, train loss: 0.05227, val loss: 0.05368\n",
      "Interaction training epoch: 61, train loss: 0.05217, val loss: 0.05210\n",
      "Interaction training epoch: 62, train loss: 0.05023, val loss: 0.05074\n",
      "Interaction training epoch: 63, train loss: 0.05179, val loss: 0.05284\n",
      "Interaction training epoch: 64, train loss: 0.04959, val loss: 0.05129\n",
      "Interaction training epoch: 65, train loss: 0.05320, val loss: 0.05225\n",
      "Interaction training epoch: 66, train loss: 0.05295, val loss: 0.05326\n",
      "Interaction training epoch: 67, train loss: 0.05028, val loss: 0.05144\n",
      "Interaction training epoch: 68, train loss: 0.05112, val loss: 0.05151\n",
      "Interaction training epoch: 69, train loss: 0.05228, val loss: 0.05248\n",
      "Interaction training epoch: 70, train loss: 0.04994, val loss: 0.05062\n",
      "Interaction training epoch: 71, train loss: 0.05225, val loss: 0.05286\n",
      "Interaction training epoch: 72, train loss: 0.05069, val loss: 0.05180\n",
      "Interaction training epoch: 73, train loss: 0.05077, val loss: 0.05196\n",
      "Interaction training epoch: 74, train loss: 0.04967, val loss: 0.05076\n",
      "Interaction training epoch: 75, train loss: 0.05064, val loss: 0.05071\n",
      "Interaction training epoch: 76, train loss: 0.04886, val loss: 0.05004\n",
      "Interaction training epoch: 77, train loss: 0.05097, val loss: 0.05075\n",
      "Interaction training epoch: 78, train loss: 0.05171, val loss: 0.05128\n",
      "Interaction training epoch: 79, train loss: 0.05178, val loss: 0.05168\n",
      "Interaction training epoch: 80, train loss: 0.04972, val loss: 0.05111\n",
      "Interaction training epoch: 81, train loss: 0.05307, val loss: 0.05407\n",
      "Interaction training epoch: 82, train loss: 0.05287, val loss: 0.05331\n",
      "Interaction training epoch: 83, train loss: 0.05019, val loss: 0.05065\n",
      "Interaction training epoch: 84, train loss: 0.05109, val loss: 0.05177\n",
      "Interaction training epoch: 85, train loss: 0.05241, val loss: 0.05395\n",
      "Interaction training epoch: 86, train loss: 0.05169, val loss: 0.05187\n",
      "Interaction training epoch: 87, train loss: 0.05013, val loss: 0.05031\n",
      "Interaction training epoch: 88, train loss: 0.05249, val loss: 0.05324\n",
      "Interaction training epoch: 89, train loss: 0.04905, val loss: 0.04955\n",
      "Interaction training epoch: 90, train loss: 0.05048, val loss: 0.05104\n",
      "Interaction training epoch: 91, train loss: 0.05291, val loss: 0.05320\n",
      "Interaction training epoch: 92, train loss: 0.05194, val loss: 0.05184\n",
      "Interaction training epoch: 93, train loss: 0.04813, val loss: 0.04857\n",
      "Interaction training epoch: 94, train loss: 0.05434, val loss: 0.05466\n",
      "Interaction training epoch: 95, train loss: 0.05394, val loss: 0.05306\n",
      "Interaction training epoch: 96, train loss: 0.04954, val loss: 0.04995\n",
      "Interaction training epoch: 97, train loss: 0.05003, val loss: 0.05058\n",
      "Interaction training epoch: 98, train loss: 0.05197, val loss: 0.05219\n",
      "Interaction training epoch: 99, train loss: 0.05436, val loss: 0.05280\n",
      "Interaction training epoch: 100, train loss: 0.05350, val loss: 0.05437\n",
      "Interaction training epoch: 101, train loss: 0.04964, val loss: 0.04980\n",
      "Interaction training epoch: 102, train loss: 0.05136, val loss: 0.05087\n",
      "Interaction training epoch: 103, train loss: 0.05162, val loss: 0.05191\n",
      "Interaction training epoch: 104, train loss: 0.05118, val loss: 0.05150\n",
      "Interaction training epoch: 105, train loss: 0.04993, val loss: 0.04937\n",
      "Interaction training epoch: 106, train loss: 0.05171, val loss: 0.05164\n",
      "Interaction training epoch: 107, train loss: 0.05193, val loss: 0.05075\n",
      "Interaction training epoch: 108, train loss: 0.05102, val loss: 0.05183\n",
      "Interaction training epoch: 109, train loss: 0.05261, val loss: 0.05324\n",
      "Interaction training epoch: 110, train loss: 0.05047, val loss: 0.05100\n",
      "Interaction training epoch: 111, train loss: 0.05046, val loss: 0.05063\n",
      "Interaction training epoch: 112, train loss: 0.04811, val loss: 0.04809\n",
      "Interaction training epoch: 113, train loss: 0.05250, val loss: 0.05298\n",
      "Interaction training epoch: 114, train loss: 0.05036, val loss: 0.05112\n",
      "Interaction training epoch: 115, train loss: 0.05119, val loss: 0.05140\n",
      "Interaction training epoch: 116, train loss: 0.04975, val loss: 0.05112\n",
      "Interaction training epoch: 117, train loss: 0.04911, val loss: 0.04994\n",
      "Interaction training epoch: 118, train loss: 0.04990, val loss: 0.05012\n",
      "Interaction training epoch: 119, train loss: 0.04854, val loss: 0.04775\n",
      "Interaction training epoch: 120, train loss: 0.04985, val loss: 0.04987\n",
      "Interaction training epoch: 121, train loss: 0.04779, val loss: 0.04856\n",
      "Interaction training epoch: 122, train loss: 0.04712, val loss: 0.04773\n",
      "Interaction training epoch: 123, train loss: 0.05164, val loss: 0.05255\n",
      "Interaction training epoch: 124, train loss: 0.05327, val loss: 0.05365\n",
      "Interaction training epoch: 125, train loss: 0.05348, val loss: 0.05414\n",
      "Interaction training epoch: 126, train loss: 0.04873, val loss: 0.04978\n",
      "Interaction training epoch: 127, train loss: 0.04866, val loss: 0.04894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 128, train loss: 0.04947, val loss: 0.05037\n",
      "Interaction training epoch: 129, train loss: 0.04701, val loss: 0.04711\n",
      "Interaction training epoch: 130, train loss: 0.04867, val loss: 0.04878\n",
      "Interaction training epoch: 131, train loss: 0.04995, val loss: 0.05004\n",
      "Interaction training epoch: 132, train loss: 0.05004, val loss: 0.05008\n",
      "Interaction training epoch: 133, train loss: 0.04970, val loss: 0.05023\n",
      "Interaction training epoch: 134, train loss: 0.04969, val loss: 0.05041\n",
      "Interaction training epoch: 135, train loss: 0.04826, val loss: 0.04935\n",
      "Interaction training epoch: 136, train loss: 0.04956, val loss: 0.05077\n",
      "Interaction training epoch: 137, train loss: 0.04937, val loss: 0.05116\n",
      "Interaction training epoch: 138, train loss: 0.05042, val loss: 0.05126\n",
      "Interaction training epoch: 139, train loss: 0.04910, val loss: 0.04944\n",
      "Interaction training epoch: 140, train loss: 0.05235, val loss: 0.05329\n",
      "Interaction training epoch: 141, train loss: 0.05151, val loss: 0.05199\n",
      "Interaction training epoch: 142, train loss: 0.04935, val loss: 0.04965\n",
      "Interaction training epoch: 143, train loss: 0.05663, val loss: 0.05667\n",
      "Interaction training epoch: 144, train loss: 0.05217, val loss: 0.05297\n",
      "Interaction training epoch: 145, train loss: 0.05139, val loss: 0.05143\n",
      "Interaction training epoch: 146, train loss: 0.05350, val loss: 0.05388\n",
      "Interaction training epoch: 147, train loss: 0.06195, val loss: 0.06171\n",
      "Interaction training epoch: 148, train loss: 0.05923, val loss: 0.05958\n",
      "Interaction training epoch: 149, train loss: 0.05897, val loss: 0.05842\n",
      "Interaction training epoch: 150, train loss: 0.05688, val loss: 0.05663\n",
      "Interaction training epoch: 151, train loss: 0.05292, val loss: 0.05339\n",
      "Interaction training epoch: 152, train loss: 0.04638, val loss: 0.04708\n",
      "Interaction training epoch: 153, train loss: 0.04830, val loss: 0.05070\n",
      "Interaction training epoch: 154, train loss: 0.04798, val loss: 0.04952\n",
      "Interaction training epoch: 155, train loss: 0.04659, val loss: 0.04786\n",
      "Interaction training epoch: 156, train loss: 0.04767, val loss: 0.04876\n",
      "Interaction training epoch: 157, train loss: 0.04853, val loss: 0.04917\n",
      "Interaction training epoch: 158, train loss: 0.04677, val loss: 0.04803\n",
      "Interaction training epoch: 159, train loss: 0.04901, val loss: 0.04931\n",
      "Interaction training epoch: 160, train loss: 0.04754, val loss: 0.04893\n",
      "Interaction training epoch: 161, train loss: 0.04874, val loss: 0.04950\n",
      "Interaction training epoch: 162, train loss: 0.04595, val loss: 0.04721\n",
      "Interaction training epoch: 163, train loss: 0.04732, val loss: 0.04956\n",
      "Interaction training epoch: 164, train loss: 0.04567, val loss: 0.04749\n",
      "Interaction training epoch: 165, train loss: 0.05013, val loss: 0.05115\n",
      "Interaction training epoch: 166, train loss: 0.04663, val loss: 0.04790\n",
      "Interaction training epoch: 167, train loss: 0.04602, val loss: 0.04784\n",
      "Interaction training epoch: 168, train loss: 0.04789, val loss: 0.04800\n",
      "Interaction training epoch: 169, train loss: 0.04480, val loss: 0.04615\n",
      "Interaction training epoch: 170, train loss: 0.04657, val loss: 0.04712\n",
      "Interaction training epoch: 171, train loss: 0.04515, val loss: 0.04688\n",
      "Interaction training epoch: 172, train loss: 0.04745, val loss: 0.04904\n",
      "Interaction training epoch: 173, train loss: 0.04801, val loss: 0.05050\n",
      "Interaction training epoch: 174, train loss: 0.05297, val loss: 0.05389\n",
      "Interaction training epoch: 175, train loss: 0.04522, val loss: 0.04687\n",
      "Interaction training epoch: 176, train loss: 0.04654, val loss: 0.04709\n",
      "Interaction training epoch: 177, train loss: 0.04462, val loss: 0.04576\n",
      "Interaction training epoch: 178, train loss: 0.04715, val loss: 0.04880\n",
      "Interaction training epoch: 179, train loss: 0.04515, val loss: 0.04673\n",
      "Interaction training epoch: 180, train loss: 0.04652, val loss: 0.04824\n",
      "Interaction training epoch: 181, train loss: 0.04757, val loss: 0.04936\n",
      "Interaction training epoch: 182, train loss: 0.04771, val loss: 0.04909\n",
      "Interaction training epoch: 183, train loss: 0.04999, val loss: 0.05186\n",
      "Interaction training epoch: 184, train loss: 0.04691, val loss: 0.04877\n",
      "Interaction training epoch: 185, train loss: 0.04599, val loss: 0.04763\n",
      "Interaction training epoch: 186, train loss: 0.04839, val loss: 0.04969\n",
      "Interaction training epoch: 187, train loss: 0.04634, val loss: 0.04846\n",
      "Interaction training epoch: 188, train loss: 0.04686, val loss: 0.04828\n",
      "Interaction training epoch: 189, train loss: 0.04527, val loss: 0.04603\n",
      "Interaction training epoch: 190, train loss: 0.04811, val loss: 0.05046\n",
      "Interaction training epoch: 191, train loss: 0.04668, val loss: 0.04794\n",
      "Interaction training epoch: 192, train loss: 0.04580, val loss: 0.04673\n",
      "Interaction training epoch: 193, train loss: 0.04953, val loss: 0.05077\n",
      "Interaction training epoch: 194, train loss: 0.04637, val loss: 0.04745\n",
      "Interaction training epoch: 195, train loss: 0.05291, val loss: 0.05606\n",
      "Interaction training epoch: 196, train loss: 0.04552, val loss: 0.04706\n",
      "Interaction training epoch: 197, train loss: 0.04954, val loss: 0.05175\n",
      "Interaction training epoch: 198, train loss: 0.04504, val loss: 0.04659\n",
      "Interaction training epoch: 199, train loss: 0.04939, val loss: 0.05068\n",
      "Interaction training epoch: 200, train loss: 0.04824, val loss: 0.04969\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########7 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.04624, val loss: 0.04648\n",
      "Interaction tuning epoch: 2, train loss: 0.04654, val loss: 0.04675\n",
      "Interaction tuning epoch: 3, train loss: 0.04688, val loss: 0.04770\n",
      "Interaction tuning epoch: 4, train loss: 0.04703, val loss: 0.04714\n",
      "Interaction tuning epoch: 5, train loss: 0.04871, val loss: 0.04870\n",
      "Interaction tuning epoch: 6, train loss: 0.04715, val loss: 0.04685\n",
      "Interaction tuning epoch: 7, train loss: 0.04753, val loss: 0.04769\n",
      "Interaction tuning epoch: 8, train loss: 0.04804, val loss: 0.04839\n",
      "Interaction tuning epoch: 9, train loss: 0.04656, val loss: 0.04746\n",
      "Interaction tuning epoch: 10, train loss: 0.04708, val loss: 0.04792\n",
      "Interaction tuning epoch: 11, train loss: 0.04593, val loss: 0.04607\n",
      "Interaction tuning epoch: 12, train loss: 0.04889, val loss: 0.05019\n",
      "Interaction tuning epoch: 13, train loss: 0.04756, val loss: 0.04795\n",
      "Interaction tuning epoch: 14, train loss: 0.05056, val loss: 0.04891\n",
      "Interaction tuning epoch: 15, train loss: 0.05649, val loss: 0.05700\n",
      "Interaction tuning epoch: 16, train loss: 0.04770, val loss: 0.04777\n",
      "Interaction tuning epoch: 17, train loss: 0.04665, val loss: 0.04734\n",
      "Interaction tuning epoch: 18, train loss: 0.04709, val loss: 0.04763\n",
      "Interaction tuning epoch: 19, train loss: 0.04762, val loss: 0.04775\n",
      "Interaction tuning epoch: 20, train loss: 0.04647, val loss: 0.04691\n",
      "Interaction tuning epoch: 21, train loss: 0.04858, val loss: 0.04904\n",
      "Interaction tuning epoch: 22, train loss: 0.04763, val loss: 0.04743\n",
      "Interaction tuning epoch: 23, train loss: 0.04883, val loss: 0.04879\n",
      "Interaction tuning epoch: 24, train loss: 0.04608, val loss: 0.04728\n",
      "Interaction tuning epoch: 25, train loss: 0.04999, val loss: 0.04940\n",
      "Interaction tuning epoch: 26, train loss: 0.04916, val loss: 0.04857\n",
      "Interaction tuning epoch: 27, train loss: 0.04784, val loss: 0.04855\n",
      "Interaction tuning epoch: 28, train loss: 0.04702, val loss: 0.04758\n",
      "Interaction tuning epoch: 29, train loss: 0.04685, val loss: 0.04720\n",
      "Interaction tuning epoch: 30, train loss: 0.04688, val loss: 0.04800\n",
      "Interaction tuning epoch: 31, train loss: 0.04656, val loss: 0.04725\n",
      "Interaction tuning epoch: 32, train loss: 0.04698, val loss: 0.04718\n",
      "Interaction tuning epoch: 33, train loss: 0.04648, val loss: 0.04674\n",
      "Interaction tuning epoch: 34, train loss: 0.04622, val loss: 0.04627\n",
      "Interaction tuning epoch: 35, train loss: 0.04695, val loss: 0.04653\n",
      "Interaction tuning epoch: 36, train loss: 0.04638, val loss: 0.04708\n",
      "Interaction tuning epoch: 37, train loss: 0.04517, val loss: 0.04599\n",
      "Interaction tuning epoch: 38, train loss: 0.04594, val loss: 0.04626\n",
      "Interaction tuning epoch: 39, train loss: 0.04806, val loss: 0.04884\n",
      "Interaction tuning epoch: 40, train loss: 0.04762, val loss: 0.04781\n",
      "Interaction tuning epoch: 41, train loss: 0.04762, val loss: 0.04909\n",
      "Interaction tuning epoch: 42, train loss: 0.04746, val loss: 0.04805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 43, train loss: 0.04604, val loss: 0.04612\n",
      "Interaction tuning epoch: 44, train loss: 0.04636, val loss: 0.04673\n",
      "Interaction tuning epoch: 45, train loss: 0.04643, val loss: 0.04712\n",
      "Interaction tuning epoch: 46, train loss: 0.04656, val loss: 0.04678\n",
      "Interaction tuning epoch: 47, train loss: 0.04635, val loss: 0.04667\n",
      "Interaction tuning epoch: 48, train loss: 0.04816, val loss: 0.04882\n",
      "Interaction tuning epoch: 49, train loss: 0.04683, val loss: 0.04711\n",
      "Interaction tuning epoch: 50, train loss: 0.04845, val loss: 0.04905\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 33.7941198348999\n",
      "After the gam stage, training error is 0.04845 , validation error is 0.04905\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 1.232198\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.043115 validation MAE=0.046656,rank=2\n",
      "[SoftImpute] Iter 2: observed MAE=0.040993 validation MAE=0.045569,rank=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 3: observed MAE=0.039180 validation MAE=0.044575,rank=2\n",
      "[SoftImpute] Iter 4: observed MAE=0.037615 validation MAE=0.043669,rank=2\n",
      "[SoftImpute] Iter 5: observed MAE=0.036253 validation MAE=0.042843,rank=2\n",
      "[SoftImpute] Iter 6: observed MAE=0.035064 validation MAE=0.042131,rank=2\n",
      "[SoftImpute] Iter 7: observed MAE=0.034025 validation MAE=0.041514,rank=2\n",
      "[SoftImpute] Iter 8: observed MAE=0.033103 validation MAE=0.040960,rank=2\n",
      "[SoftImpute] Iter 9: observed MAE=0.032286 validation MAE=0.040461,rank=2\n",
      "[SoftImpute] Iter 10: observed MAE=0.031551 validation MAE=0.040013,rank=2\n",
      "[SoftImpute] Iter 11: observed MAE=0.030891 validation MAE=0.039607,rank=2\n",
      "[SoftImpute] Iter 12: observed MAE=0.030304 validation MAE=0.039229,rank=2\n",
      "[SoftImpute] Iter 13: observed MAE=0.029774 validation MAE=0.038878,rank=2\n",
      "[SoftImpute] Iter 14: observed MAE=0.029297 validation MAE=0.038554,rank=2\n",
      "[SoftImpute] Iter 15: observed MAE=0.028865 validation MAE=0.038252,rank=2\n",
      "[SoftImpute] Iter 16: observed MAE=0.028471 validation MAE=0.037978,rank=2\n",
      "[SoftImpute] Iter 17: observed MAE=0.028113 validation MAE=0.037722,rank=2\n",
      "[SoftImpute] Iter 18: observed MAE=0.027785 validation MAE=0.037491,rank=2\n",
      "[SoftImpute] Iter 19: observed MAE=0.027486 validation MAE=0.037278,rank=2\n",
      "[SoftImpute] Iter 20: observed MAE=0.027211 validation MAE=0.037074,rank=2\n",
      "[SoftImpute] Iter 21: observed MAE=0.026956 validation MAE=0.036885,rank=2\n",
      "[SoftImpute] Iter 22: observed MAE=0.026718 validation MAE=0.036708,rank=2\n",
      "[SoftImpute] Iter 23: observed MAE=0.026497 validation MAE=0.036538,rank=2\n",
      "[SoftImpute] Iter 24: observed MAE=0.026293 validation MAE=0.036375,rank=2\n",
      "[SoftImpute] Iter 25: observed MAE=0.026101 validation MAE=0.036226,rank=2\n",
      "[SoftImpute] Iter 26: observed MAE=0.025924 validation MAE=0.036083,rank=2\n",
      "[SoftImpute] Iter 27: observed MAE=0.025758 validation MAE=0.035945,rank=2\n",
      "[SoftImpute] Iter 28: observed MAE=0.025602 validation MAE=0.035809,rank=2\n",
      "[SoftImpute] Iter 29: observed MAE=0.025455 validation MAE=0.035683,rank=2\n",
      "[SoftImpute] Iter 30: observed MAE=0.025316 validation MAE=0.035565,rank=2\n",
      "[SoftImpute] Iter 31: observed MAE=0.025184 validation MAE=0.035451,rank=2\n",
      "[SoftImpute] Iter 32: observed MAE=0.025060 validation MAE=0.035344,rank=2\n",
      "[SoftImpute] Iter 33: observed MAE=0.024941 validation MAE=0.035238,rank=2\n",
      "[SoftImpute] Iter 34: observed MAE=0.024827 validation MAE=0.035139,rank=2\n",
      "[SoftImpute] Iter 35: observed MAE=0.024718 validation MAE=0.035043,rank=2\n",
      "[SoftImpute] Iter 36: observed MAE=0.024614 validation MAE=0.034949,rank=2\n",
      "[SoftImpute] Iter 37: observed MAE=0.024515 validation MAE=0.034858,rank=2\n",
      "[SoftImpute] Iter 38: observed MAE=0.024421 validation MAE=0.034769,rank=2\n",
      "[SoftImpute] Iter 39: observed MAE=0.024330 validation MAE=0.034681,rank=2\n",
      "[SoftImpute] Iter 40: observed MAE=0.024243 validation MAE=0.034594,rank=2\n",
      "[SoftImpute] Iter 41: observed MAE=0.024160 validation MAE=0.034511,rank=2\n",
      "[SoftImpute] Iter 42: observed MAE=0.024080 validation MAE=0.034429,rank=2\n",
      "[SoftImpute] Iter 43: observed MAE=0.024003 validation MAE=0.034347,rank=2\n",
      "[SoftImpute] Iter 44: observed MAE=0.023929 validation MAE=0.034265,rank=2\n",
      "[SoftImpute] Iter 45: observed MAE=0.023857 validation MAE=0.034185,rank=2\n",
      "[SoftImpute] Iter 46: observed MAE=0.023787 validation MAE=0.034105,rank=2\n",
      "[SoftImpute] Iter 47: observed MAE=0.023720 validation MAE=0.034029,rank=2\n",
      "[SoftImpute] Iter 48: observed MAE=0.023656 validation MAE=0.033955,rank=2\n",
      "[SoftImpute] Iter 49: observed MAE=0.023595 validation MAE=0.033882,rank=2\n",
      "[SoftImpute] Iter 50: observed MAE=0.023536 validation MAE=0.033809,rank=2\n",
      "[SoftImpute] Iter 51: observed MAE=0.023479 validation MAE=0.033739,rank=2\n",
      "[SoftImpute] Iter 52: observed MAE=0.023423 validation MAE=0.033671,rank=2\n",
      "[SoftImpute] Iter 53: observed MAE=0.023370 validation MAE=0.033606,rank=2\n",
      "[SoftImpute] Iter 54: observed MAE=0.023318 validation MAE=0.033543,rank=2\n",
      "[SoftImpute] Iter 55: observed MAE=0.023268 validation MAE=0.033481,rank=2\n",
      "[SoftImpute] Iter 56: observed MAE=0.023219 validation MAE=0.033423,rank=2\n",
      "[SoftImpute] Iter 57: observed MAE=0.023171 validation MAE=0.033366,rank=2\n",
      "[SoftImpute] Iter 58: observed MAE=0.023125 validation MAE=0.033310,rank=2\n",
      "[SoftImpute] Iter 59: observed MAE=0.023080 validation MAE=0.033256,rank=2\n",
      "[SoftImpute] Iter 60: observed MAE=0.023036 validation MAE=0.033202,rank=2\n",
      "[SoftImpute] Iter 61: observed MAE=0.022994 validation MAE=0.033149,rank=2\n",
      "[SoftImpute] Iter 62: observed MAE=0.022953 validation MAE=0.033099,rank=2\n",
      "[SoftImpute] Iter 63: observed MAE=0.022914 validation MAE=0.033050,rank=2\n",
      "[SoftImpute] Iter 64: observed MAE=0.022876 validation MAE=0.033002,rank=2\n",
      "[SoftImpute] Iter 65: observed MAE=0.022839 validation MAE=0.032956,rank=2\n",
      "[SoftImpute] Iter 66: observed MAE=0.022802 validation MAE=0.032913,rank=2\n",
      "[SoftImpute] Iter 67: observed MAE=0.022767 validation MAE=0.032870,rank=2\n",
      "[SoftImpute] Iter 68: observed MAE=0.022732 validation MAE=0.032830,rank=2\n",
      "[SoftImpute] Iter 69: observed MAE=0.022698 validation MAE=0.032792,rank=2\n",
      "[SoftImpute] Iter 70: observed MAE=0.022665 validation MAE=0.032754,rank=2\n",
      "[SoftImpute] Iter 71: observed MAE=0.022633 validation MAE=0.032717,rank=2\n",
      "[SoftImpute] Iter 72: observed MAE=0.022601 validation MAE=0.032681,rank=2\n",
      "[SoftImpute] Iter 73: observed MAE=0.022570 validation MAE=0.032644,rank=2\n",
      "[SoftImpute] Iter 74: observed MAE=0.022539 validation MAE=0.032610,rank=2\n",
      "[SoftImpute] Iter 75: observed MAE=0.022509 validation MAE=0.032577,rank=2\n",
      "[SoftImpute] Iter 76: observed MAE=0.022479 validation MAE=0.032545,rank=2\n",
      "[SoftImpute] Iter 77: observed MAE=0.022451 validation MAE=0.032515,rank=2\n",
      "[SoftImpute] Iter 78: observed MAE=0.022423 validation MAE=0.032484,rank=2\n",
      "[SoftImpute] Iter 79: observed MAE=0.022396 validation MAE=0.032453,rank=2\n",
      "[SoftImpute] Iter 80: observed MAE=0.022369 validation MAE=0.032423,rank=2\n",
      "[SoftImpute] Iter 81: observed MAE=0.022344 validation MAE=0.032394,rank=2\n",
      "[SoftImpute] Iter 82: observed MAE=0.022318 validation MAE=0.032368,rank=2\n",
      "[SoftImpute] Iter 83: observed MAE=0.022294 validation MAE=0.032343,rank=2\n",
      "[SoftImpute] Iter 84: observed MAE=0.022269 validation MAE=0.032318,rank=2\n",
      "[SoftImpute] Iter 85: observed MAE=0.022245 validation MAE=0.032294,rank=2\n",
      "[SoftImpute] Iter 86: observed MAE=0.022222 validation MAE=0.032270,rank=2\n",
      "[SoftImpute] Iter 87: observed MAE=0.022199 validation MAE=0.032247,rank=2\n",
      "[SoftImpute] Iter 88: observed MAE=0.022177 validation MAE=0.032224,rank=2\n",
      "[SoftImpute] Iter 89: observed MAE=0.022156 validation MAE=0.032201,rank=2\n",
      "[SoftImpute] Iter 90: observed MAE=0.022134 validation MAE=0.032179,rank=2\n",
      "[SoftImpute] Iter 91: observed MAE=0.022114 validation MAE=0.032159,rank=2\n",
      "[SoftImpute] Iter 92: observed MAE=0.022094 validation MAE=0.032139,rank=2\n",
      "[SoftImpute] Iter 93: observed MAE=0.022074 validation MAE=0.032119,rank=2\n",
      "[SoftImpute] Iter 94: observed MAE=0.022055 validation MAE=0.032101,rank=2\n",
      "[SoftImpute] Iter 95: observed MAE=0.022036 validation MAE=0.032083,rank=2\n",
      "[SoftImpute] Iter 96: observed MAE=0.022018 validation MAE=0.032066,rank=2\n",
      "[SoftImpute] Iter 97: observed MAE=0.022001 validation MAE=0.032051,rank=2\n",
      "[SoftImpute] Iter 98: observed MAE=0.021983 validation MAE=0.032037,rank=2\n",
      "[SoftImpute] Iter 99: observed MAE=0.021966 validation MAE=0.032023,rank=2\n",
      "[SoftImpute] Iter 100: observed MAE=0.021950 validation MAE=0.032009,rank=2\n",
      "[SoftImpute] Iter 101: observed MAE=0.021934 validation MAE=0.031996,rank=2\n",
      "[SoftImpute] Iter 102: observed MAE=0.021918 validation MAE=0.031983,rank=2\n",
      "[SoftImpute] Iter 103: observed MAE=0.021903 validation MAE=0.031971,rank=2\n",
      "[SoftImpute] Iter 104: observed MAE=0.021888 validation MAE=0.031959,rank=2\n",
      "[SoftImpute] Iter 105: observed MAE=0.021873 validation MAE=0.031948,rank=2\n",
      "[SoftImpute] Iter 106: observed MAE=0.021859 validation MAE=0.031936,rank=2\n",
      "[SoftImpute] Iter 107: observed MAE=0.021845 validation MAE=0.031925,rank=2\n",
      "[SoftImpute] Iter 108: observed MAE=0.021831 validation MAE=0.031914,rank=2\n",
      "[SoftImpute] Iter 109: observed MAE=0.021818 validation MAE=0.031903,rank=2\n",
      "[SoftImpute] Iter 110: observed MAE=0.021805 validation MAE=0.031891,rank=2\n",
      "[SoftImpute] Iter 111: observed MAE=0.021792 validation MAE=0.031880,rank=2\n",
      "[SoftImpute] Iter 112: observed MAE=0.021780 validation MAE=0.031869,rank=2\n",
      "[SoftImpute] Iter 113: observed MAE=0.021768 validation MAE=0.031859,rank=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 114: observed MAE=0.021756 validation MAE=0.031850,rank=2\n",
      "[SoftImpute] Iter 115: observed MAE=0.021744 validation MAE=0.031842,rank=2\n",
      "[SoftImpute] Iter 116: observed MAE=0.021732 validation MAE=0.031834,rank=2\n",
      "[SoftImpute] Iter 117: observed MAE=0.021721 validation MAE=0.031825,rank=2\n",
      "[SoftImpute] Iter 118: observed MAE=0.021710 validation MAE=0.031817,rank=2\n",
      "[SoftImpute] Iter 119: observed MAE=0.021699 validation MAE=0.031808,rank=2\n",
      "[SoftImpute] Iter 120: observed MAE=0.021688 validation MAE=0.031800,rank=2\n",
      "[SoftImpute] Iter 121: observed MAE=0.021678 validation MAE=0.031793,rank=2\n",
      "[SoftImpute] Iter 122: observed MAE=0.021668 validation MAE=0.031786,rank=2\n",
      "[SoftImpute] Iter 123: observed MAE=0.021657 validation MAE=0.031779,rank=2\n",
      "[SoftImpute] Iter 124: observed MAE=0.021648 validation MAE=0.031775,rank=2\n",
      "[SoftImpute] Iter 125: observed MAE=0.021638 validation MAE=0.031770,rank=2\n",
      "[SoftImpute] Iter 126: observed MAE=0.021629 validation MAE=0.031765,rank=2\n",
      "[SoftImpute] Iter 127: observed MAE=0.021619 validation MAE=0.031762,rank=2\n",
      "[SoftImpute] Iter 128: observed MAE=0.021610 validation MAE=0.031758,rank=2\n",
      "[SoftImpute] Iter 129: observed MAE=0.021601 validation MAE=0.031754,rank=2\n",
      "[SoftImpute] Iter 130: observed MAE=0.021592 validation MAE=0.031751,rank=2\n",
      "[SoftImpute] Iter 131: observed MAE=0.021584 validation MAE=0.031747,rank=2\n",
      "[SoftImpute] Iter 132: observed MAE=0.021575 validation MAE=0.031743,rank=2\n",
      "[SoftImpute] Iter 133: observed MAE=0.021567 validation MAE=0.031739,rank=2\n",
      "[SoftImpute] Iter 134: observed MAE=0.021559 validation MAE=0.031736,rank=2\n",
      "[SoftImpute] Iter 135: observed MAE=0.021551 validation MAE=0.031732,rank=2\n",
      "[SoftImpute] Iter 136: observed MAE=0.021543 validation MAE=0.031728,rank=2\n",
      "[SoftImpute] Iter 137: observed MAE=0.021535 validation MAE=0.031725,rank=2\n",
      "[SoftImpute] Iter 138: observed MAE=0.021528 validation MAE=0.031722,rank=2\n",
      "[SoftImpute] Iter 139: observed MAE=0.021521 validation MAE=0.031719,rank=2\n",
      "[SoftImpute] Iter 140: observed MAE=0.021513 validation MAE=0.031715,rank=2\n",
      "[SoftImpute] Iter 141: observed MAE=0.021506 validation MAE=0.031712,rank=2\n",
      "[SoftImpute] Iter 142: observed MAE=0.021500 validation MAE=0.031709,rank=2\n",
      "[SoftImpute] Iter 143: observed MAE=0.021493 validation MAE=0.031706,rank=2\n",
      "[SoftImpute] Iter 144: observed MAE=0.021486 validation MAE=0.031703,rank=2\n",
      "[SoftImpute] Iter 145: observed MAE=0.021479 validation MAE=0.031701,rank=2\n",
      "[SoftImpute] Iter 146: observed MAE=0.021473 validation MAE=0.031700,rank=2\n",
      "[SoftImpute] Iter 147: observed MAE=0.021467 validation MAE=0.031699,rank=2\n",
      "[SoftImpute] Iter 148: observed MAE=0.021460 validation MAE=0.031698,rank=2\n",
      "[SoftImpute] Iter 149: observed MAE=0.021454 validation MAE=0.031697,rank=2\n",
      "[SoftImpute] Iter 150: observed MAE=0.021448 validation MAE=0.031697,rank=2\n",
      "[SoftImpute] Iter 151: observed MAE=0.021442 validation MAE=0.031698,rank=2\n",
      "[SoftImpute] Iter 152: observed MAE=0.021436 validation MAE=0.031698,rank=2\n",
      "[SoftImpute] Stopped after iteration 152 for lambda=0.024644\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 4.986656904220581\n",
      "After the matrix factor stage, training error is 0.02144, validation error is 0.03170\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.30185, val loss: 0.30478\n",
      "Main effects training epoch: 2, train loss: 0.23228, val loss: 0.23617\n",
      "Main effects training epoch: 3, train loss: 0.17997, val loss: 0.18315\n",
      "Main effects training epoch: 4, train loss: 0.14845, val loss: 0.15215\n",
      "Main effects training epoch: 5, train loss: 0.13484, val loss: 0.13695\n",
      "Main effects training epoch: 6, train loss: 0.13033, val loss: 0.13155\n",
      "Main effects training epoch: 7, train loss: 0.13055, val loss: 0.13149\n",
      "Main effects training epoch: 8, train loss: 0.12971, val loss: 0.12988\n",
      "Main effects training epoch: 9, train loss: 0.12862, val loss: 0.12919\n",
      "Main effects training epoch: 10, train loss: 0.12828, val loss: 0.12938\n",
      "Main effects training epoch: 11, train loss: 0.12732, val loss: 0.12789\n",
      "Main effects training epoch: 12, train loss: 0.12659, val loss: 0.12743\n",
      "Main effects training epoch: 13, train loss: 0.12563, val loss: 0.12654\n",
      "Main effects training epoch: 14, train loss: 0.12325, val loss: 0.12456\n",
      "Main effects training epoch: 15, train loss: 0.11949, val loss: 0.12161\n",
      "Main effects training epoch: 16, train loss: 0.11686, val loss: 0.11922\n",
      "Main effects training epoch: 17, train loss: 0.11423, val loss: 0.11562\n",
      "Main effects training epoch: 18, train loss: 0.11484, val loss: 0.11687\n",
      "Main effects training epoch: 19, train loss: 0.11188, val loss: 0.11473\n",
      "Main effects training epoch: 20, train loss: 0.11089, val loss: 0.11324\n",
      "Main effects training epoch: 21, train loss: 0.10849, val loss: 0.11127\n",
      "Main effects training epoch: 22, train loss: 0.10778, val loss: 0.11002\n",
      "Main effects training epoch: 23, train loss: 0.10761, val loss: 0.10934\n",
      "Main effects training epoch: 24, train loss: 0.10695, val loss: 0.10962\n",
      "Main effects training epoch: 25, train loss: 0.10869, val loss: 0.11093\n",
      "Main effects training epoch: 26, train loss: 0.10721, val loss: 0.11016\n",
      "Main effects training epoch: 27, train loss: 0.10692, val loss: 0.10911\n",
      "Main effects training epoch: 28, train loss: 0.10656, val loss: 0.10921\n",
      "Main effects training epoch: 29, train loss: 0.10666, val loss: 0.10915\n",
      "Main effects training epoch: 30, train loss: 0.10688, val loss: 0.10958\n",
      "Main effects training epoch: 31, train loss: 0.10689, val loss: 0.10943\n",
      "Main effects training epoch: 32, train loss: 0.10622, val loss: 0.10918\n",
      "Main effects training epoch: 33, train loss: 0.10631, val loss: 0.10928\n",
      "Main effects training epoch: 34, train loss: 0.10605, val loss: 0.10868\n",
      "Main effects training epoch: 35, train loss: 0.10599, val loss: 0.10879\n",
      "Main effects training epoch: 36, train loss: 0.10605, val loss: 0.10911\n",
      "Main effects training epoch: 37, train loss: 0.10704, val loss: 0.10981\n",
      "Main effects training epoch: 38, train loss: 0.10655, val loss: 0.10926\n",
      "Main effects training epoch: 39, train loss: 0.10607, val loss: 0.10888\n",
      "Main effects training epoch: 40, train loss: 0.10588, val loss: 0.10859\n",
      "Main effects training epoch: 41, train loss: 0.10600, val loss: 0.10889\n",
      "Main effects training epoch: 42, train loss: 0.10616, val loss: 0.10903\n",
      "Main effects training epoch: 43, train loss: 0.10609, val loss: 0.10883\n",
      "Main effects training epoch: 44, train loss: 0.10630, val loss: 0.10899\n",
      "Main effects training epoch: 45, train loss: 0.10686, val loss: 0.10928\n",
      "Main effects training epoch: 46, train loss: 0.10599, val loss: 0.10904\n",
      "Main effects training epoch: 47, train loss: 0.10577, val loss: 0.10887\n",
      "Main effects training epoch: 48, train loss: 0.10584, val loss: 0.10888\n",
      "Main effects training epoch: 49, train loss: 0.10576, val loss: 0.10866\n",
      "Main effects training epoch: 50, train loss: 0.10617, val loss: 0.10903\n",
      "Main effects training epoch: 51, train loss: 0.10592, val loss: 0.10893\n",
      "Main effects training epoch: 52, train loss: 0.10606, val loss: 0.10892\n",
      "Main effects training epoch: 53, train loss: 0.10577, val loss: 0.10888\n",
      "Main effects training epoch: 54, train loss: 0.10586, val loss: 0.10875\n",
      "Main effects training epoch: 55, train loss: 0.10635, val loss: 0.10929\n",
      "Main effects training epoch: 56, train loss: 0.10600, val loss: 0.10895\n",
      "Main effects training epoch: 57, train loss: 0.10614, val loss: 0.10942\n",
      "Main effects training epoch: 58, train loss: 0.10591, val loss: 0.10905\n",
      "Main effects training epoch: 59, train loss: 0.10587, val loss: 0.10866\n",
      "Main effects training epoch: 60, train loss: 0.10580, val loss: 0.10885\n",
      "Main effects training epoch: 61, train loss: 0.10590, val loss: 0.10868\n",
      "Main effects training epoch: 62, train loss: 0.10611, val loss: 0.10911\n",
      "Main effects training epoch: 63, train loss: 0.10587, val loss: 0.10877\n",
      "Main effects training epoch: 64, train loss: 0.10611, val loss: 0.10908\n",
      "Main effects training epoch: 65, train loss: 0.10597, val loss: 0.10888\n",
      "Main effects training epoch: 66, train loss: 0.10579, val loss: 0.10886\n",
      "Main effects training epoch: 67, train loss: 0.10588, val loss: 0.10853\n",
      "Main effects training epoch: 68, train loss: 0.10601, val loss: 0.10926\n",
      "Main effects training epoch: 69, train loss: 0.10583, val loss: 0.10866\n",
      "Main effects training epoch: 70, train loss: 0.10585, val loss: 0.10893\n",
      "Main effects training epoch: 71, train loss: 0.10592, val loss: 0.10879\n",
      "Main effects training epoch: 72, train loss: 0.10582, val loss: 0.10901\n",
      "Main effects training epoch: 73, train loss: 0.10586, val loss: 0.10890\n",
      "Main effects training epoch: 74, train loss: 0.10596, val loss: 0.10893\n",
      "Main effects training epoch: 75, train loss: 0.10614, val loss: 0.10884\n",
      "Main effects training epoch: 76, train loss: 0.10583, val loss: 0.10904\n",
      "Main effects training epoch: 77, train loss: 0.10592, val loss: 0.10890\n",
      "Main effects training epoch: 78, train loss: 0.10580, val loss: 0.10881\n",
      "Main effects training epoch: 79, train loss: 0.10594, val loss: 0.10889\n",
      "Main effects training epoch: 80, train loss: 0.10607, val loss: 0.10888\n",
      "Main effects training epoch: 81, train loss: 0.10581, val loss: 0.10891\n",
      "Main effects training epoch: 82, train loss: 0.10593, val loss: 0.10922\n",
      "Main effects training epoch: 83, train loss: 0.10574, val loss: 0.10873\n",
      "Main effects training epoch: 84, train loss: 0.10573, val loss: 0.10877\n",
      "Main effects training epoch: 85, train loss: 0.10636, val loss: 0.10929\n",
      "Main effects training epoch: 86, train loss: 0.10583, val loss: 0.10874\n",
      "Main effects training epoch: 87, train loss: 0.10614, val loss: 0.10928\n",
      "Main effects training epoch: 88, train loss: 0.10616, val loss: 0.10949\n",
      "Main effects training epoch: 89, train loss: 0.10589, val loss: 0.10865\n",
      "Main effects training epoch: 90, train loss: 0.10604, val loss: 0.10903\n",
      "Main effects training epoch: 91, train loss: 0.10588, val loss: 0.10892\n",
      "Main effects training epoch: 92, train loss: 0.10612, val loss: 0.10925\n",
      "Main effects training epoch: 93, train loss: 0.10583, val loss: 0.10911\n",
      "Main effects training epoch: 94, train loss: 0.10604, val loss: 0.10904\n",
      "Main effects training epoch: 95, train loss: 0.10576, val loss: 0.10903\n",
      "Main effects training epoch: 96, train loss: 0.10568, val loss: 0.10869\n",
      "Main effects training epoch: 97, train loss: 0.10588, val loss: 0.10926\n",
      "Main effects training epoch: 98, train loss: 0.10606, val loss: 0.10910\n",
      "Main effects training epoch: 99, train loss: 0.10649, val loss: 0.10930\n",
      "Main effects training epoch: 100, train loss: 0.10628, val loss: 0.10948\n",
      "Main effects training epoch: 101, train loss: 0.10586, val loss: 0.10912\n",
      "Main effects training epoch: 102, train loss: 0.10570, val loss: 0.10869\n",
      "Main effects training epoch: 103, train loss: 0.10643, val loss: 0.10951\n",
      "Main effects training epoch: 104, train loss: 0.10650, val loss: 0.10946\n",
      "Main effects training epoch: 105, train loss: 0.10626, val loss: 0.10954\n",
      "Main effects training epoch: 106, train loss: 0.10594, val loss: 0.10896\n",
      "Main effects training epoch: 107, train loss: 0.10579, val loss: 0.10878\n",
      "Main effects training epoch: 108, train loss: 0.10607, val loss: 0.10922\n",
      "Main effects training epoch: 109, train loss: 0.10581, val loss: 0.10902\n",
      "Main effects training epoch: 110, train loss: 0.10590, val loss: 0.10892\n",
      "Main effects training epoch: 111, train loss: 0.10605, val loss: 0.10924\n",
      "Main effects training epoch: 112, train loss: 0.10580, val loss: 0.10875\n",
      "Main effects training epoch: 113, train loss: 0.10587, val loss: 0.10900\n",
      "Main effects training epoch: 114, train loss: 0.10581, val loss: 0.10883\n",
      "Main effects training epoch: 115, train loss: 0.10618, val loss: 0.10915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 116, train loss: 0.10615, val loss: 0.10949\n",
      "Main effects training epoch: 117, train loss: 0.10602, val loss: 0.10894\n",
      "Main effects training epoch: 118, train loss: 0.10595, val loss: 0.10914\n",
      "Main effects training epoch: 119, train loss: 0.10618, val loss: 0.10895\n",
      "Main effects training epoch: 120, train loss: 0.10601, val loss: 0.10941\n",
      "Main effects training epoch: 121, train loss: 0.10588, val loss: 0.10869\n",
      "Main effects training epoch: 122, train loss: 0.10576, val loss: 0.10885\n",
      "Main effects training epoch: 123, train loss: 0.10578, val loss: 0.10899\n",
      "Main effects training epoch: 124, train loss: 0.10583, val loss: 0.10934\n",
      "Main effects training epoch: 125, train loss: 0.10610, val loss: 0.10904\n",
      "Main effects training epoch: 126, train loss: 0.10593, val loss: 0.10917\n",
      "Main effects training epoch: 127, train loss: 0.10579, val loss: 0.10863\n",
      "Main effects training epoch: 128, train loss: 0.10583, val loss: 0.10933\n",
      "Main effects training epoch: 129, train loss: 0.10580, val loss: 0.10919\n",
      "Main effects training epoch: 130, train loss: 0.10573, val loss: 0.10874\n",
      "Main effects training epoch: 131, train loss: 0.10601, val loss: 0.10951\n",
      "Main effects training epoch: 132, train loss: 0.10600, val loss: 0.10905\n",
      "Main effects training epoch: 133, train loss: 0.10601, val loss: 0.10931\n",
      "Main effects training epoch: 134, train loss: 0.10566, val loss: 0.10882\n",
      "Main effects training epoch: 135, train loss: 0.10590, val loss: 0.10909\n",
      "Main effects training epoch: 136, train loss: 0.10621, val loss: 0.10930\n",
      "Main effects training epoch: 137, train loss: 0.10597, val loss: 0.10906\n",
      "Main effects training epoch: 138, train loss: 0.10583, val loss: 0.10895\n",
      "Main effects training epoch: 139, train loss: 0.10580, val loss: 0.10894\n",
      "Main effects training epoch: 140, train loss: 0.10591, val loss: 0.10891\n",
      "Main effects training epoch: 141, train loss: 0.10595, val loss: 0.10903\n",
      "Main effects training epoch: 142, train loss: 0.10601, val loss: 0.10932\n",
      "Main effects training epoch: 143, train loss: 0.10596, val loss: 0.10892\n",
      "Main effects training epoch: 144, train loss: 0.10583, val loss: 0.10925\n",
      "Main effects training epoch: 145, train loss: 0.10572, val loss: 0.10887\n",
      "Main effects training epoch: 146, train loss: 0.10574, val loss: 0.10897\n",
      "Main effects training epoch: 147, train loss: 0.10575, val loss: 0.10903\n",
      "Main effects training epoch: 148, train loss: 0.10589, val loss: 0.10894\n",
      "Main effects training epoch: 149, train loss: 0.10577, val loss: 0.10889\n",
      "Main effects training epoch: 150, train loss: 0.10569, val loss: 0.10895\n",
      "Main effects training epoch: 151, train loss: 0.10575, val loss: 0.10884\n",
      "Main effects training epoch: 152, train loss: 0.10606, val loss: 0.10908\n",
      "Main effects training epoch: 153, train loss: 0.10639, val loss: 0.10965\n",
      "Main effects training epoch: 154, train loss: 0.10596, val loss: 0.10920\n",
      "Main effects training epoch: 155, train loss: 0.10578, val loss: 0.10908\n",
      "Main effects training epoch: 156, train loss: 0.10569, val loss: 0.10881\n",
      "Main effects training epoch: 157, train loss: 0.10570, val loss: 0.10898\n",
      "Main effects training epoch: 158, train loss: 0.10595, val loss: 0.10931\n",
      "Main effects training epoch: 159, train loss: 0.10577, val loss: 0.10920\n",
      "Main effects training epoch: 160, train loss: 0.10578, val loss: 0.10935\n",
      "Main effects training epoch: 161, train loss: 0.10579, val loss: 0.10872\n",
      "Main effects training epoch: 162, train loss: 0.10575, val loss: 0.10904\n",
      "Main effects training epoch: 163, train loss: 0.10597, val loss: 0.10913\n",
      "Main effects training epoch: 164, train loss: 0.10605, val loss: 0.10923\n",
      "Main effects training epoch: 165, train loss: 0.10604, val loss: 0.10954\n",
      "Main effects training epoch: 166, train loss: 0.10575, val loss: 0.10883\n",
      "Main effects training epoch: 167, train loss: 0.10578, val loss: 0.10918\n",
      "Main effects training epoch: 168, train loss: 0.10576, val loss: 0.10888\n",
      "Early stop at epoch 168, with validation loss: 0.10888\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.10587, val loss: 0.10915\n",
      "Main effects tuning epoch: 2, train loss: 0.10583, val loss: 0.10863\n",
      "Main effects tuning epoch: 3, train loss: 0.10593, val loss: 0.10918\n",
      "Main effects tuning epoch: 4, train loss: 0.10580, val loss: 0.10863\n",
      "Main effects tuning epoch: 5, train loss: 0.10595, val loss: 0.10896\n",
      "Main effects tuning epoch: 6, train loss: 0.10614, val loss: 0.10907\n",
      "Main effects tuning epoch: 7, train loss: 0.10617, val loss: 0.10891\n",
      "Main effects tuning epoch: 8, train loss: 0.10639, val loss: 0.10952\n",
      "Main effects tuning epoch: 9, train loss: 0.10631, val loss: 0.10952\n",
      "Main effects tuning epoch: 10, train loss: 0.10598, val loss: 0.10913\n",
      "Main effects tuning epoch: 11, train loss: 0.10616, val loss: 0.10872\n",
      "Main effects tuning epoch: 12, train loss: 0.10613, val loss: 0.10940\n",
      "Main effects tuning epoch: 13, train loss: 0.10649, val loss: 0.10913\n",
      "Main effects tuning epoch: 14, train loss: 0.10595, val loss: 0.10906\n",
      "Main effects tuning epoch: 15, train loss: 0.10576, val loss: 0.10901\n",
      "Main effects tuning epoch: 16, train loss: 0.10591, val loss: 0.10867\n",
      "Main effects tuning epoch: 17, train loss: 0.10614, val loss: 0.10929\n",
      "Main effects tuning epoch: 18, train loss: 0.10616, val loss: 0.10914\n",
      "Main effects tuning epoch: 19, train loss: 0.10591, val loss: 0.10901\n",
      "Main effects tuning epoch: 20, train loss: 0.10585, val loss: 0.10893\n",
      "Main effects tuning epoch: 21, train loss: 0.10582, val loss: 0.10851\n",
      "Main effects tuning epoch: 22, train loss: 0.10595, val loss: 0.10917\n",
      "Main effects tuning epoch: 23, train loss: 0.10589, val loss: 0.10893\n",
      "Main effects tuning epoch: 24, train loss: 0.10579, val loss: 0.10899\n",
      "Main effects tuning epoch: 25, train loss: 0.10580, val loss: 0.10907\n",
      "Main effects tuning epoch: 26, train loss: 0.10612, val loss: 0.10929\n",
      "Main effects tuning epoch: 27, train loss: 0.10613, val loss: 0.10892\n",
      "Main effects tuning epoch: 28, train loss: 0.10608, val loss: 0.10898\n",
      "Main effects tuning epoch: 29, train loss: 0.10588, val loss: 0.10904\n",
      "Main effects tuning epoch: 30, train loss: 0.10603, val loss: 0.10875\n",
      "Main effects tuning epoch: 31, train loss: 0.10580, val loss: 0.10885\n",
      "Main effects tuning epoch: 32, train loss: 0.10586, val loss: 0.10892\n",
      "Main effects tuning epoch: 33, train loss: 0.10584, val loss: 0.10877\n",
      "Main effects tuning epoch: 34, train loss: 0.10610, val loss: 0.10890\n",
      "Main effects tuning epoch: 35, train loss: 0.10602, val loss: 0.10950\n",
      "Main effects tuning epoch: 36, train loss: 0.10577, val loss: 0.10870\n",
      "Main effects tuning epoch: 37, train loss: 0.10576, val loss: 0.10890\n",
      "Main effects tuning epoch: 38, train loss: 0.10590, val loss: 0.10894\n",
      "Main effects tuning epoch: 39, train loss: 0.10597, val loss: 0.10915\n",
      "Main effects tuning epoch: 40, train loss: 0.10575, val loss: 0.10856\n",
      "Main effects tuning epoch: 41, train loss: 0.10577, val loss: 0.10899\n",
      "Main effects tuning epoch: 42, train loss: 0.10600, val loss: 0.10888\n",
      "Main effects tuning epoch: 43, train loss: 0.10624, val loss: 0.10938\n",
      "Main effects tuning epoch: 44, train loss: 0.10606, val loss: 0.10863\n",
      "Main effects tuning epoch: 45, train loss: 0.10614, val loss: 0.10961\n",
      "Main effects tuning epoch: 46, train loss: 0.10598, val loss: 0.10874\n",
      "Main effects tuning epoch: 47, train loss: 0.10623, val loss: 0.10916\n",
      "Main effects tuning epoch: 48, train loss: 0.10628, val loss: 0.10910\n",
      "Main effects tuning epoch: 49, train loss: 0.10612, val loss: 0.10902\n",
      "Main effects tuning epoch: 50, train loss: 0.10600, val loss: 0.10922\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.16656, val loss: 0.16504\n",
      "Interaction training epoch: 2, train loss: 0.19799, val loss: 0.19730\n",
      "Interaction training epoch: 3, train loss: 0.07919, val loss: 0.08135\n",
      "Interaction training epoch: 4, train loss: 0.06592, val loss: 0.06695\n",
      "Interaction training epoch: 5, train loss: 0.06719, val loss: 0.06623\n",
      "Interaction training epoch: 6, train loss: 0.07116, val loss: 0.06992\n",
      "Interaction training epoch: 7, train loss: 0.05765, val loss: 0.05869\n",
      "Interaction training epoch: 8, train loss: 0.05791, val loss: 0.05822\n",
      "Interaction training epoch: 9, train loss: 0.06091, val loss: 0.06046\n",
      "Interaction training epoch: 10, train loss: 0.05754, val loss: 0.05788\n",
      "Interaction training epoch: 11, train loss: 0.05624, val loss: 0.05624\n",
      "Interaction training epoch: 12, train loss: 0.05651, val loss: 0.05600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 13, train loss: 0.06101, val loss: 0.06080\n",
      "Interaction training epoch: 14, train loss: 0.05709, val loss: 0.05641\n",
      "Interaction training epoch: 15, train loss: 0.06077, val loss: 0.06101\n",
      "Interaction training epoch: 16, train loss: 0.05239, val loss: 0.05197\n",
      "Interaction training epoch: 17, train loss: 0.05618, val loss: 0.05592\n",
      "Interaction training epoch: 18, train loss: 0.05477, val loss: 0.05468\n",
      "Interaction training epoch: 19, train loss: 0.05152, val loss: 0.05113\n",
      "Interaction training epoch: 20, train loss: 0.05519, val loss: 0.05482\n",
      "Interaction training epoch: 21, train loss: 0.05818, val loss: 0.05633\n",
      "Interaction training epoch: 22, train loss: 0.05614, val loss: 0.05550\n",
      "Interaction training epoch: 23, train loss: 0.05384, val loss: 0.05252\n",
      "Interaction training epoch: 24, train loss: 0.05143, val loss: 0.05136\n",
      "Interaction training epoch: 25, train loss: 0.05185, val loss: 0.05118\n",
      "Interaction training epoch: 26, train loss: 0.05409, val loss: 0.05515\n",
      "Interaction training epoch: 27, train loss: 0.05171, val loss: 0.05189\n",
      "Interaction training epoch: 28, train loss: 0.05284, val loss: 0.05307\n",
      "Interaction training epoch: 29, train loss: 0.05251, val loss: 0.05306\n",
      "Interaction training epoch: 30, train loss: 0.05654, val loss: 0.05667\n",
      "Interaction training epoch: 31, train loss: 0.05008, val loss: 0.05013\n",
      "Interaction training epoch: 32, train loss: 0.05165, val loss: 0.05175\n",
      "Interaction training epoch: 33, train loss: 0.05094, val loss: 0.05147\n",
      "Interaction training epoch: 34, train loss: 0.05154, val loss: 0.05255\n",
      "Interaction training epoch: 35, train loss: 0.05178, val loss: 0.05163\n",
      "Interaction training epoch: 36, train loss: 0.05189, val loss: 0.05137\n",
      "Interaction training epoch: 37, train loss: 0.05261, val loss: 0.05262\n",
      "Interaction training epoch: 38, train loss: 0.05395, val loss: 0.05444\n",
      "Interaction training epoch: 39, train loss: 0.05053, val loss: 0.05093\n",
      "Interaction training epoch: 40, train loss: 0.05186, val loss: 0.05165\n",
      "Interaction training epoch: 41, train loss: 0.05174, val loss: 0.05183\n",
      "Interaction training epoch: 42, train loss: 0.05645, val loss: 0.05707\n",
      "Interaction training epoch: 43, train loss: 0.05130, val loss: 0.05143\n",
      "Interaction training epoch: 44, train loss: 0.05233, val loss: 0.05213\n",
      "Interaction training epoch: 45, train loss: 0.05242, val loss: 0.05266\n",
      "Interaction training epoch: 46, train loss: 0.05234, val loss: 0.05256\n",
      "Interaction training epoch: 47, train loss: 0.05096, val loss: 0.05121\n",
      "Interaction training epoch: 48, train loss: 0.05324, val loss: 0.05317\n",
      "Interaction training epoch: 49, train loss: 0.05143, val loss: 0.05175\n",
      "Interaction training epoch: 50, train loss: 0.05906, val loss: 0.05866\n",
      "Interaction training epoch: 51, train loss: 0.05278, val loss: 0.05220\n",
      "Interaction training epoch: 52, train loss: 0.05429, val loss: 0.05321\n",
      "Interaction training epoch: 53, train loss: 0.05247, val loss: 0.05225\n",
      "Interaction training epoch: 54, train loss: 0.05061, val loss: 0.05129\n",
      "Interaction training epoch: 55, train loss: 0.05239, val loss: 0.05237\n",
      "Interaction training epoch: 56, train loss: 0.05174, val loss: 0.05287\n",
      "Interaction training epoch: 57, train loss: 0.05160, val loss: 0.05111\n",
      "Interaction training epoch: 58, train loss: 0.05180, val loss: 0.05292\n",
      "Interaction training epoch: 59, train loss: 0.04982, val loss: 0.05009\n",
      "Interaction training epoch: 60, train loss: 0.05227, val loss: 0.05368\n",
      "Interaction training epoch: 61, train loss: 0.05217, val loss: 0.05210\n",
      "Interaction training epoch: 62, train loss: 0.05023, val loss: 0.05074\n",
      "Interaction training epoch: 63, train loss: 0.05179, val loss: 0.05284\n",
      "Interaction training epoch: 64, train loss: 0.04959, val loss: 0.05129\n",
      "Interaction training epoch: 65, train loss: 0.05320, val loss: 0.05225\n",
      "Interaction training epoch: 66, train loss: 0.05295, val loss: 0.05326\n",
      "Interaction training epoch: 67, train loss: 0.05028, val loss: 0.05144\n",
      "Interaction training epoch: 68, train loss: 0.05112, val loss: 0.05151\n",
      "Interaction training epoch: 69, train loss: 0.05228, val loss: 0.05248\n",
      "Interaction training epoch: 70, train loss: 0.04994, val loss: 0.05062\n",
      "Interaction training epoch: 71, train loss: 0.05225, val loss: 0.05286\n",
      "Interaction training epoch: 72, train loss: 0.05069, val loss: 0.05180\n",
      "Interaction training epoch: 73, train loss: 0.05077, val loss: 0.05196\n",
      "Interaction training epoch: 74, train loss: 0.04967, val loss: 0.05076\n",
      "Interaction training epoch: 75, train loss: 0.05064, val loss: 0.05071\n",
      "Interaction training epoch: 76, train loss: 0.04886, val loss: 0.05004\n",
      "Interaction training epoch: 77, train loss: 0.05097, val loss: 0.05075\n",
      "Interaction training epoch: 78, train loss: 0.05171, val loss: 0.05128\n",
      "Interaction training epoch: 79, train loss: 0.05178, val loss: 0.05168\n",
      "Interaction training epoch: 80, train loss: 0.04972, val loss: 0.05111\n",
      "Interaction training epoch: 81, train loss: 0.05307, val loss: 0.05407\n",
      "Interaction training epoch: 82, train loss: 0.05287, val loss: 0.05331\n",
      "Interaction training epoch: 83, train loss: 0.05019, val loss: 0.05065\n",
      "Interaction training epoch: 84, train loss: 0.05109, val loss: 0.05177\n",
      "Interaction training epoch: 85, train loss: 0.05241, val loss: 0.05395\n",
      "Interaction training epoch: 86, train loss: 0.05169, val loss: 0.05187\n",
      "Interaction training epoch: 87, train loss: 0.05013, val loss: 0.05031\n",
      "Interaction training epoch: 88, train loss: 0.05249, val loss: 0.05324\n",
      "Interaction training epoch: 89, train loss: 0.04905, val loss: 0.04955\n",
      "Interaction training epoch: 90, train loss: 0.05048, val loss: 0.05104\n",
      "Interaction training epoch: 91, train loss: 0.05291, val loss: 0.05320\n",
      "Interaction training epoch: 92, train loss: 0.05194, val loss: 0.05184\n",
      "Interaction training epoch: 93, train loss: 0.04813, val loss: 0.04857\n",
      "Interaction training epoch: 94, train loss: 0.05434, val loss: 0.05466\n",
      "Interaction training epoch: 95, train loss: 0.05394, val loss: 0.05306\n",
      "Interaction training epoch: 96, train loss: 0.04954, val loss: 0.04995\n",
      "Interaction training epoch: 97, train loss: 0.05003, val loss: 0.05058\n",
      "Interaction training epoch: 98, train loss: 0.05197, val loss: 0.05219\n",
      "Interaction training epoch: 99, train loss: 0.05436, val loss: 0.05280\n",
      "Interaction training epoch: 100, train loss: 0.05350, val loss: 0.05437\n",
      "Interaction training epoch: 101, train loss: 0.04964, val loss: 0.04980\n",
      "Interaction training epoch: 102, train loss: 0.05136, val loss: 0.05087\n",
      "Interaction training epoch: 103, train loss: 0.05162, val loss: 0.05191\n",
      "Interaction training epoch: 104, train loss: 0.05118, val loss: 0.05150\n",
      "Interaction training epoch: 105, train loss: 0.04993, val loss: 0.04937\n",
      "Interaction training epoch: 106, train loss: 0.05171, val loss: 0.05164\n",
      "Interaction training epoch: 107, train loss: 0.05193, val loss: 0.05075\n",
      "Interaction training epoch: 108, train loss: 0.05102, val loss: 0.05183\n",
      "Interaction training epoch: 109, train loss: 0.05261, val loss: 0.05324\n",
      "Interaction training epoch: 110, train loss: 0.05047, val loss: 0.05100\n",
      "Interaction training epoch: 111, train loss: 0.05046, val loss: 0.05063\n",
      "Interaction training epoch: 112, train loss: 0.04811, val loss: 0.04809\n",
      "Interaction training epoch: 113, train loss: 0.05250, val loss: 0.05298\n",
      "Interaction training epoch: 114, train loss: 0.05036, val loss: 0.05112\n",
      "Interaction training epoch: 115, train loss: 0.05119, val loss: 0.05140\n",
      "Interaction training epoch: 116, train loss: 0.04975, val loss: 0.05112\n",
      "Interaction training epoch: 117, train loss: 0.04911, val loss: 0.04994\n",
      "Interaction training epoch: 118, train loss: 0.04990, val loss: 0.05012\n",
      "Interaction training epoch: 119, train loss: 0.04854, val loss: 0.04775\n",
      "Interaction training epoch: 120, train loss: 0.04985, val loss: 0.04987\n",
      "Interaction training epoch: 121, train loss: 0.04779, val loss: 0.04856\n",
      "Interaction training epoch: 122, train loss: 0.04712, val loss: 0.04773\n",
      "Interaction training epoch: 123, train loss: 0.05164, val loss: 0.05255\n",
      "Interaction training epoch: 124, train loss: 0.05327, val loss: 0.05365\n",
      "Interaction training epoch: 125, train loss: 0.05348, val loss: 0.05414\n",
      "Interaction training epoch: 126, train loss: 0.04873, val loss: 0.04978\n",
      "Interaction training epoch: 127, train loss: 0.04866, val loss: 0.04894\n",
      "Interaction training epoch: 128, train loss: 0.04947, val loss: 0.05037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 129, train loss: 0.04701, val loss: 0.04711\n",
      "Interaction training epoch: 130, train loss: 0.04867, val loss: 0.04878\n",
      "Interaction training epoch: 131, train loss: 0.04995, val loss: 0.05004\n",
      "Interaction training epoch: 132, train loss: 0.05004, val loss: 0.05008\n",
      "Interaction training epoch: 133, train loss: 0.04970, val loss: 0.05023\n",
      "Interaction training epoch: 134, train loss: 0.04969, val loss: 0.05041\n",
      "Interaction training epoch: 135, train loss: 0.04826, val loss: 0.04935\n",
      "Interaction training epoch: 136, train loss: 0.04956, val loss: 0.05077\n",
      "Interaction training epoch: 137, train loss: 0.04937, val loss: 0.05116\n",
      "Interaction training epoch: 138, train loss: 0.05042, val loss: 0.05126\n",
      "Interaction training epoch: 139, train loss: 0.04910, val loss: 0.04944\n",
      "Interaction training epoch: 140, train loss: 0.05235, val loss: 0.05329\n",
      "Interaction training epoch: 141, train loss: 0.05151, val loss: 0.05199\n",
      "Interaction training epoch: 142, train loss: 0.04935, val loss: 0.04965\n",
      "Interaction training epoch: 143, train loss: 0.05663, val loss: 0.05667\n",
      "Interaction training epoch: 144, train loss: 0.05217, val loss: 0.05297\n",
      "Interaction training epoch: 145, train loss: 0.05139, val loss: 0.05143\n",
      "Interaction training epoch: 146, train loss: 0.05350, val loss: 0.05388\n",
      "Interaction training epoch: 147, train loss: 0.06195, val loss: 0.06171\n",
      "Interaction training epoch: 148, train loss: 0.05923, val loss: 0.05958\n",
      "Interaction training epoch: 149, train loss: 0.05897, val loss: 0.05842\n",
      "Interaction training epoch: 150, train loss: 0.05688, val loss: 0.05663\n",
      "Interaction training epoch: 151, train loss: 0.05292, val loss: 0.05339\n",
      "Interaction training epoch: 152, train loss: 0.04638, val loss: 0.04708\n",
      "Interaction training epoch: 153, train loss: 0.04830, val loss: 0.05070\n",
      "Interaction training epoch: 154, train loss: 0.04798, val loss: 0.04952\n",
      "Interaction training epoch: 155, train loss: 0.04659, val loss: 0.04786\n",
      "Interaction training epoch: 156, train loss: 0.04767, val loss: 0.04876\n",
      "Interaction training epoch: 157, train loss: 0.04853, val loss: 0.04917\n",
      "Interaction training epoch: 158, train loss: 0.04677, val loss: 0.04803\n",
      "Interaction training epoch: 159, train loss: 0.04901, val loss: 0.04931\n",
      "Interaction training epoch: 160, train loss: 0.04754, val loss: 0.04893\n",
      "Interaction training epoch: 161, train loss: 0.04874, val loss: 0.04950\n",
      "Interaction training epoch: 162, train loss: 0.04595, val loss: 0.04721\n",
      "Interaction training epoch: 163, train loss: 0.04732, val loss: 0.04956\n",
      "Interaction training epoch: 164, train loss: 0.04567, val loss: 0.04749\n",
      "Interaction training epoch: 165, train loss: 0.05013, val loss: 0.05115\n",
      "Interaction training epoch: 166, train loss: 0.04663, val loss: 0.04790\n",
      "Interaction training epoch: 167, train loss: 0.04602, val loss: 0.04784\n",
      "Interaction training epoch: 168, train loss: 0.04789, val loss: 0.04800\n",
      "Interaction training epoch: 169, train loss: 0.04480, val loss: 0.04615\n",
      "Interaction training epoch: 170, train loss: 0.04657, val loss: 0.04712\n",
      "Interaction training epoch: 171, train loss: 0.04515, val loss: 0.04688\n",
      "Interaction training epoch: 172, train loss: 0.04745, val loss: 0.04904\n",
      "Interaction training epoch: 173, train loss: 0.04801, val loss: 0.05050\n",
      "Interaction training epoch: 174, train loss: 0.05297, val loss: 0.05389\n",
      "Interaction training epoch: 175, train loss: 0.04522, val loss: 0.04687\n",
      "Interaction training epoch: 176, train loss: 0.04654, val loss: 0.04709\n",
      "Interaction training epoch: 177, train loss: 0.04462, val loss: 0.04576\n",
      "Interaction training epoch: 178, train loss: 0.04715, val loss: 0.04880\n",
      "Interaction training epoch: 179, train loss: 0.04515, val loss: 0.04673\n",
      "Interaction training epoch: 180, train loss: 0.04652, val loss: 0.04824\n",
      "Interaction training epoch: 181, train loss: 0.04757, val loss: 0.04936\n",
      "Interaction training epoch: 182, train loss: 0.04771, val loss: 0.04909\n",
      "Interaction training epoch: 183, train loss: 0.04999, val loss: 0.05186\n",
      "Interaction training epoch: 184, train loss: 0.04691, val loss: 0.04877\n",
      "Interaction training epoch: 185, train loss: 0.04599, val loss: 0.04763\n",
      "Interaction training epoch: 186, train loss: 0.04839, val loss: 0.04969\n",
      "Interaction training epoch: 187, train loss: 0.04634, val loss: 0.04846\n",
      "Interaction training epoch: 188, train loss: 0.04686, val loss: 0.04828\n",
      "Interaction training epoch: 189, train loss: 0.04527, val loss: 0.04603\n",
      "Interaction training epoch: 190, train loss: 0.04811, val loss: 0.05046\n",
      "Interaction training epoch: 191, train loss: 0.04668, val loss: 0.04794\n",
      "Interaction training epoch: 192, train loss: 0.04580, val loss: 0.04673\n",
      "Interaction training epoch: 193, train loss: 0.04953, val loss: 0.05077\n",
      "Interaction training epoch: 194, train loss: 0.04637, val loss: 0.04745\n",
      "Interaction training epoch: 195, train loss: 0.05291, val loss: 0.05606\n",
      "Interaction training epoch: 196, train loss: 0.04552, val loss: 0.04706\n",
      "Interaction training epoch: 197, train loss: 0.04954, val loss: 0.05175\n",
      "Interaction training epoch: 198, train loss: 0.04504, val loss: 0.04659\n",
      "Interaction training epoch: 199, train loss: 0.04939, val loss: 0.05068\n",
      "Interaction training epoch: 200, train loss: 0.04824, val loss: 0.04969\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########7 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.04624, val loss: 0.04648\n",
      "Interaction tuning epoch: 2, train loss: 0.04654, val loss: 0.04675\n",
      "Interaction tuning epoch: 3, train loss: 0.04688, val loss: 0.04770\n",
      "Interaction tuning epoch: 4, train loss: 0.04703, val loss: 0.04714\n",
      "Interaction tuning epoch: 5, train loss: 0.04871, val loss: 0.04870\n",
      "Interaction tuning epoch: 6, train loss: 0.04715, val loss: 0.04685\n",
      "Interaction tuning epoch: 7, train loss: 0.04753, val loss: 0.04769\n",
      "Interaction tuning epoch: 8, train loss: 0.04804, val loss: 0.04839\n",
      "Interaction tuning epoch: 9, train loss: 0.04656, val loss: 0.04746\n",
      "Interaction tuning epoch: 10, train loss: 0.04708, val loss: 0.04792\n",
      "Interaction tuning epoch: 11, train loss: 0.04593, val loss: 0.04607\n",
      "Interaction tuning epoch: 12, train loss: 0.04889, val loss: 0.05019\n",
      "Interaction tuning epoch: 13, train loss: 0.04756, val loss: 0.04795\n",
      "Interaction tuning epoch: 14, train loss: 0.05056, val loss: 0.04891\n",
      "Interaction tuning epoch: 15, train loss: 0.05649, val loss: 0.05700\n",
      "Interaction tuning epoch: 16, train loss: 0.04770, val loss: 0.04777\n",
      "Interaction tuning epoch: 17, train loss: 0.04665, val loss: 0.04734\n",
      "Interaction tuning epoch: 18, train loss: 0.04709, val loss: 0.04763\n",
      "Interaction tuning epoch: 19, train loss: 0.04762, val loss: 0.04775\n",
      "Interaction tuning epoch: 20, train loss: 0.04647, val loss: 0.04691\n",
      "Interaction tuning epoch: 21, train loss: 0.04858, val loss: 0.04904\n",
      "Interaction tuning epoch: 22, train loss: 0.04763, val loss: 0.04743\n",
      "Interaction tuning epoch: 23, train loss: 0.04883, val loss: 0.04879\n",
      "Interaction tuning epoch: 24, train loss: 0.04608, val loss: 0.04728\n",
      "Interaction tuning epoch: 25, train loss: 0.04999, val loss: 0.04940\n",
      "Interaction tuning epoch: 26, train loss: 0.04916, val loss: 0.04857\n",
      "Interaction tuning epoch: 27, train loss: 0.04784, val loss: 0.04855\n",
      "Interaction tuning epoch: 28, train loss: 0.04702, val loss: 0.04758\n",
      "Interaction tuning epoch: 29, train loss: 0.04685, val loss: 0.04720\n",
      "Interaction tuning epoch: 30, train loss: 0.04688, val loss: 0.04800\n",
      "Interaction tuning epoch: 31, train loss: 0.04656, val loss: 0.04725\n",
      "Interaction tuning epoch: 32, train loss: 0.04698, val loss: 0.04718\n",
      "Interaction tuning epoch: 33, train loss: 0.04648, val loss: 0.04674\n",
      "Interaction tuning epoch: 34, train loss: 0.04622, val loss: 0.04627\n",
      "Interaction tuning epoch: 35, train loss: 0.04695, val loss: 0.04653\n",
      "Interaction tuning epoch: 36, train loss: 0.04638, val loss: 0.04708\n",
      "Interaction tuning epoch: 37, train loss: 0.04517, val loss: 0.04599\n",
      "Interaction tuning epoch: 38, train loss: 0.04594, val loss: 0.04626\n",
      "Interaction tuning epoch: 39, train loss: 0.04806, val loss: 0.04884\n",
      "Interaction tuning epoch: 40, train loss: 0.04762, val loss: 0.04781\n",
      "Interaction tuning epoch: 41, train loss: 0.04762, val loss: 0.04909\n",
      "Interaction tuning epoch: 42, train loss: 0.04746, val loss: 0.04805\n",
      "Interaction tuning epoch: 43, train loss: 0.04604, val loss: 0.04612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 44, train loss: 0.04636, val loss: 0.04673\n",
      "Interaction tuning epoch: 45, train loss: 0.04643, val loss: 0.04712\n",
      "Interaction tuning epoch: 46, train loss: 0.04656, val loss: 0.04678\n",
      "Interaction tuning epoch: 47, train loss: 0.04635, val loss: 0.04667\n",
      "Interaction tuning epoch: 48, train loss: 0.04816, val loss: 0.04882\n",
      "Interaction tuning epoch: 49, train loss: 0.04683, val loss: 0.04711\n",
      "Interaction tuning epoch: 50, train loss: 0.04845, val loss: 0.04905\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 34.65741181373596\n",
      "After the gam stage, training error is 0.04845 , validation error is 0.04905\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 1.232198\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.041688 validation MAE=0.046390,rank=3\n",
      "[SoftImpute] Iter 2: observed MAE=0.039251 validation MAE=0.045242,rank=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 3: observed MAE=0.037198 validation MAE=0.044225,rank=3\n",
      "[SoftImpute] Iter 4: observed MAE=0.035449 validation MAE=0.043339,rank=3\n",
      "[SoftImpute] Iter 5: observed MAE=0.033955 validation MAE=0.042560,rank=3\n",
      "[SoftImpute] Iter 6: observed MAE=0.032672 validation MAE=0.041874,rank=3\n",
      "[SoftImpute] Iter 7: observed MAE=0.031564 validation MAE=0.041301,rank=3\n",
      "[SoftImpute] Iter 8: observed MAE=0.030599 validation MAE=0.040805,rank=3\n",
      "[SoftImpute] Iter 9: observed MAE=0.029748 validation MAE=0.040355,rank=3\n",
      "[SoftImpute] Iter 10: observed MAE=0.029001 validation MAE=0.039946,rank=3\n",
      "[SoftImpute] Iter 11: observed MAE=0.028337 validation MAE=0.039579,rank=3\n",
      "[SoftImpute] Iter 12: observed MAE=0.027742 validation MAE=0.039235,rank=3\n",
      "[SoftImpute] Iter 13: observed MAE=0.027206 validation MAE=0.038912,rank=3\n",
      "[SoftImpute] Iter 14: observed MAE=0.026721 validation MAE=0.038614,rank=3\n",
      "[SoftImpute] Iter 15: observed MAE=0.026278 validation MAE=0.038332,rank=3\n",
      "[SoftImpute] Iter 16: observed MAE=0.025874 validation MAE=0.038069,rank=3\n",
      "[SoftImpute] Iter 17: observed MAE=0.025500 validation MAE=0.037828,rank=3\n",
      "[SoftImpute] Iter 18: observed MAE=0.025158 validation MAE=0.037613,rank=3\n",
      "[SoftImpute] Iter 19: observed MAE=0.024841 validation MAE=0.037419,rank=3\n",
      "[SoftImpute] Iter 20: observed MAE=0.024546 validation MAE=0.037238,rank=3\n",
      "[SoftImpute] Iter 21: observed MAE=0.024268 validation MAE=0.037064,rank=3\n",
      "[SoftImpute] Iter 22: observed MAE=0.024008 validation MAE=0.036899,rank=3\n",
      "[SoftImpute] Iter 23: observed MAE=0.023762 validation MAE=0.036741,rank=3\n",
      "[SoftImpute] Iter 24: observed MAE=0.023529 validation MAE=0.036587,rank=3\n",
      "[SoftImpute] Iter 25: observed MAE=0.023309 validation MAE=0.036444,rank=3\n",
      "[SoftImpute] Iter 26: observed MAE=0.023101 validation MAE=0.036307,rank=3\n",
      "[SoftImpute] Iter 27: observed MAE=0.022904 validation MAE=0.036175,rank=3\n",
      "[SoftImpute] Iter 28: observed MAE=0.022715 validation MAE=0.036049,rank=3\n",
      "[SoftImpute] Iter 29: observed MAE=0.022537 validation MAE=0.035928,rank=3\n",
      "[SoftImpute] Iter 30: observed MAE=0.022367 validation MAE=0.035807,rank=3\n",
      "[SoftImpute] Iter 31: observed MAE=0.022203 validation MAE=0.035690,rank=3\n",
      "[SoftImpute] Iter 32: observed MAE=0.022045 validation MAE=0.035577,rank=3\n",
      "[SoftImpute] Iter 33: observed MAE=0.021894 validation MAE=0.035467,rank=3\n",
      "[SoftImpute] Iter 34: observed MAE=0.021747 validation MAE=0.035358,rank=3\n",
      "[SoftImpute] Iter 35: observed MAE=0.021606 validation MAE=0.035252,rank=3\n",
      "[SoftImpute] Iter 36: observed MAE=0.021468 validation MAE=0.035146,rank=3\n",
      "[SoftImpute] Iter 37: observed MAE=0.021335 validation MAE=0.035045,rank=3\n",
      "[SoftImpute] Iter 38: observed MAE=0.021206 validation MAE=0.034945,rank=3\n",
      "[SoftImpute] Iter 39: observed MAE=0.021081 validation MAE=0.034845,rank=3\n",
      "[SoftImpute] Iter 40: observed MAE=0.020959 validation MAE=0.034747,rank=3\n",
      "[SoftImpute] Iter 41: observed MAE=0.020840 validation MAE=0.034653,rank=3\n",
      "[SoftImpute] Iter 42: observed MAE=0.020725 validation MAE=0.034560,rank=3\n",
      "[SoftImpute] Iter 43: observed MAE=0.020613 validation MAE=0.034467,rank=3\n",
      "[SoftImpute] Iter 44: observed MAE=0.020505 validation MAE=0.034377,rank=3\n",
      "[SoftImpute] Iter 45: observed MAE=0.020401 validation MAE=0.034289,rank=3\n",
      "[SoftImpute] Iter 46: observed MAE=0.020299 validation MAE=0.034203,rank=3\n",
      "[SoftImpute] Iter 47: observed MAE=0.020199 validation MAE=0.034123,rank=3\n",
      "[SoftImpute] Iter 48: observed MAE=0.020101 validation MAE=0.034046,rank=3\n",
      "[SoftImpute] Iter 49: observed MAE=0.020005 validation MAE=0.033968,rank=3\n",
      "[SoftImpute] Iter 50: observed MAE=0.019912 validation MAE=0.033889,rank=3\n",
      "[SoftImpute] Iter 51: observed MAE=0.019821 validation MAE=0.033813,rank=3\n",
      "[SoftImpute] Iter 52: observed MAE=0.019732 validation MAE=0.033737,rank=3\n",
      "[SoftImpute] Iter 53: observed MAE=0.019644 validation MAE=0.033664,rank=3\n",
      "[SoftImpute] Iter 54: observed MAE=0.019559 validation MAE=0.033593,rank=3\n",
      "[SoftImpute] Iter 55: observed MAE=0.019474 validation MAE=0.033524,rank=3\n",
      "[SoftImpute] Iter 56: observed MAE=0.019391 validation MAE=0.033454,rank=3\n",
      "[SoftImpute] Iter 57: observed MAE=0.019309 validation MAE=0.033386,rank=3\n",
      "[SoftImpute] Iter 58: observed MAE=0.019229 validation MAE=0.033317,rank=3\n",
      "[SoftImpute] Iter 59: observed MAE=0.019149 validation MAE=0.033250,rank=3\n",
      "[SoftImpute] Iter 60: observed MAE=0.019072 validation MAE=0.033185,rank=3\n",
      "[SoftImpute] Iter 61: observed MAE=0.018996 validation MAE=0.033126,rank=3\n",
      "[SoftImpute] Iter 62: observed MAE=0.018922 validation MAE=0.033072,rank=3\n",
      "[SoftImpute] Iter 63: observed MAE=0.018849 validation MAE=0.033020,rank=3\n",
      "[SoftImpute] Iter 64: observed MAE=0.018778 validation MAE=0.032969,rank=3\n",
      "[SoftImpute] Iter 65: observed MAE=0.018708 validation MAE=0.032919,rank=3\n",
      "[SoftImpute] Iter 66: observed MAE=0.018639 validation MAE=0.032868,rank=3\n",
      "[SoftImpute] Iter 67: observed MAE=0.018572 validation MAE=0.032818,rank=3\n",
      "[SoftImpute] Iter 68: observed MAE=0.018505 validation MAE=0.032767,rank=3\n",
      "[SoftImpute] Iter 69: observed MAE=0.018439 validation MAE=0.032716,rank=3\n",
      "[SoftImpute] Iter 70: observed MAE=0.018375 validation MAE=0.032665,rank=3\n",
      "[SoftImpute] Iter 71: observed MAE=0.018312 validation MAE=0.032615,rank=3\n",
      "[SoftImpute] Iter 72: observed MAE=0.018251 validation MAE=0.032567,rank=3\n",
      "[SoftImpute] Iter 73: observed MAE=0.018190 validation MAE=0.032519,rank=3\n",
      "[SoftImpute] Iter 74: observed MAE=0.018131 validation MAE=0.032471,rank=3\n",
      "[SoftImpute] Iter 75: observed MAE=0.018073 validation MAE=0.032425,rank=3\n",
      "[SoftImpute] Iter 76: observed MAE=0.018016 validation MAE=0.032379,rank=3\n",
      "[SoftImpute] Iter 77: observed MAE=0.017960 validation MAE=0.032333,rank=3\n",
      "[SoftImpute] Iter 78: observed MAE=0.017905 validation MAE=0.032286,rank=3\n",
      "[SoftImpute] Iter 79: observed MAE=0.017852 validation MAE=0.032239,rank=3\n",
      "[SoftImpute] Iter 80: observed MAE=0.017799 validation MAE=0.032192,rank=3\n",
      "[SoftImpute] Iter 81: observed MAE=0.017748 validation MAE=0.032145,rank=3\n",
      "[SoftImpute] Iter 82: observed MAE=0.017697 validation MAE=0.032099,rank=3\n",
      "[SoftImpute] Iter 83: observed MAE=0.017648 validation MAE=0.032054,rank=3\n",
      "[SoftImpute] Iter 84: observed MAE=0.017599 validation MAE=0.032008,rank=3\n",
      "[SoftImpute] Iter 85: observed MAE=0.017551 validation MAE=0.031962,rank=3\n",
      "[SoftImpute] Iter 86: observed MAE=0.017503 validation MAE=0.031919,rank=3\n",
      "[SoftImpute] Iter 87: observed MAE=0.017457 validation MAE=0.031876,rank=3\n",
      "[SoftImpute] Iter 88: observed MAE=0.017412 validation MAE=0.031832,rank=3\n",
      "[SoftImpute] Iter 89: observed MAE=0.017367 validation MAE=0.031789,rank=3\n",
      "[SoftImpute] Iter 90: observed MAE=0.017323 validation MAE=0.031746,rank=3\n",
      "[SoftImpute] Iter 91: observed MAE=0.017280 validation MAE=0.031702,rank=3\n",
      "[SoftImpute] Iter 92: observed MAE=0.017237 validation MAE=0.031658,rank=3\n",
      "[SoftImpute] Iter 93: observed MAE=0.017196 validation MAE=0.031615,rank=3\n",
      "[SoftImpute] Iter 94: observed MAE=0.017155 validation MAE=0.031572,rank=3\n",
      "[SoftImpute] Iter 95: observed MAE=0.017115 validation MAE=0.031530,rank=3\n",
      "[SoftImpute] Iter 96: observed MAE=0.017076 validation MAE=0.031487,rank=3\n",
      "[SoftImpute] Iter 97: observed MAE=0.017037 validation MAE=0.031445,rank=3\n",
      "[SoftImpute] Iter 98: observed MAE=0.016999 validation MAE=0.031404,rank=3\n",
      "[SoftImpute] Iter 99: observed MAE=0.016962 validation MAE=0.031362,rank=3\n",
      "[SoftImpute] Iter 100: observed MAE=0.016924 validation MAE=0.031320,rank=3\n",
      "[SoftImpute] Iter 101: observed MAE=0.016888 validation MAE=0.031279,rank=3\n",
      "[SoftImpute] Iter 102: observed MAE=0.016851 validation MAE=0.031239,rank=3\n",
      "[SoftImpute] Iter 103: observed MAE=0.016816 validation MAE=0.031200,rank=3\n",
      "[SoftImpute] Iter 104: observed MAE=0.016780 validation MAE=0.031161,rank=3\n",
      "[SoftImpute] Iter 105: observed MAE=0.016745 validation MAE=0.031123,rank=3\n",
      "[SoftImpute] Iter 106: observed MAE=0.016711 validation MAE=0.031084,rank=3\n",
      "[SoftImpute] Iter 107: observed MAE=0.016677 validation MAE=0.031046,rank=3\n",
      "[SoftImpute] Iter 108: observed MAE=0.016644 validation MAE=0.031007,rank=3\n",
      "[SoftImpute] Iter 109: observed MAE=0.016611 validation MAE=0.030969,rank=3\n",
      "[SoftImpute] Iter 110: observed MAE=0.016580 validation MAE=0.030930,rank=3\n",
      "[SoftImpute] Iter 111: observed MAE=0.016548 validation MAE=0.030892,rank=3\n",
      "[SoftImpute] Iter 112: observed MAE=0.016518 validation MAE=0.030854,rank=3\n",
      "[SoftImpute] Iter 113: observed MAE=0.016487 validation MAE=0.030817,rank=3\n",
      "[SoftImpute] Iter 114: observed MAE=0.016458 validation MAE=0.030781,rank=3\n",
      "[SoftImpute] Iter 115: observed MAE=0.016428 validation MAE=0.030745,rank=3\n",
      "[SoftImpute] Iter 116: observed MAE=0.016399 validation MAE=0.030710,rank=3\n",
      "[SoftImpute] Iter 117: observed MAE=0.016371 validation MAE=0.030674,rank=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 118: observed MAE=0.016343 validation MAE=0.030640,rank=3\n",
      "[SoftImpute] Iter 119: observed MAE=0.016315 validation MAE=0.030605,rank=3\n",
      "[SoftImpute] Iter 120: observed MAE=0.016289 validation MAE=0.030572,rank=3\n",
      "[SoftImpute] Iter 121: observed MAE=0.016262 validation MAE=0.030538,rank=3\n",
      "[SoftImpute] Iter 122: observed MAE=0.016236 validation MAE=0.030506,rank=3\n",
      "[SoftImpute] Iter 123: observed MAE=0.016211 validation MAE=0.030474,rank=3\n",
      "[SoftImpute] Iter 124: observed MAE=0.016186 validation MAE=0.030444,rank=3\n",
      "[SoftImpute] Iter 125: observed MAE=0.016161 validation MAE=0.030415,rank=3\n",
      "[SoftImpute] Iter 126: observed MAE=0.016136 validation MAE=0.030386,rank=3\n",
      "[SoftImpute] Iter 127: observed MAE=0.016112 validation MAE=0.030357,rank=3\n",
      "[SoftImpute] Iter 128: observed MAE=0.016089 validation MAE=0.030330,rank=3\n",
      "[SoftImpute] Iter 129: observed MAE=0.016065 validation MAE=0.030303,rank=3\n",
      "[SoftImpute] Iter 130: observed MAE=0.016042 validation MAE=0.030275,rank=3\n",
      "[SoftImpute] Iter 131: observed MAE=0.016020 validation MAE=0.030248,rank=3\n",
      "[SoftImpute] Iter 132: observed MAE=0.015998 validation MAE=0.030221,rank=3\n",
      "[SoftImpute] Iter 133: observed MAE=0.015976 validation MAE=0.030195,rank=3\n",
      "[SoftImpute] Iter 134: observed MAE=0.015955 validation MAE=0.030168,rank=3\n",
      "[SoftImpute] Iter 135: observed MAE=0.015934 validation MAE=0.030142,rank=3\n",
      "[SoftImpute] Iter 136: observed MAE=0.015913 validation MAE=0.030116,rank=3\n",
      "[SoftImpute] Iter 137: observed MAE=0.015893 validation MAE=0.030091,rank=3\n",
      "[SoftImpute] Iter 138: observed MAE=0.015872 validation MAE=0.030065,rank=3\n",
      "[SoftImpute] Iter 139: observed MAE=0.015853 validation MAE=0.030041,rank=3\n",
      "[SoftImpute] Iter 140: observed MAE=0.015833 validation MAE=0.030018,rank=3\n",
      "[SoftImpute] Iter 141: observed MAE=0.015814 validation MAE=0.029994,rank=3\n",
      "[SoftImpute] Iter 142: observed MAE=0.015795 validation MAE=0.029971,rank=3\n",
      "[SoftImpute] Iter 143: observed MAE=0.015777 validation MAE=0.029949,rank=3\n",
      "[SoftImpute] Iter 144: observed MAE=0.015759 validation MAE=0.029927,rank=3\n",
      "[SoftImpute] Iter 145: observed MAE=0.015741 validation MAE=0.029904,rank=3\n",
      "[SoftImpute] Iter 146: observed MAE=0.015723 validation MAE=0.029882,rank=3\n",
      "[SoftImpute] Iter 147: observed MAE=0.015706 validation MAE=0.029861,rank=3\n",
      "[SoftImpute] Iter 148: observed MAE=0.015689 validation MAE=0.029841,rank=3\n",
      "[SoftImpute] Iter 149: observed MAE=0.015673 validation MAE=0.029822,rank=3\n",
      "[SoftImpute] Iter 150: observed MAE=0.015656 validation MAE=0.029803,rank=3\n",
      "[SoftImpute] Iter 151: observed MAE=0.015640 validation MAE=0.029785,rank=3\n",
      "[SoftImpute] Iter 152: observed MAE=0.015624 validation MAE=0.029768,rank=3\n",
      "[SoftImpute] Iter 153: observed MAE=0.015609 validation MAE=0.029750,rank=3\n",
      "[SoftImpute] Iter 154: observed MAE=0.015594 validation MAE=0.029733,rank=3\n",
      "[SoftImpute] Iter 155: observed MAE=0.015579 validation MAE=0.029715,rank=3\n",
      "[SoftImpute] Iter 156: observed MAE=0.015564 validation MAE=0.029700,rank=3\n",
      "[SoftImpute] Iter 157: observed MAE=0.015550 validation MAE=0.029686,rank=3\n",
      "[SoftImpute] Iter 158: observed MAE=0.015535 validation MAE=0.029673,rank=3\n",
      "[SoftImpute] Iter 159: observed MAE=0.015521 validation MAE=0.029660,rank=3\n",
      "[SoftImpute] Iter 160: observed MAE=0.015508 validation MAE=0.029648,rank=3\n",
      "[SoftImpute] Iter 161: observed MAE=0.015494 validation MAE=0.029636,rank=3\n",
      "[SoftImpute] Iter 162: observed MAE=0.015481 validation MAE=0.029624,rank=3\n",
      "[SoftImpute] Iter 163: observed MAE=0.015467 validation MAE=0.029612,rank=3\n",
      "[SoftImpute] Iter 164: observed MAE=0.015454 validation MAE=0.029600,rank=3\n",
      "[SoftImpute] Iter 165: observed MAE=0.015441 validation MAE=0.029588,rank=3\n",
      "[SoftImpute] Iter 166: observed MAE=0.015429 validation MAE=0.029577,rank=3\n",
      "[SoftImpute] Iter 167: observed MAE=0.015416 validation MAE=0.029565,rank=3\n",
      "[SoftImpute] Iter 168: observed MAE=0.015404 validation MAE=0.029553,rank=3\n",
      "[SoftImpute] Iter 169: observed MAE=0.015391 validation MAE=0.029541,rank=3\n",
      "[SoftImpute] Iter 170: observed MAE=0.015379 validation MAE=0.029530,rank=3\n",
      "[SoftImpute] Iter 171: observed MAE=0.015368 validation MAE=0.029518,rank=3\n",
      "[SoftImpute] Iter 172: observed MAE=0.015356 validation MAE=0.029507,rank=3\n",
      "[SoftImpute] Iter 173: observed MAE=0.015345 validation MAE=0.029496,rank=3\n",
      "[SoftImpute] Iter 174: observed MAE=0.015334 validation MAE=0.029486,rank=3\n",
      "[SoftImpute] Iter 175: observed MAE=0.015323 validation MAE=0.029475,rank=3\n",
      "[SoftImpute] Iter 176: observed MAE=0.015312 validation MAE=0.029465,rank=3\n",
      "[SoftImpute] Iter 177: observed MAE=0.015302 validation MAE=0.029455,rank=3\n",
      "[SoftImpute] Iter 178: observed MAE=0.015292 validation MAE=0.029445,rank=3\n",
      "[SoftImpute] Iter 179: observed MAE=0.015282 validation MAE=0.029436,rank=3\n",
      "[SoftImpute] Iter 180: observed MAE=0.015272 validation MAE=0.029428,rank=3\n",
      "[SoftImpute] Iter 181: observed MAE=0.015262 validation MAE=0.029420,rank=3\n",
      "[SoftImpute] Iter 182: observed MAE=0.015252 validation MAE=0.029412,rank=3\n",
      "[SoftImpute] Iter 183: observed MAE=0.015243 validation MAE=0.029404,rank=3\n",
      "[SoftImpute] Iter 184: observed MAE=0.015233 validation MAE=0.029396,rank=3\n",
      "[SoftImpute] Iter 185: observed MAE=0.015224 validation MAE=0.029388,rank=3\n",
      "[SoftImpute] Iter 186: observed MAE=0.015214 validation MAE=0.029381,rank=3\n",
      "[SoftImpute] Iter 187: observed MAE=0.015205 validation MAE=0.029374,rank=3\n",
      "[SoftImpute] Iter 188: observed MAE=0.015196 validation MAE=0.029368,rank=3\n",
      "[SoftImpute] Iter 189: observed MAE=0.015187 validation MAE=0.029362,rank=3\n",
      "[SoftImpute] Iter 190: observed MAE=0.015179 validation MAE=0.029356,rank=3\n",
      "[SoftImpute] Iter 191: observed MAE=0.015170 validation MAE=0.029350,rank=3\n",
      "[SoftImpute] Iter 192: observed MAE=0.015162 validation MAE=0.029345,rank=3\n",
      "[SoftImpute] Iter 193: observed MAE=0.015153 validation MAE=0.029339,rank=3\n",
      "[SoftImpute] Iter 194: observed MAE=0.015145 validation MAE=0.029335,rank=3\n",
      "[SoftImpute] Iter 195: observed MAE=0.015137 validation MAE=0.029331,rank=3\n",
      "[SoftImpute] Iter 196: observed MAE=0.015129 validation MAE=0.029328,rank=3\n",
      "[SoftImpute] Iter 197: observed MAE=0.015121 validation MAE=0.029325,rank=3\n",
      "[SoftImpute] Iter 198: observed MAE=0.015113 validation MAE=0.029322,rank=3\n",
      "[SoftImpute] Iter 199: observed MAE=0.015106 validation MAE=0.029319,rank=3\n",
      "[SoftImpute] Iter 200: observed MAE=0.015098 validation MAE=0.029316,rank=3\n",
      "[SoftImpute] Iter 201: observed MAE=0.015091 validation MAE=0.029313,rank=3\n",
      "[SoftImpute] Iter 202: observed MAE=0.015084 validation MAE=0.029310,rank=3\n",
      "[SoftImpute] Iter 203: observed MAE=0.015076 validation MAE=0.029306,rank=3\n",
      "[SoftImpute] Iter 204: observed MAE=0.015069 validation MAE=0.029303,rank=3\n",
      "[SoftImpute] Iter 205: observed MAE=0.015062 validation MAE=0.029300,rank=3\n",
      "[SoftImpute] Iter 206: observed MAE=0.015056 validation MAE=0.029297,rank=3\n",
      "[SoftImpute] Iter 207: observed MAE=0.015049 validation MAE=0.029294,rank=3\n",
      "[SoftImpute] Iter 208: observed MAE=0.015042 validation MAE=0.029290,rank=3\n",
      "[SoftImpute] Iter 209: observed MAE=0.015036 validation MAE=0.029287,rank=3\n",
      "[SoftImpute] Iter 210: observed MAE=0.015029 validation MAE=0.029284,rank=3\n",
      "[SoftImpute] Iter 211: observed MAE=0.015023 validation MAE=0.029281,rank=3\n",
      "[SoftImpute] Iter 212: observed MAE=0.015017 validation MAE=0.029278,rank=3\n",
      "[SoftImpute] Iter 213: observed MAE=0.015010 validation MAE=0.029274,rank=3\n",
      "[SoftImpute] Iter 214: observed MAE=0.015004 validation MAE=0.029271,rank=3\n",
      "[SoftImpute] Iter 215: observed MAE=0.014998 validation MAE=0.029268,rank=3\n",
      "[SoftImpute] Iter 216: observed MAE=0.014993 validation MAE=0.029265,rank=3\n",
      "[SoftImpute] Iter 217: observed MAE=0.014987 validation MAE=0.029262,rank=3\n",
      "[SoftImpute] Iter 218: observed MAE=0.014981 validation MAE=0.029259,rank=3\n",
      "[SoftImpute] Iter 219: observed MAE=0.014975 validation MAE=0.029256,rank=3\n",
      "[SoftImpute] Iter 220: observed MAE=0.014970 validation MAE=0.029253,rank=3\n",
      "[SoftImpute] Iter 221: observed MAE=0.014964 validation MAE=0.029249,rank=3\n",
      "[SoftImpute] Iter 222: observed MAE=0.014959 validation MAE=0.029246,rank=3\n",
      "[SoftImpute] Iter 223: observed MAE=0.014953 validation MAE=0.029244,rank=3\n",
      "[SoftImpute] Iter 224: observed MAE=0.014948 validation MAE=0.029241,rank=3\n",
      "[SoftImpute] Iter 225: observed MAE=0.014943 validation MAE=0.029239,rank=3\n",
      "[SoftImpute] Iter 226: observed MAE=0.014937 validation MAE=0.029236,rank=3\n",
      "[SoftImpute] Iter 227: observed MAE=0.014932 validation MAE=0.029234,rank=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 228: observed MAE=0.014927 validation MAE=0.029232,rank=3\n",
      "[SoftImpute] Iter 229: observed MAE=0.014922 validation MAE=0.029230,rank=3\n",
      "[SoftImpute] Iter 230: observed MAE=0.014917 validation MAE=0.029228,rank=3\n",
      "[SoftImpute] Iter 231: observed MAE=0.014912 validation MAE=0.029227,rank=3\n",
      "[SoftImpute] Iter 232: observed MAE=0.014908 validation MAE=0.029226,rank=3\n",
      "[SoftImpute] Iter 233: observed MAE=0.014903 validation MAE=0.029225,rank=3\n",
      "[SoftImpute] Iter 234: observed MAE=0.014898 validation MAE=0.029224,rank=3\n",
      "[SoftImpute] Iter 235: observed MAE=0.014893 validation MAE=0.029224,rank=3\n",
      "[SoftImpute] Iter 236: observed MAE=0.014889 validation MAE=0.029223,rank=3\n",
      "[SoftImpute] Iter 237: observed MAE=0.014884 validation MAE=0.029223,rank=3\n",
      "[SoftImpute] Iter 238: observed MAE=0.014880 validation MAE=0.029223,rank=3\n",
      "[SoftImpute] Iter 239: observed MAE=0.014875 validation MAE=0.029222,rank=3\n",
      "[SoftImpute] Iter 240: observed MAE=0.014871 validation MAE=0.029222,rank=3\n",
      "[SoftImpute] Iter 241: observed MAE=0.014867 validation MAE=0.029221,rank=3\n",
      "[SoftImpute] Iter 242: observed MAE=0.014862 validation MAE=0.029221,rank=3\n",
      "[SoftImpute] Iter 243: observed MAE=0.014858 validation MAE=0.029221,rank=3\n",
      "[SoftImpute] Iter 244: observed MAE=0.014854 validation MAE=0.029221,rank=3\n",
      "[SoftImpute] Iter 245: observed MAE=0.014850 validation MAE=0.029221,rank=3\n",
      "[SoftImpute] Iter 246: observed MAE=0.014845 validation MAE=0.029221,rank=3\n",
      "[SoftImpute] Iter 247: observed MAE=0.014841 validation MAE=0.029222,rank=3\n",
      "[SoftImpute] Stopped after iteration 247 for lambda=0.024644\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 7.0646703243255615\n",
      "After the matrix factor stage, training error is 0.01484, validation error is 0.02922\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.30185, val loss: 0.30478\n",
      "Main effects training epoch: 2, train loss: 0.23228, val loss: 0.23617\n",
      "Main effects training epoch: 3, train loss: 0.17997, val loss: 0.18315\n",
      "Main effects training epoch: 4, train loss: 0.14845, val loss: 0.15215\n",
      "Main effects training epoch: 5, train loss: 0.13484, val loss: 0.13695\n",
      "Main effects training epoch: 6, train loss: 0.13033, val loss: 0.13155\n",
      "Main effects training epoch: 7, train loss: 0.13055, val loss: 0.13149\n",
      "Main effects training epoch: 8, train loss: 0.12971, val loss: 0.12988\n",
      "Main effects training epoch: 9, train loss: 0.12862, val loss: 0.12919\n",
      "Main effects training epoch: 10, train loss: 0.12828, val loss: 0.12938\n",
      "Main effects training epoch: 11, train loss: 0.12732, val loss: 0.12789\n",
      "Main effects training epoch: 12, train loss: 0.12659, val loss: 0.12743\n",
      "Main effects training epoch: 13, train loss: 0.12563, val loss: 0.12654\n",
      "Main effects training epoch: 14, train loss: 0.12325, val loss: 0.12456\n",
      "Main effects training epoch: 15, train loss: 0.11949, val loss: 0.12161\n",
      "Main effects training epoch: 16, train loss: 0.11686, val loss: 0.11922\n",
      "Main effects training epoch: 17, train loss: 0.11423, val loss: 0.11562\n",
      "Main effects training epoch: 18, train loss: 0.11484, val loss: 0.11687\n",
      "Main effects training epoch: 19, train loss: 0.11188, val loss: 0.11473\n",
      "Main effects training epoch: 20, train loss: 0.11089, val loss: 0.11324\n",
      "Main effects training epoch: 21, train loss: 0.10849, val loss: 0.11127\n",
      "Main effects training epoch: 22, train loss: 0.10778, val loss: 0.11002\n",
      "Main effects training epoch: 23, train loss: 0.10761, val loss: 0.10934\n",
      "Main effects training epoch: 24, train loss: 0.10695, val loss: 0.10962\n",
      "Main effects training epoch: 25, train loss: 0.10869, val loss: 0.11093\n",
      "Main effects training epoch: 26, train loss: 0.10721, val loss: 0.11016\n",
      "Main effects training epoch: 27, train loss: 0.10692, val loss: 0.10911\n",
      "Main effects training epoch: 28, train loss: 0.10656, val loss: 0.10921\n",
      "Main effects training epoch: 29, train loss: 0.10666, val loss: 0.10915\n",
      "Main effects training epoch: 30, train loss: 0.10688, val loss: 0.10958\n",
      "Main effects training epoch: 31, train loss: 0.10689, val loss: 0.10943\n",
      "Main effects training epoch: 32, train loss: 0.10622, val loss: 0.10918\n",
      "Main effects training epoch: 33, train loss: 0.10631, val loss: 0.10928\n",
      "Main effects training epoch: 34, train loss: 0.10605, val loss: 0.10868\n",
      "Main effects training epoch: 35, train loss: 0.10599, val loss: 0.10879\n",
      "Main effects training epoch: 36, train loss: 0.10605, val loss: 0.10911\n",
      "Main effects training epoch: 37, train loss: 0.10704, val loss: 0.10981\n",
      "Main effects training epoch: 38, train loss: 0.10655, val loss: 0.10926\n",
      "Main effects training epoch: 39, train loss: 0.10607, val loss: 0.10888\n",
      "Main effects training epoch: 40, train loss: 0.10588, val loss: 0.10859\n",
      "Main effects training epoch: 41, train loss: 0.10600, val loss: 0.10889\n",
      "Main effects training epoch: 42, train loss: 0.10616, val loss: 0.10903\n",
      "Main effects training epoch: 43, train loss: 0.10609, val loss: 0.10883\n",
      "Main effects training epoch: 44, train loss: 0.10630, val loss: 0.10899\n",
      "Main effects training epoch: 45, train loss: 0.10686, val loss: 0.10928\n",
      "Main effects training epoch: 46, train loss: 0.10599, val loss: 0.10904\n",
      "Main effects training epoch: 47, train loss: 0.10577, val loss: 0.10887\n",
      "Main effects training epoch: 48, train loss: 0.10584, val loss: 0.10888\n",
      "Main effects training epoch: 49, train loss: 0.10576, val loss: 0.10866\n",
      "Main effects training epoch: 50, train loss: 0.10617, val loss: 0.10903\n",
      "Main effects training epoch: 51, train loss: 0.10592, val loss: 0.10893\n",
      "Main effects training epoch: 52, train loss: 0.10606, val loss: 0.10892\n",
      "Main effects training epoch: 53, train loss: 0.10577, val loss: 0.10888\n",
      "Main effects training epoch: 54, train loss: 0.10586, val loss: 0.10875\n",
      "Main effects training epoch: 55, train loss: 0.10635, val loss: 0.10929\n",
      "Main effects training epoch: 56, train loss: 0.10600, val loss: 0.10895\n",
      "Main effects training epoch: 57, train loss: 0.10614, val loss: 0.10942\n",
      "Main effects training epoch: 58, train loss: 0.10591, val loss: 0.10905\n",
      "Main effects training epoch: 59, train loss: 0.10587, val loss: 0.10866\n",
      "Main effects training epoch: 60, train loss: 0.10580, val loss: 0.10885\n",
      "Main effects training epoch: 61, train loss: 0.10590, val loss: 0.10868\n",
      "Main effects training epoch: 62, train loss: 0.10611, val loss: 0.10911\n",
      "Main effects training epoch: 63, train loss: 0.10587, val loss: 0.10877\n",
      "Main effects training epoch: 64, train loss: 0.10611, val loss: 0.10908\n",
      "Main effects training epoch: 65, train loss: 0.10597, val loss: 0.10888\n",
      "Main effects training epoch: 66, train loss: 0.10579, val loss: 0.10886\n",
      "Main effects training epoch: 67, train loss: 0.10588, val loss: 0.10853\n",
      "Main effects training epoch: 68, train loss: 0.10601, val loss: 0.10926\n",
      "Main effects training epoch: 69, train loss: 0.10583, val loss: 0.10866\n",
      "Main effects training epoch: 70, train loss: 0.10585, val loss: 0.10893\n",
      "Main effects training epoch: 71, train loss: 0.10592, val loss: 0.10879\n",
      "Main effects training epoch: 72, train loss: 0.10582, val loss: 0.10901\n",
      "Main effects training epoch: 73, train loss: 0.10586, val loss: 0.10890\n",
      "Main effects training epoch: 74, train loss: 0.10596, val loss: 0.10893\n",
      "Main effects training epoch: 75, train loss: 0.10614, val loss: 0.10884\n",
      "Main effects training epoch: 76, train loss: 0.10583, val loss: 0.10904\n",
      "Main effects training epoch: 77, train loss: 0.10592, val loss: 0.10890\n",
      "Main effects training epoch: 78, train loss: 0.10580, val loss: 0.10881\n",
      "Main effects training epoch: 79, train loss: 0.10594, val loss: 0.10889\n",
      "Main effects training epoch: 80, train loss: 0.10607, val loss: 0.10888\n",
      "Main effects training epoch: 81, train loss: 0.10581, val loss: 0.10891\n",
      "Main effects training epoch: 82, train loss: 0.10593, val loss: 0.10922\n",
      "Main effects training epoch: 83, train loss: 0.10574, val loss: 0.10873\n",
      "Main effects training epoch: 84, train loss: 0.10573, val loss: 0.10877\n",
      "Main effects training epoch: 85, train loss: 0.10636, val loss: 0.10929\n",
      "Main effects training epoch: 86, train loss: 0.10583, val loss: 0.10874\n",
      "Main effects training epoch: 87, train loss: 0.10614, val loss: 0.10928\n",
      "Main effects training epoch: 88, train loss: 0.10616, val loss: 0.10949\n",
      "Main effects training epoch: 89, train loss: 0.10589, val loss: 0.10865\n",
      "Main effects training epoch: 90, train loss: 0.10604, val loss: 0.10903\n",
      "Main effects training epoch: 91, train loss: 0.10588, val loss: 0.10892\n",
      "Main effects training epoch: 92, train loss: 0.10612, val loss: 0.10925\n",
      "Main effects training epoch: 93, train loss: 0.10583, val loss: 0.10911\n",
      "Main effects training epoch: 94, train loss: 0.10604, val loss: 0.10904\n",
      "Main effects training epoch: 95, train loss: 0.10576, val loss: 0.10903\n",
      "Main effects training epoch: 96, train loss: 0.10568, val loss: 0.10869\n",
      "Main effects training epoch: 97, train loss: 0.10588, val loss: 0.10926\n",
      "Main effects training epoch: 98, train loss: 0.10606, val loss: 0.10910\n",
      "Main effects training epoch: 99, train loss: 0.10649, val loss: 0.10930\n",
      "Main effects training epoch: 100, train loss: 0.10628, val loss: 0.10948\n",
      "Main effects training epoch: 101, train loss: 0.10586, val loss: 0.10912\n",
      "Main effects training epoch: 102, train loss: 0.10570, val loss: 0.10869\n",
      "Main effects training epoch: 103, train loss: 0.10643, val loss: 0.10951\n",
      "Main effects training epoch: 104, train loss: 0.10650, val loss: 0.10946\n",
      "Main effects training epoch: 105, train loss: 0.10626, val loss: 0.10954\n",
      "Main effects training epoch: 106, train loss: 0.10594, val loss: 0.10896\n",
      "Main effects training epoch: 107, train loss: 0.10579, val loss: 0.10878\n",
      "Main effects training epoch: 108, train loss: 0.10607, val loss: 0.10922\n",
      "Main effects training epoch: 109, train loss: 0.10581, val loss: 0.10902\n",
      "Main effects training epoch: 110, train loss: 0.10590, val loss: 0.10892\n",
      "Main effects training epoch: 111, train loss: 0.10605, val loss: 0.10924\n",
      "Main effects training epoch: 112, train loss: 0.10580, val loss: 0.10875\n",
      "Main effects training epoch: 113, train loss: 0.10587, val loss: 0.10900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 114, train loss: 0.10581, val loss: 0.10883\n",
      "Main effects training epoch: 115, train loss: 0.10618, val loss: 0.10915\n",
      "Main effects training epoch: 116, train loss: 0.10615, val loss: 0.10949\n",
      "Main effects training epoch: 117, train loss: 0.10602, val loss: 0.10894\n",
      "Main effects training epoch: 118, train loss: 0.10595, val loss: 0.10914\n",
      "Main effects training epoch: 119, train loss: 0.10618, val loss: 0.10895\n",
      "Main effects training epoch: 120, train loss: 0.10601, val loss: 0.10941\n",
      "Main effects training epoch: 121, train loss: 0.10588, val loss: 0.10869\n",
      "Main effects training epoch: 122, train loss: 0.10576, val loss: 0.10885\n",
      "Main effects training epoch: 123, train loss: 0.10578, val loss: 0.10899\n",
      "Main effects training epoch: 124, train loss: 0.10583, val loss: 0.10934\n",
      "Main effects training epoch: 125, train loss: 0.10610, val loss: 0.10904\n",
      "Main effects training epoch: 126, train loss: 0.10593, val loss: 0.10917\n",
      "Main effects training epoch: 127, train loss: 0.10579, val loss: 0.10863\n",
      "Main effects training epoch: 128, train loss: 0.10583, val loss: 0.10933\n",
      "Main effects training epoch: 129, train loss: 0.10580, val loss: 0.10919\n",
      "Main effects training epoch: 130, train loss: 0.10573, val loss: 0.10874\n",
      "Main effects training epoch: 131, train loss: 0.10601, val loss: 0.10951\n",
      "Main effects training epoch: 132, train loss: 0.10600, val loss: 0.10905\n",
      "Main effects training epoch: 133, train loss: 0.10601, val loss: 0.10931\n",
      "Main effects training epoch: 134, train loss: 0.10566, val loss: 0.10882\n",
      "Main effects training epoch: 135, train loss: 0.10590, val loss: 0.10909\n",
      "Main effects training epoch: 136, train loss: 0.10621, val loss: 0.10930\n",
      "Main effects training epoch: 137, train loss: 0.10597, val loss: 0.10906\n",
      "Main effects training epoch: 138, train loss: 0.10583, val loss: 0.10895\n",
      "Main effects training epoch: 139, train loss: 0.10580, val loss: 0.10894\n",
      "Main effects training epoch: 140, train loss: 0.10591, val loss: 0.10891\n",
      "Main effects training epoch: 141, train loss: 0.10595, val loss: 0.10903\n",
      "Main effects training epoch: 142, train loss: 0.10601, val loss: 0.10932\n",
      "Main effects training epoch: 143, train loss: 0.10596, val loss: 0.10892\n",
      "Main effects training epoch: 144, train loss: 0.10583, val loss: 0.10925\n",
      "Main effects training epoch: 145, train loss: 0.10572, val loss: 0.10887\n",
      "Main effects training epoch: 146, train loss: 0.10574, val loss: 0.10897\n",
      "Main effects training epoch: 147, train loss: 0.10575, val loss: 0.10903\n",
      "Main effects training epoch: 148, train loss: 0.10589, val loss: 0.10894\n",
      "Main effects training epoch: 149, train loss: 0.10577, val loss: 0.10889\n",
      "Main effects training epoch: 150, train loss: 0.10569, val loss: 0.10895\n",
      "Main effects training epoch: 151, train loss: 0.10575, val loss: 0.10884\n",
      "Main effects training epoch: 152, train loss: 0.10606, val loss: 0.10908\n",
      "Main effects training epoch: 153, train loss: 0.10639, val loss: 0.10965\n",
      "Main effects training epoch: 154, train loss: 0.10596, val loss: 0.10920\n",
      "Main effects training epoch: 155, train loss: 0.10578, val loss: 0.10908\n",
      "Main effects training epoch: 156, train loss: 0.10569, val loss: 0.10881\n",
      "Main effects training epoch: 157, train loss: 0.10570, val loss: 0.10898\n",
      "Main effects training epoch: 158, train loss: 0.10595, val loss: 0.10931\n",
      "Main effects training epoch: 159, train loss: 0.10577, val loss: 0.10920\n",
      "Main effects training epoch: 160, train loss: 0.10578, val loss: 0.10935\n",
      "Main effects training epoch: 161, train loss: 0.10579, val loss: 0.10872\n",
      "Main effects training epoch: 162, train loss: 0.10575, val loss: 0.10904\n",
      "Main effects training epoch: 163, train loss: 0.10597, val loss: 0.10913\n",
      "Main effects training epoch: 164, train loss: 0.10605, val loss: 0.10923\n",
      "Main effects training epoch: 165, train loss: 0.10604, val loss: 0.10954\n",
      "Main effects training epoch: 166, train loss: 0.10575, val loss: 0.10883\n",
      "Main effects training epoch: 167, train loss: 0.10578, val loss: 0.10918\n",
      "Main effects training epoch: 168, train loss: 0.10576, val loss: 0.10888\n",
      "Early stop at epoch 168, with validation loss: 0.10888\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.10587, val loss: 0.10915\n",
      "Main effects tuning epoch: 2, train loss: 0.10583, val loss: 0.10863\n",
      "Main effects tuning epoch: 3, train loss: 0.10593, val loss: 0.10918\n",
      "Main effects tuning epoch: 4, train loss: 0.10580, val loss: 0.10863\n",
      "Main effects tuning epoch: 5, train loss: 0.10595, val loss: 0.10896\n",
      "Main effects tuning epoch: 6, train loss: 0.10614, val loss: 0.10907\n",
      "Main effects tuning epoch: 7, train loss: 0.10617, val loss: 0.10891\n",
      "Main effects tuning epoch: 8, train loss: 0.10639, val loss: 0.10952\n",
      "Main effects tuning epoch: 9, train loss: 0.10631, val loss: 0.10952\n",
      "Main effects tuning epoch: 10, train loss: 0.10598, val loss: 0.10913\n",
      "Main effects tuning epoch: 11, train loss: 0.10616, val loss: 0.10872\n",
      "Main effects tuning epoch: 12, train loss: 0.10613, val loss: 0.10940\n",
      "Main effects tuning epoch: 13, train loss: 0.10649, val loss: 0.10913\n",
      "Main effects tuning epoch: 14, train loss: 0.10595, val loss: 0.10906\n",
      "Main effects tuning epoch: 15, train loss: 0.10576, val loss: 0.10901\n",
      "Main effects tuning epoch: 16, train loss: 0.10591, val loss: 0.10867\n",
      "Main effects tuning epoch: 17, train loss: 0.10614, val loss: 0.10929\n",
      "Main effects tuning epoch: 18, train loss: 0.10616, val loss: 0.10914\n",
      "Main effects tuning epoch: 19, train loss: 0.10591, val loss: 0.10901\n",
      "Main effects tuning epoch: 20, train loss: 0.10585, val loss: 0.10893\n",
      "Main effects tuning epoch: 21, train loss: 0.10582, val loss: 0.10851\n",
      "Main effects tuning epoch: 22, train loss: 0.10595, val loss: 0.10917\n",
      "Main effects tuning epoch: 23, train loss: 0.10589, val loss: 0.10893\n",
      "Main effects tuning epoch: 24, train loss: 0.10579, val loss: 0.10899\n",
      "Main effects tuning epoch: 25, train loss: 0.10580, val loss: 0.10907\n",
      "Main effects tuning epoch: 26, train loss: 0.10612, val loss: 0.10929\n",
      "Main effects tuning epoch: 27, train loss: 0.10613, val loss: 0.10892\n",
      "Main effects tuning epoch: 28, train loss: 0.10608, val loss: 0.10898\n",
      "Main effects tuning epoch: 29, train loss: 0.10588, val loss: 0.10904\n",
      "Main effects tuning epoch: 30, train loss: 0.10603, val loss: 0.10875\n",
      "Main effects tuning epoch: 31, train loss: 0.10580, val loss: 0.10885\n",
      "Main effects tuning epoch: 32, train loss: 0.10586, val loss: 0.10892\n",
      "Main effects tuning epoch: 33, train loss: 0.10584, val loss: 0.10877\n",
      "Main effects tuning epoch: 34, train loss: 0.10610, val loss: 0.10890\n",
      "Main effects tuning epoch: 35, train loss: 0.10602, val loss: 0.10950\n",
      "Main effects tuning epoch: 36, train loss: 0.10577, val loss: 0.10870\n",
      "Main effects tuning epoch: 37, train loss: 0.10576, val loss: 0.10890\n",
      "Main effects tuning epoch: 38, train loss: 0.10590, val loss: 0.10894\n",
      "Main effects tuning epoch: 39, train loss: 0.10597, val loss: 0.10915\n",
      "Main effects tuning epoch: 40, train loss: 0.10575, val loss: 0.10856\n",
      "Main effects tuning epoch: 41, train loss: 0.10577, val loss: 0.10899\n",
      "Main effects tuning epoch: 42, train loss: 0.10600, val loss: 0.10888\n",
      "Main effects tuning epoch: 43, train loss: 0.10624, val loss: 0.10938\n",
      "Main effects tuning epoch: 44, train loss: 0.10606, val loss: 0.10863\n",
      "Main effects tuning epoch: 45, train loss: 0.10614, val loss: 0.10961\n",
      "Main effects tuning epoch: 46, train loss: 0.10598, val loss: 0.10874\n",
      "Main effects tuning epoch: 47, train loss: 0.10623, val loss: 0.10916\n",
      "Main effects tuning epoch: 48, train loss: 0.10628, val loss: 0.10910\n",
      "Main effects tuning epoch: 49, train loss: 0.10612, val loss: 0.10902\n",
      "Main effects tuning epoch: 50, train loss: 0.10600, val loss: 0.10922\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.16656, val loss: 0.16504\n",
      "Interaction training epoch: 2, train loss: 0.19799, val loss: 0.19730\n",
      "Interaction training epoch: 3, train loss: 0.07919, val loss: 0.08135\n",
      "Interaction training epoch: 4, train loss: 0.06592, val loss: 0.06695\n",
      "Interaction training epoch: 5, train loss: 0.06719, val loss: 0.06623\n",
      "Interaction training epoch: 6, train loss: 0.07116, val loss: 0.06992\n",
      "Interaction training epoch: 7, train loss: 0.05765, val loss: 0.05869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 8, train loss: 0.05791, val loss: 0.05822\n",
      "Interaction training epoch: 9, train loss: 0.06091, val loss: 0.06046\n",
      "Interaction training epoch: 10, train loss: 0.05754, val loss: 0.05788\n",
      "Interaction training epoch: 11, train loss: 0.05624, val loss: 0.05624\n",
      "Interaction training epoch: 12, train loss: 0.05651, val loss: 0.05600\n",
      "Interaction training epoch: 13, train loss: 0.06101, val loss: 0.06080\n",
      "Interaction training epoch: 14, train loss: 0.05709, val loss: 0.05641\n",
      "Interaction training epoch: 15, train loss: 0.06077, val loss: 0.06101\n",
      "Interaction training epoch: 16, train loss: 0.05239, val loss: 0.05197\n",
      "Interaction training epoch: 17, train loss: 0.05618, val loss: 0.05592\n",
      "Interaction training epoch: 18, train loss: 0.05477, val loss: 0.05468\n",
      "Interaction training epoch: 19, train loss: 0.05152, val loss: 0.05113\n",
      "Interaction training epoch: 20, train loss: 0.05519, val loss: 0.05482\n",
      "Interaction training epoch: 21, train loss: 0.05818, val loss: 0.05633\n",
      "Interaction training epoch: 22, train loss: 0.05614, val loss: 0.05550\n",
      "Interaction training epoch: 23, train loss: 0.05384, val loss: 0.05252\n",
      "Interaction training epoch: 24, train loss: 0.05143, val loss: 0.05136\n",
      "Interaction training epoch: 25, train loss: 0.05185, val loss: 0.05118\n",
      "Interaction training epoch: 26, train loss: 0.05409, val loss: 0.05515\n",
      "Interaction training epoch: 27, train loss: 0.05171, val loss: 0.05189\n",
      "Interaction training epoch: 28, train loss: 0.05284, val loss: 0.05307\n",
      "Interaction training epoch: 29, train loss: 0.05251, val loss: 0.05306\n",
      "Interaction training epoch: 30, train loss: 0.05654, val loss: 0.05667\n",
      "Interaction training epoch: 31, train loss: 0.05008, val loss: 0.05013\n",
      "Interaction training epoch: 32, train loss: 0.05165, val loss: 0.05175\n",
      "Interaction training epoch: 33, train loss: 0.05094, val loss: 0.05147\n",
      "Interaction training epoch: 34, train loss: 0.05154, val loss: 0.05255\n",
      "Interaction training epoch: 35, train loss: 0.05178, val loss: 0.05163\n",
      "Interaction training epoch: 36, train loss: 0.05189, val loss: 0.05137\n",
      "Interaction training epoch: 37, train loss: 0.05261, val loss: 0.05262\n",
      "Interaction training epoch: 38, train loss: 0.05395, val loss: 0.05444\n",
      "Interaction training epoch: 39, train loss: 0.05053, val loss: 0.05093\n",
      "Interaction training epoch: 40, train loss: 0.05186, val loss: 0.05165\n",
      "Interaction training epoch: 41, train loss: 0.05174, val loss: 0.05183\n",
      "Interaction training epoch: 42, train loss: 0.05645, val loss: 0.05707\n",
      "Interaction training epoch: 43, train loss: 0.05130, val loss: 0.05143\n",
      "Interaction training epoch: 44, train loss: 0.05233, val loss: 0.05213\n",
      "Interaction training epoch: 45, train loss: 0.05242, val loss: 0.05266\n",
      "Interaction training epoch: 46, train loss: 0.05234, val loss: 0.05256\n",
      "Interaction training epoch: 47, train loss: 0.05096, val loss: 0.05121\n",
      "Interaction training epoch: 48, train loss: 0.05324, val loss: 0.05317\n",
      "Interaction training epoch: 49, train loss: 0.05143, val loss: 0.05175\n",
      "Interaction training epoch: 50, train loss: 0.05906, val loss: 0.05866\n",
      "Interaction training epoch: 51, train loss: 0.05278, val loss: 0.05220\n",
      "Interaction training epoch: 52, train loss: 0.05429, val loss: 0.05321\n",
      "Interaction training epoch: 53, train loss: 0.05247, val loss: 0.05225\n",
      "Interaction training epoch: 54, train loss: 0.05061, val loss: 0.05129\n",
      "Interaction training epoch: 55, train loss: 0.05239, val loss: 0.05237\n",
      "Interaction training epoch: 56, train loss: 0.05174, val loss: 0.05287\n",
      "Interaction training epoch: 57, train loss: 0.05160, val loss: 0.05111\n",
      "Interaction training epoch: 58, train loss: 0.05180, val loss: 0.05292\n",
      "Interaction training epoch: 59, train loss: 0.04982, val loss: 0.05009\n",
      "Interaction training epoch: 60, train loss: 0.05227, val loss: 0.05368\n",
      "Interaction training epoch: 61, train loss: 0.05217, val loss: 0.05210\n",
      "Interaction training epoch: 62, train loss: 0.05023, val loss: 0.05074\n",
      "Interaction training epoch: 63, train loss: 0.05179, val loss: 0.05284\n",
      "Interaction training epoch: 64, train loss: 0.04959, val loss: 0.05129\n",
      "Interaction training epoch: 65, train loss: 0.05320, val loss: 0.05225\n",
      "Interaction training epoch: 66, train loss: 0.05295, val loss: 0.05326\n",
      "Interaction training epoch: 67, train loss: 0.05028, val loss: 0.05144\n",
      "Interaction training epoch: 68, train loss: 0.05112, val loss: 0.05151\n",
      "Interaction training epoch: 69, train loss: 0.05228, val loss: 0.05248\n",
      "Interaction training epoch: 70, train loss: 0.04994, val loss: 0.05062\n",
      "Interaction training epoch: 71, train loss: 0.05225, val loss: 0.05286\n",
      "Interaction training epoch: 72, train loss: 0.05069, val loss: 0.05180\n",
      "Interaction training epoch: 73, train loss: 0.05077, val loss: 0.05196\n",
      "Interaction training epoch: 74, train loss: 0.04967, val loss: 0.05076\n",
      "Interaction training epoch: 75, train loss: 0.05064, val loss: 0.05071\n",
      "Interaction training epoch: 76, train loss: 0.04886, val loss: 0.05004\n",
      "Interaction training epoch: 77, train loss: 0.05097, val loss: 0.05075\n",
      "Interaction training epoch: 78, train loss: 0.05171, val loss: 0.05128\n",
      "Interaction training epoch: 79, train loss: 0.05178, val loss: 0.05168\n",
      "Interaction training epoch: 80, train loss: 0.04972, val loss: 0.05111\n",
      "Interaction training epoch: 81, train loss: 0.05307, val loss: 0.05407\n",
      "Interaction training epoch: 82, train loss: 0.05287, val loss: 0.05331\n",
      "Interaction training epoch: 83, train loss: 0.05019, val loss: 0.05065\n",
      "Interaction training epoch: 84, train loss: 0.05109, val loss: 0.05177\n",
      "Interaction training epoch: 85, train loss: 0.05241, val loss: 0.05395\n",
      "Interaction training epoch: 86, train loss: 0.05169, val loss: 0.05187\n",
      "Interaction training epoch: 87, train loss: 0.05013, val loss: 0.05031\n",
      "Interaction training epoch: 88, train loss: 0.05249, val loss: 0.05324\n",
      "Interaction training epoch: 89, train loss: 0.04905, val loss: 0.04955\n",
      "Interaction training epoch: 90, train loss: 0.05048, val loss: 0.05104\n",
      "Interaction training epoch: 91, train loss: 0.05291, val loss: 0.05320\n",
      "Interaction training epoch: 92, train loss: 0.05194, val loss: 0.05184\n",
      "Interaction training epoch: 93, train loss: 0.04813, val loss: 0.04857\n",
      "Interaction training epoch: 94, train loss: 0.05434, val loss: 0.05466\n",
      "Interaction training epoch: 95, train loss: 0.05394, val loss: 0.05306\n",
      "Interaction training epoch: 96, train loss: 0.04954, val loss: 0.04995\n",
      "Interaction training epoch: 97, train loss: 0.05003, val loss: 0.05058\n",
      "Interaction training epoch: 98, train loss: 0.05197, val loss: 0.05219\n",
      "Interaction training epoch: 99, train loss: 0.05436, val loss: 0.05280\n",
      "Interaction training epoch: 100, train loss: 0.05350, val loss: 0.05437\n",
      "Interaction training epoch: 101, train loss: 0.04964, val loss: 0.04980\n",
      "Interaction training epoch: 102, train loss: 0.05136, val loss: 0.05087\n",
      "Interaction training epoch: 103, train loss: 0.05162, val loss: 0.05191\n",
      "Interaction training epoch: 104, train loss: 0.05118, val loss: 0.05150\n",
      "Interaction training epoch: 105, train loss: 0.04993, val loss: 0.04937\n",
      "Interaction training epoch: 106, train loss: 0.05171, val loss: 0.05164\n",
      "Interaction training epoch: 107, train loss: 0.05193, val loss: 0.05075\n",
      "Interaction training epoch: 108, train loss: 0.05102, val loss: 0.05183\n",
      "Interaction training epoch: 109, train loss: 0.05261, val loss: 0.05324\n",
      "Interaction training epoch: 110, train loss: 0.05047, val loss: 0.05100\n",
      "Interaction training epoch: 111, train loss: 0.05046, val loss: 0.05063\n",
      "Interaction training epoch: 112, train loss: 0.04811, val loss: 0.04809\n",
      "Interaction training epoch: 113, train loss: 0.05250, val loss: 0.05298\n",
      "Interaction training epoch: 114, train loss: 0.05036, val loss: 0.05112\n",
      "Interaction training epoch: 115, train loss: 0.05119, val loss: 0.05140\n",
      "Interaction training epoch: 116, train loss: 0.04975, val loss: 0.05112\n",
      "Interaction training epoch: 117, train loss: 0.04911, val loss: 0.04994\n",
      "Interaction training epoch: 118, train loss: 0.04990, val loss: 0.05012\n",
      "Interaction training epoch: 119, train loss: 0.04854, val loss: 0.04775\n",
      "Interaction training epoch: 120, train loss: 0.04985, val loss: 0.04987\n",
      "Interaction training epoch: 121, train loss: 0.04779, val loss: 0.04856\n",
      "Interaction training epoch: 122, train loss: 0.04712, val loss: 0.04773\n",
      "Interaction training epoch: 123, train loss: 0.05164, val loss: 0.05255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 124, train loss: 0.05327, val loss: 0.05365\n",
      "Interaction training epoch: 125, train loss: 0.05348, val loss: 0.05414\n",
      "Interaction training epoch: 126, train loss: 0.04873, val loss: 0.04978\n",
      "Interaction training epoch: 127, train loss: 0.04866, val loss: 0.04894\n",
      "Interaction training epoch: 128, train loss: 0.04947, val loss: 0.05037\n",
      "Interaction training epoch: 129, train loss: 0.04701, val loss: 0.04711\n",
      "Interaction training epoch: 130, train loss: 0.04867, val loss: 0.04878\n",
      "Interaction training epoch: 131, train loss: 0.04995, val loss: 0.05004\n",
      "Interaction training epoch: 132, train loss: 0.05004, val loss: 0.05008\n",
      "Interaction training epoch: 133, train loss: 0.04970, val loss: 0.05023\n",
      "Interaction training epoch: 134, train loss: 0.04969, val loss: 0.05041\n",
      "Interaction training epoch: 135, train loss: 0.04826, val loss: 0.04935\n",
      "Interaction training epoch: 136, train loss: 0.04956, val loss: 0.05077\n",
      "Interaction training epoch: 137, train loss: 0.04937, val loss: 0.05116\n",
      "Interaction training epoch: 138, train loss: 0.05042, val loss: 0.05126\n",
      "Interaction training epoch: 139, train loss: 0.04910, val loss: 0.04944\n",
      "Interaction training epoch: 140, train loss: 0.05235, val loss: 0.05329\n",
      "Interaction training epoch: 141, train loss: 0.05151, val loss: 0.05199\n",
      "Interaction training epoch: 142, train loss: 0.04935, val loss: 0.04965\n",
      "Interaction training epoch: 143, train loss: 0.05663, val loss: 0.05667\n",
      "Interaction training epoch: 144, train loss: 0.05217, val loss: 0.05297\n",
      "Interaction training epoch: 145, train loss: 0.05139, val loss: 0.05143\n",
      "Interaction training epoch: 146, train loss: 0.05350, val loss: 0.05388\n",
      "Interaction training epoch: 147, train loss: 0.06195, val loss: 0.06171\n",
      "Interaction training epoch: 148, train loss: 0.05923, val loss: 0.05958\n",
      "Interaction training epoch: 149, train loss: 0.05897, val loss: 0.05842\n",
      "Interaction training epoch: 150, train loss: 0.05688, val loss: 0.05663\n",
      "Interaction training epoch: 151, train loss: 0.05292, val loss: 0.05339\n",
      "Interaction training epoch: 152, train loss: 0.04638, val loss: 0.04708\n",
      "Interaction training epoch: 153, train loss: 0.04830, val loss: 0.05070\n",
      "Interaction training epoch: 154, train loss: 0.04798, val loss: 0.04952\n",
      "Interaction training epoch: 155, train loss: 0.04659, val loss: 0.04786\n",
      "Interaction training epoch: 156, train loss: 0.04767, val loss: 0.04876\n",
      "Interaction training epoch: 157, train loss: 0.04853, val loss: 0.04917\n",
      "Interaction training epoch: 158, train loss: 0.04677, val loss: 0.04803\n",
      "Interaction training epoch: 159, train loss: 0.04901, val loss: 0.04931\n",
      "Interaction training epoch: 160, train loss: 0.04754, val loss: 0.04893\n",
      "Interaction training epoch: 161, train loss: 0.04874, val loss: 0.04950\n",
      "Interaction training epoch: 162, train loss: 0.04595, val loss: 0.04721\n",
      "Interaction training epoch: 163, train loss: 0.04732, val loss: 0.04956\n",
      "Interaction training epoch: 164, train loss: 0.04567, val loss: 0.04749\n",
      "Interaction training epoch: 165, train loss: 0.05013, val loss: 0.05115\n",
      "Interaction training epoch: 166, train loss: 0.04663, val loss: 0.04790\n",
      "Interaction training epoch: 167, train loss: 0.04602, val loss: 0.04784\n",
      "Interaction training epoch: 168, train loss: 0.04789, val loss: 0.04800\n",
      "Interaction training epoch: 169, train loss: 0.04480, val loss: 0.04615\n",
      "Interaction training epoch: 170, train loss: 0.04657, val loss: 0.04712\n",
      "Interaction training epoch: 171, train loss: 0.04515, val loss: 0.04688\n",
      "Interaction training epoch: 172, train loss: 0.04745, val loss: 0.04904\n",
      "Interaction training epoch: 173, train loss: 0.04801, val loss: 0.05050\n",
      "Interaction training epoch: 174, train loss: 0.05297, val loss: 0.05389\n",
      "Interaction training epoch: 175, train loss: 0.04522, val loss: 0.04687\n",
      "Interaction training epoch: 176, train loss: 0.04654, val loss: 0.04709\n",
      "Interaction training epoch: 177, train loss: 0.04462, val loss: 0.04576\n",
      "Interaction training epoch: 178, train loss: 0.04715, val loss: 0.04880\n",
      "Interaction training epoch: 179, train loss: 0.04515, val loss: 0.04673\n",
      "Interaction training epoch: 180, train loss: 0.04652, val loss: 0.04824\n",
      "Interaction training epoch: 181, train loss: 0.04757, val loss: 0.04936\n",
      "Interaction training epoch: 182, train loss: 0.04771, val loss: 0.04909\n",
      "Interaction training epoch: 183, train loss: 0.04999, val loss: 0.05186\n",
      "Interaction training epoch: 184, train loss: 0.04691, val loss: 0.04877\n",
      "Interaction training epoch: 185, train loss: 0.04599, val loss: 0.04763\n",
      "Interaction training epoch: 186, train loss: 0.04839, val loss: 0.04969\n",
      "Interaction training epoch: 187, train loss: 0.04634, val loss: 0.04846\n",
      "Interaction training epoch: 188, train loss: 0.04686, val loss: 0.04828\n",
      "Interaction training epoch: 189, train loss: 0.04527, val loss: 0.04603\n",
      "Interaction training epoch: 190, train loss: 0.04811, val loss: 0.05046\n",
      "Interaction training epoch: 191, train loss: 0.04668, val loss: 0.04794\n",
      "Interaction training epoch: 192, train loss: 0.04580, val loss: 0.04673\n",
      "Interaction training epoch: 193, train loss: 0.04953, val loss: 0.05077\n",
      "Interaction training epoch: 194, train loss: 0.04637, val loss: 0.04745\n",
      "Interaction training epoch: 195, train loss: 0.05291, val loss: 0.05606\n",
      "Interaction training epoch: 196, train loss: 0.04552, val loss: 0.04706\n",
      "Interaction training epoch: 197, train loss: 0.04954, val loss: 0.05175\n",
      "Interaction training epoch: 198, train loss: 0.04504, val loss: 0.04659\n",
      "Interaction training epoch: 199, train loss: 0.04939, val loss: 0.05068\n",
      "Interaction training epoch: 200, train loss: 0.04824, val loss: 0.04969\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########7 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.04624, val loss: 0.04648\n",
      "Interaction tuning epoch: 2, train loss: 0.04654, val loss: 0.04675\n",
      "Interaction tuning epoch: 3, train loss: 0.04688, val loss: 0.04770\n",
      "Interaction tuning epoch: 4, train loss: 0.04703, val loss: 0.04714\n",
      "Interaction tuning epoch: 5, train loss: 0.04871, val loss: 0.04870\n",
      "Interaction tuning epoch: 6, train loss: 0.04715, val loss: 0.04685\n",
      "Interaction tuning epoch: 7, train loss: 0.04753, val loss: 0.04769\n",
      "Interaction tuning epoch: 8, train loss: 0.04804, val loss: 0.04839\n",
      "Interaction tuning epoch: 9, train loss: 0.04656, val loss: 0.04746\n",
      "Interaction tuning epoch: 10, train loss: 0.04708, val loss: 0.04792\n",
      "Interaction tuning epoch: 11, train loss: 0.04593, val loss: 0.04607\n",
      "Interaction tuning epoch: 12, train loss: 0.04889, val loss: 0.05019\n",
      "Interaction tuning epoch: 13, train loss: 0.04756, val loss: 0.04795\n",
      "Interaction tuning epoch: 14, train loss: 0.05056, val loss: 0.04891\n",
      "Interaction tuning epoch: 15, train loss: 0.05649, val loss: 0.05700\n",
      "Interaction tuning epoch: 16, train loss: 0.04770, val loss: 0.04777\n",
      "Interaction tuning epoch: 17, train loss: 0.04665, val loss: 0.04734\n",
      "Interaction tuning epoch: 18, train loss: 0.04709, val loss: 0.04763\n",
      "Interaction tuning epoch: 19, train loss: 0.04762, val loss: 0.04775\n",
      "Interaction tuning epoch: 20, train loss: 0.04647, val loss: 0.04691\n",
      "Interaction tuning epoch: 21, train loss: 0.04858, val loss: 0.04904\n",
      "Interaction tuning epoch: 22, train loss: 0.04763, val loss: 0.04743\n",
      "Interaction tuning epoch: 23, train loss: 0.04883, val loss: 0.04879\n",
      "Interaction tuning epoch: 24, train loss: 0.04608, val loss: 0.04728\n",
      "Interaction tuning epoch: 25, train loss: 0.04999, val loss: 0.04940\n",
      "Interaction tuning epoch: 26, train loss: 0.04916, val loss: 0.04857\n",
      "Interaction tuning epoch: 27, train loss: 0.04784, val loss: 0.04855\n",
      "Interaction tuning epoch: 28, train loss: 0.04702, val loss: 0.04758\n",
      "Interaction tuning epoch: 29, train loss: 0.04685, val loss: 0.04720\n",
      "Interaction tuning epoch: 30, train loss: 0.04688, val loss: 0.04800\n",
      "Interaction tuning epoch: 31, train loss: 0.04656, val loss: 0.04725\n",
      "Interaction tuning epoch: 32, train loss: 0.04698, val loss: 0.04718\n",
      "Interaction tuning epoch: 33, train loss: 0.04648, val loss: 0.04674\n",
      "Interaction tuning epoch: 34, train loss: 0.04622, val loss: 0.04627\n",
      "Interaction tuning epoch: 35, train loss: 0.04695, val loss: 0.04653\n",
      "Interaction tuning epoch: 36, train loss: 0.04638, val loss: 0.04708\n",
      "Interaction tuning epoch: 37, train loss: 0.04517, val loss: 0.04599\n",
      "Interaction tuning epoch: 38, train loss: 0.04594, val loss: 0.04626\n",
      "Interaction tuning epoch: 39, train loss: 0.04806, val loss: 0.04884\n",
      "Interaction tuning epoch: 40, train loss: 0.04762, val loss: 0.04781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 41, train loss: 0.04762, val loss: 0.04909\n",
      "Interaction tuning epoch: 42, train loss: 0.04746, val loss: 0.04805\n",
      "Interaction tuning epoch: 43, train loss: 0.04604, val loss: 0.04612\n",
      "Interaction tuning epoch: 44, train loss: 0.04636, val loss: 0.04673\n",
      "Interaction tuning epoch: 45, train loss: 0.04643, val loss: 0.04712\n",
      "Interaction tuning epoch: 46, train loss: 0.04656, val loss: 0.04678\n",
      "Interaction tuning epoch: 47, train loss: 0.04635, val loss: 0.04667\n",
      "Interaction tuning epoch: 48, train loss: 0.04816, val loss: 0.04882\n",
      "Interaction tuning epoch: 49, train loss: 0.04683, val loss: 0.04711\n",
      "Interaction tuning epoch: 50, train loss: 0.04845, val loss: 0.04905\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 34.39619565010071\n",
      "After the gam stage, training error is 0.04845 , validation error is 0.04905\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 1.232198\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.041245 validation MAE=0.047137,rank=4\n",
      "[SoftImpute] Iter 2: observed MAE=0.038593 validation MAE=0.046110,rank=4\n",
      "[SoftImpute] Iter 3: observed MAE=0.036322 validation MAE=0.045187,rank=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 4: observed MAE=0.034371 validation MAE=0.044397,rank=4\n",
      "[SoftImpute] Iter 5: observed MAE=0.032681 validation MAE=0.043685,rank=4\n",
      "[SoftImpute] Iter 6: observed MAE=0.031210 validation MAE=0.043036,rank=4\n",
      "[SoftImpute] Iter 7: observed MAE=0.029931 validation MAE=0.042444,rank=4\n",
      "[SoftImpute] Iter 8: observed MAE=0.028807 validation MAE=0.041898,rank=4\n",
      "[SoftImpute] Iter 9: observed MAE=0.027806 validation MAE=0.041396,rank=4\n",
      "[SoftImpute] Iter 10: observed MAE=0.026911 validation MAE=0.040925,rank=4\n",
      "[SoftImpute] Iter 11: observed MAE=0.026106 validation MAE=0.040493,rank=4\n",
      "[SoftImpute] Iter 12: observed MAE=0.025381 validation MAE=0.040087,rank=4\n",
      "[SoftImpute] Iter 13: observed MAE=0.024724 validation MAE=0.039705,rank=4\n",
      "[SoftImpute] Iter 14: observed MAE=0.024127 validation MAE=0.039347,rank=4\n",
      "[SoftImpute] Iter 15: observed MAE=0.023581 validation MAE=0.039010,rank=4\n",
      "[SoftImpute] Iter 16: observed MAE=0.023078 validation MAE=0.038693,rank=4\n",
      "[SoftImpute] Iter 17: observed MAE=0.022613 validation MAE=0.038394,rank=4\n",
      "[SoftImpute] Iter 18: observed MAE=0.022187 validation MAE=0.038110,rank=4\n",
      "[SoftImpute] Iter 19: observed MAE=0.021790 validation MAE=0.037845,rank=4\n",
      "[SoftImpute] Iter 20: observed MAE=0.021416 validation MAE=0.037598,rank=4\n",
      "[SoftImpute] Iter 21: observed MAE=0.021068 validation MAE=0.037363,rank=4\n",
      "[SoftImpute] Iter 22: observed MAE=0.020740 validation MAE=0.037139,rank=4\n",
      "[SoftImpute] Iter 23: observed MAE=0.020432 validation MAE=0.036921,rank=4\n",
      "[SoftImpute] Iter 24: observed MAE=0.020143 validation MAE=0.036712,rank=4\n",
      "[SoftImpute] Iter 25: observed MAE=0.019870 validation MAE=0.036514,rank=4\n",
      "[SoftImpute] Iter 26: observed MAE=0.019609 validation MAE=0.036326,rank=4\n",
      "[SoftImpute] Iter 27: observed MAE=0.019360 validation MAE=0.036147,rank=4\n",
      "[SoftImpute] Iter 28: observed MAE=0.019123 validation MAE=0.035979,rank=4\n",
      "[SoftImpute] Iter 29: observed MAE=0.018899 validation MAE=0.035820,rank=4\n",
      "[SoftImpute] Iter 30: observed MAE=0.018685 validation MAE=0.035666,rank=4\n",
      "[SoftImpute] Iter 31: observed MAE=0.018479 validation MAE=0.035518,rank=4\n",
      "[SoftImpute] Iter 32: observed MAE=0.018283 validation MAE=0.035373,rank=4\n",
      "[SoftImpute] Iter 33: observed MAE=0.018096 validation MAE=0.035235,rank=4\n",
      "[SoftImpute] Iter 34: observed MAE=0.017915 validation MAE=0.035105,rank=4\n",
      "[SoftImpute] Iter 35: observed MAE=0.017742 validation MAE=0.034984,rank=4\n",
      "[SoftImpute] Iter 36: observed MAE=0.017576 validation MAE=0.034870,rank=4\n",
      "[SoftImpute] Iter 37: observed MAE=0.017416 validation MAE=0.034759,rank=4\n",
      "[SoftImpute] Iter 38: observed MAE=0.017262 validation MAE=0.034651,rank=4\n",
      "[SoftImpute] Iter 39: observed MAE=0.017115 validation MAE=0.034547,rank=4\n",
      "[SoftImpute] Iter 40: observed MAE=0.016973 validation MAE=0.034448,rank=4\n",
      "[SoftImpute] Iter 41: observed MAE=0.016837 validation MAE=0.034352,rank=4\n",
      "[SoftImpute] Iter 42: observed MAE=0.016706 validation MAE=0.034260,rank=4\n",
      "[SoftImpute] Iter 43: observed MAE=0.016579 validation MAE=0.034170,rank=4\n",
      "[SoftImpute] Iter 44: observed MAE=0.016457 validation MAE=0.034082,rank=4\n",
      "[SoftImpute] Iter 45: observed MAE=0.016340 validation MAE=0.033995,rank=4\n",
      "[SoftImpute] Iter 46: observed MAE=0.016228 validation MAE=0.033911,rank=4\n",
      "[SoftImpute] Iter 47: observed MAE=0.016118 validation MAE=0.033829,rank=4\n",
      "[SoftImpute] Iter 48: observed MAE=0.016012 validation MAE=0.033750,rank=4\n",
      "[SoftImpute] Iter 49: observed MAE=0.015909 validation MAE=0.033672,rank=4\n",
      "[SoftImpute] Iter 50: observed MAE=0.015810 validation MAE=0.033595,rank=4\n",
      "[SoftImpute] Iter 51: observed MAE=0.015715 validation MAE=0.033520,rank=4\n",
      "[SoftImpute] Iter 52: observed MAE=0.015622 validation MAE=0.033446,rank=4\n",
      "[SoftImpute] Iter 53: observed MAE=0.015532 validation MAE=0.033374,rank=4\n",
      "[SoftImpute] Iter 54: observed MAE=0.015445 validation MAE=0.033304,rank=4\n",
      "[SoftImpute] Iter 55: observed MAE=0.015359 validation MAE=0.033237,rank=4\n",
      "[SoftImpute] Iter 56: observed MAE=0.015277 validation MAE=0.033174,rank=4\n",
      "[SoftImpute] Iter 57: observed MAE=0.015197 validation MAE=0.033114,rank=4\n",
      "[SoftImpute] Iter 58: observed MAE=0.015119 validation MAE=0.033057,rank=4\n",
      "[SoftImpute] Iter 59: observed MAE=0.015043 validation MAE=0.033002,rank=4\n",
      "[SoftImpute] Iter 60: observed MAE=0.014969 validation MAE=0.032946,rank=4\n",
      "[SoftImpute] Iter 61: observed MAE=0.014897 validation MAE=0.032892,rank=4\n",
      "[SoftImpute] Iter 62: observed MAE=0.014825 validation MAE=0.032839,rank=4\n",
      "[SoftImpute] Iter 63: observed MAE=0.014756 validation MAE=0.032789,rank=4\n",
      "[SoftImpute] Iter 64: observed MAE=0.014689 validation MAE=0.032740,rank=4\n",
      "[SoftImpute] Iter 65: observed MAE=0.014623 validation MAE=0.032693,rank=4\n",
      "[SoftImpute] Iter 66: observed MAE=0.014559 validation MAE=0.032646,rank=4\n",
      "[SoftImpute] Iter 67: observed MAE=0.014497 validation MAE=0.032600,rank=4\n",
      "[SoftImpute] Iter 68: observed MAE=0.014436 validation MAE=0.032553,rank=4\n",
      "[SoftImpute] Iter 69: observed MAE=0.014377 validation MAE=0.032507,rank=4\n",
      "[SoftImpute] Iter 70: observed MAE=0.014319 validation MAE=0.032462,rank=4\n",
      "[SoftImpute] Iter 71: observed MAE=0.014262 validation MAE=0.032417,rank=4\n",
      "[SoftImpute] Iter 72: observed MAE=0.014207 validation MAE=0.032373,rank=4\n",
      "[SoftImpute] Iter 73: observed MAE=0.014152 validation MAE=0.032329,rank=4\n",
      "[SoftImpute] Iter 74: observed MAE=0.014099 validation MAE=0.032287,rank=4\n",
      "[SoftImpute] Iter 75: observed MAE=0.014047 validation MAE=0.032245,rank=4\n",
      "[SoftImpute] Iter 76: observed MAE=0.013995 validation MAE=0.032205,rank=4\n",
      "[SoftImpute] Iter 77: observed MAE=0.013944 validation MAE=0.032165,rank=4\n",
      "[SoftImpute] Iter 78: observed MAE=0.013895 validation MAE=0.032126,rank=4\n",
      "[SoftImpute] Iter 79: observed MAE=0.013846 validation MAE=0.032086,rank=4\n",
      "[SoftImpute] Iter 80: observed MAE=0.013799 validation MAE=0.032048,rank=4\n",
      "[SoftImpute] Iter 81: observed MAE=0.013752 validation MAE=0.032009,rank=4\n",
      "[SoftImpute] Iter 82: observed MAE=0.013706 validation MAE=0.031971,rank=4\n",
      "[SoftImpute] Iter 83: observed MAE=0.013661 validation MAE=0.031933,rank=4\n",
      "[SoftImpute] Iter 84: observed MAE=0.013617 validation MAE=0.031895,rank=4\n",
      "[SoftImpute] Iter 85: observed MAE=0.013573 validation MAE=0.031860,rank=4\n",
      "[SoftImpute] Iter 86: observed MAE=0.013531 validation MAE=0.031827,rank=4\n",
      "[SoftImpute] Iter 87: observed MAE=0.013489 validation MAE=0.031794,rank=4\n",
      "[SoftImpute] Iter 88: observed MAE=0.013448 validation MAE=0.031763,rank=4\n",
      "[SoftImpute] Iter 89: observed MAE=0.013407 validation MAE=0.031731,rank=4\n",
      "[SoftImpute] Iter 90: observed MAE=0.013367 validation MAE=0.031700,rank=4\n",
      "[SoftImpute] Iter 91: observed MAE=0.013328 validation MAE=0.031669,rank=4\n",
      "[SoftImpute] Iter 92: observed MAE=0.013289 validation MAE=0.031638,rank=4\n",
      "[SoftImpute] Iter 93: observed MAE=0.013251 validation MAE=0.031608,rank=4\n",
      "[SoftImpute] Iter 94: observed MAE=0.013214 validation MAE=0.031579,rank=4\n",
      "[SoftImpute] Iter 95: observed MAE=0.013177 validation MAE=0.031549,rank=4\n",
      "[SoftImpute] Iter 96: observed MAE=0.013142 validation MAE=0.031519,rank=4\n",
      "[SoftImpute] Iter 97: observed MAE=0.013106 validation MAE=0.031489,rank=4\n",
      "[SoftImpute] Iter 98: observed MAE=0.013072 validation MAE=0.031461,rank=4\n",
      "[SoftImpute] Iter 99: observed MAE=0.013037 validation MAE=0.031432,rank=4\n",
      "[SoftImpute] Iter 100: observed MAE=0.013003 validation MAE=0.031403,rank=4\n",
      "[SoftImpute] Iter 101: observed MAE=0.012970 validation MAE=0.031373,rank=4\n",
      "[SoftImpute] Iter 102: observed MAE=0.012937 validation MAE=0.031344,rank=4\n",
      "[SoftImpute] Iter 103: observed MAE=0.012905 validation MAE=0.031314,rank=4\n",
      "[SoftImpute] Iter 104: observed MAE=0.012873 validation MAE=0.031285,rank=4\n",
      "[SoftImpute] Iter 105: observed MAE=0.012841 validation MAE=0.031257,rank=4\n",
      "[SoftImpute] Iter 106: observed MAE=0.012811 validation MAE=0.031229,rank=4\n",
      "[SoftImpute] Iter 107: observed MAE=0.012780 validation MAE=0.031200,rank=4\n",
      "[SoftImpute] Iter 108: observed MAE=0.012750 validation MAE=0.031172,rank=4\n",
      "[SoftImpute] Iter 109: observed MAE=0.012720 validation MAE=0.031143,rank=4\n",
      "[SoftImpute] Iter 110: observed MAE=0.012691 validation MAE=0.031114,rank=4\n",
      "[SoftImpute] Iter 111: observed MAE=0.012663 validation MAE=0.031085,rank=4\n",
      "[SoftImpute] Iter 112: observed MAE=0.012635 validation MAE=0.031057,rank=4\n",
      "[SoftImpute] Iter 113: observed MAE=0.012607 validation MAE=0.031028,rank=4\n",
      "[SoftImpute] Iter 114: observed MAE=0.012580 validation MAE=0.030999,rank=4\n",
      "[SoftImpute] Iter 115: observed MAE=0.012553 validation MAE=0.030970,rank=4\n",
      "[SoftImpute] Iter 116: observed MAE=0.012526 validation MAE=0.030943,rank=4\n",
      "[SoftImpute] Iter 117: observed MAE=0.012500 validation MAE=0.030916,rank=4\n",
      "[SoftImpute] Iter 118: observed MAE=0.012474 validation MAE=0.030889,rank=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 119: observed MAE=0.012448 validation MAE=0.030862,rank=4\n",
      "[SoftImpute] Iter 120: observed MAE=0.012423 validation MAE=0.030835,rank=4\n",
      "[SoftImpute] Iter 121: observed MAE=0.012398 validation MAE=0.030808,rank=4\n",
      "[SoftImpute] Iter 122: observed MAE=0.012374 validation MAE=0.030781,rank=4\n",
      "[SoftImpute] Iter 123: observed MAE=0.012349 validation MAE=0.030755,rank=4\n",
      "[SoftImpute] Iter 124: observed MAE=0.012326 validation MAE=0.030729,rank=4\n",
      "[SoftImpute] Iter 125: observed MAE=0.012302 validation MAE=0.030703,rank=4\n",
      "[SoftImpute] Iter 126: observed MAE=0.012279 validation MAE=0.030677,rank=4\n",
      "[SoftImpute] Iter 127: observed MAE=0.012256 validation MAE=0.030651,rank=4\n",
      "[SoftImpute] Iter 128: observed MAE=0.012234 validation MAE=0.030626,rank=4\n",
      "[SoftImpute] Iter 129: observed MAE=0.012211 validation MAE=0.030601,rank=4\n",
      "[SoftImpute] Iter 130: observed MAE=0.012189 validation MAE=0.030575,rank=4\n",
      "[SoftImpute] Iter 131: observed MAE=0.012167 validation MAE=0.030550,rank=4\n",
      "[SoftImpute] Iter 132: observed MAE=0.012146 validation MAE=0.030525,rank=4\n",
      "[SoftImpute] Iter 133: observed MAE=0.012124 validation MAE=0.030499,rank=4\n",
      "[SoftImpute] Iter 134: observed MAE=0.012103 validation MAE=0.030475,rank=4\n",
      "[SoftImpute] Iter 135: observed MAE=0.012083 validation MAE=0.030450,rank=4\n",
      "[SoftImpute] Iter 136: observed MAE=0.012062 validation MAE=0.030426,rank=4\n",
      "[SoftImpute] Iter 137: observed MAE=0.012042 validation MAE=0.030402,rank=4\n",
      "[SoftImpute] Iter 138: observed MAE=0.012021 validation MAE=0.030378,rank=4\n",
      "[SoftImpute] Iter 139: observed MAE=0.012002 validation MAE=0.030353,rank=4\n",
      "[SoftImpute] Iter 140: observed MAE=0.011982 validation MAE=0.030329,rank=4\n",
      "[SoftImpute] Iter 141: observed MAE=0.011962 validation MAE=0.030305,rank=4\n",
      "[SoftImpute] Iter 142: observed MAE=0.011943 validation MAE=0.030281,rank=4\n",
      "[SoftImpute] Iter 143: observed MAE=0.011924 validation MAE=0.030256,rank=4\n",
      "[SoftImpute] Iter 144: observed MAE=0.011905 validation MAE=0.030232,rank=4\n",
      "[SoftImpute] Iter 145: observed MAE=0.011886 validation MAE=0.030208,rank=4\n",
      "[SoftImpute] Iter 146: observed MAE=0.011867 validation MAE=0.030184,rank=4\n",
      "[SoftImpute] Iter 147: observed MAE=0.011849 validation MAE=0.030160,rank=4\n",
      "[SoftImpute] Iter 148: observed MAE=0.011831 validation MAE=0.030136,rank=4\n",
      "[SoftImpute] Iter 149: observed MAE=0.011813 validation MAE=0.030112,rank=4\n",
      "[SoftImpute] Iter 150: observed MAE=0.011795 validation MAE=0.030087,rank=4\n",
      "[SoftImpute] Iter 151: observed MAE=0.011777 validation MAE=0.030063,rank=4\n",
      "[SoftImpute] Iter 152: observed MAE=0.011760 validation MAE=0.030038,rank=4\n",
      "[SoftImpute] Iter 153: observed MAE=0.011742 validation MAE=0.030013,rank=4\n",
      "[SoftImpute] Iter 154: observed MAE=0.011725 validation MAE=0.029988,rank=4\n",
      "[SoftImpute] Iter 155: observed MAE=0.011708 validation MAE=0.029963,rank=4\n",
      "[SoftImpute] Iter 156: observed MAE=0.011691 validation MAE=0.029938,rank=4\n",
      "[SoftImpute] Iter 157: observed MAE=0.011674 validation MAE=0.029914,rank=4\n",
      "[SoftImpute] Iter 158: observed MAE=0.011658 validation MAE=0.029890,rank=4\n",
      "[SoftImpute] Iter 159: observed MAE=0.011641 validation MAE=0.029865,rank=4\n",
      "[SoftImpute] Iter 160: observed MAE=0.011625 validation MAE=0.029841,rank=4\n",
      "[SoftImpute] Iter 161: observed MAE=0.011608 validation MAE=0.029816,rank=4\n",
      "[SoftImpute] Iter 162: observed MAE=0.011592 validation MAE=0.029791,rank=4\n",
      "[SoftImpute] Iter 163: observed MAE=0.011576 validation MAE=0.029767,rank=4\n",
      "[SoftImpute] Iter 164: observed MAE=0.011560 validation MAE=0.029744,rank=4\n",
      "[SoftImpute] Iter 165: observed MAE=0.011545 validation MAE=0.029720,rank=4\n",
      "[SoftImpute] Iter 166: observed MAE=0.011529 validation MAE=0.029696,rank=4\n",
      "[SoftImpute] Iter 167: observed MAE=0.011513 validation MAE=0.029672,rank=4\n",
      "[SoftImpute] Iter 168: observed MAE=0.011498 validation MAE=0.029649,rank=4\n",
      "[SoftImpute] Iter 169: observed MAE=0.011482 validation MAE=0.029626,rank=4\n",
      "[SoftImpute] Iter 170: observed MAE=0.011467 validation MAE=0.029603,rank=4\n",
      "[SoftImpute] Iter 171: observed MAE=0.011451 validation MAE=0.029581,rank=4\n",
      "[SoftImpute] Iter 172: observed MAE=0.011436 validation MAE=0.029558,rank=4\n",
      "[SoftImpute] Iter 173: observed MAE=0.011421 validation MAE=0.029535,rank=4\n",
      "[SoftImpute] Iter 174: observed MAE=0.011406 validation MAE=0.029512,rank=4\n",
      "[SoftImpute] Iter 175: observed MAE=0.011391 validation MAE=0.029489,rank=4\n",
      "[SoftImpute] Iter 176: observed MAE=0.011376 validation MAE=0.029467,rank=4\n",
      "[SoftImpute] Iter 177: observed MAE=0.011362 validation MAE=0.029445,rank=4\n",
      "[SoftImpute] Iter 178: observed MAE=0.011347 validation MAE=0.029422,rank=4\n",
      "[SoftImpute] Iter 179: observed MAE=0.011333 validation MAE=0.029400,rank=4\n",
      "[SoftImpute] Iter 180: observed MAE=0.011318 validation MAE=0.029377,rank=4\n",
      "[SoftImpute] Iter 181: observed MAE=0.011304 validation MAE=0.029355,rank=4\n",
      "[SoftImpute] Iter 182: observed MAE=0.011290 validation MAE=0.029334,rank=4\n",
      "[SoftImpute] Iter 183: observed MAE=0.011276 validation MAE=0.029314,rank=4\n",
      "[SoftImpute] Iter 184: observed MAE=0.011262 validation MAE=0.029294,rank=4\n",
      "[SoftImpute] Iter 185: observed MAE=0.011248 validation MAE=0.029273,rank=4\n",
      "[SoftImpute] Iter 186: observed MAE=0.011234 validation MAE=0.029253,rank=4\n",
      "[SoftImpute] Iter 187: observed MAE=0.011220 validation MAE=0.029233,rank=4\n",
      "[SoftImpute] Iter 188: observed MAE=0.011206 validation MAE=0.029214,rank=4\n",
      "[SoftImpute] Iter 189: observed MAE=0.011192 validation MAE=0.029194,rank=4\n",
      "[SoftImpute] Iter 190: observed MAE=0.011179 validation MAE=0.029175,rank=4\n",
      "[SoftImpute] Iter 191: observed MAE=0.011165 validation MAE=0.029155,rank=4\n",
      "[SoftImpute] Iter 192: observed MAE=0.011151 validation MAE=0.029136,rank=4\n",
      "[SoftImpute] Iter 193: observed MAE=0.011137 validation MAE=0.029116,rank=4\n",
      "[SoftImpute] Iter 194: observed MAE=0.011124 validation MAE=0.029097,rank=4\n",
      "[SoftImpute] Iter 195: observed MAE=0.011110 validation MAE=0.029077,rank=4\n",
      "[SoftImpute] Iter 196: observed MAE=0.011097 validation MAE=0.029057,rank=4\n",
      "[SoftImpute] Iter 197: observed MAE=0.011084 validation MAE=0.029036,rank=4\n",
      "[SoftImpute] Iter 198: observed MAE=0.011070 validation MAE=0.029016,rank=4\n",
      "[SoftImpute] Iter 199: observed MAE=0.011057 validation MAE=0.028996,rank=4\n",
      "[SoftImpute] Iter 200: observed MAE=0.011044 validation MAE=0.028975,rank=4\n",
      "[SoftImpute] Iter 201: observed MAE=0.011031 validation MAE=0.028954,rank=4\n",
      "[SoftImpute] Iter 202: observed MAE=0.011018 validation MAE=0.028933,rank=4\n",
      "[SoftImpute] Iter 203: observed MAE=0.011005 validation MAE=0.028912,rank=4\n",
      "[SoftImpute] Iter 204: observed MAE=0.010992 validation MAE=0.028891,rank=4\n",
      "[SoftImpute] Iter 205: observed MAE=0.010978 validation MAE=0.028870,rank=4\n",
      "[SoftImpute] Iter 206: observed MAE=0.010965 validation MAE=0.028850,rank=4\n",
      "[SoftImpute] Iter 207: observed MAE=0.010952 validation MAE=0.028829,rank=4\n",
      "[SoftImpute] Iter 208: observed MAE=0.010939 validation MAE=0.028808,rank=4\n",
      "[SoftImpute] Iter 209: observed MAE=0.010926 validation MAE=0.028787,rank=4\n",
      "[SoftImpute] Iter 210: observed MAE=0.010913 validation MAE=0.028765,rank=4\n",
      "[SoftImpute] Iter 211: observed MAE=0.010901 validation MAE=0.028744,rank=4\n",
      "[SoftImpute] Iter 212: observed MAE=0.010888 validation MAE=0.028722,rank=4\n",
      "[SoftImpute] Iter 213: observed MAE=0.010875 validation MAE=0.028701,rank=4\n",
      "[SoftImpute] Iter 214: observed MAE=0.010862 validation MAE=0.028679,rank=4\n",
      "[SoftImpute] Iter 215: observed MAE=0.010849 validation MAE=0.028658,rank=4\n",
      "[SoftImpute] Iter 216: observed MAE=0.010837 validation MAE=0.028637,rank=4\n",
      "[SoftImpute] Iter 217: observed MAE=0.010824 validation MAE=0.028615,rank=4\n",
      "[SoftImpute] Iter 218: observed MAE=0.010811 validation MAE=0.028594,rank=4\n",
      "[SoftImpute] Iter 219: observed MAE=0.010799 validation MAE=0.028573,rank=4\n",
      "[SoftImpute] Iter 220: observed MAE=0.010786 validation MAE=0.028552,rank=4\n",
      "[SoftImpute] Iter 221: observed MAE=0.010773 validation MAE=0.028531,rank=4\n",
      "[SoftImpute] Iter 222: observed MAE=0.010761 validation MAE=0.028509,rank=4\n",
      "[SoftImpute] Iter 223: observed MAE=0.010749 validation MAE=0.028488,rank=4\n",
      "[SoftImpute] Iter 224: observed MAE=0.010736 validation MAE=0.028466,rank=4\n",
      "[SoftImpute] Iter 225: observed MAE=0.010724 validation MAE=0.028444,rank=4\n",
      "[SoftImpute] Iter 226: observed MAE=0.010712 validation MAE=0.028422,rank=4\n",
      "[SoftImpute] Iter 227: observed MAE=0.010699 validation MAE=0.028400,rank=4\n",
      "[SoftImpute] Iter 228: observed MAE=0.010687 validation MAE=0.028378,rank=4\n",
      "[SoftImpute] Iter 229: observed MAE=0.010675 validation MAE=0.028355,rank=4\n",
      "[SoftImpute] Iter 230: observed MAE=0.010663 validation MAE=0.028333,rank=4\n",
      "[SoftImpute] Iter 231: observed MAE=0.010651 validation MAE=0.028311,rank=4\n",
      "[SoftImpute] Iter 232: observed MAE=0.010639 validation MAE=0.028289,rank=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 233: observed MAE=0.010628 validation MAE=0.028267,rank=4\n",
      "[SoftImpute] Iter 234: observed MAE=0.010616 validation MAE=0.028245,rank=4\n",
      "[SoftImpute] Iter 235: observed MAE=0.010604 validation MAE=0.028223,rank=4\n",
      "[SoftImpute] Iter 236: observed MAE=0.010592 validation MAE=0.028202,rank=4\n",
      "[SoftImpute] Iter 237: observed MAE=0.010580 validation MAE=0.028181,rank=4\n",
      "[SoftImpute] Iter 238: observed MAE=0.010568 validation MAE=0.028160,rank=4\n",
      "[SoftImpute] Iter 239: observed MAE=0.010557 validation MAE=0.028138,rank=4\n",
      "[SoftImpute] Iter 240: observed MAE=0.010546 validation MAE=0.028117,rank=4\n",
      "[SoftImpute] Iter 241: observed MAE=0.010534 validation MAE=0.028096,rank=4\n",
      "[SoftImpute] Iter 242: observed MAE=0.010523 validation MAE=0.028075,rank=4\n",
      "[SoftImpute] Iter 243: observed MAE=0.010511 validation MAE=0.028053,rank=4\n",
      "[SoftImpute] Iter 244: observed MAE=0.010500 validation MAE=0.028032,rank=4\n",
      "[SoftImpute] Iter 245: observed MAE=0.010488 validation MAE=0.028011,rank=4\n",
      "[SoftImpute] Iter 246: observed MAE=0.010477 validation MAE=0.027989,rank=4\n",
      "[SoftImpute] Iter 247: observed MAE=0.010465 validation MAE=0.027968,rank=4\n",
      "[SoftImpute] Iter 248: observed MAE=0.010454 validation MAE=0.027946,rank=4\n",
      "[SoftImpute] Iter 249: observed MAE=0.010442 validation MAE=0.027924,rank=4\n",
      "[SoftImpute] Iter 250: observed MAE=0.010431 validation MAE=0.027902,rank=4\n",
      "[SoftImpute] Iter 251: observed MAE=0.010420 validation MAE=0.027879,rank=4\n",
      "[SoftImpute] Iter 252: observed MAE=0.010409 validation MAE=0.027856,rank=4\n",
      "[SoftImpute] Iter 253: observed MAE=0.010398 validation MAE=0.027833,rank=4\n",
      "[SoftImpute] Iter 254: observed MAE=0.010387 validation MAE=0.027810,rank=4\n",
      "[SoftImpute] Iter 255: observed MAE=0.010376 validation MAE=0.027787,rank=4\n",
      "[SoftImpute] Iter 256: observed MAE=0.010365 validation MAE=0.027763,rank=4\n",
      "[SoftImpute] Iter 257: observed MAE=0.010354 validation MAE=0.027740,rank=4\n",
      "[SoftImpute] Iter 258: observed MAE=0.010343 validation MAE=0.027716,rank=4\n",
      "[SoftImpute] Iter 259: observed MAE=0.010332 validation MAE=0.027692,rank=4\n",
      "[SoftImpute] Iter 260: observed MAE=0.010321 validation MAE=0.027667,rank=4\n",
      "[SoftImpute] Iter 261: observed MAE=0.010310 validation MAE=0.027643,rank=4\n",
      "[SoftImpute] Iter 262: observed MAE=0.010299 validation MAE=0.027618,rank=4\n",
      "[SoftImpute] Iter 263: observed MAE=0.010288 validation MAE=0.027594,rank=4\n",
      "[SoftImpute] Iter 264: observed MAE=0.010277 validation MAE=0.027569,rank=4\n",
      "[SoftImpute] Iter 265: observed MAE=0.010266 validation MAE=0.027544,rank=4\n",
      "[SoftImpute] Iter 266: observed MAE=0.010255 validation MAE=0.027519,rank=4\n",
      "[SoftImpute] Iter 267: observed MAE=0.010244 validation MAE=0.027493,rank=4\n",
      "[SoftImpute] Iter 268: observed MAE=0.010233 validation MAE=0.027468,rank=4\n",
      "[SoftImpute] Iter 269: observed MAE=0.010222 validation MAE=0.027442,rank=4\n",
      "[SoftImpute] Iter 270: observed MAE=0.010211 validation MAE=0.027416,rank=4\n",
      "[SoftImpute] Iter 271: observed MAE=0.010200 validation MAE=0.027390,rank=4\n",
      "[SoftImpute] Iter 272: observed MAE=0.010189 validation MAE=0.027364,rank=4\n",
      "[SoftImpute] Iter 273: observed MAE=0.010179 validation MAE=0.027337,rank=4\n",
      "[SoftImpute] Iter 274: observed MAE=0.010168 validation MAE=0.027310,rank=4\n",
      "[SoftImpute] Iter 275: observed MAE=0.010158 validation MAE=0.027283,rank=4\n",
      "[SoftImpute] Iter 276: observed MAE=0.010147 validation MAE=0.027255,rank=4\n",
      "[SoftImpute] Iter 277: observed MAE=0.010137 validation MAE=0.027228,rank=4\n",
      "[SoftImpute] Iter 278: observed MAE=0.010127 validation MAE=0.027200,rank=4\n",
      "[SoftImpute] Iter 279: observed MAE=0.010117 validation MAE=0.027172,rank=4\n",
      "[SoftImpute] Iter 280: observed MAE=0.010107 validation MAE=0.027144,rank=4\n",
      "[SoftImpute] Iter 281: observed MAE=0.010097 validation MAE=0.027116,rank=4\n",
      "[SoftImpute] Iter 282: observed MAE=0.010087 validation MAE=0.027088,rank=4\n",
      "[SoftImpute] Iter 283: observed MAE=0.010077 validation MAE=0.027060,rank=4\n",
      "[SoftImpute] Iter 284: observed MAE=0.010067 validation MAE=0.027031,rank=4\n",
      "[SoftImpute] Iter 285: observed MAE=0.010057 validation MAE=0.027003,rank=4\n",
      "[SoftImpute] Iter 286: observed MAE=0.010047 validation MAE=0.026975,rank=4\n",
      "[SoftImpute] Iter 287: observed MAE=0.010037 validation MAE=0.026947,rank=4\n",
      "[SoftImpute] Iter 288: observed MAE=0.010027 validation MAE=0.026919,rank=4\n",
      "[SoftImpute] Iter 289: observed MAE=0.010017 validation MAE=0.026891,rank=4\n",
      "[SoftImpute] Iter 290: observed MAE=0.010008 validation MAE=0.026863,rank=4\n",
      "[SoftImpute] Iter 291: observed MAE=0.009998 validation MAE=0.026835,rank=4\n",
      "[SoftImpute] Iter 292: observed MAE=0.009988 validation MAE=0.026806,rank=4\n",
      "[SoftImpute] Iter 293: observed MAE=0.009979 validation MAE=0.026778,rank=4\n",
      "[SoftImpute] Iter 294: observed MAE=0.009969 validation MAE=0.026750,rank=4\n",
      "[SoftImpute] Iter 295: observed MAE=0.009959 validation MAE=0.026722,rank=4\n",
      "[SoftImpute] Iter 296: observed MAE=0.009950 validation MAE=0.026694,rank=4\n",
      "[SoftImpute] Iter 297: observed MAE=0.009940 validation MAE=0.026666,rank=4\n",
      "[SoftImpute] Iter 298: observed MAE=0.009931 validation MAE=0.026638,rank=4\n",
      "[SoftImpute] Iter 299: observed MAE=0.009921 validation MAE=0.026610,rank=4\n",
      "[SoftImpute] Iter 300: observed MAE=0.009911 validation MAE=0.026582,rank=4\n",
      "[SoftImpute] Iter 301: observed MAE=0.009901 validation MAE=0.026553,rank=4\n",
      "[SoftImpute] Iter 302: observed MAE=0.009892 validation MAE=0.026525,rank=4\n",
      "[SoftImpute] Iter 303: observed MAE=0.009882 validation MAE=0.026497,rank=4\n",
      "[SoftImpute] Iter 304: observed MAE=0.009872 validation MAE=0.026468,rank=4\n",
      "[SoftImpute] Iter 305: observed MAE=0.009863 validation MAE=0.026441,rank=4\n",
      "[SoftImpute] Iter 306: observed MAE=0.009853 validation MAE=0.026413,rank=4\n",
      "[SoftImpute] Iter 307: observed MAE=0.009843 validation MAE=0.026385,rank=4\n",
      "[SoftImpute] Iter 308: observed MAE=0.009834 validation MAE=0.026357,rank=4\n",
      "[SoftImpute] Iter 309: observed MAE=0.009824 validation MAE=0.026330,rank=4\n",
      "[SoftImpute] Iter 310: observed MAE=0.009814 validation MAE=0.026303,rank=4\n",
      "[SoftImpute] Iter 311: observed MAE=0.009805 validation MAE=0.026276,rank=4\n",
      "[SoftImpute] Iter 312: observed MAE=0.009795 validation MAE=0.026248,rank=4\n",
      "[SoftImpute] Iter 313: observed MAE=0.009786 validation MAE=0.026221,rank=4\n",
      "[SoftImpute] Iter 314: observed MAE=0.009777 validation MAE=0.026193,rank=4\n",
      "[SoftImpute] Iter 315: observed MAE=0.009767 validation MAE=0.026166,rank=4\n",
      "[SoftImpute] Iter 316: observed MAE=0.009758 validation MAE=0.026138,rank=4\n",
      "[SoftImpute] Iter 317: observed MAE=0.009749 validation MAE=0.026110,rank=4\n",
      "[SoftImpute] Iter 318: observed MAE=0.009740 validation MAE=0.026083,rank=4\n",
      "[SoftImpute] Iter 319: observed MAE=0.009730 validation MAE=0.026055,rank=4\n",
      "[SoftImpute] Iter 320: observed MAE=0.009721 validation MAE=0.026027,rank=4\n",
      "[SoftImpute] Iter 321: observed MAE=0.009712 validation MAE=0.025999,rank=4\n",
      "[SoftImpute] Iter 322: observed MAE=0.009703 validation MAE=0.025971,rank=4\n",
      "[SoftImpute] Iter 323: observed MAE=0.009693 validation MAE=0.025943,rank=4\n",
      "[SoftImpute] Iter 324: observed MAE=0.009684 validation MAE=0.025916,rank=4\n",
      "[SoftImpute] Iter 325: observed MAE=0.009675 validation MAE=0.025888,rank=4\n",
      "[SoftImpute] Iter 326: observed MAE=0.009666 validation MAE=0.025860,rank=4\n",
      "[SoftImpute] Iter 327: observed MAE=0.009657 validation MAE=0.025833,rank=4\n",
      "[SoftImpute] Iter 328: observed MAE=0.009648 validation MAE=0.025805,rank=4\n",
      "[SoftImpute] Iter 329: observed MAE=0.009639 validation MAE=0.025778,rank=4\n",
      "[SoftImpute] Iter 330: observed MAE=0.009630 validation MAE=0.025751,rank=4\n",
      "[SoftImpute] Iter 331: observed MAE=0.009621 validation MAE=0.025724,rank=4\n",
      "[SoftImpute] Iter 332: observed MAE=0.009612 validation MAE=0.025697,rank=4\n",
      "[SoftImpute] Iter 333: observed MAE=0.009604 validation MAE=0.025670,rank=4\n",
      "[SoftImpute] Iter 334: observed MAE=0.009595 validation MAE=0.025643,rank=4\n",
      "[SoftImpute] Iter 335: observed MAE=0.009586 validation MAE=0.025616,rank=4\n",
      "[SoftImpute] Iter 336: observed MAE=0.009578 validation MAE=0.025590,rank=4\n",
      "[SoftImpute] Iter 337: observed MAE=0.009569 validation MAE=0.025564,rank=4\n",
      "[SoftImpute] Iter 338: observed MAE=0.009561 validation MAE=0.025538,rank=4\n",
      "[SoftImpute] Iter 339: observed MAE=0.009553 validation MAE=0.025512,rank=4\n",
      "[SoftImpute] Iter 340: observed MAE=0.009544 validation MAE=0.025487,rank=4\n",
      "[SoftImpute] Iter 341: observed MAE=0.009536 validation MAE=0.025462,rank=4\n",
      "[SoftImpute] Iter 342: observed MAE=0.009528 validation MAE=0.025437,rank=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 343: observed MAE=0.009520 validation MAE=0.025412,rank=4\n",
      "[SoftImpute] Iter 344: observed MAE=0.009512 validation MAE=0.025387,rank=4\n",
      "[SoftImpute] Iter 345: observed MAE=0.009503 validation MAE=0.025363,rank=4\n",
      "[SoftImpute] Iter 346: observed MAE=0.009495 validation MAE=0.025339,rank=4\n",
      "[SoftImpute] Iter 347: observed MAE=0.009487 validation MAE=0.025315,rank=4\n",
      "[SoftImpute] Iter 348: observed MAE=0.009479 validation MAE=0.025291,rank=4\n",
      "[SoftImpute] Iter 349: observed MAE=0.009471 validation MAE=0.025268,rank=4\n",
      "[SoftImpute] Iter 350: observed MAE=0.009463 validation MAE=0.025244,rank=4\n",
      "[SoftImpute] Iter 351: observed MAE=0.009456 validation MAE=0.025221,rank=4\n",
      "[SoftImpute] Iter 352: observed MAE=0.009448 validation MAE=0.025197,rank=4\n",
      "[SoftImpute] Iter 353: observed MAE=0.009440 validation MAE=0.025174,rank=4\n",
      "[SoftImpute] Iter 354: observed MAE=0.009432 validation MAE=0.025150,rank=4\n",
      "[SoftImpute] Iter 355: observed MAE=0.009424 validation MAE=0.025127,rank=4\n",
      "[SoftImpute] Iter 356: observed MAE=0.009417 validation MAE=0.025103,rank=4\n",
      "[SoftImpute] Iter 357: observed MAE=0.009409 validation MAE=0.025080,rank=4\n",
      "[SoftImpute] Iter 358: observed MAE=0.009402 validation MAE=0.025056,rank=4\n",
      "[SoftImpute] Iter 359: observed MAE=0.009394 validation MAE=0.025034,rank=4\n",
      "[SoftImpute] Iter 360: observed MAE=0.009387 validation MAE=0.025011,rank=4\n",
      "[SoftImpute] Iter 361: observed MAE=0.009380 validation MAE=0.024989,rank=4\n",
      "[SoftImpute] Iter 362: observed MAE=0.009372 validation MAE=0.024966,rank=4\n",
      "[SoftImpute] Iter 363: observed MAE=0.009365 validation MAE=0.024944,rank=4\n",
      "[SoftImpute] Iter 364: observed MAE=0.009358 validation MAE=0.024922,rank=4\n",
      "[SoftImpute] Iter 365: observed MAE=0.009351 validation MAE=0.024899,rank=4\n",
      "[SoftImpute] Iter 366: observed MAE=0.009344 validation MAE=0.024877,rank=4\n",
      "[SoftImpute] Iter 367: observed MAE=0.009337 validation MAE=0.024855,rank=4\n",
      "[SoftImpute] Iter 368: observed MAE=0.009330 validation MAE=0.024833,rank=4\n",
      "[SoftImpute] Iter 369: observed MAE=0.009323 validation MAE=0.024811,rank=4\n",
      "[SoftImpute] Iter 370: observed MAE=0.009317 validation MAE=0.024789,rank=4\n",
      "[SoftImpute] Iter 371: observed MAE=0.009310 validation MAE=0.024767,rank=4\n",
      "[SoftImpute] Iter 372: observed MAE=0.009303 validation MAE=0.024745,rank=4\n",
      "[SoftImpute] Iter 373: observed MAE=0.009296 validation MAE=0.024724,rank=4\n",
      "[SoftImpute] Iter 374: observed MAE=0.009290 validation MAE=0.024702,rank=4\n",
      "[SoftImpute] Iter 375: observed MAE=0.009283 validation MAE=0.024680,rank=4\n",
      "[SoftImpute] Iter 376: observed MAE=0.009277 validation MAE=0.024659,rank=4\n",
      "[SoftImpute] Iter 377: observed MAE=0.009270 validation MAE=0.024637,rank=4\n",
      "[SoftImpute] Iter 378: observed MAE=0.009264 validation MAE=0.024616,rank=4\n",
      "[SoftImpute] Iter 379: observed MAE=0.009257 validation MAE=0.024595,rank=4\n",
      "[SoftImpute] Iter 380: observed MAE=0.009251 validation MAE=0.024575,rank=4\n",
      "[SoftImpute] Iter 381: observed MAE=0.009245 validation MAE=0.024554,rank=4\n",
      "[SoftImpute] Iter 382: observed MAE=0.009238 validation MAE=0.024534,rank=4\n",
      "[SoftImpute] Iter 383: observed MAE=0.009232 validation MAE=0.024515,rank=4\n",
      "[SoftImpute] Iter 384: observed MAE=0.009226 validation MAE=0.024495,rank=4\n",
      "[SoftImpute] Iter 385: observed MAE=0.009220 validation MAE=0.024476,rank=4\n",
      "[SoftImpute] Iter 386: observed MAE=0.009214 validation MAE=0.024457,rank=4\n",
      "[SoftImpute] Iter 387: observed MAE=0.009208 validation MAE=0.024439,rank=4\n",
      "[SoftImpute] Iter 388: observed MAE=0.009202 validation MAE=0.024420,rank=4\n",
      "[SoftImpute] Iter 389: observed MAE=0.009196 validation MAE=0.024401,rank=4\n",
      "[SoftImpute] Iter 390: observed MAE=0.009190 validation MAE=0.024383,rank=4\n",
      "[SoftImpute] Iter 391: observed MAE=0.009184 validation MAE=0.024364,rank=4\n",
      "[SoftImpute] Iter 392: observed MAE=0.009178 validation MAE=0.024345,rank=4\n",
      "[SoftImpute] Iter 393: observed MAE=0.009173 validation MAE=0.024327,rank=4\n",
      "[SoftImpute] Iter 394: observed MAE=0.009167 validation MAE=0.024308,rank=4\n",
      "[SoftImpute] Iter 395: observed MAE=0.009161 validation MAE=0.024290,rank=4\n",
      "[SoftImpute] Iter 396: observed MAE=0.009155 validation MAE=0.024271,rank=4\n",
      "[SoftImpute] Iter 397: observed MAE=0.009150 validation MAE=0.024253,rank=4\n",
      "[SoftImpute] Iter 398: observed MAE=0.009144 validation MAE=0.024235,rank=4\n",
      "[SoftImpute] Iter 399: observed MAE=0.009138 validation MAE=0.024216,rank=4\n",
      "[SoftImpute] Iter 400: observed MAE=0.009133 validation MAE=0.024198,rank=4\n",
      "[SoftImpute] Iter 401: observed MAE=0.009127 validation MAE=0.024180,rank=4\n",
      "[SoftImpute] Iter 402: observed MAE=0.009122 validation MAE=0.024162,rank=4\n",
      "[SoftImpute] Iter 403: observed MAE=0.009116 validation MAE=0.024144,rank=4\n",
      "[SoftImpute] Iter 404: observed MAE=0.009111 validation MAE=0.024127,rank=4\n",
      "[SoftImpute] Iter 405: observed MAE=0.009105 validation MAE=0.024110,rank=4\n",
      "[SoftImpute] Iter 406: observed MAE=0.009100 validation MAE=0.024093,rank=4\n",
      "[SoftImpute] Iter 407: observed MAE=0.009094 validation MAE=0.024076,rank=4\n",
      "[SoftImpute] Iter 408: observed MAE=0.009089 validation MAE=0.024059,rank=4\n",
      "[SoftImpute] Iter 409: observed MAE=0.009083 validation MAE=0.024042,rank=4\n",
      "[SoftImpute] Iter 410: observed MAE=0.009078 validation MAE=0.024025,rank=4\n",
      "[SoftImpute] Iter 411: observed MAE=0.009073 validation MAE=0.024008,rank=4\n",
      "[SoftImpute] Iter 412: observed MAE=0.009067 validation MAE=0.023991,rank=4\n",
      "[SoftImpute] Iter 413: observed MAE=0.009062 validation MAE=0.023974,rank=4\n",
      "[SoftImpute] Iter 414: observed MAE=0.009057 validation MAE=0.023957,rank=4\n",
      "[SoftImpute] Iter 415: observed MAE=0.009052 validation MAE=0.023940,rank=4\n",
      "[SoftImpute] Iter 416: observed MAE=0.009047 validation MAE=0.023924,rank=4\n",
      "[SoftImpute] Iter 417: observed MAE=0.009042 validation MAE=0.023908,rank=4\n",
      "[SoftImpute] Iter 418: observed MAE=0.009037 validation MAE=0.023893,rank=4\n",
      "[SoftImpute] Iter 419: observed MAE=0.009031 validation MAE=0.023877,rank=4\n",
      "[SoftImpute] Iter 420: observed MAE=0.009026 validation MAE=0.023862,rank=4\n",
      "[SoftImpute] Iter 421: observed MAE=0.009021 validation MAE=0.023846,rank=4\n",
      "[SoftImpute] Iter 422: observed MAE=0.009017 validation MAE=0.023831,rank=4\n",
      "[SoftImpute] Iter 423: observed MAE=0.009012 validation MAE=0.023816,rank=4\n",
      "[SoftImpute] Iter 424: observed MAE=0.009007 validation MAE=0.023802,rank=4\n",
      "[SoftImpute] Iter 425: observed MAE=0.009002 validation MAE=0.023787,rank=4\n",
      "[SoftImpute] Iter 426: observed MAE=0.008997 validation MAE=0.023773,rank=4\n",
      "[SoftImpute] Iter 427: observed MAE=0.008992 validation MAE=0.023759,rank=4\n",
      "[SoftImpute] Iter 428: observed MAE=0.008988 validation MAE=0.023745,rank=4\n",
      "[SoftImpute] Iter 429: observed MAE=0.008983 validation MAE=0.023731,rank=4\n",
      "[SoftImpute] Iter 430: observed MAE=0.008978 validation MAE=0.023717,rank=4\n",
      "[SoftImpute] Iter 431: observed MAE=0.008973 validation MAE=0.023704,rank=4\n",
      "[SoftImpute] Iter 432: observed MAE=0.008969 validation MAE=0.023690,rank=4\n",
      "[SoftImpute] Iter 433: observed MAE=0.008964 validation MAE=0.023676,rank=4\n",
      "[SoftImpute] Iter 434: observed MAE=0.008959 validation MAE=0.023663,rank=4\n",
      "[SoftImpute] Iter 435: observed MAE=0.008955 validation MAE=0.023650,rank=4\n",
      "[SoftImpute] Iter 436: observed MAE=0.008950 validation MAE=0.023636,rank=4\n",
      "[SoftImpute] Iter 437: observed MAE=0.008946 validation MAE=0.023623,rank=4\n",
      "[SoftImpute] Iter 438: observed MAE=0.008941 validation MAE=0.023610,rank=4\n",
      "[SoftImpute] Iter 439: observed MAE=0.008937 validation MAE=0.023597,rank=4\n",
      "[SoftImpute] Iter 440: observed MAE=0.008932 validation MAE=0.023585,rank=4\n",
      "[SoftImpute] Iter 441: observed MAE=0.008928 validation MAE=0.023573,rank=4\n",
      "[SoftImpute] Iter 442: observed MAE=0.008923 validation MAE=0.023561,rank=4\n",
      "[SoftImpute] Iter 443: observed MAE=0.008919 validation MAE=0.023549,rank=4\n",
      "[SoftImpute] Iter 444: observed MAE=0.008915 validation MAE=0.023537,rank=4\n",
      "[SoftImpute] Iter 445: observed MAE=0.008910 validation MAE=0.023525,rank=4\n",
      "[SoftImpute] Iter 446: observed MAE=0.008906 validation MAE=0.023514,rank=4\n",
      "[SoftImpute] Iter 447: observed MAE=0.008902 validation MAE=0.023503,rank=4\n",
      "[SoftImpute] Iter 448: observed MAE=0.008898 validation MAE=0.023491,rank=4\n",
      "[SoftImpute] Iter 449: observed MAE=0.008894 validation MAE=0.023480,rank=4\n",
      "[SoftImpute] Iter 450: observed MAE=0.008889 validation MAE=0.023469,rank=4\n",
      "[SoftImpute] Iter 451: observed MAE=0.008885 validation MAE=0.023458,rank=4\n",
      "[SoftImpute] Iter 452: observed MAE=0.008881 validation MAE=0.023448,rank=4\n",
      "[SoftImpute] Iter 453: observed MAE=0.008877 validation MAE=0.023437,rank=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 454: observed MAE=0.008873 validation MAE=0.023426,rank=4\n",
      "[SoftImpute] Iter 455: observed MAE=0.008869 validation MAE=0.023415,rank=4\n",
      "[SoftImpute] Iter 456: observed MAE=0.008865 validation MAE=0.023404,rank=4\n",
      "[SoftImpute] Iter 457: observed MAE=0.008861 validation MAE=0.023393,rank=4\n",
      "[SoftImpute] Iter 458: observed MAE=0.008857 validation MAE=0.023383,rank=4\n",
      "[SoftImpute] Iter 459: observed MAE=0.008854 validation MAE=0.023373,rank=4\n",
      "[SoftImpute] Iter 460: observed MAE=0.008850 validation MAE=0.023362,rank=4\n",
      "[SoftImpute] Iter 461: observed MAE=0.008846 validation MAE=0.023352,rank=4\n",
      "[SoftImpute] Iter 462: observed MAE=0.008842 validation MAE=0.023342,rank=4\n",
      "[SoftImpute] Iter 463: observed MAE=0.008838 validation MAE=0.023331,rank=4\n",
      "[SoftImpute] Iter 464: observed MAE=0.008835 validation MAE=0.023321,rank=4\n",
      "[SoftImpute] Iter 465: observed MAE=0.008831 validation MAE=0.023311,rank=4\n",
      "[SoftImpute] Iter 466: observed MAE=0.008827 validation MAE=0.023300,rank=4\n",
      "[SoftImpute] Iter 467: observed MAE=0.008824 validation MAE=0.023290,rank=4\n",
      "[SoftImpute] Iter 468: observed MAE=0.008820 validation MAE=0.023280,rank=4\n",
      "[SoftImpute] Iter 469: observed MAE=0.008817 validation MAE=0.023270,rank=4\n",
      "[SoftImpute] Iter 470: observed MAE=0.008813 validation MAE=0.023261,rank=4\n",
      "[SoftImpute] Iter 471: observed MAE=0.008810 validation MAE=0.023251,rank=4\n",
      "[SoftImpute] Iter 472: observed MAE=0.008807 validation MAE=0.023241,rank=4\n",
      "[SoftImpute] Iter 473: observed MAE=0.008803 validation MAE=0.023231,rank=4\n",
      "[SoftImpute] Iter 474: observed MAE=0.008800 validation MAE=0.023222,rank=4\n",
      "[SoftImpute] Iter 475: observed MAE=0.008796 validation MAE=0.023212,rank=4\n",
      "[SoftImpute] Iter 476: observed MAE=0.008793 validation MAE=0.023203,rank=4\n",
      "[SoftImpute] Iter 477: observed MAE=0.008790 validation MAE=0.023194,rank=4\n",
      "[SoftImpute] Iter 478: observed MAE=0.008787 validation MAE=0.023184,rank=4\n",
      "[SoftImpute] Iter 479: observed MAE=0.008783 validation MAE=0.023175,rank=4\n",
      "[SoftImpute] Iter 480: observed MAE=0.008780 validation MAE=0.023166,rank=4\n",
      "[SoftImpute] Iter 481: observed MAE=0.008777 validation MAE=0.023156,rank=4\n",
      "[SoftImpute] Iter 482: observed MAE=0.008774 validation MAE=0.023147,rank=4\n",
      "[SoftImpute] Iter 483: observed MAE=0.008771 validation MAE=0.023138,rank=4\n",
      "[SoftImpute] Iter 484: observed MAE=0.008768 validation MAE=0.023129,rank=4\n",
      "[SoftImpute] Iter 485: observed MAE=0.008764 validation MAE=0.023120,rank=4\n",
      "[SoftImpute] Iter 486: observed MAE=0.008761 validation MAE=0.023111,rank=4\n",
      "[SoftImpute] Iter 487: observed MAE=0.008758 validation MAE=0.023103,rank=4\n",
      "[SoftImpute] Iter 488: observed MAE=0.008755 validation MAE=0.023094,rank=4\n",
      "[SoftImpute] Iter 489: observed MAE=0.008753 validation MAE=0.023085,rank=4\n",
      "[SoftImpute] Iter 490: observed MAE=0.008750 validation MAE=0.023077,rank=4\n",
      "[SoftImpute] Iter 491: observed MAE=0.008747 validation MAE=0.023068,rank=4\n",
      "[SoftImpute] Iter 492: observed MAE=0.008744 validation MAE=0.023060,rank=4\n",
      "[SoftImpute] Iter 493: observed MAE=0.008741 validation MAE=0.023052,rank=4\n",
      "[SoftImpute] Iter 494: observed MAE=0.008738 validation MAE=0.023043,rank=4\n",
      "[SoftImpute] Iter 495: observed MAE=0.008735 validation MAE=0.023035,rank=4\n",
      "[SoftImpute] Iter 496: observed MAE=0.008732 validation MAE=0.023027,rank=4\n",
      "[SoftImpute] Iter 497: observed MAE=0.008729 validation MAE=0.023018,rank=4\n",
      "[SoftImpute] Iter 498: observed MAE=0.008726 validation MAE=0.023010,rank=4\n",
      "[SoftImpute] Iter 499: observed MAE=0.008723 validation MAE=0.023002,rank=4\n",
      "[SoftImpute] Iter 500: observed MAE=0.008720 validation MAE=0.022994,rank=4\n",
      "[SoftImpute] Stopped after iteration 500 for lambda=0.024644\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 17.25569725036621\n",
      "After the matrix factor stage, training error is 0.00872, validation error is 0.02299\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.30185, val loss: 0.30478\n",
      "Main effects training epoch: 2, train loss: 0.23228, val loss: 0.23617\n",
      "Main effects training epoch: 3, train loss: 0.17997, val loss: 0.18315\n",
      "Main effects training epoch: 4, train loss: 0.14845, val loss: 0.15215\n",
      "Main effects training epoch: 5, train loss: 0.13484, val loss: 0.13695\n",
      "Main effects training epoch: 6, train loss: 0.13033, val loss: 0.13155\n",
      "Main effects training epoch: 7, train loss: 0.13055, val loss: 0.13149\n",
      "Main effects training epoch: 8, train loss: 0.12971, val loss: 0.12988\n",
      "Main effects training epoch: 9, train loss: 0.12862, val loss: 0.12919\n",
      "Main effects training epoch: 10, train loss: 0.12828, val loss: 0.12938\n",
      "Main effects training epoch: 11, train loss: 0.12732, val loss: 0.12789\n",
      "Main effects training epoch: 12, train loss: 0.12659, val loss: 0.12743\n",
      "Main effects training epoch: 13, train loss: 0.12563, val loss: 0.12654\n",
      "Main effects training epoch: 14, train loss: 0.12325, val loss: 0.12456\n",
      "Main effects training epoch: 15, train loss: 0.11949, val loss: 0.12161\n",
      "Main effects training epoch: 16, train loss: 0.11686, val loss: 0.11922\n",
      "Main effects training epoch: 17, train loss: 0.11423, val loss: 0.11562\n",
      "Main effects training epoch: 18, train loss: 0.11484, val loss: 0.11687\n",
      "Main effects training epoch: 19, train loss: 0.11188, val loss: 0.11473\n",
      "Main effects training epoch: 20, train loss: 0.11089, val loss: 0.11324\n",
      "Main effects training epoch: 21, train loss: 0.10849, val loss: 0.11127\n",
      "Main effects training epoch: 22, train loss: 0.10778, val loss: 0.11002\n",
      "Main effects training epoch: 23, train loss: 0.10761, val loss: 0.10934\n",
      "Main effects training epoch: 24, train loss: 0.10695, val loss: 0.10962\n",
      "Main effects training epoch: 25, train loss: 0.10869, val loss: 0.11093\n",
      "Main effects training epoch: 26, train loss: 0.10721, val loss: 0.11016\n",
      "Main effects training epoch: 27, train loss: 0.10692, val loss: 0.10911\n",
      "Main effects training epoch: 28, train loss: 0.10656, val loss: 0.10921\n",
      "Main effects training epoch: 29, train loss: 0.10666, val loss: 0.10915\n",
      "Main effects training epoch: 30, train loss: 0.10688, val loss: 0.10958\n",
      "Main effects training epoch: 31, train loss: 0.10689, val loss: 0.10943\n",
      "Main effects training epoch: 32, train loss: 0.10622, val loss: 0.10918\n",
      "Main effects training epoch: 33, train loss: 0.10631, val loss: 0.10928\n",
      "Main effects training epoch: 34, train loss: 0.10605, val loss: 0.10868\n",
      "Main effects training epoch: 35, train loss: 0.10599, val loss: 0.10879\n",
      "Main effects training epoch: 36, train loss: 0.10605, val loss: 0.10911\n",
      "Main effects training epoch: 37, train loss: 0.10704, val loss: 0.10981\n",
      "Main effects training epoch: 38, train loss: 0.10655, val loss: 0.10926\n",
      "Main effects training epoch: 39, train loss: 0.10607, val loss: 0.10888\n",
      "Main effects training epoch: 40, train loss: 0.10588, val loss: 0.10859\n",
      "Main effects training epoch: 41, train loss: 0.10600, val loss: 0.10889\n",
      "Main effects training epoch: 42, train loss: 0.10616, val loss: 0.10903\n",
      "Main effects training epoch: 43, train loss: 0.10609, val loss: 0.10883\n",
      "Main effects training epoch: 44, train loss: 0.10630, val loss: 0.10899\n",
      "Main effects training epoch: 45, train loss: 0.10686, val loss: 0.10928\n",
      "Main effects training epoch: 46, train loss: 0.10599, val loss: 0.10904\n",
      "Main effects training epoch: 47, train loss: 0.10577, val loss: 0.10887\n",
      "Main effects training epoch: 48, train loss: 0.10584, val loss: 0.10888\n",
      "Main effects training epoch: 49, train loss: 0.10576, val loss: 0.10866\n",
      "Main effects training epoch: 50, train loss: 0.10617, val loss: 0.10903\n",
      "Main effects training epoch: 51, train loss: 0.10592, val loss: 0.10893\n",
      "Main effects training epoch: 52, train loss: 0.10606, val loss: 0.10892\n",
      "Main effects training epoch: 53, train loss: 0.10577, val loss: 0.10888\n",
      "Main effects training epoch: 54, train loss: 0.10586, val loss: 0.10875\n",
      "Main effects training epoch: 55, train loss: 0.10635, val loss: 0.10929\n",
      "Main effects training epoch: 56, train loss: 0.10600, val loss: 0.10895\n",
      "Main effects training epoch: 57, train loss: 0.10614, val loss: 0.10942\n",
      "Main effects training epoch: 58, train loss: 0.10591, val loss: 0.10905\n",
      "Main effects training epoch: 59, train loss: 0.10587, val loss: 0.10866\n",
      "Main effects training epoch: 60, train loss: 0.10580, val loss: 0.10885\n",
      "Main effects training epoch: 61, train loss: 0.10590, val loss: 0.10868\n",
      "Main effects training epoch: 62, train loss: 0.10611, val loss: 0.10911\n",
      "Main effects training epoch: 63, train loss: 0.10587, val loss: 0.10877\n",
      "Main effects training epoch: 64, train loss: 0.10611, val loss: 0.10908\n",
      "Main effects training epoch: 65, train loss: 0.10597, val loss: 0.10888\n",
      "Main effects training epoch: 66, train loss: 0.10579, val loss: 0.10886\n",
      "Main effects training epoch: 67, train loss: 0.10588, val loss: 0.10853\n",
      "Main effects training epoch: 68, train loss: 0.10601, val loss: 0.10926\n",
      "Main effects training epoch: 69, train loss: 0.10583, val loss: 0.10866\n",
      "Main effects training epoch: 70, train loss: 0.10585, val loss: 0.10893\n",
      "Main effects training epoch: 71, train loss: 0.10592, val loss: 0.10879\n",
      "Main effects training epoch: 72, train loss: 0.10582, val loss: 0.10901\n",
      "Main effects training epoch: 73, train loss: 0.10586, val loss: 0.10890\n",
      "Main effects training epoch: 74, train loss: 0.10596, val loss: 0.10893\n",
      "Main effects training epoch: 75, train loss: 0.10614, val loss: 0.10884\n",
      "Main effects training epoch: 76, train loss: 0.10583, val loss: 0.10904\n",
      "Main effects training epoch: 77, train loss: 0.10592, val loss: 0.10890\n",
      "Main effects training epoch: 78, train loss: 0.10580, val loss: 0.10881\n",
      "Main effects training epoch: 79, train loss: 0.10594, val loss: 0.10889\n",
      "Main effects training epoch: 80, train loss: 0.10607, val loss: 0.10888\n",
      "Main effects training epoch: 81, train loss: 0.10581, val loss: 0.10891\n",
      "Main effects training epoch: 82, train loss: 0.10593, val loss: 0.10922\n",
      "Main effects training epoch: 83, train loss: 0.10574, val loss: 0.10873\n",
      "Main effects training epoch: 84, train loss: 0.10573, val loss: 0.10877\n",
      "Main effects training epoch: 85, train loss: 0.10636, val loss: 0.10929\n",
      "Main effects training epoch: 86, train loss: 0.10583, val loss: 0.10874\n",
      "Main effects training epoch: 87, train loss: 0.10614, val loss: 0.10928\n",
      "Main effects training epoch: 88, train loss: 0.10616, val loss: 0.10949\n",
      "Main effects training epoch: 89, train loss: 0.10589, val loss: 0.10865\n",
      "Main effects training epoch: 90, train loss: 0.10604, val loss: 0.10903\n",
      "Main effects training epoch: 91, train loss: 0.10588, val loss: 0.10892\n",
      "Main effects training epoch: 92, train loss: 0.10612, val loss: 0.10925\n",
      "Main effects training epoch: 93, train loss: 0.10583, val loss: 0.10911\n",
      "Main effects training epoch: 94, train loss: 0.10604, val loss: 0.10904\n",
      "Main effects training epoch: 95, train loss: 0.10576, val loss: 0.10903\n",
      "Main effects training epoch: 96, train loss: 0.10568, val loss: 0.10869\n",
      "Main effects training epoch: 97, train loss: 0.10588, val loss: 0.10926\n",
      "Main effects training epoch: 98, train loss: 0.10606, val loss: 0.10910\n",
      "Main effects training epoch: 99, train loss: 0.10649, val loss: 0.10930\n",
      "Main effects training epoch: 100, train loss: 0.10628, val loss: 0.10948\n",
      "Main effects training epoch: 101, train loss: 0.10586, val loss: 0.10912\n",
      "Main effects training epoch: 102, train loss: 0.10570, val loss: 0.10869\n",
      "Main effects training epoch: 103, train loss: 0.10643, val loss: 0.10951\n",
      "Main effects training epoch: 104, train loss: 0.10650, val loss: 0.10946\n",
      "Main effects training epoch: 105, train loss: 0.10626, val loss: 0.10954\n",
      "Main effects training epoch: 106, train loss: 0.10594, val loss: 0.10896\n",
      "Main effects training epoch: 107, train loss: 0.10579, val loss: 0.10878\n",
      "Main effects training epoch: 108, train loss: 0.10607, val loss: 0.10922\n",
      "Main effects training epoch: 109, train loss: 0.10581, val loss: 0.10902\n",
      "Main effects training epoch: 110, train loss: 0.10590, val loss: 0.10892\n",
      "Main effects training epoch: 111, train loss: 0.10605, val loss: 0.10924\n",
      "Main effects training epoch: 112, train loss: 0.10580, val loss: 0.10875\n",
      "Main effects training epoch: 113, train loss: 0.10587, val loss: 0.10900\n",
      "Main effects training epoch: 114, train loss: 0.10581, val loss: 0.10883\n",
      "Main effects training epoch: 115, train loss: 0.10618, val loss: 0.10915\n",
      "Main effects training epoch: 116, train loss: 0.10615, val loss: 0.10949\n",
      "Main effects training epoch: 117, train loss: 0.10602, val loss: 0.10894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 118, train loss: 0.10595, val loss: 0.10914\n",
      "Main effects training epoch: 119, train loss: 0.10618, val loss: 0.10895\n",
      "Main effects training epoch: 120, train loss: 0.10601, val loss: 0.10941\n",
      "Main effects training epoch: 121, train loss: 0.10588, val loss: 0.10869\n",
      "Main effects training epoch: 122, train loss: 0.10576, val loss: 0.10885\n",
      "Main effects training epoch: 123, train loss: 0.10578, val loss: 0.10899\n",
      "Main effects training epoch: 124, train loss: 0.10583, val loss: 0.10934\n",
      "Main effects training epoch: 125, train loss: 0.10610, val loss: 0.10904\n",
      "Main effects training epoch: 126, train loss: 0.10593, val loss: 0.10917\n",
      "Main effects training epoch: 127, train loss: 0.10579, val loss: 0.10863\n",
      "Main effects training epoch: 128, train loss: 0.10583, val loss: 0.10933\n",
      "Main effects training epoch: 129, train loss: 0.10580, val loss: 0.10919\n",
      "Main effects training epoch: 130, train loss: 0.10573, val loss: 0.10874\n",
      "Main effects training epoch: 131, train loss: 0.10601, val loss: 0.10951\n",
      "Main effects training epoch: 132, train loss: 0.10600, val loss: 0.10905\n",
      "Main effects training epoch: 133, train loss: 0.10601, val loss: 0.10931\n",
      "Main effects training epoch: 134, train loss: 0.10566, val loss: 0.10882\n",
      "Main effects training epoch: 135, train loss: 0.10590, val loss: 0.10909\n",
      "Main effects training epoch: 136, train loss: 0.10621, val loss: 0.10930\n",
      "Main effects training epoch: 137, train loss: 0.10597, val loss: 0.10906\n",
      "Main effects training epoch: 138, train loss: 0.10583, val loss: 0.10895\n",
      "Main effects training epoch: 139, train loss: 0.10580, val loss: 0.10894\n",
      "Main effects training epoch: 140, train loss: 0.10591, val loss: 0.10891\n",
      "Main effects training epoch: 141, train loss: 0.10595, val loss: 0.10903\n",
      "Main effects training epoch: 142, train loss: 0.10601, val loss: 0.10932\n",
      "Main effects training epoch: 143, train loss: 0.10596, val loss: 0.10892\n",
      "Main effects training epoch: 144, train loss: 0.10583, val loss: 0.10925\n",
      "Main effects training epoch: 145, train loss: 0.10572, val loss: 0.10887\n",
      "Main effects training epoch: 146, train loss: 0.10574, val loss: 0.10897\n",
      "Main effects training epoch: 147, train loss: 0.10575, val loss: 0.10903\n",
      "Main effects training epoch: 148, train loss: 0.10589, val loss: 0.10894\n",
      "Main effects training epoch: 149, train loss: 0.10577, val loss: 0.10889\n",
      "Main effects training epoch: 150, train loss: 0.10569, val loss: 0.10895\n",
      "Main effects training epoch: 151, train loss: 0.10575, val loss: 0.10884\n",
      "Main effects training epoch: 152, train loss: 0.10606, val loss: 0.10908\n",
      "Main effects training epoch: 153, train loss: 0.10639, val loss: 0.10965\n",
      "Main effects training epoch: 154, train loss: 0.10596, val loss: 0.10920\n",
      "Main effects training epoch: 155, train loss: 0.10578, val loss: 0.10908\n",
      "Main effects training epoch: 156, train loss: 0.10569, val loss: 0.10881\n",
      "Main effects training epoch: 157, train loss: 0.10570, val loss: 0.10898\n",
      "Main effects training epoch: 158, train loss: 0.10595, val loss: 0.10931\n",
      "Main effects training epoch: 159, train loss: 0.10577, val loss: 0.10920\n",
      "Main effects training epoch: 160, train loss: 0.10578, val loss: 0.10935\n",
      "Main effects training epoch: 161, train loss: 0.10579, val loss: 0.10872\n",
      "Main effects training epoch: 162, train loss: 0.10575, val loss: 0.10904\n",
      "Main effects training epoch: 163, train loss: 0.10597, val loss: 0.10913\n",
      "Main effects training epoch: 164, train loss: 0.10605, val loss: 0.10923\n",
      "Main effects training epoch: 165, train loss: 0.10604, val loss: 0.10954\n",
      "Main effects training epoch: 166, train loss: 0.10575, val loss: 0.10883\n",
      "Main effects training epoch: 167, train loss: 0.10578, val loss: 0.10918\n",
      "Main effects training epoch: 168, train loss: 0.10576, val loss: 0.10888\n",
      "Early stop at epoch 168, with validation loss: 0.10888\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.10587, val loss: 0.10915\n",
      "Main effects tuning epoch: 2, train loss: 0.10583, val loss: 0.10863\n",
      "Main effects tuning epoch: 3, train loss: 0.10593, val loss: 0.10918\n",
      "Main effects tuning epoch: 4, train loss: 0.10580, val loss: 0.10863\n",
      "Main effects tuning epoch: 5, train loss: 0.10595, val loss: 0.10896\n",
      "Main effects tuning epoch: 6, train loss: 0.10614, val loss: 0.10907\n",
      "Main effects tuning epoch: 7, train loss: 0.10617, val loss: 0.10891\n",
      "Main effects tuning epoch: 8, train loss: 0.10639, val loss: 0.10952\n",
      "Main effects tuning epoch: 9, train loss: 0.10631, val loss: 0.10952\n",
      "Main effects tuning epoch: 10, train loss: 0.10598, val loss: 0.10913\n",
      "Main effects tuning epoch: 11, train loss: 0.10616, val loss: 0.10872\n",
      "Main effects tuning epoch: 12, train loss: 0.10613, val loss: 0.10940\n",
      "Main effects tuning epoch: 13, train loss: 0.10649, val loss: 0.10913\n",
      "Main effects tuning epoch: 14, train loss: 0.10595, val loss: 0.10906\n",
      "Main effects tuning epoch: 15, train loss: 0.10576, val loss: 0.10901\n",
      "Main effects tuning epoch: 16, train loss: 0.10591, val loss: 0.10867\n",
      "Main effects tuning epoch: 17, train loss: 0.10614, val loss: 0.10929\n",
      "Main effects tuning epoch: 18, train loss: 0.10616, val loss: 0.10914\n",
      "Main effects tuning epoch: 19, train loss: 0.10591, val loss: 0.10901\n",
      "Main effects tuning epoch: 20, train loss: 0.10585, val loss: 0.10893\n",
      "Main effects tuning epoch: 21, train loss: 0.10582, val loss: 0.10851\n",
      "Main effects tuning epoch: 22, train loss: 0.10595, val loss: 0.10917\n",
      "Main effects tuning epoch: 23, train loss: 0.10589, val loss: 0.10893\n",
      "Main effects tuning epoch: 24, train loss: 0.10579, val loss: 0.10899\n",
      "Main effects tuning epoch: 25, train loss: 0.10580, val loss: 0.10907\n",
      "Main effects tuning epoch: 26, train loss: 0.10612, val loss: 0.10929\n",
      "Main effects tuning epoch: 27, train loss: 0.10613, val loss: 0.10892\n",
      "Main effects tuning epoch: 28, train loss: 0.10608, val loss: 0.10898\n",
      "Main effects tuning epoch: 29, train loss: 0.10588, val loss: 0.10904\n",
      "Main effects tuning epoch: 30, train loss: 0.10603, val loss: 0.10875\n",
      "Main effects tuning epoch: 31, train loss: 0.10580, val loss: 0.10885\n",
      "Main effects tuning epoch: 32, train loss: 0.10586, val loss: 0.10892\n",
      "Main effects tuning epoch: 33, train loss: 0.10584, val loss: 0.10877\n",
      "Main effects tuning epoch: 34, train loss: 0.10610, val loss: 0.10890\n",
      "Main effects tuning epoch: 35, train loss: 0.10602, val loss: 0.10950\n",
      "Main effects tuning epoch: 36, train loss: 0.10577, val loss: 0.10870\n",
      "Main effects tuning epoch: 37, train loss: 0.10576, val loss: 0.10890\n",
      "Main effects tuning epoch: 38, train loss: 0.10590, val loss: 0.10894\n",
      "Main effects tuning epoch: 39, train loss: 0.10597, val loss: 0.10915\n",
      "Main effects tuning epoch: 40, train loss: 0.10575, val loss: 0.10856\n",
      "Main effects tuning epoch: 41, train loss: 0.10577, val loss: 0.10899\n",
      "Main effects tuning epoch: 42, train loss: 0.10600, val loss: 0.10888\n",
      "Main effects tuning epoch: 43, train loss: 0.10624, val loss: 0.10938\n",
      "Main effects tuning epoch: 44, train loss: 0.10606, val loss: 0.10863\n",
      "Main effects tuning epoch: 45, train loss: 0.10614, val loss: 0.10961\n",
      "Main effects tuning epoch: 46, train loss: 0.10598, val loss: 0.10874\n",
      "Main effects tuning epoch: 47, train loss: 0.10623, val loss: 0.10916\n",
      "Main effects tuning epoch: 48, train loss: 0.10628, val loss: 0.10910\n",
      "Main effects tuning epoch: 49, train loss: 0.10612, val loss: 0.10902\n",
      "Main effects tuning epoch: 50, train loss: 0.10600, val loss: 0.10922\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.16656, val loss: 0.16504\n",
      "Interaction training epoch: 2, train loss: 0.19799, val loss: 0.19730\n",
      "Interaction training epoch: 3, train loss: 0.07919, val loss: 0.08135\n",
      "Interaction training epoch: 4, train loss: 0.06592, val loss: 0.06695\n",
      "Interaction training epoch: 5, train loss: 0.06719, val loss: 0.06623\n",
      "Interaction training epoch: 6, train loss: 0.07116, val loss: 0.06992\n",
      "Interaction training epoch: 7, train loss: 0.05765, val loss: 0.05869\n",
      "Interaction training epoch: 8, train loss: 0.05791, val loss: 0.05822\n",
      "Interaction training epoch: 9, train loss: 0.06091, val loss: 0.06046\n",
      "Interaction training epoch: 10, train loss: 0.05754, val loss: 0.05788\n",
      "Interaction training epoch: 11, train loss: 0.05624, val loss: 0.05624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 12, train loss: 0.05651, val loss: 0.05600\n",
      "Interaction training epoch: 13, train loss: 0.06101, val loss: 0.06080\n",
      "Interaction training epoch: 14, train loss: 0.05709, val loss: 0.05641\n",
      "Interaction training epoch: 15, train loss: 0.06077, val loss: 0.06101\n",
      "Interaction training epoch: 16, train loss: 0.05239, val loss: 0.05197\n",
      "Interaction training epoch: 17, train loss: 0.05618, val loss: 0.05592\n",
      "Interaction training epoch: 18, train loss: 0.05477, val loss: 0.05468\n",
      "Interaction training epoch: 19, train loss: 0.05152, val loss: 0.05113\n",
      "Interaction training epoch: 20, train loss: 0.05519, val loss: 0.05482\n",
      "Interaction training epoch: 21, train loss: 0.05818, val loss: 0.05633\n",
      "Interaction training epoch: 22, train loss: 0.05614, val loss: 0.05550\n",
      "Interaction training epoch: 23, train loss: 0.05384, val loss: 0.05252\n",
      "Interaction training epoch: 24, train loss: 0.05143, val loss: 0.05136\n",
      "Interaction training epoch: 25, train loss: 0.05185, val loss: 0.05118\n",
      "Interaction training epoch: 26, train loss: 0.05409, val loss: 0.05515\n",
      "Interaction training epoch: 27, train loss: 0.05171, val loss: 0.05189\n",
      "Interaction training epoch: 28, train loss: 0.05284, val loss: 0.05307\n",
      "Interaction training epoch: 29, train loss: 0.05251, val loss: 0.05306\n",
      "Interaction training epoch: 30, train loss: 0.05654, val loss: 0.05667\n",
      "Interaction training epoch: 31, train loss: 0.05008, val loss: 0.05013\n",
      "Interaction training epoch: 32, train loss: 0.05165, val loss: 0.05175\n",
      "Interaction training epoch: 33, train loss: 0.05094, val loss: 0.05147\n",
      "Interaction training epoch: 34, train loss: 0.05154, val loss: 0.05255\n",
      "Interaction training epoch: 35, train loss: 0.05178, val loss: 0.05163\n",
      "Interaction training epoch: 36, train loss: 0.05189, val loss: 0.05137\n",
      "Interaction training epoch: 37, train loss: 0.05261, val loss: 0.05262\n",
      "Interaction training epoch: 38, train loss: 0.05395, val loss: 0.05444\n",
      "Interaction training epoch: 39, train loss: 0.05053, val loss: 0.05093\n",
      "Interaction training epoch: 40, train loss: 0.05186, val loss: 0.05165\n",
      "Interaction training epoch: 41, train loss: 0.05174, val loss: 0.05183\n",
      "Interaction training epoch: 42, train loss: 0.05645, val loss: 0.05707\n",
      "Interaction training epoch: 43, train loss: 0.05130, val loss: 0.05143\n",
      "Interaction training epoch: 44, train loss: 0.05233, val loss: 0.05213\n",
      "Interaction training epoch: 45, train loss: 0.05242, val loss: 0.05266\n",
      "Interaction training epoch: 46, train loss: 0.05234, val loss: 0.05256\n",
      "Interaction training epoch: 47, train loss: 0.05096, val loss: 0.05121\n",
      "Interaction training epoch: 48, train loss: 0.05324, val loss: 0.05317\n",
      "Interaction training epoch: 49, train loss: 0.05143, val loss: 0.05175\n",
      "Interaction training epoch: 50, train loss: 0.05906, val loss: 0.05866\n",
      "Interaction training epoch: 51, train loss: 0.05278, val loss: 0.05220\n",
      "Interaction training epoch: 52, train loss: 0.05429, val loss: 0.05321\n",
      "Interaction training epoch: 53, train loss: 0.05247, val loss: 0.05225\n",
      "Interaction training epoch: 54, train loss: 0.05061, val loss: 0.05129\n",
      "Interaction training epoch: 55, train loss: 0.05239, val loss: 0.05237\n",
      "Interaction training epoch: 56, train loss: 0.05174, val loss: 0.05287\n",
      "Interaction training epoch: 57, train loss: 0.05160, val loss: 0.05111\n",
      "Interaction training epoch: 58, train loss: 0.05180, val loss: 0.05292\n",
      "Interaction training epoch: 59, train loss: 0.04982, val loss: 0.05009\n",
      "Interaction training epoch: 60, train loss: 0.05227, val loss: 0.05368\n",
      "Interaction training epoch: 61, train loss: 0.05217, val loss: 0.05210\n",
      "Interaction training epoch: 62, train loss: 0.05023, val loss: 0.05074\n",
      "Interaction training epoch: 63, train loss: 0.05179, val loss: 0.05284\n",
      "Interaction training epoch: 64, train loss: 0.04959, val loss: 0.05129\n",
      "Interaction training epoch: 65, train loss: 0.05320, val loss: 0.05225\n",
      "Interaction training epoch: 66, train loss: 0.05295, val loss: 0.05326\n",
      "Interaction training epoch: 67, train loss: 0.05028, val loss: 0.05144\n",
      "Interaction training epoch: 68, train loss: 0.05112, val loss: 0.05151\n",
      "Interaction training epoch: 69, train loss: 0.05228, val loss: 0.05248\n",
      "Interaction training epoch: 70, train loss: 0.04994, val loss: 0.05062\n",
      "Interaction training epoch: 71, train loss: 0.05225, val loss: 0.05286\n",
      "Interaction training epoch: 72, train loss: 0.05069, val loss: 0.05180\n",
      "Interaction training epoch: 73, train loss: 0.05077, val loss: 0.05196\n",
      "Interaction training epoch: 74, train loss: 0.04967, val loss: 0.05076\n",
      "Interaction training epoch: 75, train loss: 0.05064, val loss: 0.05071\n",
      "Interaction training epoch: 76, train loss: 0.04886, val loss: 0.05004\n",
      "Interaction training epoch: 77, train loss: 0.05097, val loss: 0.05075\n",
      "Interaction training epoch: 78, train loss: 0.05171, val loss: 0.05128\n",
      "Interaction training epoch: 79, train loss: 0.05178, val loss: 0.05168\n",
      "Interaction training epoch: 80, train loss: 0.04972, val loss: 0.05111\n",
      "Interaction training epoch: 81, train loss: 0.05307, val loss: 0.05407\n",
      "Interaction training epoch: 82, train loss: 0.05287, val loss: 0.05331\n",
      "Interaction training epoch: 83, train loss: 0.05019, val loss: 0.05065\n",
      "Interaction training epoch: 84, train loss: 0.05109, val loss: 0.05177\n",
      "Interaction training epoch: 85, train loss: 0.05241, val loss: 0.05395\n",
      "Interaction training epoch: 86, train loss: 0.05169, val loss: 0.05187\n",
      "Interaction training epoch: 87, train loss: 0.05013, val loss: 0.05031\n",
      "Interaction training epoch: 88, train loss: 0.05249, val loss: 0.05324\n",
      "Interaction training epoch: 89, train loss: 0.04905, val loss: 0.04955\n",
      "Interaction training epoch: 90, train loss: 0.05048, val loss: 0.05104\n",
      "Interaction training epoch: 91, train loss: 0.05291, val loss: 0.05320\n",
      "Interaction training epoch: 92, train loss: 0.05194, val loss: 0.05184\n",
      "Interaction training epoch: 93, train loss: 0.04813, val loss: 0.04857\n",
      "Interaction training epoch: 94, train loss: 0.05434, val loss: 0.05466\n",
      "Interaction training epoch: 95, train loss: 0.05394, val loss: 0.05306\n",
      "Interaction training epoch: 96, train loss: 0.04954, val loss: 0.04995\n",
      "Interaction training epoch: 97, train loss: 0.05003, val loss: 0.05058\n",
      "Interaction training epoch: 98, train loss: 0.05197, val loss: 0.05219\n",
      "Interaction training epoch: 99, train loss: 0.05436, val loss: 0.05280\n",
      "Interaction training epoch: 100, train loss: 0.05350, val loss: 0.05437\n",
      "Interaction training epoch: 101, train loss: 0.04964, val loss: 0.04980\n",
      "Interaction training epoch: 102, train loss: 0.05136, val loss: 0.05087\n",
      "Interaction training epoch: 103, train loss: 0.05162, val loss: 0.05191\n",
      "Interaction training epoch: 104, train loss: 0.05118, val loss: 0.05150\n",
      "Interaction training epoch: 105, train loss: 0.04993, val loss: 0.04937\n",
      "Interaction training epoch: 106, train loss: 0.05171, val loss: 0.05164\n",
      "Interaction training epoch: 107, train loss: 0.05193, val loss: 0.05075\n",
      "Interaction training epoch: 108, train loss: 0.05102, val loss: 0.05183\n",
      "Interaction training epoch: 109, train loss: 0.05261, val loss: 0.05324\n",
      "Interaction training epoch: 110, train loss: 0.05047, val loss: 0.05100\n",
      "Interaction training epoch: 111, train loss: 0.05046, val loss: 0.05063\n",
      "Interaction training epoch: 112, train loss: 0.04811, val loss: 0.04809\n",
      "Interaction training epoch: 113, train loss: 0.05250, val loss: 0.05298\n",
      "Interaction training epoch: 114, train loss: 0.05036, val loss: 0.05112\n",
      "Interaction training epoch: 115, train loss: 0.05119, val loss: 0.05140\n",
      "Interaction training epoch: 116, train loss: 0.04975, val loss: 0.05112\n",
      "Interaction training epoch: 117, train loss: 0.04911, val loss: 0.04994\n",
      "Interaction training epoch: 118, train loss: 0.04990, val loss: 0.05012\n",
      "Interaction training epoch: 119, train loss: 0.04854, val loss: 0.04775\n",
      "Interaction training epoch: 120, train loss: 0.04985, val loss: 0.04987\n",
      "Interaction training epoch: 121, train loss: 0.04779, val loss: 0.04856\n",
      "Interaction training epoch: 122, train loss: 0.04712, val loss: 0.04773\n",
      "Interaction training epoch: 123, train loss: 0.05164, val loss: 0.05255\n",
      "Interaction training epoch: 124, train loss: 0.05327, val loss: 0.05365\n",
      "Interaction training epoch: 125, train loss: 0.05348, val loss: 0.05414\n",
      "Interaction training epoch: 126, train loss: 0.04873, val loss: 0.04978\n",
      "Interaction training epoch: 127, train loss: 0.04866, val loss: 0.04894\n",
      "Interaction training epoch: 128, train loss: 0.04947, val loss: 0.05037\n",
      "Interaction training epoch: 129, train loss: 0.04701, val loss: 0.04711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 130, train loss: 0.04867, val loss: 0.04878\n",
      "Interaction training epoch: 131, train loss: 0.04995, val loss: 0.05004\n",
      "Interaction training epoch: 132, train loss: 0.05004, val loss: 0.05008\n",
      "Interaction training epoch: 133, train loss: 0.04970, val loss: 0.05023\n",
      "Interaction training epoch: 134, train loss: 0.04969, val loss: 0.05041\n",
      "Interaction training epoch: 135, train loss: 0.04826, val loss: 0.04935\n",
      "Interaction training epoch: 136, train loss: 0.04956, val loss: 0.05077\n",
      "Interaction training epoch: 137, train loss: 0.04937, val loss: 0.05116\n",
      "Interaction training epoch: 138, train loss: 0.05042, val loss: 0.05126\n",
      "Interaction training epoch: 139, train loss: 0.04910, val loss: 0.04944\n",
      "Interaction training epoch: 140, train loss: 0.05235, val loss: 0.05329\n",
      "Interaction training epoch: 141, train loss: 0.05151, val loss: 0.05199\n",
      "Interaction training epoch: 142, train loss: 0.04935, val loss: 0.04965\n",
      "Interaction training epoch: 143, train loss: 0.05663, val loss: 0.05667\n",
      "Interaction training epoch: 144, train loss: 0.05217, val loss: 0.05297\n",
      "Interaction training epoch: 145, train loss: 0.05139, val loss: 0.05143\n",
      "Interaction training epoch: 146, train loss: 0.05350, val loss: 0.05388\n",
      "Interaction training epoch: 147, train loss: 0.06195, val loss: 0.06171\n",
      "Interaction training epoch: 148, train loss: 0.05923, val loss: 0.05958\n",
      "Interaction training epoch: 149, train loss: 0.05897, val loss: 0.05842\n",
      "Interaction training epoch: 150, train loss: 0.05688, val loss: 0.05663\n",
      "Interaction training epoch: 151, train loss: 0.05292, val loss: 0.05339\n",
      "Interaction training epoch: 152, train loss: 0.04638, val loss: 0.04708\n",
      "Interaction training epoch: 153, train loss: 0.04830, val loss: 0.05070\n",
      "Interaction training epoch: 154, train loss: 0.04798, val loss: 0.04952\n",
      "Interaction training epoch: 155, train loss: 0.04659, val loss: 0.04786\n",
      "Interaction training epoch: 156, train loss: 0.04767, val loss: 0.04876\n",
      "Interaction training epoch: 157, train loss: 0.04853, val loss: 0.04917\n",
      "Interaction training epoch: 158, train loss: 0.04677, val loss: 0.04803\n",
      "Interaction training epoch: 159, train loss: 0.04901, val loss: 0.04931\n",
      "Interaction training epoch: 160, train loss: 0.04754, val loss: 0.04893\n",
      "Interaction training epoch: 161, train loss: 0.04874, val loss: 0.04950\n",
      "Interaction training epoch: 162, train loss: 0.04595, val loss: 0.04721\n",
      "Interaction training epoch: 163, train loss: 0.04732, val loss: 0.04956\n",
      "Interaction training epoch: 164, train loss: 0.04567, val loss: 0.04749\n",
      "Interaction training epoch: 165, train loss: 0.05013, val loss: 0.05115\n",
      "Interaction training epoch: 166, train loss: 0.04663, val loss: 0.04790\n",
      "Interaction training epoch: 167, train loss: 0.04602, val loss: 0.04784\n",
      "Interaction training epoch: 168, train loss: 0.04789, val loss: 0.04800\n",
      "Interaction training epoch: 169, train loss: 0.04480, val loss: 0.04615\n",
      "Interaction training epoch: 170, train loss: 0.04657, val loss: 0.04712\n",
      "Interaction training epoch: 171, train loss: 0.04515, val loss: 0.04688\n",
      "Interaction training epoch: 172, train loss: 0.04745, val loss: 0.04904\n",
      "Interaction training epoch: 173, train loss: 0.04801, val loss: 0.05050\n",
      "Interaction training epoch: 174, train loss: 0.05297, val loss: 0.05389\n",
      "Interaction training epoch: 175, train loss: 0.04522, val loss: 0.04687\n",
      "Interaction training epoch: 176, train loss: 0.04654, val loss: 0.04709\n",
      "Interaction training epoch: 177, train loss: 0.04462, val loss: 0.04576\n",
      "Interaction training epoch: 178, train loss: 0.04715, val loss: 0.04880\n",
      "Interaction training epoch: 179, train loss: 0.04515, val loss: 0.04673\n",
      "Interaction training epoch: 180, train loss: 0.04652, val loss: 0.04824\n",
      "Interaction training epoch: 181, train loss: 0.04757, val loss: 0.04936\n",
      "Interaction training epoch: 182, train loss: 0.04771, val loss: 0.04909\n",
      "Interaction training epoch: 183, train loss: 0.04999, val loss: 0.05186\n",
      "Interaction training epoch: 184, train loss: 0.04691, val loss: 0.04877\n",
      "Interaction training epoch: 185, train loss: 0.04599, val loss: 0.04763\n",
      "Interaction training epoch: 186, train loss: 0.04839, val loss: 0.04969\n",
      "Interaction training epoch: 187, train loss: 0.04634, val loss: 0.04846\n",
      "Interaction training epoch: 188, train loss: 0.04686, val loss: 0.04828\n",
      "Interaction training epoch: 189, train loss: 0.04527, val loss: 0.04603\n",
      "Interaction training epoch: 190, train loss: 0.04811, val loss: 0.05046\n",
      "Interaction training epoch: 191, train loss: 0.04668, val loss: 0.04794\n",
      "Interaction training epoch: 192, train loss: 0.04580, val loss: 0.04673\n",
      "Interaction training epoch: 193, train loss: 0.04953, val loss: 0.05077\n",
      "Interaction training epoch: 194, train loss: 0.04637, val loss: 0.04745\n",
      "Interaction training epoch: 195, train loss: 0.05291, val loss: 0.05606\n",
      "Interaction training epoch: 196, train loss: 0.04552, val loss: 0.04706\n",
      "Interaction training epoch: 197, train loss: 0.04954, val loss: 0.05175\n",
      "Interaction training epoch: 198, train loss: 0.04504, val loss: 0.04659\n",
      "Interaction training epoch: 199, train loss: 0.04939, val loss: 0.05068\n",
      "Interaction training epoch: 200, train loss: 0.04824, val loss: 0.04969\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########7 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.04624, val loss: 0.04648\n",
      "Interaction tuning epoch: 2, train loss: 0.04654, val loss: 0.04675\n",
      "Interaction tuning epoch: 3, train loss: 0.04688, val loss: 0.04770\n",
      "Interaction tuning epoch: 4, train loss: 0.04703, val loss: 0.04714\n",
      "Interaction tuning epoch: 5, train loss: 0.04871, val loss: 0.04870\n",
      "Interaction tuning epoch: 6, train loss: 0.04715, val loss: 0.04685\n",
      "Interaction tuning epoch: 7, train loss: 0.04753, val loss: 0.04769\n",
      "Interaction tuning epoch: 8, train loss: 0.04804, val loss: 0.04839\n",
      "Interaction tuning epoch: 9, train loss: 0.04656, val loss: 0.04746\n",
      "Interaction tuning epoch: 10, train loss: 0.04708, val loss: 0.04792\n",
      "Interaction tuning epoch: 11, train loss: 0.04593, val loss: 0.04607\n",
      "Interaction tuning epoch: 12, train loss: 0.04889, val loss: 0.05019\n",
      "Interaction tuning epoch: 13, train loss: 0.04756, val loss: 0.04795\n",
      "Interaction tuning epoch: 14, train loss: 0.05056, val loss: 0.04891\n",
      "Interaction tuning epoch: 15, train loss: 0.05649, val loss: 0.05700\n",
      "Interaction tuning epoch: 16, train loss: 0.04770, val loss: 0.04777\n",
      "Interaction tuning epoch: 17, train loss: 0.04665, val loss: 0.04734\n",
      "Interaction tuning epoch: 18, train loss: 0.04709, val loss: 0.04763\n",
      "Interaction tuning epoch: 19, train loss: 0.04762, val loss: 0.04775\n",
      "Interaction tuning epoch: 20, train loss: 0.04647, val loss: 0.04691\n",
      "Interaction tuning epoch: 21, train loss: 0.04858, val loss: 0.04904\n",
      "Interaction tuning epoch: 22, train loss: 0.04763, val loss: 0.04743\n",
      "Interaction tuning epoch: 23, train loss: 0.04883, val loss: 0.04879\n",
      "Interaction tuning epoch: 24, train loss: 0.04608, val loss: 0.04728\n",
      "Interaction tuning epoch: 25, train loss: 0.04999, val loss: 0.04940\n",
      "Interaction tuning epoch: 26, train loss: 0.04916, val loss: 0.04857\n",
      "Interaction tuning epoch: 27, train loss: 0.04784, val loss: 0.04855\n",
      "Interaction tuning epoch: 28, train loss: 0.04702, val loss: 0.04758\n",
      "Interaction tuning epoch: 29, train loss: 0.04685, val loss: 0.04720\n",
      "Interaction tuning epoch: 30, train loss: 0.04688, val loss: 0.04800\n",
      "Interaction tuning epoch: 31, train loss: 0.04656, val loss: 0.04725\n",
      "Interaction tuning epoch: 32, train loss: 0.04698, val loss: 0.04718\n",
      "Interaction tuning epoch: 33, train loss: 0.04648, val loss: 0.04674\n",
      "Interaction tuning epoch: 34, train loss: 0.04622, val loss: 0.04627\n",
      "Interaction tuning epoch: 35, train loss: 0.04695, val loss: 0.04653\n",
      "Interaction tuning epoch: 36, train loss: 0.04638, val loss: 0.04708\n",
      "Interaction tuning epoch: 37, train loss: 0.04517, val loss: 0.04599\n",
      "Interaction tuning epoch: 38, train loss: 0.04594, val loss: 0.04626\n",
      "Interaction tuning epoch: 39, train loss: 0.04806, val loss: 0.04884\n",
      "Interaction tuning epoch: 40, train loss: 0.04762, val loss: 0.04781\n",
      "Interaction tuning epoch: 41, train loss: 0.04762, val loss: 0.04909\n",
      "Interaction tuning epoch: 42, train loss: 0.04746, val loss: 0.04805\n",
      "Interaction tuning epoch: 43, train loss: 0.04604, val loss: 0.04612\n",
      "Interaction tuning epoch: 44, train loss: 0.04636, val loss: 0.04673\n",
      "Interaction tuning epoch: 45, train loss: 0.04643, val loss: 0.04712\n",
      "Interaction tuning epoch: 46, train loss: 0.04656, val loss: 0.04678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 47, train loss: 0.04635, val loss: 0.04667\n",
      "Interaction tuning epoch: 48, train loss: 0.04816, val loss: 0.04882\n",
      "Interaction tuning epoch: 49, train loss: 0.04683, val loss: 0.04711\n",
      "Interaction tuning epoch: 50, train loss: 0.04845, val loss: 0.04905\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 36.098867654800415\n",
      "After the gam stage, training error is 0.04845 , validation error is 0.04905\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 1.232198\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.039195 validation MAE=0.046481,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.036071 validation MAE=0.045379,rank=5\n",
      "[SoftImpute] Iter 3: observed MAE=0.033524 validation MAE=0.044418,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.031399 validation MAE=0.043560,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.029603 validation MAE=0.042803,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.028081 validation MAE=0.042139,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.026758 validation MAE=0.041551,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 8: observed MAE=0.025597 validation MAE=0.041019,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.024567 validation MAE=0.040523,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.023644 validation MAE=0.040091,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.022812 validation MAE=0.039694,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.022061 validation MAE=0.039328,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.021377 validation MAE=0.038993,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.020750 validation MAE=0.038676,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.020176 validation MAE=0.038375,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.019647 validation MAE=0.038087,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.019158 validation MAE=0.037815,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.018703 validation MAE=0.037551,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.018285 validation MAE=0.037301,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.017896 validation MAE=0.037067,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.017531 validation MAE=0.036850,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.017189 validation MAE=0.036644,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.016867 validation MAE=0.036444,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.016566 validation MAE=0.036248,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.016282 validation MAE=0.036060,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.016013 validation MAE=0.035877,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.015758 validation MAE=0.035706,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.015517 validation MAE=0.035539,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.015288 validation MAE=0.035372,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.015070 validation MAE=0.035210,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.014861 validation MAE=0.035051,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.014662 validation MAE=0.034896,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.014471 validation MAE=0.034742,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.014288 validation MAE=0.034591,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.014112 validation MAE=0.034443,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.013943 validation MAE=0.034298,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.013781 validation MAE=0.034158,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.013625 validation MAE=0.034027,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.013476 validation MAE=0.033898,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.013332 validation MAE=0.033770,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.013194 validation MAE=0.033643,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.013060 validation MAE=0.033520,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.012931 validation MAE=0.033405,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.012806 validation MAE=0.033290,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.012684 validation MAE=0.033175,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.012568 validation MAE=0.033062,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.012455 validation MAE=0.032951,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.012346 validation MAE=0.032842,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.012240 validation MAE=0.032735,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.012137 validation MAE=0.032628,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.012036 validation MAE=0.032523,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.011939 validation MAE=0.032421,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.011845 validation MAE=0.032319,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.011753 validation MAE=0.032220,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.011664 validation MAE=0.032126,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.011577 validation MAE=0.032035,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.011494 validation MAE=0.031944,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.011412 validation MAE=0.031854,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.011332 validation MAE=0.031764,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.011254 validation MAE=0.031676,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.011178 validation MAE=0.031588,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.011104 validation MAE=0.031500,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.011032 validation MAE=0.031414,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.010961 validation MAE=0.031329,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.010892 validation MAE=0.031246,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.010825 validation MAE=0.031164,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.010759 validation MAE=0.031083,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.010695 validation MAE=0.031003,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.010632 validation MAE=0.030925,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.010570 validation MAE=0.030847,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.010510 validation MAE=0.030769,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.010452 validation MAE=0.030691,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.010394 validation MAE=0.030614,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.010338 validation MAE=0.030538,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.010283 validation MAE=0.030463,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.010229 validation MAE=0.030388,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.010176 validation MAE=0.030314,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.010124 validation MAE=0.030241,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.010073 validation MAE=0.030169,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.010024 validation MAE=0.030097,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.009975 validation MAE=0.030026,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.009927 validation MAE=0.029957,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.009880 validation MAE=0.029887,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.009834 validation MAE=0.029819,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.009789 validation MAE=0.029752,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.009745 validation MAE=0.029685,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.009701 validation MAE=0.029619,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.009659 validation MAE=0.029553,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.009617 validation MAE=0.029488,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.009576 validation MAE=0.029424,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.009536 validation MAE=0.029362,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.009496 validation MAE=0.029302,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.009457 validation MAE=0.029243,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.009418 validation MAE=0.029185,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.009381 validation MAE=0.029127,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.009344 validation MAE=0.029070,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.009308 validation MAE=0.029015,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.009272 validation MAE=0.028960,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.009237 validation MAE=0.028906,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.009203 validation MAE=0.028853,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.009169 validation MAE=0.028802,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.009135 validation MAE=0.028752,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.009103 validation MAE=0.028701,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.009070 validation MAE=0.028651,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.009039 validation MAE=0.028602,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.009007 validation MAE=0.028553,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.008976 validation MAE=0.028504,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.008946 validation MAE=0.028456,rank=5\n",
      "[SoftImpute] Iter 109: observed MAE=0.008916 validation MAE=0.028408,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.008887 validation MAE=0.028361,rank=5\n",
      "[SoftImpute] Iter 111: observed MAE=0.008858 validation MAE=0.028314,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.008829 validation MAE=0.028268,rank=5\n",
      "[SoftImpute] Iter 113: observed MAE=0.008802 validation MAE=0.028223,rank=5\n",
      "[SoftImpute] Iter 114: observed MAE=0.008774 validation MAE=0.028178,rank=5\n",
      "[SoftImpute] Iter 115: observed MAE=0.008747 validation MAE=0.028133,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.008720 validation MAE=0.028088,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.008694 validation MAE=0.028043,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.008669 validation MAE=0.027998,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.008643 validation MAE=0.027954,rank=5\n",
      "[SoftImpute] Iter 120: observed MAE=0.008619 validation MAE=0.027910,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.008594 validation MAE=0.027866,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 122: observed MAE=0.008570 validation MAE=0.027823,rank=5\n",
      "[SoftImpute] Iter 123: observed MAE=0.008546 validation MAE=0.027780,rank=5\n",
      "[SoftImpute] Iter 124: observed MAE=0.008523 validation MAE=0.027738,rank=5\n",
      "[SoftImpute] Iter 125: observed MAE=0.008500 validation MAE=0.027695,rank=5\n",
      "[SoftImpute] Iter 126: observed MAE=0.008477 validation MAE=0.027653,rank=5\n",
      "[SoftImpute] Iter 127: observed MAE=0.008455 validation MAE=0.027612,rank=5\n",
      "[SoftImpute] Iter 128: observed MAE=0.008433 validation MAE=0.027571,rank=5\n",
      "[SoftImpute] Iter 129: observed MAE=0.008411 validation MAE=0.027530,rank=5\n",
      "[SoftImpute] Iter 130: observed MAE=0.008389 validation MAE=0.027490,rank=5\n",
      "[SoftImpute] Iter 131: observed MAE=0.008368 validation MAE=0.027451,rank=5\n",
      "[SoftImpute] Iter 132: observed MAE=0.008347 validation MAE=0.027414,rank=5\n",
      "[SoftImpute] Iter 133: observed MAE=0.008327 validation MAE=0.027376,rank=5\n",
      "[SoftImpute] Iter 134: observed MAE=0.008307 validation MAE=0.027339,rank=5\n",
      "[SoftImpute] Iter 135: observed MAE=0.008287 validation MAE=0.027302,rank=5\n",
      "[SoftImpute] Iter 136: observed MAE=0.008267 validation MAE=0.027265,rank=5\n",
      "[SoftImpute] Iter 137: observed MAE=0.008248 validation MAE=0.027229,rank=5\n",
      "[SoftImpute] Iter 138: observed MAE=0.008229 validation MAE=0.027192,rank=5\n",
      "[SoftImpute] Iter 139: observed MAE=0.008210 validation MAE=0.027156,rank=5\n",
      "[SoftImpute] Iter 140: observed MAE=0.008191 validation MAE=0.027121,rank=5\n",
      "[SoftImpute] Iter 141: observed MAE=0.008173 validation MAE=0.027085,rank=5\n",
      "[SoftImpute] Iter 142: observed MAE=0.008155 validation MAE=0.027050,rank=5\n",
      "[SoftImpute] Iter 143: observed MAE=0.008137 validation MAE=0.027015,rank=5\n",
      "[SoftImpute] Iter 144: observed MAE=0.008119 validation MAE=0.026980,rank=5\n",
      "[SoftImpute] Iter 145: observed MAE=0.008102 validation MAE=0.026945,rank=5\n",
      "[SoftImpute] Iter 146: observed MAE=0.008085 validation MAE=0.026911,rank=5\n",
      "[SoftImpute] Iter 147: observed MAE=0.008068 validation MAE=0.026876,rank=5\n",
      "[SoftImpute] Iter 148: observed MAE=0.008052 validation MAE=0.026843,rank=5\n",
      "[SoftImpute] Iter 149: observed MAE=0.008035 validation MAE=0.026809,rank=5\n",
      "[SoftImpute] Iter 150: observed MAE=0.008019 validation MAE=0.026775,rank=5\n",
      "[SoftImpute] Iter 151: observed MAE=0.008003 validation MAE=0.026742,rank=5\n",
      "[SoftImpute] Iter 152: observed MAE=0.007988 validation MAE=0.026709,rank=5\n",
      "[SoftImpute] Iter 153: observed MAE=0.007973 validation MAE=0.026677,rank=5\n",
      "[SoftImpute] Iter 154: observed MAE=0.007957 validation MAE=0.026645,rank=5\n",
      "[SoftImpute] Iter 155: observed MAE=0.007943 validation MAE=0.026614,rank=5\n",
      "[SoftImpute] Iter 156: observed MAE=0.007928 validation MAE=0.026582,rank=5\n",
      "[SoftImpute] Iter 157: observed MAE=0.007913 validation MAE=0.026551,rank=5\n",
      "[SoftImpute] Iter 158: observed MAE=0.007899 validation MAE=0.026520,rank=5\n",
      "[SoftImpute] Iter 159: observed MAE=0.007884 validation MAE=0.026488,rank=5\n",
      "[SoftImpute] Iter 160: observed MAE=0.007870 validation MAE=0.026457,rank=5\n",
      "[SoftImpute] Iter 161: observed MAE=0.007857 validation MAE=0.026426,rank=5\n",
      "[SoftImpute] Iter 162: observed MAE=0.007843 validation MAE=0.026395,rank=5\n",
      "[SoftImpute] Iter 163: observed MAE=0.007829 validation MAE=0.026364,rank=5\n",
      "[SoftImpute] Iter 164: observed MAE=0.007816 validation MAE=0.026333,rank=5\n",
      "[SoftImpute] Iter 165: observed MAE=0.007803 validation MAE=0.026302,rank=5\n",
      "[SoftImpute] Iter 166: observed MAE=0.007790 validation MAE=0.026272,rank=5\n",
      "[SoftImpute] Iter 167: observed MAE=0.007777 validation MAE=0.026242,rank=5\n",
      "[SoftImpute] Iter 168: observed MAE=0.007764 validation MAE=0.026212,rank=5\n",
      "[SoftImpute] Iter 169: observed MAE=0.007752 validation MAE=0.026182,rank=5\n",
      "[SoftImpute] Iter 170: observed MAE=0.007739 validation MAE=0.026152,rank=5\n",
      "[SoftImpute] Iter 171: observed MAE=0.007727 validation MAE=0.026122,rank=5\n",
      "[SoftImpute] Iter 172: observed MAE=0.007715 validation MAE=0.026093,rank=5\n",
      "[SoftImpute] Iter 173: observed MAE=0.007703 validation MAE=0.026064,rank=5\n",
      "[SoftImpute] Iter 174: observed MAE=0.007691 validation MAE=0.026035,rank=5\n",
      "[SoftImpute] Iter 175: observed MAE=0.007679 validation MAE=0.026006,rank=5\n",
      "[SoftImpute] Iter 176: observed MAE=0.007667 validation MAE=0.025977,rank=5\n",
      "[SoftImpute] Iter 177: observed MAE=0.007656 validation MAE=0.025949,rank=5\n",
      "[SoftImpute] Iter 178: observed MAE=0.007644 validation MAE=0.025920,rank=5\n",
      "[SoftImpute] Iter 179: observed MAE=0.007633 validation MAE=0.025891,rank=5\n",
      "[SoftImpute] Iter 180: observed MAE=0.007622 validation MAE=0.025863,rank=5\n",
      "[SoftImpute] Iter 181: observed MAE=0.007611 validation MAE=0.025834,rank=5\n",
      "[SoftImpute] Iter 182: observed MAE=0.007600 validation MAE=0.025806,rank=5\n",
      "[SoftImpute] Iter 183: observed MAE=0.007589 validation MAE=0.025778,rank=5\n",
      "[SoftImpute] Iter 184: observed MAE=0.007578 validation MAE=0.025749,rank=5\n",
      "[SoftImpute] Iter 185: observed MAE=0.007567 validation MAE=0.025721,rank=5\n",
      "[SoftImpute] Iter 186: observed MAE=0.007557 validation MAE=0.025693,rank=5\n",
      "[SoftImpute] Iter 187: observed MAE=0.007546 validation MAE=0.025666,rank=5\n",
      "[SoftImpute] Iter 188: observed MAE=0.007536 validation MAE=0.025638,rank=5\n",
      "[SoftImpute] Iter 189: observed MAE=0.007526 validation MAE=0.025611,rank=5\n",
      "[SoftImpute] Iter 190: observed MAE=0.007515 validation MAE=0.025584,rank=5\n",
      "[SoftImpute] Iter 191: observed MAE=0.007505 validation MAE=0.025557,rank=5\n",
      "[SoftImpute] Iter 192: observed MAE=0.007495 validation MAE=0.025530,rank=5\n",
      "[SoftImpute] Iter 193: observed MAE=0.007485 validation MAE=0.025503,rank=5\n",
      "[SoftImpute] Iter 194: observed MAE=0.007475 validation MAE=0.025476,rank=5\n",
      "[SoftImpute] Iter 195: observed MAE=0.007466 validation MAE=0.025449,rank=5\n",
      "[SoftImpute] Iter 196: observed MAE=0.007456 validation MAE=0.025423,rank=5\n",
      "[SoftImpute] Iter 197: observed MAE=0.007447 validation MAE=0.025397,rank=5\n",
      "[SoftImpute] Iter 198: observed MAE=0.007437 validation MAE=0.025370,rank=5\n",
      "[SoftImpute] Iter 199: observed MAE=0.007428 validation MAE=0.025345,rank=5\n",
      "[SoftImpute] Iter 200: observed MAE=0.007419 validation MAE=0.025319,rank=5\n",
      "[SoftImpute] Iter 201: observed MAE=0.007409 validation MAE=0.025293,rank=5\n",
      "[SoftImpute] Iter 202: observed MAE=0.007400 validation MAE=0.025267,rank=5\n",
      "[SoftImpute] Iter 203: observed MAE=0.007391 validation MAE=0.025242,rank=5\n",
      "[SoftImpute] Iter 204: observed MAE=0.007382 validation MAE=0.025216,rank=5\n",
      "[SoftImpute] Iter 205: observed MAE=0.007373 validation MAE=0.025191,rank=5\n",
      "[SoftImpute] Iter 206: observed MAE=0.007364 validation MAE=0.025166,rank=5\n",
      "[SoftImpute] Iter 207: observed MAE=0.007356 validation MAE=0.025141,rank=5\n",
      "[SoftImpute] Iter 208: observed MAE=0.007347 validation MAE=0.025116,rank=5\n",
      "[SoftImpute] Iter 209: observed MAE=0.007338 validation MAE=0.025092,rank=5\n",
      "[SoftImpute] Iter 210: observed MAE=0.007330 validation MAE=0.025067,rank=5\n",
      "[SoftImpute] Iter 211: observed MAE=0.007321 validation MAE=0.025043,rank=5\n",
      "[SoftImpute] Iter 212: observed MAE=0.007313 validation MAE=0.025018,rank=5\n",
      "[SoftImpute] Iter 213: observed MAE=0.007304 validation MAE=0.024993,rank=5\n",
      "[SoftImpute] Iter 214: observed MAE=0.007296 validation MAE=0.024969,rank=5\n",
      "[SoftImpute] Iter 215: observed MAE=0.007288 validation MAE=0.024944,rank=5\n",
      "[SoftImpute] Iter 216: observed MAE=0.007280 validation MAE=0.024920,rank=5\n",
      "[SoftImpute] Iter 217: observed MAE=0.007272 validation MAE=0.024896,rank=5\n",
      "[SoftImpute] Iter 218: observed MAE=0.007264 validation MAE=0.024872,rank=5\n",
      "[SoftImpute] Iter 219: observed MAE=0.007256 validation MAE=0.024847,rank=5\n",
      "[SoftImpute] Iter 220: observed MAE=0.007248 validation MAE=0.024823,rank=5\n",
      "[SoftImpute] Iter 221: observed MAE=0.007240 validation MAE=0.024800,rank=5\n",
      "[SoftImpute] Iter 222: observed MAE=0.007232 validation MAE=0.024776,rank=5\n",
      "[SoftImpute] Iter 223: observed MAE=0.007224 validation MAE=0.024753,rank=5\n",
      "[SoftImpute] Iter 224: observed MAE=0.007217 validation MAE=0.024730,rank=5\n",
      "[SoftImpute] Iter 225: observed MAE=0.007209 validation MAE=0.024707,rank=5\n",
      "[SoftImpute] Iter 226: observed MAE=0.007202 validation MAE=0.024684,rank=5\n",
      "[SoftImpute] Iter 227: observed MAE=0.007194 validation MAE=0.024661,rank=5\n",
      "[SoftImpute] Iter 228: observed MAE=0.007187 validation MAE=0.024638,rank=5\n",
      "[SoftImpute] Iter 229: observed MAE=0.007179 validation MAE=0.024615,rank=5\n",
      "[SoftImpute] Iter 230: observed MAE=0.007172 validation MAE=0.024593,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 231: observed MAE=0.007164 validation MAE=0.024570,rank=5\n",
      "[SoftImpute] Iter 232: observed MAE=0.007157 validation MAE=0.024548,rank=5\n",
      "[SoftImpute] Iter 233: observed MAE=0.007150 validation MAE=0.024525,rank=5\n",
      "[SoftImpute] Iter 234: observed MAE=0.007143 validation MAE=0.024503,rank=5\n",
      "[SoftImpute] Iter 235: observed MAE=0.007135 validation MAE=0.024480,rank=5\n",
      "[SoftImpute] Iter 236: observed MAE=0.007128 validation MAE=0.024458,rank=5\n",
      "[SoftImpute] Iter 237: observed MAE=0.007121 validation MAE=0.024436,rank=5\n",
      "[SoftImpute] Iter 238: observed MAE=0.007114 validation MAE=0.024413,rank=5\n",
      "[SoftImpute] Iter 239: observed MAE=0.007107 validation MAE=0.024391,rank=5\n",
      "[SoftImpute] Iter 240: observed MAE=0.007100 validation MAE=0.024369,rank=5\n",
      "[SoftImpute] Iter 241: observed MAE=0.007093 validation MAE=0.024347,rank=5\n",
      "[SoftImpute] Iter 242: observed MAE=0.007086 validation MAE=0.024325,rank=5\n",
      "[SoftImpute] Iter 243: observed MAE=0.007079 validation MAE=0.024302,rank=5\n",
      "[SoftImpute] Iter 244: observed MAE=0.007072 validation MAE=0.024280,rank=5\n",
      "[SoftImpute] Iter 245: observed MAE=0.007065 validation MAE=0.024258,rank=5\n",
      "[SoftImpute] Iter 246: observed MAE=0.007058 validation MAE=0.024236,rank=5\n",
      "[SoftImpute] Iter 247: observed MAE=0.007051 validation MAE=0.024214,rank=5\n",
      "[SoftImpute] Iter 248: observed MAE=0.007044 validation MAE=0.024192,rank=5\n",
      "[SoftImpute] Iter 249: observed MAE=0.007038 validation MAE=0.024171,rank=5\n",
      "[SoftImpute] Iter 250: observed MAE=0.007031 validation MAE=0.024149,rank=5\n",
      "[SoftImpute] Iter 251: observed MAE=0.007024 validation MAE=0.024128,rank=5\n",
      "[SoftImpute] Iter 252: observed MAE=0.007017 validation MAE=0.024106,rank=5\n",
      "[SoftImpute] Iter 253: observed MAE=0.007011 validation MAE=0.024085,rank=5\n",
      "[SoftImpute] Iter 254: observed MAE=0.007004 validation MAE=0.024064,rank=5\n",
      "[SoftImpute] Iter 255: observed MAE=0.006997 validation MAE=0.024043,rank=5\n",
      "[SoftImpute] Iter 256: observed MAE=0.006991 validation MAE=0.024023,rank=5\n",
      "[SoftImpute] Iter 257: observed MAE=0.006984 validation MAE=0.024002,rank=5\n",
      "[SoftImpute] Iter 258: observed MAE=0.006977 validation MAE=0.023981,rank=5\n",
      "[SoftImpute] Iter 259: observed MAE=0.006971 validation MAE=0.023961,rank=5\n",
      "[SoftImpute] Iter 260: observed MAE=0.006964 validation MAE=0.023940,rank=5\n",
      "[SoftImpute] Iter 261: observed MAE=0.006958 validation MAE=0.023919,rank=5\n",
      "[SoftImpute] Iter 262: observed MAE=0.006951 validation MAE=0.023899,rank=5\n",
      "[SoftImpute] Iter 263: observed MAE=0.006945 validation MAE=0.023879,rank=5\n",
      "[SoftImpute] Iter 264: observed MAE=0.006938 validation MAE=0.023859,rank=5\n",
      "[SoftImpute] Iter 265: observed MAE=0.006932 validation MAE=0.023839,rank=5\n",
      "[SoftImpute] Iter 266: observed MAE=0.006926 validation MAE=0.023819,rank=5\n",
      "[SoftImpute] Iter 267: observed MAE=0.006919 validation MAE=0.023799,rank=5\n",
      "[SoftImpute] Iter 268: observed MAE=0.006913 validation MAE=0.023780,rank=5\n",
      "[SoftImpute] Iter 269: observed MAE=0.006907 validation MAE=0.023761,rank=5\n",
      "[SoftImpute] Iter 270: observed MAE=0.006901 validation MAE=0.023741,rank=5\n",
      "[SoftImpute] Iter 271: observed MAE=0.006894 validation MAE=0.023722,rank=5\n",
      "[SoftImpute] Iter 272: observed MAE=0.006888 validation MAE=0.023703,rank=5\n",
      "[SoftImpute] Iter 273: observed MAE=0.006882 validation MAE=0.023684,rank=5\n",
      "[SoftImpute] Iter 274: observed MAE=0.006876 validation MAE=0.023665,rank=5\n",
      "[SoftImpute] Iter 275: observed MAE=0.006870 validation MAE=0.023646,rank=5\n",
      "[SoftImpute] Iter 276: observed MAE=0.006864 validation MAE=0.023627,rank=5\n",
      "[SoftImpute] Iter 277: observed MAE=0.006858 validation MAE=0.023608,rank=5\n",
      "[SoftImpute] Iter 278: observed MAE=0.006852 validation MAE=0.023589,rank=5\n",
      "[SoftImpute] Iter 279: observed MAE=0.006846 validation MAE=0.023570,rank=5\n",
      "[SoftImpute] Iter 280: observed MAE=0.006839 validation MAE=0.023551,rank=5\n",
      "[SoftImpute] Iter 281: observed MAE=0.006833 validation MAE=0.023532,rank=5\n",
      "[SoftImpute] Iter 282: observed MAE=0.006827 validation MAE=0.023513,rank=5\n",
      "[SoftImpute] Iter 283: observed MAE=0.006821 validation MAE=0.023494,rank=5\n",
      "[SoftImpute] Iter 284: observed MAE=0.006815 validation MAE=0.023475,rank=5\n",
      "[SoftImpute] Iter 285: observed MAE=0.006809 validation MAE=0.023456,rank=5\n",
      "[SoftImpute] Iter 286: observed MAE=0.006803 validation MAE=0.023438,rank=5\n",
      "[SoftImpute] Iter 287: observed MAE=0.006797 validation MAE=0.023419,rank=5\n",
      "[SoftImpute] Iter 288: observed MAE=0.006791 validation MAE=0.023400,rank=5\n",
      "[SoftImpute] Iter 289: observed MAE=0.006785 validation MAE=0.023382,rank=5\n",
      "[SoftImpute] Iter 290: observed MAE=0.006779 validation MAE=0.023363,rank=5\n",
      "[SoftImpute] Iter 291: observed MAE=0.006773 validation MAE=0.023345,rank=5\n",
      "[SoftImpute] Iter 292: observed MAE=0.006767 validation MAE=0.023326,rank=5\n",
      "[SoftImpute] Iter 293: observed MAE=0.006761 validation MAE=0.023308,rank=5\n",
      "[SoftImpute] Iter 294: observed MAE=0.006755 validation MAE=0.023290,rank=5\n",
      "[SoftImpute] Iter 295: observed MAE=0.006749 validation MAE=0.023272,rank=5\n",
      "[SoftImpute] Iter 296: observed MAE=0.006743 validation MAE=0.023254,rank=5\n",
      "[SoftImpute] Iter 297: observed MAE=0.006737 validation MAE=0.023236,rank=5\n",
      "[SoftImpute] Iter 298: observed MAE=0.006731 validation MAE=0.023218,rank=5\n",
      "[SoftImpute] Iter 299: observed MAE=0.006725 validation MAE=0.023201,rank=5\n",
      "[SoftImpute] Iter 300: observed MAE=0.006719 validation MAE=0.023183,rank=5\n",
      "[SoftImpute] Iter 301: observed MAE=0.006714 validation MAE=0.023166,rank=5\n",
      "[SoftImpute] Iter 302: observed MAE=0.006708 validation MAE=0.023149,rank=5\n",
      "[SoftImpute] Iter 303: observed MAE=0.006702 validation MAE=0.023132,rank=5\n",
      "[SoftImpute] Iter 304: observed MAE=0.006696 validation MAE=0.023115,rank=5\n",
      "[SoftImpute] Iter 305: observed MAE=0.006690 validation MAE=0.023099,rank=5\n",
      "[SoftImpute] Iter 306: observed MAE=0.006684 validation MAE=0.023082,rank=5\n",
      "[SoftImpute] Iter 307: observed MAE=0.006679 validation MAE=0.023066,rank=5\n",
      "[SoftImpute] Iter 308: observed MAE=0.006673 validation MAE=0.023049,rank=5\n",
      "[SoftImpute] Iter 309: observed MAE=0.006667 validation MAE=0.023033,rank=5\n",
      "[SoftImpute] Iter 310: observed MAE=0.006661 validation MAE=0.023017,rank=5\n",
      "[SoftImpute] Iter 311: observed MAE=0.006656 validation MAE=0.023001,rank=5\n",
      "[SoftImpute] Iter 312: observed MAE=0.006650 validation MAE=0.022985,rank=5\n",
      "[SoftImpute] Iter 313: observed MAE=0.006644 validation MAE=0.022968,rank=5\n",
      "[SoftImpute] Iter 314: observed MAE=0.006638 validation MAE=0.022953,rank=5\n",
      "[SoftImpute] Iter 315: observed MAE=0.006633 validation MAE=0.022937,rank=5\n",
      "[SoftImpute] Iter 316: observed MAE=0.006627 validation MAE=0.022921,rank=5\n",
      "[SoftImpute] Iter 317: observed MAE=0.006622 validation MAE=0.022905,rank=5\n",
      "[SoftImpute] Iter 318: observed MAE=0.006616 validation MAE=0.022890,rank=5\n",
      "[SoftImpute] Iter 319: observed MAE=0.006610 validation MAE=0.022874,rank=5\n",
      "[SoftImpute] Iter 320: observed MAE=0.006605 validation MAE=0.022859,rank=5\n",
      "[SoftImpute] Iter 321: observed MAE=0.006599 validation MAE=0.022843,rank=5\n",
      "[SoftImpute] Iter 322: observed MAE=0.006593 validation MAE=0.022827,rank=5\n",
      "[SoftImpute] Iter 323: observed MAE=0.006588 validation MAE=0.022811,rank=5\n",
      "[SoftImpute] Iter 324: observed MAE=0.006582 validation MAE=0.022796,rank=5\n",
      "[SoftImpute] Iter 325: observed MAE=0.006576 validation MAE=0.022780,rank=5\n",
      "[SoftImpute] Iter 326: observed MAE=0.006571 validation MAE=0.022764,rank=5\n",
      "[SoftImpute] Iter 327: observed MAE=0.006565 validation MAE=0.022748,rank=5\n",
      "[SoftImpute] Iter 328: observed MAE=0.006560 validation MAE=0.022733,rank=5\n",
      "[SoftImpute] Iter 329: observed MAE=0.006554 validation MAE=0.022717,rank=5\n",
      "[SoftImpute] Iter 330: observed MAE=0.006549 validation MAE=0.022702,rank=5\n",
      "[SoftImpute] Iter 331: observed MAE=0.006543 validation MAE=0.022686,rank=5\n",
      "[SoftImpute] Iter 332: observed MAE=0.006538 validation MAE=0.022671,rank=5\n",
      "[SoftImpute] Iter 333: observed MAE=0.006532 validation MAE=0.022656,rank=5\n",
      "[SoftImpute] Iter 334: observed MAE=0.006527 validation MAE=0.022641,rank=5\n",
      "[SoftImpute] Iter 335: observed MAE=0.006522 validation MAE=0.022626,rank=5\n",
      "[SoftImpute] Iter 336: observed MAE=0.006516 validation MAE=0.022611,rank=5\n",
      "[SoftImpute] Iter 337: observed MAE=0.006511 validation MAE=0.022596,rank=5\n",
      "[SoftImpute] Iter 338: observed MAE=0.006506 validation MAE=0.022581,rank=5\n",
      "[SoftImpute] Iter 339: observed MAE=0.006501 validation MAE=0.022566,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 340: observed MAE=0.006495 validation MAE=0.022551,rank=5\n",
      "[SoftImpute] Iter 341: observed MAE=0.006490 validation MAE=0.022536,rank=5\n",
      "[SoftImpute] Iter 342: observed MAE=0.006485 validation MAE=0.022521,rank=5\n",
      "[SoftImpute] Iter 343: observed MAE=0.006480 validation MAE=0.022506,rank=5\n",
      "[SoftImpute] Iter 344: observed MAE=0.006475 validation MAE=0.022491,rank=5\n",
      "[SoftImpute] Iter 345: observed MAE=0.006470 validation MAE=0.022476,rank=5\n",
      "[SoftImpute] Iter 346: observed MAE=0.006464 validation MAE=0.022461,rank=5\n",
      "[SoftImpute] Iter 347: observed MAE=0.006459 validation MAE=0.022446,rank=5\n",
      "[SoftImpute] Iter 348: observed MAE=0.006454 validation MAE=0.022431,rank=5\n",
      "[SoftImpute] Iter 349: observed MAE=0.006449 validation MAE=0.022416,rank=5\n",
      "[SoftImpute] Iter 350: observed MAE=0.006444 validation MAE=0.022401,rank=5\n",
      "[SoftImpute] Iter 351: observed MAE=0.006439 validation MAE=0.022387,rank=5\n",
      "[SoftImpute] Iter 352: observed MAE=0.006434 validation MAE=0.022373,rank=5\n",
      "[SoftImpute] Iter 353: observed MAE=0.006429 validation MAE=0.022360,rank=5\n",
      "[SoftImpute] Iter 354: observed MAE=0.006425 validation MAE=0.022347,rank=5\n",
      "[SoftImpute] Iter 355: observed MAE=0.006420 validation MAE=0.022333,rank=5\n",
      "[SoftImpute] Iter 356: observed MAE=0.006415 validation MAE=0.022320,rank=5\n",
      "[SoftImpute] Iter 357: observed MAE=0.006410 validation MAE=0.022307,rank=5\n",
      "[SoftImpute] Iter 358: observed MAE=0.006405 validation MAE=0.022293,rank=5\n",
      "[SoftImpute] Iter 359: observed MAE=0.006400 validation MAE=0.022280,rank=5\n",
      "[SoftImpute] Iter 360: observed MAE=0.006395 validation MAE=0.022267,rank=5\n",
      "[SoftImpute] Iter 361: observed MAE=0.006391 validation MAE=0.022254,rank=5\n",
      "[SoftImpute] Iter 362: observed MAE=0.006386 validation MAE=0.022241,rank=5\n",
      "[SoftImpute] Iter 363: observed MAE=0.006381 validation MAE=0.022228,rank=5\n",
      "[SoftImpute] Iter 364: observed MAE=0.006376 validation MAE=0.022215,rank=5\n",
      "[SoftImpute] Iter 365: observed MAE=0.006372 validation MAE=0.022201,rank=5\n",
      "[SoftImpute] Iter 366: observed MAE=0.006367 validation MAE=0.022188,rank=5\n",
      "[SoftImpute] Iter 367: observed MAE=0.006362 validation MAE=0.022175,rank=5\n",
      "[SoftImpute] Iter 368: observed MAE=0.006358 validation MAE=0.022162,rank=5\n",
      "[SoftImpute] Iter 369: observed MAE=0.006353 validation MAE=0.022149,rank=5\n",
      "[SoftImpute] Iter 370: observed MAE=0.006348 validation MAE=0.022136,rank=5\n",
      "[SoftImpute] Iter 371: observed MAE=0.006344 validation MAE=0.022123,rank=5\n",
      "[SoftImpute] Iter 372: observed MAE=0.006339 validation MAE=0.022110,rank=5\n",
      "[SoftImpute] Iter 373: observed MAE=0.006335 validation MAE=0.022098,rank=5\n",
      "[SoftImpute] Iter 374: observed MAE=0.006330 validation MAE=0.022085,rank=5\n",
      "[SoftImpute] Iter 375: observed MAE=0.006326 validation MAE=0.022072,rank=5\n",
      "[SoftImpute] Iter 376: observed MAE=0.006321 validation MAE=0.022059,rank=5\n",
      "[SoftImpute] Iter 377: observed MAE=0.006317 validation MAE=0.022047,rank=5\n",
      "[SoftImpute] Iter 378: observed MAE=0.006312 validation MAE=0.022034,rank=5\n",
      "[SoftImpute] Iter 379: observed MAE=0.006308 validation MAE=0.022021,rank=5\n",
      "[SoftImpute] Iter 380: observed MAE=0.006303 validation MAE=0.022008,rank=5\n",
      "[SoftImpute] Iter 381: observed MAE=0.006299 validation MAE=0.021995,rank=5\n",
      "[SoftImpute] Iter 382: observed MAE=0.006295 validation MAE=0.021983,rank=5\n",
      "[SoftImpute] Iter 383: observed MAE=0.006290 validation MAE=0.021970,rank=5\n",
      "[SoftImpute] Iter 384: observed MAE=0.006286 validation MAE=0.021958,rank=5\n",
      "[SoftImpute] Iter 385: observed MAE=0.006282 validation MAE=0.021945,rank=5\n",
      "[SoftImpute] Iter 386: observed MAE=0.006277 validation MAE=0.021933,rank=5\n",
      "[SoftImpute] Iter 387: observed MAE=0.006273 validation MAE=0.021921,rank=5\n",
      "[SoftImpute] Iter 388: observed MAE=0.006269 validation MAE=0.021908,rank=5\n",
      "[SoftImpute] Iter 389: observed MAE=0.006264 validation MAE=0.021896,rank=5\n",
      "[SoftImpute] Iter 390: observed MAE=0.006260 validation MAE=0.021884,rank=5\n",
      "[SoftImpute] Iter 391: observed MAE=0.006256 validation MAE=0.021872,rank=5\n",
      "[SoftImpute] Iter 392: observed MAE=0.006252 validation MAE=0.021860,rank=5\n",
      "[SoftImpute] Iter 393: observed MAE=0.006248 validation MAE=0.021848,rank=5\n",
      "[SoftImpute] Iter 394: observed MAE=0.006243 validation MAE=0.021835,rank=5\n",
      "[SoftImpute] Iter 395: observed MAE=0.006239 validation MAE=0.021823,rank=5\n",
      "[SoftImpute] Iter 396: observed MAE=0.006235 validation MAE=0.021811,rank=5\n",
      "[SoftImpute] Iter 397: observed MAE=0.006231 validation MAE=0.021799,rank=5\n",
      "[SoftImpute] Iter 398: observed MAE=0.006227 validation MAE=0.021788,rank=5\n",
      "[SoftImpute] Iter 399: observed MAE=0.006223 validation MAE=0.021776,rank=5\n",
      "[SoftImpute] Iter 400: observed MAE=0.006219 validation MAE=0.021764,rank=5\n",
      "[SoftImpute] Iter 401: observed MAE=0.006215 validation MAE=0.021752,rank=5\n",
      "[SoftImpute] Iter 402: observed MAE=0.006211 validation MAE=0.021741,rank=5\n",
      "[SoftImpute] Iter 403: observed MAE=0.006207 validation MAE=0.021729,rank=5\n",
      "[SoftImpute] Iter 404: observed MAE=0.006203 validation MAE=0.021718,rank=5\n",
      "[SoftImpute] Iter 405: observed MAE=0.006199 validation MAE=0.021706,rank=5\n",
      "[SoftImpute] Iter 406: observed MAE=0.006195 validation MAE=0.021695,rank=5\n",
      "[SoftImpute] Iter 407: observed MAE=0.006192 validation MAE=0.021683,rank=5\n",
      "[SoftImpute] Iter 408: observed MAE=0.006188 validation MAE=0.021672,rank=5\n",
      "[SoftImpute] Iter 409: observed MAE=0.006184 validation MAE=0.021661,rank=5\n",
      "[SoftImpute] Iter 410: observed MAE=0.006180 validation MAE=0.021649,rank=5\n",
      "[SoftImpute] Iter 411: observed MAE=0.006176 validation MAE=0.021638,rank=5\n",
      "[SoftImpute] Iter 412: observed MAE=0.006173 validation MAE=0.021627,rank=5\n",
      "[SoftImpute] Iter 413: observed MAE=0.006169 validation MAE=0.021616,rank=5\n",
      "[SoftImpute] Stopped after iteration 413 for lambda=0.024644\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 12.324775695800781\n",
      "After the matrix factor stage, training error is 0.00617, validation error is 0.02162\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.30185, val loss: 0.30478\n",
      "Main effects training epoch: 2, train loss: 0.23228, val loss: 0.23617\n",
      "Main effects training epoch: 3, train loss: 0.17997, val loss: 0.18315\n",
      "Main effects training epoch: 4, train loss: 0.14845, val loss: 0.15215\n",
      "Main effects training epoch: 5, train loss: 0.13484, val loss: 0.13695\n",
      "Main effects training epoch: 6, train loss: 0.13033, val loss: 0.13155\n",
      "Main effects training epoch: 7, train loss: 0.13055, val loss: 0.13149\n",
      "Main effects training epoch: 8, train loss: 0.12971, val loss: 0.12988\n",
      "Main effects training epoch: 9, train loss: 0.12862, val loss: 0.12919\n",
      "Main effects training epoch: 10, train loss: 0.12828, val loss: 0.12938\n",
      "Main effects training epoch: 11, train loss: 0.12732, val loss: 0.12789\n",
      "Main effects training epoch: 12, train loss: 0.12659, val loss: 0.12743\n",
      "Main effects training epoch: 13, train loss: 0.12563, val loss: 0.12654\n",
      "Main effects training epoch: 14, train loss: 0.12325, val loss: 0.12456\n",
      "Main effects training epoch: 15, train loss: 0.11949, val loss: 0.12161\n",
      "Main effects training epoch: 16, train loss: 0.11686, val loss: 0.11922\n",
      "Main effects training epoch: 17, train loss: 0.11423, val loss: 0.11562\n",
      "Main effects training epoch: 18, train loss: 0.11484, val loss: 0.11687\n",
      "Main effects training epoch: 19, train loss: 0.11188, val loss: 0.11473\n",
      "Main effects training epoch: 20, train loss: 0.11089, val loss: 0.11324\n",
      "Main effects training epoch: 21, train loss: 0.10849, val loss: 0.11127\n",
      "Main effects training epoch: 22, train loss: 0.10778, val loss: 0.11002\n",
      "Main effects training epoch: 23, train loss: 0.10761, val loss: 0.10934\n",
      "Main effects training epoch: 24, train loss: 0.10695, val loss: 0.10962\n",
      "Main effects training epoch: 25, train loss: 0.10869, val loss: 0.11093\n",
      "Main effects training epoch: 26, train loss: 0.10721, val loss: 0.11016\n",
      "Main effects training epoch: 27, train loss: 0.10692, val loss: 0.10911\n",
      "Main effects training epoch: 28, train loss: 0.10656, val loss: 0.10921\n",
      "Main effects training epoch: 29, train loss: 0.10666, val loss: 0.10915\n",
      "Main effects training epoch: 30, train loss: 0.10688, val loss: 0.10958\n",
      "Main effects training epoch: 31, train loss: 0.10689, val loss: 0.10943\n",
      "Main effects training epoch: 32, train loss: 0.10622, val loss: 0.10918\n",
      "Main effects training epoch: 33, train loss: 0.10631, val loss: 0.10928\n",
      "Main effects training epoch: 34, train loss: 0.10605, val loss: 0.10868\n",
      "Main effects training epoch: 35, train loss: 0.10599, val loss: 0.10879\n",
      "Main effects training epoch: 36, train loss: 0.10605, val loss: 0.10911\n",
      "Main effects training epoch: 37, train loss: 0.10704, val loss: 0.10981\n",
      "Main effects training epoch: 38, train loss: 0.10655, val loss: 0.10926\n",
      "Main effects training epoch: 39, train loss: 0.10607, val loss: 0.10888\n",
      "Main effects training epoch: 40, train loss: 0.10588, val loss: 0.10859\n",
      "Main effects training epoch: 41, train loss: 0.10600, val loss: 0.10889\n",
      "Main effects training epoch: 42, train loss: 0.10616, val loss: 0.10903\n",
      "Main effects training epoch: 43, train loss: 0.10609, val loss: 0.10883\n",
      "Main effects training epoch: 44, train loss: 0.10630, val loss: 0.10899\n",
      "Main effects training epoch: 45, train loss: 0.10686, val loss: 0.10928\n",
      "Main effects training epoch: 46, train loss: 0.10599, val loss: 0.10904\n",
      "Main effects training epoch: 47, train loss: 0.10577, val loss: 0.10887\n",
      "Main effects training epoch: 48, train loss: 0.10584, val loss: 0.10888\n",
      "Main effects training epoch: 49, train loss: 0.10576, val loss: 0.10866\n",
      "Main effects training epoch: 50, train loss: 0.10617, val loss: 0.10903\n",
      "Main effects training epoch: 51, train loss: 0.10592, val loss: 0.10893\n",
      "Main effects training epoch: 52, train loss: 0.10606, val loss: 0.10892\n",
      "Main effects training epoch: 53, train loss: 0.10577, val loss: 0.10888\n",
      "Main effects training epoch: 54, train loss: 0.10586, val loss: 0.10875\n",
      "Main effects training epoch: 55, train loss: 0.10635, val loss: 0.10929\n",
      "Main effects training epoch: 56, train loss: 0.10600, val loss: 0.10895\n",
      "Main effects training epoch: 57, train loss: 0.10614, val loss: 0.10942\n",
      "Main effects training epoch: 58, train loss: 0.10591, val loss: 0.10905\n",
      "Main effects training epoch: 59, train loss: 0.10587, val loss: 0.10866\n",
      "Main effects training epoch: 60, train loss: 0.10580, val loss: 0.10885\n",
      "Main effects training epoch: 61, train loss: 0.10590, val loss: 0.10868\n",
      "Main effects training epoch: 62, train loss: 0.10611, val loss: 0.10911\n",
      "Main effects training epoch: 63, train loss: 0.10587, val loss: 0.10877\n",
      "Main effects training epoch: 64, train loss: 0.10611, val loss: 0.10908\n",
      "Main effects training epoch: 65, train loss: 0.10597, val loss: 0.10888\n",
      "Main effects training epoch: 66, train loss: 0.10579, val loss: 0.10886\n",
      "Main effects training epoch: 67, train loss: 0.10588, val loss: 0.10853\n",
      "Main effects training epoch: 68, train loss: 0.10601, val loss: 0.10926\n",
      "Main effects training epoch: 69, train loss: 0.10583, val loss: 0.10866\n",
      "Main effects training epoch: 70, train loss: 0.10585, val loss: 0.10893\n",
      "Main effects training epoch: 71, train loss: 0.10592, val loss: 0.10879\n",
      "Main effects training epoch: 72, train loss: 0.10582, val loss: 0.10901\n",
      "Main effects training epoch: 73, train loss: 0.10586, val loss: 0.10890\n",
      "Main effects training epoch: 74, train loss: 0.10596, val loss: 0.10893\n",
      "Main effects training epoch: 75, train loss: 0.10614, val loss: 0.10884\n",
      "Main effects training epoch: 76, train loss: 0.10583, val loss: 0.10904\n",
      "Main effects training epoch: 77, train loss: 0.10592, val loss: 0.10890\n",
      "Main effects training epoch: 78, train loss: 0.10580, val loss: 0.10881\n",
      "Main effects training epoch: 79, train loss: 0.10594, val loss: 0.10889\n",
      "Main effects training epoch: 80, train loss: 0.10607, val loss: 0.10888\n",
      "Main effects training epoch: 81, train loss: 0.10581, val loss: 0.10891\n",
      "Main effects training epoch: 82, train loss: 0.10593, val loss: 0.10922\n",
      "Main effects training epoch: 83, train loss: 0.10574, val loss: 0.10873\n",
      "Main effects training epoch: 84, train loss: 0.10573, val loss: 0.10877\n",
      "Main effects training epoch: 85, train loss: 0.10636, val loss: 0.10929\n",
      "Main effects training epoch: 86, train loss: 0.10583, val loss: 0.10874\n",
      "Main effects training epoch: 87, train loss: 0.10614, val loss: 0.10928\n",
      "Main effects training epoch: 88, train loss: 0.10616, val loss: 0.10949\n",
      "Main effects training epoch: 89, train loss: 0.10589, val loss: 0.10865\n",
      "Main effects training epoch: 90, train loss: 0.10604, val loss: 0.10903\n",
      "Main effects training epoch: 91, train loss: 0.10588, val loss: 0.10892\n",
      "Main effects training epoch: 92, train loss: 0.10612, val loss: 0.10925\n",
      "Main effects training epoch: 93, train loss: 0.10583, val loss: 0.10911\n",
      "Main effects training epoch: 94, train loss: 0.10604, val loss: 0.10904\n",
      "Main effects training epoch: 95, train loss: 0.10576, val loss: 0.10903\n",
      "Main effects training epoch: 96, train loss: 0.10568, val loss: 0.10869\n",
      "Main effects training epoch: 97, train loss: 0.10588, val loss: 0.10926\n",
      "Main effects training epoch: 98, train loss: 0.10606, val loss: 0.10910\n",
      "Main effects training epoch: 99, train loss: 0.10649, val loss: 0.10930\n",
      "Main effects training epoch: 100, train loss: 0.10628, val loss: 0.10948\n",
      "Main effects training epoch: 101, train loss: 0.10586, val loss: 0.10912\n",
      "Main effects training epoch: 102, train loss: 0.10570, val loss: 0.10869\n",
      "Main effects training epoch: 103, train loss: 0.10643, val loss: 0.10951\n",
      "Main effects training epoch: 104, train loss: 0.10650, val loss: 0.10946\n",
      "Main effects training epoch: 105, train loss: 0.10626, val loss: 0.10954\n",
      "Main effects training epoch: 106, train loss: 0.10594, val loss: 0.10896\n",
      "Main effects training epoch: 107, train loss: 0.10579, val loss: 0.10878\n",
      "Main effects training epoch: 108, train loss: 0.10607, val loss: 0.10922\n",
      "Main effects training epoch: 109, train loss: 0.10581, val loss: 0.10902\n",
      "Main effects training epoch: 110, train loss: 0.10590, val loss: 0.10892\n",
      "Main effects training epoch: 111, train loss: 0.10605, val loss: 0.10924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 112, train loss: 0.10580, val loss: 0.10875\n",
      "Main effects training epoch: 113, train loss: 0.10587, val loss: 0.10900\n",
      "Main effects training epoch: 114, train loss: 0.10581, val loss: 0.10883\n",
      "Main effects training epoch: 115, train loss: 0.10618, val loss: 0.10915\n",
      "Main effects training epoch: 116, train loss: 0.10615, val loss: 0.10949\n",
      "Main effects training epoch: 117, train loss: 0.10602, val loss: 0.10894\n",
      "Main effects training epoch: 118, train loss: 0.10595, val loss: 0.10914\n",
      "Main effects training epoch: 119, train loss: 0.10618, val loss: 0.10895\n",
      "Main effects training epoch: 120, train loss: 0.10601, val loss: 0.10941\n",
      "Main effects training epoch: 121, train loss: 0.10588, val loss: 0.10869\n",
      "Main effects training epoch: 122, train loss: 0.10576, val loss: 0.10885\n",
      "Main effects training epoch: 123, train loss: 0.10578, val loss: 0.10899\n",
      "Main effects training epoch: 124, train loss: 0.10583, val loss: 0.10934\n",
      "Main effects training epoch: 125, train loss: 0.10610, val loss: 0.10904\n",
      "Main effects training epoch: 126, train loss: 0.10593, val loss: 0.10917\n",
      "Main effects training epoch: 127, train loss: 0.10579, val loss: 0.10863\n",
      "Main effects training epoch: 128, train loss: 0.10583, val loss: 0.10933\n",
      "Main effects training epoch: 129, train loss: 0.10580, val loss: 0.10919\n",
      "Main effects training epoch: 130, train loss: 0.10573, val loss: 0.10874\n",
      "Main effects training epoch: 131, train loss: 0.10601, val loss: 0.10951\n",
      "Main effects training epoch: 132, train loss: 0.10600, val loss: 0.10905\n",
      "Main effects training epoch: 133, train loss: 0.10601, val loss: 0.10931\n",
      "Main effects training epoch: 134, train loss: 0.10566, val loss: 0.10882\n",
      "Main effects training epoch: 135, train loss: 0.10590, val loss: 0.10909\n",
      "Main effects training epoch: 136, train loss: 0.10621, val loss: 0.10930\n",
      "Main effects training epoch: 137, train loss: 0.10597, val loss: 0.10906\n",
      "Main effects training epoch: 138, train loss: 0.10583, val loss: 0.10895\n",
      "Main effects training epoch: 139, train loss: 0.10580, val loss: 0.10894\n",
      "Main effects training epoch: 140, train loss: 0.10591, val loss: 0.10891\n",
      "Main effects training epoch: 141, train loss: 0.10595, val loss: 0.10903\n",
      "Main effects training epoch: 142, train loss: 0.10601, val loss: 0.10932\n",
      "Main effects training epoch: 143, train loss: 0.10596, val loss: 0.10892\n",
      "Main effects training epoch: 144, train loss: 0.10583, val loss: 0.10925\n",
      "Main effects training epoch: 145, train loss: 0.10572, val loss: 0.10887\n",
      "Main effects training epoch: 146, train loss: 0.10574, val loss: 0.10897\n",
      "Main effects training epoch: 147, train loss: 0.10575, val loss: 0.10903\n",
      "Main effects training epoch: 148, train loss: 0.10589, val loss: 0.10894\n",
      "Main effects training epoch: 149, train loss: 0.10577, val loss: 0.10889\n",
      "Main effects training epoch: 150, train loss: 0.10569, val loss: 0.10895\n",
      "Main effects training epoch: 151, train loss: 0.10575, val loss: 0.10884\n",
      "Main effects training epoch: 152, train loss: 0.10606, val loss: 0.10908\n",
      "Main effects training epoch: 153, train loss: 0.10639, val loss: 0.10965\n",
      "Main effects training epoch: 154, train loss: 0.10596, val loss: 0.10920\n",
      "Main effects training epoch: 155, train loss: 0.10578, val loss: 0.10908\n",
      "Main effects training epoch: 156, train loss: 0.10569, val loss: 0.10881\n",
      "Main effects training epoch: 157, train loss: 0.10570, val loss: 0.10898\n",
      "Main effects training epoch: 158, train loss: 0.10595, val loss: 0.10931\n",
      "Main effects training epoch: 159, train loss: 0.10577, val loss: 0.10920\n",
      "Main effects training epoch: 160, train loss: 0.10578, val loss: 0.10935\n",
      "Main effects training epoch: 161, train loss: 0.10579, val loss: 0.10872\n",
      "Main effects training epoch: 162, train loss: 0.10575, val loss: 0.10904\n",
      "Main effects training epoch: 163, train loss: 0.10597, val loss: 0.10913\n",
      "Main effects training epoch: 164, train loss: 0.10605, val loss: 0.10923\n",
      "Main effects training epoch: 165, train loss: 0.10604, val loss: 0.10954\n",
      "Main effects training epoch: 166, train loss: 0.10575, val loss: 0.10883\n",
      "Main effects training epoch: 167, train loss: 0.10578, val loss: 0.10918\n",
      "Main effects training epoch: 168, train loss: 0.10576, val loss: 0.10888\n",
      "Early stop at epoch 168, with validation loss: 0.10888\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.10587, val loss: 0.10915\n",
      "Main effects tuning epoch: 2, train loss: 0.10583, val loss: 0.10863\n",
      "Main effects tuning epoch: 3, train loss: 0.10593, val loss: 0.10918\n",
      "Main effects tuning epoch: 4, train loss: 0.10580, val loss: 0.10863\n",
      "Main effects tuning epoch: 5, train loss: 0.10595, val loss: 0.10896\n",
      "Main effects tuning epoch: 6, train loss: 0.10614, val loss: 0.10907\n",
      "Main effects tuning epoch: 7, train loss: 0.10617, val loss: 0.10891\n",
      "Main effects tuning epoch: 8, train loss: 0.10639, val loss: 0.10952\n",
      "Main effects tuning epoch: 9, train loss: 0.10631, val loss: 0.10952\n",
      "Main effects tuning epoch: 10, train loss: 0.10598, val loss: 0.10913\n",
      "Main effects tuning epoch: 11, train loss: 0.10616, val loss: 0.10872\n",
      "Main effects tuning epoch: 12, train loss: 0.10613, val loss: 0.10940\n",
      "Main effects tuning epoch: 13, train loss: 0.10649, val loss: 0.10913\n",
      "Main effects tuning epoch: 14, train loss: 0.10595, val loss: 0.10906\n",
      "Main effects tuning epoch: 15, train loss: 0.10576, val loss: 0.10901\n",
      "Main effects tuning epoch: 16, train loss: 0.10591, val loss: 0.10867\n",
      "Main effects tuning epoch: 17, train loss: 0.10614, val loss: 0.10929\n",
      "Main effects tuning epoch: 18, train loss: 0.10616, val loss: 0.10914\n",
      "Main effects tuning epoch: 19, train loss: 0.10591, val loss: 0.10901\n",
      "Main effects tuning epoch: 20, train loss: 0.10585, val loss: 0.10893\n",
      "Main effects tuning epoch: 21, train loss: 0.10582, val loss: 0.10851\n",
      "Main effects tuning epoch: 22, train loss: 0.10595, val loss: 0.10917\n",
      "Main effects tuning epoch: 23, train loss: 0.10589, val loss: 0.10893\n",
      "Main effects tuning epoch: 24, train loss: 0.10579, val loss: 0.10899\n",
      "Main effects tuning epoch: 25, train loss: 0.10580, val loss: 0.10907\n",
      "Main effects tuning epoch: 26, train loss: 0.10612, val loss: 0.10929\n",
      "Main effects tuning epoch: 27, train loss: 0.10613, val loss: 0.10892\n",
      "Main effects tuning epoch: 28, train loss: 0.10608, val loss: 0.10898\n",
      "Main effects tuning epoch: 29, train loss: 0.10588, val loss: 0.10904\n",
      "Main effects tuning epoch: 30, train loss: 0.10603, val loss: 0.10875\n",
      "Main effects tuning epoch: 31, train loss: 0.10580, val loss: 0.10885\n",
      "Main effects tuning epoch: 32, train loss: 0.10586, val loss: 0.10892\n",
      "Main effects tuning epoch: 33, train loss: 0.10584, val loss: 0.10877\n",
      "Main effects tuning epoch: 34, train loss: 0.10610, val loss: 0.10890\n",
      "Main effects tuning epoch: 35, train loss: 0.10602, val loss: 0.10950\n",
      "Main effects tuning epoch: 36, train loss: 0.10577, val loss: 0.10870\n",
      "Main effects tuning epoch: 37, train loss: 0.10576, val loss: 0.10890\n",
      "Main effects tuning epoch: 38, train loss: 0.10590, val loss: 0.10894\n",
      "Main effects tuning epoch: 39, train loss: 0.10597, val loss: 0.10915\n",
      "Main effects tuning epoch: 40, train loss: 0.10575, val loss: 0.10856\n",
      "Main effects tuning epoch: 41, train loss: 0.10577, val loss: 0.10899\n",
      "Main effects tuning epoch: 42, train loss: 0.10600, val loss: 0.10888\n",
      "Main effects tuning epoch: 43, train loss: 0.10624, val loss: 0.10938\n",
      "Main effects tuning epoch: 44, train loss: 0.10606, val loss: 0.10863\n",
      "Main effects tuning epoch: 45, train loss: 0.10614, val loss: 0.10961\n",
      "Main effects tuning epoch: 46, train loss: 0.10598, val loss: 0.10874\n",
      "Main effects tuning epoch: 47, train loss: 0.10623, val loss: 0.10916\n",
      "Main effects tuning epoch: 48, train loss: 0.10628, val loss: 0.10910\n",
      "Main effects tuning epoch: 49, train loss: 0.10612, val loss: 0.10902\n",
      "Main effects tuning epoch: 50, train loss: 0.10600, val loss: 0.10922\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.16656, val loss: 0.16504\n",
      "Interaction training epoch: 2, train loss: 0.19799, val loss: 0.19730\n",
      "Interaction training epoch: 3, train loss: 0.07919, val loss: 0.08135\n",
      "Interaction training epoch: 4, train loss: 0.06592, val loss: 0.06695\n",
      "Interaction training epoch: 5, train loss: 0.06719, val loss: 0.06623\n",
      "Interaction training epoch: 6, train loss: 0.07116, val loss: 0.06992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 7, train loss: 0.05765, val loss: 0.05869\n",
      "Interaction training epoch: 8, train loss: 0.05791, val loss: 0.05822\n",
      "Interaction training epoch: 9, train loss: 0.06091, val loss: 0.06046\n",
      "Interaction training epoch: 10, train loss: 0.05754, val loss: 0.05788\n",
      "Interaction training epoch: 11, train loss: 0.05624, val loss: 0.05624\n",
      "Interaction training epoch: 12, train loss: 0.05651, val loss: 0.05600\n",
      "Interaction training epoch: 13, train loss: 0.06101, val loss: 0.06080\n",
      "Interaction training epoch: 14, train loss: 0.05709, val loss: 0.05641\n",
      "Interaction training epoch: 15, train loss: 0.06077, val loss: 0.06101\n",
      "Interaction training epoch: 16, train loss: 0.05239, val loss: 0.05197\n",
      "Interaction training epoch: 17, train loss: 0.05618, val loss: 0.05592\n",
      "Interaction training epoch: 18, train loss: 0.05477, val loss: 0.05468\n",
      "Interaction training epoch: 19, train loss: 0.05152, val loss: 0.05113\n",
      "Interaction training epoch: 20, train loss: 0.05519, val loss: 0.05482\n",
      "Interaction training epoch: 21, train loss: 0.05818, val loss: 0.05633\n",
      "Interaction training epoch: 22, train loss: 0.05614, val loss: 0.05550\n",
      "Interaction training epoch: 23, train loss: 0.05384, val loss: 0.05252\n",
      "Interaction training epoch: 24, train loss: 0.05143, val loss: 0.05136\n",
      "Interaction training epoch: 25, train loss: 0.05185, val loss: 0.05118\n",
      "Interaction training epoch: 26, train loss: 0.05409, val loss: 0.05515\n",
      "Interaction training epoch: 27, train loss: 0.05171, val loss: 0.05189\n",
      "Interaction training epoch: 28, train loss: 0.05284, val loss: 0.05307\n",
      "Interaction training epoch: 29, train loss: 0.05251, val loss: 0.05306\n",
      "Interaction training epoch: 30, train loss: 0.05654, val loss: 0.05667\n",
      "Interaction training epoch: 31, train loss: 0.05008, val loss: 0.05013\n",
      "Interaction training epoch: 32, train loss: 0.05165, val loss: 0.05175\n",
      "Interaction training epoch: 33, train loss: 0.05094, val loss: 0.05147\n",
      "Interaction training epoch: 34, train loss: 0.05154, val loss: 0.05255\n",
      "Interaction training epoch: 35, train loss: 0.05178, val loss: 0.05163\n",
      "Interaction training epoch: 36, train loss: 0.05189, val loss: 0.05137\n",
      "Interaction training epoch: 37, train loss: 0.05261, val loss: 0.05262\n",
      "Interaction training epoch: 38, train loss: 0.05395, val loss: 0.05444\n",
      "Interaction training epoch: 39, train loss: 0.05053, val loss: 0.05093\n",
      "Interaction training epoch: 40, train loss: 0.05186, val loss: 0.05165\n",
      "Interaction training epoch: 41, train loss: 0.05174, val loss: 0.05183\n",
      "Interaction training epoch: 42, train loss: 0.05645, val loss: 0.05707\n",
      "Interaction training epoch: 43, train loss: 0.05130, val loss: 0.05143\n",
      "Interaction training epoch: 44, train loss: 0.05233, val loss: 0.05213\n",
      "Interaction training epoch: 45, train loss: 0.05242, val loss: 0.05266\n",
      "Interaction training epoch: 46, train loss: 0.05234, val loss: 0.05256\n",
      "Interaction training epoch: 47, train loss: 0.05096, val loss: 0.05121\n",
      "Interaction training epoch: 48, train loss: 0.05324, val loss: 0.05317\n",
      "Interaction training epoch: 49, train loss: 0.05143, val loss: 0.05175\n",
      "Interaction training epoch: 50, train loss: 0.05906, val loss: 0.05866\n",
      "Interaction training epoch: 51, train loss: 0.05278, val loss: 0.05220\n",
      "Interaction training epoch: 52, train loss: 0.05429, val loss: 0.05321\n",
      "Interaction training epoch: 53, train loss: 0.05247, val loss: 0.05225\n",
      "Interaction training epoch: 54, train loss: 0.05061, val loss: 0.05129\n",
      "Interaction training epoch: 55, train loss: 0.05239, val loss: 0.05237\n",
      "Interaction training epoch: 56, train loss: 0.05174, val loss: 0.05287\n",
      "Interaction training epoch: 57, train loss: 0.05160, val loss: 0.05111\n",
      "Interaction training epoch: 58, train loss: 0.05180, val loss: 0.05292\n",
      "Interaction training epoch: 59, train loss: 0.04982, val loss: 0.05009\n",
      "Interaction training epoch: 60, train loss: 0.05227, val loss: 0.05368\n",
      "Interaction training epoch: 61, train loss: 0.05217, val loss: 0.05210\n",
      "Interaction training epoch: 62, train loss: 0.05023, val loss: 0.05074\n",
      "Interaction training epoch: 63, train loss: 0.05179, val loss: 0.05284\n",
      "Interaction training epoch: 64, train loss: 0.04959, val loss: 0.05129\n",
      "Interaction training epoch: 65, train loss: 0.05320, val loss: 0.05225\n",
      "Interaction training epoch: 66, train loss: 0.05295, val loss: 0.05326\n",
      "Interaction training epoch: 67, train loss: 0.05028, val loss: 0.05144\n",
      "Interaction training epoch: 68, train loss: 0.05112, val loss: 0.05151\n",
      "Interaction training epoch: 69, train loss: 0.05228, val loss: 0.05248\n",
      "Interaction training epoch: 70, train loss: 0.04994, val loss: 0.05062\n",
      "Interaction training epoch: 71, train loss: 0.05225, val loss: 0.05286\n",
      "Interaction training epoch: 72, train loss: 0.05069, val loss: 0.05180\n",
      "Interaction training epoch: 73, train loss: 0.05077, val loss: 0.05196\n",
      "Interaction training epoch: 74, train loss: 0.04967, val loss: 0.05076\n",
      "Interaction training epoch: 75, train loss: 0.05064, val loss: 0.05071\n",
      "Interaction training epoch: 76, train loss: 0.04886, val loss: 0.05004\n",
      "Interaction training epoch: 77, train loss: 0.05097, val loss: 0.05075\n",
      "Interaction training epoch: 78, train loss: 0.05171, val loss: 0.05128\n",
      "Interaction training epoch: 79, train loss: 0.05178, val loss: 0.05168\n",
      "Interaction training epoch: 80, train loss: 0.04972, val loss: 0.05111\n",
      "Interaction training epoch: 81, train loss: 0.05307, val loss: 0.05407\n",
      "Interaction training epoch: 82, train loss: 0.05287, val loss: 0.05331\n",
      "Interaction training epoch: 83, train loss: 0.05019, val loss: 0.05065\n",
      "Interaction training epoch: 84, train loss: 0.05109, val loss: 0.05177\n",
      "Interaction training epoch: 85, train loss: 0.05241, val loss: 0.05395\n",
      "Interaction training epoch: 86, train loss: 0.05169, val loss: 0.05187\n",
      "Interaction training epoch: 87, train loss: 0.05013, val loss: 0.05031\n",
      "Interaction training epoch: 88, train loss: 0.05249, val loss: 0.05324\n",
      "Interaction training epoch: 89, train loss: 0.04905, val loss: 0.04955\n",
      "Interaction training epoch: 90, train loss: 0.05048, val loss: 0.05104\n",
      "Interaction training epoch: 91, train loss: 0.05291, val loss: 0.05320\n",
      "Interaction training epoch: 92, train loss: 0.05194, val loss: 0.05184\n",
      "Interaction training epoch: 93, train loss: 0.04813, val loss: 0.04857\n",
      "Interaction training epoch: 94, train loss: 0.05434, val loss: 0.05466\n",
      "Interaction training epoch: 95, train loss: 0.05394, val loss: 0.05306\n",
      "Interaction training epoch: 96, train loss: 0.04954, val loss: 0.04995\n",
      "Interaction training epoch: 97, train loss: 0.05003, val loss: 0.05058\n",
      "Interaction training epoch: 98, train loss: 0.05197, val loss: 0.05219\n",
      "Interaction training epoch: 99, train loss: 0.05436, val loss: 0.05280\n",
      "Interaction training epoch: 100, train loss: 0.05350, val loss: 0.05437\n",
      "Interaction training epoch: 101, train loss: 0.04964, val loss: 0.04980\n",
      "Interaction training epoch: 102, train loss: 0.05136, val loss: 0.05087\n",
      "Interaction training epoch: 103, train loss: 0.05162, val loss: 0.05191\n",
      "Interaction training epoch: 104, train loss: 0.05118, val loss: 0.05150\n",
      "Interaction training epoch: 105, train loss: 0.04993, val loss: 0.04937\n",
      "Interaction training epoch: 106, train loss: 0.05171, val loss: 0.05164\n",
      "Interaction training epoch: 107, train loss: 0.05193, val loss: 0.05075\n",
      "Interaction training epoch: 108, train loss: 0.05102, val loss: 0.05183\n",
      "Interaction training epoch: 109, train loss: 0.05261, val loss: 0.05324\n",
      "Interaction training epoch: 110, train loss: 0.05047, val loss: 0.05100\n",
      "Interaction training epoch: 111, train loss: 0.05046, val loss: 0.05063\n",
      "Interaction training epoch: 112, train loss: 0.04811, val loss: 0.04809\n",
      "Interaction training epoch: 113, train loss: 0.05250, val loss: 0.05298\n",
      "Interaction training epoch: 114, train loss: 0.05036, val loss: 0.05112\n",
      "Interaction training epoch: 115, train loss: 0.05119, val loss: 0.05140\n",
      "Interaction training epoch: 116, train loss: 0.04975, val loss: 0.05112\n",
      "Interaction training epoch: 117, train loss: 0.04911, val loss: 0.04994\n",
      "Interaction training epoch: 118, train loss: 0.04990, val loss: 0.05012\n",
      "Interaction training epoch: 119, train loss: 0.04854, val loss: 0.04775\n",
      "Interaction training epoch: 120, train loss: 0.04985, val loss: 0.04987\n",
      "Interaction training epoch: 121, train loss: 0.04779, val loss: 0.04856\n",
      "Interaction training epoch: 122, train loss: 0.04712, val loss: 0.04773\n",
      "Interaction training epoch: 123, train loss: 0.05164, val loss: 0.05255\n",
      "Interaction training epoch: 124, train loss: 0.05327, val loss: 0.05365\n",
      "Interaction training epoch: 125, train loss: 0.05348, val loss: 0.05414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 126, train loss: 0.04873, val loss: 0.04978\n",
      "Interaction training epoch: 127, train loss: 0.04866, val loss: 0.04894\n",
      "Interaction training epoch: 128, train loss: 0.04947, val loss: 0.05037\n",
      "Interaction training epoch: 129, train loss: 0.04701, val loss: 0.04711\n",
      "Interaction training epoch: 130, train loss: 0.04867, val loss: 0.04878\n",
      "Interaction training epoch: 131, train loss: 0.04995, val loss: 0.05004\n",
      "Interaction training epoch: 132, train loss: 0.05004, val loss: 0.05008\n",
      "Interaction training epoch: 133, train loss: 0.04970, val loss: 0.05023\n",
      "Interaction training epoch: 134, train loss: 0.04969, val loss: 0.05041\n",
      "Interaction training epoch: 135, train loss: 0.04826, val loss: 0.04935\n",
      "Interaction training epoch: 136, train loss: 0.04956, val loss: 0.05077\n",
      "Interaction training epoch: 137, train loss: 0.04937, val loss: 0.05116\n",
      "Interaction training epoch: 138, train loss: 0.05042, val loss: 0.05126\n",
      "Interaction training epoch: 139, train loss: 0.04910, val loss: 0.04944\n",
      "Interaction training epoch: 140, train loss: 0.05235, val loss: 0.05329\n",
      "Interaction training epoch: 141, train loss: 0.05151, val loss: 0.05199\n",
      "Interaction training epoch: 142, train loss: 0.04935, val loss: 0.04965\n",
      "Interaction training epoch: 143, train loss: 0.05663, val loss: 0.05667\n",
      "Interaction training epoch: 144, train loss: 0.05217, val loss: 0.05297\n",
      "Interaction training epoch: 145, train loss: 0.05139, val loss: 0.05143\n",
      "Interaction training epoch: 146, train loss: 0.05350, val loss: 0.05388\n",
      "Interaction training epoch: 147, train loss: 0.06195, val loss: 0.06171\n",
      "Interaction training epoch: 148, train loss: 0.05923, val loss: 0.05958\n",
      "Interaction training epoch: 149, train loss: 0.05897, val loss: 0.05842\n",
      "Interaction training epoch: 150, train loss: 0.05688, val loss: 0.05663\n",
      "Interaction training epoch: 151, train loss: 0.05292, val loss: 0.05339\n",
      "Interaction training epoch: 152, train loss: 0.04638, val loss: 0.04708\n",
      "Interaction training epoch: 153, train loss: 0.04830, val loss: 0.05070\n",
      "Interaction training epoch: 154, train loss: 0.04798, val loss: 0.04952\n",
      "Interaction training epoch: 155, train loss: 0.04659, val loss: 0.04786\n",
      "Interaction training epoch: 156, train loss: 0.04767, val loss: 0.04876\n",
      "Interaction training epoch: 157, train loss: 0.04853, val loss: 0.04917\n",
      "Interaction training epoch: 158, train loss: 0.04677, val loss: 0.04803\n",
      "Interaction training epoch: 159, train loss: 0.04901, val loss: 0.04931\n",
      "Interaction training epoch: 160, train loss: 0.04754, val loss: 0.04893\n",
      "Interaction training epoch: 161, train loss: 0.04874, val loss: 0.04950\n",
      "Interaction training epoch: 162, train loss: 0.04595, val loss: 0.04721\n",
      "Interaction training epoch: 163, train loss: 0.04732, val loss: 0.04956\n",
      "Interaction training epoch: 164, train loss: 0.04567, val loss: 0.04749\n",
      "Interaction training epoch: 165, train loss: 0.05013, val loss: 0.05115\n",
      "Interaction training epoch: 166, train loss: 0.04663, val loss: 0.04790\n",
      "Interaction training epoch: 167, train loss: 0.04602, val loss: 0.04784\n",
      "Interaction training epoch: 168, train loss: 0.04789, val loss: 0.04800\n",
      "Interaction training epoch: 169, train loss: 0.04480, val loss: 0.04615\n",
      "Interaction training epoch: 170, train loss: 0.04657, val loss: 0.04712\n",
      "Interaction training epoch: 171, train loss: 0.04515, val loss: 0.04688\n",
      "Interaction training epoch: 172, train loss: 0.04745, val loss: 0.04904\n",
      "Interaction training epoch: 173, train loss: 0.04801, val loss: 0.05050\n",
      "Interaction training epoch: 174, train loss: 0.05297, val loss: 0.05389\n",
      "Interaction training epoch: 175, train loss: 0.04522, val loss: 0.04687\n",
      "Interaction training epoch: 176, train loss: 0.04654, val loss: 0.04709\n",
      "Interaction training epoch: 177, train loss: 0.04462, val loss: 0.04576\n",
      "Interaction training epoch: 178, train loss: 0.04715, val loss: 0.04880\n",
      "Interaction training epoch: 179, train loss: 0.04515, val loss: 0.04673\n",
      "Interaction training epoch: 180, train loss: 0.04652, val loss: 0.04824\n",
      "Interaction training epoch: 181, train loss: 0.04757, val loss: 0.04936\n",
      "Interaction training epoch: 182, train loss: 0.04771, val loss: 0.04909\n",
      "Interaction training epoch: 183, train loss: 0.04999, val loss: 0.05186\n",
      "Interaction training epoch: 184, train loss: 0.04691, val loss: 0.04877\n",
      "Interaction training epoch: 185, train loss: 0.04599, val loss: 0.04763\n",
      "Interaction training epoch: 186, train loss: 0.04839, val loss: 0.04969\n",
      "Interaction training epoch: 187, train loss: 0.04634, val loss: 0.04846\n",
      "Interaction training epoch: 188, train loss: 0.04686, val loss: 0.04828\n",
      "Interaction training epoch: 189, train loss: 0.04527, val loss: 0.04603\n",
      "Interaction training epoch: 190, train loss: 0.04811, val loss: 0.05046\n",
      "Interaction training epoch: 191, train loss: 0.04668, val loss: 0.04794\n",
      "Interaction training epoch: 192, train loss: 0.04580, val loss: 0.04673\n",
      "Interaction training epoch: 193, train loss: 0.04953, val loss: 0.05077\n",
      "Interaction training epoch: 194, train loss: 0.04637, val loss: 0.04745\n",
      "Interaction training epoch: 195, train loss: 0.05291, val loss: 0.05606\n",
      "Interaction training epoch: 196, train loss: 0.04552, val loss: 0.04706\n",
      "Interaction training epoch: 197, train loss: 0.04954, val loss: 0.05175\n",
      "Interaction training epoch: 198, train loss: 0.04504, val loss: 0.04659\n",
      "Interaction training epoch: 199, train loss: 0.04939, val loss: 0.05068\n",
      "Interaction training epoch: 200, train loss: 0.04824, val loss: 0.04969\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########7 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.04624, val loss: 0.04648\n",
      "Interaction tuning epoch: 2, train loss: 0.04654, val loss: 0.04675\n",
      "Interaction tuning epoch: 3, train loss: 0.04688, val loss: 0.04770\n",
      "Interaction tuning epoch: 4, train loss: 0.04703, val loss: 0.04714\n",
      "Interaction tuning epoch: 5, train loss: 0.04871, val loss: 0.04870\n",
      "Interaction tuning epoch: 6, train loss: 0.04715, val loss: 0.04685\n",
      "Interaction tuning epoch: 7, train loss: 0.04753, val loss: 0.04769\n",
      "Interaction tuning epoch: 8, train loss: 0.04804, val loss: 0.04839\n",
      "Interaction tuning epoch: 9, train loss: 0.04656, val loss: 0.04746\n",
      "Interaction tuning epoch: 10, train loss: 0.04708, val loss: 0.04792\n",
      "Interaction tuning epoch: 11, train loss: 0.04593, val loss: 0.04607\n",
      "Interaction tuning epoch: 12, train loss: 0.04889, val loss: 0.05019\n",
      "Interaction tuning epoch: 13, train loss: 0.04756, val loss: 0.04795\n",
      "Interaction tuning epoch: 14, train loss: 0.05056, val loss: 0.04891\n",
      "Interaction tuning epoch: 15, train loss: 0.05649, val loss: 0.05700\n",
      "Interaction tuning epoch: 16, train loss: 0.04770, val loss: 0.04777\n",
      "Interaction tuning epoch: 17, train loss: 0.04665, val loss: 0.04734\n",
      "Interaction tuning epoch: 18, train loss: 0.04709, val loss: 0.04763\n",
      "Interaction tuning epoch: 19, train loss: 0.04762, val loss: 0.04775\n",
      "Interaction tuning epoch: 20, train loss: 0.04647, val loss: 0.04691\n",
      "Interaction tuning epoch: 21, train loss: 0.04858, val loss: 0.04904\n",
      "Interaction tuning epoch: 22, train loss: 0.04763, val loss: 0.04743\n",
      "Interaction tuning epoch: 23, train loss: 0.04883, val loss: 0.04879\n",
      "Interaction tuning epoch: 24, train loss: 0.04608, val loss: 0.04728\n",
      "Interaction tuning epoch: 25, train loss: 0.04999, val loss: 0.04940\n",
      "Interaction tuning epoch: 26, train loss: 0.04916, val loss: 0.04857\n",
      "Interaction tuning epoch: 27, train loss: 0.04784, val loss: 0.04855\n",
      "Interaction tuning epoch: 28, train loss: 0.04702, val loss: 0.04758\n",
      "Interaction tuning epoch: 29, train loss: 0.04685, val loss: 0.04720\n",
      "Interaction tuning epoch: 30, train loss: 0.04688, val loss: 0.04800\n",
      "Interaction tuning epoch: 31, train loss: 0.04656, val loss: 0.04725\n",
      "Interaction tuning epoch: 32, train loss: 0.04698, val loss: 0.04718\n",
      "Interaction tuning epoch: 33, train loss: 0.04648, val loss: 0.04674\n",
      "Interaction tuning epoch: 34, train loss: 0.04622, val loss: 0.04627\n",
      "Interaction tuning epoch: 35, train loss: 0.04695, val loss: 0.04653\n",
      "Interaction tuning epoch: 36, train loss: 0.04638, val loss: 0.04708\n",
      "Interaction tuning epoch: 37, train loss: 0.04517, val loss: 0.04599\n",
      "Interaction tuning epoch: 38, train loss: 0.04594, val loss: 0.04626\n",
      "Interaction tuning epoch: 39, train loss: 0.04806, val loss: 0.04884\n",
      "Interaction tuning epoch: 40, train loss: 0.04762, val loss: 0.04781\n",
      "Interaction tuning epoch: 41, train loss: 0.04762, val loss: 0.04909\n",
      "Interaction tuning epoch: 42, train loss: 0.04746, val loss: 0.04805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 43, train loss: 0.04604, val loss: 0.04612\n",
      "Interaction tuning epoch: 44, train loss: 0.04636, val loss: 0.04673\n",
      "Interaction tuning epoch: 45, train loss: 0.04643, val loss: 0.04712\n",
      "Interaction tuning epoch: 46, train loss: 0.04656, val loss: 0.04678\n",
      "Interaction tuning epoch: 47, train loss: 0.04635, val loss: 0.04667\n",
      "Interaction tuning epoch: 48, train loss: 0.04816, val loss: 0.04882\n",
      "Interaction tuning epoch: 49, train loss: 0.04683, val loss: 0.04711\n",
      "Interaction tuning epoch: 50, train loss: 0.04845, val loss: 0.04905\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 33.804330587387085\n",
      "After the gam stage, training error is 0.04845 , validation error is 0.04905\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 1.232198\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.038219 validation MAE=0.046685,rank=6\n",
      "[SoftImpute] Iter 2: observed MAE=0.034932 validation MAE=0.045743,rank=6\n",
      "[SoftImpute] Iter 3: observed MAE=0.032323 validation MAE=0.044931,rank=6\n",
      "[SoftImpute] Iter 4: observed MAE=0.030190 validation MAE=0.044244,rank=6\n",
      "[SoftImpute] Iter 5: observed MAE=0.028410 validation MAE=0.043656,rank=6\n",
      "[SoftImpute] Iter 6: observed MAE=0.026894 validation MAE=0.043144,rank=6\n",
      "[SoftImpute] Iter 7: observed MAE=0.025584 validation MAE=0.042713,rank=6\n",
      "[SoftImpute] Iter 8: observed MAE=0.024431 validation MAE=0.042306,rank=6\n",
      "[SoftImpute] Iter 9: observed MAE=0.023408 validation MAE=0.041937,rank=6\n",
      "[SoftImpute] Iter 10: observed MAE=0.022497 validation MAE=0.041590,rank=6\n",
      "[SoftImpute] Iter 11: observed MAE=0.021677 validation MAE=0.041270,rank=6\n",
      "[SoftImpute] Iter 12: observed MAE=0.020938 validation MAE=0.040985,rank=6\n",
      "[SoftImpute] Iter 13: observed MAE=0.020269 validation MAE=0.040722,rank=6\n",
      "[SoftImpute] Iter 14: observed MAE=0.019653 validation MAE=0.040488,rank=6\n",
      "[SoftImpute] Iter 15: observed MAE=0.019089 validation MAE=0.040263,rank=6\n",
      "[SoftImpute] Iter 16: observed MAE=0.018568 validation MAE=0.040046,rank=6\n",
      "[SoftImpute] Iter 17: observed MAE=0.018085 validation MAE=0.039845,rank=6\n",
      "[SoftImpute] Iter 18: observed MAE=0.017638 validation MAE=0.039655,rank=6\n",
      "[SoftImpute] Iter 19: observed MAE=0.017225 validation MAE=0.039469,rank=6\n",
      "[SoftImpute] Iter 20: observed MAE=0.016839 validation MAE=0.039286,rank=6\n",
      "[SoftImpute] Iter 21: observed MAE=0.016481 validation MAE=0.039108,rank=6\n",
      "[SoftImpute] Iter 22: observed MAE=0.016146 validation MAE=0.038932,rank=6\n",
      "[SoftImpute] Iter 23: observed MAE=0.015833 validation MAE=0.038758,rank=6\n",
      "[SoftImpute] Iter 24: observed MAE=0.015539 validation MAE=0.038587,rank=6\n",
      "[SoftImpute] Iter 25: observed MAE=0.015263 validation MAE=0.038422,rank=6\n",
      "[SoftImpute] Iter 26: observed MAE=0.015002 validation MAE=0.038261,rank=6\n",
      "[SoftImpute] Iter 27: observed MAE=0.014754 validation MAE=0.038108,rank=6\n",
      "[SoftImpute] Iter 28: observed MAE=0.014520 validation MAE=0.037959,rank=6\n",
      "[SoftImpute] Iter 29: observed MAE=0.014297 validation MAE=0.037816,rank=6\n",
      "[SoftImpute] Iter 30: observed MAE=0.014086 validation MAE=0.037676,rank=6\n",
      "[SoftImpute] Iter 31: observed MAE=0.013885 validation MAE=0.037546,rank=6\n",
      "[SoftImpute] Iter 32: observed MAE=0.013695 validation MAE=0.037420,rank=6\n",
      "[SoftImpute] Iter 33: observed MAE=0.013513 validation MAE=0.037295,rank=6\n",
      "[SoftImpute] Iter 34: observed MAE=0.013340 validation MAE=0.037175,rank=6\n",
      "[SoftImpute] Iter 35: observed MAE=0.013175 validation MAE=0.037060,rank=6\n",
      "[SoftImpute] Iter 36: observed MAE=0.013018 validation MAE=0.036948,rank=6\n",
      "[SoftImpute] Iter 37: observed MAE=0.012866 validation MAE=0.036841,rank=6\n",
      "[SoftImpute] Iter 38: observed MAE=0.012721 validation MAE=0.036737,rank=6\n",
      "[SoftImpute] Iter 39: observed MAE=0.012581 validation MAE=0.036636,rank=6\n",
      "[SoftImpute] Iter 40: observed MAE=0.012447 validation MAE=0.036538,rank=6\n",
      "[SoftImpute] Iter 41: observed MAE=0.012319 validation MAE=0.036441,rank=6\n",
      "[SoftImpute] Iter 42: observed MAE=0.012195 validation MAE=0.036345,rank=6\n",
      "[SoftImpute] Iter 43: observed MAE=0.012077 validation MAE=0.036252,rank=6\n",
      "[SoftImpute] Iter 44: observed MAE=0.011963 validation MAE=0.036160,rank=6\n",
      "[SoftImpute] Iter 45: observed MAE=0.011854 validation MAE=0.036069,rank=6\n",
      "[SoftImpute] Iter 46: observed MAE=0.011748 validation MAE=0.035978,rank=6\n",
      "[SoftImpute] Iter 47: observed MAE=0.011646 validation MAE=0.035887,rank=6\n",
      "[SoftImpute] Iter 48: observed MAE=0.011548 validation MAE=0.035799,rank=6\n",
      "[SoftImpute] Iter 49: observed MAE=0.011452 validation MAE=0.035712,rank=6\n",
      "[SoftImpute] Iter 50: observed MAE=0.011359 validation MAE=0.035625,rank=6\n",
      "[SoftImpute] Iter 51: observed MAE=0.011270 validation MAE=0.035539,rank=6\n",
      "[SoftImpute] Iter 52: observed MAE=0.011182 validation MAE=0.035453,rank=6\n",
      "[SoftImpute] Iter 53: observed MAE=0.011098 validation MAE=0.035367,rank=6\n",
      "[SoftImpute] Iter 54: observed MAE=0.011016 validation MAE=0.035283,rank=6\n",
      "[SoftImpute] Iter 55: observed MAE=0.010937 validation MAE=0.035201,rank=6\n",
      "[SoftImpute] Iter 56: observed MAE=0.010860 validation MAE=0.035121,rank=6\n",
      "[SoftImpute] Iter 57: observed MAE=0.010786 validation MAE=0.035043,rank=6\n",
      "[SoftImpute] Iter 58: observed MAE=0.010713 validation MAE=0.034967,rank=6\n",
      "[SoftImpute] Iter 59: observed MAE=0.010642 validation MAE=0.034892,rank=6\n",
      "[SoftImpute] Iter 60: observed MAE=0.010573 validation MAE=0.034816,rank=6\n",
      "[SoftImpute] Iter 61: observed MAE=0.010506 validation MAE=0.034740,rank=6\n",
      "[SoftImpute] Iter 62: observed MAE=0.010441 validation MAE=0.034664,rank=6\n",
      "[SoftImpute] Iter 63: observed MAE=0.010377 validation MAE=0.034591,rank=6\n",
      "[SoftImpute] Iter 64: observed MAE=0.010315 validation MAE=0.034520,rank=6\n",
      "[SoftImpute] Iter 65: observed MAE=0.010254 validation MAE=0.034450,rank=6\n",
      "[SoftImpute] Iter 66: observed MAE=0.010195 validation MAE=0.034380,rank=6\n",
      "[SoftImpute] Iter 67: observed MAE=0.010137 validation MAE=0.034311,rank=6\n",
      "[SoftImpute] Iter 68: observed MAE=0.010080 validation MAE=0.034242,rank=6\n",
      "[SoftImpute] Iter 69: observed MAE=0.010024 validation MAE=0.034174,rank=6\n",
      "[SoftImpute] Iter 70: observed MAE=0.009970 validation MAE=0.034106,rank=6\n",
      "[SoftImpute] Iter 71: observed MAE=0.009917 validation MAE=0.034039,rank=6\n",
      "[SoftImpute] Iter 72: observed MAE=0.009865 validation MAE=0.033973,rank=6\n",
      "[SoftImpute] Iter 73: observed MAE=0.009814 validation MAE=0.033907,rank=6\n",
      "[SoftImpute] Iter 74: observed MAE=0.009765 validation MAE=0.033844,rank=6\n",
      "[SoftImpute] Iter 75: observed MAE=0.009716 validation MAE=0.033782,rank=6\n",
      "[SoftImpute] Iter 76: observed MAE=0.009669 validation MAE=0.033721,rank=6\n",
      "[SoftImpute] Iter 77: observed MAE=0.009623 validation MAE=0.033659,rank=6\n",
      "[SoftImpute] Iter 78: observed MAE=0.009578 validation MAE=0.033599,rank=6\n",
      "[SoftImpute] Iter 79: observed MAE=0.009534 validation MAE=0.033539,rank=6\n",
      "[SoftImpute] Iter 80: observed MAE=0.009490 validation MAE=0.033480,rank=6\n",
      "[SoftImpute] Iter 81: observed MAE=0.009448 validation MAE=0.033420,rank=6\n",
      "[SoftImpute] Iter 82: observed MAE=0.009406 validation MAE=0.033360,rank=6\n",
      "[SoftImpute] Iter 83: observed MAE=0.009366 validation MAE=0.033301,rank=6\n",
      "[SoftImpute] Iter 84: observed MAE=0.009326 validation MAE=0.033242,rank=6\n",
      "[SoftImpute] Iter 85: observed MAE=0.009287 validation MAE=0.033184,rank=6\n",
      "[SoftImpute] Iter 86: observed MAE=0.009249 validation MAE=0.033128,rank=6\n",
      "[SoftImpute] Iter 87: observed MAE=0.009211 validation MAE=0.033072,rank=6\n",
      "[SoftImpute] Iter 88: observed MAE=0.009174 validation MAE=0.033017,rank=6\n",
      "[SoftImpute] Iter 89: observed MAE=0.009137 validation MAE=0.032963,rank=6\n",
      "[SoftImpute] Iter 90: observed MAE=0.009101 validation MAE=0.032910,rank=6\n",
      "[SoftImpute] Iter 91: observed MAE=0.009066 validation MAE=0.032858,rank=6\n",
      "[SoftImpute] Iter 92: observed MAE=0.009031 validation MAE=0.032806,rank=6\n",
      "[SoftImpute] Iter 93: observed MAE=0.008997 validation MAE=0.032755,rank=6\n",
      "[SoftImpute] Iter 94: observed MAE=0.008964 validation MAE=0.032704,rank=6\n",
      "[SoftImpute] Iter 95: observed MAE=0.008930 validation MAE=0.032654,rank=6\n",
      "[SoftImpute] Iter 96: observed MAE=0.008898 validation MAE=0.032604,rank=6\n",
      "[SoftImpute] Iter 97: observed MAE=0.008866 validation MAE=0.032555,rank=6\n",
      "[SoftImpute] Iter 98: observed MAE=0.008834 validation MAE=0.032505,rank=6\n",
      "[SoftImpute] Iter 99: observed MAE=0.008803 validation MAE=0.032457,rank=6\n",
      "[SoftImpute] Iter 100: observed MAE=0.008772 validation MAE=0.032409,rank=6\n",
      "[SoftImpute] Iter 101: observed MAE=0.008742 validation MAE=0.032360,rank=6\n",
      "[SoftImpute] Iter 102: observed MAE=0.008713 validation MAE=0.032312,rank=6\n",
      "[SoftImpute] Iter 103: observed MAE=0.008684 validation MAE=0.032264,rank=6\n",
      "[SoftImpute] Iter 104: observed MAE=0.008655 validation MAE=0.032216,rank=6\n",
      "[SoftImpute] Iter 105: observed MAE=0.008627 validation MAE=0.032169,rank=6\n",
      "[SoftImpute] Iter 106: observed MAE=0.008599 validation MAE=0.032123,rank=6\n",
      "[SoftImpute] Iter 107: observed MAE=0.008572 validation MAE=0.032078,rank=6\n",
      "[SoftImpute] Iter 108: observed MAE=0.008545 validation MAE=0.032032,rank=6\n",
      "[SoftImpute] Iter 109: observed MAE=0.008518 validation MAE=0.031986,rank=6\n",
      "[SoftImpute] Iter 110: observed MAE=0.008492 validation MAE=0.031941,rank=6\n",
      "[SoftImpute] Iter 111: observed MAE=0.008466 validation MAE=0.031896,rank=6\n",
      "[SoftImpute] Iter 112: observed MAE=0.008441 validation MAE=0.031851,rank=6\n",
      "[SoftImpute] Iter 113: observed MAE=0.008415 validation MAE=0.031806,rank=6\n",
      "[SoftImpute] Iter 114: observed MAE=0.008390 validation MAE=0.031761,rank=6\n",
      "[SoftImpute] Iter 115: observed MAE=0.008366 validation MAE=0.031716,rank=6\n",
      "[SoftImpute] Iter 116: observed MAE=0.008342 validation MAE=0.031671,rank=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 117: observed MAE=0.008318 validation MAE=0.031626,rank=6\n",
      "[SoftImpute] Iter 118: observed MAE=0.008295 validation MAE=0.031581,rank=6\n",
      "[SoftImpute] Iter 119: observed MAE=0.008271 validation MAE=0.031537,rank=6\n",
      "[SoftImpute] Iter 120: observed MAE=0.008248 validation MAE=0.031492,rank=6\n",
      "[SoftImpute] Iter 121: observed MAE=0.008226 validation MAE=0.031448,rank=6\n",
      "[SoftImpute] Iter 122: observed MAE=0.008203 validation MAE=0.031403,rank=6\n",
      "[SoftImpute] Iter 123: observed MAE=0.008181 validation MAE=0.031359,rank=6\n",
      "[SoftImpute] Iter 124: observed MAE=0.008160 validation MAE=0.031314,rank=6\n",
      "[SoftImpute] Iter 125: observed MAE=0.008138 validation MAE=0.031270,rank=6\n",
      "[SoftImpute] Iter 126: observed MAE=0.008117 validation MAE=0.031225,rank=6\n",
      "[SoftImpute] Iter 127: observed MAE=0.008096 validation MAE=0.031181,rank=6\n",
      "[SoftImpute] Iter 128: observed MAE=0.008075 validation MAE=0.031137,rank=6\n",
      "[SoftImpute] Iter 129: observed MAE=0.008055 validation MAE=0.031093,rank=6\n",
      "[SoftImpute] Iter 130: observed MAE=0.008034 validation MAE=0.031049,rank=6\n",
      "[SoftImpute] Iter 131: observed MAE=0.008014 validation MAE=0.031005,rank=6\n",
      "[SoftImpute] Iter 132: observed MAE=0.007994 validation MAE=0.030963,rank=6\n",
      "[SoftImpute] Iter 133: observed MAE=0.007975 validation MAE=0.030921,rank=6\n",
      "[SoftImpute] Iter 134: observed MAE=0.007955 validation MAE=0.030880,rank=6\n",
      "[SoftImpute] Iter 135: observed MAE=0.007936 validation MAE=0.030838,rank=6\n",
      "[SoftImpute] Iter 136: observed MAE=0.007917 validation MAE=0.030797,rank=6\n",
      "[SoftImpute] Iter 137: observed MAE=0.007898 validation MAE=0.030756,rank=6\n",
      "[SoftImpute] Iter 138: observed MAE=0.007880 validation MAE=0.030715,rank=6\n",
      "[SoftImpute] Iter 139: observed MAE=0.007861 validation MAE=0.030674,rank=6\n",
      "[SoftImpute] Iter 140: observed MAE=0.007843 validation MAE=0.030632,rank=6\n",
      "[SoftImpute] Iter 141: observed MAE=0.007825 validation MAE=0.030591,rank=6\n",
      "[SoftImpute] Iter 142: observed MAE=0.007807 validation MAE=0.030550,rank=6\n",
      "[SoftImpute] Iter 143: observed MAE=0.007790 validation MAE=0.030510,rank=6\n",
      "[SoftImpute] Iter 144: observed MAE=0.007772 validation MAE=0.030469,rank=6\n",
      "[SoftImpute] Iter 145: observed MAE=0.007755 validation MAE=0.030429,rank=6\n",
      "[SoftImpute] Iter 146: observed MAE=0.007738 validation MAE=0.030389,rank=6\n",
      "[SoftImpute] Iter 147: observed MAE=0.007721 validation MAE=0.030349,rank=6\n",
      "[SoftImpute] Iter 148: observed MAE=0.007704 validation MAE=0.030309,rank=6\n",
      "[SoftImpute] Iter 149: observed MAE=0.007688 validation MAE=0.030269,rank=6\n",
      "[SoftImpute] Iter 150: observed MAE=0.007672 validation MAE=0.030230,rank=6\n",
      "[SoftImpute] Iter 151: observed MAE=0.007656 validation MAE=0.030191,rank=6\n",
      "[SoftImpute] Iter 152: observed MAE=0.007640 validation MAE=0.030152,rank=6\n",
      "[SoftImpute] Iter 153: observed MAE=0.007624 validation MAE=0.030114,rank=6\n",
      "[SoftImpute] Iter 154: observed MAE=0.007608 validation MAE=0.030076,rank=6\n",
      "[SoftImpute] Iter 155: observed MAE=0.007592 validation MAE=0.030038,rank=6\n",
      "[SoftImpute] Iter 156: observed MAE=0.007577 validation MAE=0.030001,rank=6\n",
      "[SoftImpute] Iter 157: observed MAE=0.007561 validation MAE=0.029965,rank=6\n",
      "[SoftImpute] Iter 158: observed MAE=0.007546 validation MAE=0.029929,rank=6\n",
      "[SoftImpute] Iter 159: observed MAE=0.007531 validation MAE=0.029893,rank=6\n",
      "[SoftImpute] Iter 160: observed MAE=0.007516 validation MAE=0.029857,rank=6\n",
      "[SoftImpute] Iter 161: observed MAE=0.007502 validation MAE=0.029822,rank=6\n",
      "[SoftImpute] Iter 162: observed MAE=0.007487 validation MAE=0.029786,rank=6\n",
      "[SoftImpute] Iter 163: observed MAE=0.007472 validation MAE=0.029751,rank=6\n",
      "[SoftImpute] Iter 164: observed MAE=0.007458 validation MAE=0.029716,rank=6\n",
      "[SoftImpute] Iter 165: observed MAE=0.007444 validation MAE=0.029681,rank=6\n",
      "[SoftImpute] Iter 166: observed MAE=0.007430 validation MAE=0.029645,rank=6\n",
      "[SoftImpute] Iter 167: observed MAE=0.007416 validation MAE=0.029610,rank=6\n",
      "[SoftImpute] Iter 168: observed MAE=0.007402 validation MAE=0.029575,rank=6\n",
      "[SoftImpute] Iter 169: observed MAE=0.007388 validation MAE=0.029540,rank=6\n",
      "[SoftImpute] Iter 170: observed MAE=0.007375 validation MAE=0.029506,rank=6\n",
      "[SoftImpute] Iter 171: observed MAE=0.007361 validation MAE=0.029471,rank=6\n",
      "[SoftImpute] Iter 172: observed MAE=0.007348 validation MAE=0.029436,rank=6\n",
      "[SoftImpute] Iter 173: observed MAE=0.007335 validation MAE=0.029401,rank=6\n",
      "[SoftImpute] Iter 174: observed MAE=0.007321 validation MAE=0.029366,rank=6\n",
      "[SoftImpute] Iter 175: observed MAE=0.007308 validation MAE=0.029332,rank=6\n",
      "[SoftImpute] Iter 176: observed MAE=0.007295 validation MAE=0.029297,rank=6\n",
      "[SoftImpute] Iter 177: observed MAE=0.007283 validation MAE=0.029262,rank=6\n",
      "[SoftImpute] Iter 178: observed MAE=0.007270 validation MAE=0.029228,rank=6\n",
      "[SoftImpute] Iter 179: observed MAE=0.007257 validation MAE=0.029194,rank=6\n",
      "[SoftImpute] Iter 180: observed MAE=0.007244 validation MAE=0.029161,rank=6\n",
      "[SoftImpute] Iter 181: observed MAE=0.007232 validation MAE=0.029127,rank=6\n",
      "[SoftImpute] Iter 182: observed MAE=0.007220 validation MAE=0.029094,rank=6\n",
      "[SoftImpute] Iter 183: observed MAE=0.007207 validation MAE=0.029061,rank=6\n",
      "[SoftImpute] Iter 184: observed MAE=0.007195 validation MAE=0.029028,rank=6\n",
      "[SoftImpute] Iter 185: observed MAE=0.007183 validation MAE=0.028995,rank=6\n",
      "[SoftImpute] Iter 186: observed MAE=0.007171 validation MAE=0.028962,rank=6\n",
      "[SoftImpute] Iter 187: observed MAE=0.007159 validation MAE=0.028929,rank=6\n",
      "[SoftImpute] Iter 188: observed MAE=0.007147 validation MAE=0.028896,rank=6\n",
      "[SoftImpute] Iter 189: observed MAE=0.007135 validation MAE=0.028863,rank=6\n",
      "[SoftImpute] Iter 190: observed MAE=0.007123 validation MAE=0.028831,rank=6\n",
      "[SoftImpute] Iter 191: observed MAE=0.007111 validation MAE=0.028799,rank=6\n",
      "[SoftImpute] Iter 192: observed MAE=0.007100 validation MAE=0.028767,rank=6\n",
      "[SoftImpute] Iter 193: observed MAE=0.007088 validation MAE=0.028735,rank=6\n",
      "[SoftImpute] Iter 194: observed MAE=0.007076 validation MAE=0.028704,rank=6\n",
      "[SoftImpute] Iter 195: observed MAE=0.007065 validation MAE=0.028673,rank=6\n",
      "[SoftImpute] Iter 196: observed MAE=0.007053 validation MAE=0.028642,rank=6\n",
      "[SoftImpute] Iter 197: observed MAE=0.007042 validation MAE=0.028612,rank=6\n",
      "[SoftImpute] Iter 198: observed MAE=0.007031 validation MAE=0.028581,rank=6\n",
      "[SoftImpute] Iter 199: observed MAE=0.007019 validation MAE=0.028550,rank=6\n",
      "[SoftImpute] Iter 200: observed MAE=0.007008 validation MAE=0.028519,rank=6\n",
      "[SoftImpute] Iter 201: observed MAE=0.006997 validation MAE=0.028488,rank=6\n",
      "[SoftImpute] Iter 202: observed MAE=0.006986 validation MAE=0.028457,rank=6\n",
      "[SoftImpute] Iter 203: observed MAE=0.006975 validation MAE=0.028426,rank=6\n",
      "[SoftImpute] Iter 204: observed MAE=0.006964 validation MAE=0.028395,rank=6\n",
      "[SoftImpute] Iter 205: observed MAE=0.006953 validation MAE=0.028364,rank=6\n",
      "[SoftImpute] Iter 206: observed MAE=0.006943 validation MAE=0.028332,rank=6\n",
      "[SoftImpute] Iter 207: observed MAE=0.006932 validation MAE=0.028301,rank=6\n",
      "[SoftImpute] Iter 208: observed MAE=0.006921 validation MAE=0.028270,rank=6\n",
      "[SoftImpute] Iter 209: observed MAE=0.006910 validation MAE=0.028238,rank=6\n",
      "[SoftImpute] Iter 210: observed MAE=0.006900 validation MAE=0.028207,rank=6\n",
      "[SoftImpute] Iter 211: observed MAE=0.006889 validation MAE=0.028176,rank=6\n",
      "[SoftImpute] Iter 212: observed MAE=0.006879 validation MAE=0.028144,rank=6\n",
      "[SoftImpute] Iter 213: observed MAE=0.006868 validation MAE=0.028113,rank=6\n",
      "[SoftImpute] Iter 214: observed MAE=0.006858 validation MAE=0.028083,rank=6\n",
      "[SoftImpute] Iter 215: observed MAE=0.006847 validation MAE=0.028052,rank=6\n",
      "[SoftImpute] Iter 216: observed MAE=0.006837 validation MAE=0.028022,rank=6\n",
      "[SoftImpute] Iter 217: observed MAE=0.006827 validation MAE=0.027992,rank=6\n",
      "[SoftImpute] Iter 218: observed MAE=0.006816 validation MAE=0.027963,rank=6\n",
      "[SoftImpute] Iter 219: observed MAE=0.006806 validation MAE=0.027933,rank=6\n",
      "[SoftImpute] Iter 220: observed MAE=0.006796 validation MAE=0.027903,rank=6\n",
      "[SoftImpute] Iter 221: observed MAE=0.006786 validation MAE=0.027872,rank=6\n",
      "[SoftImpute] Iter 222: observed MAE=0.006776 validation MAE=0.027843,rank=6\n",
      "[SoftImpute] Iter 223: observed MAE=0.006766 validation MAE=0.027813,rank=6\n",
      "[SoftImpute] Iter 224: observed MAE=0.006756 validation MAE=0.027784,rank=6\n",
      "[SoftImpute] Iter 225: observed MAE=0.006746 validation MAE=0.027754,rank=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 226: observed MAE=0.006737 validation MAE=0.027725,rank=6\n",
      "[SoftImpute] Iter 227: observed MAE=0.006727 validation MAE=0.027696,rank=6\n",
      "[SoftImpute] Iter 228: observed MAE=0.006717 validation MAE=0.027667,rank=6\n",
      "[SoftImpute] Iter 229: observed MAE=0.006708 validation MAE=0.027637,rank=6\n",
      "[SoftImpute] Iter 230: observed MAE=0.006698 validation MAE=0.027608,rank=6\n",
      "[SoftImpute] Iter 231: observed MAE=0.006689 validation MAE=0.027579,rank=6\n",
      "[SoftImpute] Iter 232: observed MAE=0.006679 validation MAE=0.027550,rank=6\n",
      "[SoftImpute] Iter 233: observed MAE=0.006670 validation MAE=0.027521,rank=6\n",
      "[SoftImpute] Iter 234: observed MAE=0.006660 validation MAE=0.027492,rank=6\n",
      "[SoftImpute] Iter 235: observed MAE=0.006651 validation MAE=0.027463,rank=6\n",
      "[SoftImpute] Iter 236: observed MAE=0.006642 validation MAE=0.027434,rank=6\n",
      "[SoftImpute] Iter 237: observed MAE=0.006632 validation MAE=0.027405,rank=6\n",
      "[SoftImpute] Iter 238: observed MAE=0.006623 validation MAE=0.027376,rank=6\n",
      "[SoftImpute] Iter 239: observed MAE=0.006614 validation MAE=0.027348,rank=6\n",
      "[SoftImpute] Iter 240: observed MAE=0.006605 validation MAE=0.027320,rank=6\n",
      "[SoftImpute] Iter 241: observed MAE=0.006596 validation MAE=0.027292,rank=6\n",
      "[SoftImpute] Iter 242: observed MAE=0.006586 validation MAE=0.027264,rank=6\n",
      "[SoftImpute] Iter 243: observed MAE=0.006577 validation MAE=0.027236,rank=6\n",
      "[SoftImpute] Iter 244: observed MAE=0.006568 validation MAE=0.027208,rank=6\n",
      "[SoftImpute] Iter 245: observed MAE=0.006559 validation MAE=0.027180,rank=6\n",
      "[SoftImpute] Iter 246: observed MAE=0.006551 validation MAE=0.027152,rank=6\n",
      "[SoftImpute] Iter 247: observed MAE=0.006542 validation MAE=0.027124,rank=6\n",
      "[SoftImpute] Iter 248: observed MAE=0.006533 validation MAE=0.027096,rank=6\n",
      "[SoftImpute] Iter 249: observed MAE=0.006524 validation MAE=0.027068,rank=6\n",
      "[SoftImpute] Iter 250: observed MAE=0.006516 validation MAE=0.027040,rank=6\n",
      "[SoftImpute] Iter 251: observed MAE=0.006507 validation MAE=0.027012,rank=6\n",
      "[SoftImpute] Iter 252: observed MAE=0.006498 validation MAE=0.026984,rank=6\n",
      "[SoftImpute] Iter 253: observed MAE=0.006490 validation MAE=0.026956,rank=6\n",
      "[SoftImpute] Iter 254: observed MAE=0.006481 validation MAE=0.026928,rank=6\n",
      "[SoftImpute] Iter 255: observed MAE=0.006473 validation MAE=0.026900,rank=6\n",
      "[SoftImpute] Iter 256: observed MAE=0.006464 validation MAE=0.026873,rank=6\n",
      "[SoftImpute] Iter 257: observed MAE=0.006456 validation MAE=0.026845,rank=6\n",
      "[SoftImpute] Iter 258: observed MAE=0.006447 validation MAE=0.026818,rank=6\n",
      "[SoftImpute] Iter 259: observed MAE=0.006439 validation MAE=0.026790,rank=6\n",
      "[SoftImpute] Iter 260: observed MAE=0.006431 validation MAE=0.026763,rank=6\n",
      "[SoftImpute] Iter 261: observed MAE=0.006423 validation MAE=0.026735,rank=6\n",
      "[SoftImpute] Iter 262: observed MAE=0.006415 validation MAE=0.026708,rank=6\n",
      "[SoftImpute] Iter 263: observed MAE=0.006407 validation MAE=0.026681,rank=6\n",
      "[SoftImpute] Iter 264: observed MAE=0.006399 validation MAE=0.026654,rank=6\n",
      "[SoftImpute] Iter 265: observed MAE=0.006391 validation MAE=0.026627,rank=6\n",
      "[SoftImpute] Iter 266: observed MAE=0.006383 validation MAE=0.026601,rank=6\n",
      "[SoftImpute] Iter 267: observed MAE=0.006375 validation MAE=0.026574,rank=6\n",
      "[SoftImpute] Iter 268: observed MAE=0.006367 validation MAE=0.026547,rank=6\n",
      "[SoftImpute] Iter 269: observed MAE=0.006359 validation MAE=0.026521,rank=6\n",
      "[SoftImpute] Iter 270: observed MAE=0.006351 validation MAE=0.026494,rank=6\n",
      "[SoftImpute] Iter 271: observed MAE=0.006343 validation MAE=0.026468,rank=6\n",
      "[SoftImpute] Iter 272: observed MAE=0.006335 validation MAE=0.026441,rank=6\n",
      "[SoftImpute] Iter 273: observed MAE=0.006327 validation MAE=0.026415,rank=6\n",
      "[SoftImpute] Iter 274: observed MAE=0.006320 validation MAE=0.026389,rank=6\n",
      "[SoftImpute] Iter 275: observed MAE=0.006312 validation MAE=0.026363,rank=6\n",
      "[SoftImpute] Iter 276: observed MAE=0.006304 validation MAE=0.026337,rank=6\n",
      "[SoftImpute] Iter 277: observed MAE=0.006296 validation MAE=0.026312,rank=6\n",
      "[SoftImpute] Iter 278: observed MAE=0.006289 validation MAE=0.026287,rank=6\n",
      "[SoftImpute] Iter 279: observed MAE=0.006281 validation MAE=0.026261,rank=6\n",
      "[SoftImpute] Iter 280: observed MAE=0.006273 validation MAE=0.026236,rank=6\n",
      "[SoftImpute] Iter 281: observed MAE=0.006266 validation MAE=0.026210,rank=6\n",
      "[SoftImpute] Iter 282: observed MAE=0.006258 validation MAE=0.026185,rank=6\n",
      "[SoftImpute] Iter 283: observed MAE=0.006250 validation MAE=0.026159,rank=6\n",
      "[SoftImpute] Iter 284: observed MAE=0.006243 validation MAE=0.026134,rank=6\n",
      "[SoftImpute] Iter 285: observed MAE=0.006235 validation MAE=0.026109,rank=6\n",
      "[SoftImpute] Iter 286: observed MAE=0.006228 validation MAE=0.026084,rank=6\n",
      "[SoftImpute] Iter 287: observed MAE=0.006221 validation MAE=0.026059,rank=6\n",
      "[SoftImpute] Iter 288: observed MAE=0.006213 validation MAE=0.026035,rank=6\n",
      "[SoftImpute] Iter 289: observed MAE=0.006206 validation MAE=0.026011,rank=6\n",
      "[SoftImpute] Iter 290: observed MAE=0.006199 validation MAE=0.025987,rank=6\n",
      "[SoftImpute] Iter 291: observed MAE=0.006191 validation MAE=0.025963,rank=6\n",
      "[SoftImpute] Iter 292: observed MAE=0.006184 validation MAE=0.025939,rank=6\n",
      "[SoftImpute] Iter 293: observed MAE=0.006177 validation MAE=0.025915,rank=6\n",
      "[SoftImpute] Iter 294: observed MAE=0.006170 validation MAE=0.025891,rank=6\n",
      "[SoftImpute] Iter 295: observed MAE=0.006162 validation MAE=0.025867,rank=6\n",
      "[SoftImpute] Iter 296: observed MAE=0.006155 validation MAE=0.025843,rank=6\n",
      "[SoftImpute] Iter 297: observed MAE=0.006148 validation MAE=0.025819,rank=6\n",
      "[SoftImpute] Iter 298: observed MAE=0.006141 validation MAE=0.025796,rank=6\n",
      "[SoftImpute] Iter 299: observed MAE=0.006134 validation MAE=0.025772,rank=6\n",
      "[SoftImpute] Iter 300: observed MAE=0.006126 validation MAE=0.025748,rank=6\n",
      "[SoftImpute] Iter 301: observed MAE=0.006119 validation MAE=0.025724,rank=6\n",
      "[SoftImpute] Iter 302: observed MAE=0.006112 validation MAE=0.025700,rank=6\n",
      "[SoftImpute] Iter 303: observed MAE=0.006105 validation MAE=0.025676,rank=6\n",
      "[SoftImpute] Iter 304: observed MAE=0.006098 validation MAE=0.025652,rank=6\n",
      "[SoftImpute] Iter 305: observed MAE=0.006091 validation MAE=0.025628,rank=6\n",
      "[SoftImpute] Iter 306: observed MAE=0.006084 validation MAE=0.025603,rank=6\n",
      "[SoftImpute] Iter 307: observed MAE=0.006077 validation MAE=0.025579,rank=6\n",
      "[SoftImpute] Iter 308: observed MAE=0.006070 validation MAE=0.025555,rank=6\n",
      "[SoftImpute] Iter 309: observed MAE=0.006063 validation MAE=0.025532,rank=6\n",
      "[SoftImpute] Iter 310: observed MAE=0.006056 validation MAE=0.025508,rank=6\n",
      "[SoftImpute] Iter 311: observed MAE=0.006050 validation MAE=0.025485,rank=6\n",
      "[SoftImpute] Iter 312: observed MAE=0.006043 validation MAE=0.025461,rank=6\n",
      "[SoftImpute] Iter 313: observed MAE=0.006036 validation MAE=0.025438,rank=6\n",
      "[SoftImpute] Iter 314: observed MAE=0.006029 validation MAE=0.025415,rank=6\n",
      "[SoftImpute] Iter 315: observed MAE=0.006022 validation MAE=0.025391,rank=6\n",
      "[SoftImpute] Iter 316: observed MAE=0.006015 validation MAE=0.025368,rank=6\n",
      "[SoftImpute] Iter 317: observed MAE=0.006009 validation MAE=0.025344,rank=6\n",
      "[SoftImpute] Iter 318: observed MAE=0.006002 validation MAE=0.025321,rank=6\n",
      "[SoftImpute] Iter 319: observed MAE=0.005995 validation MAE=0.025297,rank=6\n",
      "[SoftImpute] Iter 320: observed MAE=0.005988 validation MAE=0.025274,rank=6\n",
      "[SoftImpute] Iter 321: observed MAE=0.005981 validation MAE=0.025250,rank=6\n",
      "[SoftImpute] Iter 322: observed MAE=0.005974 validation MAE=0.025227,rank=6\n",
      "[SoftImpute] Iter 323: observed MAE=0.005968 validation MAE=0.025204,rank=6\n",
      "[SoftImpute] Iter 324: observed MAE=0.005961 validation MAE=0.025180,rank=6\n",
      "[SoftImpute] Iter 325: observed MAE=0.005954 validation MAE=0.025157,rank=6\n",
      "[SoftImpute] Iter 326: observed MAE=0.005947 validation MAE=0.025133,rank=6\n",
      "[SoftImpute] Iter 327: observed MAE=0.005941 validation MAE=0.025110,rank=6\n",
      "[SoftImpute] Iter 328: observed MAE=0.005934 validation MAE=0.025086,rank=6\n",
      "[SoftImpute] Iter 329: observed MAE=0.005927 validation MAE=0.025063,rank=6\n",
      "[SoftImpute] Iter 330: observed MAE=0.005920 validation MAE=0.025039,rank=6\n",
      "[SoftImpute] Iter 331: observed MAE=0.005914 validation MAE=0.025016,rank=6\n",
      "[SoftImpute] Iter 332: observed MAE=0.005907 validation MAE=0.024993,rank=6\n",
      "[SoftImpute] Iter 333: observed MAE=0.005901 validation MAE=0.024970,rank=6\n",
      "[SoftImpute] Iter 334: observed MAE=0.005894 validation MAE=0.024948,rank=6\n",
      "[SoftImpute] Iter 335: observed MAE=0.005888 validation MAE=0.024925,rank=6\n",
      "[SoftImpute] Iter 336: observed MAE=0.005881 validation MAE=0.024902,rank=6\n",
      "[SoftImpute] Iter 337: observed MAE=0.005875 validation MAE=0.024879,rank=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 338: observed MAE=0.005868 validation MAE=0.024856,rank=6\n",
      "[SoftImpute] Iter 339: observed MAE=0.005862 validation MAE=0.024833,rank=6\n",
      "[SoftImpute] Iter 340: observed MAE=0.005855 validation MAE=0.024810,rank=6\n",
      "[SoftImpute] Iter 341: observed MAE=0.005849 validation MAE=0.024787,rank=6\n",
      "[SoftImpute] Iter 342: observed MAE=0.005842 validation MAE=0.024764,rank=6\n",
      "[SoftImpute] Iter 343: observed MAE=0.005836 validation MAE=0.024741,rank=6\n",
      "[SoftImpute] Iter 344: observed MAE=0.005829 validation MAE=0.024718,rank=6\n",
      "[SoftImpute] Iter 345: observed MAE=0.005823 validation MAE=0.024695,rank=6\n",
      "[SoftImpute] Iter 346: observed MAE=0.005816 validation MAE=0.024671,rank=6\n",
      "[SoftImpute] Iter 347: observed MAE=0.005810 validation MAE=0.024648,rank=6\n",
      "[SoftImpute] Iter 348: observed MAE=0.005803 validation MAE=0.024625,rank=6\n",
      "[SoftImpute] Iter 349: observed MAE=0.005797 validation MAE=0.024601,rank=6\n",
      "[SoftImpute] Iter 350: observed MAE=0.005790 validation MAE=0.024578,rank=6\n",
      "[SoftImpute] Iter 351: observed MAE=0.005784 validation MAE=0.024554,rank=6\n",
      "[SoftImpute] Iter 352: observed MAE=0.005777 validation MAE=0.024531,rank=6\n",
      "[SoftImpute] Iter 353: observed MAE=0.005771 validation MAE=0.024507,rank=6\n",
      "[SoftImpute] Iter 354: observed MAE=0.005764 validation MAE=0.024484,rank=6\n",
      "[SoftImpute] Iter 355: observed MAE=0.005758 validation MAE=0.024461,rank=6\n",
      "[SoftImpute] Iter 356: observed MAE=0.005751 validation MAE=0.024438,rank=6\n",
      "[SoftImpute] Iter 357: observed MAE=0.005745 validation MAE=0.024415,rank=6\n",
      "[SoftImpute] Iter 358: observed MAE=0.005739 validation MAE=0.024392,rank=6\n",
      "[SoftImpute] Iter 359: observed MAE=0.005732 validation MAE=0.024369,rank=6\n",
      "[SoftImpute] Iter 360: observed MAE=0.005726 validation MAE=0.024346,rank=6\n",
      "[SoftImpute] Iter 361: observed MAE=0.005719 validation MAE=0.024323,rank=6\n",
      "[SoftImpute] Iter 362: observed MAE=0.005713 validation MAE=0.024299,rank=6\n",
      "[SoftImpute] Iter 363: observed MAE=0.005706 validation MAE=0.024276,rank=6\n",
      "[SoftImpute] Iter 364: observed MAE=0.005700 validation MAE=0.024253,rank=6\n",
      "[SoftImpute] Iter 365: observed MAE=0.005693 validation MAE=0.024229,rank=6\n",
      "[SoftImpute] Iter 366: observed MAE=0.005687 validation MAE=0.024206,rank=6\n",
      "[SoftImpute] Iter 367: observed MAE=0.005681 validation MAE=0.024183,rank=6\n",
      "[SoftImpute] Iter 368: observed MAE=0.005674 validation MAE=0.024159,rank=6\n",
      "[SoftImpute] Iter 369: observed MAE=0.005668 validation MAE=0.024136,rank=6\n",
      "[SoftImpute] Iter 370: observed MAE=0.005661 validation MAE=0.024113,rank=6\n",
      "[SoftImpute] Iter 371: observed MAE=0.005655 validation MAE=0.024089,rank=6\n",
      "[SoftImpute] Iter 372: observed MAE=0.005648 validation MAE=0.024066,rank=6\n",
      "[SoftImpute] Iter 373: observed MAE=0.005642 validation MAE=0.024043,rank=6\n",
      "[SoftImpute] Iter 374: observed MAE=0.005636 validation MAE=0.024020,rank=6\n",
      "[SoftImpute] Iter 375: observed MAE=0.005629 validation MAE=0.023997,rank=6\n",
      "[SoftImpute] Iter 376: observed MAE=0.005623 validation MAE=0.023974,rank=6\n",
      "[SoftImpute] Iter 377: observed MAE=0.005616 validation MAE=0.023951,rank=6\n",
      "[SoftImpute] Iter 378: observed MAE=0.005610 validation MAE=0.023928,rank=6\n",
      "[SoftImpute] Iter 379: observed MAE=0.005604 validation MAE=0.023905,rank=6\n",
      "[SoftImpute] Iter 380: observed MAE=0.005597 validation MAE=0.023883,rank=6\n",
      "[SoftImpute] Iter 381: observed MAE=0.005591 validation MAE=0.023861,rank=6\n",
      "[SoftImpute] Iter 382: observed MAE=0.005585 validation MAE=0.023839,rank=6\n",
      "[SoftImpute] Iter 383: observed MAE=0.005578 validation MAE=0.023818,rank=6\n",
      "[SoftImpute] Iter 384: observed MAE=0.005572 validation MAE=0.023796,rank=6\n",
      "[SoftImpute] Iter 385: observed MAE=0.005566 validation MAE=0.023775,rank=6\n",
      "[SoftImpute] Iter 386: observed MAE=0.005560 validation MAE=0.023753,rank=6\n",
      "[SoftImpute] Iter 387: observed MAE=0.005553 validation MAE=0.023732,rank=6\n",
      "[SoftImpute] Iter 388: observed MAE=0.005547 validation MAE=0.023711,rank=6\n",
      "[SoftImpute] Iter 389: observed MAE=0.005541 validation MAE=0.023690,rank=6\n",
      "[SoftImpute] Iter 390: observed MAE=0.005535 validation MAE=0.023670,rank=6\n",
      "[SoftImpute] Iter 391: observed MAE=0.005529 validation MAE=0.023649,rank=6\n",
      "[SoftImpute] Iter 392: observed MAE=0.005522 validation MAE=0.023629,rank=6\n",
      "[SoftImpute] Iter 393: observed MAE=0.005516 validation MAE=0.023609,rank=6\n",
      "[SoftImpute] Iter 394: observed MAE=0.005510 validation MAE=0.023589,rank=6\n",
      "[SoftImpute] Iter 395: observed MAE=0.005504 validation MAE=0.023568,rank=6\n",
      "[SoftImpute] Iter 396: observed MAE=0.005498 validation MAE=0.023548,rank=6\n",
      "[SoftImpute] Iter 397: observed MAE=0.005492 validation MAE=0.023527,rank=6\n",
      "[SoftImpute] Iter 398: observed MAE=0.005486 validation MAE=0.023507,rank=6\n",
      "[SoftImpute] Iter 399: observed MAE=0.005479 validation MAE=0.023487,rank=6\n",
      "[SoftImpute] Iter 400: observed MAE=0.005473 validation MAE=0.023467,rank=6\n",
      "[SoftImpute] Iter 401: observed MAE=0.005467 validation MAE=0.023447,rank=6\n",
      "[SoftImpute] Iter 402: observed MAE=0.005461 validation MAE=0.023428,rank=6\n",
      "[SoftImpute] Iter 403: observed MAE=0.005455 validation MAE=0.023408,rank=6\n",
      "[SoftImpute] Iter 404: observed MAE=0.005449 validation MAE=0.023388,rank=6\n",
      "[SoftImpute] Iter 405: observed MAE=0.005443 validation MAE=0.023369,rank=6\n",
      "[SoftImpute] Iter 406: observed MAE=0.005437 validation MAE=0.023349,rank=6\n",
      "[SoftImpute] Iter 407: observed MAE=0.005431 validation MAE=0.023329,rank=6\n",
      "[SoftImpute] Iter 408: observed MAE=0.005425 validation MAE=0.023310,rank=6\n",
      "[SoftImpute] Iter 409: observed MAE=0.005419 validation MAE=0.023290,rank=6\n",
      "[SoftImpute] Iter 410: observed MAE=0.005413 validation MAE=0.023271,rank=6\n",
      "[SoftImpute] Iter 411: observed MAE=0.005407 validation MAE=0.023252,rank=6\n",
      "[SoftImpute] Iter 412: observed MAE=0.005401 validation MAE=0.023233,rank=6\n",
      "[SoftImpute] Iter 413: observed MAE=0.005395 validation MAE=0.023213,rank=6\n",
      "[SoftImpute] Iter 414: observed MAE=0.005389 validation MAE=0.023194,rank=6\n",
      "[SoftImpute] Iter 415: observed MAE=0.005383 validation MAE=0.023175,rank=6\n",
      "[SoftImpute] Iter 416: observed MAE=0.005377 validation MAE=0.023156,rank=6\n",
      "[SoftImpute] Iter 417: observed MAE=0.005371 validation MAE=0.023137,rank=6\n",
      "[SoftImpute] Iter 418: observed MAE=0.005365 validation MAE=0.023118,rank=6\n",
      "[SoftImpute] Iter 419: observed MAE=0.005360 validation MAE=0.023099,rank=6\n",
      "[SoftImpute] Iter 420: observed MAE=0.005354 validation MAE=0.023080,rank=6\n",
      "[SoftImpute] Iter 421: observed MAE=0.005348 validation MAE=0.023060,rank=6\n",
      "[SoftImpute] Iter 422: observed MAE=0.005342 validation MAE=0.023041,rank=6\n",
      "[SoftImpute] Iter 423: observed MAE=0.005336 validation MAE=0.023022,rank=6\n",
      "[SoftImpute] Iter 424: observed MAE=0.005331 validation MAE=0.023003,rank=6\n",
      "[SoftImpute] Iter 425: observed MAE=0.005325 validation MAE=0.022983,rank=6\n",
      "[SoftImpute] Iter 426: observed MAE=0.005319 validation MAE=0.022964,rank=6\n",
      "[SoftImpute] Iter 427: observed MAE=0.005314 validation MAE=0.022946,rank=6\n",
      "[SoftImpute] Iter 428: observed MAE=0.005308 validation MAE=0.022927,rank=6\n",
      "[SoftImpute] Iter 429: observed MAE=0.005302 validation MAE=0.022909,rank=6\n",
      "[SoftImpute] Iter 430: observed MAE=0.005297 validation MAE=0.022890,rank=6\n",
      "[SoftImpute] Iter 431: observed MAE=0.005291 validation MAE=0.022872,rank=6\n",
      "[SoftImpute] Iter 432: observed MAE=0.005286 validation MAE=0.022854,rank=6\n",
      "[SoftImpute] Iter 433: observed MAE=0.005280 validation MAE=0.022836,rank=6\n",
      "[SoftImpute] Iter 434: observed MAE=0.005275 validation MAE=0.022817,rank=6\n",
      "[SoftImpute] Iter 435: observed MAE=0.005269 validation MAE=0.022799,rank=6\n",
      "[SoftImpute] Iter 436: observed MAE=0.005264 validation MAE=0.022781,rank=6\n",
      "[SoftImpute] Iter 437: observed MAE=0.005258 validation MAE=0.022763,rank=6\n",
      "[SoftImpute] Iter 438: observed MAE=0.005253 validation MAE=0.022745,rank=6\n",
      "[SoftImpute] Iter 439: observed MAE=0.005248 validation MAE=0.022727,rank=6\n",
      "[SoftImpute] Iter 440: observed MAE=0.005242 validation MAE=0.022709,rank=6\n",
      "[SoftImpute] Iter 441: observed MAE=0.005237 validation MAE=0.022691,rank=6\n",
      "[SoftImpute] Iter 442: observed MAE=0.005232 validation MAE=0.022673,rank=6\n",
      "[SoftImpute] Iter 443: observed MAE=0.005226 validation MAE=0.022655,rank=6\n",
      "[SoftImpute] Iter 444: observed MAE=0.005221 validation MAE=0.022637,rank=6\n",
      "[SoftImpute] Iter 445: observed MAE=0.005216 validation MAE=0.022619,rank=6\n",
      "[SoftImpute] Iter 446: observed MAE=0.005211 validation MAE=0.022601,rank=6\n",
      "[SoftImpute] Iter 447: observed MAE=0.005206 validation MAE=0.022583,rank=6\n",
      "[SoftImpute] Iter 448: observed MAE=0.005200 validation MAE=0.022566,rank=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 449: observed MAE=0.005195 validation MAE=0.022548,rank=6\n",
      "[SoftImpute] Iter 450: observed MAE=0.005190 validation MAE=0.022531,rank=6\n",
      "[SoftImpute] Iter 451: observed MAE=0.005185 validation MAE=0.022513,rank=6\n",
      "[SoftImpute] Iter 452: observed MAE=0.005180 validation MAE=0.022496,rank=6\n",
      "[SoftImpute] Iter 453: observed MAE=0.005175 validation MAE=0.022478,rank=6\n",
      "[SoftImpute] Iter 454: observed MAE=0.005170 validation MAE=0.022461,rank=6\n",
      "[SoftImpute] Iter 455: observed MAE=0.005165 validation MAE=0.022443,rank=6\n",
      "[SoftImpute] Iter 456: observed MAE=0.005160 validation MAE=0.022426,rank=6\n",
      "[SoftImpute] Iter 457: observed MAE=0.005155 validation MAE=0.022408,rank=6\n",
      "[SoftImpute] Iter 458: observed MAE=0.005150 validation MAE=0.022391,rank=6\n",
      "[SoftImpute] Iter 459: observed MAE=0.005145 validation MAE=0.022374,rank=6\n",
      "[SoftImpute] Iter 460: observed MAE=0.005140 validation MAE=0.022357,rank=6\n",
      "[SoftImpute] Iter 461: observed MAE=0.005135 validation MAE=0.022340,rank=6\n",
      "[SoftImpute] Iter 462: observed MAE=0.005130 validation MAE=0.022323,rank=6\n",
      "[SoftImpute] Iter 463: observed MAE=0.005125 validation MAE=0.022306,rank=6\n",
      "[SoftImpute] Iter 464: observed MAE=0.005121 validation MAE=0.022289,rank=6\n",
      "[SoftImpute] Iter 465: observed MAE=0.005116 validation MAE=0.022272,rank=6\n",
      "[SoftImpute] Iter 466: observed MAE=0.005111 validation MAE=0.022255,rank=6\n",
      "[SoftImpute] Iter 467: observed MAE=0.005106 validation MAE=0.022238,rank=6\n",
      "[SoftImpute] Iter 468: observed MAE=0.005101 validation MAE=0.022222,rank=6\n",
      "[SoftImpute] Iter 469: observed MAE=0.005097 validation MAE=0.022205,rank=6\n",
      "[SoftImpute] Iter 470: observed MAE=0.005092 validation MAE=0.022189,rank=6\n",
      "[SoftImpute] Iter 471: observed MAE=0.005087 validation MAE=0.022172,rank=6\n",
      "[SoftImpute] Iter 472: observed MAE=0.005083 validation MAE=0.022156,rank=6\n",
      "[SoftImpute] Iter 473: observed MAE=0.005078 validation MAE=0.022139,rank=6\n",
      "[SoftImpute] Iter 474: observed MAE=0.005073 validation MAE=0.022123,rank=6\n",
      "[SoftImpute] Iter 475: observed MAE=0.005069 validation MAE=0.022107,rank=6\n",
      "[SoftImpute] Iter 476: observed MAE=0.005064 validation MAE=0.022090,rank=6\n",
      "[SoftImpute] Iter 477: observed MAE=0.005060 validation MAE=0.022074,rank=6\n",
      "[SoftImpute] Iter 478: observed MAE=0.005055 validation MAE=0.022058,rank=6\n",
      "[SoftImpute] Iter 479: observed MAE=0.005051 validation MAE=0.022042,rank=6\n",
      "[SoftImpute] Iter 480: observed MAE=0.005046 validation MAE=0.022026,rank=6\n",
      "[SoftImpute] Iter 481: observed MAE=0.005042 validation MAE=0.022010,rank=6\n",
      "[SoftImpute] Iter 482: observed MAE=0.005037 validation MAE=0.021994,rank=6\n",
      "[SoftImpute] Iter 483: observed MAE=0.005033 validation MAE=0.021978,rank=6\n",
      "[SoftImpute] Iter 484: observed MAE=0.005029 validation MAE=0.021962,rank=6\n",
      "[SoftImpute] Iter 485: observed MAE=0.005024 validation MAE=0.021947,rank=6\n",
      "[SoftImpute] Iter 486: observed MAE=0.005020 validation MAE=0.021931,rank=6\n",
      "[SoftImpute] Iter 487: observed MAE=0.005016 validation MAE=0.021916,rank=6\n",
      "[SoftImpute] Iter 488: observed MAE=0.005012 validation MAE=0.021900,rank=6\n",
      "[SoftImpute] Iter 489: observed MAE=0.005007 validation MAE=0.021885,rank=6\n",
      "[SoftImpute] Iter 490: observed MAE=0.005003 validation MAE=0.021870,rank=6\n",
      "[SoftImpute] Iter 491: observed MAE=0.004999 validation MAE=0.021855,rank=6\n",
      "[SoftImpute] Iter 492: observed MAE=0.004995 validation MAE=0.021839,rank=6\n",
      "[SoftImpute] Iter 493: observed MAE=0.004991 validation MAE=0.021824,rank=6\n",
      "[SoftImpute] Iter 494: observed MAE=0.004987 validation MAE=0.021809,rank=6\n",
      "[SoftImpute] Iter 495: observed MAE=0.004982 validation MAE=0.021794,rank=6\n",
      "[SoftImpute] Iter 496: observed MAE=0.004978 validation MAE=0.021779,rank=6\n",
      "[SoftImpute] Iter 497: observed MAE=0.004974 validation MAE=0.021764,rank=6\n",
      "[SoftImpute] Iter 498: observed MAE=0.004970 validation MAE=0.021749,rank=6\n",
      "[SoftImpute] Iter 499: observed MAE=0.004966 validation MAE=0.021735,rank=6\n",
      "[SoftImpute] Iter 500: observed MAE=0.004962 validation MAE=0.021720,rank=6\n",
      "[SoftImpute] Stopped after iteration 500 for lambda=0.024644\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 15.314727783203125\n",
      "After the matrix factor stage, training error is 0.00496, validation error is 0.02172\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.30185, val loss: 0.30478\n",
      "Main effects training epoch: 2, train loss: 0.23228, val loss: 0.23617\n",
      "Main effects training epoch: 3, train loss: 0.17997, val loss: 0.18315\n",
      "Main effects training epoch: 4, train loss: 0.14845, val loss: 0.15215\n",
      "Main effects training epoch: 5, train loss: 0.13484, val loss: 0.13695\n",
      "Main effects training epoch: 6, train loss: 0.13033, val loss: 0.13155\n",
      "Main effects training epoch: 7, train loss: 0.13055, val loss: 0.13149\n",
      "Main effects training epoch: 8, train loss: 0.12971, val loss: 0.12988\n",
      "Main effects training epoch: 9, train loss: 0.12862, val loss: 0.12919\n",
      "Main effects training epoch: 10, train loss: 0.12828, val loss: 0.12938\n",
      "Main effects training epoch: 11, train loss: 0.12732, val loss: 0.12789\n",
      "Main effects training epoch: 12, train loss: 0.12659, val loss: 0.12743\n",
      "Main effects training epoch: 13, train loss: 0.12563, val loss: 0.12654\n",
      "Main effects training epoch: 14, train loss: 0.12325, val loss: 0.12456\n",
      "Main effects training epoch: 15, train loss: 0.11949, val loss: 0.12161\n",
      "Main effects training epoch: 16, train loss: 0.11686, val loss: 0.11922\n",
      "Main effects training epoch: 17, train loss: 0.11423, val loss: 0.11562\n",
      "Main effects training epoch: 18, train loss: 0.11484, val loss: 0.11687\n",
      "Main effects training epoch: 19, train loss: 0.11188, val loss: 0.11473\n",
      "Main effects training epoch: 20, train loss: 0.11089, val loss: 0.11324\n",
      "Main effects training epoch: 21, train loss: 0.10849, val loss: 0.11127\n",
      "Main effects training epoch: 22, train loss: 0.10778, val loss: 0.11002\n",
      "Main effects training epoch: 23, train loss: 0.10761, val loss: 0.10934\n",
      "Main effects training epoch: 24, train loss: 0.10695, val loss: 0.10962\n",
      "Main effects training epoch: 25, train loss: 0.10869, val loss: 0.11093\n",
      "Main effects training epoch: 26, train loss: 0.10721, val loss: 0.11016\n",
      "Main effects training epoch: 27, train loss: 0.10692, val loss: 0.10911\n",
      "Main effects training epoch: 28, train loss: 0.10656, val loss: 0.10921\n",
      "Main effects training epoch: 29, train loss: 0.10666, val loss: 0.10915\n",
      "Main effects training epoch: 30, train loss: 0.10688, val loss: 0.10958\n",
      "Main effects training epoch: 31, train loss: 0.10689, val loss: 0.10943\n",
      "Main effects training epoch: 32, train loss: 0.10622, val loss: 0.10918\n",
      "Main effects training epoch: 33, train loss: 0.10631, val loss: 0.10928\n",
      "Main effects training epoch: 34, train loss: 0.10605, val loss: 0.10868\n",
      "Main effects training epoch: 35, train loss: 0.10599, val loss: 0.10879\n",
      "Main effects training epoch: 36, train loss: 0.10605, val loss: 0.10911\n",
      "Main effects training epoch: 37, train loss: 0.10704, val loss: 0.10981\n",
      "Main effects training epoch: 38, train loss: 0.10655, val loss: 0.10926\n",
      "Main effects training epoch: 39, train loss: 0.10607, val loss: 0.10888\n",
      "Main effects training epoch: 40, train loss: 0.10588, val loss: 0.10859\n",
      "Main effects training epoch: 41, train loss: 0.10600, val loss: 0.10889\n",
      "Main effects training epoch: 42, train loss: 0.10616, val loss: 0.10903\n",
      "Main effects training epoch: 43, train loss: 0.10609, val loss: 0.10883\n",
      "Main effects training epoch: 44, train loss: 0.10630, val loss: 0.10899\n",
      "Main effects training epoch: 45, train loss: 0.10686, val loss: 0.10928\n",
      "Main effects training epoch: 46, train loss: 0.10599, val loss: 0.10904\n",
      "Main effects training epoch: 47, train loss: 0.10577, val loss: 0.10887\n",
      "Main effects training epoch: 48, train loss: 0.10584, val loss: 0.10888\n",
      "Main effects training epoch: 49, train loss: 0.10576, val loss: 0.10866\n",
      "Main effects training epoch: 50, train loss: 0.10617, val loss: 0.10903\n",
      "Main effects training epoch: 51, train loss: 0.10592, val loss: 0.10893\n",
      "Main effects training epoch: 52, train loss: 0.10606, val loss: 0.10892\n",
      "Main effects training epoch: 53, train loss: 0.10577, val loss: 0.10888\n",
      "Main effects training epoch: 54, train loss: 0.10586, val loss: 0.10875\n",
      "Main effects training epoch: 55, train loss: 0.10635, val loss: 0.10929\n",
      "Main effects training epoch: 56, train loss: 0.10600, val loss: 0.10895\n",
      "Main effects training epoch: 57, train loss: 0.10614, val loss: 0.10942\n",
      "Main effects training epoch: 58, train loss: 0.10591, val loss: 0.10905\n",
      "Main effects training epoch: 59, train loss: 0.10587, val loss: 0.10866\n",
      "Main effects training epoch: 60, train loss: 0.10580, val loss: 0.10885\n",
      "Main effects training epoch: 61, train loss: 0.10590, val loss: 0.10868\n",
      "Main effects training epoch: 62, train loss: 0.10611, val loss: 0.10911\n",
      "Main effects training epoch: 63, train loss: 0.10587, val loss: 0.10877\n",
      "Main effects training epoch: 64, train loss: 0.10611, val loss: 0.10908\n",
      "Main effects training epoch: 65, train loss: 0.10597, val loss: 0.10888\n",
      "Main effects training epoch: 66, train loss: 0.10579, val loss: 0.10886\n",
      "Main effects training epoch: 67, train loss: 0.10588, val loss: 0.10853\n",
      "Main effects training epoch: 68, train loss: 0.10601, val loss: 0.10926\n",
      "Main effects training epoch: 69, train loss: 0.10583, val loss: 0.10866\n",
      "Main effects training epoch: 70, train loss: 0.10585, val loss: 0.10893\n",
      "Main effects training epoch: 71, train loss: 0.10592, val loss: 0.10879\n",
      "Main effects training epoch: 72, train loss: 0.10582, val loss: 0.10901\n",
      "Main effects training epoch: 73, train loss: 0.10586, val loss: 0.10890\n",
      "Main effects training epoch: 74, train loss: 0.10596, val loss: 0.10893\n",
      "Main effects training epoch: 75, train loss: 0.10614, val loss: 0.10884\n",
      "Main effects training epoch: 76, train loss: 0.10583, val loss: 0.10904\n",
      "Main effects training epoch: 77, train loss: 0.10592, val loss: 0.10890\n",
      "Main effects training epoch: 78, train loss: 0.10580, val loss: 0.10881\n",
      "Main effects training epoch: 79, train loss: 0.10594, val loss: 0.10889\n",
      "Main effects training epoch: 80, train loss: 0.10607, val loss: 0.10888\n",
      "Main effects training epoch: 81, train loss: 0.10581, val loss: 0.10891\n",
      "Main effects training epoch: 82, train loss: 0.10593, val loss: 0.10922\n",
      "Main effects training epoch: 83, train loss: 0.10574, val loss: 0.10873\n",
      "Main effects training epoch: 84, train loss: 0.10573, val loss: 0.10877\n",
      "Main effects training epoch: 85, train loss: 0.10636, val loss: 0.10929\n",
      "Main effects training epoch: 86, train loss: 0.10583, val loss: 0.10874\n",
      "Main effects training epoch: 87, train loss: 0.10614, val loss: 0.10928\n",
      "Main effects training epoch: 88, train loss: 0.10616, val loss: 0.10949\n",
      "Main effects training epoch: 89, train loss: 0.10589, val loss: 0.10865\n",
      "Main effects training epoch: 90, train loss: 0.10604, val loss: 0.10903\n",
      "Main effects training epoch: 91, train loss: 0.10588, val loss: 0.10892\n",
      "Main effects training epoch: 92, train loss: 0.10612, val loss: 0.10925\n",
      "Main effects training epoch: 93, train loss: 0.10583, val loss: 0.10911\n",
      "Main effects training epoch: 94, train loss: 0.10604, val loss: 0.10904\n",
      "Main effects training epoch: 95, train loss: 0.10576, val loss: 0.10903\n",
      "Main effects training epoch: 96, train loss: 0.10568, val loss: 0.10869\n",
      "Main effects training epoch: 97, train loss: 0.10588, val loss: 0.10926\n",
      "Main effects training epoch: 98, train loss: 0.10606, val loss: 0.10910\n",
      "Main effects training epoch: 99, train loss: 0.10649, val loss: 0.10930\n",
      "Main effects training epoch: 100, train loss: 0.10628, val loss: 0.10948\n",
      "Main effects training epoch: 101, train loss: 0.10586, val loss: 0.10912\n",
      "Main effects training epoch: 102, train loss: 0.10570, val loss: 0.10869\n",
      "Main effects training epoch: 103, train loss: 0.10643, val loss: 0.10951\n",
      "Main effects training epoch: 104, train loss: 0.10650, val loss: 0.10946\n",
      "Main effects training epoch: 105, train loss: 0.10626, val loss: 0.10954\n",
      "Main effects training epoch: 106, train loss: 0.10594, val loss: 0.10896\n",
      "Main effects training epoch: 107, train loss: 0.10579, val loss: 0.10878\n",
      "Main effects training epoch: 108, train loss: 0.10607, val loss: 0.10922\n",
      "Main effects training epoch: 109, train loss: 0.10581, val loss: 0.10902\n",
      "Main effects training epoch: 110, train loss: 0.10590, val loss: 0.10892\n",
      "Main effects training epoch: 111, train loss: 0.10605, val loss: 0.10924\n",
      "Main effects training epoch: 112, train loss: 0.10580, val loss: 0.10875\n",
      "Main effects training epoch: 113, train loss: 0.10587, val loss: 0.10900\n",
      "Main effects training epoch: 114, train loss: 0.10581, val loss: 0.10883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 115, train loss: 0.10618, val loss: 0.10915\n",
      "Main effects training epoch: 116, train loss: 0.10615, val loss: 0.10949\n",
      "Main effects training epoch: 117, train loss: 0.10602, val loss: 0.10894\n",
      "Main effects training epoch: 118, train loss: 0.10595, val loss: 0.10914\n",
      "Main effects training epoch: 119, train loss: 0.10618, val loss: 0.10895\n",
      "Main effects training epoch: 120, train loss: 0.10601, val loss: 0.10941\n",
      "Main effects training epoch: 121, train loss: 0.10588, val loss: 0.10869\n",
      "Main effects training epoch: 122, train loss: 0.10576, val loss: 0.10885\n",
      "Main effects training epoch: 123, train loss: 0.10578, val loss: 0.10899\n",
      "Main effects training epoch: 124, train loss: 0.10583, val loss: 0.10934\n",
      "Main effects training epoch: 125, train loss: 0.10610, val loss: 0.10904\n",
      "Main effects training epoch: 126, train loss: 0.10593, val loss: 0.10917\n",
      "Main effects training epoch: 127, train loss: 0.10579, val loss: 0.10863\n",
      "Main effects training epoch: 128, train loss: 0.10583, val loss: 0.10933\n",
      "Main effects training epoch: 129, train loss: 0.10580, val loss: 0.10919\n",
      "Main effects training epoch: 130, train loss: 0.10573, val loss: 0.10874\n",
      "Main effects training epoch: 131, train loss: 0.10601, val loss: 0.10951\n",
      "Main effects training epoch: 132, train loss: 0.10600, val loss: 0.10905\n",
      "Main effects training epoch: 133, train loss: 0.10601, val loss: 0.10931\n",
      "Main effects training epoch: 134, train loss: 0.10566, val loss: 0.10882\n",
      "Main effects training epoch: 135, train loss: 0.10590, val loss: 0.10909\n",
      "Main effects training epoch: 136, train loss: 0.10621, val loss: 0.10930\n",
      "Main effects training epoch: 137, train loss: 0.10597, val loss: 0.10906\n",
      "Main effects training epoch: 138, train loss: 0.10583, val loss: 0.10895\n",
      "Main effects training epoch: 139, train loss: 0.10580, val loss: 0.10894\n",
      "Main effects training epoch: 140, train loss: 0.10591, val loss: 0.10891\n",
      "Main effects training epoch: 141, train loss: 0.10595, val loss: 0.10903\n",
      "Main effects training epoch: 142, train loss: 0.10601, val loss: 0.10932\n",
      "Main effects training epoch: 143, train loss: 0.10596, val loss: 0.10892\n",
      "Main effects training epoch: 144, train loss: 0.10583, val loss: 0.10925\n",
      "Main effects training epoch: 145, train loss: 0.10572, val loss: 0.10887\n",
      "Main effects training epoch: 146, train loss: 0.10574, val loss: 0.10897\n",
      "Main effects training epoch: 147, train loss: 0.10575, val loss: 0.10903\n",
      "Main effects training epoch: 148, train loss: 0.10589, val loss: 0.10894\n",
      "Main effects training epoch: 149, train loss: 0.10577, val loss: 0.10889\n",
      "Main effects training epoch: 150, train loss: 0.10569, val loss: 0.10895\n",
      "Main effects training epoch: 151, train loss: 0.10575, val loss: 0.10884\n",
      "Main effects training epoch: 152, train loss: 0.10606, val loss: 0.10908\n",
      "Main effects training epoch: 153, train loss: 0.10639, val loss: 0.10965\n",
      "Main effects training epoch: 154, train loss: 0.10596, val loss: 0.10920\n",
      "Main effects training epoch: 155, train loss: 0.10578, val loss: 0.10908\n",
      "Main effects training epoch: 156, train loss: 0.10569, val loss: 0.10881\n",
      "Main effects training epoch: 157, train loss: 0.10570, val loss: 0.10898\n",
      "Main effects training epoch: 158, train loss: 0.10595, val loss: 0.10931\n",
      "Main effects training epoch: 159, train loss: 0.10577, val loss: 0.10920\n",
      "Main effects training epoch: 160, train loss: 0.10578, val loss: 0.10935\n",
      "Main effects training epoch: 161, train loss: 0.10579, val loss: 0.10872\n",
      "Main effects training epoch: 162, train loss: 0.10575, val loss: 0.10904\n",
      "Main effects training epoch: 163, train loss: 0.10597, val loss: 0.10913\n",
      "Main effects training epoch: 164, train loss: 0.10605, val loss: 0.10923\n",
      "Main effects training epoch: 165, train loss: 0.10604, val loss: 0.10954\n",
      "Main effects training epoch: 166, train loss: 0.10575, val loss: 0.10883\n",
      "Main effects training epoch: 167, train loss: 0.10578, val loss: 0.10918\n",
      "Main effects training epoch: 168, train loss: 0.10576, val loss: 0.10888\n",
      "Early stop at epoch 168, with validation loss: 0.10888\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.10587, val loss: 0.10915\n",
      "Main effects tuning epoch: 2, train loss: 0.10583, val loss: 0.10863\n",
      "Main effects tuning epoch: 3, train loss: 0.10593, val loss: 0.10918\n",
      "Main effects tuning epoch: 4, train loss: 0.10580, val loss: 0.10863\n",
      "Main effects tuning epoch: 5, train loss: 0.10595, val loss: 0.10896\n",
      "Main effects tuning epoch: 6, train loss: 0.10614, val loss: 0.10907\n",
      "Main effects tuning epoch: 7, train loss: 0.10617, val loss: 0.10891\n",
      "Main effects tuning epoch: 8, train loss: 0.10639, val loss: 0.10952\n",
      "Main effects tuning epoch: 9, train loss: 0.10631, val loss: 0.10952\n",
      "Main effects tuning epoch: 10, train loss: 0.10598, val loss: 0.10913\n",
      "Main effects tuning epoch: 11, train loss: 0.10616, val loss: 0.10872\n",
      "Main effects tuning epoch: 12, train loss: 0.10613, val loss: 0.10940\n",
      "Main effects tuning epoch: 13, train loss: 0.10649, val loss: 0.10913\n",
      "Main effects tuning epoch: 14, train loss: 0.10595, val loss: 0.10906\n",
      "Main effects tuning epoch: 15, train loss: 0.10576, val loss: 0.10901\n",
      "Main effects tuning epoch: 16, train loss: 0.10591, val loss: 0.10867\n",
      "Main effects tuning epoch: 17, train loss: 0.10614, val loss: 0.10929\n",
      "Main effects tuning epoch: 18, train loss: 0.10616, val loss: 0.10914\n",
      "Main effects tuning epoch: 19, train loss: 0.10591, val loss: 0.10901\n",
      "Main effects tuning epoch: 20, train loss: 0.10585, val loss: 0.10893\n",
      "Main effects tuning epoch: 21, train loss: 0.10582, val loss: 0.10851\n",
      "Main effects tuning epoch: 22, train loss: 0.10595, val loss: 0.10917\n",
      "Main effects tuning epoch: 23, train loss: 0.10589, val loss: 0.10893\n",
      "Main effects tuning epoch: 24, train loss: 0.10579, val loss: 0.10899\n",
      "Main effects tuning epoch: 25, train loss: 0.10580, val loss: 0.10907\n",
      "Main effects tuning epoch: 26, train loss: 0.10612, val loss: 0.10929\n",
      "Main effects tuning epoch: 27, train loss: 0.10613, val loss: 0.10892\n",
      "Main effects tuning epoch: 28, train loss: 0.10608, val loss: 0.10898\n",
      "Main effects tuning epoch: 29, train loss: 0.10588, val loss: 0.10904\n",
      "Main effects tuning epoch: 30, train loss: 0.10603, val loss: 0.10875\n",
      "Main effects tuning epoch: 31, train loss: 0.10580, val loss: 0.10885\n",
      "Main effects tuning epoch: 32, train loss: 0.10586, val loss: 0.10892\n",
      "Main effects tuning epoch: 33, train loss: 0.10584, val loss: 0.10877\n",
      "Main effects tuning epoch: 34, train loss: 0.10610, val loss: 0.10890\n",
      "Main effects tuning epoch: 35, train loss: 0.10602, val loss: 0.10950\n",
      "Main effects tuning epoch: 36, train loss: 0.10577, val loss: 0.10870\n",
      "Main effects tuning epoch: 37, train loss: 0.10576, val loss: 0.10890\n",
      "Main effects tuning epoch: 38, train loss: 0.10590, val loss: 0.10894\n",
      "Main effects tuning epoch: 39, train loss: 0.10597, val loss: 0.10915\n",
      "Main effects tuning epoch: 40, train loss: 0.10575, val loss: 0.10856\n",
      "Main effects tuning epoch: 41, train loss: 0.10577, val loss: 0.10899\n",
      "Main effects tuning epoch: 42, train loss: 0.10600, val loss: 0.10888\n",
      "Main effects tuning epoch: 43, train loss: 0.10624, val loss: 0.10938\n",
      "Main effects tuning epoch: 44, train loss: 0.10606, val loss: 0.10863\n",
      "Main effects tuning epoch: 45, train loss: 0.10614, val loss: 0.10961\n",
      "Main effects tuning epoch: 46, train loss: 0.10598, val loss: 0.10874\n",
      "Main effects tuning epoch: 47, train loss: 0.10623, val loss: 0.10916\n",
      "Main effects tuning epoch: 48, train loss: 0.10628, val loss: 0.10910\n",
      "Main effects tuning epoch: 49, train loss: 0.10612, val loss: 0.10902\n",
      "Main effects tuning epoch: 50, train loss: 0.10600, val loss: 0.10922\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.16656, val loss: 0.16504\n",
      "Interaction training epoch: 2, train loss: 0.19799, val loss: 0.19730\n",
      "Interaction training epoch: 3, train loss: 0.07919, val loss: 0.08135\n",
      "Interaction training epoch: 4, train loss: 0.06592, val loss: 0.06695\n",
      "Interaction training epoch: 5, train loss: 0.06719, val loss: 0.06623\n",
      "Interaction training epoch: 6, train loss: 0.07116, val loss: 0.06992\n",
      "Interaction training epoch: 7, train loss: 0.05765, val loss: 0.05869\n",
      "Interaction training epoch: 8, train loss: 0.05791, val loss: 0.05822\n",
      "Interaction training epoch: 9, train loss: 0.06091, val loss: 0.06046\n",
      "Interaction training epoch: 10, train loss: 0.05754, val loss: 0.05788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 11, train loss: 0.05624, val loss: 0.05624\n",
      "Interaction training epoch: 12, train loss: 0.05651, val loss: 0.05600\n",
      "Interaction training epoch: 13, train loss: 0.06101, val loss: 0.06080\n",
      "Interaction training epoch: 14, train loss: 0.05709, val loss: 0.05641\n",
      "Interaction training epoch: 15, train loss: 0.06077, val loss: 0.06101\n",
      "Interaction training epoch: 16, train loss: 0.05239, val loss: 0.05197\n",
      "Interaction training epoch: 17, train loss: 0.05618, val loss: 0.05592\n",
      "Interaction training epoch: 18, train loss: 0.05477, val loss: 0.05468\n",
      "Interaction training epoch: 19, train loss: 0.05152, val loss: 0.05113\n",
      "Interaction training epoch: 20, train loss: 0.05519, val loss: 0.05482\n",
      "Interaction training epoch: 21, train loss: 0.05818, val loss: 0.05633\n",
      "Interaction training epoch: 22, train loss: 0.05614, val loss: 0.05550\n",
      "Interaction training epoch: 23, train loss: 0.05384, val loss: 0.05252\n",
      "Interaction training epoch: 24, train loss: 0.05143, val loss: 0.05136\n",
      "Interaction training epoch: 25, train loss: 0.05185, val loss: 0.05118\n",
      "Interaction training epoch: 26, train loss: 0.05409, val loss: 0.05515\n",
      "Interaction training epoch: 27, train loss: 0.05171, val loss: 0.05189\n",
      "Interaction training epoch: 28, train loss: 0.05284, val loss: 0.05307\n",
      "Interaction training epoch: 29, train loss: 0.05251, val loss: 0.05306\n",
      "Interaction training epoch: 30, train loss: 0.05654, val loss: 0.05667\n",
      "Interaction training epoch: 31, train loss: 0.05008, val loss: 0.05013\n",
      "Interaction training epoch: 32, train loss: 0.05165, val loss: 0.05175\n",
      "Interaction training epoch: 33, train loss: 0.05094, val loss: 0.05147\n",
      "Interaction training epoch: 34, train loss: 0.05154, val loss: 0.05255\n",
      "Interaction training epoch: 35, train loss: 0.05178, val loss: 0.05163\n",
      "Interaction training epoch: 36, train loss: 0.05189, val loss: 0.05137\n",
      "Interaction training epoch: 37, train loss: 0.05261, val loss: 0.05262\n",
      "Interaction training epoch: 38, train loss: 0.05395, val loss: 0.05444\n",
      "Interaction training epoch: 39, train loss: 0.05053, val loss: 0.05093\n",
      "Interaction training epoch: 40, train loss: 0.05186, val loss: 0.05165\n",
      "Interaction training epoch: 41, train loss: 0.05174, val loss: 0.05183\n",
      "Interaction training epoch: 42, train loss: 0.05645, val loss: 0.05707\n",
      "Interaction training epoch: 43, train loss: 0.05130, val loss: 0.05143\n",
      "Interaction training epoch: 44, train loss: 0.05233, val loss: 0.05213\n",
      "Interaction training epoch: 45, train loss: 0.05242, val loss: 0.05266\n",
      "Interaction training epoch: 46, train loss: 0.05234, val loss: 0.05256\n",
      "Interaction training epoch: 47, train loss: 0.05096, val loss: 0.05121\n",
      "Interaction training epoch: 48, train loss: 0.05324, val loss: 0.05317\n",
      "Interaction training epoch: 49, train loss: 0.05143, val loss: 0.05175\n",
      "Interaction training epoch: 50, train loss: 0.05906, val loss: 0.05866\n",
      "Interaction training epoch: 51, train loss: 0.05278, val loss: 0.05220\n",
      "Interaction training epoch: 52, train loss: 0.05429, val loss: 0.05321\n",
      "Interaction training epoch: 53, train loss: 0.05247, val loss: 0.05225\n",
      "Interaction training epoch: 54, train loss: 0.05061, val loss: 0.05129\n",
      "Interaction training epoch: 55, train loss: 0.05239, val loss: 0.05237\n",
      "Interaction training epoch: 56, train loss: 0.05174, val loss: 0.05287\n",
      "Interaction training epoch: 57, train loss: 0.05160, val loss: 0.05111\n",
      "Interaction training epoch: 58, train loss: 0.05180, val loss: 0.05292\n",
      "Interaction training epoch: 59, train loss: 0.04982, val loss: 0.05009\n",
      "Interaction training epoch: 60, train loss: 0.05227, val loss: 0.05368\n",
      "Interaction training epoch: 61, train loss: 0.05217, val loss: 0.05210\n",
      "Interaction training epoch: 62, train loss: 0.05023, val loss: 0.05074\n",
      "Interaction training epoch: 63, train loss: 0.05179, val loss: 0.05284\n",
      "Interaction training epoch: 64, train loss: 0.04959, val loss: 0.05129\n",
      "Interaction training epoch: 65, train loss: 0.05320, val loss: 0.05225\n",
      "Interaction training epoch: 66, train loss: 0.05295, val loss: 0.05326\n",
      "Interaction training epoch: 67, train loss: 0.05028, val loss: 0.05144\n",
      "Interaction training epoch: 68, train loss: 0.05112, val loss: 0.05151\n",
      "Interaction training epoch: 69, train loss: 0.05228, val loss: 0.05248\n",
      "Interaction training epoch: 70, train loss: 0.04994, val loss: 0.05062\n",
      "Interaction training epoch: 71, train loss: 0.05225, val loss: 0.05286\n",
      "Interaction training epoch: 72, train loss: 0.05069, val loss: 0.05180\n",
      "Interaction training epoch: 73, train loss: 0.05077, val loss: 0.05196\n",
      "Interaction training epoch: 74, train loss: 0.04967, val loss: 0.05076\n",
      "Interaction training epoch: 75, train loss: 0.05064, val loss: 0.05071\n",
      "Interaction training epoch: 76, train loss: 0.04886, val loss: 0.05004\n",
      "Interaction training epoch: 77, train loss: 0.05097, val loss: 0.05075\n",
      "Interaction training epoch: 78, train loss: 0.05171, val loss: 0.05128\n",
      "Interaction training epoch: 79, train loss: 0.05178, val loss: 0.05168\n",
      "Interaction training epoch: 80, train loss: 0.04972, val loss: 0.05111\n",
      "Interaction training epoch: 81, train loss: 0.05307, val loss: 0.05407\n",
      "Interaction training epoch: 82, train loss: 0.05287, val loss: 0.05331\n",
      "Interaction training epoch: 83, train loss: 0.05019, val loss: 0.05065\n",
      "Interaction training epoch: 84, train loss: 0.05109, val loss: 0.05177\n",
      "Interaction training epoch: 85, train loss: 0.05241, val loss: 0.05395\n",
      "Interaction training epoch: 86, train loss: 0.05169, val loss: 0.05187\n",
      "Interaction training epoch: 87, train loss: 0.05013, val loss: 0.05031\n",
      "Interaction training epoch: 88, train loss: 0.05249, val loss: 0.05324\n",
      "Interaction training epoch: 89, train loss: 0.04905, val loss: 0.04955\n",
      "Interaction training epoch: 90, train loss: 0.05048, val loss: 0.05104\n",
      "Interaction training epoch: 91, train loss: 0.05291, val loss: 0.05320\n",
      "Interaction training epoch: 92, train loss: 0.05194, val loss: 0.05184\n",
      "Interaction training epoch: 93, train loss: 0.04813, val loss: 0.04857\n",
      "Interaction training epoch: 94, train loss: 0.05434, val loss: 0.05466\n",
      "Interaction training epoch: 95, train loss: 0.05394, val loss: 0.05306\n",
      "Interaction training epoch: 96, train loss: 0.04954, val loss: 0.04995\n",
      "Interaction training epoch: 97, train loss: 0.05003, val loss: 0.05058\n",
      "Interaction training epoch: 98, train loss: 0.05197, val loss: 0.05219\n",
      "Interaction training epoch: 99, train loss: 0.05436, val loss: 0.05280\n",
      "Interaction training epoch: 100, train loss: 0.05350, val loss: 0.05437\n",
      "Interaction training epoch: 101, train loss: 0.04964, val loss: 0.04980\n",
      "Interaction training epoch: 102, train loss: 0.05136, val loss: 0.05087\n",
      "Interaction training epoch: 103, train loss: 0.05162, val loss: 0.05191\n",
      "Interaction training epoch: 104, train loss: 0.05118, val loss: 0.05150\n",
      "Interaction training epoch: 105, train loss: 0.04993, val loss: 0.04937\n",
      "Interaction training epoch: 106, train loss: 0.05171, val loss: 0.05164\n",
      "Interaction training epoch: 107, train loss: 0.05193, val loss: 0.05075\n",
      "Interaction training epoch: 108, train loss: 0.05102, val loss: 0.05183\n",
      "Interaction training epoch: 109, train loss: 0.05261, val loss: 0.05324\n",
      "Interaction training epoch: 110, train loss: 0.05047, val loss: 0.05100\n",
      "Interaction training epoch: 111, train loss: 0.05046, val loss: 0.05063\n",
      "Interaction training epoch: 112, train loss: 0.04811, val loss: 0.04809\n",
      "Interaction training epoch: 113, train loss: 0.05250, val loss: 0.05298\n",
      "Interaction training epoch: 114, train loss: 0.05036, val loss: 0.05112\n",
      "Interaction training epoch: 115, train loss: 0.05119, val loss: 0.05140\n",
      "Interaction training epoch: 116, train loss: 0.04975, val loss: 0.05112\n",
      "Interaction training epoch: 117, train loss: 0.04911, val loss: 0.04994\n",
      "Interaction training epoch: 118, train loss: 0.04990, val loss: 0.05012\n",
      "Interaction training epoch: 119, train loss: 0.04854, val loss: 0.04775\n",
      "Interaction training epoch: 120, train loss: 0.04985, val loss: 0.04987\n",
      "Interaction training epoch: 121, train loss: 0.04779, val loss: 0.04856\n",
      "Interaction training epoch: 122, train loss: 0.04712, val loss: 0.04773\n",
      "Interaction training epoch: 123, train loss: 0.05164, val loss: 0.05255\n",
      "Interaction training epoch: 124, train loss: 0.05327, val loss: 0.05365\n",
      "Interaction training epoch: 125, train loss: 0.05348, val loss: 0.05414\n",
      "Interaction training epoch: 126, train loss: 0.04873, val loss: 0.04978\n",
      "Interaction training epoch: 127, train loss: 0.04866, val loss: 0.04894\n",
      "Interaction training epoch: 128, train loss: 0.04947, val loss: 0.05037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 129, train loss: 0.04701, val loss: 0.04711\n",
      "Interaction training epoch: 130, train loss: 0.04867, val loss: 0.04878\n",
      "Interaction training epoch: 131, train loss: 0.04995, val loss: 0.05004\n",
      "Interaction training epoch: 132, train loss: 0.05004, val loss: 0.05008\n",
      "Interaction training epoch: 133, train loss: 0.04970, val loss: 0.05023\n",
      "Interaction training epoch: 134, train loss: 0.04969, val loss: 0.05041\n",
      "Interaction training epoch: 135, train loss: 0.04826, val loss: 0.04935\n",
      "Interaction training epoch: 136, train loss: 0.04956, val loss: 0.05077\n",
      "Interaction training epoch: 137, train loss: 0.04937, val loss: 0.05116\n",
      "Interaction training epoch: 138, train loss: 0.05042, val loss: 0.05126\n",
      "Interaction training epoch: 139, train loss: 0.04910, val loss: 0.04944\n",
      "Interaction training epoch: 140, train loss: 0.05235, val loss: 0.05329\n",
      "Interaction training epoch: 141, train loss: 0.05151, val loss: 0.05199\n",
      "Interaction training epoch: 142, train loss: 0.04935, val loss: 0.04965\n",
      "Interaction training epoch: 143, train loss: 0.05663, val loss: 0.05667\n",
      "Interaction training epoch: 144, train loss: 0.05217, val loss: 0.05297\n",
      "Interaction training epoch: 145, train loss: 0.05139, val loss: 0.05143\n",
      "Interaction training epoch: 146, train loss: 0.05350, val loss: 0.05388\n",
      "Interaction training epoch: 147, train loss: 0.06195, val loss: 0.06171\n",
      "Interaction training epoch: 148, train loss: 0.05923, val loss: 0.05958\n",
      "Interaction training epoch: 149, train loss: 0.05897, val loss: 0.05842\n",
      "Interaction training epoch: 150, train loss: 0.05688, val loss: 0.05663\n",
      "Interaction training epoch: 151, train loss: 0.05292, val loss: 0.05339\n",
      "Interaction training epoch: 152, train loss: 0.04638, val loss: 0.04708\n",
      "Interaction training epoch: 153, train loss: 0.04830, val loss: 0.05070\n",
      "Interaction training epoch: 154, train loss: 0.04798, val loss: 0.04952\n",
      "Interaction training epoch: 155, train loss: 0.04659, val loss: 0.04786\n",
      "Interaction training epoch: 156, train loss: 0.04767, val loss: 0.04876\n",
      "Interaction training epoch: 157, train loss: 0.04853, val loss: 0.04917\n",
      "Interaction training epoch: 158, train loss: 0.04677, val loss: 0.04803\n",
      "Interaction training epoch: 159, train loss: 0.04901, val loss: 0.04931\n",
      "Interaction training epoch: 160, train loss: 0.04754, val loss: 0.04893\n",
      "Interaction training epoch: 161, train loss: 0.04874, val loss: 0.04950\n",
      "Interaction training epoch: 162, train loss: 0.04595, val loss: 0.04721\n",
      "Interaction training epoch: 163, train loss: 0.04732, val loss: 0.04956\n",
      "Interaction training epoch: 164, train loss: 0.04567, val loss: 0.04749\n",
      "Interaction training epoch: 165, train loss: 0.05013, val loss: 0.05115\n",
      "Interaction training epoch: 166, train loss: 0.04663, val loss: 0.04790\n",
      "Interaction training epoch: 167, train loss: 0.04602, val loss: 0.04784\n",
      "Interaction training epoch: 168, train loss: 0.04789, val loss: 0.04800\n",
      "Interaction training epoch: 169, train loss: 0.04480, val loss: 0.04615\n",
      "Interaction training epoch: 170, train loss: 0.04657, val loss: 0.04712\n",
      "Interaction training epoch: 171, train loss: 0.04515, val loss: 0.04688\n",
      "Interaction training epoch: 172, train loss: 0.04745, val loss: 0.04904\n",
      "Interaction training epoch: 173, train loss: 0.04801, val loss: 0.05050\n",
      "Interaction training epoch: 174, train loss: 0.05297, val loss: 0.05389\n",
      "Interaction training epoch: 175, train loss: 0.04522, val loss: 0.04687\n",
      "Interaction training epoch: 176, train loss: 0.04654, val loss: 0.04709\n",
      "Interaction training epoch: 177, train loss: 0.04462, val loss: 0.04576\n",
      "Interaction training epoch: 178, train loss: 0.04715, val loss: 0.04880\n",
      "Interaction training epoch: 179, train loss: 0.04515, val loss: 0.04673\n",
      "Interaction training epoch: 180, train loss: 0.04652, val loss: 0.04824\n",
      "Interaction training epoch: 181, train loss: 0.04757, val loss: 0.04936\n",
      "Interaction training epoch: 182, train loss: 0.04771, val loss: 0.04909\n",
      "Interaction training epoch: 183, train loss: 0.04999, val loss: 0.05186\n",
      "Interaction training epoch: 184, train loss: 0.04691, val loss: 0.04877\n",
      "Interaction training epoch: 185, train loss: 0.04599, val loss: 0.04763\n",
      "Interaction training epoch: 186, train loss: 0.04839, val loss: 0.04969\n",
      "Interaction training epoch: 187, train loss: 0.04634, val loss: 0.04846\n",
      "Interaction training epoch: 188, train loss: 0.04686, val loss: 0.04828\n",
      "Interaction training epoch: 189, train loss: 0.04527, val loss: 0.04603\n",
      "Interaction training epoch: 190, train loss: 0.04811, val loss: 0.05046\n",
      "Interaction training epoch: 191, train loss: 0.04668, val loss: 0.04794\n",
      "Interaction training epoch: 192, train loss: 0.04580, val loss: 0.04673\n",
      "Interaction training epoch: 193, train loss: 0.04953, val loss: 0.05077\n",
      "Interaction training epoch: 194, train loss: 0.04637, val loss: 0.04745\n",
      "Interaction training epoch: 195, train loss: 0.05291, val loss: 0.05606\n",
      "Interaction training epoch: 196, train loss: 0.04552, val loss: 0.04706\n",
      "Interaction training epoch: 197, train loss: 0.04954, val loss: 0.05175\n",
      "Interaction training epoch: 198, train loss: 0.04504, val loss: 0.04659\n",
      "Interaction training epoch: 199, train loss: 0.04939, val loss: 0.05068\n",
      "Interaction training epoch: 200, train loss: 0.04824, val loss: 0.04969\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########7 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.04624, val loss: 0.04648\n",
      "Interaction tuning epoch: 2, train loss: 0.04654, val loss: 0.04675\n",
      "Interaction tuning epoch: 3, train loss: 0.04688, val loss: 0.04770\n",
      "Interaction tuning epoch: 4, train loss: 0.04703, val loss: 0.04714\n",
      "Interaction tuning epoch: 5, train loss: 0.04871, val loss: 0.04870\n",
      "Interaction tuning epoch: 6, train loss: 0.04715, val loss: 0.04685\n",
      "Interaction tuning epoch: 7, train loss: 0.04753, val loss: 0.04769\n",
      "Interaction tuning epoch: 8, train loss: 0.04804, val loss: 0.04839\n",
      "Interaction tuning epoch: 9, train loss: 0.04656, val loss: 0.04746\n",
      "Interaction tuning epoch: 10, train loss: 0.04708, val loss: 0.04792\n",
      "Interaction tuning epoch: 11, train loss: 0.04593, val loss: 0.04607\n",
      "Interaction tuning epoch: 12, train loss: 0.04889, val loss: 0.05019\n",
      "Interaction tuning epoch: 13, train loss: 0.04756, val loss: 0.04795\n",
      "Interaction tuning epoch: 14, train loss: 0.05056, val loss: 0.04891\n",
      "Interaction tuning epoch: 15, train loss: 0.05649, val loss: 0.05700\n",
      "Interaction tuning epoch: 16, train loss: 0.04770, val loss: 0.04777\n",
      "Interaction tuning epoch: 17, train loss: 0.04665, val loss: 0.04734\n",
      "Interaction tuning epoch: 18, train loss: 0.04709, val loss: 0.04763\n",
      "Interaction tuning epoch: 19, train loss: 0.04762, val loss: 0.04775\n",
      "Interaction tuning epoch: 20, train loss: 0.04647, val loss: 0.04691\n",
      "Interaction tuning epoch: 21, train loss: 0.04858, val loss: 0.04904\n",
      "Interaction tuning epoch: 22, train loss: 0.04763, val loss: 0.04743\n",
      "Interaction tuning epoch: 23, train loss: 0.04883, val loss: 0.04879\n",
      "Interaction tuning epoch: 24, train loss: 0.04608, val loss: 0.04728\n",
      "Interaction tuning epoch: 25, train loss: 0.04999, val loss: 0.04940\n",
      "Interaction tuning epoch: 26, train loss: 0.04916, val loss: 0.04857\n",
      "Interaction tuning epoch: 27, train loss: 0.04784, val loss: 0.04855\n",
      "Interaction tuning epoch: 28, train loss: 0.04702, val loss: 0.04758\n",
      "Interaction tuning epoch: 29, train loss: 0.04685, val loss: 0.04720\n",
      "Interaction tuning epoch: 30, train loss: 0.04688, val loss: 0.04800\n",
      "Interaction tuning epoch: 31, train loss: 0.04656, val loss: 0.04725\n",
      "Interaction tuning epoch: 32, train loss: 0.04698, val loss: 0.04718\n",
      "Interaction tuning epoch: 33, train loss: 0.04648, val loss: 0.04674\n",
      "Interaction tuning epoch: 34, train loss: 0.04622, val loss: 0.04627\n",
      "Interaction tuning epoch: 35, train loss: 0.04695, val loss: 0.04653\n",
      "Interaction tuning epoch: 36, train loss: 0.04638, val loss: 0.04708\n",
      "Interaction tuning epoch: 37, train loss: 0.04517, val loss: 0.04599\n",
      "Interaction tuning epoch: 38, train loss: 0.04594, val loss: 0.04626\n",
      "Interaction tuning epoch: 39, train loss: 0.04806, val loss: 0.04884\n",
      "Interaction tuning epoch: 40, train loss: 0.04762, val loss: 0.04781\n",
      "Interaction tuning epoch: 41, train loss: 0.04762, val loss: 0.04909\n",
      "Interaction tuning epoch: 42, train loss: 0.04746, val loss: 0.04805\n",
      "Interaction tuning epoch: 43, train loss: 0.04604, val loss: 0.04612\n",
      "Interaction tuning epoch: 44, train loss: 0.04636, val loss: 0.04673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 45, train loss: 0.04643, val loss: 0.04712\n",
      "Interaction tuning epoch: 46, train loss: 0.04656, val loss: 0.04678\n",
      "Interaction tuning epoch: 47, train loss: 0.04635, val loss: 0.04667\n",
      "Interaction tuning epoch: 48, train loss: 0.04816, val loss: 0.04882\n",
      "Interaction tuning epoch: 49, train loss: 0.04683, val loss: 0.04711\n",
      "Interaction tuning epoch: 50, train loss: 0.04845, val loss: 0.04905\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 37.16763472557068\n",
      "After the gam stage, training error is 0.04845 , validation error is 0.04905\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 1.232198\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.037189 validation MAE=0.046211,rank=7\n",
      "[SoftImpute] Iter 2: observed MAE=0.033661 validation MAE=0.045081,rank=7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 3: observed MAE=0.030862 validation MAE=0.044093,rank=7\n",
      "[SoftImpute] Iter 4: observed MAE=0.028581 validation MAE=0.043256,rank=7\n",
      "[SoftImpute] Iter 5: observed MAE=0.026673 validation MAE=0.042520,rank=7\n",
      "[SoftImpute] Iter 6: observed MAE=0.025039 validation MAE=0.041888,rank=7\n",
      "[SoftImpute] Iter 7: observed MAE=0.023625 validation MAE=0.041318,rank=7\n",
      "[SoftImpute] Iter 8: observed MAE=0.022397 validation MAE=0.040824,rank=7\n",
      "[SoftImpute] Iter 9: observed MAE=0.021328 validation MAE=0.040384,rank=7\n",
      "[SoftImpute] Iter 10: observed MAE=0.020387 validation MAE=0.040000,rank=7\n",
      "[SoftImpute] Iter 11: observed MAE=0.019551 validation MAE=0.039654,rank=7\n",
      "[SoftImpute] Iter 12: observed MAE=0.018800 validation MAE=0.039370,rank=7\n",
      "[SoftImpute] Iter 13: observed MAE=0.018123 validation MAE=0.039122,rank=7\n",
      "[SoftImpute] Iter 14: observed MAE=0.017510 validation MAE=0.038891,rank=7\n",
      "[SoftImpute] Iter 15: observed MAE=0.016956 validation MAE=0.038676,rank=7\n",
      "[SoftImpute] Iter 16: observed MAE=0.016452 validation MAE=0.038480,rank=7\n",
      "[SoftImpute] Iter 17: observed MAE=0.015991 validation MAE=0.038298,rank=7\n",
      "[SoftImpute] Iter 18: observed MAE=0.015565 validation MAE=0.038130,rank=7\n",
      "[SoftImpute] Iter 19: observed MAE=0.015171 validation MAE=0.037980,rank=7\n",
      "[SoftImpute] Iter 20: observed MAE=0.014805 validation MAE=0.037844,rank=7\n",
      "[SoftImpute] Iter 21: observed MAE=0.014464 validation MAE=0.037715,rank=7\n",
      "[SoftImpute] Iter 22: observed MAE=0.014147 validation MAE=0.037594,rank=7\n",
      "[SoftImpute] Iter 23: observed MAE=0.013851 validation MAE=0.037484,rank=7\n",
      "[SoftImpute] Iter 24: observed MAE=0.013573 validation MAE=0.037384,rank=7\n",
      "[SoftImpute] Iter 25: observed MAE=0.013313 validation MAE=0.037291,rank=7\n",
      "[SoftImpute] Iter 26: observed MAE=0.013068 validation MAE=0.037206,rank=7\n",
      "[SoftImpute] Iter 27: observed MAE=0.012836 validation MAE=0.037124,rank=7\n",
      "[SoftImpute] Iter 28: observed MAE=0.012616 validation MAE=0.037043,rank=7\n",
      "[SoftImpute] Iter 29: observed MAE=0.012407 validation MAE=0.036967,rank=7\n",
      "[SoftImpute] Iter 30: observed MAE=0.012208 validation MAE=0.036896,rank=7\n",
      "[SoftImpute] Iter 31: observed MAE=0.012019 validation MAE=0.036825,rank=7\n",
      "[SoftImpute] Iter 32: observed MAE=0.011839 validation MAE=0.036755,rank=7\n",
      "[SoftImpute] Iter 33: observed MAE=0.011669 validation MAE=0.036686,rank=7\n",
      "[SoftImpute] Iter 34: observed MAE=0.011506 validation MAE=0.036619,rank=7\n",
      "[SoftImpute] Iter 35: observed MAE=0.011352 validation MAE=0.036555,rank=7\n",
      "[SoftImpute] Iter 36: observed MAE=0.011203 validation MAE=0.036494,rank=7\n",
      "[SoftImpute] Iter 37: observed MAE=0.011062 validation MAE=0.036434,rank=7\n",
      "[SoftImpute] Iter 38: observed MAE=0.010926 validation MAE=0.036375,rank=7\n",
      "[SoftImpute] Iter 39: observed MAE=0.010795 validation MAE=0.036320,rank=7\n",
      "[SoftImpute] Iter 40: observed MAE=0.010669 validation MAE=0.036265,rank=7\n",
      "[SoftImpute] Iter 41: observed MAE=0.010549 validation MAE=0.036213,rank=7\n",
      "[SoftImpute] Iter 42: observed MAE=0.010432 validation MAE=0.036164,rank=7\n",
      "[SoftImpute] Iter 43: observed MAE=0.010320 validation MAE=0.036117,rank=7\n",
      "[SoftImpute] Iter 44: observed MAE=0.010212 validation MAE=0.036069,rank=7\n",
      "[SoftImpute] Iter 45: observed MAE=0.010108 validation MAE=0.036020,rank=7\n",
      "[SoftImpute] Iter 46: observed MAE=0.010007 validation MAE=0.035972,rank=7\n",
      "[SoftImpute] Iter 47: observed MAE=0.009910 validation MAE=0.035927,rank=7\n",
      "[SoftImpute] Iter 48: observed MAE=0.009816 validation MAE=0.035883,rank=7\n",
      "[SoftImpute] Iter 49: observed MAE=0.009725 validation MAE=0.035839,rank=7\n",
      "[SoftImpute] Iter 50: observed MAE=0.009637 validation MAE=0.035793,rank=7\n",
      "[SoftImpute] Iter 51: observed MAE=0.009552 validation MAE=0.035748,rank=7\n",
      "[SoftImpute] Iter 52: observed MAE=0.009471 validation MAE=0.035701,rank=7\n",
      "[SoftImpute] Iter 53: observed MAE=0.009391 validation MAE=0.035654,rank=7\n",
      "[SoftImpute] Iter 54: observed MAE=0.009314 validation MAE=0.035607,rank=7\n",
      "[SoftImpute] Iter 55: observed MAE=0.009238 validation MAE=0.035560,rank=7\n",
      "[SoftImpute] Iter 56: observed MAE=0.009165 validation MAE=0.035513,rank=7\n",
      "[SoftImpute] Iter 57: observed MAE=0.009093 validation MAE=0.035467,rank=7\n",
      "[SoftImpute] Iter 58: observed MAE=0.009024 validation MAE=0.035420,rank=7\n",
      "[SoftImpute] Iter 59: observed MAE=0.008956 validation MAE=0.035377,rank=7\n",
      "[SoftImpute] Iter 60: observed MAE=0.008890 validation MAE=0.035333,rank=7\n",
      "[SoftImpute] Iter 61: observed MAE=0.008826 validation MAE=0.035290,rank=7\n",
      "[SoftImpute] Iter 62: observed MAE=0.008764 validation MAE=0.035247,rank=7\n",
      "[SoftImpute] Iter 63: observed MAE=0.008703 validation MAE=0.035204,rank=7\n",
      "[SoftImpute] Iter 64: observed MAE=0.008643 validation MAE=0.035162,rank=7\n",
      "[SoftImpute] Iter 65: observed MAE=0.008585 validation MAE=0.035121,rank=7\n",
      "[SoftImpute] Iter 66: observed MAE=0.008529 validation MAE=0.035079,rank=7\n",
      "[SoftImpute] Iter 67: observed MAE=0.008474 validation MAE=0.035037,rank=7\n",
      "[SoftImpute] Iter 68: observed MAE=0.008420 validation MAE=0.034994,rank=7\n",
      "[SoftImpute] Iter 69: observed MAE=0.008367 validation MAE=0.034952,rank=7\n",
      "[SoftImpute] Iter 70: observed MAE=0.008315 validation MAE=0.034910,rank=7\n",
      "[SoftImpute] Iter 71: observed MAE=0.008265 validation MAE=0.034868,rank=7\n",
      "[SoftImpute] Iter 72: observed MAE=0.008215 validation MAE=0.034825,rank=7\n",
      "[SoftImpute] Iter 73: observed MAE=0.008167 validation MAE=0.034782,rank=7\n",
      "[SoftImpute] Iter 74: observed MAE=0.008120 validation MAE=0.034739,rank=7\n",
      "[SoftImpute] Iter 75: observed MAE=0.008074 validation MAE=0.034697,rank=7\n",
      "[SoftImpute] Iter 76: observed MAE=0.008029 validation MAE=0.034655,rank=7\n",
      "[SoftImpute] Iter 77: observed MAE=0.007986 validation MAE=0.034615,rank=7\n",
      "[SoftImpute] Iter 78: observed MAE=0.007943 validation MAE=0.034574,rank=7\n",
      "[SoftImpute] Iter 79: observed MAE=0.007901 validation MAE=0.034533,rank=7\n",
      "[SoftImpute] Iter 80: observed MAE=0.007860 validation MAE=0.034492,rank=7\n",
      "[SoftImpute] Iter 81: observed MAE=0.007820 validation MAE=0.034451,rank=7\n",
      "[SoftImpute] Iter 82: observed MAE=0.007781 validation MAE=0.034411,rank=7\n",
      "[SoftImpute] Iter 83: observed MAE=0.007742 validation MAE=0.034370,rank=7\n",
      "[SoftImpute] Iter 84: observed MAE=0.007705 validation MAE=0.034330,rank=7\n",
      "[SoftImpute] Iter 85: observed MAE=0.007668 validation MAE=0.034289,rank=7\n",
      "[SoftImpute] Iter 86: observed MAE=0.007632 validation MAE=0.034249,rank=7\n",
      "[SoftImpute] Iter 87: observed MAE=0.007596 validation MAE=0.034209,rank=7\n",
      "[SoftImpute] Iter 88: observed MAE=0.007562 validation MAE=0.034169,rank=7\n",
      "[SoftImpute] Iter 89: observed MAE=0.007528 validation MAE=0.034129,rank=7\n",
      "[SoftImpute] Iter 90: observed MAE=0.007495 validation MAE=0.034089,rank=7\n",
      "[SoftImpute] Iter 91: observed MAE=0.007462 validation MAE=0.034050,rank=7\n",
      "[SoftImpute] Iter 92: observed MAE=0.007430 validation MAE=0.034010,rank=7\n",
      "[SoftImpute] Iter 93: observed MAE=0.007399 validation MAE=0.033972,rank=7\n",
      "[SoftImpute] Iter 94: observed MAE=0.007368 validation MAE=0.033936,rank=7\n",
      "[SoftImpute] Iter 95: observed MAE=0.007338 validation MAE=0.033901,rank=7\n",
      "[SoftImpute] Iter 96: observed MAE=0.007308 validation MAE=0.033866,rank=7\n",
      "[SoftImpute] Iter 97: observed MAE=0.007279 validation MAE=0.033831,rank=7\n",
      "[SoftImpute] Iter 98: observed MAE=0.007250 validation MAE=0.033796,rank=7\n",
      "[SoftImpute] Iter 99: observed MAE=0.007221 validation MAE=0.033761,rank=7\n",
      "[SoftImpute] Iter 100: observed MAE=0.007193 validation MAE=0.033725,rank=7\n",
      "[SoftImpute] Iter 101: observed MAE=0.007166 validation MAE=0.033690,rank=7\n",
      "[SoftImpute] Iter 102: observed MAE=0.007139 validation MAE=0.033655,rank=7\n",
      "[SoftImpute] Iter 103: observed MAE=0.007112 validation MAE=0.033621,rank=7\n",
      "[SoftImpute] Iter 104: observed MAE=0.007086 validation MAE=0.033586,rank=7\n",
      "[SoftImpute] Iter 105: observed MAE=0.007061 validation MAE=0.033552,rank=7\n",
      "[SoftImpute] Iter 106: observed MAE=0.007036 validation MAE=0.033518,rank=7\n",
      "[SoftImpute] Iter 107: observed MAE=0.007011 validation MAE=0.033484,rank=7\n",
      "[SoftImpute] Iter 108: observed MAE=0.006987 validation MAE=0.033450,rank=7\n",
      "[SoftImpute] Iter 109: observed MAE=0.006963 validation MAE=0.033416,rank=7\n",
      "[SoftImpute] Iter 110: observed MAE=0.006939 validation MAE=0.033382,rank=7\n",
      "[SoftImpute] Iter 111: observed MAE=0.006916 validation MAE=0.033348,rank=7\n",
      "[SoftImpute] Iter 112: observed MAE=0.006893 validation MAE=0.033315,rank=7\n",
      "[SoftImpute] Iter 113: observed MAE=0.006871 validation MAE=0.033281,rank=7\n",
      "[SoftImpute] Iter 114: observed MAE=0.006849 validation MAE=0.033248,rank=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 115: observed MAE=0.006827 validation MAE=0.033215,rank=7\n",
      "[SoftImpute] Iter 116: observed MAE=0.006805 validation MAE=0.033182,rank=7\n",
      "[SoftImpute] Iter 117: observed MAE=0.006784 validation MAE=0.033149,rank=7\n",
      "[SoftImpute] Iter 118: observed MAE=0.006764 validation MAE=0.033117,rank=7\n",
      "[SoftImpute] Iter 119: observed MAE=0.006743 validation MAE=0.033085,rank=7\n",
      "[SoftImpute] Iter 120: observed MAE=0.006723 validation MAE=0.033054,rank=7\n",
      "[SoftImpute] Iter 121: observed MAE=0.006703 validation MAE=0.033023,rank=7\n",
      "[SoftImpute] Iter 122: observed MAE=0.006683 validation MAE=0.032991,rank=7\n",
      "[SoftImpute] Iter 123: observed MAE=0.006664 validation MAE=0.032960,rank=7\n",
      "[SoftImpute] Iter 124: observed MAE=0.006645 validation MAE=0.032928,rank=7\n",
      "[SoftImpute] Iter 125: observed MAE=0.006626 validation MAE=0.032896,rank=7\n",
      "[SoftImpute] Iter 126: observed MAE=0.006608 validation MAE=0.032865,rank=7\n",
      "[SoftImpute] Iter 127: observed MAE=0.006590 validation MAE=0.032834,rank=7\n",
      "[SoftImpute] Iter 128: observed MAE=0.006572 validation MAE=0.032803,rank=7\n",
      "[SoftImpute] Iter 129: observed MAE=0.006554 validation MAE=0.032772,rank=7\n",
      "[SoftImpute] Iter 130: observed MAE=0.006536 validation MAE=0.032741,rank=7\n",
      "[SoftImpute] Iter 131: observed MAE=0.006519 validation MAE=0.032710,rank=7\n",
      "[SoftImpute] Iter 132: observed MAE=0.006502 validation MAE=0.032679,rank=7\n",
      "[SoftImpute] Iter 133: observed MAE=0.006485 validation MAE=0.032648,rank=7\n",
      "[SoftImpute] Iter 134: observed MAE=0.006469 validation MAE=0.032617,rank=7\n",
      "[SoftImpute] Iter 135: observed MAE=0.006452 validation MAE=0.032586,rank=7\n",
      "[SoftImpute] Iter 136: observed MAE=0.006436 validation MAE=0.032556,rank=7\n",
      "[SoftImpute] Iter 137: observed MAE=0.006420 validation MAE=0.032525,rank=7\n",
      "[SoftImpute] Iter 138: observed MAE=0.006405 validation MAE=0.032494,rank=7\n",
      "[SoftImpute] Iter 139: observed MAE=0.006389 validation MAE=0.032464,rank=7\n",
      "[SoftImpute] Iter 140: observed MAE=0.006374 validation MAE=0.032433,rank=7\n",
      "[SoftImpute] Iter 141: observed MAE=0.006358 validation MAE=0.032403,rank=7\n",
      "[SoftImpute] Iter 142: observed MAE=0.006343 validation MAE=0.032373,rank=7\n",
      "[SoftImpute] Iter 143: observed MAE=0.006329 validation MAE=0.032343,rank=7\n",
      "[SoftImpute] Iter 144: observed MAE=0.006314 validation MAE=0.032313,rank=7\n",
      "[SoftImpute] Iter 145: observed MAE=0.006299 validation MAE=0.032283,rank=7\n",
      "[SoftImpute] Iter 146: observed MAE=0.006285 validation MAE=0.032253,rank=7\n",
      "[SoftImpute] Iter 147: observed MAE=0.006271 validation MAE=0.032222,rank=7\n",
      "[SoftImpute] Iter 148: observed MAE=0.006257 validation MAE=0.032192,rank=7\n",
      "[SoftImpute] Iter 149: observed MAE=0.006243 validation MAE=0.032162,rank=7\n",
      "[SoftImpute] Iter 150: observed MAE=0.006230 validation MAE=0.032132,rank=7\n",
      "[SoftImpute] Iter 151: observed MAE=0.006216 validation MAE=0.032101,rank=7\n",
      "[SoftImpute] Iter 152: observed MAE=0.006203 validation MAE=0.032071,rank=7\n",
      "[SoftImpute] Iter 153: observed MAE=0.006190 validation MAE=0.032041,rank=7\n",
      "[SoftImpute] Iter 154: observed MAE=0.006177 validation MAE=0.032011,rank=7\n",
      "[SoftImpute] Iter 155: observed MAE=0.006164 validation MAE=0.031982,rank=7\n",
      "[SoftImpute] Iter 156: observed MAE=0.006151 validation MAE=0.031952,rank=7\n",
      "[SoftImpute] Iter 157: observed MAE=0.006139 validation MAE=0.031922,rank=7\n",
      "[SoftImpute] Iter 158: observed MAE=0.006126 validation MAE=0.031893,rank=7\n",
      "[SoftImpute] Iter 159: observed MAE=0.006114 validation MAE=0.031864,rank=7\n",
      "[SoftImpute] Iter 160: observed MAE=0.006102 validation MAE=0.031835,rank=7\n",
      "[SoftImpute] Iter 161: observed MAE=0.006090 validation MAE=0.031805,rank=7\n",
      "[SoftImpute] Iter 162: observed MAE=0.006078 validation MAE=0.031776,rank=7\n",
      "[SoftImpute] Iter 163: observed MAE=0.006066 validation MAE=0.031747,rank=7\n",
      "[SoftImpute] Iter 164: observed MAE=0.006054 validation MAE=0.031718,rank=7\n",
      "[SoftImpute] Iter 165: observed MAE=0.006043 validation MAE=0.031689,rank=7\n",
      "[SoftImpute] Iter 166: observed MAE=0.006031 validation MAE=0.031660,rank=7\n",
      "[SoftImpute] Iter 167: observed MAE=0.006020 validation MAE=0.031631,rank=7\n",
      "[SoftImpute] Iter 168: observed MAE=0.006009 validation MAE=0.031602,rank=7\n",
      "[SoftImpute] Iter 169: observed MAE=0.005998 validation MAE=0.031573,rank=7\n",
      "[SoftImpute] Iter 170: observed MAE=0.005986 validation MAE=0.031545,rank=7\n",
      "[SoftImpute] Iter 171: observed MAE=0.005975 validation MAE=0.031516,rank=7\n",
      "[SoftImpute] Iter 172: observed MAE=0.005965 validation MAE=0.031487,rank=7\n",
      "[SoftImpute] Iter 173: observed MAE=0.005954 validation MAE=0.031459,rank=7\n",
      "[SoftImpute] Iter 174: observed MAE=0.005943 validation MAE=0.031430,rank=7\n",
      "[SoftImpute] Iter 175: observed MAE=0.005933 validation MAE=0.031401,rank=7\n",
      "[SoftImpute] Iter 176: observed MAE=0.005923 validation MAE=0.031372,rank=7\n",
      "[SoftImpute] Iter 177: observed MAE=0.005912 validation MAE=0.031344,rank=7\n",
      "[SoftImpute] Iter 178: observed MAE=0.005902 validation MAE=0.031315,rank=7\n",
      "[SoftImpute] Iter 179: observed MAE=0.005892 validation MAE=0.031287,rank=7\n",
      "[SoftImpute] Iter 180: observed MAE=0.005882 validation MAE=0.031259,rank=7\n",
      "[SoftImpute] Iter 181: observed MAE=0.005872 validation MAE=0.031231,rank=7\n",
      "[SoftImpute] Iter 182: observed MAE=0.005863 validation MAE=0.031203,rank=7\n",
      "[SoftImpute] Iter 183: observed MAE=0.005853 validation MAE=0.031175,rank=7\n",
      "[SoftImpute] Iter 184: observed MAE=0.005844 validation MAE=0.031148,rank=7\n",
      "[SoftImpute] Iter 185: observed MAE=0.005834 validation MAE=0.031120,rank=7\n",
      "[SoftImpute] Iter 186: observed MAE=0.005825 validation MAE=0.031092,rank=7\n",
      "[SoftImpute] Iter 187: observed MAE=0.005815 validation MAE=0.031065,rank=7\n",
      "[SoftImpute] Iter 188: observed MAE=0.005806 validation MAE=0.031037,rank=7\n",
      "[SoftImpute] Iter 189: observed MAE=0.005797 validation MAE=0.031010,rank=7\n",
      "[SoftImpute] Iter 190: observed MAE=0.005788 validation MAE=0.030982,rank=7\n",
      "[SoftImpute] Iter 191: observed MAE=0.005779 validation MAE=0.030954,rank=7\n",
      "[SoftImpute] Iter 192: observed MAE=0.005770 validation MAE=0.030927,rank=7\n",
      "[SoftImpute] Iter 193: observed MAE=0.005762 validation MAE=0.030899,rank=7\n",
      "[SoftImpute] Iter 194: observed MAE=0.005753 validation MAE=0.030871,rank=7\n",
      "[SoftImpute] Iter 195: observed MAE=0.005744 validation MAE=0.030844,rank=7\n",
      "[SoftImpute] Iter 196: observed MAE=0.005736 validation MAE=0.030816,rank=7\n",
      "[SoftImpute] Iter 197: observed MAE=0.005727 validation MAE=0.030789,rank=7\n",
      "[SoftImpute] Iter 198: observed MAE=0.005719 validation MAE=0.030762,rank=7\n",
      "[SoftImpute] Iter 199: observed MAE=0.005710 validation MAE=0.030734,rank=7\n",
      "[SoftImpute] Iter 200: observed MAE=0.005702 validation MAE=0.030707,rank=7\n",
      "[SoftImpute] Iter 201: observed MAE=0.005694 validation MAE=0.030680,rank=7\n",
      "[SoftImpute] Iter 202: observed MAE=0.005685 validation MAE=0.030654,rank=7\n",
      "[SoftImpute] Iter 203: observed MAE=0.005677 validation MAE=0.030627,rank=7\n",
      "[SoftImpute] Iter 204: observed MAE=0.005669 validation MAE=0.030601,rank=7\n",
      "[SoftImpute] Iter 205: observed MAE=0.005661 validation MAE=0.030574,rank=7\n",
      "[SoftImpute] Iter 206: observed MAE=0.005653 validation MAE=0.030548,rank=7\n",
      "[SoftImpute] Iter 207: observed MAE=0.005645 validation MAE=0.030521,rank=7\n",
      "[SoftImpute] Iter 208: observed MAE=0.005637 validation MAE=0.030494,rank=7\n",
      "[SoftImpute] Iter 209: observed MAE=0.005630 validation MAE=0.030469,rank=7\n",
      "[SoftImpute] Iter 210: observed MAE=0.005622 validation MAE=0.030443,rank=7\n",
      "[SoftImpute] Iter 211: observed MAE=0.005614 validation MAE=0.030418,rank=7\n",
      "[SoftImpute] Iter 212: observed MAE=0.005607 validation MAE=0.030393,rank=7\n",
      "[SoftImpute] Iter 213: observed MAE=0.005599 validation MAE=0.030367,rank=7\n",
      "[SoftImpute] Iter 214: observed MAE=0.005592 validation MAE=0.030342,rank=7\n",
      "[SoftImpute] Iter 215: observed MAE=0.005584 validation MAE=0.030317,rank=7\n",
      "[SoftImpute] Iter 216: observed MAE=0.005577 validation MAE=0.030292,rank=7\n",
      "[SoftImpute] Iter 217: observed MAE=0.005570 validation MAE=0.030267,rank=7\n",
      "[SoftImpute] Iter 218: observed MAE=0.005563 validation MAE=0.030241,rank=7\n",
      "[SoftImpute] Iter 219: observed MAE=0.005555 validation MAE=0.030216,rank=7\n",
      "[SoftImpute] Iter 220: observed MAE=0.005548 validation MAE=0.030191,rank=7\n",
      "[SoftImpute] Iter 221: observed MAE=0.005541 validation MAE=0.030166,rank=7\n",
      "[SoftImpute] Iter 222: observed MAE=0.005534 validation MAE=0.030140,rank=7\n",
      "[SoftImpute] Iter 223: observed MAE=0.005527 validation MAE=0.030115,rank=7\n",
      "[SoftImpute] Iter 224: observed MAE=0.005520 validation MAE=0.030090,rank=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 225: observed MAE=0.005513 validation MAE=0.030065,rank=7\n",
      "[SoftImpute] Iter 226: observed MAE=0.005506 validation MAE=0.030039,rank=7\n",
      "[SoftImpute] Iter 227: observed MAE=0.005500 validation MAE=0.030014,rank=7\n",
      "[SoftImpute] Iter 228: observed MAE=0.005493 validation MAE=0.029989,rank=7\n",
      "[SoftImpute] Iter 229: observed MAE=0.005486 validation MAE=0.029964,rank=7\n",
      "[SoftImpute] Iter 230: observed MAE=0.005479 validation MAE=0.029939,rank=7\n",
      "[SoftImpute] Iter 231: observed MAE=0.005473 validation MAE=0.029914,rank=7\n",
      "[SoftImpute] Iter 232: observed MAE=0.005466 validation MAE=0.029889,rank=7\n",
      "[SoftImpute] Iter 233: observed MAE=0.005460 validation MAE=0.029864,rank=7\n",
      "[SoftImpute] Iter 234: observed MAE=0.005453 validation MAE=0.029839,rank=7\n",
      "[SoftImpute] Iter 235: observed MAE=0.005447 validation MAE=0.029815,rank=7\n",
      "[SoftImpute] Iter 236: observed MAE=0.005440 validation MAE=0.029790,rank=7\n",
      "[SoftImpute] Iter 237: observed MAE=0.005434 validation MAE=0.029766,rank=7\n",
      "[SoftImpute] Iter 238: observed MAE=0.005427 validation MAE=0.029741,rank=7\n",
      "[SoftImpute] Iter 239: observed MAE=0.005421 validation MAE=0.029717,rank=7\n",
      "[SoftImpute] Iter 240: observed MAE=0.005415 validation MAE=0.029692,rank=7\n",
      "[SoftImpute] Iter 241: observed MAE=0.005409 validation MAE=0.029667,rank=7\n",
      "[SoftImpute] Iter 242: observed MAE=0.005402 validation MAE=0.029643,rank=7\n",
      "[SoftImpute] Iter 243: observed MAE=0.005396 validation MAE=0.029619,rank=7\n",
      "[SoftImpute] Iter 244: observed MAE=0.005390 validation MAE=0.029594,rank=7\n",
      "[SoftImpute] Iter 245: observed MAE=0.005384 validation MAE=0.029570,rank=7\n",
      "[SoftImpute] Iter 246: observed MAE=0.005378 validation MAE=0.029546,rank=7\n",
      "[SoftImpute] Iter 247: observed MAE=0.005372 validation MAE=0.029521,rank=7\n",
      "[SoftImpute] Iter 248: observed MAE=0.005366 validation MAE=0.029497,rank=7\n",
      "[SoftImpute] Iter 249: observed MAE=0.005360 validation MAE=0.029473,rank=7\n",
      "[SoftImpute] Iter 250: observed MAE=0.005354 validation MAE=0.029449,rank=7\n",
      "[SoftImpute] Iter 251: observed MAE=0.005348 validation MAE=0.029425,rank=7\n",
      "[SoftImpute] Iter 252: observed MAE=0.005342 validation MAE=0.029401,rank=7\n",
      "[SoftImpute] Iter 253: observed MAE=0.005336 validation MAE=0.029376,rank=7\n",
      "[SoftImpute] Iter 254: observed MAE=0.005330 validation MAE=0.029352,rank=7\n",
      "[SoftImpute] Iter 255: observed MAE=0.005325 validation MAE=0.029328,rank=7\n",
      "[SoftImpute] Iter 256: observed MAE=0.005319 validation MAE=0.029305,rank=7\n",
      "[SoftImpute] Iter 257: observed MAE=0.005313 validation MAE=0.029281,rank=7\n",
      "[SoftImpute] Iter 258: observed MAE=0.005308 validation MAE=0.029257,rank=7\n",
      "[SoftImpute] Iter 259: observed MAE=0.005302 validation MAE=0.029233,rank=7\n",
      "[SoftImpute] Iter 260: observed MAE=0.005297 validation MAE=0.029210,rank=7\n",
      "[SoftImpute] Iter 261: observed MAE=0.005291 validation MAE=0.029186,rank=7\n",
      "[SoftImpute] Iter 262: observed MAE=0.005286 validation MAE=0.029163,rank=7\n",
      "[SoftImpute] Iter 263: observed MAE=0.005280 validation MAE=0.029139,rank=7\n",
      "[SoftImpute] Iter 264: observed MAE=0.005275 validation MAE=0.029115,rank=7\n",
      "[SoftImpute] Iter 265: observed MAE=0.005269 validation MAE=0.029092,rank=7\n",
      "[SoftImpute] Iter 266: observed MAE=0.005264 validation MAE=0.029068,rank=7\n",
      "[SoftImpute] Iter 267: observed MAE=0.005259 validation MAE=0.029044,rank=7\n",
      "[SoftImpute] Iter 268: observed MAE=0.005253 validation MAE=0.029021,rank=7\n",
      "[SoftImpute] Iter 269: observed MAE=0.005248 validation MAE=0.028997,rank=7\n",
      "[SoftImpute] Iter 270: observed MAE=0.005243 validation MAE=0.028974,rank=7\n",
      "[SoftImpute] Iter 271: observed MAE=0.005237 validation MAE=0.028950,rank=7\n",
      "[SoftImpute] Iter 272: observed MAE=0.005232 validation MAE=0.028927,rank=7\n",
      "[SoftImpute] Iter 273: observed MAE=0.005227 validation MAE=0.028904,rank=7\n",
      "[SoftImpute] Iter 274: observed MAE=0.005222 validation MAE=0.028881,rank=7\n",
      "[SoftImpute] Iter 275: observed MAE=0.005217 validation MAE=0.028858,rank=7\n",
      "[SoftImpute] Iter 276: observed MAE=0.005212 validation MAE=0.028836,rank=7\n",
      "[SoftImpute] Iter 277: observed MAE=0.005207 validation MAE=0.028814,rank=7\n",
      "[SoftImpute] Iter 278: observed MAE=0.005202 validation MAE=0.028791,rank=7\n",
      "[SoftImpute] Iter 279: observed MAE=0.005197 validation MAE=0.028769,rank=7\n",
      "[SoftImpute] Iter 280: observed MAE=0.005192 validation MAE=0.028747,rank=7\n",
      "[SoftImpute] Iter 281: observed MAE=0.005187 validation MAE=0.028725,rank=7\n",
      "[SoftImpute] Iter 282: observed MAE=0.005182 validation MAE=0.028703,rank=7\n",
      "[SoftImpute] Iter 283: observed MAE=0.005177 validation MAE=0.028681,rank=7\n",
      "[SoftImpute] Iter 284: observed MAE=0.005173 validation MAE=0.028659,rank=7\n",
      "[SoftImpute] Iter 285: observed MAE=0.005168 validation MAE=0.028637,rank=7\n",
      "[SoftImpute] Iter 286: observed MAE=0.005163 validation MAE=0.028615,rank=7\n",
      "[SoftImpute] Iter 287: observed MAE=0.005158 validation MAE=0.028593,rank=7\n",
      "[SoftImpute] Iter 288: observed MAE=0.005153 validation MAE=0.028572,rank=7\n",
      "[SoftImpute] Iter 289: observed MAE=0.005149 validation MAE=0.028550,rank=7\n",
      "[SoftImpute] Iter 290: observed MAE=0.005144 validation MAE=0.028528,rank=7\n",
      "[SoftImpute] Iter 291: observed MAE=0.005139 validation MAE=0.028506,rank=7\n",
      "[SoftImpute] Iter 292: observed MAE=0.005135 validation MAE=0.028485,rank=7\n",
      "[SoftImpute] Iter 293: observed MAE=0.005130 validation MAE=0.028463,rank=7\n",
      "[SoftImpute] Iter 294: observed MAE=0.005126 validation MAE=0.028441,rank=7\n",
      "[SoftImpute] Iter 295: observed MAE=0.005121 validation MAE=0.028420,rank=7\n",
      "[SoftImpute] Iter 296: observed MAE=0.005116 validation MAE=0.028398,rank=7\n",
      "[SoftImpute] Iter 297: observed MAE=0.005112 validation MAE=0.028377,rank=7\n",
      "[SoftImpute] Iter 298: observed MAE=0.005107 validation MAE=0.028355,rank=7\n",
      "[SoftImpute] Iter 299: observed MAE=0.005103 validation MAE=0.028334,rank=7\n",
      "[SoftImpute] Iter 300: observed MAE=0.005099 validation MAE=0.028312,rank=7\n",
      "[SoftImpute] Iter 301: observed MAE=0.005094 validation MAE=0.028291,rank=7\n",
      "[SoftImpute] Iter 302: observed MAE=0.005090 validation MAE=0.028269,rank=7\n",
      "[SoftImpute] Iter 303: observed MAE=0.005085 validation MAE=0.028248,rank=7\n",
      "[SoftImpute] Iter 304: observed MAE=0.005081 validation MAE=0.028226,rank=7\n",
      "[SoftImpute] Iter 305: observed MAE=0.005077 validation MAE=0.028205,rank=7\n",
      "[SoftImpute] Iter 306: observed MAE=0.005073 validation MAE=0.028183,rank=7\n",
      "[SoftImpute] Iter 307: observed MAE=0.005068 validation MAE=0.028162,rank=7\n",
      "[SoftImpute] Iter 308: observed MAE=0.005064 validation MAE=0.028140,rank=7\n",
      "[SoftImpute] Iter 309: observed MAE=0.005060 validation MAE=0.028119,rank=7\n",
      "[SoftImpute] Iter 310: observed MAE=0.005056 validation MAE=0.028097,rank=7\n",
      "[SoftImpute] Iter 311: observed MAE=0.005052 validation MAE=0.028075,rank=7\n",
      "[SoftImpute] Iter 312: observed MAE=0.005047 validation MAE=0.028054,rank=7\n",
      "[SoftImpute] Iter 313: observed MAE=0.005043 validation MAE=0.028033,rank=7\n",
      "[SoftImpute] Iter 314: observed MAE=0.005039 validation MAE=0.028011,rank=7\n",
      "[SoftImpute] Iter 315: observed MAE=0.005035 validation MAE=0.027990,rank=7\n",
      "[SoftImpute] Iter 316: observed MAE=0.005031 validation MAE=0.027969,rank=7\n",
      "[SoftImpute] Iter 317: observed MAE=0.005027 validation MAE=0.027948,rank=7\n",
      "[SoftImpute] Iter 318: observed MAE=0.005023 validation MAE=0.027927,rank=7\n",
      "[SoftImpute] Iter 319: observed MAE=0.005019 validation MAE=0.027906,rank=7\n",
      "[SoftImpute] Iter 320: observed MAE=0.005015 validation MAE=0.027886,rank=7\n",
      "[SoftImpute] Iter 321: observed MAE=0.005011 validation MAE=0.027866,rank=7\n",
      "[SoftImpute] Iter 322: observed MAE=0.005007 validation MAE=0.027846,rank=7\n",
      "[SoftImpute] Iter 323: observed MAE=0.005003 validation MAE=0.027826,rank=7\n",
      "[SoftImpute] Iter 324: observed MAE=0.004999 validation MAE=0.027806,rank=7\n",
      "[SoftImpute] Iter 325: observed MAE=0.004995 validation MAE=0.027786,rank=7\n",
      "[SoftImpute] Iter 326: observed MAE=0.004992 validation MAE=0.027766,rank=7\n",
      "[SoftImpute] Iter 327: observed MAE=0.004988 validation MAE=0.027746,rank=7\n",
      "[SoftImpute] Iter 328: observed MAE=0.004984 validation MAE=0.027726,rank=7\n",
      "[SoftImpute] Iter 329: observed MAE=0.004980 validation MAE=0.027706,rank=7\n",
      "[SoftImpute] Iter 330: observed MAE=0.004976 validation MAE=0.027686,rank=7\n",
      "[SoftImpute] Iter 331: observed MAE=0.004972 validation MAE=0.027666,rank=7\n",
      "[SoftImpute] Iter 332: observed MAE=0.004969 validation MAE=0.027647,rank=7\n",
      "[SoftImpute] Iter 333: observed MAE=0.004965 validation MAE=0.027627,rank=7\n",
      "[SoftImpute] Iter 334: observed MAE=0.004961 validation MAE=0.027608,rank=7\n",
      "[SoftImpute] Iter 335: observed MAE=0.004957 validation MAE=0.027589,rank=7\n",
      "[SoftImpute] Iter 336: observed MAE=0.004954 validation MAE=0.027570,rank=7\n",
      "[SoftImpute] Iter 337: observed MAE=0.004950 validation MAE=0.027550,rank=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 338: observed MAE=0.004946 validation MAE=0.027531,rank=7\n",
      "[SoftImpute] Iter 339: observed MAE=0.004943 validation MAE=0.027513,rank=7\n",
      "[SoftImpute] Iter 340: observed MAE=0.004939 validation MAE=0.027494,rank=7\n",
      "[SoftImpute] Iter 341: observed MAE=0.004935 validation MAE=0.027475,rank=7\n",
      "[SoftImpute] Iter 342: observed MAE=0.004932 validation MAE=0.027457,rank=7\n",
      "[SoftImpute] Iter 343: observed MAE=0.004928 validation MAE=0.027438,rank=7\n",
      "[SoftImpute] Iter 344: observed MAE=0.004925 validation MAE=0.027419,rank=7\n",
      "[SoftImpute] Iter 345: observed MAE=0.004921 validation MAE=0.027401,rank=7\n",
      "[SoftImpute] Iter 346: observed MAE=0.004917 validation MAE=0.027382,rank=7\n",
      "[SoftImpute] Iter 347: observed MAE=0.004914 validation MAE=0.027363,rank=7\n",
      "[SoftImpute] Iter 348: observed MAE=0.004910 validation MAE=0.027345,rank=7\n",
      "[SoftImpute] Iter 349: observed MAE=0.004907 validation MAE=0.027326,rank=7\n",
      "[SoftImpute] Iter 350: observed MAE=0.004903 validation MAE=0.027308,rank=7\n",
      "[SoftImpute] Iter 351: observed MAE=0.004900 validation MAE=0.027290,rank=7\n",
      "[SoftImpute] Iter 352: observed MAE=0.004897 validation MAE=0.027272,rank=7\n",
      "[SoftImpute] Iter 353: observed MAE=0.004893 validation MAE=0.027254,rank=7\n",
      "[SoftImpute] Iter 354: observed MAE=0.004890 validation MAE=0.027236,rank=7\n",
      "[SoftImpute] Iter 355: observed MAE=0.004886 validation MAE=0.027218,rank=7\n",
      "[SoftImpute] Iter 356: observed MAE=0.004883 validation MAE=0.027200,rank=7\n",
      "[SoftImpute] Iter 357: observed MAE=0.004880 validation MAE=0.027182,rank=7\n",
      "[SoftImpute] Iter 358: observed MAE=0.004876 validation MAE=0.027164,rank=7\n",
      "[SoftImpute] Iter 359: observed MAE=0.004873 validation MAE=0.027147,rank=7\n",
      "[SoftImpute] Iter 360: observed MAE=0.004870 validation MAE=0.027130,rank=7\n",
      "[SoftImpute] Iter 361: observed MAE=0.004866 validation MAE=0.027113,rank=7\n",
      "[SoftImpute] Iter 362: observed MAE=0.004863 validation MAE=0.027097,rank=7\n",
      "[SoftImpute] Iter 363: observed MAE=0.004860 validation MAE=0.027080,rank=7\n",
      "[SoftImpute] Iter 364: observed MAE=0.004857 validation MAE=0.027064,rank=7\n",
      "[SoftImpute] Iter 365: observed MAE=0.004853 validation MAE=0.027047,rank=7\n",
      "[SoftImpute] Iter 366: observed MAE=0.004850 validation MAE=0.027031,rank=7\n",
      "[SoftImpute] Iter 367: observed MAE=0.004847 validation MAE=0.027015,rank=7\n",
      "[SoftImpute] Iter 368: observed MAE=0.004844 validation MAE=0.026999,rank=7\n",
      "[SoftImpute] Iter 369: observed MAE=0.004841 validation MAE=0.026983,rank=7\n",
      "[SoftImpute] Iter 370: observed MAE=0.004838 validation MAE=0.026967,rank=7\n",
      "[SoftImpute] Iter 371: observed MAE=0.004834 validation MAE=0.026951,rank=7\n",
      "[SoftImpute] Iter 372: observed MAE=0.004831 validation MAE=0.026935,rank=7\n",
      "[SoftImpute] Iter 373: observed MAE=0.004828 validation MAE=0.026919,rank=7\n",
      "[SoftImpute] Iter 374: observed MAE=0.004825 validation MAE=0.026903,rank=7\n",
      "[SoftImpute] Iter 375: observed MAE=0.004822 validation MAE=0.026887,rank=7\n",
      "[SoftImpute] Iter 376: observed MAE=0.004819 validation MAE=0.026871,rank=7\n",
      "[SoftImpute] Iter 377: observed MAE=0.004816 validation MAE=0.026856,rank=7\n",
      "[SoftImpute] Iter 378: observed MAE=0.004813 validation MAE=0.026841,rank=7\n",
      "[SoftImpute] Iter 379: observed MAE=0.004810 validation MAE=0.026826,rank=7\n",
      "[SoftImpute] Iter 380: observed MAE=0.004807 validation MAE=0.026811,rank=7\n",
      "[SoftImpute] Iter 381: observed MAE=0.004804 validation MAE=0.026796,rank=7\n",
      "[SoftImpute] Iter 382: observed MAE=0.004801 validation MAE=0.026781,rank=7\n",
      "[SoftImpute] Iter 383: observed MAE=0.004798 validation MAE=0.026766,rank=7\n",
      "[SoftImpute] Iter 384: observed MAE=0.004795 validation MAE=0.026752,rank=7\n",
      "[SoftImpute] Iter 385: observed MAE=0.004792 validation MAE=0.026738,rank=7\n",
      "[SoftImpute] Iter 386: observed MAE=0.004789 validation MAE=0.026723,rank=7\n",
      "[SoftImpute] Iter 387: observed MAE=0.004786 validation MAE=0.026709,rank=7\n",
      "[SoftImpute] Iter 388: observed MAE=0.004783 validation MAE=0.026695,rank=7\n",
      "[SoftImpute] Iter 389: observed MAE=0.004780 validation MAE=0.026681,rank=7\n",
      "[SoftImpute] Iter 390: observed MAE=0.004778 validation MAE=0.026667,rank=7\n",
      "[SoftImpute] Iter 391: observed MAE=0.004775 validation MAE=0.026653,rank=7\n",
      "[SoftImpute] Iter 392: observed MAE=0.004772 validation MAE=0.026638,rank=7\n",
      "[SoftImpute] Iter 393: observed MAE=0.004769 validation MAE=0.026625,rank=7\n",
      "[SoftImpute] Iter 394: observed MAE=0.004766 validation MAE=0.026611,rank=7\n",
      "[SoftImpute] Iter 395: observed MAE=0.004763 validation MAE=0.026597,rank=7\n",
      "[SoftImpute] Iter 396: observed MAE=0.004760 validation MAE=0.026584,rank=7\n",
      "[SoftImpute] Iter 397: observed MAE=0.004758 validation MAE=0.026570,rank=7\n",
      "[SoftImpute] Iter 398: observed MAE=0.004755 validation MAE=0.026557,rank=7\n",
      "[SoftImpute] Iter 399: observed MAE=0.004752 validation MAE=0.026544,rank=7\n",
      "[SoftImpute] Iter 400: observed MAE=0.004749 validation MAE=0.026530,rank=7\n",
      "[SoftImpute] Iter 401: observed MAE=0.004746 validation MAE=0.026517,rank=7\n",
      "[SoftImpute] Iter 402: observed MAE=0.004744 validation MAE=0.026505,rank=7\n",
      "[SoftImpute] Iter 403: observed MAE=0.004741 validation MAE=0.026492,rank=7\n",
      "[SoftImpute] Iter 404: observed MAE=0.004738 validation MAE=0.026479,rank=7\n",
      "[SoftImpute] Iter 405: observed MAE=0.004735 validation MAE=0.026467,rank=7\n",
      "[SoftImpute] Iter 406: observed MAE=0.004733 validation MAE=0.026454,rank=7\n",
      "[SoftImpute] Iter 407: observed MAE=0.004730 validation MAE=0.026442,rank=7\n",
      "[SoftImpute] Iter 408: observed MAE=0.004727 validation MAE=0.026430,rank=7\n",
      "[SoftImpute] Iter 409: observed MAE=0.004725 validation MAE=0.026418,rank=7\n",
      "[SoftImpute] Iter 410: observed MAE=0.004722 validation MAE=0.026406,rank=7\n",
      "[SoftImpute] Iter 411: observed MAE=0.004719 validation MAE=0.026393,rank=7\n",
      "[SoftImpute] Iter 412: observed MAE=0.004717 validation MAE=0.026381,rank=7\n",
      "[SoftImpute] Iter 413: observed MAE=0.004714 validation MAE=0.026369,rank=7\n",
      "[SoftImpute] Iter 414: observed MAE=0.004711 validation MAE=0.026357,rank=7\n",
      "[SoftImpute] Iter 415: observed MAE=0.004709 validation MAE=0.026345,rank=7\n",
      "[SoftImpute] Stopped after iteration 415 for lambda=0.024644\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 11.884446620941162\n",
      "After the matrix factor stage, training error is 0.00471, validation error is 0.02635\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.30185, val loss: 0.30478\n",
      "Main effects training epoch: 2, train loss: 0.23228, val loss: 0.23617\n",
      "Main effects training epoch: 3, train loss: 0.17997, val loss: 0.18315\n",
      "Main effects training epoch: 4, train loss: 0.14845, val loss: 0.15215\n",
      "Main effects training epoch: 5, train loss: 0.13484, val loss: 0.13695\n",
      "Main effects training epoch: 6, train loss: 0.13033, val loss: 0.13155\n",
      "Main effects training epoch: 7, train loss: 0.13055, val loss: 0.13149\n",
      "Main effects training epoch: 8, train loss: 0.12971, val loss: 0.12988\n",
      "Main effects training epoch: 9, train loss: 0.12862, val loss: 0.12919\n",
      "Main effects training epoch: 10, train loss: 0.12828, val loss: 0.12938\n",
      "Main effects training epoch: 11, train loss: 0.12732, val loss: 0.12789\n",
      "Main effects training epoch: 12, train loss: 0.12659, val loss: 0.12743\n",
      "Main effects training epoch: 13, train loss: 0.12563, val loss: 0.12654\n",
      "Main effects training epoch: 14, train loss: 0.12325, val loss: 0.12456\n",
      "Main effects training epoch: 15, train loss: 0.11949, val loss: 0.12161\n",
      "Main effects training epoch: 16, train loss: 0.11686, val loss: 0.11922\n",
      "Main effects training epoch: 17, train loss: 0.11423, val loss: 0.11562\n",
      "Main effects training epoch: 18, train loss: 0.11484, val loss: 0.11687\n",
      "Main effects training epoch: 19, train loss: 0.11188, val loss: 0.11473\n",
      "Main effects training epoch: 20, train loss: 0.11089, val loss: 0.11324\n",
      "Main effects training epoch: 21, train loss: 0.10849, val loss: 0.11127\n",
      "Main effects training epoch: 22, train loss: 0.10778, val loss: 0.11002\n",
      "Main effects training epoch: 23, train loss: 0.10761, val loss: 0.10934\n",
      "Main effects training epoch: 24, train loss: 0.10695, val loss: 0.10962\n",
      "Main effects training epoch: 25, train loss: 0.10869, val loss: 0.11093\n",
      "Main effects training epoch: 26, train loss: 0.10721, val loss: 0.11016\n",
      "Main effects training epoch: 27, train loss: 0.10692, val loss: 0.10911\n",
      "Main effects training epoch: 28, train loss: 0.10656, val loss: 0.10921\n",
      "Main effects training epoch: 29, train loss: 0.10666, val loss: 0.10915\n",
      "Main effects training epoch: 30, train loss: 0.10688, val loss: 0.10958\n",
      "Main effects training epoch: 31, train loss: 0.10689, val loss: 0.10943\n",
      "Main effects training epoch: 32, train loss: 0.10622, val loss: 0.10918\n",
      "Main effects training epoch: 33, train loss: 0.10631, val loss: 0.10928\n",
      "Main effects training epoch: 34, train loss: 0.10605, val loss: 0.10868\n",
      "Main effects training epoch: 35, train loss: 0.10599, val loss: 0.10879\n",
      "Main effects training epoch: 36, train loss: 0.10605, val loss: 0.10911\n",
      "Main effects training epoch: 37, train loss: 0.10704, val loss: 0.10981\n",
      "Main effects training epoch: 38, train loss: 0.10655, val loss: 0.10926\n",
      "Main effects training epoch: 39, train loss: 0.10607, val loss: 0.10888\n",
      "Main effects training epoch: 40, train loss: 0.10588, val loss: 0.10859\n",
      "Main effects training epoch: 41, train loss: 0.10600, val loss: 0.10889\n",
      "Main effects training epoch: 42, train loss: 0.10616, val loss: 0.10903\n",
      "Main effects training epoch: 43, train loss: 0.10609, val loss: 0.10883\n",
      "Main effects training epoch: 44, train loss: 0.10630, val loss: 0.10899\n",
      "Main effects training epoch: 45, train loss: 0.10686, val loss: 0.10928\n",
      "Main effects training epoch: 46, train loss: 0.10599, val loss: 0.10904\n",
      "Main effects training epoch: 47, train loss: 0.10577, val loss: 0.10887\n",
      "Main effects training epoch: 48, train loss: 0.10584, val loss: 0.10888\n",
      "Main effects training epoch: 49, train loss: 0.10576, val loss: 0.10866\n",
      "Main effects training epoch: 50, train loss: 0.10617, val loss: 0.10903\n",
      "Main effects training epoch: 51, train loss: 0.10592, val loss: 0.10893\n",
      "Main effects training epoch: 52, train loss: 0.10606, val loss: 0.10892\n",
      "Main effects training epoch: 53, train loss: 0.10577, val loss: 0.10888\n",
      "Main effects training epoch: 54, train loss: 0.10586, val loss: 0.10875\n",
      "Main effects training epoch: 55, train loss: 0.10635, val loss: 0.10929\n",
      "Main effects training epoch: 56, train loss: 0.10600, val loss: 0.10895\n",
      "Main effects training epoch: 57, train loss: 0.10614, val loss: 0.10942\n",
      "Main effects training epoch: 58, train loss: 0.10591, val loss: 0.10905\n",
      "Main effects training epoch: 59, train loss: 0.10587, val loss: 0.10866\n",
      "Main effects training epoch: 60, train loss: 0.10580, val loss: 0.10885\n",
      "Main effects training epoch: 61, train loss: 0.10590, val loss: 0.10868\n",
      "Main effects training epoch: 62, train loss: 0.10611, val loss: 0.10911\n",
      "Main effects training epoch: 63, train loss: 0.10587, val loss: 0.10877\n",
      "Main effects training epoch: 64, train loss: 0.10611, val loss: 0.10908\n",
      "Main effects training epoch: 65, train loss: 0.10597, val loss: 0.10888\n",
      "Main effects training epoch: 66, train loss: 0.10579, val loss: 0.10886\n",
      "Main effects training epoch: 67, train loss: 0.10588, val loss: 0.10853\n",
      "Main effects training epoch: 68, train loss: 0.10601, val loss: 0.10926\n",
      "Main effects training epoch: 69, train loss: 0.10583, val loss: 0.10866\n",
      "Main effects training epoch: 70, train loss: 0.10585, val loss: 0.10893\n",
      "Main effects training epoch: 71, train loss: 0.10592, val loss: 0.10879\n",
      "Main effects training epoch: 72, train loss: 0.10582, val loss: 0.10901\n",
      "Main effects training epoch: 73, train loss: 0.10586, val loss: 0.10890\n",
      "Main effects training epoch: 74, train loss: 0.10596, val loss: 0.10893\n",
      "Main effects training epoch: 75, train loss: 0.10614, val loss: 0.10884\n",
      "Main effects training epoch: 76, train loss: 0.10583, val loss: 0.10904\n",
      "Main effects training epoch: 77, train loss: 0.10592, val loss: 0.10890\n",
      "Main effects training epoch: 78, train loss: 0.10580, val loss: 0.10881\n",
      "Main effects training epoch: 79, train loss: 0.10594, val loss: 0.10889\n",
      "Main effects training epoch: 80, train loss: 0.10607, val loss: 0.10888\n",
      "Main effects training epoch: 81, train loss: 0.10581, val loss: 0.10891\n",
      "Main effects training epoch: 82, train loss: 0.10593, val loss: 0.10922\n",
      "Main effects training epoch: 83, train loss: 0.10574, val loss: 0.10873\n",
      "Main effects training epoch: 84, train loss: 0.10573, val loss: 0.10877\n",
      "Main effects training epoch: 85, train loss: 0.10636, val loss: 0.10929\n",
      "Main effects training epoch: 86, train loss: 0.10583, val loss: 0.10874\n",
      "Main effects training epoch: 87, train loss: 0.10614, val loss: 0.10928\n",
      "Main effects training epoch: 88, train loss: 0.10616, val loss: 0.10949\n",
      "Main effects training epoch: 89, train loss: 0.10589, val loss: 0.10865\n",
      "Main effects training epoch: 90, train loss: 0.10604, val loss: 0.10903\n",
      "Main effects training epoch: 91, train loss: 0.10588, val loss: 0.10892\n",
      "Main effects training epoch: 92, train loss: 0.10612, val loss: 0.10925\n",
      "Main effects training epoch: 93, train loss: 0.10583, val loss: 0.10911\n",
      "Main effects training epoch: 94, train loss: 0.10604, val loss: 0.10904\n",
      "Main effects training epoch: 95, train loss: 0.10576, val loss: 0.10903\n",
      "Main effects training epoch: 96, train loss: 0.10568, val loss: 0.10869\n",
      "Main effects training epoch: 97, train loss: 0.10588, val loss: 0.10926\n",
      "Main effects training epoch: 98, train loss: 0.10606, val loss: 0.10910\n",
      "Main effects training epoch: 99, train loss: 0.10649, val loss: 0.10930\n",
      "Main effects training epoch: 100, train loss: 0.10628, val loss: 0.10948\n",
      "Main effects training epoch: 101, train loss: 0.10586, val loss: 0.10912\n",
      "Main effects training epoch: 102, train loss: 0.10570, val loss: 0.10869\n",
      "Main effects training epoch: 103, train loss: 0.10643, val loss: 0.10951\n",
      "Main effects training epoch: 104, train loss: 0.10650, val loss: 0.10946\n",
      "Main effects training epoch: 105, train loss: 0.10626, val loss: 0.10954\n",
      "Main effects training epoch: 106, train loss: 0.10594, val loss: 0.10896\n",
      "Main effects training epoch: 107, train loss: 0.10579, val loss: 0.10878\n",
      "Main effects training epoch: 108, train loss: 0.10607, val loss: 0.10922\n",
      "Main effects training epoch: 109, train loss: 0.10581, val loss: 0.10902\n",
      "Main effects training epoch: 110, train loss: 0.10590, val loss: 0.10892\n",
      "Main effects training epoch: 111, train loss: 0.10605, val loss: 0.10924\n",
      "Main effects training epoch: 112, train loss: 0.10580, val loss: 0.10875\n",
      "Main effects training epoch: 113, train loss: 0.10587, val loss: 0.10900\n",
      "Main effects training epoch: 114, train loss: 0.10581, val loss: 0.10883\n",
      "Main effects training epoch: 115, train loss: 0.10618, val loss: 0.10915\n",
      "Main effects training epoch: 116, train loss: 0.10615, val loss: 0.10949\n",
      "Main effects training epoch: 117, train loss: 0.10602, val loss: 0.10894\n",
      "Main effects training epoch: 118, train loss: 0.10595, val loss: 0.10914\n",
      "Main effects training epoch: 119, train loss: 0.10618, val loss: 0.10895\n",
      "Main effects training epoch: 120, train loss: 0.10601, val loss: 0.10941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 121, train loss: 0.10588, val loss: 0.10869\n",
      "Main effects training epoch: 122, train loss: 0.10576, val loss: 0.10885\n",
      "Main effects training epoch: 123, train loss: 0.10578, val loss: 0.10899\n",
      "Main effects training epoch: 124, train loss: 0.10583, val loss: 0.10934\n",
      "Main effects training epoch: 125, train loss: 0.10610, val loss: 0.10904\n",
      "Main effects training epoch: 126, train loss: 0.10593, val loss: 0.10917\n",
      "Main effects training epoch: 127, train loss: 0.10579, val loss: 0.10863\n",
      "Main effects training epoch: 128, train loss: 0.10583, val loss: 0.10933\n",
      "Main effects training epoch: 129, train loss: 0.10580, val loss: 0.10919\n",
      "Main effects training epoch: 130, train loss: 0.10573, val loss: 0.10874\n",
      "Main effects training epoch: 131, train loss: 0.10601, val loss: 0.10951\n",
      "Main effects training epoch: 132, train loss: 0.10600, val loss: 0.10905\n",
      "Main effects training epoch: 133, train loss: 0.10601, val loss: 0.10931\n",
      "Main effects training epoch: 134, train loss: 0.10566, val loss: 0.10882\n",
      "Main effects training epoch: 135, train loss: 0.10590, val loss: 0.10909\n",
      "Main effects training epoch: 136, train loss: 0.10621, val loss: 0.10930\n",
      "Main effects training epoch: 137, train loss: 0.10597, val loss: 0.10906\n",
      "Main effects training epoch: 138, train loss: 0.10583, val loss: 0.10895\n",
      "Main effects training epoch: 139, train loss: 0.10580, val loss: 0.10894\n",
      "Main effects training epoch: 140, train loss: 0.10591, val loss: 0.10891\n",
      "Main effects training epoch: 141, train loss: 0.10595, val loss: 0.10903\n",
      "Main effects training epoch: 142, train loss: 0.10601, val loss: 0.10932\n",
      "Main effects training epoch: 143, train loss: 0.10596, val loss: 0.10892\n",
      "Main effects training epoch: 144, train loss: 0.10583, val loss: 0.10925\n",
      "Main effects training epoch: 145, train loss: 0.10572, val loss: 0.10887\n",
      "Main effects training epoch: 146, train loss: 0.10574, val loss: 0.10897\n",
      "Main effects training epoch: 147, train loss: 0.10575, val loss: 0.10903\n",
      "Main effects training epoch: 148, train loss: 0.10589, val loss: 0.10894\n",
      "Main effects training epoch: 149, train loss: 0.10577, val loss: 0.10889\n",
      "Main effects training epoch: 150, train loss: 0.10569, val loss: 0.10895\n",
      "Main effects training epoch: 151, train loss: 0.10575, val loss: 0.10884\n",
      "Main effects training epoch: 152, train loss: 0.10606, val loss: 0.10908\n",
      "Main effects training epoch: 153, train loss: 0.10639, val loss: 0.10965\n",
      "Main effects training epoch: 154, train loss: 0.10596, val loss: 0.10920\n",
      "Main effects training epoch: 155, train loss: 0.10578, val loss: 0.10908\n",
      "Main effects training epoch: 156, train loss: 0.10569, val loss: 0.10881\n",
      "Main effects training epoch: 157, train loss: 0.10570, val loss: 0.10898\n",
      "Main effects training epoch: 158, train loss: 0.10595, val loss: 0.10931\n",
      "Main effects training epoch: 159, train loss: 0.10577, val loss: 0.10920\n",
      "Main effects training epoch: 160, train loss: 0.10578, val loss: 0.10935\n",
      "Main effects training epoch: 161, train loss: 0.10579, val loss: 0.10872\n",
      "Main effects training epoch: 162, train loss: 0.10575, val loss: 0.10904\n",
      "Main effects training epoch: 163, train loss: 0.10597, val loss: 0.10913\n",
      "Main effects training epoch: 164, train loss: 0.10605, val loss: 0.10923\n",
      "Main effects training epoch: 165, train loss: 0.10604, val loss: 0.10954\n",
      "Main effects training epoch: 166, train loss: 0.10575, val loss: 0.10883\n",
      "Main effects training epoch: 167, train loss: 0.10578, val loss: 0.10918\n",
      "Main effects training epoch: 168, train loss: 0.10576, val loss: 0.10888\n",
      "Early stop at epoch 168, with validation loss: 0.10888\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.10587, val loss: 0.10915\n",
      "Main effects tuning epoch: 2, train loss: 0.10583, val loss: 0.10863\n",
      "Main effects tuning epoch: 3, train loss: 0.10593, val loss: 0.10918\n",
      "Main effects tuning epoch: 4, train loss: 0.10580, val loss: 0.10863\n",
      "Main effects tuning epoch: 5, train loss: 0.10595, val loss: 0.10896\n",
      "Main effects tuning epoch: 6, train loss: 0.10614, val loss: 0.10907\n",
      "Main effects tuning epoch: 7, train loss: 0.10617, val loss: 0.10891\n",
      "Main effects tuning epoch: 8, train loss: 0.10639, val loss: 0.10952\n",
      "Main effects tuning epoch: 9, train loss: 0.10631, val loss: 0.10952\n",
      "Main effects tuning epoch: 10, train loss: 0.10598, val loss: 0.10913\n",
      "Main effects tuning epoch: 11, train loss: 0.10616, val loss: 0.10872\n",
      "Main effects tuning epoch: 12, train loss: 0.10613, val loss: 0.10940\n",
      "Main effects tuning epoch: 13, train loss: 0.10649, val loss: 0.10913\n",
      "Main effects tuning epoch: 14, train loss: 0.10595, val loss: 0.10906\n",
      "Main effects tuning epoch: 15, train loss: 0.10576, val loss: 0.10901\n",
      "Main effects tuning epoch: 16, train loss: 0.10591, val loss: 0.10867\n",
      "Main effects tuning epoch: 17, train loss: 0.10614, val loss: 0.10929\n",
      "Main effects tuning epoch: 18, train loss: 0.10616, val loss: 0.10914\n",
      "Main effects tuning epoch: 19, train loss: 0.10591, val loss: 0.10901\n",
      "Main effects tuning epoch: 20, train loss: 0.10585, val loss: 0.10893\n",
      "Main effects tuning epoch: 21, train loss: 0.10582, val loss: 0.10851\n",
      "Main effects tuning epoch: 22, train loss: 0.10595, val loss: 0.10917\n",
      "Main effects tuning epoch: 23, train loss: 0.10589, val loss: 0.10893\n",
      "Main effects tuning epoch: 24, train loss: 0.10579, val loss: 0.10899\n",
      "Main effects tuning epoch: 25, train loss: 0.10580, val loss: 0.10907\n",
      "Main effects tuning epoch: 26, train loss: 0.10612, val loss: 0.10929\n",
      "Main effects tuning epoch: 27, train loss: 0.10613, val loss: 0.10892\n",
      "Main effects tuning epoch: 28, train loss: 0.10608, val loss: 0.10898\n",
      "Main effects tuning epoch: 29, train loss: 0.10588, val loss: 0.10904\n",
      "Main effects tuning epoch: 30, train loss: 0.10603, val loss: 0.10875\n",
      "Main effects tuning epoch: 31, train loss: 0.10580, val loss: 0.10885\n",
      "Main effects tuning epoch: 32, train loss: 0.10586, val loss: 0.10892\n",
      "Main effects tuning epoch: 33, train loss: 0.10584, val loss: 0.10877\n",
      "Main effects tuning epoch: 34, train loss: 0.10610, val loss: 0.10890\n",
      "Main effects tuning epoch: 35, train loss: 0.10602, val loss: 0.10950\n",
      "Main effects tuning epoch: 36, train loss: 0.10577, val loss: 0.10870\n",
      "Main effects tuning epoch: 37, train loss: 0.10576, val loss: 0.10890\n",
      "Main effects tuning epoch: 38, train loss: 0.10590, val loss: 0.10894\n",
      "Main effects tuning epoch: 39, train loss: 0.10597, val loss: 0.10915\n",
      "Main effects tuning epoch: 40, train loss: 0.10575, val loss: 0.10856\n",
      "Main effects tuning epoch: 41, train loss: 0.10577, val loss: 0.10899\n",
      "Main effects tuning epoch: 42, train loss: 0.10600, val loss: 0.10888\n",
      "Main effects tuning epoch: 43, train loss: 0.10624, val loss: 0.10938\n",
      "Main effects tuning epoch: 44, train loss: 0.10606, val loss: 0.10863\n",
      "Main effects tuning epoch: 45, train loss: 0.10614, val loss: 0.10961\n",
      "Main effects tuning epoch: 46, train loss: 0.10598, val loss: 0.10874\n",
      "Main effects tuning epoch: 47, train loss: 0.10623, val loss: 0.10916\n",
      "Main effects tuning epoch: 48, train loss: 0.10628, val loss: 0.10910\n",
      "Main effects tuning epoch: 49, train loss: 0.10612, val loss: 0.10902\n",
      "Main effects tuning epoch: 50, train loss: 0.10600, val loss: 0.10922\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.16656, val loss: 0.16504\n",
      "Interaction training epoch: 2, train loss: 0.19799, val loss: 0.19730\n",
      "Interaction training epoch: 3, train loss: 0.07919, val loss: 0.08135\n",
      "Interaction training epoch: 4, train loss: 0.06592, val loss: 0.06695\n",
      "Interaction training epoch: 5, train loss: 0.06719, val loss: 0.06623\n",
      "Interaction training epoch: 6, train loss: 0.07116, val loss: 0.06992\n",
      "Interaction training epoch: 7, train loss: 0.05765, val loss: 0.05869\n",
      "Interaction training epoch: 8, train loss: 0.05791, val loss: 0.05822\n",
      "Interaction training epoch: 9, train loss: 0.06091, val loss: 0.06046\n",
      "Interaction training epoch: 10, train loss: 0.05754, val loss: 0.05788\n",
      "Interaction training epoch: 11, train loss: 0.05624, val loss: 0.05624\n",
      "Interaction training epoch: 12, train loss: 0.05651, val loss: 0.05600\n",
      "Interaction training epoch: 13, train loss: 0.06101, val loss: 0.06080\n",
      "Interaction training epoch: 14, train loss: 0.05709, val loss: 0.05641\n",
      "Interaction training epoch: 15, train loss: 0.06077, val loss: 0.06101\n",
      "Interaction training epoch: 16, train loss: 0.05239, val loss: 0.05197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 17, train loss: 0.05618, val loss: 0.05592\n",
      "Interaction training epoch: 18, train loss: 0.05477, val loss: 0.05468\n",
      "Interaction training epoch: 19, train loss: 0.05152, val loss: 0.05113\n",
      "Interaction training epoch: 20, train loss: 0.05519, val loss: 0.05482\n",
      "Interaction training epoch: 21, train loss: 0.05818, val loss: 0.05633\n",
      "Interaction training epoch: 22, train loss: 0.05614, val loss: 0.05550\n",
      "Interaction training epoch: 23, train loss: 0.05384, val loss: 0.05252\n",
      "Interaction training epoch: 24, train loss: 0.05143, val loss: 0.05136\n",
      "Interaction training epoch: 25, train loss: 0.05185, val loss: 0.05118\n",
      "Interaction training epoch: 26, train loss: 0.05409, val loss: 0.05515\n",
      "Interaction training epoch: 27, train loss: 0.05171, val loss: 0.05189\n",
      "Interaction training epoch: 28, train loss: 0.05284, val loss: 0.05307\n",
      "Interaction training epoch: 29, train loss: 0.05251, val loss: 0.05306\n",
      "Interaction training epoch: 30, train loss: 0.05654, val loss: 0.05667\n",
      "Interaction training epoch: 31, train loss: 0.05008, val loss: 0.05013\n",
      "Interaction training epoch: 32, train loss: 0.05165, val loss: 0.05175\n",
      "Interaction training epoch: 33, train loss: 0.05094, val loss: 0.05147\n",
      "Interaction training epoch: 34, train loss: 0.05154, val loss: 0.05255\n",
      "Interaction training epoch: 35, train loss: 0.05178, val loss: 0.05163\n",
      "Interaction training epoch: 36, train loss: 0.05189, val loss: 0.05137\n",
      "Interaction training epoch: 37, train loss: 0.05261, val loss: 0.05262\n",
      "Interaction training epoch: 38, train loss: 0.05395, val loss: 0.05444\n",
      "Interaction training epoch: 39, train loss: 0.05053, val loss: 0.05093\n",
      "Interaction training epoch: 40, train loss: 0.05186, val loss: 0.05165\n",
      "Interaction training epoch: 41, train loss: 0.05174, val loss: 0.05183\n",
      "Interaction training epoch: 42, train loss: 0.05645, val loss: 0.05707\n",
      "Interaction training epoch: 43, train loss: 0.05130, val loss: 0.05143\n",
      "Interaction training epoch: 44, train loss: 0.05233, val loss: 0.05213\n",
      "Interaction training epoch: 45, train loss: 0.05242, val loss: 0.05266\n",
      "Interaction training epoch: 46, train loss: 0.05234, val loss: 0.05256\n",
      "Interaction training epoch: 47, train loss: 0.05096, val loss: 0.05121\n",
      "Interaction training epoch: 48, train loss: 0.05324, val loss: 0.05317\n",
      "Interaction training epoch: 49, train loss: 0.05143, val loss: 0.05175\n",
      "Interaction training epoch: 50, train loss: 0.05906, val loss: 0.05866\n",
      "Interaction training epoch: 51, train loss: 0.05278, val loss: 0.05220\n",
      "Interaction training epoch: 52, train loss: 0.05429, val loss: 0.05321\n",
      "Interaction training epoch: 53, train loss: 0.05247, val loss: 0.05225\n",
      "Interaction training epoch: 54, train loss: 0.05061, val loss: 0.05129\n",
      "Interaction training epoch: 55, train loss: 0.05239, val loss: 0.05237\n",
      "Interaction training epoch: 56, train loss: 0.05174, val loss: 0.05287\n",
      "Interaction training epoch: 57, train loss: 0.05160, val loss: 0.05111\n",
      "Interaction training epoch: 58, train loss: 0.05180, val loss: 0.05292\n",
      "Interaction training epoch: 59, train loss: 0.04982, val loss: 0.05009\n",
      "Interaction training epoch: 60, train loss: 0.05227, val loss: 0.05368\n",
      "Interaction training epoch: 61, train loss: 0.05217, val loss: 0.05210\n",
      "Interaction training epoch: 62, train loss: 0.05023, val loss: 0.05074\n",
      "Interaction training epoch: 63, train loss: 0.05179, val loss: 0.05284\n",
      "Interaction training epoch: 64, train loss: 0.04959, val loss: 0.05129\n",
      "Interaction training epoch: 65, train loss: 0.05320, val loss: 0.05225\n",
      "Interaction training epoch: 66, train loss: 0.05295, val loss: 0.05326\n",
      "Interaction training epoch: 67, train loss: 0.05028, val loss: 0.05144\n",
      "Interaction training epoch: 68, train loss: 0.05112, val loss: 0.05151\n",
      "Interaction training epoch: 69, train loss: 0.05228, val loss: 0.05248\n",
      "Interaction training epoch: 70, train loss: 0.04994, val loss: 0.05062\n",
      "Interaction training epoch: 71, train loss: 0.05225, val loss: 0.05286\n",
      "Interaction training epoch: 72, train loss: 0.05069, val loss: 0.05180\n",
      "Interaction training epoch: 73, train loss: 0.05077, val loss: 0.05196\n",
      "Interaction training epoch: 74, train loss: 0.04967, val loss: 0.05076\n",
      "Interaction training epoch: 75, train loss: 0.05064, val loss: 0.05071\n",
      "Interaction training epoch: 76, train loss: 0.04886, val loss: 0.05004\n",
      "Interaction training epoch: 77, train loss: 0.05097, val loss: 0.05075\n",
      "Interaction training epoch: 78, train loss: 0.05171, val loss: 0.05128\n",
      "Interaction training epoch: 79, train loss: 0.05178, val loss: 0.05168\n",
      "Interaction training epoch: 80, train loss: 0.04972, val loss: 0.05111\n",
      "Interaction training epoch: 81, train loss: 0.05307, val loss: 0.05407\n",
      "Interaction training epoch: 82, train loss: 0.05287, val loss: 0.05331\n",
      "Interaction training epoch: 83, train loss: 0.05019, val loss: 0.05065\n",
      "Interaction training epoch: 84, train loss: 0.05109, val loss: 0.05177\n",
      "Interaction training epoch: 85, train loss: 0.05241, val loss: 0.05395\n",
      "Interaction training epoch: 86, train loss: 0.05169, val loss: 0.05187\n",
      "Interaction training epoch: 87, train loss: 0.05013, val loss: 0.05031\n",
      "Interaction training epoch: 88, train loss: 0.05249, val loss: 0.05324\n",
      "Interaction training epoch: 89, train loss: 0.04905, val loss: 0.04955\n",
      "Interaction training epoch: 90, train loss: 0.05048, val loss: 0.05104\n",
      "Interaction training epoch: 91, train loss: 0.05291, val loss: 0.05320\n",
      "Interaction training epoch: 92, train loss: 0.05194, val loss: 0.05184\n",
      "Interaction training epoch: 93, train loss: 0.04813, val loss: 0.04857\n",
      "Interaction training epoch: 94, train loss: 0.05434, val loss: 0.05466\n",
      "Interaction training epoch: 95, train loss: 0.05394, val loss: 0.05306\n",
      "Interaction training epoch: 96, train loss: 0.04954, val loss: 0.04995\n",
      "Interaction training epoch: 97, train loss: 0.05003, val loss: 0.05058\n",
      "Interaction training epoch: 98, train loss: 0.05197, val loss: 0.05219\n",
      "Interaction training epoch: 99, train loss: 0.05436, val loss: 0.05280\n",
      "Interaction training epoch: 100, train loss: 0.05350, val loss: 0.05437\n",
      "Interaction training epoch: 101, train loss: 0.04964, val loss: 0.04980\n",
      "Interaction training epoch: 102, train loss: 0.05136, val loss: 0.05087\n",
      "Interaction training epoch: 103, train loss: 0.05162, val loss: 0.05191\n",
      "Interaction training epoch: 104, train loss: 0.05118, val loss: 0.05150\n",
      "Interaction training epoch: 105, train loss: 0.04993, val loss: 0.04937\n",
      "Interaction training epoch: 106, train loss: 0.05171, val loss: 0.05164\n",
      "Interaction training epoch: 107, train loss: 0.05193, val loss: 0.05075\n",
      "Interaction training epoch: 108, train loss: 0.05102, val loss: 0.05183\n",
      "Interaction training epoch: 109, train loss: 0.05261, val loss: 0.05324\n",
      "Interaction training epoch: 110, train loss: 0.05047, val loss: 0.05100\n",
      "Interaction training epoch: 111, train loss: 0.05046, val loss: 0.05063\n",
      "Interaction training epoch: 112, train loss: 0.04811, val loss: 0.04809\n",
      "Interaction training epoch: 113, train loss: 0.05250, val loss: 0.05298\n",
      "Interaction training epoch: 114, train loss: 0.05036, val loss: 0.05112\n",
      "Interaction training epoch: 115, train loss: 0.05119, val loss: 0.05140\n",
      "Interaction training epoch: 116, train loss: 0.04975, val loss: 0.05112\n",
      "Interaction training epoch: 117, train loss: 0.04911, val loss: 0.04994\n",
      "Interaction training epoch: 118, train loss: 0.04990, val loss: 0.05012\n",
      "Interaction training epoch: 119, train loss: 0.04854, val loss: 0.04775\n",
      "Interaction training epoch: 120, train loss: 0.04985, val loss: 0.04987\n",
      "Interaction training epoch: 121, train loss: 0.04779, val loss: 0.04856\n",
      "Interaction training epoch: 122, train loss: 0.04712, val loss: 0.04773\n",
      "Interaction training epoch: 123, train loss: 0.05164, val loss: 0.05255\n",
      "Interaction training epoch: 124, train loss: 0.05327, val loss: 0.05365\n",
      "Interaction training epoch: 125, train loss: 0.05348, val loss: 0.05414\n",
      "Interaction training epoch: 126, train loss: 0.04873, val loss: 0.04978\n",
      "Interaction training epoch: 127, train loss: 0.04866, val loss: 0.04894\n",
      "Interaction training epoch: 128, train loss: 0.04947, val loss: 0.05037\n",
      "Interaction training epoch: 129, train loss: 0.04701, val loss: 0.04711\n",
      "Interaction training epoch: 130, train loss: 0.04867, val loss: 0.04878\n",
      "Interaction training epoch: 131, train loss: 0.04995, val loss: 0.05004\n",
      "Interaction training epoch: 132, train loss: 0.05004, val loss: 0.05008\n",
      "Interaction training epoch: 133, train loss: 0.04970, val loss: 0.05023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 134, train loss: 0.04969, val loss: 0.05041\n",
      "Interaction training epoch: 135, train loss: 0.04826, val loss: 0.04935\n",
      "Interaction training epoch: 136, train loss: 0.04956, val loss: 0.05077\n",
      "Interaction training epoch: 137, train loss: 0.04937, val loss: 0.05116\n",
      "Interaction training epoch: 138, train loss: 0.05042, val loss: 0.05126\n",
      "Interaction training epoch: 139, train loss: 0.04910, val loss: 0.04944\n",
      "Interaction training epoch: 140, train loss: 0.05235, val loss: 0.05329\n",
      "Interaction training epoch: 141, train loss: 0.05151, val loss: 0.05199\n",
      "Interaction training epoch: 142, train loss: 0.04935, val loss: 0.04965\n",
      "Interaction training epoch: 143, train loss: 0.05663, val loss: 0.05667\n",
      "Interaction training epoch: 144, train loss: 0.05217, val loss: 0.05297\n",
      "Interaction training epoch: 145, train loss: 0.05139, val loss: 0.05143\n",
      "Interaction training epoch: 146, train loss: 0.05350, val loss: 0.05388\n",
      "Interaction training epoch: 147, train loss: 0.06195, val loss: 0.06171\n",
      "Interaction training epoch: 148, train loss: 0.05923, val loss: 0.05958\n",
      "Interaction training epoch: 149, train loss: 0.05897, val loss: 0.05842\n",
      "Interaction training epoch: 150, train loss: 0.05688, val loss: 0.05663\n",
      "Interaction training epoch: 151, train loss: 0.05292, val loss: 0.05339\n",
      "Interaction training epoch: 152, train loss: 0.04638, val loss: 0.04708\n",
      "Interaction training epoch: 153, train loss: 0.04830, val loss: 0.05070\n",
      "Interaction training epoch: 154, train loss: 0.04798, val loss: 0.04952\n",
      "Interaction training epoch: 155, train loss: 0.04659, val loss: 0.04786\n",
      "Interaction training epoch: 156, train loss: 0.04767, val loss: 0.04876\n",
      "Interaction training epoch: 157, train loss: 0.04853, val loss: 0.04917\n",
      "Interaction training epoch: 158, train loss: 0.04677, val loss: 0.04803\n",
      "Interaction training epoch: 159, train loss: 0.04901, val loss: 0.04931\n",
      "Interaction training epoch: 160, train loss: 0.04754, val loss: 0.04893\n",
      "Interaction training epoch: 161, train loss: 0.04874, val loss: 0.04950\n",
      "Interaction training epoch: 162, train loss: 0.04595, val loss: 0.04721\n",
      "Interaction training epoch: 163, train loss: 0.04732, val loss: 0.04956\n",
      "Interaction training epoch: 164, train loss: 0.04567, val loss: 0.04749\n",
      "Interaction training epoch: 165, train loss: 0.05013, val loss: 0.05115\n",
      "Interaction training epoch: 166, train loss: 0.04663, val loss: 0.04790\n",
      "Interaction training epoch: 167, train loss: 0.04602, val loss: 0.04784\n",
      "Interaction training epoch: 168, train loss: 0.04789, val loss: 0.04800\n",
      "Interaction training epoch: 169, train loss: 0.04480, val loss: 0.04615\n",
      "Interaction training epoch: 170, train loss: 0.04657, val loss: 0.04712\n",
      "Interaction training epoch: 171, train loss: 0.04515, val loss: 0.04688\n",
      "Interaction training epoch: 172, train loss: 0.04745, val loss: 0.04904\n",
      "Interaction training epoch: 173, train loss: 0.04801, val loss: 0.05050\n",
      "Interaction training epoch: 174, train loss: 0.05297, val loss: 0.05389\n",
      "Interaction training epoch: 175, train loss: 0.04522, val loss: 0.04687\n",
      "Interaction training epoch: 176, train loss: 0.04654, val loss: 0.04709\n",
      "Interaction training epoch: 177, train loss: 0.04462, val loss: 0.04576\n",
      "Interaction training epoch: 178, train loss: 0.04715, val loss: 0.04880\n",
      "Interaction training epoch: 179, train loss: 0.04515, val loss: 0.04673\n",
      "Interaction training epoch: 180, train loss: 0.04652, val loss: 0.04824\n",
      "Interaction training epoch: 181, train loss: 0.04757, val loss: 0.04936\n",
      "Interaction training epoch: 182, train loss: 0.04771, val loss: 0.04909\n",
      "Interaction training epoch: 183, train loss: 0.04999, val loss: 0.05186\n",
      "Interaction training epoch: 184, train loss: 0.04691, val loss: 0.04877\n",
      "Interaction training epoch: 185, train loss: 0.04599, val loss: 0.04763\n",
      "Interaction training epoch: 186, train loss: 0.04839, val loss: 0.04969\n",
      "Interaction training epoch: 187, train loss: 0.04634, val loss: 0.04846\n",
      "Interaction training epoch: 188, train loss: 0.04686, val loss: 0.04828\n",
      "Interaction training epoch: 189, train loss: 0.04527, val loss: 0.04603\n",
      "Interaction training epoch: 190, train loss: 0.04811, val loss: 0.05046\n",
      "Interaction training epoch: 191, train loss: 0.04668, val loss: 0.04794\n",
      "Interaction training epoch: 192, train loss: 0.04580, val loss: 0.04673\n",
      "Interaction training epoch: 193, train loss: 0.04953, val loss: 0.05077\n",
      "Interaction training epoch: 194, train loss: 0.04637, val loss: 0.04745\n",
      "Interaction training epoch: 195, train loss: 0.05291, val loss: 0.05606\n",
      "Interaction training epoch: 196, train loss: 0.04552, val loss: 0.04706\n",
      "Interaction training epoch: 197, train loss: 0.04954, val loss: 0.05175\n",
      "Interaction training epoch: 198, train loss: 0.04504, val loss: 0.04659\n",
      "Interaction training epoch: 199, train loss: 0.04939, val loss: 0.05068\n",
      "Interaction training epoch: 200, train loss: 0.04824, val loss: 0.04969\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########7 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.04624, val loss: 0.04648\n",
      "Interaction tuning epoch: 2, train loss: 0.04654, val loss: 0.04675\n",
      "Interaction tuning epoch: 3, train loss: 0.04688, val loss: 0.04770\n",
      "Interaction tuning epoch: 4, train loss: 0.04703, val loss: 0.04714\n",
      "Interaction tuning epoch: 5, train loss: 0.04871, val loss: 0.04870\n",
      "Interaction tuning epoch: 6, train loss: 0.04715, val loss: 0.04685\n",
      "Interaction tuning epoch: 7, train loss: 0.04753, val loss: 0.04769\n",
      "Interaction tuning epoch: 8, train loss: 0.04804, val loss: 0.04839\n",
      "Interaction tuning epoch: 9, train loss: 0.04656, val loss: 0.04746\n",
      "Interaction tuning epoch: 10, train loss: 0.04708, val loss: 0.04792\n",
      "Interaction tuning epoch: 11, train loss: 0.04593, val loss: 0.04607\n",
      "Interaction tuning epoch: 12, train loss: 0.04889, val loss: 0.05019\n",
      "Interaction tuning epoch: 13, train loss: 0.04756, val loss: 0.04795\n",
      "Interaction tuning epoch: 14, train loss: 0.05056, val loss: 0.04891\n",
      "Interaction tuning epoch: 15, train loss: 0.05649, val loss: 0.05700\n",
      "Interaction tuning epoch: 16, train loss: 0.04770, val loss: 0.04777\n",
      "Interaction tuning epoch: 17, train loss: 0.04665, val loss: 0.04734\n",
      "Interaction tuning epoch: 18, train loss: 0.04709, val loss: 0.04763\n",
      "Interaction tuning epoch: 19, train loss: 0.04762, val loss: 0.04775\n",
      "Interaction tuning epoch: 20, train loss: 0.04647, val loss: 0.04691\n",
      "Interaction tuning epoch: 21, train loss: 0.04858, val loss: 0.04904\n",
      "Interaction tuning epoch: 22, train loss: 0.04763, val loss: 0.04743\n",
      "Interaction tuning epoch: 23, train loss: 0.04883, val loss: 0.04879\n",
      "Interaction tuning epoch: 24, train loss: 0.04608, val loss: 0.04728\n",
      "Interaction tuning epoch: 25, train loss: 0.04999, val loss: 0.04940\n",
      "Interaction tuning epoch: 26, train loss: 0.04916, val loss: 0.04857\n",
      "Interaction tuning epoch: 27, train loss: 0.04784, val loss: 0.04855\n",
      "Interaction tuning epoch: 28, train loss: 0.04702, val loss: 0.04758\n",
      "Interaction tuning epoch: 29, train loss: 0.04685, val loss: 0.04720\n",
      "Interaction tuning epoch: 30, train loss: 0.04688, val loss: 0.04800\n",
      "Interaction tuning epoch: 31, train loss: 0.04656, val loss: 0.04725\n",
      "Interaction tuning epoch: 32, train loss: 0.04698, val loss: 0.04718\n",
      "Interaction tuning epoch: 33, train loss: 0.04648, val loss: 0.04674\n",
      "Interaction tuning epoch: 34, train loss: 0.04622, val loss: 0.04627\n",
      "Interaction tuning epoch: 35, train loss: 0.04695, val loss: 0.04653\n",
      "Interaction tuning epoch: 36, train loss: 0.04638, val loss: 0.04708\n",
      "Interaction tuning epoch: 37, train loss: 0.04517, val loss: 0.04599\n",
      "Interaction tuning epoch: 38, train loss: 0.04594, val loss: 0.04626\n",
      "Interaction tuning epoch: 39, train loss: 0.04806, val loss: 0.04884\n",
      "Interaction tuning epoch: 40, train loss: 0.04762, val loss: 0.04781\n",
      "Interaction tuning epoch: 41, train loss: 0.04762, val loss: 0.04909\n",
      "Interaction tuning epoch: 42, train loss: 0.04746, val loss: 0.04805\n",
      "Interaction tuning epoch: 43, train loss: 0.04604, val loss: 0.04612\n",
      "Interaction tuning epoch: 44, train loss: 0.04636, val loss: 0.04673\n",
      "Interaction tuning epoch: 45, train loss: 0.04643, val loss: 0.04712\n",
      "Interaction tuning epoch: 46, train loss: 0.04656, val loss: 0.04678\n",
      "Interaction tuning epoch: 47, train loss: 0.04635, val loss: 0.04667\n",
      "Interaction tuning epoch: 48, train loss: 0.04816, val loss: 0.04882\n",
      "Interaction tuning epoch: 49, train loss: 0.04683, val loss: 0.04711\n",
      "Interaction tuning epoch: 50, train loss: 0.04845, val loss: 0.04905\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 35.22208261489868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After the gam stage, training error is 0.04845 , validation error is 0.04905\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 1.232198\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.036380 validation MAE=0.046299,rank=8\n",
      "[SoftImpute] Iter 2: observed MAE=0.032684 validation MAE=0.045229,rank=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 3: observed MAE=0.029760 validation MAE=0.044313,rank=8\n",
      "[SoftImpute] Iter 4: observed MAE=0.027361 validation MAE=0.043525,rank=8\n",
      "[SoftImpute] Iter 5: observed MAE=0.025363 validation MAE=0.042847,rank=8\n",
      "[SoftImpute] Iter 6: observed MAE=0.023664 validation MAE=0.042252,rank=8\n",
      "[SoftImpute] Iter 7: observed MAE=0.022191 validation MAE=0.041741,rank=8\n",
      "[SoftImpute] Iter 8: observed MAE=0.020914 validation MAE=0.041299,rank=8\n",
      "[SoftImpute] Iter 9: observed MAE=0.019801 validation MAE=0.040904,rank=8\n",
      "[SoftImpute] Iter 10: observed MAE=0.018824 validation MAE=0.040558,rank=8\n",
      "[SoftImpute] Iter 11: observed MAE=0.017959 validation MAE=0.040259,rank=8\n",
      "[SoftImpute] Iter 12: observed MAE=0.017182 validation MAE=0.039994,rank=8\n",
      "[SoftImpute] Iter 13: observed MAE=0.016489 validation MAE=0.039760,rank=8\n",
      "[SoftImpute] Iter 14: observed MAE=0.015867 validation MAE=0.039557,rank=8\n",
      "[SoftImpute] Iter 15: observed MAE=0.015307 validation MAE=0.039372,rank=8\n",
      "[SoftImpute] Iter 16: observed MAE=0.014798 validation MAE=0.039208,rank=8\n",
      "[SoftImpute] Iter 17: observed MAE=0.014334 validation MAE=0.039065,rank=8\n",
      "[SoftImpute] Iter 18: observed MAE=0.013906 validation MAE=0.038938,rank=8\n",
      "[SoftImpute] Iter 19: observed MAE=0.013512 validation MAE=0.038814,rank=8\n",
      "[SoftImpute] Iter 20: observed MAE=0.013149 validation MAE=0.038698,rank=8\n",
      "[SoftImpute] Iter 21: observed MAE=0.012815 validation MAE=0.038585,rank=8\n",
      "[SoftImpute] Iter 22: observed MAE=0.012507 validation MAE=0.038475,rank=8\n",
      "[SoftImpute] Iter 23: observed MAE=0.012220 validation MAE=0.038379,rank=8\n",
      "[SoftImpute] Iter 24: observed MAE=0.011955 validation MAE=0.038300,rank=8\n",
      "[SoftImpute] Iter 25: observed MAE=0.011707 validation MAE=0.038231,rank=8\n",
      "[SoftImpute] Iter 26: observed MAE=0.011475 validation MAE=0.038173,rank=8\n",
      "[SoftImpute] Iter 27: observed MAE=0.011257 validation MAE=0.038118,rank=8\n",
      "[SoftImpute] Iter 28: observed MAE=0.011053 validation MAE=0.038064,rank=8\n",
      "[SoftImpute] Iter 29: observed MAE=0.010860 validation MAE=0.038012,rank=8\n",
      "[SoftImpute] Iter 30: observed MAE=0.010678 validation MAE=0.037961,rank=8\n",
      "[SoftImpute] Iter 31: observed MAE=0.010506 validation MAE=0.037911,rank=8\n",
      "[SoftImpute] Iter 32: observed MAE=0.010343 validation MAE=0.037862,rank=8\n",
      "[SoftImpute] Iter 33: observed MAE=0.010188 validation MAE=0.037815,rank=8\n",
      "[SoftImpute] Iter 34: observed MAE=0.010041 validation MAE=0.037766,rank=8\n",
      "[SoftImpute] Iter 35: observed MAE=0.009901 validation MAE=0.037718,rank=8\n",
      "[SoftImpute] Iter 36: observed MAE=0.009768 validation MAE=0.037671,rank=8\n",
      "[SoftImpute] Iter 37: observed MAE=0.009641 validation MAE=0.037624,rank=8\n",
      "[SoftImpute] Iter 38: observed MAE=0.009519 validation MAE=0.037576,rank=8\n",
      "[SoftImpute] Iter 39: observed MAE=0.009404 validation MAE=0.037531,rank=8\n",
      "[SoftImpute] Iter 40: observed MAE=0.009293 validation MAE=0.037489,rank=8\n",
      "[SoftImpute] Iter 41: observed MAE=0.009186 validation MAE=0.037448,rank=8\n",
      "[SoftImpute] Iter 42: observed MAE=0.009083 validation MAE=0.037406,rank=8\n",
      "[SoftImpute] Iter 43: observed MAE=0.008985 validation MAE=0.037365,rank=8\n",
      "[SoftImpute] Iter 44: observed MAE=0.008890 validation MAE=0.037324,rank=8\n",
      "[SoftImpute] Iter 45: observed MAE=0.008799 validation MAE=0.037284,rank=8\n",
      "[SoftImpute] Iter 46: observed MAE=0.008710 validation MAE=0.037242,rank=8\n",
      "[SoftImpute] Iter 47: observed MAE=0.008625 validation MAE=0.037203,rank=8\n",
      "[SoftImpute] Iter 48: observed MAE=0.008543 validation MAE=0.037164,rank=8\n",
      "[SoftImpute] Iter 49: observed MAE=0.008463 validation MAE=0.037125,rank=8\n",
      "[SoftImpute] Iter 50: observed MAE=0.008386 validation MAE=0.037087,rank=8\n",
      "[SoftImpute] Iter 51: observed MAE=0.008312 validation MAE=0.037049,rank=8\n",
      "[SoftImpute] Iter 52: observed MAE=0.008240 validation MAE=0.037010,rank=8\n",
      "[SoftImpute] Iter 53: observed MAE=0.008170 validation MAE=0.036971,rank=8\n",
      "[SoftImpute] Iter 54: observed MAE=0.008103 validation MAE=0.036931,rank=8\n",
      "[SoftImpute] Iter 55: observed MAE=0.008037 validation MAE=0.036892,rank=8\n",
      "[SoftImpute] Iter 56: observed MAE=0.007974 validation MAE=0.036852,rank=8\n",
      "[SoftImpute] Iter 57: observed MAE=0.007912 validation MAE=0.036812,rank=8\n",
      "[SoftImpute] Iter 58: observed MAE=0.007852 validation MAE=0.036773,rank=8\n",
      "[SoftImpute] Iter 59: observed MAE=0.007795 validation MAE=0.036733,rank=8\n",
      "[SoftImpute] Iter 60: observed MAE=0.007738 validation MAE=0.036694,rank=8\n",
      "[SoftImpute] Iter 61: observed MAE=0.007684 validation MAE=0.036655,rank=8\n",
      "[SoftImpute] Iter 62: observed MAE=0.007630 validation MAE=0.036616,rank=8\n",
      "[SoftImpute] Iter 63: observed MAE=0.007579 validation MAE=0.036578,rank=8\n",
      "[SoftImpute] Iter 64: observed MAE=0.007529 validation MAE=0.036541,rank=8\n",
      "[SoftImpute] Iter 65: observed MAE=0.007480 validation MAE=0.036505,rank=8\n",
      "[SoftImpute] Iter 66: observed MAE=0.007432 validation MAE=0.036468,rank=8\n",
      "[SoftImpute] Iter 67: observed MAE=0.007385 validation MAE=0.036431,rank=8\n",
      "[SoftImpute] Iter 68: observed MAE=0.007340 validation MAE=0.036394,rank=8\n",
      "[SoftImpute] Iter 69: observed MAE=0.007295 validation MAE=0.036356,rank=8\n",
      "[SoftImpute] Iter 70: observed MAE=0.007252 validation MAE=0.036319,rank=8\n",
      "[SoftImpute] Iter 71: observed MAE=0.007210 validation MAE=0.036282,rank=8\n",
      "[SoftImpute] Iter 72: observed MAE=0.007168 validation MAE=0.036246,rank=8\n",
      "[SoftImpute] Iter 73: observed MAE=0.007128 validation MAE=0.036211,rank=8\n",
      "[SoftImpute] Iter 74: observed MAE=0.007088 validation MAE=0.036176,rank=8\n",
      "[SoftImpute] Iter 75: observed MAE=0.007050 validation MAE=0.036141,rank=8\n",
      "[SoftImpute] Iter 76: observed MAE=0.007012 validation MAE=0.036106,rank=8\n",
      "[SoftImpute] Iter 77: observed MAE=0.006975 validation MAE=0.036071,rank=8\n",
      "[SoftImpute] Iter 78: observed MAE=0.006939 validation MAE=0.036036,rank=8\n",
      "[SoftImpute] Iter 79: observed MAE=0.006904 validation MAE=0.036001,rank=8\n",
      "[SoftImpute] Iter 80: observed MAE=0.006869 validation MAE=0.035966,rank=8\n",
      "[SoftImpute] Iter 81: observed MAE=0.006835 validation MAE=0.035931,rank=8\n",
      "[SoftImpute] Iter 82: observed MAE=0.006802 validation MAE=0.035895,rank=8\n",
      "[SoftImpute] Iter 83: observed MAE=0.006770 validation MAE=0.035860,rank=8\n",
      "[SoftImpute] Iter 84: observed MAE=0.006738 validation MAE=0.035826,rank=8\n",
      "[SoftImpute] Iter 85: observed MAE=0.006706 validation MAE=0.035791,rank=8\n",
      "[SoftImpute] Iter 86: observed MAE=0.006676 validation MAE=0.035757,rank=8\n",
      "[SoftImpute] Iter 87: observed MAE=0.006646 validation MAE=0.035722,rank=8\n",
      "[SoftImpute] Iter 88: observed MAE=0.006616 validation MAE=0.035688,rank=8\n",
      "[SoftImpute] Iter 89: observed MAE=0.006587 validation MAE=0.035654,rank=8\n",
      "[SoftImpute] Iter 90: observed MAE=0.006559 validation MAE=0.035620,rank=8\n",
      "[SoftImpute] Iter 91: observed MAE=0.006531 validation MAE=0.035587,rank=8\n",
      "[SoftImpute] Iter 92: observed MAE=0.006504 validation MAE=0.035554,rank=8\n",
      "[SoftImpute] Iter 93: observed MAE=0.006477 validation MAE=0.035521,rank=8\n",
      "[SoftImpute] Iter 94: observed MAE=0.006451 validation MAE=0.035487,rank=8\n",
      "[SoftImpute] Iter 95: observed MAE=0.006425 validation MAE=0.035454,rank=8\n",
      "[SoftImpute] Iter 96: observed MAE=0.006399 validation MAE=0.035421,rank=8\n",
      "[SoftImpute] Iter 97: observed MAE=0.006374 validation MAE=0.035390,rank=8\n",
      "[SoftImpute] Iter 98: observed MAE=0.006349 validation MAE=0.035358,rank=8\n",
      "[SoftImpute] Iter 99: observed MAE=0.006325 validation MAE=0.035327,rank=8\n",
      "[SoftImpute] Iter 100: observed MAE=0.006301 validation MAE=0.035297,rank=8\n",
      "[SoftImpute] Iter 101: observed MAE=0.006278 validation MAE=0.035267,rank=8\n",
      "[SoftImpute] Iter 102: observed MAE=0.006255 validation MAE=0.035238,rank=8\n",
      "[SoftImpute] Iter 103: observed MAE=0.006232 validation MAE=0.035208,rank=8\n",
      "[SoftImpute] Iter 104: observed MAE=0.006209 validation MAE=0.035179,rank=8\n",
      "[SoftImpute] Iter 105: observed MAE=0.006187 validation MAE=0.035151,rank=8\n",
      "[SoftImpute] Iter 106: observed MAE=0.006166 validation MAE=0.035122,rank=8\n",
      "[SoftImpute] Iter 107: observed MAE=0.006145 validation MAE=0.035094,rank=8\n",
      "[SoftImpute] Iter 108: observed MAE=0.006124 validation MAE=0.035066,rank=8\n",
      "[SoftImpute] Iter 109: observed MAE=0.006103 validation MAE=0.035038,rank=8\n",
      "[SoftImpute] Iter 110: observed MAE=0.006083 validation MAE=0.035010,rank=8\n",
      "[SoftImpute] Iter 111: observed MAE=0.006063 validation MAE=0.034982,rank=8\n",
      "[SoftImpute] Iter 112: observed MAE=0.006043 validation MAE=0.034954,rank=8\n",
      "[SoftImpute] Iter 113: observed MAE=0.006024 validation MAE=0.034927,rank=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 114: observed MAE=0.006005 validation MAE=0.034899,rank=8\n",
      "[SoftImpute] Iter 115: observed MAE=0.005986 validation MAE=0.034872,rank=8\n",
      "[SoftImpute] Iter 116: observed MAE=0.005968 validation MAE=0.034845,rank=8\n",
      "[SoftImpute] Iter 117: observed MAE=0.005949 validation MAE=0.034819,rank=8\n",
      "[SoftImpute] Iter 118: observed MAE=0.005931 validation MAE=0.034792,rank=8\n",
      "[SoftImpute] Iter 119: observed MAE=0.005914 validation MAE=0.034765,rank=8\n",
      "[SoftImpute] Iter 120: observed MAE=0.005896 validation MAE=0.034738,rank=8\n",
      "[SoftImpute] Iter 121: observed MAE=0.005879 validation MAE=0.034711,rank=8\n",
      "[SoftImpute] Iter 122: observed MAE=0.005862 validation MAE=0.034684,rank=8\n",
      "[SoftImpute] Iter 123: observed MAE=0.005845 validation MAE=0.034657,rank=8\n",
      "[SoftImpute] Iter 124: observed MAE=0.005829 validation MAE=0.034630,rank=8\n",
      "[SoftImpute] Iter 125: observed MAE=0.005812 validation MAE=0.034603,rank=8\n",
      "[SoftImpute] Iter 126: observed MAE=0.005796 validation MAE=0.034575,rank=8\n",
      "[SoftImpute] Iter 127: observed MAE=0.005780 validation MAE=0.034548,rank=8\n",
      "[SoftImpute] Iter 128: observed MAE=0.005765 validation MAE=0.034521,rank=8\n",
      "[SoftImpute] Iter 129: observed MAE=0.005749 validation MAE=0.034493,rank=8\n",
      "[SoftImpute] Iter 130: observed MAE=0.005734 validation MAE=0.034466,rank=8\n",
      "[SoftImpute] Iter 131: observed MAE=0.005719 validation MAE=0.034438,rank=8\n",
      "[SoftImpute] Iter 132: observed MAE=0.005704 validation MAE=0.034411,rank=8\n",
      "[SoftImpute] Iter 133: observed MAE=0.005689 validation MAE=0.034384,rank=8\n",
      "[SoftImpute] Iter 134: observed MAE=0.005675 validation MAE=0.034357,rank=8\n",
      "[SoftImpute] Iter 135: observed MAE=0.005661 validation MAE=0.034331,rank=8\n",
      "[SoftImpute] Iter 136: observed MAE=0.005646 validation MAE=0.034304,rank=8\n",
      "[SoftImpute] Iter 137: observed MAE=0.005633 validation MAE=0.034278,rank=8\n",
      "[SoftImpute] Iter 138: observed MAE=0.005619 validation MAE=0.034251,rank=8\n",
      "[SoftImpute] Iter 139: observed MAE=0.005605 validation MAE=0.034225,rank=8\n",
      "[SoftImpute] Iter 140: observed MAE=0.005592 validation MAE=0.034198,rank=8\n",
      "[SoftImpute] Iter 141: observed MAE=0.005579 validation MAE=0.034172,rank=8\n",
      "[SoftImpute] Iter 142: observed MAE=0.005565 validation MAE=0.034145,rank=8\n",
      "[SoftImpute] Iter 143: observed MAE=0.005552 validation MAE=0.034119,rank=8\n",
      "[SoftImpute] Iter 144: observed MAE=0.005540 validation MAE=0.034093,rank=8\n",
      "[SoftImpute] Iter 145: observed MAE=0.005527 validation MAE=0.034066,rank=8\n",
      "[SoftImpute] Iter 146: observed MAE=0.005515 validation MAE=0.034040,rank=8\n",
      "[SoftImpute] Iter 147: observed MAE=0.005502 validation MAE=0.034014,rank=8\n",
      "[SoftImpute] Iter 148: observed MAE=0.005490 validation MAE=0.033988,rank=8\n",
      "[SoftImpute] Iter 149: observed MAE=0.005478 validation MAE=0.033962,rank=8\n",
      "[SoftImpute] Iter 150: observed MAE=0.005467 validation MAE=0.033936,rank=8\n",
      "[SoftImpute] Iter 151: observed MAE=0.005455 validation MAE=0.033910,rank=8\n",
      "[SoftImpute] Iter 152: observed MAE=0.005443 validation MAE=0.033884,rank=8\n",
      "[SoftImpute] Iter 153: observed MAE=0.005432 validation MAE=0.033859,rank=8\n",
      "[SoftImpute] Iter 154: observed MAE=0.005420 validation MAE=0.033834,rank=8\n",
      "[SoftImpute] Iter 155: observed MAE=0.005409 validation MAE=0.033808,rank=8\n",
      "[SoftImpute] Iter 156: observed MAE=0.005398 validation MAE=0.033784,rank=8\n",
      "[SoftImpute] Iter 157: observed MAE=0.005387 validation MAE=0.033759,rank=8\n",
      "[SoftImpute] Iter 158: observed MAE=0.005376 validation MAE=0.033734,rank=8\n",
      "[SoftImpute] Iter 159: observed MAE=0.005366 validation MAE=0.033709,rank=8\n",
      "[SoftImpute] Iter 160: observed MAE=0.005355 validation MAE=0.033684,rank=8\n",
      "[SoftImpute] Iter 161: observed MAE=0.005345 validation MAE=0.033660,rank=8\n",
      "[SoftImpute] Iter 162: observed MAE=0.005334 validation MAE=0.033635,rank=8\n",
      "[SoftImpute] Iter 163: observed MAE=0.005324 validation MAE=0.033610,rank=8\n",
      "[SoftImpute] Iter 164: observed MAE=0.005314 validation MAE=0.033586,rank=8\n",
      "[SoftImpute] Iter 165: observed MAE=0.005304 validation MAE=0.033561,rank=8\n",
      "[SoftImpute] Iter 166: observed MAE=0.005294 validation MAE=0.033537,rank=8\n",
      "[SoftImpute] Iter 167: observed MAE=0.005284 validation MAE=0.033513,rank=8\n",
      "[SoftImpute] Iter 168: observed MAE=0.005275 validation MAE=0.033489,rank=8\n",
      "[SoftImpute] Iter 169: observed MAE=0.005265 validation MAE=0.033465,rank=8\n",
      "[SoftImpute] Iter 170: observed MAE=0.005256 validation MAE=0.033441,rank=8\n",
      "[SoftImpute] Iter 171: observed MAE=0.005246 validation MAE=0.033416,rank=8\n",
      "[SoftImpute] Iter 172: observed MAE=0.005237 validation MAE=0.033393,rank=8\n",
      "[SoftImpute] Iter 173: observed MAE=0.005228 validation MAE=0.033369,rank=8\n",
      "[SoftImpute] Iter 174: observed MAE=0.005219 validation MAE=0.033345,rank=8\n",
      "[SoftImpute] Iter 175: observed MAE=0.005210 validation MAE=0.033321,rank=8\n",
      "[SoftImpute] Iter 176: observed MAE=0.005201 validation MAE=0.033298,rank=8\n",
      "[SoftImpute] Iter 177: observed MAE=0.005192 validation MAE=0.033274,rank=8\n",
      "[SoftImpute] Iter 178: observed MAE=0.005184 validation MAE=0.033251,rank=8\n",
      "[SoftImpute] Iter 179: observed MAE=0.005175 validation MAE=0.033228,rank=8\n",
      "[SoftImpute] Iter 180: observed MAE=0.005167 validation MAE=0.033206,rank=8\n",
      "[SoftImpute] Iter 181: observed MAE=0.005158 validation MAE=0.033183,rank=8\n",
      "[SoftImpute] Iter 182: observed MAE=0.005150 validation MAE=0.033161,rank=8\n",
      "[SoftImpute] Iter 183: observed MAE=0.005142 validation MAE=0.033139,rank=8\n",
      "[SoftImpute] Iter 184: observed MAE=0.005134 validation MAE=0.033116,rank=8\n",
      "[SoftImpute] Iter 185: observed MAE=0.005126 validation MAE=0.033095,rank=8\n",
      "[SoftImpute] Iter 186: observed MAE=0.005118 validation MAE=0.033073,rank=8\n",
      "[SoftImpute] Iter 187: observed MAE=0.005110 validation MAE=0.033051,rank=8\n",
      "[SoftImpute] Iter 188: observed MAE=0.005103 validation MAE=0.033029,rank=8\n",
      "[SoftImpute] Iter 189: observed MAE=0.005095 validation MAE=0.033008,rank=8\n",
      "[SoftImpute] Iter 190: observed MAE=0.005088 validation MAE=0.032988,rank=8\n",
      "[SoftImpute] Iter 191: observed MAE=0.005080 validation MAE=0.032967,rank=8\n",
      "[SoftImpute] Iter 192: observed MAE=0.005073 validation MAE=0.032947,rank=8\n",
      "[SoftImpute] Iter 193: observed MAE=0.005066 validation MAE=0.032926,rank=8\n",
      "[SoftImpute] Iter 194: observed MAE=0.005058 validation MAE=0.032906,rank=8\n",
      "[SoftImpute] Iter 195: observed MAE=0.005051 validation MAE=0.032885,rank=8\n",
      "[SoftImpute] Iter 196: observed MAE=0.005044 validation MAE=0.032865,rank=8\n",
      "[SoftImpute] Iter 197: observed MAE=0.005037 validation MAE=0.032844,rank=8\n",
      "[SoftImpute] Iter 198: observed MAE=0.005030 validation MAE=0.032824,rank=8\n",
      "[SoftImpute] Iter 199: observed MAE=0.005023 validation MAE=0.032803,rank=8\n",
      "[SoftImpute] Iter 200: observed MAE=0.005016 validation MAE=0.032783,rank=8\n",
      "[SoftImpute] Iter 201: observed MAE=0.005010 validation MAE=0.032762,rank=8\n",
      "[SoftImpute] Iter 202: observed MAE=0.005003 validation MAE=0.032742,rank=8\n",
      "[SoftImpute] Iter 203: observed MAE=0.004996 validation MAE=0.032721,rank=8\n",
      "[SoftImpute] Iter 204: observed MAE=0.004990 validation MAE=0.032700,rank=8\n",
      "[SoftImpute] Iter 205: observed MAE=0.004983 validation MAE=0.032680,rank=8\n",
      "[SoftImpute] Iter 206: observed MAE=0.004977 validation MAE=0.032659,rank=8\n",
      "[SoftImpute] Iter 207: observed MAE=0.004970 validation MAE=0.032639,rank=8\n",
      "[SoftImpute] Iter 208: observed MAE=0.004964 validation MAE=0.032618,rank=8\n",
      "[SoftImpute] Iter 209: observed MAE=0.004958 validation MAE=0.032598,rank=8\n",
      "[SoftImpute] Iter 210: observed MAE=0.004951 validation MAE=0.032578,rank=8\n",
      "[SoftImpute] Iter 211: observed MAE=0.004945 validation MAE=0.032558,rank=8\n",
      "[SoftImpute] Iter 212: observed MAE=0.004939 validation MAE=0.032537,rank=8\n",
      "[SoftImpute] Iter 213: observed MAE=0.004933 validation MAE=0.032517,rank=8\n",
      "[SoftImpute] Iter 214: observed MAE=0.004927 validation MAE=0.032497,rank=8\n",
      "[SoftImpute] Iter 215: observed MAE=0.004921 validation MAE=0.032477,rank=8\n",
      "[SoftImpute] Iter 216: observed MAE=0.004915 validation MAE=0.032457,rank=8\n",
      "[SoftImpute] Iter 217: observed MAE=0.004909 validation MAE=0.032438,rank=8\n",
      "[SoftImpute] Iter 218: observed MAE=0.004903 validation MAE=0.032419,rank=8\n",
      "[SoftImpute] Iter 219: observed MAE=0.004898 validation MAE=0.032400,rank=8\n",
      "[SoftImpute] Iter 220: observed MAE=0.004892 validation MAE=0.032380,rank=8\n",
      "[SoftImpute] Iter 221: observed MAE=0.004886 validation MAE=0.032361,rank=8\n",
      "[SoftImpute] Iter 222: observed MAE=0.004881 validation MAE=0.032342,rank=8\n",
      "[SoftImpute] Iter 223: observed MAE=0.004875 validation MAE=0.032323,rank=8\n",
      "[SoftImpute] Iter 224: observed MAE=0.004869 validation MAE=0.032303,rank=8\n",
      "[SoftImpute] Iter 225: observed MAE=0.004864 validation MAE=0.032284,rank=8\n",
      "[SoftImpute] Iter 226: observed MAE=0.004859 validation MAE=0.032265,rank=8\n",
      "[SoftImpute] Iter 227: observed MAE=0.004853 validation MAE=0.032245,rank=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 228: observed MAE=0.004848 validation MAE=0.032226,rank=8\n",
      "[SoftImpute] Iter 229: observed MAE=0.004843 validation MAE=0.032207,rank=8\n",
      "[SoftImpute] Iter 230: observed MAE=0.004838 validation MAE=0.032189,rank=8\n",
      "[SoftImpute] Iter 231: observed MAE=0.004832 validation MAE=0.032170,rank=8\n",
      "[SoftImpute] Iter 232: observed MAE=0.004827 validation MAE=0.032151,rank=8\n",
      "[SoftImpute] Iter 233: observed MAE=0.004822 validation MAE=0.032132,rank=8\n",
      "[SoftImpute] Iter 234: observed MAE=0.004817 validation MAE=0.032113,rank=8\n",
      "[SoftImpute] Iter 235: observed MAE=0.004812 validation MAE=0.032094,rank=8\n",
      "[SoftImpute] Iter 236: observed MAE=0.004807 validation MAE=0.032076,rank=8\n",
      "[SoftImpute] Iter 237: observed MAE=0.004802 validation MAE=0.032057,rank=8\n",
      "[SoftImpute] Iter 238: observed MAE=0.004797 validation MAE=0.032038,rank=8\n",
      "[SoftImpute] Iter 239: observed MAE=0.004792 validation MAE=0.032019,rank=8\n",
      "[SoftImpute] Iter 240: observed MAE=0.004787 validation MAE=0.032000,rank=8\n",
      "[SoftImpute] Iter 241: observed MAE=0.004783 validation MAE=0.031981,rank=8\n",
      "[SoftImpute] Iter 242: observed MAE=0.004778 validation MAE=0.031962,rank=8\n",
      "[SoftImpute] Iter 243: observed MAE=0.004773 validation MAE=0.031943,rank=8\n",
      "[SoftImpute] Iter 244: observed MAE=0.004769 validation MAE=0.031924,rank=8\n",
      "[SoftImpute] Iter 245: observed MAE=0.004764 validation MAE=0.031906,rank=8\n",
      "[SoftImpute] Iter 246: observed MAE=0.004759 validation MAE=0.031887,rank=8\n",
      "[SoftImpute] Iter 247: observed MAE=0.004755 validation MAE=0.031868,rank=8\n",
      "[SoftImpute] Iter 248: observed MAE=0.004750 validation MAE=0.031849,rank=8\n",
      "[SoftImpute] Iter 249: observed MAE=0.004746 validation MAE=0.031830,rank=8\n",
      "[SoftImpute] Iter 250: observed MAE=0.004741 validation MAE=0.031812,rank=8\n",
      "[SoftImpute] Iter 251: observed MAE=0.004737 validation MAE=0.031794,rank=8\n",
      "[SoftImpute] Iter 252: observed MAE=0.004732 validation MAE=0.031776,rank=8\n",
      "[SoftImpute] Iter 253: observed MAE=0.004728 validation MAE=0.031758,rank=8\n",
      "[SoftImpute] Iter 254: observed MAE=0.004723 validation MAE=0.031741,rank=8\n",
      "[SoftImpute] Iter 255: observed MAE=0.004719 validation MAE=0.031723,rank=8\n",
      "[SoftImpute] Iter 256: observed MAE=0.004715 validation MAE=0.031705,rank=8\n",
      "[SoftImpute] Iter 257: observed MAE=0.004710 validation MAE=0.031687,rank=8\n",
      "[SoftImpute] Iter 258: observed MAE=0.004706 validation MAE=0.031670,rank=8\n",
      "[SoftImpute] Iter 259: observed MAE=0.004702 validation MAE=0.031652,rank=8\n",
      "[SoftImpute] Iter 260: observed MAE=0.004697 validation MAE=0.031634,rank=8\n",
      "[SoftImpute] Iter 261: observed MAE=0.004693 validation MAE=0.031616,rank=8\n",
      "[SoftImpute] Iter 262: observed MAE=0.004689 validation MAE=0.031598,rank=8\n",
      "[SoftImpute] Iter 263: observed MAE=0.004685 validation MAE=0.031581,rank=8\n",
      "[SoftImpute] Iter 264: observed MAE=0.004681 validation MAE=0.031563,rank=8\n",
      "[SoftImpute] Iter 265: observed MAE=0.004677 validation MAE=0.031545,rank=8\n",
      "[SoftImpute] Iter 266: observed MAE=0.004672 validation MAE=0.031527,rank=8\n",
      "[SoftImpute] Iter 267: observed MAE=0.004668 validation MAE=0.031509,rank=8\n",
      "[SoftImpute] Iter 268: observed MAE=0.004664 validation MAE=0.031492,rank=8\n",
      "[SoftImpute] Iter 269: observed MAE=0.004660 validation MAE=0.031475,rank=8\n",
      "[SoftImpute] Iter 270: observed MAE=0.004656 validation MAE=0.031457,rank=8\n",
      "[SoftImpute] Iter 271: observed MAE=0.004652 validation MAE=0.031440,rank=8\n",
      "[SoftImpute] Iter 272: observed MAE=0.004648 validation MAE=0.031423,rank=8\n",
      "[SoftImpute] Iter 273: observed MAE=0.004645 validation MAE=0.031406,rank=8\n",
      "[SoftImpute] Iter 274: observed MAE=0.004641 validation MAE=0.031388,rank=8\n",
      "[SoftImpute] Iter 275: observed MAE=0.004637 validation MAE=0.031371,rank=8\n",
      "[SoftImpute] Iter 276: observed MAE=0.004633 validation MAE=0.031354,rank=8\n",
      "[SoftImpute] Iter 277: observed MAE=0.004629 validation MAE=0.031337,rank=8\n",
      "[SoftImpute] Iter 278: observed MAE=0.004625 validation MAE=0.031320,rank=8\n",
      "[SoftImpute] Iter 279: observed MAE=0.004622 validation MAE=0.031303,rank=8\n",
      "[SoftImpute] Iter 280: observed MAE=0.004618 validation MAE=0.031287,rank=8\n",
      "[SoftImpute] Iter 281: observed MAE=0.004614 validation MAE=0.031270,rank=8\n",
      "[SoftImpute] Iter 282: observed MAE=0.004611 validation MAE=0.031253,rank=8\n",
      "[SoftImpute] Iter 283: observed MAE=0.004607 validation MAE=0.031236,rank=8\n",
      "[SoftImpute] Iter 284: observed MAE=0.004603 validation MAE=0.031219,rank=8\n",
      "[SoftImpute] Iter 285: observed MAE=0.004600 validation MAE=0.031202,rank=8\n",
      "[SoftImpute] Iter 286: observed MAE=0.004596 validation MAE=0.031186,rank=8\n",
      "[SoftImpute] Iter 287: observed MAE=0.004593 validation MAE=0.031169,rank=8\n",
      "[SoftImpute] Iter 288: observed MAE=0.004589 validation MAE=0.031152,rank=8\n",
      "[SoftImpute] Iter 289: observed MAE=0.004586 validation MAE=0.031135,rank=8\n",
      "[SoftImpute] Iter 290: observed MAE=0.004582 validation MAE=0.031119,rank=8\n",
      "[SoftImpute] Iter 291: observed MAE=0.004579 validation MAE=0.031102,rank=8\n",
      "[SoftImpute] Iter 292: observed MAE=0.004575 validation MAE=0.031085,rank=8\n",
      "[SoftImpute] Iter 293: observed MAE=0.004572 validation MAE=0.031069,rank=8\n",
      "[SoftImpute] Iter 294: observed MAE=0.004568 validation MAE=0.031052,rank=8\n",
      "[SoftImpute] Iter 295: observed MAE=0.004565 validation MAE=0.031036,rank=8\n",
      "[SoftImpute] Iter 296: observed MAE=0.004561 validation MAE=0.031019,rank=8\n",
      "[SoftImpute] Iter 297: observed MAE=0.004558 validation MAE=0.031003,rank=8\n",
      "[SoftImpute] Iter 298: observed MAE=0.004555 validation MAE=0.030986,rank=8\n",
      "[SoftImpute] Iter 299: observed MAE=0.004551 validation MAE=0.030970,rank=8\n",
      "[SoftImpute] Iter 300: observed MAE=0.004548 validation MAE=0.030953,rank=8\n",
      "[SoftImpute] Iter 301: observed MAE=0.004545 validation MAE=0.030936,rank=8\n",
      "[SoftImpute] Iter 302: observed MAE=0.004542 validation MAE=0.030920,rank=8\n",
      "[SoftImpute] Iter 303: observed MAE=0.004538 validation MAE=0.030903,rank=8\n",
      "[SoftImpute] Iter 304: observed MAE=0.004535 validation MAE=0.030887,rank=8\n",
      "[SoftImpute] Iter 305: observed MAE=0.004532 validation MAE=0.030871,rank=8\n",
      "[SoftImpute] Iter 306: observed MAE=0.004529 validation MAE=0.030854,rank=8\n",
      "[SoftImpute] Iter 307: observed MAE=0.004525 validation MAE=0.030838,rank=8\n",
      "[SoftImpute] Iter 308: observed MAE=0.004522 validation MAE=0.030822,rank=8\n",
      "[SoftImpute] Iter 309: observed MAE=0.004519 validation MAE=0.030806,rank=8\n",
      "[SoftImpute] Iter 310: observed MAE=0.004516 validation MAE=0.030790,rank=8\n",
      "[SoftImpute] Iter 311: observed MAE=0.004513 validation MAE=0.030774,rank=8\n",
      "[SoftImpute] Iter 312: observed MAE=0.004510 validation MAE=0.030759,rank=8\n",
      "[SoftImpute] Iter 313: observed MAE=0.004507 validation MAE=0.030743,rank=8\n",
      "[SoftImpute] Iter 314: observed MAE=0.004504 validation MAE=0.030727,rank=8\n",
      "[SoftImpute] Iter 315: observed MAE=0.004500 validation MAE=0.030711,rank=8\n",
      "[SoftImpute] Iter 316: observed MAE=0.004497 validation MAE=0.030696,rank=8\n",
      "[SoftImpute] Iter 317: observed MAE=0.004494 validation MAE=0.030680,rank=8\n",
      "[SoftImpute] Iter 318: observed MAE=0.004491 validation MAE=0.030664,rank=8\n",
      "[SoftImpute] Iter 319: observed MAE=0.004488 validation MAE=0.030649,rank=8\n",
      "[SoftImpute] Iter 320: observed MAE=0.004485 validation MAE=0.030633,rank=8\n",
      "[SoftImpute] Iter 321: observed MAE=0.004482 validation MAE=0.030618,rank=8\n",
      "[SoftImpute] Iter 322: observed MAE=0.004479 validation MAE=0.030602,rank=8\n",
      "[SoftImpute] Iter 323: observed MAE=0.004477 validation MAE=0.030586,rank=8\n",
      "[SoftImpute] Iter 324: observed MAE=0.004474 validation MAE=0.030571,rank=8\n",
      "[SoftImpute] Iter 325: observed MAE=0.004471 validation MAE=0.030555,rank=8\n",
      "[SoftImpute] Iter 326: observed MAE=0.004468 validation MAE=0.030540,rank=8\n",
      "[SoftImpute] Iter 327: observed MAE=0.004465 validation MAE=0.030524,rank=8\n",
      "[SoftImpute] Iter 328: observed MAE=0.004462 validation MAE=0.030508,rank=8\n",
      "[SoftImpute] Iter 329: observed MAE=0.004459 validation MAE=0.030493,rank=8\n",
      "[SoftImpute] Iter 330: observed MAE=0.004456 validation MAE=0.030477,rank=8\n",
      "[SoftImpute] Iter 331: observed MAE=0.004453 validation MAE=0.030461,rank=8\n",
      "[SoftImpute] Iter 332: observed MAE=0.004451 validation MAE=0.030446,rank=8\n",
      "[SoftImpute] Iter 333: observed MAE=0.004448 validation MAE=0.030430,rank=8\n",
      "[SoftImpute] Iter 334: observed MAE=0.004445 validation MAE=0.030414,rank=8\n",
      "[SoftImpute] Iter 335: observed MAE=0.004442 validation MAE=0.030399,rank=8\n",
      "[SoftImpute] Iter 336: observed MAE=0.004440 validation MAE=0.030383,rank=8\n",
      "[SoftImpute] Iter 337: observed MAE=0.004437 validation MAE=0.030368,rank=8\n",
      "[SoftImpute] Iter 338: observed MAE=0.004434 validation MAE=0.030353,rank=8\n",
      "[SoftImpute] Iter 339: observed MAE=0.004431 validation MAE=0.030338,rank=8\n",
      "[SoftImpute] Iter 340: observed MAE=0.004429 validation MAE=0.030323,rank=8\n",
      "[SoftImpute] Iter 341: observed MAE=0.004426 validation MAE=0.030308,rank=8\n",
      "[SoftImpute] Iter 342: observed MAE=0.004423 validation MAE=0.030293,rank=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 343: observed MAE=0.004421 validation MAE=0.030278,rank=8\n",
      "[SoftImpute] Iter 344: observed MAE=0.004418 validation MAE=0.030263,rank=8\n",
      "[SoftImpute] Iter 345: observed MAE=0.004415 validation MAE=0.030248,rank=8\n",
      "[SoftImpute] Iter 346: observed MAE=0.004413 validation MAE=0.030233,rank=8\n",
      "[SoftImpute] Iter 347: observed MAE=0.004410 validation MAE=0.030218,rank=8\n",
      "[SoftImpute] Iter 348: observed MAE=0.004407 validation MAE=0.030203,rank=8\n",
      "[SoftImpute] Iter 349: observed MAE=0.004405 validation MAE=0.030188,rank=8\n",
      "[SoftImpute] Iter 350: observed MAE=0.004402 validation MAE=0.030173,rank=8\n",
      "[SoftImpute] Iter 351: observed MAE=0.004400 validation MAE=0.030158,rank=8\n",
      "[SoftImpute] Iter 352: observed MAE=0.004397 validation MAE=0.030143,rank=8\n",
      "[SoftImpute] Iter 353: observed MAE=0.004395 validation MAE=0.030128,rank=8\n",
      "[SoftImpute] Iter 354: observed MAE=0.004392 validation MAE=0.030114,rank=8\n",
      "[SoftImpute] Iter 355: observed MAE=0.004389 validation MAE=0.030099,rank=8\n",
      "[SoftImpute] Iter 356: observed MAE=0.004387 validation MAE=0.030084,rank=8\n",
      "[SoftImpute] Iter 357: observed MAE=0.004384 validation MAE=0.030069,rank=8\n",
      "[SoftImpute] Iter 358: observed MAE=0.004382 validation MAE=0.030054,rank=8\n",
      "[SoftImpute] Iter 359: observed MAE=0.004379 validation MAE=0.030039,rank=8\n",
      "[SoftImpute] Iter 360: observed MAE=0.004377 validation MAE=0.030025,rank=8\n",
      "[SoftImpute] Iter 361: observed MAE=0.004374 validation MAE=0.030010,rank=8\n",
      "[SoftImpute] Iter 362: observed MAE=0.004372 validation MAE=0.029995,rank=8\n",
      "[SoftImpute] Iter 363: observed MAE=0.004370 validation MAE=0.029981,rank=8\n",
      "[SoftImpute] Iter 364: observed MAE=0.004367 validation MAE=0.029966,rank=8\n",
      "[SoftImpute] Iter 365: observed MAE=0.004365 validation MAE=0.029952,rank=8\n",
      "[SoftImpute] Iter 366: observed MAE=0.004362 validation MAE=0.029937,rank=8\n",
      "[SoftImpute] Iter 367: observed MAE=0.004360 validation MAE=0.029923,rank=8\n",
      "[SoftImpute] Iter 368: observed MAE=0.004357 validation MAE=0.029909,rank=8\n",
      "[SoftImpute] Iter 369: observed MAE=0.004355 validation MAE=0.029895,rank=8\n",
      "[SoftImpute] Iter 370: observed MAE=0.004353 validation MAE=0.029881,rank=8\n",
      "[SoftImpute] Iter 371: observed MAE=0.004350 validation MAE=0.029868,rank=8\n",
      "[SoftImpute] Iter 372: observed MAE=0.004348 validation MAE=0.029854,rank=8\n",
      "[SoftImpute] Iter 373: observed MAE=0.004346 validation MAE=0.029840,rank=8\n",
      "[SoftImpute] Iter 374: observed MAE=0.004343 validation MAE=0.029826,rank=8\n",
      "[SoftImpute] Iter 375: observed MAE=0.004341 validation MAE=0.029812,rank=8\n",
      "[SoftImpute] Iter 376: observed MAE=0.004339 validation MAE=0.029798,rank=8\n",
      "[SoftImpute] Iter 377: observed MAE=0.004336 validation MAE=0.029784,rank=8\n",
      "[SoftImpute] Iter 378: observed MAE=0.004334 validation MAE=0.029771,rank=8\n",
      "[SoftImpute] Iter 379: observed MAE=0.004332 validation MAE=0.029757,rank=8\n",
      "[SoftImpute] Iter 380: observed MAE=0.004329 validation MAE=0.029744,rank=8\n",
      "[SoftImpute] Iter 381: observed MAE=0.004327 validation MAE=0.029731,rank=8\n",
      "[SoftImpute] Iter 382: observed MAE=0.004325 validation MAE=0.029718,rank=8\n",
      "[SoftImpute] Iter 383: observed MAE=0.004323 validation MAE=0.029705,rank=8\n",
      "[SoftImpute] Iter 384: observed MAE=0.004320 validation MAE=0.029692,rank=8\n",
      "[SoftImpute] Iter 385: observed MAE=0.004318 validation MAE=0.029679,rank=8\n",
      "[SoftImpute] Iter 386: observed MAE=0.004316 validation MAE=0.029666,rank=8\n",
      "[SoftImpute] Iter 387: observed MAE=0.004314 validation MAE=0.029653,rank=8\n",
      "[SoftImpute] Iter 388: observed MAE=0.004311 validation MAE=0.029640,rank=8\n",
      "[SoftImpute] Iter 389: observed MAE=0.004309 validation MAE=0.029627,rank=8\n",
      "[SoftImpute] Iter 390: observed MAE=0.004307 validation MAE=0.029614,rank=8\n",
      "[SoftImpute] Iter 391: observed MAE=0.004305 validation MAE=0.029601,rank=8\n",
      "[SoftImpute] Iter 392: observed MAE=0.004303 validation MAE=0.029588,rank=8\n",
      "[SoftImpute] Iter 393: observed MAE=0.004300 validation MAE=0.029576,rank=8\n",
      "[SoftImpute] Iter 394: observed MAE=0.004298 validation MAE=0.029563,rank=8\n",
      "[SoftImpute] Iter 395: observed MAE=0.004296 validation MAE=0.029551,rank=8\n",
      "[SoftImpute] Iter 396: observed MAE=0.004294 validation MAE=0.029538,rank=8\n",
      "[SoftImpute] Iter 397: observed MAE=0.004292 validation MAE=0.029526,rank=8\n",
      "[SoftImpute] Iter 398: observed MAE=0.004290 validation MAE=0.029513,rank=8\n",
      "[SoftImpute] Iter 399: observed MAE=0.004288 validation MAE=0.029501,rank=8\n",
      "[SoftImpute] Iter 400: observed MAE=0.004285 validation MAE=0.029488,rank=8\n",
      "[SoftImpute] Iter 401: observed MAE=0.004283 validation MAE=0.029476,rank=8\n",
      "[SoftImpute] Iter 402: observed MAE=0.004281 validation MAE=0.029464,rank=8\n",
      "[SoftImpute] Iter 403: observed MAE=0.004279 validation MAE=0.029452,rank=8\n",
      "[SoftImpute] Iter 404: observed MAE=0.004277 validation MAE=0.029440,rank=8\n",
      "[SoftImpute] Iter 405: observed MAE=0.004275 validation MAE=0.029428,rank=8\n",
      "[SoftImpute] Iter 406: observed MAE=0.004273 validation MAE=0.029416,rank=8\n",
      "[SoftImpute] Iter 407: observed MAE=0.004271 validation MAE=0.029404,rank=8\n",
      "[SoftImpute] Iter 408: observed MAE=0.004269 validation MAE=0.029392,rank=8\n",
      "[SoftImpute] Iter 409: observed MAE=0.004267 validation MAE=0.029380,rank=8\n",
      "[SoftImpute] Iter 410: observed MAE=0.004264 validation MAE=0.029368,rank=8\n",
      "[SoftImpute] Iter 411: observed MAE=0.004262 validation MAE=0.029356,rank=8\n",
      "[SoftImpute] Iter 412: observed MAE=0.004260 validation MAE=0.029345,rank=8\n",
      "[SoftImpute] Iter 413: observed MAE=0.004258 validation MAE=0.029333,rank=8\n",
      "[SoftImpute] Iter 414: observed MAE=0.004256 validation MAE=0.029321,rank=8\n",
      "[SoftImpute] Iter 415: observed MAE=0.004254 validation MAE=0.029310,rank=8\n",
      "[SoftImpute] Iter 416: observed MAE=0.004252 validation MAE=0.029298,rank=8\n",
      "[SoftImpute] Iter 417: observed MAE=0.004250 validation MAE=0.029286,rank=8\n",
      "[SoftImpute] Iter 418: observed MAE=0.004248 validation MAE=0.029274,rank=8\n",
      "[SoftImpute] Iter 419: observed MAE=0.004246 validation MAE=0.029262,rank=8\n",
      "[SoftImpute] Iter 420: observed MAE=0.004244 validation MAE=0.029251,rank=8\n",
      "[SoftImpute] Iter 421: observed MAE=0.004242 validation MAE=0.029239,rank=8\n",
      "[SoftImpute] Iter 422: observed MAE=0.004240 validation MAE=0.029227,rank=8\n",
      "[SoftImpute] Iter 423: observed MAE=0.004238 validation MAE=0.029215,rank=8\n",
      "[SoftImpute] Iter 424: observed MAE=0.004236 validation MAE=0.029204,rank=8\n",
      "[SoftImpute] Iter 425: observed MAE=0.004234 validation MAE=0.029192,rank=8\n",
      "[SoftImpute] Stopped after iteration 425 for lambda=0.024644\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 12.246062278747559\n",
      "After the matrix factor stage, training error is 0.00423, validation error is 0.02919\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.30185, val loss: 0.30478\n",
      "Main effects training epoch: 2, train loss: 0.23228, val loss: 0.23617\n",
      "Main effects training epoch: 3, train loss: 0.17997, val loss: 0.18315\n",
      "Main effects training epoch: 4, train loss: 0.14845, val loss: 0.15215\n",
      "Main effects training epoch: 5, train loss: 0.13484, val loss: 0.13695\n",
      "Main effects training epoch: 6, train loss: 0.13033, val loss: 0.13155\n",
      "Main effects training epoch: 7, train loss: 0.13055, val loss: 0.13149\n",
      "Main effects training epoch: 8, train loss: 0.12971, val loss: 0.12988\n",
      "Main effects training epoch: 9, train loss: 0.12862, val loss: 0.12919\n",
      "Main effects training epoch: 10, train loss: 0.12828, val loss: 0.12938\n",
      "Main effects training epoch: 11, train loss: 0.12732, val loss: 0.12789\n",
      "Main effects training epoch: 12, train loss: 0.12659, val loss: 0.12743\n",
      "Main effects training epoch: 13, train loss: 0.12563, val loss: 0.12654\n",
      "Main effects training epoch: 14, train loss: 0.12325, val loss: 0.12456\n",
      "Main effects training epoch: 15, train loss: 0.11949, val loss: 0.12161\n",
      "Main effects training epoch: 16, train loss: 0.11686, val loss: 0.11922\n",
      "Main effects training epoch: 17, train loss: 0.11423, val loss: 0.11562\n",
      "Main effects training epoch: 18, train loss: 0.11484, val loss: 0.11687\n",
      "Main effects training epoch: 19, train loss: 0.11188, val loss: 0.11473\n",
      "Main effects training epoch: 20, train loss: 0.11089, val loss: 0.11324\n",
      "Main effects training epoch: 21, train loss: 0.10849, val loss: 0.11127\n",
      "Main effects training epoch: 22, train loss: 0.10778, val loss: 0.11002\n",
      "Main effects training epoch: 23, train loss: 0.10761, val loss: 0.10934\n",
      "Main effects training epoch: 24, train loss: 0.10695, val loss: 0.10962\n",
      "Main effects training epoch: 25, train loss: 0.10869, val loss: 0.11093\n",
      "Main effects training epoch: 26, train loss: 0.10721, val loss: 0.11016\n",
      "Main effects training epoch: 27, train loss: 0.10692, val loss: 0.10911\n",
      "Main effects training epoch: 28, train loss: 0.10656, val loss: 0.10921\n",
      "Main effects training epoch: 29, train loss: 0.10666, val loss: 0.10915\n",
      "Main effects training epoch: 30, train loss: 0.10688, val loss: 0.10958\n",
      "Main effects training epoch: 31, train loss: 0.10689, val loss: 0.10943\n",
      "Main effects training epoch: 32, train loss: 0.10622, val loss: 0.10918\n",
      "Main effects training epoch: 33, train loss: 0.10631, val loss: 0.10928\n",
      "Main effects training epoch: 34, train loss: 0.10605, val loss: 0.10868\n",
      "Main effects training epoch: 35, train loss: 0.10599, val loss: 0.10879\n",
      "Main effects training epoch: 36, train loss: 0.10605, val loss: 0.10911\n",
      "Main effects training epoch: 37, train loss: 0.10704, val loss: 0.10981\n",
      "Main effects training epoch: 38, train loss: 0.10655, val loss: 0.10926\n",
      "Main effects training epoch: 39, train loss: 0.10607, val loss: 0.10888\n",
      "Main effects training epoch: 40, train loss: 0.10588, val loss: 0.10859\n",
      "Main effects training epoch: 41, train loss: 0.10600, val loss: 0.10889\n",
      "Main effects training epoch: 42, train loss: 0.10616, val loss: 0.10903\n",
      "Main effects training epoch: 43, train loss: 0.10609, val loss: 0.10883\n",
      "Main effects training epoch: 44, train loss: 0.10630, val loss: 0.10899\n",
      "Main effects training epoch: 45, train loss: 0.10686, val loss: 0.10928\n",
      "Main effects training epoch: 46, train loss: 0.10599, val loss: 0.10904\n",
      "Main effects training epoch: 47, train loss: 0.10577, val loss: 0.10887\n",
      "Main effects training epoch: 48, train loss: 0.10584, val loss: 0.10888\n",
      "Main effects training epoch: 49, train loss: 0.10576, val loss: 0.10866\n",
      "Main effects training epoch: 50, train loss: 0.10617, val loss: 0.10903\n",
      "Main effects training epoch: 51, train loss: 0.10592, val loss: 0.10893\n",
      "Main effects training epoch: 52, train loss: 0.10606, val loss: 0.10892\n",
      "Main effects training epoch: 53, train loss: 0.10577, val loss: 0.10888\n",
      "Main effects training epoch: 54, train loss: 0.10586, val loss: 0.10875\n",
      "Main effects training epoch: 55, train loss: 0.10635, val loss: 0.10929\n",
      "Main effects training epoch: 56, train loss: 0.10600, val loss: 0.10895\n",
      "Main effects training epoch: 57, train loss: 0.10614, val loss: 0.10942\n",
      "Main effects training epoch: 58, train loss: 0.10591, val loss: 0.10905\n",
      "Main effects training epoch: 59, train loss: 0.10587, val loss: 0.10866\n",
      "Main effects training epoch: 60, train loss: 0.10580, val loss: 0.10885\n",
      "Main effects training epoch: 61, train loss: 0.10590, val loss: 0.10868\n",
      "Main effects training epoch: 62, train loss: 0.10611, val loss: 0.10911\n",
      "Main effects training epoch: 63, train loss: 0.10587, val loss: 0.10877\n",
      "Main effects training epoch: 64, train loss: 0.10611, val loss: 0.10908\n",
      "Main effects training epoch: 65, train loss: 0.10597, val loss: 0.10888\n",
      "Main effects training epoch: 66, train loss: 0.10579, val loss: 0.10886\n",
      "Main effects training epoch: 67, train loss: 0.10588, val loss: 0.10853\n",
      "Main effects training epoch: 68, train loss: 0.10601, val loss: 0.10926\n",
      "Main effects training epoch: 69, train loss: 0.10583, val loss: 0.10866\n",
      "Main effects training epoch: 70, train loss: 0.10585, val loss: 0.10893\n",
      "Main effects training epoch: 71, train loss: 0.10592, val loss: 0.10879\n",
      "Main effects training epoch: 72, train loss: 0.10582, val loss: 0.10901\n",
      "Main effects training epoch: 73, train loss: 0.10586, val loss: 0.10890\n",
      "Main effects training epoch: 74, train loss: 0.10596, val loss: 0.10893\n",
      "Main effects training epoch: 75, train loss: 0.10614, val loss: 0.10884\n",
      "Main effects training epoch: 76, train loss: 0.10583, val loss: 0.10904\n",
      "Main effects training epoch: 77, train loss: 0.10592, val loss: 0.10890\n",
      "Main effects training epoch: 78, train loss: 0.10580, val loss: 0.10881\n",
      "Main effects training epoch: 79, train loss: 0.10594, val loss: 0.10889\n",
      "Main effects training epoch: 80, train loss: 0.10607, val loss: 0.10888\n",
      "Main effects training epoch: 81, train loss: 0.10581, val loss: 0.10891\n",
      "Main effects training epoch: 82, train loss: 0.10593, val loss: 0.10922\n",
      "Main effects training epoch: 83, train loss: 0.10574, val loss: 0.10873\n",
      "Main effects training epoch: 84, train loss: 0.10573, val loss: 0.10877\n",
      "Main effects training epoch: 85, train loss: 0.10636, val loss: 0.10929\n",
      "Main effects training epoch: 86, train loss: 0.10583, val loss: 0.10874\n",
      "Main effects training epoch: 87, train loss: 0.10614, val loss: 0.10928\n",
      "Main effects training epoch: 88, train loss: 0.10616, val loss: 0.10949\n",
      "Main effects training epoch: 89, train loss: 0.10589, val loss: 0.10865\n",
      "Main effects training epoch: 90, train loss: 0.10604, val loss: 0.10903\n",
      "Main effects training epoch: 91, train loss: 0.10588, val loss: 0.10892\n",
      "Main effects training epoch: 92, train loss: 0.10612, val loss: 0.10925\n",
      "Main effects training epoch: 93, train loss: 0.10583, val loss: 0.10911\n",
      "Main effects training epoch: 94, train loss: 0.10604, val loss: 0.10904\n",
      "Main effects training epoch: 95, train loss: 0.10576, val loss: 0.10903\n",
      "Main effects training epoch: 96, train loss: 0.10568, val loss: 0.10869\n",
      "Main effects training epoch: 97, train loss: 0.10588, val loss: 0.10926\n",
      "Main effects training epoch: 98, train loss: 0.10606, val loss: 0.10910\n",
      "Main effects training epoch: 99, train loss: 0.10649, val loss: 0.10930\n",
      "Main effects training epoch: 100, train loss: 0.10628, val loss: 0.10948\n",
      "Main effects training epoch: 101, train loss: 0.10586, val loss: 0.10912\n",
      "Main effects training epoch: 102, train loss: 0.10570, val loss: 0.10869\n",
      "Main effects training epoch: 103, train loss: 0.10643, val loss: 0.10951\n",
      "Main effects training epoch: 104, train loss: 0.10650, val loss: 0.10946\n",
      "Main effects training epoch: 105, train loss: 0.10626, val loss: 0.10954\n",
      "Main effects training epoch: 106, train loss: 0.10594, val loss: 0.10896\n",
      "Main effects training epoch: 107, train loss: 0.10579, val loss: 0.10878\n",
      "Main effects training epoch: 108, train loss: 0.10607, val loss: 0.10922\n",
      "Main effects training epoch: 109, train loss: 0.10581, val loss: 0.10902\n",
      "Main effects training epoch: 110, train loss: 0.10590, val loss: 0.10892\n",
      "Main effects training epoch: 111, train loss: 0.10605, val loss: 0.10924\n",
      "Main effects training epoch: 112, train loss: 0.10580, val loss: 0.10875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 113, train loss: 0.10587, val loss: 0.10900\n",
      "Main effects training epoch: 114, train loss: 0.10581, val loss: 0.10883\n",
      "Main effects training epoch: 115, train loss: 0.10618, val loss: 0.10915\n",
      "Main effects training epoch: 116, train loss: 0.10615, val loss: 0.10949\n",
      "Main effects training epoch: 117, train loss: 0.10602, val loss: 0.10894\n",
      "Main effects training epoch: 118, train loss: 0.10595, val loss: 0.10914\n",
      "Main effects training epoch: 119, train loss: 0.10618, val loss: 0.10895\n",
      "Main effects training epoch: 120, train loss: 0.10601, val loss: 0.10941\n",
      "Main effects training epoch: 121, train loss: 0.10588, val loss: 0.10869\n",
      "Main effects training epoch: 122, train loss: 0.10576, val loss: 0.10885\n",
      "Main effects training epoch: 123, train loss: 0.10578, val loss: 0.10899\n",
      "Main effects training epoch: 124, train loss: 0.10583, val loss: 0.10934\n",
      "Main effects training epoch: 125, train loss: 0.10610, val loss: 0.10904\n",
      "Main effects training epoch: 126, train loss: 0.10593, val loss: 0.10917\n",
      "Main effects training epoch: 127, train loss: 0.10579, val loss: 0.10863\n",
      "Main effects training epoch: 128, train loss: 0.10583, val loss: 0.10933\n",
      "Main effects training epoch: 129, train loss: 0.10580, val loss: 0.10919\n",
      "Main effects training epoch: 130, train loss: 0.10573, val loss: 0.10874\n",
      "Main effects training epoch: 131, train loss: 0.10601, val loss: 0.10951\n",
      "Main effects training epoch: 132, train loss: 0.10600, val loss: 0.10905\n",
      "Main effects training epoch: 133, train loss: 0.10601, val loss: 0.10931\n",
      "Main effects training epoch: 134, train loss: 0.10566, val loss: 0.10882\n",
      "Main effects training epoch: 135, train loss: 0.10590, val loss: 0.10909\n",
      "Main effects training epoch: 136, train loss: 0.10621, val loss: 0.10930\n",
      "Main effects training epoch: 137, train loss: 0.10597, val loss: 0.10906\n",
      "Main effects training epoch: 138, train loss: 0.10583, val loss: 0.10895\n",
      "Main effects training epoch: 139, train loss: 0.10580, val loss: 0.10894\n",
      "Main effects training epoch: 140, train loss: 0.10591, val loss: 0.10891\n",
      "Main effects training epoch: 141, train loss: 0.10595, val loss: 0.10903\n",
      "Main effects training epoch: 142, train loss: 0.10601, val loss: 0.10932\n",
      "Main effects training epoch: 143, train loss: 0.10596, val loss: 0.10892\n",
      "Main effects training epoch: 144, train loss: 0.10583, val loss: 0.10925\n",
      "Main effects training epoch: 145, train loss: 0.10572, val loss: 0.10887\n",
      "Main effects training epoch: 146, train loss: 0.10574, val loss: 0.10897\n",
      "Main effects training epoch: 147, train loss: 0.10575, val loss: 0.10903\n",
      "Main effects training epoch: 148, train loss: 0.10589, val loss: 0.10894\n",
      "Main effects training epoch: 149, train loss: 0.10577, val loss: 0.10889\n",
      "Main effects training epoch: 150, train loss: 0.10569, val loss: 0.10895\n",
      "Main effects training epoch: 151, train loss: 0.10575, val loss: 0.10884\n",
      "Main effects training epoch: 152, train loss: 0.10606, val loss: 0.10908\n",
      "Main effects training epoch: 153, train loss: 0.10639, val loss: 0.10965\n",
      "Main effects training epoch: 154, train loss: 0.10596, val loss: 0.10920\n",
      "Main effects training epoch: 155, train loss: 0.10578, val loss: 0.10908\n",
      "Main effects training epoch: 156, train loss: 0.10569, val loss: 0.10881\n",
      "Main effects training epoch: 157, train loss: 0.10570, val loss: 0.10898\n",
      "Main effects training epoch: 158, train loss: 0.10595, val loss: 0.10931\n",
      "Main effects training epoch: 159, train loss: 0.10577, val loss: 0.10920\n",
      "Main effects training epoch: 160, train loss: 0.10578, val loss: 0.10935\n",
      "Main effects training epoch: 161, train loss: 0.10579, val loss: 0.10872\n",
      "Main effects training epoch: 162, train loss: 0.10575, val loss: 0.10904\n",
      "Main effects training epoch: 163, train loss: 0.10597, val loss: 0.10913\n",
      "Main effects training epoch: 164, train loss: 0.10605, val loss: 0.10923\n",
      "Main effects training epoch: 165, train loss: 0.10604, val loss: 0.10954\n",
      "Main effects training epoch: 166, train loss: 0.10575, val loss: 0.10883\n",
      "Main effects training epoch: 167, train loss: 0.10578, val loss: 0.10918\n",
      "Main effects training epoch: 168, train loss: 0.10576, val loss: 0.10888\n",
      "Early stop at epoch 168, with validation loss: 0.10888\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.10587, val loss: 0.10915\n",
      "Main effects tuning epoch: 2, train loss: 0.10583, val loss: 0.10863\n",
      "Main effects tuning epoch: 3, train loss: 0.10593, val loss: 0.10918\n",
      "Main effects tuning epoch: 4, train loss: 0.10580, val loss: 0.10863\n",
      "Main effects tuning epoch: 5, train loss: 0.10595, val loss: 0.10896\n",
      "Main effects tuning epoch: 6, train loss: 0.10614, val loss: 0.10907\n",
      "Main effects tuning epoch: 7, train loss: 0.10617, val loss: 0.10891\n",
      "Main effects tuning epoch: 8, train loss: 0.10639, val loss: 0.10952\n",
      "Main effects tuning epoch: 9, train loss: 0.10631, val loss: 0.10952\n",
      "Main effects tuning epoch: 10, train loss: 0.10598, val loss: 0.10913\n",
      "Main effects tuning epoch: 11, train loss: 0.10616, val loss: 0.10872\n",
      "Main effects tuning epoch: 12, train loss: 0.10613, val loss: 0.10940\n",
      "Main effects tuning epoch: 13, train loss: 0.10649, val loss: 0.10913\n",
      "Main effects tuning epoch: 14, train loss: 0.10595, val loss: 0.10906\n",
      "Main effects tuning epoch: 15, train loss: 0.10576, val loss: 0.10901\n",
      "Main effects tuning epoch: 16, train loss: 0.10591, val loss: 0.10867\n",
      "Main effects tuning epoch: 17, train loss: 0.10614, val loss: 0.10929\n",
      "Main effects tuning epoch: 18, train loss: 0.10616, val loss: 0.10914\n",
      "Main effects tuning epoch: 19, train loss: 0.10591, val loss: 0.10901\n",
      "Main effects tuning epoch: 20, train loss: 0.10585, val loss: 0.10893\n",
      "Main effects tuning epoch: 21, train loss: 0.10582, val loss: 0.10851\n",
      "Main effects tuning epoch: 22, train loss: 0.10595, val loss: 0.10917\n",
      "Main effects tuning epoch: 23, train loss: 0.10589, val loss: 0.10893\n",
      "Main effects tuning epoch: 24, train loss: 0.10579, val loss: 0.10899\n",
      "Main effects tuning epoch: 25, train loss: 0.10580, val loss: 0.10907\n",
      "Main effects tuning epoch: 26, train loss: 0.10612, val loss: 0.10929\n",
      "Main effects tuning epoch: 27, train loss: 0.10613, val loss: 0.10892\n",
      "Main effects tuning epoch: 28, train loss: 0.10608, val loss: 0.10898\n",
      "Main effects tuning epoch: 29, train loss: 0.10588, val loss: 0.10904\n",
      "Main effects tuning epoch: 30, train loss: 0.10603, val loss: 0.10875\n",
      "Main effects tuning epoch: 31, train loss: 0.10580, val loss: 0.10885\n",
      "Main effects tuning epoch: 32, train loss: 0.10586, val loss: 0.10892\n",
      "Main effects tuning epoch: 33, train loss: 0.10584, val loss: 0.10877\n",
      "Main effects tuning epoch: 34, train loss: 0.10610, val loss: 0.10890\n",
      "Main effects tuning epoch: 35, train loss: 0.10602, val loss: 0.10950\n",
      "Main effects tuning epoch: 36, train loss: 0.10577, val loss: 0.10870\n",
      "Main effects tuning epoch: 37, train loss: 0.10576, val loss: 0.10890\n",
      "Main effects tuning epoch: 38, train loss: 0.10590, val loss: 0.10894\n",
      "Main effects tuning epoch: 39, train loss: 0.10597, val loss: 0.10915\n",
      "Main effects tuning epoch: 40, train loss: 0.10575, val loss: 0.10856\n",
      "Main effects tuning epoch: 41, train loss: 0.10577, val loss: 0.10899\n",
      "Main effects tuning epoch: 42, train loss: 0.10600, val loss: 0.10888\n",
      "Main effects tuning epoch: 43, train loss: 0.10624, val loss: 0.10938\n",
      "Main effects tuning epoch: 44, train loss: 0.10606, val loss: 0.10863\n",
      "Main effects tuning epoch: 45, train loss: 0.10614, val loss: 0.10961\n",
      "Main effects tuning epoch: 46, train loss: 0.10598, val loss: 0.10874\n",
      "Main effects tuning epoch: 47, train loss: 0.10623, val loss: 0.10916\n",
      "Main effects tuning epoch: 48, train loss: 0.10628, val loss: 0.10910\n",
      "Main effects tuning epoch: 49, train loss: 0.10612, val loss: 0.10902\n",
      "Main effects tuning epoch: 50, train loss: 0.10600, val loss: 0.10922\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.16656, val loss: 0.16504\n",
      "Interaction training epoch: 2, train loss: 0.19799, val loss: 0.19730\n",
      "Interaction training epoch: 3, train loss: 0.07919, val loss: 0.08135\n",
      "Interaction training epoch: 4, train loss: 0.06592, val loss: 0.06695\n",
      "Interaction training epoch: 5, train loss: 0.06719, val loss: 0.06623\n",
      "Interaction training epoch: 6, train loss: 0.07116, val loss: 0.06992\n",
      "Interaction training epoch: 7, train loss: 0.05765, val loss: 0.05869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 8, train loss: 0.05791, val loss: 0.05822\n",
      "Interaction training epoch: 9, train loss: 0.06091, val loss: 0.06046\n",
      "Interaction training epoch: 10, train loss: 0.05754, val loss: 0.05788\n",
      "Interaction training epoch: 11, train loss: 0.05624, val loss: 0.05624\n",
      "Interaction training epoch: 12, train loss: 0.05651, val loss: 0.05600\n",
      "Interaction training epoch: 13, train loss: 0.06101, val loss: 0.06080\n",
      "Interaction training epoch: 14, train loss: 0.05709, val loss: 0.05641\n",
      "Interaction training epoch: 15, train loss: 0.06077, val loss: 0.06101\n",
      "Interaction training epoch: 16, train loss: 0.05239, val loss: 0.05197\n",
      "Interaction training epoch: 17, train loss: 0.05618, val loss: 0.05592\n",
      "Interaction training epoch: 18, train loss: 0.05477, val loss: 0.05468\n",
      "Interaction training epoch: 19, train loss: 0.05152, val loss: 0.05113\n",
      "Interaction training epoch: 20, train loss: 0.05519, val loss: 0.05482\n",
      "Interaction training epoch: 21, train loss: 0.05818, val loss: 0.05633\n",
      "Interaction training epoch: 22, train loss: 0.05614, val loss: 0.05550\n",
      "Interaction training epoch: 23, train loss: 0.05384, val loss: 0.05252\n",
      "Interaction training epoch: 24, train loss: 0.05143, val loss: 0.05136\n",
      "Interaction training epoch: 25, train loss: 0.05185, val loss: 0.05118\n",
      "Interaction training epoch: 26, train loss: 0.05409, val loss: 0.05515\n",
      "Interaction training epoch: 27, train loss: 0.05171, val loss: 0.05189\n",
      "Interaction training epoch: 28, train loss: 0.05284, val loss: 0.05307\n",
      "Interaction training epoch: 29, train loss: 0.05251, val loss: 0.05306\n",
      "Interaction training epoch: 30, train loss: 0.05654, val loss: 0.05667\n",
      "Interaction training epoch: 31, train loss: 0.05008, val loss: 0.05013\n",
      "Interaction training epoch: 32, train loss: 0.05165, val loss: 0.05175\n",
      "Interaction training epoch: 33, train loss: 0.05094, val loss: 0.05147\n",
      "Interaction training epoch: 34, train loss: 0.05154, val loss: 0.05255\n",
      "Interaction training epoch: 35, train loss: 0.05178, val loss: 0.05163\n",
      "Interaction training epoch: 36, train loss: 0.05189, val loss: 0.05137\n",
      "Interaction training epoch: 37, train loss: 0.05261, val loss: 0.05262\n",
      "Interaction training epoch: 38, train loss: 0.05395, val loss: 0.05444\n",
      "Interaction training epoch: 39, train loss: 0.05053, val loss: 0.05093\n",
      "Interaction training epoch: 40, train loss: 0.05186, val loss: 0.05165\n",
      "Interaction training epoch: 41, train loss: 0.05174, val loss: 0.05183\n",
      "Interaction training epoch: 42, train loss: 0.05645, val loss: 0.05707\n",
      "Interaction training epoch: 43, train loss: 0.05130, val loss: 0.05143\n",
      "Interaction training epoch: 44, train loss: 0.05233, val loss: 0.05213\n",
      "Interaction training epoch: 45, train loss: 0.05242, val loss: 0.05266\n",
      "Interaction training epoch: 46, train loss: 0.05234, val loss: 0.05256\n",
      "Interaction training epoch: 47, train loss: 0.05096, val loss: 0.05121\n",
      "Interaction training epoch: 48, train loss: 0.05324, val loss: 0.05317\n",
      "Interaction training epoch: 49, train loss: 0.05143, val loss: 0.05175\n",
      "Interaction training epoch: 50, train loss: 0.05906, val loss: 0.05866\n",
      "Interaction training epoch: 51, train loss: 0.05278, val loss: 0.05220\n",
      "Interaction training epoch: 52, train loss: 0.05429, val loss: 0.05321\n",
      "Interaction training epoch: 53, train loss: 0.05247, val loss: 0.05225\n",
      "Interaction training epoch: 54, train loss: 0.05061, val loss: 0.05129\n",
      "Interaction training epoch: 55, train loss: 0.05239, val loss: 0.05237\n",
      "Interaction training epoch: 56, train loss: 0.05174, val loss: 0.05287\n",
      "Interaction training epoch: 57, train loss: 0.05160, val loss: 0.05111\n",
      "Interaction training epoch: 58, train loss: 0.05180, val loss: 0.05292\n",
      "Interaction training epoch: 59, train loss: 0.04982, val loss: 0.05009\n",
      "Interaction training epoch: 60, train loss: 0.05227, val loss: 0.05368\n",
      "Interaction training epoch: 61, train loss: 0.05217, val loss: 0.05210\n",
      "Interaction training epoch: 62, train loss: 0.05023, val loss: 0.05074\n",
      "Interaction training epoch: 63, train loss: 0.05179, val loss: 0.05284\n",
      "Interaction training epoch: 64, train loss: 0.04959, val loss: 0.05129\n",
      "Interaction training epoch: 65, train loss: 0.05320, val loss: 0.05225\n",
      "Interaction training epoch: 66, train loss: 0.05295, val loss: 0.05326\n",
      "Interaction training epoch: 67, train loss: 0.05028, val loss: 0.05144\n",
      "Interaction training epoch: 68, train loss: 0.05112, val loss: 0.05151\n",
      "Interaction training epoch: 69, train loss: 0.05228, val loss: 0.05248\n",
      "Interaction training epoch: 70, train loss: 0.04994, val loss: 0.05062\n",
      "Interaction training epoch: 71, train loss: 0.05225, val loss: 0.05286\n",
      "Interaction training epoch: 72, train loss: 0.05069, val loss: 0.05180\n",
      "Interaction training epoch: 73, train loss: 0.05077, val loss: 0.05196\n",
      "Interaction training epoch: 74, train loss: 0.04967, val loss: 0.05076\n",
      "Interaction training epoch: 75, train loss: 0.05064, val loss: 0.05071\n",
      "Interaction training epoch: 76, train loss: 0.04886, val loss: 0.05004\n",
      "Interaction training epoch: 77, train loss: 0.05097, val loss: 0.05075\n",
      "Interaction training epoch: 78, train loss: 0.05171, val loss: 0.05128\n",
      "Interaction training epoch: 79, train loss: 0.05178, val loss: 0.05168\n",
      "Interaction training epoch: 80, train loss: 0.04972, val loss: 0.05111\n",
      "Interaction training epoch: 81, train loss: 0.05307, val loss: 0.05407\n",
      "Interaction training epoch: 82, train loss: 0.05287, val loss: 0.05331\n",
      "Interaction training epoch: 83, train loss: 0.05019, val loss: 0.05065\n",
      "Interaction training epoch: 84, train loss: 0.05109, val loss: 0.05177\n",
      "Interaction training epoch: 85, train loss: 0.05241, val loss: 0.05395\n",
      "Interaction training epoch: 86, train loss: 0.05169, val loss: 0.05187\n",
      "Interaction training epoch: 87, train loss: 0.05013, val loss: 0.05031\n",
      "Interaction training epoch: 88, train loss: 0.05249, val loss: 0.05324\n",
      "Interaction training epoch: 89, train loss: 0.04905, val loss: 0.04955\n",
      "Interaction training epoch: 90, train loss: 0.05048, val loss: 0.05104\n",
      "Interaction training epoch: 91, train loss: 0.05291, val loss: 0.05320\n",
      "Interaction training epoch: 92, train loss: 0.05194, val loss: 0.05184\n",
      "Interaction training epoch: 93, train loss: 0.04813, val loss: 0.04857\n",
      "Interaction training epoch: 94, train loss: 0.05434, val loss: 0.05466\n",
      "Interaction training epoch: 95, train loss: 0.05394, val loss: 0.05306\n",
      "Interaction training epoch: 96, train loss: 0.04954, val loss: 0.04995\n",
      "Interaction training epoch: 97, train loss: 0.05003, val loss: 0.05058\n",
      "Interaction training epoch: 98, train loss: 0.05197, val loss: 0.05219\n",
      "Interaction training epoch: 99, train loss: 0.05436, val loss: 0.05280\n",
      "Interaction training epoch: 100, train loss: 0.05350, val loss: 0.05437\n",
      "Interaction training epoch: 101, train loss: 0.04964, val loss: 0.04980\n",
      "Interaction training epoch: 102, train loss: 0.05136, val loss: 0.05087\n",
      "Interaction training epoch: 103, train loss: 0.05162, val loss: 0.05191\n",
      "Interaction training epoch: 104, train loss: 0.05118, val loss: 0.05150\n",
      "Interaction training epoch: 105, train loss: 0.04993, val loss: 0.04937\n",
      "Interaction training epoch: 106, train loss: 0.05171, val loss: 0.05164\n",
      "Interaction training epoch: 107, train loss: 0.05193, val loss: 0.05075\n",
      "Interaction training epoch: 108, train loss: 0.05102, val loss: 0.05183\n",
      "Interaction training epoch: 109, train loss: 0.05261, val loss: 0.05324\n",
      "Interaction training epoch: 110, train loss: 0.05047, val loss: 0.05100\n",
      "Interaction training epoch: 111, train loss: 0.05046, val loss: 0.05063\n",
      "Interaction training epoch: 112, train loss: 0.04811, val loss: 0.04809\n",
      "Interaction training epoch: 113, train loss: 0.05250, val loss: 0.05298\n",
      "Interaction training epoch: 114, train loss: 0.05036, val loss: 0.05112\n",
      "Interaction training epoch: 115, train loss: 0.05119, val loss: 0.05140\n",
      "Interaction training epoch: 116, train loss: 0.04975, val loss: 0.05112\n",
      "Interaction training epoch: 117, train loss: 0.04911, val loss: 0.04994\n",
      "Interaction training epoch: 118, train loss: 0.04990, val loss: 0.05012\n",
      "Interaction training epoch: 119, train loss: 0.04854, val loss: 0.04775\n",
      "Interaction training epoch: 120, train loss: 0.04985, val loss: 0.04987\n",
      "Interaction training epoch: 121, train loss: 0.04779, val loss: 0.04856\n",
      "Interaction training epoch: 122, train loss: 0.04712, val loss: 0.04773\n",
      "Interaction training epoch: 123, train loss: 0.05164, val loss: 0.05255\n",
      "Interaction training epoch: 124, train loss: 0.05327, val loss: 0.05365\n",
      "Interaction training epoch: 125, train loss: 0.05348, val loss: 0.05414\n",
      "Interaction training epoch: 126, train loss: 0.04873, val loss: 0.04978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 127, train loss: 0.04866, val loss: 0.04894\n",
      "Interaction training epoch: 128, train loss: 0.04947, val loss: 0.05037\n",
      "Interaction training epoch: 129, train loss: 0.04701, val loss: 0.04711\n",
      "Interaction training epoch: 130, train loss: 0.04867, val loss: 0.04878\n",
      "Interaction training epoch: 131, train loss: 0.04995, val loss: 0.05004\n",
      "Interaction training epoch: 132, train loss: 0.05004, val loss: 0.05008\n",
      "Interaction training epoch: 133, train loss: 0.04970, val loss: 0.05023\n",
      "Interaction training epoch: 134, train loss: 0.04969, val loss: 0.05041\n",
      "Interaction training epoch: 135, train loss: 0.04826, val loss: 0.04935\n",
      "Interaction training epoch: 136, train loss: 0.04956, val loss: 0.05077\n",
      "Interaction training epoch: 137, train loss: 0.04937, val loss: 0.05116\n",
      "Interaction training epoch: 138, train loss: 0.05042, val loss: 0.05126\n",
      "Interaction training epoch: 139, train loss: 0.04910, val loss: 0.04944\n",
      "Interaction training epoch: 140, train loss: 0.05235, val loss: 0.05329\n",
      "Interaction training epoch: 141, train loss: 0.05151, val loss: 0.05199\n",
      "Interaction training epoch: 142, train loss: 0.04935, val loss: 0.04965\n",
      "Interaction training epoch: 143, train loss: 0.05663, val loss: 0.05667\n",
      "Interaction training epoch: 144, train loss: 0.05217, val loss: 0.05297\n",
      "Interaction training epoch: 145, train loss: 0.05139, val loss: 0.05143\n",
      "Interaction training epoch: 146, train loss: 0.05350, val loss: 0.05388\n",
      "Interaction training epoch: 147, train loss: 0.06195, val loss: 0.06171\n",
      "Interaction training epoch: 148, train loss: 0.05923, val loss: 0.05958\n",
      "Interaction training epoch: 149, train loss: 0.05897, val loss: 0.05842\n",
      "Interaction training epoch: 150, train loss: 0.05688, val loss: 0.05663\n",
      "Interaction training epoch: 151, train loss: 0.05292, val loss: 0.05339\n",
      "Interaction training epoch: 152, train loss: 0.04638, val loss: 0.04708\n",
      "Interaction training epoch: 153, train loss: 0.04830, val loss: 0.05070\n",
      "Interaction training epoch: 154, train loss: 0.04798, val loss: 0.04952\n",
      "Interaction training epoch: 155, train loss: 0.04659, val loss: 0.04786\n",
      "Interaction training epoch: 156, train loss: 0.04767, val loss: 0.04876\n",
      "Interaction training epoch: 157, train loss: 0.04853, val loss: 0.04917\n",
      "Interaction training epoch: 158, train loss: 0.04677, val loss: 0.04803\n",
      "Interaction training epoch: 159, train loss: 0.04901, val loss: 0.04931\n",
      "Interaction training epoch: 160, train loss: 0.04754, val loss: 0.04893\n",
      "Interaction training epoch: 161, train loss: 0.04874, val loss: 0.04950\n",
      "Interaction training epoch: 162, train loss: 0.04595, val loss: 0.04721\n",
      "Interaction training epoch: 163, train loss: 0.04732, val loss: 0.04956\n",
      "Interaction training epoch: 164, train loss: 0.04567, val loss: 0.04749\n",
      "Interaction training epoch: 165, train loss: 0.05013, val loss: 0.05115\n",
      "Interaction training epoch: 166, train loss: 0.04663, val loss: 0.04790\n",
      "Interaction training epoch: 167, train loss: 0.04602, val loss: 0.04784\n",
      "Interaction training epoch: 168, train loss: 0.04789, val loss: 0.04800\n",
      "Interaction training epoch: 169, train loss: 0.04480, val loss: 0.04615\n",
      "Interaction training epoch: 170, train loss: 0.04657, val loss: 0.04712\n",
      "Interaction training epoch: 171, train loss: 0.04515, val loss: 0.04688\n",
      "Interaction training epoch: 172, train loss: 0.04745, val loss: 0.04904\n",
      "Interaction training epoch: 173, train loss: 0.04801, val loss: 0.05050\n",
      "Interaction training epoch: 174, train loss: 0.05297, val loss: 0.05389\n",
      "Interaction training epoch: 175, train loss: 0.04522, val loss: 0.04687\n",
      "Interaction training epoch: 176, train loss: 0.04654, val loss: 0.04709\n",
      "Interaction training epoch: 177, train loss: 0.04462, val loss: 0.04576\n",
      "Interaction training epoch: 178, train loss: 0.04715, val loss: 0.04880\n",
      "Interaction training epoch: 179, train loss: 0.04515, val loss: 0.04673\n",
      "Interaction training epoch: 180, train loss: 0.04652, val loss: 0.04824\n",
      "Interaction training epoch: 181, train loss: 0.04757, val loss: 0.04936\n",
      "Interaction training epoch: 182, train loss: 0.04771, val loss: 0.04909\n",
      "Interaction training epoch: 183, train loss: 0.04999, val loss: 0.05186\n",
      "Interaction training epoch: 184, train loss: 0.04691, val loss: 0.04877\n",
      "Interaction training epoch: 185, train loss: 0.04599, val loss: 0.04763\n",
      "Interaction training epoch: 186, train loss: 0.04839, val loss: 0.04969\n",
      "Interaction training epoch: 187, train loss: 0.04634, val loss: 0.04846\n",
      "Interaction training epoch: 188, train loss: 0.04686, val loss: 0.04828\n",
      "Interaction training epoch: 189, train loss: 0.04527, val loss: 0.04603\n",
      "Interaction training epoch: 190, train loss: 0.04811, val loss: 0.05046\n",
      "Interaction training epoch: 191, train loss: 0.04668, val loss: 0.04794\n",
      "Interaction training epoch: 192, train loss: 0.04580, val loss: 0.04673\n",
      "Interaction training epoch: 193, train loss: 0.04953, val loss: 0.05077\n",
      "Interaction training epoch: 194, train loss: 0.04637, val loss: 0.04745\n",
      "Interaction training epoch: 195, train loss: 0.05291, val loss: 0.05606\n",
      "Interaction training epoch: 196, train loss: 0.04552, val loss: 0.04706\n",
      "Interaction training epoch: 197, train loss: 0.04954, val loss: 0.05175\n",
      "Interaction training epoch: 198, train loss: 0.04504, val loss: 0.04659\n",
      "Interaction training epoch: 199, train loss: 0.04939, val loss: 0.05068\n",
      "Interaction training epoch: 200, train loss: 0.04824, val loss: 0.04969\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########7 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.04624, val loss: 0.04648\n",
      "Interaction tuning epoch: 2, train loss: 0.04654, val loss: 0.04675\n",
      "Interaction tuning epoch: 3, train loss: 0.04688, val loss: 0.04770\n",
      "Interaction tuning epoch: 4, train loss: 0.04703, val loss: 0.04714\n",
      "Interaction tuning epoch: 5, train loss: 0.04871, val loss: 0.04870\n",
      "Interaction tuning epoch: 6, train loss: 0.04715, val loss: 0.04685\n",
      "Interaction tuning epoch: 7, train loss: 0.04753, val loss: 0.04769\n",
      "Interaction tuning epoch: 8, train loss: 0.04804, val loss: 0.04839\n",
      "Interaction tuning epoch: 9, train loss: 0.04656, val loss: 0.04746\n",
      "Interaction tuning epoch: 10, train loss: 0.04708, val loss: 0.04792\n",
      "Interaction tuning epoch: 11, train loss: 0.04593, val loss: 0.04607\n",
      "Interaction tuning epoch: 12, train loss: 0.04889, val loss: 0.05019\n",
      "Interaction tuning epoch: 13, train loss: 0.04756, val loss: 0.04795\n",
      "Interaction tuning epoch: 14, train loss: 0.05056, val loss: 0.04891\n",
      "Interaction tuning epoch: 15, train loss: 0.05649, val loss: 0.05700\n",
      "Interaction tuning epoch: 16, train loss: 0.04770, val loss: 0.04777\n",
      "Interaction tuning epoch: 17, train loss: 0.04665, val loss: 0.04734\n",
      "Interaction tuning epoch: 18, train loss: 0.04709, val loss: 0.04763\n",
      "Interaction tuning epoch: 19, train loss: 0.04762, val loss: 0.04775\n",
      "Interaction tuning epoch: 20, train loss: 0.04647, val loss: 0.04691\n",
      "Interaction tuning epoch: 21, train loss: 0.04858, val loss: 0.04904\n",
      "Interaction tuning epoch: 22, train loss: 0.04763, val loss: 0.04743\n",
      "Interaction tuning epoch: 23, train loss: 0.04883, val loss: 0.04879\n",
      "Interaction tuning epoch: 24, train loss: 0.04608, val loss: 0.04728\n",
      "Interaction tuning epoch: 25, train loss: 0.04999, val loss: 0.04940\n",
      "Interaction tuning epoch: 26, train loss: 0.04916, val loss: 0.04857\n",
      "Interaction tuning epoch: 27, train loss: 0.04784, val loss: 0.04855\n",
      "Interaction tuning epoch: 28, train loss: 0.04702, val loss: 0.04758\n",
      "Interaction tuning epoch: 29, train loss: 0.04685, val loss: 0.04720\n",
      "Interaction tuning epoch: 30, train loss: 0.04688, val loss: 0.04800\n",
      "Interaction tuning epoch: 31, train loss: 0.04656, val loss: 0.04725\n",
      "Interaction tuning epoch: 32, train loss: 0.04698, val loss: 0.04718\n",
      "Interaction tuning epoch: 33, train loss: 0.04648, val loss: 0.04674\n",
      "Interaction tuning epoch: 34, train loss: 0.04622, val loss: 0.04627\n",
      "Interaction tuning epoch: 35, train loss: 0.04695, val loss: 0.04653\n",
      "Interaction tuning epoch: 36, train loss: 0.04638, val loss: 0.04708\n",
      "Interaction tuning epoch: 37, train loss: 0.04517, val loss: 0.04599\n",
      "Interaction tuning epoch: 38, train loss: 0.04594, val loss: 0.04626\n",
      "Interaction tuning epoch: 39, train loss: 0.04806, val loss: 0.04884\n",
      "Interaction tuning epoch: 40, train loss: 0.04762, val loss: 0.04781\n",
      "Interaction tuning epoch: 41, train loss: 0.04762, val loss: 0.04909\n",
      "Interaction tuning epoch: 42, train loss: 0.04746, val loss: 0.04805\n",
      "Interaction tuning epoch: 43, train loss: 0.04604, val loss: 0.04612\n",
      "Interaction tuning epoch: 44, train loss: 0.04636, val loss: 0.04673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 45, train loss: 0.04643, val loss: 0.04712\n",
      "Interaction tuning epoch: 46, train loss: 0.04656, val loss: 0.04678\n",
      "Interaction tuning epoch: 47, train loss: 0.04635, val loss: 0.04667\n",
      "Interaction tuning epoch: 48, train loss: 0.04816, val loss: 0.04882\n",
      "Interaction tuning epoch: 49, train loss: 0.04683, val loss: 0.04711\n",
      "Interaction tuning epoch: 50, train loss: 0.04845, val loss: 0.04905\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 37.30984044075012\n",
      "After the gam stage, training error is 0.04845 , validation error is 0.04905\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 1.232198\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.035131 validation MAE=0.046171,rank=9\n",
      "[SoftImpute] Iter 2: observed MAE=0.031137 validation MAE=0.045068,rank=9\n",
      "[SoftImpute] Iter 3: observed MAE=0.027978 validation MAE=0.044148,rank=9\n",
      "[SoftImpute] Iter 4: observed MAE=0.025426 validation MAE=0.043403,rank=9\n",
      "[SoftImpute] Iter 5: observed MAE=0.023332 validation MAE=0.042769,rank=9\n",
      "[SoftImpute] Iter 6: observed MAE=0.021578 validation MAE=0.042217,rank=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 7: observed MAE=0.020097 validation MAE=0.041741,rank=9\n",
      "[SoftImpute] Iter 8: observed MAE=0.018826 validation MAE=0.041314,rank=9\n",
      "[SoftImpute] Iter 9: observed MAE=0.017726 validation MAE=0.040935,rank=9\n",
      "[SoftImpute] Iter 10: observed MAE=0.016765 validation MAE=0.040604,rank=9\n",
      "[SoftImpute] Iter 11: observed MAE=0.015917 validation MAE=0.040315,rank=9\n",
      "[SoftImpute] Iter 12: observed MAE=0.015166 validation MAE=0.040059,rank=9\n",
      "[SoftImpute] Iter 13: observed MAE=0.014499 validation MAE=0.039832,rank=9\n",
      "[SoftImpute] Iter 14: observed MAE=0.013896 validation MAE=0.039621,rank=9\n",
      "[SoftImpute] Iter 15: observed MAE=0.013354 validation MAE=0.039434,rank=9\n",
      "[SoftImpute] Iter 16: observed MAE=0.012861 validation MAE=0.039264,rank=9\n",
      "[SoftImpute] Iter 17: observed MAE=0.012413 validation MAE=0.039103,rank=9\n",
      "[SoftImpute] Iter 18: observed MAE=0.012002 validation MAE=0.038949,rank=9\n",
      "[SoftImpute] Iter 19: observed MAE=0.011626 validation MAE=0.038804,rank=9\n",
      "[SoftImpute] Iter 20: observed MAE=0.011280 validation MAE=0.038670,rank=9\n",
      "[SoftImpute] Iter 21: observed MAE=0.010961 validation MAE=0.038549,rank=9\n",
      "[SoftImpute] Iter 22: observed MAE=0.010666 validation MAE=0.038436,rank=9\n",
      "[SoftImpute] Iter 23: observed MAE=0.010391 validation MAE=0.038331,rank=9\n",
      "[SoftImpute] Iter 24: observed MAE=0.010136 validation MAE=0.038241,rank=9\n",
      "[SoftImpute] Iter 25: observed MAE=0.009896 validation MAE=0.038157,rank=9\n",
      "[SoftImpute] Iter 26: observed MAE=0.009672 validation MAE=0.038076,rank=9\n",
      "[SoftImpute] Iter 27: observed MAE=0.009461 validation MAE=0.038001,rank=9\n",
      "[SoftImpute] Iter 28: observed MAE=0.009265 validation MAE=0.037932,rank=9\n",
      "[SoftImpute] Iter 29: observed MAE=0.009079 validation MAE=0.037867,rank=9\n",
      "[SoftImpute] Iter 30: observed MAE=0.008905 validation MAE=0.037803,rank=9\n",
      "[SoftImpute] Iter 31: observed MAE=0.008740 validation MAE=0.037742,rank=9\n",
      "[SoftImpute] Iter 32: observed MAE=0.008585 validation MAE=0.037681,rank=9\n",
      "[SoftImpute] Iter 33: observed MAE=0.008437 validation MAE=0.037621,rank=9\n",
      "[SoftImpute] Iter 34: observed MAE=0.008296 validation MAE=0.037563,rank=9\n",
      "[SoftImpute] Iter 35: observed MAE=0.008163 validation MAE=0.037507,rank=9\n",
      "[SoftImpute] Iter 36: observed MAE=0.008036 validation MAE=0.037453,rank=9\n",
      "[SoftImpute] Iter 37: observed MAE=0.007915 validation MAE=0.037399,rank=9\n",
      "[SoftImpute] Iter 38: observed MAE=0.007800 validation MAE=0.037346,rank=9\n",
      "[SoftImpute] Iter 39: observed MAE=0.007690 validation MAE=0.037296,rank=9\n",
      "[SoftImpute] Iter 40: observed MAE=0.007585 validation MAE=0.037246,rank=9\n",
      "[SoftImpute] Iter 41: observed MAE=0.007486 validation MAE=0.037197,rank=9\n",
      "[SoftImpute] Iter 42: observed MAE=0.007390 validation MAE=0.037149,rank=9\n",
      "[SoftImpute] Iter 43: observed MAE=0.007298 validation MAE=0.037101,rank=9\n",
      "[SoftImpute] Iter 44: observed MAE=0.007209 validation MAE=0.037053,rank=9\n",
      "[SoftImpute] Iter 45: observed MAE=0.007124 validation MAE=0.037006,rank=9\n",
      "[SoftImpute] Iter 46: observed MAE=0.007042 validation MAE=0.036960,rank=9\n",
      "[SoftImpute] Iter 47: observed MAE=0.006964 validation MAE=0.036914,rank=9\n",
      "[SoftImpute] Iter 48: observed MAE=0.006888 validation MAE=0.036870,rank=9\n",
      "[SoftImpute] Iter 49: observed MAE=0.006816 validation MAE=0.036827,rank=9\n",
      "[SoftImpute] Iter 50: observed MAE=0.006746 validation MAE=0.036784,rank=9\n",
      "[SoftImpute] Iter 51: observed MAE=0.006679 validation MAE=0.036742,rank=9\n",
      "[SoftImpute] Iter 52: observed MAE=0.006614 validation MAE=0.036701,rank=9\n",
      "[SoftImpute] Iter 53: observed MAE=0.006552 validation MAE=0.036661,rank=9\n",
      "[SoftImpute] Iter 54: observed MAE=0.006491 validation MAE=0.036623,rank=9\n",
      "[SoftImpute] Iter 55: observed MAE=0.006433 validation MAE=0.036584,rank=9\n",
      "[SoftImpute] Iter 56: observed MAE=0.006377 validation MAE=0.036546,rank=9\n",
      "[SoftImpute] Iter 57: observed MAE=0.006322 validation MAE=0.036509,rank=9\n",
      "[SoftImpute] Iter 58: observed MAE=0.006270 validation MAE=0.036472,rank=9\n",
      "[SoftImpute] Iter 59: observed MAE=0.006218 validation MAE=0.036436,rank=9\n",
      "[SoftImpute] Iter 60: observed MAE=0.006169 validation MAE=0.036400,rank=9\n",
      "[SoftImpute] Iter 61: observed MAE=0.006121 validation MAE=0.036365,rank=9\n",
      "[SoftImpute] Iter 62: observed MAE=0.006075 validation MAE=0.036330,rank=9\n",
      "[SoftImpute] Iter 63: observed MAE=0.006030 validation MAE=0.036295,rank=9\n",
      "[SoftImpute] Iter 64: observed MAE=0.005987 validation MAE=0.036260,rank=9\n",
      "[SoftImpute] Iter 65: observed MAE=0.005945 validation MAE=0.036225,rank=9\n",
      "[SoftImpute] Iter 66: observed MAE=0.005904 validation MAE=0.036190,rank=9\n",
      "[SoftImpute] Iter 67: observed MAE=0.005864 validation MAE=0.036156,rank=9\n",
      "[SoftImpute] Iter 68: observed MAE=0.005825 validation MAE=0.036122,rank=9\n",
      "[SoftImpute] Iter 69: observed MAE=0.005788 validation MAE=0.036088,rank=9\n",
      "[SoftImpute] Iter 70: observed MAE=0.005751 validation MAE=0.036055,rank=9\n",
      "[SoftImpute] Iter 71: observed MAE=0.005715 validation MAE=0.036022,rank=9\n",
      "[SoftImpute] Iter 72: observed MAE=0.005681 validation MAE=0.035988,rank=9\n",
      "[SoftImpute] Iter 73: observed MAE=0.005647 validation MAE=0.035955,rank=9\n",
      "[SoftImpute] Iter 74: observed MAE=0.005614 validation MAE=0.035922,rank=9\n",
      "[SoftImpute] Iter 75: observed MAE=0.005582 validation MAE=0.035890,rank=9\n",
      "[SoftImpute] Iter 76: observed MAE=0.005551 validation MAE=0.035858,rank=9\n",
      "[SoftImpute] Iter 77: observed MAE=0.005520 validation MAE=0.035826,rank=9\n",
      "[SoftImpute] Iter 78: observed MAE=0.005490 validation MAE=0.035794,rank=9\n",
      "[SoftImpute] Iter 79: observed MAE=0.005461 validation MAE=0.035763,rank=9\n",
      "[SoftImpute] Iter 80: observed MAE=0.005433 validation MAE=0.035731,rank=9\n",
      "[SoftImpute] Iter 81: observed MAE=0.005406 validation MAE=0.035700,rank=9\n",
      "[SoftImpute] Iter 82: observed MAE=0.005379 validation MAE=0.035668,rank=9\n",
      "[SoftImpute] Iter 83: observed MAE=0.005352 validation MAE=0.035637,rank=9\n",
      "[SoftImpute] Iter 84: observed MAE=0.005327 validation MAE=0.035606,rank=9\n",
      "[SoftImpute] Iter 85: observed MAE=0.005302 validation MAE=0.035576,rank=9\n",
      "[SoftImpute] Iter 86: observed MAE=0.005277 validation MAE=0.035545,rank=9\n",
      "[SoftImpute] Iter 87: observed MAE=0.005253 validation MAE=0.035514,rank=9\n",
      "[SoftImpute] Iter 88: observed MAE=0.005229 validation MAE=0.035484,rank=9\n",
      "[SoftImpute] Iter 89: observed MAE=0.005206 validation MAE=0.035454,rank=9\n",
      "[SoftImpute] Iter 90: observed MAE=0.005184 validation MAE=0.035424,rank=9\n",
      "[SoftImpute] Iter 91: observed MAE=0.005162 validation MAE=0.035395,rank=9\n",
      "[SoftImpute] Iter 92: observed MAE=0.005140 validation MAE=0.035366,rank=9\n",
      "[SoftImpute] Iter 93: observed MAE=0.005119 validation MAE=0.035337,rank=9\n",
      "[SoftImpute] Iter 94: observed MAE=0.005098 validation MAE=0.035309,rank=9\n",
      "[SoftImpute] Iter 95: observed MAE=0.005078 validation MAE=0.035281,rank=9\n",
      "[SoftImpute] Iter 96: observed MAE=0.005058 validation MAE=0.035253,rank=9\n",
      "[SoftImpute] Iter 97: observed MAE=0.005039 validation MAE=0.035225,rank=9\n",
      "[SoftImpute] Iter 98: observed MAE=0.005020 validation MAE=0.035197,rank=9\n",
      "[SoftImpute] Iter 99: observed MAE=0.005001 validation MAE=0.035169,rank=9\n",
      "[SoftImpute] Iter 100: observed MAE=0.004983 validation MAE=0.035142,rank=9\n",
      "[SoftImpute] Iter 101: observed MAE=0.004965 validation MAE=0.035114,rank=9\n",
      "[SoftImpute] Iter 102: observed MAE=0.004947 validation MAE=0.035087,rank=9\n",
      "[SoftImpute] Iter 103: observed MAE=0.004930 validation MAE=0.035059,rank=9\n",
      "[SoftImpute] Iter 104: observed MAE=0.004913 validation MAE=0.035032,rank=9\n",
      "[SoftImpute] Iter 105: observed MAE=0.004896 validation MAE=0.035005,rank=9\n",
      "[SoftImpute] Iter 106: observed MAE=0.004879 validation MAE=0.034977,rank=9\n",
      "[SoftImpute] Iter 107: observed MAE=0.004863 validation MAE=0.034951,rank=9\n",
      "[SoftImpute] Iter 108: observed MAE=0.004848 validation MAE=0.034924,rank=9\n",
      "[SoftImpute] Iter 109: observed MAE=0.004832 validation MAE=0.034897,rank=9\n",
      "[SoftImpute] Iter 110: observed MAE=0.004817 validation MAE=0.034870,rank=9\n",
      "[SoftImpute] Iter 111: observed MAE=0.004802 validation MAE=0.034844,rank=9\n",
      "[SoftImpute] Iter 112: observed MAE=0.004787 validation MAE=0.034818,rank=9\n",
      "[SoftImpute] Iter 113: observed MAE=0.004773 validation MAE=0.034792,rank=9\n",
      "[SoftImpute] Iter 114: observed MAE=0.004759 validation MAE=0.034766,rank=9\n",
      "[SoftImpute] Iter 115: observed MAE=0.004745 validation MAE=0.034740,rank=9\n",
      "[SoftImpute] Iter 116: observed MAE=0.004731 validation MAE=0.034714,rank=9\n",
      "[SoftImpute] Iter 117: observed MAE=0.004718 validation MAE=0.034689,rank=9\n",
      "[SoftImpute] Iter 118: observed MAE=0.004704 validation MAE=0.034663,rank=9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 119: observed MAE=0.004691 validation MAE=0.034638,rank=9\n",
      "[SoftImpute] Iter 120: observed MAE=0.004679 validation MAE=0.034613,rank=9\n",
      "[SoftImpute] Iter 121: observed MAE=0.004666 validation MAE=0.034588,rank=9\n",
      "[SoftImpute] Iter 122: observed MAE=0.004654 validation MAE=0.034563,rank=9\n",
      "[SoftImpute] Iter 123: observed MAE=0.004641 validation MAE=0.034539,rank=9\n",
      "[SoftImpute] Iter 124: observed MAE=0.004629 validation MAE=0.034514,rank=9\n",
      "[SoftImpute] Iter 125: observed MAE=0.004618 validation MAE=0.034490,rank=9\n",
      "[SoftImpute] Iter 126: observed MAE=0.004606 validation MAE=0.034465,rank=9\n",
      "[SoftImpute] Iter 127: observed MAE=0.004594 validation MAE=0.034441,rank=9\n",
      "[SoftImpute] Iter 128: observed MAE=0.004583 validation MAE=0.034418,rank=9\n",
      "[SoftImpute] Iter 129: observed MAE=0.004572 validation MAE=0.034395,rank=9\n",
      "[SoftImpute] Iter 130: observed MAE=0.004561 validation MAE=0.034371,rank=9\n",
      "[SoftImpute] Iter 131: observed MAE=0.004550 validation MAE=0.034348,rank=9\n",
      "[SoftImpute] Iter 132: observed MAE=0.004539 validation MAE=0.034325,rank=9\n",
      "[SoftImpute] Iter 133: observed MAE=0.004529 validation MAE=0.034302,rank=9\n",
      "[SoftImpute] Iter 134: observed MAE=0.004518 validation MAE=0.034279,rank=9\n",
      "[SoftImpute] Iter 135: observed MAE=0.004508 validation MAE=0.034256,rank=9\n",
      "[SoftImpute] Iter 136: observed MAE=0.004498 validation MAE=0.034233,rank=9\n",
      "[SoftImpute] Iter 137: observed MAE=0.004488 validation MAE=0.034210,rank=9\n",
      "[SoftImpute] Iter 138: observed MAE=0.004478 validation MAE=0.034187,rank=9\n",
      "[SoftImpute] Iter 139: observed MAE=0.004469 validation MAE=0.034164,rank=9\n",
      "[SoftImpute] Iter 140: observed MAE=0.004459 validation MAE=0.034141,rank=9\n",
      "[SoftImpute] Iter 141: observed MAE=0.004450 validation MAE=0.034118,rank=9\n",
      "[SoftImpute] Iter 142: observed MAE=0.004440 validation MAE=0.034096,rank=9\n",
      "[SoftImpute] Iter 143: observed MAE=0.004431 validation MAE=0.034073,rank=9\n",
      "[SoftImpute] Iter 144: observed MAE=0.004422 validation MAE=0.034050,rank=9\n",
      "[SoftImpute] Iter 145: observed MAE=0.004413 validation MAE=0.034027,rank=9\n",
      "[SoftImpute] Iter 146: observed MAE=0.004405 validation MAE=0.034004,rank=9\n",
      "[SoftImpute] Iter 147: observed MAE=0.004396 validation MAE=0.033981,rank=9\n",
      "[SoftImpute] Iter 148: observed MAE=0.004387 validation MAE=0.033958,rank=9\n",
      "[SoftImpute] Iter 149: observed MAE=0.004379 validation MAE=0.033936,rank=9\n",
      "[SoftImpute] Iter 150: observed MAE=0.004371 validation MAE=0.033913,rank=9\n",
      "[SoftImpute] Iter 151: observed MAE=0.004362 validation MAE=0.033890,rank=9\n",
      "[SoftImpute] Iter 152: observed MAE=0.004354 validation MAE=0.033867,rank=9\n",
      "[SoftImpute] Iter 153: observed MAE=0.004346 validation MAE=0.033845,rank=9\n",
      "[SoftImpute] Iter 154: observed MAE=0.004338 validation MAE=0.033822,rank=9\n",
      "[SoftImpute] Iter 155: observed MAE=0.004331 validation MAE=0.033800,rank=9\n",
      "[SoftImpute] Iter 156: observed MAE=0.004323 validation MAE=0.033778,rank=9\n",
      "[SoftImpute] Iter 157: observed MAE=0.004315 validation MAE=0.033756,rank=9\n",
      "[SoftImpute] Iter 158: observed MAE=0.004308 validation MAE=0.033734,rank=9\n",
      "[SoftImpute] Iter 159: observed MAE=0.004300 validation MAE=0.033712,rank=9\n",
      "[SoftImpute] Iter 160: observed MAE=0.004293 validation MAE=0.033690,rank=9\n",
      "[SoftImpute] Iter 161: observed MAE=0.004286 validation MAE=0.033668,rank=9\n",
      "[SoftImpute] Iter 162: observed MAE=0.004279 validation MAE=0.033646,rank=9\n",
      "[SoftImpute] Iter 163: observed MAE=0.004271 validation MAE=0.033624,rank=9\n",
      "[SoftImpute] Iter 164: observed MAE=0.004264 validation MAE=0.033603,rank=9\n",
      "[SoftImpute] Iter 165: observed MAE=0.004258 validation MAE=0.033581,rank=9\n",
      "[SoftImpute] Iter 166: observed MAE=0.004251 validation MAE=0.033559,rank=9\n",
      "[SoftImpute] Iter 167: observed MAE=0.004244 validation MAE=0.033538,rank=9\n",
      "[SoftImpute] Iter 168: observed MAE=0.004237 validation MAE=0.033516,rank=9\n",
      "[SoftImpute] Iter 169: observed MAE=0.004231 validation MAE=0.033495,rank=9\n",
      "[SoftImpute] Iter 170: observed MAE=0.004224 validation MAE=0.033473,rank=9\n",
      "[SoftImpute] Iter 171: observed MAE=0.004218 validation MAE=0.033452,rank=9\n",
      "[SoftImpute] Iter 172: observed MAE=0.004211 validation MAE=0.033431,rank=9\n",
      "[SoftImpute] Iter 173: observed MAE=0.004205 validation MAE=0.033410,rank=9\n",
      "[SoftImpute] Iter 174: observed MAE=0.004199 validation MAE=0.033389,rank=9\n",
      "[SoftImpute] Iter 175: observed MAE=0.004193 validation MAE=0.033367,rank=9\n",
      "[SoftImpute] Iter 176: observed MAE=0.004187 validation MAE=0.033346,rank=9\n",
      "[SoftImpute] Iter 177: observed MAE=0.004181 validation MAE=0.033325,rank=9\n",
      "[SoftImpute] Iter 178: observed MAE=0.004175 validation MAE=0.033304,rank=9\n",
      "[SoftImpute] Iter 179: observed MAE=0.004169 validation MAE=0.033283,rank=9\n",
      "[SoftImpute] Iter 180: observed MAE=0.004163 validation MAE=0.033262,rank=9\n",
      "[SoftImpute] Iter 181: observed MAE=0.004158 validation MAE=0.033241,rank=9\n",
      "[SoftImpute] Iter 182: observed MAE=0.004152 validation MAE=0.033220,rank=9\n",
      "[SoftImpute] Iter 183: observed MAE=0.004146 validation MAE=0.033200,rank=9\n",
      "[SoftImpute] Iter 184: observed MAE=0.004141 validation MAE=0.033180,rank=9\n",
      "[SoftImpute] Iter 185: observed MAE=0.004135 validation MAE=0.033159,rank=9\n",
      "[SoftImpute] Iter 186: observed MAE=0.004130 validation MAE=0.033139,rank=9\n",
      "[SoftImpute] Iter 187: observed MAE=0.004125 validation MAE=0.033119,rank=9\n",
      "[SoftImpute] Iter 188: observed MAE=0.004119 validation MAE=0.033099,rank=9\n",
      "[SoftImpute] Iter 189: observed MAE=0.004114 validation MAE=0.033079,rank=9\n",
      "[SoftImpute] Iter 190: observed MAE=0.004109 validation MAE=0.033060,rank=9\n",
      "[SoftImpute] Iter 191: observed MAE=0.004104 validation MAE=0.033040,rank=9\n",
      "[SoftImpute] Iter 192: observed MAE=0.004098 validation MAE=0.033020,rank=9\n",
      "[SoftImpute] Iter 193: observed MAE=0.004093 validation MAE=0.033000,rank=9\n",
      "[SoftImpute] Iter 194: observed MAE=0.004088 validation MAE=0.032981,rank=9\n",
      "[SoftImpute] Iter 195: observed MAE=0.004083 validation MAE=0.032961,rank=9\n",
      "[SoftImpute] Iter 196: observed MAE=0.004078 validation MAE=0.032942,rank=9\n",
      "[SoftImpute] Iter 197: observed MAE=0.004073 validation MAE=0.032923,rank=9\n",
      "[SoftImpute] Iter 198: observed MAE=0.004068 validation MAE=0.032904,rank=9\n",
      "[SoftImpute] Iter 199: observed MAE=0.004064 validation MAE=0.032885,rank=9\n",
      "[SoftImpute] Iter 200: observed MAE=0.004059 validation MAE=0.032866,rank=9\n",
      "[SoftImpute] Iter 201: observed MAE=0.004054 validation MAE=0.032846,rank=9\n",
      "[SoftImpute] Iter 202: observed MAE=0.004049 validation MAE=0.032827,rank=9\n",
      "[SoftImpute] Iter 203: observed MAE=0.004045 validation MAE=0.032808,rank=9\n",
      "[SoftImpute] Iter 204: observed MAE=0.004040 validation MAE=0.032789,rank=9\n",
      "[SoftImpute] Iter 205: observed MAE=0.004036 validation MAE=0.032770,rank=9\n",
      "[SoftImpute] Iter 206: observed MAE=0.004031 validation MAE=0.032751,rank=9\n",
      "[SoftImpute] Iter 207: observed MAE=0.004027 validation MAE=0.032732,rank=9\n",
      "[SoftImpute] Iter 208: observed MAE=0.004022 validation MAE=0.032713,rank=9\n",
      "[SoftImpute] Iter 209: observed MAE=0.004018 validation MAE=0.032695,rank=9\n",
      "[SoftImpute] Iter 210: observed MAE=0.004014 validation MAE=0.032676,rank=9\n",
      "[SoftImpute] Iter 211: observed MAE=0.004009 validation MAE=0.032657,rank=9\n",
      "[SoftImpute] Iter 212: observed MAE=0.004005 validation MAE=0.032639,rank=9\n",
      "[SoftImpute] Iter 213: observed MAE=0.004001 validation MAE=0.032620,rank=9\n",
      "[SoftImpute] Iter 214: observed MAE=0.003997 validation MAE=0.032602,rank=9\n",
      "[SoftImpute] Iter 215: observed MAE=0.003993 validation MAE=0.032583,rank=9\n",
      "[SoftImpute] Iter 216: observed MAE=0.003989 validation MAE=0.032565,rank=9\n",
      "[SoftImpute] Iter 217: observed MAE=0.003985 validation MAE=0.032546,rank=9\n",
      "[SoftImpute] Iter 218: observed MAE=0.003981 validation MAE=0.032528,rank=9\n",
      "[SoftImpute] Iter 219: observed MAE=0.003977 validation MAE=0.032509,rank=9\n",
      "[SoftImpute] Iter 220: observed MAE=0.003973 validation MAE=0.032491,rank=9\n",
      "[SoftImpute] Iter 221: observed MAE=0.003969 validation MAE=0.032473,rank=9\n",
      "[SoftImpute] Iter 222: observed MAE=0.003965 validation MAE=0.032455,rank=9\n",
      "[SoftImpute] Iter 223: observed MAE=0.003961 validation MAE=0.032437,rank=9\n",
      "[SoftImpute] Iter 224: observed MAE=0.003957 validation MAE=0.032419,rank=9\n",
      "[SoftImpute] Iter 225: observed MAE=0.003954 validation MAE=0.032401,rank=9\n",
      "[SoftImpute] Iter 226: observed MAE=0.003950 validation MAE=0.032383,rank=9\n",
      "[SoftImpute] Iter 227: observed MAE=0.003946 validation MAE=0.032365,rank=9\n",
      "[SoftImpute] Iter 228: observed MAE=0.003943 validation MAE=0.032348,rank=9\n",
      "[SoftImpute] Iter 229: observed MAE=0.003939 validation MAE=0.032330,rank=9\n",
      "[SoftImpute] Iter 230: observed MAE=0.003935 validation MAE=0.032312,rank=9\n",
      "[SoftImpute] Iter 231: observed MAE=0.003932 validation MAE=0.032295,rank=9\n",
      "[SoftImpute] Iter 232: observed MAE=0.003928 validation MAE=0.032277,rank=9\n",
      "[SoftImpute] Iter 233: observed MAE=0.003925 validation MAE=0.032260,rank=9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 234: observed MAE=0.003921 validation MAE=0.032242,rank=9\n",
      "[SoftImpute] Iter 235: observed MAE=0.003918 validation MAE=0.032225,rank=9\n",
      "[SoftImpute] Iter 236: observed MAE=0.003914 validation MAE=0.032208,rank=9\n",
      "[SoftImpute] Iter 237: observed MAE=0.003911 validation MAE=0.032190,rank=9\n",
      "[SoftImpute] Iter 238: observed MAE=0.003907 validation MAE=0.032173,rank=9\n",
      "[SoftImpute] Iter 239: observed MAE=0.003904 validation MAE=0.032156,rank=9\n",
      "[SoftImpute] Iter 240: observed MAE=0.003901 validation MAE=0.032139,rank=9\n",
      "[SoftImpute] Iter 241: observed MAE=0.003898 validation MAE=0.032122,rank=9\n",
      "[SoftImpute] Iter 242: observed MAE=0.003894 validation MAE=0.032106,rank=9\n",
      "[SoftImpute] Iter 243: observed MAE=0.003891 validation MAE=0.032089,rank=9\n",
      "[SoftImpute] Iter 244: observed MAE=0.003888 validation MAE=0.032072,rank=9\n",
      "[SoftImpute] Iter 245: observed MAE=0.003885 validation MAE=0.032056,rank=9\n",
      "[SoftImpute] Iter 246: observed MAE=0.003881 validation MAE=0.032039,rank=9\n",
      "[SoftImpute] Iter 247: observed MAE=0.003878 validation MAE=0.032023,rank=9\n",
      "[SoftImpute] Iter 248: observed MAE=0.003875 validation MAE=0.032006,rank=9\n",
      "[SoftImpute] Iter 249: observed MAE=0.003872 validation MAE=0.031990,rank=9\n",
      "[SoftImpute] Iter 250: observed MAE=0.003869 validation MAE=0.031973,rank=9\n",
      "[SoftImpute] Iter 251: observed MAE=0.003866 validation MAE=0.031957,rank=9\n",
      "[SoftImpute] Iter 252: observed MAE=0.003863 validation MAE=0.031941,rank=9\n",
      "[SoftImpute] Iter 253: observed MAE=0.003860 validation MAE=0.031924,rank=9\n",
      "[SoftImpute] Iter 254: observed MAE=0.003857 validation MAE=0.031908,rank=9\n",
      "[SoftImpute] Iter 255: observed MAE=0.003854 validation MAE=0.031892,rank=9\n",
      "[SoftImpute] Iter 256: observed MAE=0.003851 validation MAE=0.031875,rank=9\n",
      "[SoftImpute] Iter 257: observed MAE=0.003848 validation MAE=0.031859,rank=9\n",
      "[SoftImpute] Iter 258: observed MAE=0.003845 validation MAE=0.031843,rank=9\n",
      "[SoftImpute] Iter 259: observed MAE=0.003842 validation MAE=0.031826,rank=9\n",
      "[SoftImpute] Iter 260: observed MAE=0.003840 validation MAE=0.031810,rank=9\n",
      "[SoftImpute] Iter 261: observed MAE=0.003837 validation MAE=0.031794,rank=9\n",
      "[SoftImpute] Iter 262: observed MAE=0.003834 validation MAE=0.031777,rank=9\n",
      "[SoftImpute] Iter 263: observed MAE=0.003831 validation MAE=0.031761,rank=9\n",
      "[SoftImpute] Iter 264: observed MAE=0.003829 validation MAE=0.031745,rank=9\n",
      "[SoftImpute] Iter 265: observed MAE=0.003826 validation MAE=0.031729,rank=9\n",
      "[SoftImpute] Iter 266: observed MAE=0.003823 validation MAE=0.031713,rank=9\n",
      "[SoftImpute] Iter 267: observed MAE=0.003820 validation MAE=0.031697,rank=9\n",
      "[SoftImpute] Iter 268: observed MAE=0.003818 validation MAE=0.031681,rank=9\n",
      "[SoftImpute] Iter 269: observed MAE=0.003815 validation MAE=0.031665,rank=9\n",
      "[SoftImpute] Iter 270: observed MAE=0.003812 validation MAE=0.031649,rank=9\n",
      "[SoftImpute] Iter 271: observed MAE=0.003810 validation MAE=0.031633,rank=9\n",
      "[SoftImpute] Iter 272: observed MAE=0.003807 validation MAE=0.031617,rank=9\n",
      "[SoftImpute] Iter 273: observed MAE=0.003805 validation MAE=0.031601,rank=9\n",
      "[SoftImpute] Iter 274: observed MAE=0.003802 validation MAE=0.031585,rank=9\n",
      "[SoftImpute] Iter 275: observed MAE=0.003799 validation MAE=0.031570,rank=9\n",
      "[SoftImpute] Iter 276: observed MAE=0.003797 validation MAE=0.031554,rank=9\n",
      "[SoftImpute] Iter 277: observed MAE=0.003794 validation MAE=0.031539,rank=9\n",
      "[SoftImpute] Iter 278: observed MAE=0.003792 validation MAE=0.031523,rank=9\n",
      "[SoftImpute] Iter 279: observed MAE=0.003789 validation MAE=0.031508,rank=9\n",
      "[SoftImpute] Iter 280: observed MAE=0.003787 validation MAE=0.031493,rank=9\n",
      "[SoftImpute] Iter 281: observed MAE=0.003784 validation MAE=0.031478,rank=9\n",
      "[SoftImpute] Iter 282: observed MAE=0.003782 validation MAE=0.031463,rank=9\n",
      "[SoftImpute] Iter 283: observed MAE=0.003780 validation MAE=0.031448,rank=9\n",
      "[SoftImpute] Iter 284: observed MAE=0.003777 validation MAE=0.031433,rank=9\n",
      "[SoftImpute] Iter 285: observed MAE=0.003775 validation MAE=0.031418,rank=9\n",
      "[SoftImpute] Iter 286: observed MAE=0.003772 validation MAE=0.031404,rank=9\n",
      "[SoftImpute] Iter 287: observed MAE=0.003770 validation MAE=0.031389,rank=9\n",
      "[SoftImpute] Iter 288: observed MAE=0.003768 validation MAE=0.031374,rank=9\n",
      "[SoftImpute] Iter 289: observed MAE=0.003765 validation MAE=0.031360,rank=9\n",
      "[SoftImpute] Iter 290: observed MAE=0.003763 validation MAE=0.031345,rank=9\n",
      "[SoftImpute] Iter 291: observed MAE=0.003761 validation MAE=0.031330,rank=9\n",
      "[SoftImpute] Iter 292: observed MAE=0.003758 validation MAE=0.031316,rank=9\n",
      "[SoftImpute] Iter 293: observed MAE=0.003756 validation MAE=0.031301,rank=9\n",
      "[SoftImpute] Iter 294: observed MAE=0.003754 validation MAE=0.031286,rank=9\n",
      "[SoftImpute] Iter 295: observed MAE=0.003752 validation MAE=0.031272,rank=9\n",
      "[SoftImpute] Iter 296: observed MAE=0.003749 validation MAE=0.031257,rank=9\n",
      "[SoftImpute] Iter 297: observed MAE=0.003747 validation MAE=0.031242,rank=9\n",
      "[SoftImpute] Iter 298: observed MAE=0.003745 validation MAE=0.031228,rank=9\n",
      "[SoftImpute] Iter 299: observed MAE=0.003743 validation MAE=0.031213,rank=9\n",
      "[SoftImpute] Iter 300: observed MAE=0.003741 validation MAE=0.031199,rank=9\n",
      "[SoftImpute] Iter 301: observed MAE=0.003738 validation MAE=0.031184,rank=9\n",
      "[SoftImpute] Iter 302: observed MAE=0.003736 validation MAE=0.031170,rank=9\n",
      "[SoftImpute] Iter 303: observed MAE=0.003734 validation MAE=0.031155,rank=9\n",
      "[SoftImpute] Iter 304: observed MAE=0.003732 validation MAE=0.031141,rank=9\n",
      "[SoftImpute] Iter 305: observed MAE=0.003730 validation MAE=0.031126,rank=9\n",
      "[SoftImpute] Iter 306: observed MAE=0.003728 validation MAE=0.031112,rank=9\n",
      "[SoftImpute] Iter 307: observed MAE=0.003725 validation MAE=0.031097,rank=9\n",
      "[SoftImpute] Iter 308: observed MAE=0.003723 validation MAE=0.031083,rank=9\n",
      "[SoftImpute] Iter 309: observed MAE=0.003721 validation MAE=0.031069,rank=9\n",
      "[SoftImpute] Iter 310: observed MAE=0.003719 validation MAE=0.031054,rank=9\n",
      "[SoftImpute] Iter 311: observed MAE=0.003717 validation MAE=0.031040,rank=9\n",
      "[SoftImpute] Iter 312: observed MAE=0.003715 validation MAE=0.031025,rank=9\n",
      "[SoftImpute] Iter 313: observed MAE=0.003713 validation MAE=0.031011,rank=9\n",
      "[SoftImpute] Iter 314: observed MAE=0.003711 validation MAE=0.030997,rank=9\n",
      "[SoftImpute] Iter 315: observed MAE=0.003709 validation MAE=0.030982,rank=9\n",
      "[SoftImpute] Iter 316: observed MAE=0.003707 validation MAE=0.030968,rank=9\n",
      "[SoftImpute] Iter 317: observed MAE=0.003704 validation MAE=0.030954,rank=9\n",
      "[SoftImpute] Iter 318: observed MAE=0.003702 validation MAE=0.030939,rank=9\n",
      "[SoftImpute] Iter 319: observed MAE=0.003700 validation MAE=0.030925,rank=9\n",
      "[SoftImpute] Iter 320: observed MAE=0.003698 validation MAE=0.030911,rank=9\n",
      "[SoftImpute] Iter 321: observed MAE=0.003696 validation MAE=0.030896,rank=9\n",
      "[SoftImpute] Iter 322: observed MAE=0.003694 validation MAE=0.030882,rank=9\n",
      "[SoftImpute] Iter 323: observed MAE=0.003692 validation MAE=0.030868,rank=9\n",
      "[SoftImpute] Iter 324: observed MAE=0.003690 validation MAE=0.030854,rank=9\n",
      "[SoftImpute] Iter 325: observed MAE=0.003688 validation MAE=0.030840,rank=9\n",
      "[SoftImpute] Iter 326: observed MAE=0.003686 validation MAE=0.030826,rank=9\n",
      "[SoftImpute] Iter 327: observed MAE=0.003684 validation MAE=0.030812,rank=9\n",
      "[SoftImpute] Iter 328: observed MAE=0.003682 validation MAE=0.030798,rank=9\n",
      "[SoftImpute] Iter 329: observed MAE=0.003680 validation MAE=0.030784,rank=9\n",
      "[SoftImpute] Iter 330: observed MAE=0.003679 validation MAE=0.030769,rank=9\n",
      "[SoftImpute] Iter 331: observed MAE=0.003677 validation MAE=0.030755,rank=9\n",
      "[SoftImpute] Iter 332: observed MAE=0.003675 validation MAE=0.030741,rank=9\n",
      "[SoftImpute] Iter 333: observed MAE=0.003673 validation MAE=0.030727,rank=9\n",
      "[SoftImpute] Iter 334: observed MAE=0.003671 validation MAE=0.030713,rank=9\n",
      "[SoftImpute] Iter 335: observed MAE=0.003669 validation MAE=0.030699,rank=9\n",
      "[SoftImpute] Iter 336: observed MAE=0.003667 validation MAE=0.030685,rank=9\n",
      "[SoftImpute] Iter 337: observed MAE=0.003665 validation MAE=0.030671,rank=9\n",
      "[SoftImpute] Iter 338: observed MAE=0.003663 validation MAE=0.030657,rank=9\n",
      "[SoftImpute] Iter 339: observed MAE=0.003662 validation MAE=0.030644,rank=9\n",
      "[SoftImpute] Iter 340: observed MAE=0.003660 validation MAE=0.030630,rank=9\n",
      "[SoftImpute] Iter 341: observed MAE=0.003658 validation MAE=0.030616,rank=9\n",
      "[SoftImpute] Iter 342: observed MAE=0.003656 validation MAE=0.030602,rank=9\n",
      "[SoftImpute] Iter 343: observed MAE=0.003654 validation MAE=0.030588,rank=9\n",
      "[SoftImpute] Iter 344: observed MAE=0.003652 validation MAE=0.030574,rank=9\n",
      "[SoftImpute] Iter 345: observed MAE=0.003651 validation MAE=0.030561,rank=9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 346: observed MAE=0.003649 validation MAE=0.030547,rank=9\n",
      "[SoftImpute] Iter 347: observed MAE=0.003647 validation MAE=0.030533,rank=9\n",
      "[SoftImpute] Iter 348: observed MAE=0.003645 validation MAE=0.030520,rank=9\n",
      "[SoftImpute] Iter 349: observed MAE=0.003644 validation MAE=0.030506,rank=9\n",
      "[SoftImpute] Iter 350: observed MAE=0.003642 validation MAE=0.030492,rank=9\n",
      "[SoftImpute] Iter 351: observed MAE=0.003640 validation MAE=0.030479,rank=9\n",
      "[SoftImpute] Iter 352: observed MAE=0.003638 validation MAE=0.030465,rank=9\n",
      "[SoftImpute] Iter 353: observed MAE=0.003636 validation MAE=0.030451,rank=9\n",
      "[SoftImpute] Iter 354: observed MAE=0.003635 validation MAE=0.030438,rank=9\n",
      "[SoftImpute] Iter 355: observed MAE=0.003633 validation MAE=0.030424,rank=9\n",
      "[SoftImpute] Iter 356: observed MAE=0.003631 validation MAE=0.030411,rank=9\n",
      "[SoftImpute] Iter 357: observed MAE=0.003630 validation MAE=0.030397,rank=9\n",
      "[SoftImpute] Iter 358: observed MAE=0.003628 validation MAE=0.030384,rank=9\n",
      "[SoftImpute] Iter 359: observed MAE=0.003626 validation MAE=0.030370,rank=9\n",
      "[SoftImpute] Stopped after iteration 359 for lambda=0.024644\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 12.255932569503784\n",
      "After the matrix factor stage, training error is 0.00363, validation error is 0.03037\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.30185, val loss: 0.30478\n",
      "Main effects training epoch: 2, train loss: 0.23228, val loss: 0.23617\n",
      "Main effects training epoch: 3, train loss: 0.17997, val loss: 0.18315\n",
      "Main effects training epoch: 4, train loss: 0.14845, val loss: 0.15215\n",
      "Main effects training epoch: 5, train loss: 0.13484, val loss: 0.13695\n",
      "Main effects training epoch: 6, train loss: 0.13033, val loss: 0.13155\n",
      "Main effects training epoch: 7, train loss: 0.13055, val loss: 0.13149\n",
      "Main effects training epoch: 8, train loss: 0.12971, val loss: 0.12988\n",
      "Main effects training epoch: 9, train loss: 0.12862, val loss: 0.12919\n",
      "Main effects training epoch: 10, train loss: 0.12828, val loss: 0.12938\n",
      "Main effects training epoch: 11, train loss: 0.12732, val loss: 0.12789\n",
      "Main effects training epoch: 12, train loss: 0.12659, val loss: 0.12743\n",
      "Main effects training epoch: 13, train loss: 0.12563, val loss: 0.12654\n",
      "Main effects training epoch: 14, train loss: 0.12325, val loss: 0.12456\n",
      "Main effects training epoch: 15, train loss: 0.11949, val loss: 0.12161\n",
      "Main effects training epoch: 16, train loss: 0.11686, val loss: 0.11922\n",
      "Main effects training epoch: 17, train loss: 0.11423, val loss: 0.11562\n",
      "Main effects training epoch: 18, train loss: 0.11484, val loss: 0.11687\n",
      "Main effects training epoch: 19, train loss: 0.11188, val loss: 0.11473\n",
      "Main effects training epoch: 20, train loss: 0.11089, val loss: 0.11324\n",
      "Main effects training epoch: 21, train loss: 0.10849, val loss: 0.11127\n",
      "Main effects training epoch: 22, train loss: 0.10778, val loss: 0.11002\n",
      "Main effects training epoch: 23, train loss: 0.10761, val loss: 0.10934\n",
      "Main effects training epoch: 24, train loss: 0.10695, val loss: 0.10962\n",
      "Main effects training epoch: 25, train loss: 0.10869, val loss: 0.11093\n",
      "Main effects training epoch: 26, train loss: 0.10721, val loss: 0.11016\n",
      "Main effects training epoch: 27, train loss: 0.10692, val loss: 0.10911\n",
      "Main effects training epoch: 28, train loss: 0.10656, val loss: 0.10921\n",
      "Main effects training epoch: 29, train loss: 0.10666, val loss: 0.10915\n",
      "Main effects training epoch: 30, train loss: 0.10688, val loss: 0.10958\n",
      "Main effects training epoch: 31, train loss: 0.10689, val loss: 0.10943\n",
      "Main effects training epoch: 32, train loss: 0.10622, val loss: 0.10918\n",
      "Main effects training epoch: 33, train loss: 0.10631, val loss: 0.10928\n",
      "Main effects training epoch: 34, train loss: 0.10605, val loss: 0.10868\n",
      "Main effects training epoch: 35, train loss: 0.10599, val loss: 0.10879\n",
      "Main effects training epoch: 36, train loss: 0.10605, val loss: 0.10911\n",
      "Main effects training epoch: 37, train loss: 0.10704, val loss: 0.10981\n",
      "Main effects training epoch: 38, train loss: 0.10655, val loss: 0.10926\n",
      "Main effects training epoch: 39, train loss: 0.10607, val loss: 0.10888\n",
      "Main effects training epoch: 40, train loss: 0.10588, val loss: 0.10859\n",
      "Main effects training epoch: 41, train loss: 0.10600, val loss: 0.10889\n",
      "Main effects training epoch: 42, train loss: 0.10616, val loss: 0.10903\n",
      "Main effects training epoch: 43, train loss: 0.10609, val loss: 0.10883\n",
      "Main effects training epoch: 44, train loss: 0.10630, val loss: 0.10899\n",
      "Main effects training epoch: 45, train loss: 0.10686, val loss: 0.10928\n",
      "Main effects training epoch: 46, train loss: 0.10599, val loss: 0.10904\n",
      "Main effects training epoch: 47, train loss: 0.10577, val loss: 0.10887\n",
      "Main effects training epoch: 48, train loss: 0.10584, val loss: 0.10888\n",
      "Main effects training epoch: 49, train loss: 0.10576, val loss: 0.10866\n",
      "Main effects training epoch: 50, train loss: 0.10617, val loss: 0.10903\n",
      "Main effects training epoch: 51, train loss: 0.10592, val loss: 0.10893\n",
      "Main effects training epoch: 52, train loss: 0.10606, val loss: 0.10892\n",
      "Main effects training epoch: 53, train loss: 0.10577, val loss: 0.10888\n",
      "Main effects training epoch: 54, train loss: 0.10586, val loss: 0.10875\n",
      "Main effects training epoch: 55, train loss: 0.10635, val loss: 0.10929\n",
      "Main effects training epoch: 56, train loss: 0.10600, val loss: 0.10895\n",
      "Main effects training epoch: 57, train loss: 0.10614, val loss: 0.10942\n",
      "Main effects training epoch: 58, train loss: 0.10591, val loss: 0.10905\n",
      "Main effects training epoch: 59, train loss: 0.10587, val loss: 0.10866\n",
      "Main effects training epoch: 60, train loss: 0.10580, val loss: 0.10885\n",
      "Main effects training epoch: 61, train loss: 0.10590, val loss: 0.10868\n",
      "Main effects training epoch: 62, train loss: 0.10611, val loss: 0.10911\n",
      "Main effects training epoch: 63, train loss: 0.10587, val loss: 0.10877\n",
      "Main effects training epoch: 64, train loss: 0.10611, val loss: 0.10908\n",
      "Main effects training epoch: 65, train loss: 0.10597, val loss: 0.10888\n",
      "Main effects training epoch: 66, train loss: 0.10579, val loss: 0.10886\n",
      "Main effects training epoch: 67, train loss: 0.10588, val loss: 0.10853\n",
      "Main effects training epoch: 68, train loss: 0.10601, val loss: 0.10926\n",
      "Main effects training epoch: 69, train loss: 0.10583, val loss: 0.10866\n",
      "Main effects training epoch: 70, train loss: 0.10585, val loss: 0.10893\n",
      "Main effects training epoch: 71, train loss: 0.10592, val loss: 0.10879\n",
      "Main effects training epoch: 72, train loss: 0.10582, val loss: 0.10901\n",
      "Main effects training epoch: 73, train loss: 0.10586, val loss: 0.10890\n",
      "Main effects training epoch: 74, train loss: 0.10596, val loss: 0.10893\n",
      "Main effects training epoch: 75, train loss: 0.10614, val loss: 0.10884\n",
      "Main effects training epoch: 76, train loss: 0.10583, val loss: 0.10904\n",
      "Main effects training epoch: 77, train loss: 0.10592, val loss: 0.10890\n",
      "Main effects training epoch: 78, train loss: 0.10580, val loss: 0.10881\n",
      "Main effects training epoch: 79, train loss: 0.10594, val loss: 0.10889\n",
      "Main effects training epoch: 80, train loss: 0.10607, val loss: 0.10888\n",
      "Main effects training epoch: 81, train loss: 0.10581, val loss: 0.10891\n",
      "Main effects training epoch: 82, train loss: 0.10593, val loss: 0.10922\n",
      "Main effects training epoch: 83, train loss: 0.10574, val loss: 0.10873\n",
      "Main effects training epoch: 84, train loss: 0.10573, val loss: 0.10877\n",
      "Main effects training epoch: 85, train loss: 0.10636, val loss: 0.10929\n",
      "Main effects training epoch: 86, train loss: 0.10583, val loss: 0.10874\n",
      "Main effects training epoch: 87, train loss: 0.10614, val loss: 0.10928\n",
      "Main effects training epoch: 88, train loss: 0.10616, val loss: 0.10949\n",
      "Main effects training epoch: 89, train loss: 0.10589, val loss: 0.10865\n",
      "Main effects training epoch: 90, train loss: 0.10604, val loss: 0.10903\n",
      "Main effects training epoch: 91, train loss: 0.10588, val loss: 0.10892\n",
      "Main effects training epoch: 92, train loss: 0.10612, val loss: 0.10925\n",
      "Main effects training epoch: 93, train loss: 0.10583, val loss: 0.10911\n",
      "Main effects training epoch: 94, train loss: 0.10604, val loss: 0.10904\n",
      "Main effects training epoch: 95, train loss: 0.10576, val loss: 0.10903\n",
      "Main effects training epoch: 96, train loss: 0.10568, val loss: 0.10869\n",
      "Main effects training epoch: 97, train loss: 0.10588, val loss: 0.10926\n",
      "Main effects training epoch: 98, train loss: 0.10606, val loss: 0.10910\n",
      "Main effects training epoch: 99, train loss: 0.10649, val loss: 0.10930\n",
      "Main effects training epoch: 100, train loss: 0.10628, val loss: 0.10948\n",
      "Main effects training epoch: 101, train loss: 0.10586, val loss: 0.10912\n",
      "Main effects training epoch: 102, train loss: 0.10570, val loss: 0.10869\n",
      "Main effects training epoch: 103, train loss: 0.10643, val loss: 0.10951\n",
      "Main effects training epoch: 104, train loss: 0.10650, val loss: 0.10946\n",
      "Main effects training epoch: 105, train loss: 0.10626, val loss: 0.10954\n",
      "Main effects training epoch: 106, train loss: 0.10594, val loss: 0.10896\n",
      "Main effects training epoch: 107, train loss: 0.10579, val loss: 0.10878\n",
      "Main effects training epoch: 108, train loss: 0.10607, val loss: 0.10922\n",
      "Main effects training epoch: 109, train loss: 0.10581, val loss: 0.10902\n",
      "Main effects training epoch: 110, train loss: 0.10590, val loss: 0.10892\n",
      "Main effects training epoch: 111, train loss: 0.10605, val loss: 0.10924\n",
      "Main effects training epoch: 112, train loss: 0.10580, val loss: 0.10875\n",
      "Main effects training epoch: 113, train loss: 0.10587, val loss: 0.10900\n",
      "Main effects training epoch: 114, train loss: 0.10581, val loss: 0.10883\n",
      "Main effects training epoch: 115, train loss: 0.10618, val loss: 0.10915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 116, train loss: 0.10615, val loss: 0.10949\n",
      "Main effects training epoch: 117, train loss: 0.10602, val loss: 0.10894\n",
      "Main effects training epoch: 118, train loss: 0.10595, val loss: 0.10914\n",
      "Main effects training epoch: 119, train loss: 0.10618, val loss: 0.10895\n",
      "Main effects training epoch: 120, train loss: 0.10601, val loss: 0.10941\n",
      "Main effects training epoch: 121, train loss: 0.10588, val loss: 0.10869\n",
      "Main effects training epoch: 122, train loss: 0.10576, val loss: 0.10885\n",
      "Main effects training epoch: 123, train loss: 0.10578, val loss: 0.10899\n",
      "Main effects training epoch: 124, train loss: 0.10583, val loss: 0.10934\n",
      "Main effects training epoch: 125, train loss: 0.10610, val loss: 0.10904\n",
      "Main effects training epoch: 126, train loss: 0.10593, val loss: 0.10917\n",
      "Main effects training epoch: 127, train loss: 0.10579, val loss: 0.10863\n",
      "Main effects training epoch: 128, train loss: 0.10583, val loss: 0.10933\n",
      "Main effects training epoch: 129, train loss: 0.10580, val loss: 0.10919\n",
      "Main effects training epoch: 130, train loss: 0.10573, val loss: 0.10874\n",
      "Main effects training epoch: 131, train loss: 0.10601, val loss: 0.10951\n",
      "Main effects training epoch: 132, train loss: 0.10600, val loss: 0.10905\n",
      "Main effects training epoch: 133, train loss: 0.10601, val loss: 0.10931\n",
      "Main effects training epoch: 134, train loss: 0.10566, val loss: 0.10882\n",
      "Main effects training epoch: 135, train loss: 0.10590, val loss: 0.10909\n",
      "Main effects training epoch: 136, train loss: 0.10621, val loss: 0.10930\n",
      "Main effects training epoch: 137, train loss: 0.10597, val loss: 0.10906\n",
      "Main effects training epoch: 138, train loss: 0.10583, val loss: 0.10895\n",
      "Main effects training epoch: 139, train loss: 0.10580, val loss: 0.10894\n",
      "Main effects training epoch: 140, train loss: 0.10591, val loss: 0.10891\n",
      "Main effects training epoch: 141, train loss: 0.10595, val loss: 0.10903\n",
      "Main effects training epoch: 142, train loss: 0.10601, val loss: 0.10932\n",
      "Main effects training epoch: 143, train loss: 0.10596, val loss: 0.10892\n",
      "Main effects training epoch: 144, train loss: 0.10583, val loss: 0.10925\n",
      "Main effects training epoch: 145, train loss: 0.10572, val loss: 0.10887\n",
      "Main effects training epoch: 146, train loss: 0.10574, val loss: 0.10897\n",
      "Main effects training epoch: 147, train loss: 0.10575, val loss: 0.10903\n",
      "Main effects training epoch: 148, train loss: 0.10589, val loss: 0.10894\n",
      "Main effects training epoch: 149, train loss: 0.10577, val loss: 0.10889\n",
      "Main effects training epoch: 150, train loss: 0.10569, val loss: 0.10895\n",
      "Main effects training epoch: 151, train loss: 0.10575, val loss: 0.10884\n",
      "Main effects training epoch: 152, train loss: 0.10606, val loss: 0.10908\n",
      "Main effects training epoch: 153, train loss: 0.10639, val loss: 0.10965\n",
      "Main effects training epoch: 154, train loss: 0.10596, val loss: 0.10920\n",
      "Main effects training epoch: 155, train loss: 0.10578, val loss: 0.10908\n",
      "Main effects training epoch: 156, train loss: 0.10569, val loss: 0.10881\n",
      "Main effects training epoch: 157, train loss: 0.10570, val loss: 0.10898\n",
      "Main effects training epoch: 158, train loss: 0.10595, val loss: 0.10931\n",
      "Main effects training epoch: 159, train loss: 0.10577, val loss: 0.10920\n",
      "Main effects training epoch: 160, train loss: 0.10578, val loss: 0.10935\n",
      "Main effects training epoch: 161, train loss: 0.10579, val loss: 0.10872\n",
      "Main effects training epoch: 162, train loss: 0.10575, val loss: 0.10904\n",
      "Main effects training epoch: 163, train loss: 0.10597, val loss: 0.10913\n",
      "Main effects training epoch: 164, train loss: 0.10605, val loss: 0.10923\n",
      "Main effects training epoch: 165, train loss: 0.10604, val loss: 0.10954\n",
      "Main effects training epoch: 166, train loss: 0.10575, val loss: 0.10883\n",
      "Main effects training epoch: 167, train loss: 0.10578, val loss: 0.10918\n",
      "Main effects training epoch: 168, train loss: 0.10576, val loss: 0.10888\n",
      "Early stop at epoch 168, with validation loss: 0.10888\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.10587, val loss: 0.10915\n",
      "Main effects tuning epoch: 2, train loss: 0.10583, val loss: 0.10863\n",
      "Main effects tuning epoch: 3, train loss: 0.10593, val loss: 0.10918\n",
      "Main effects tuning epoch: 4, train loss: 0.10580, val loss: 0.10863\n",
      "Main effects tuning epoch: 5, train loss: 0.10595, val loss: 0.10896\n",
      "Main effects tuning epoch: 6, train loss: 0.10614, val loss: 0.10907\n",
      "Main effects tuning epoch: 7, train loss: 0.10617, val loss: 0.10891\n",
      "Main effects tuning epoch: 8, train loss: 0.10639, val loss: 0.10952\n",
      "Main effects tuning epoch: 9, train loss: 0.10631, val loss: 0.10952\n",
      "Main effects tuning epoch: 10, train loss: 0.10598, val loss: 0.10913\n",
      "Main effects tuning epoch: 11, train loss: 0.10616, val loss: 0.10872\n",
      "Main effects tuning epoch: 12, train loss: 0.10613, val loss: 0.10940\n",
      "Main effects tuning epoch: 13, train loss: 0.10649, val loss: 0.10913\n",
      "Main effects tuning epoch: 14, train loss: 0.10595, val loss: 0.10906\n",
      "Main effects tuning epoch: 15, train loss: 0.10576, val loss: 0.10901\n",
      "Main effects tuning epoch: 16, train loss: 0.10591, val loss: 0.10867\n",
      "Main effects tuning epoch: 17, train loss: 0.10614, val loss: 0.10929\n",
      "Main effects tuning epoch: 18, train loss: 0.10616, val loss: 0.10914\n",
      "Main effects tuning epoch: 19, train loss: 0.10591, val loss: 0.10901\n",
      "Main effects tuning epoch: 20, train loss: 0.10585, val loss: 0.10893\n",
      "Main effects tuning epoch: 21, train loss: 0.10582, val loss: 0.10851\n",
      "Main effects tuning epoch: 22, train loss: 0.10595, val loss: 0.10917\n",
      "Main effects tuning epoch: 23, train loss: 0.10589, val loss: 0.10893\n",
      "Main effects tuning epoch: 24, train loss: 0.10579, val loss: 0.10899\n",
      "Main effects tuning epoch: 25, train loss: 0.10580, val loss: 0.10907\n",
      "Main effects tuning epoch: 26, train loss: 0.10612, val loss: 0.10929\n",
      "Main effects tuning epoch: 27, train loss: 0.10613, val loss: 0.10892\n",
      "Main effects tuning epoch: 28, train loss: 0.10608, val loss: 0.10898\n",
      "Main effects tuning epoch: 29, train loss: 0.10588, val loss: 0.10904\n",
      "Main effects tuning epoch: 30, train loss: 0.10603, val loss: 0.10875\n",
      "Main effects tuning epoch: 31, train loss: 0.10580, val loss: 0.10885\n",
      "Main effects tuning epoch: 32, train loss: 0.10586, val loss: 0.10892\n",
      "Main effects tuning epoch: 33, train loss: 0.10584, val loss: 0.10877\n",
      "Main effects tuning epoch: 34, train loss: 0.10610, val loss: 0.10890\n",
      "Main effects tuning epoch: 35, train loss: 0.10602, val loss: 0.10950\n",
      "Main effects tuning epoch: 36, train loss: 0.10577, val loss: 0.10870\n",
      "Main effects tuning epoch: 37, train loss: 0.10576, val loss: 0.10890\n",
      "Main effects tuning epoch: 38, train loss: 0.10590, val loss: 0.10894\n",
      "Main effects tuning epoch: 39, train loss: 0.10597, val loss: 0.10915\n",
      "Main effects tuning epoch: 40, train loss: 0.10575, val loss: 0.10856\n",
      "Main effects tuning epoch: 41, train loss: 0.10577, val loss: 0.10899\n",
      "Main effects tuning epoch: 42, train loss: 0.10600, val loss: 0.10888\n",
      "Main effects tuning epoch: 43, train loss: 0.10624, val loss: 0.10938\n",
      "Main effects tuning epoch: 44, train loss: 0.10606, val loss: 0.10863\n",
      "Main effects tuning epoch: 45, train loss: 0.10614, val loss: 0.10961\n",
      "Main effects tuning epoch: 46, train loss: 0.10598, val loss: 0.10874\n",
      "Main effects tuning epoch: 47, train loss: 0.10623, val loss: 0.10916\n",
      "Main effects tuning epoch: 48, train loss: 0.10628, val loss: 0.10910\n",
      "Main effects tuning epoch: 49, train loss: 0.10612, val loss: 0.10902\n",
      "Main effects tuning epoch: 50, train loss: 0.10600, val loss: 0.10922\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.16656, val loss: 0.16504\n",
      "Interaction training epoch: 2, train loss: 0.19799, val loss: 0.19730\n",
      "Interaction training epoch: 3, train loss: 0.07919, val loss: 0.08135\n",
      "Interaction training epoch: 4, train loss: 0.06592, val loss: 0.06695\n",
      "Interaction training epoch: 5, train loss: 0.06719, val loss: 0.06623\n",
      "Interaction training epoch: 6, train loss: 0.07116, val loss: 0.06992\n",
      "Interaction training epoch: 7, train loss: 0.05765, val loss: 0.05869\n",
      "Interaction training epoch: 8, train loss: 0.05791, val loss: 0.05822\n",
      "Interaction training epoch: 9, train loss: 0.06091, val loss: 0.06046\n",
      "Interaction training epoch: 10, train loss: 0.05754, val loss: 0.05788\n",
      "Interaction training epoch: 11, train loss: 0.05624, val loss: 0.05624\n",
      "Interaction training epoch: 12, train loss: 0.05651, val loss: 0.05600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 13, train loss: 0.06101, val loss: 0.06080\n",
      "Interaction training epoch: 14, train loss: 0.05709, val loss: 0.05641\n",
      "Interaction training epoch: 15, train loss: 0.06077, val loss: 0.06101\n",
      "Interaction training epoch: 16, train loss: 0.05239, val loss: 0.05197\n",
      "Interaction training epoch: 17, train loss: 0.05618, val loss: 0.05592\n",
      "Interaction training epoch: 18, train loss: 0.05477, val loss: 0.05468\n",
      "Interaction training epoch: 19, train loss: 0.05152, val loss: 0.05113\n",
      "Interaction training epoch: 20, train loss: 0.05519, val loss: 0.05482\n",
      "Interaction training epoch: 21, train loss: 0.05818, val loss: 0.05633\n",
      "Interaction training epoch: 22, train loss: 0.05614, val loss: 0.05550\n",
      "Interaction training epoch: 23, train loss: 0.05384, val loss: 0.05252\n",
      "Interaction training epoch: 24, train loss: 0.05143, val loss: 0.05136\n",
      "Interaction training epoch: 25, train loss: 0.05185, val loss: 0.05118\n",
      "Interaction training epoch: 26, train loss: 0.05409, val loss: 0.05515\n",
      "Interaction training epoch: 27, train loss: 0.05171, val loss: 0.05189\n",
      "Interaction training epoch: 28, train loss: 0.05284, val loss: 0.05307\n",
      "Interaction training epoch: 29, train loss: 0.05251, val loss: 0.05306\n",
      "Interaction training epoch: 30, train loss: 0.05654, val loss: 0.05667\n",
      "Interaction training epoch: 31, train loss: 0.05008, val loss: 0.05013\n",
      "Interaction training epoch: 32, train loss: 0.05165, val loss: 0.05175\n",
      "Interaction training epoch: 33, train loss: 0.05094, val loss: 0.05147\n",
      "Interaction training epoch: 34, train loss: 0.05154, val loss: 0.05255\n",
      "Interaction training epoch: 35, train loss: 0.05178, val loss: 0.05163\n",
      "Interaction training epoch: 36, train loss: 0.05189, val loss: 0.05137\n",
      "Interaction training epoch: 37, train loss: 0.05261, val loss: 0.05262\n",
      "Interaction training epoch: 38, train loss: 0.05395, val loss: 0.05444\n",
      "Interaction training epoch: 39, train loss: 0.05053, val loss: 0.05093\n",
      "Interaction training epoch: 40, train loss: 0.05186, val loss: 0.05165\n",
      "Interaction training epoch: 41, train loss: 0.05174, val loss: 0.05183\n",
      "Interaction training epoch: 42, train loss: 0.05645, val loss: 0.05707\n",
      "Interaction training epoch: 43, train loss: 0.05130, val loss: 0.05143\n",
      "Interaction training epoch: 44, train loss: 0.05233, val loss: 0.05213\n",
      "Interaction training epoch: 45, train loss: 0.05242, val loss: 0.05266\n",
      "Interaction training epoch: 46, train loss: 0.05234, val loss: 0.05256\n",
      "Interaction training epoch: 47, train loss: 0.05096, val loss: 0.05121\n",
      "Interaction training epoch: 48, train loss: 0.05324, val loss: 0.05317\n",
      "Interaction training epoch: 49, train loss: 0.05143, val loss: 0.05175\n",
      "Interaction training epoch: 50, train loss: 0.05906, val loss: 0.05866\n",
      "Interaction training epoch: 51, train loss: 0.05278, val loss: 0.05220\n",
      "Interaction training epoch: 52, train loss: 0.05429, val loss: 0.05321\n",
      "Interaction training epoch: 53, train loss: 0.05247, val loss: 0.05225\n",
      "Interaction training epoch: 54, train loss: 0.05061, val loss: 0.05129\n",
      "Interaction training epoch: 55, train loss: 0.05239, val loss: 0.05237\n",
      "Interaction training epoch: 56, train loss: 0.05174, val loss: 0.05287\n",
      "Interaction training epoch: 57, train loss: 0.05160, val loss: 0.05111\n",
      "Interaction training epoch: 58, train loss: 0.05180, val loss: 0.05292\n",
      "Interaction training epoch: 59, train loss: 0.04982, val loss: 0.05009\n",
      "Interaction training epoch: 60, train loss: 0.05227, val loss: 0.05368\n",
      "Interaction training epoch: 61, train loss: 0.05217, val loss: 0.05210\n",
      "Interaction training epoch: 62, train loss: 0.05023, val loss: 0.05074\n",
      "Interaction training epoch: 63, train loss: 0.05179, val loss: 0.05284\n",
      "Interaction training epoch: 64, train loss: 0.04959, val loss: 0.05129\n",
      "Interaction training epoch: 65, train loss: 0.05320, val loss: 0.05225\n",
      "Interaction training epoch: 66, train loss: 0.05295, val loss: 0.05326\n",
      "Interaction training epoch: 67, train loss: 0.05028, val loss: 0.05144\n",
      "Interaction training epoch: 68, train loss: 0.05112, val loss: 0.05151\n",
      "Interaction training epoch: 69, train loss: 0.05228, val loss: 0.05248\n",
      "Interaction training epoch: 70, train loss: 0.04994, val loss: 0.05062\n",
      "Interaction training epoch: 71, train loss: 0.05225, val loss: 0.05286\n",
      "Interaction training epoch: 72, train loss: 0.05069, val loss: 0.05180\n",
      "Interaction training epoch: 73, train loss: 0.05077, val loss: 0.05196\n",
      "Interaction training epoch: 74, train loss: 0.04967, val loss: 0.05076\n",
      "Interaction training epoch: 75, train loss: 0.05064, val loss: 0.05071\n",
      "Interaction training epoch: 76, train loss: 0.04886, val loss: 0.05004\n",
      "Interaction training epoch: 77, train loss: 0.05097, val loss: 0.05075\n",
      "Interaction training epoch: 78, train loss: 0.05171, val loss: 0.05128\n",
      "Interaction training epoch: 79, train loss: 0.05178, val loss: 0.05168\n",
      "Interaction training epoch: 80, train loss: 0.04972, val loss: 0.05111\n",
      "Interaction training epoch: 81, train loss: 0.05307, val loss: 0.05407\n",
      "Interaction training epoch: 82, train loss: 0.05287, val loss: 0.05331\n",
      "Interaction training epoch: 83, train loss: 0.05019, val loss: 0.05065\n",
      "Interaction training epoch: 84, train loss: 0.05109, val loss: 0.05177\n",
      "Interaction training epoch: 85, train loss: 0.05241, val loss: 0.05395\n",
      "Interaction training epoch: 86, train loss: 0.05169, val loss: 0.05187\n",
      "Interaction training epoch: 87, train loss: 0.05013, val loss: 0.05031\n",
      "Interaction training epoch: 88, train loss: 0.05249, val loss: 0.05324\n",
      "Interaction training epoch: 89, train loss: 0.04905, val loss: 0.04955\n",
      "Interaction training epoch: 90, train loss: 0.05048, val loss: 0.05104\n",
      "Interaction training epoch: 91, train loss: 0.05291, val loss: 0.05320\n",
      "Interaction training epoch: 92, train loss: 0.05194, val loss: 0.05184\n",
      "Interaction training epoch: 93, train loss: 0.04813, val loss: 0.04857\n",
      "Interaction training epoch: 94, train loss: 0.05434, val loss: 0.05466\n",
      "Interaction training epoch: 95, train loss: 0.05394, val loss: 0.05306\n",
      "Interaction training epoch: 96, train loss: 0.04954, val loss: 0.04995\n",
      "Interaction training epoch: 97, train loss: 0.05003, val loss: 0.05058\n",
      "Interaction training epoch: 98, train loss: 0.05197, val loss: 0.05219\n",
      "Interaction training epoch: 99, train loss: 0.05436, val loss: 0.05280\n",
      "Interaction training epoch: 100, train loss: 0.05350, val loss: 0.05437\n",
      "Interaction training epoch: 101, train loss: 0.04964, val loss: 0.04980\n",
      "Interaction training epoch: 102, train loss: 0.05136, val loss: 0.05087\n",
      "Interaction training epoch: 103, train loss: 0.05162, val loss: 0.05191\n",
      "Interaction training epoch: 104, train loss: 0.05118, val loss: 0.05150\n",
      "Interaction training epoch: 105, train loss: 0.04993, val loss: 0.04937\n",
      "Interaction training epoch: 106, train loss: 0.05171, val loss: 0.05164\n",
      "Interaction training epoch: 107, train loss: 0.05193, val loss: 0.05075\n",
      "Interaction training epoch: 108, train loss: 0.05102, val loss: 0.05183\n",
      "Interaction training epoch: 109, train loss: 0.05261, val loss: 0.05324\n",
      "Interaction training epoch: 110, train loss: 0.05047, val loss: 0.05100\n",
      "Interaction training epoch: 111, train loss: 0.05046, val loss: 0.05063\n",
      "Interaction training epoch: 112, train loss: 0.04811, val loss: 0.04809\n",
      "Interaction training epoch: 113, train loss: 0.05250, val loss: 0.05298\n",
      "Interaction training epoch: 114, train loss: 0.05036, val loss: 0.05112\n",
      "Interaction training epoch: 115, train loss: 0.05119, val loss: 0.05140\n",
      "Interaction training epoch: 116, train loss: 0.04975, val loss: 0.05112\n",
      "Interaction training epoch: 117, train loss: 0.04911, val loss: 0.04994\n",
      "Interaction training epoch: 118, train loss: 0.04990, val loss: 0.05012\n",
      "Interaction training epoch: 119, train loss: 0.04854, val loss: 0.04775\n",
      "Interaction training epoch: 120, train loss: 0.04985, val loss: 0.04987\n",
      "Interaction training epoch: 121, train loss: 0.04779, val loss: 0.04856\n",
      "Interaction training epoch: 122, train loss: 0.04712, val loss: 0.04773\n",
      "Interaction training epoch: 123, train loss: 0.05164, val loss: 0.05255\n",
      "Interaction training epoch: 124, train loss: 0.05327, val loss: 0.05365\n",
      "Interaction training epoch: 125, train loss: 0.05348, val loss: 0.05414\n",
      "Interaction training epoch: 126, train loss: 0.04873, val loss: 0.04978\n",
      "Interaction training epoch: 127, train loss: 0.04866, val loss: 0.04894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 128, train loss: 0.04947, val loss: 0.05037\n",
      "Interaction training epoch: 129, train loss: 0.04701, val loss: 0.04711\n",
      "Interaction training epoch: 130, train loss: 0.04867, val loss: 0.04878\n",
      "Interaction training epoch: 131, train loss: 0.04995, val loss: 0.05004\n",
      "Interaction training epoch: 132, train loss: 0.05004, val loss: 0.05008\n",
      "Interaction training epoch: 133, train loss: 0.04970, val loss: 0.05023\n",
      "Interaction training epoch: 134, train loss: 0.04969, val loss: 0.05041\n",
      "Interaction training epoch: 135, train loss: 0.04826, val loss: 0.04935\n",
      "Interaction training epoch: 136, train loss: 0.04956, val loss: 0.05077\n",
      "Interaction training epoch: 137, train loss: 0.04937, val loss: 0.05116\n",
      "Interaction training epoch: 138, train loss: 0.05042, val loss: 0.05126\n",
      "Interaction training epoch: 139, train loss: 0.04910, val loss: 0.04944\n",
      "Interaction training epoch: 140, train loss: 0.05235, val loss: 0.05329\n",
      "Interaction training epoch: 141, train loss: 0.05151, val loss: 0.05199\n",
      "Interaction training epoch: 142, train loss: 0.04935, val loss: 0.04965\n",
      "Interaction training epoch: 143, train loss: 0.05663, val loss: 0.05667\n",
      "Interaction training epoch: 144, train loss: 0.05217, val loss: 0.05297\n",
      "Interaction training epoch: 145, train loss: 0.05139, val loss: 0.05143\n",
      "Interaction training epoch: 146, train loss: 0.05350, val loss: 0.05388\n",
      "Interaction training epoch: 147, train loss: 0.06195, val loss: 0.06171\n",
      "Interaction training epoch: 148, train loss: 0.05923, val loss: 0.05958\n",
      "Interaction training epoch: 149, train loss: 0.05897, val loss: 0.05842\n",
      "Interaction training epoch: 150, train loss: 0.05688, val loss: 0.05663\n",
      "Interaction training epoch: 151, train loss: 0.05292, val loss: 0.05339\n",
      "Interaction training epoch: 152, train loss: 0.04638, val loss: 0.04708\n",
      "Interaction training epoch: 153, train loss: 0.04830, val loss: 0.05070\n",
      "Interaction training epoch: 154, train loss: 0.04798, val loss: 0.04952\n",
      "Interaction training epoch: 155, train loss: 0.04659, val loss: 0.04786\n",
      "Interaction training epoch: 156, train loss: 0.04767, val loss: 0.04876\n",
      "Interaction training epoch: 157, train loss: 0.04853, val loss: 0.04917\n",
      "Interaction training epoch: 158, train loss: 0.04677, val loss: 0.04803\n",
      "Interaction training epoch: 159, train loss: 0.04901, val loss: 0.04931\n",
      "Interaction training epoch: 160, train loss: 0.04754, val loss: 0.04893\n",
      "Interaction training epoch: 161, train loss: 0.04874, val loss: 0.04950\n",
      "Interaction training epoch: 162, train loss: 0.04595, val loss: 0.04721\n",
      "Interaction training epoch: 163, train loss: 0.04732, val loss: 0.04956\n",
      "Interaction training epoch: 164, train loss: 0.04567, val loss: 0.04749\n",
      "Interaction training epoch: 165, train loss: 0.05013, val loss: 0.05115\n",
      "Interaction training epoch: 166, train loss: 0.04663, val loss: 0.04790\n",
      "Interaction training epoch: 167, train loss: 0.04602, val loss: 0.04784\n",
      "Interaction training epoch: 168, train loss: 0.04789, val loss: 0.04800\n",
      "Interaction training epoch: 169, train loss: 0.04480, val loss: 0.04615\n",
      "Interaction training epoch: 170, train loss: 0.04657, val loss: 0.04712\n",
      "Interaction training epoch: 171, train loss: 0.04515, val loss: 0.04688\n",
      "Interaction training epoch: 172, train loss: 0.04745, val loss: 0.04904\n",
      "Interaction training epoch: 173, train loss: 0.04801, val loss: 0.05050\n",
      "Interaction training epoch: 174, train loss: 0.05297, val loss: 0.05389\n",
      "Interaction training epoch: 175, train loss: 0.04522, val loss: 0.04687\n",
      "Interaction training epoch: 176, train loss: 0.04654, val loss: 0.04709\n",
      "Interaction training epoch: 177, train loss: 0.04462, val loss: 0.04576\n",
      "Interaction training epoch: 178, train loss: 0.04715, val loss: 0.04880\n",
      "Interaction training epoch: 179, train loss: 0.04515, val loss: 0.04673\n",
      "Interaction training epoch: 180, train loss: 0.04652, val loss: 0.04824\n",
      "Interaction training epoch: 181, train loss: 0.04757, val loss: 0.04936\n",
      "Interaction training epoch: 182, train loss: 0.04771, val loss: 0.04909\n",
      "Interaction training epoch: 183, train loss: 0.04999, val loss: 0.05186\n",
      "Interaction training epoch: 184, train loss: 0.04691, val loss: 0.04877\n",
      "Interaction training epoch: 185, train loss: 0.04599, val loss: 0.04763\n",
      "Interaction training epoch: 186, train loss: 0.04839, val loss: 0.04969\n",
      "Interaction training epoch: 187, train loss: 0.04634, val loss: 0.04846\n",
      "Interaction training epoch: 188, train loss: 0.04686, val loss: 0.04828\n",
      "Interaction training epoch: 189, train loss: 0.04527, val loss: 0.04603\n",
      "Interaction training epoch: 190, train loss: 0.04811, val loss: 0.05046\n",
      "Interaction training epoch: 191, train loss: 0.04668, val loss: 0.04794\n",
      "Interaction training epoch: 192, train loss: 0.04580, val loss: 0.04673\n",
      "Interaction training epoch: 193, train loss: 0.04953, val loss: 0.05077\n",
      "Interaction training epoch: 194, train loss: 0.04637, val loss: 0.04745\n",
      "Interaction training epoch: 195, train loss: 0.05291, val loss: 0.05606\n",
      "Interaction training epoch: 196, train loss: 0.04552, val loss: 0.04706\n",
      "Interaction training epoch: 197, train loss: 0.04954, val loss: 0.05175\n",
      "Interaction training epoch: 198, train loss: 0.04504, val loss: 0.04659\n",
      "Interaction training epoch: 199, train loss: 0.04939, val loss: 0.05068\n",
      "Interaction training epoch: 200, train loss: 0.04824, val loss: 0.04969\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########7 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.04624, val loss: 0.04648\n",
      "Interaction tuning epoch: 2, train loss: 0.04654, val loss: 0.04675\n",
      "Interaction tuning epoch: 3, train loss: 0.04688, val loss: 0.04770\n",
      "Interaction tuning epoch: 4, train loss: 0.04703, val loss: 0.04714\n",
      "Interaction tuning epoch: 5, train loss: 0.04871, val loss: 0.04870\n",
      "Interaction tuning epoch: 6, train loss: 0.04715, val loss: 0.04685\n",
      "Interaction tuning epoch: 7, train loss: 0.04753, val loss: 0.04769\n",
      "Interaction tuning epoch: 8, train loss: 0.04804, val loss: 0.04839\n",
      "Interaction tuning epoch: 9, train loss: 0.04656, val loss: 0.04746\n",
      "Interaction tuning epoch: 10, train loss: 0.04708, val loss: 0.04792\n",
      "Interaction tuning epoch: 11, train loss: 0.04593, val loss: 0.04607\n",
      "Interaction tuning epoch: 12, train loss: 0.04889, val loss: 0.05019\n",
      "Interaction tuning epoch: 13, train loss: 0.04756, val loss: 0.04795\n",
      "Interaction tuning epoch: 14, train loss: 0.05056, val loss: 0.04891\n",
      "Interaction tuning epoch: 15, train loss: 0.05649, val loss: 0.05700\n",
      "Interaction tuning epoch: 16, train loss: 0.04770, val loss: 0.04777\n",
      "Interaction tuning epoch: 17, train loss: 0.04665, val loss: 0.04734\n",
      "Interaction tuning epoch: 18, train loss: 0.04709, val loss: 0.04763\n",
      "Interaction tuning epoch: 19, train loss: 0.04762, val loss: 0.04775\n",
      "Interaction tuning epoch: 20, train loss: 0.04647, val loss: 0.04691\n",
      "Interaction tuning epoch: 21, train loss: 0.04858, val loss: 0.04904\n",
      "Interaction tuning epoch: 22, train loss: 0.04763, val loss: 0.04743\n",
      "Interaction tuning epoch: 23, train loss: 0.04883, val loss: 0.04879\n",
      "Interaction tuning epoch: 24, train loss: 0.04608, val loss: 0.04728\n",
      "Interaction tuning epoch: 25, train loss: 0.04999, val loss: 0.04940\n",
      "Interaction tuning epoch: 26, train loss: 0.04916, val loss: 0.04857\n",
      "Interaction tuning epoch: 27, train loss: 0.04784, val loss: 0.04855\n",
      "Interaction tuning epoch: 28, train loss: 0.04702, val loss: 0.04758\n",
      "Interaction tuning epoch: 29, train loss: 0.04685, val loss: 0.04720\n",
      "Interaction tuning epoch: 30, train loss: 0.04688, val loss: 0.04800\n",
      "Interaction tuning epoch: 31, train loss: 0.04656, val loss: 0.04725\n",
      "Interaction tuning epoch: 32, train loss: 0.04698, val loss: 0.04718\n",
      "Interaction tuning epoch: 33, train loss: 0.04648, val loss: 0.04674\n",
      "Interaction tuning epoch: 34, train loss: 0.04622, val loss: 0.04627\n",
      "Interaction tuning epoch: 35, train loss: 0.04695, val loss: 0.04653\n",
      "Interaction tuning epoch: 36, train loss: 0.04638, val loss: 0.04708\n",
      "Interaction tuning epoch: 37, train loss: 0.04517, val loss: 0.04599\n",
      "Interaction tuning epoch: 38, train loss: 0.04594, val loss: 0.04626\n",
      "Interaction tuning epoch: 39, train loss: 0.04806, val loss: 0.04884\n",
      "Interaction tuning epoch: 40, train loss: 0.04762, val loss: 0.04781\n",
      "Interaction tuning epoch: 41, train loss: 0.04762, val loss: 0.04909\n",
      "Interaction tuning epoch: 42, train loss: 0.04746, val loss: 0.04805\n",
      "Interaction tuning epoch: 43, train loss: 0.04604, val loss: 0.04612\n",
      "Interaction tuning epoch: 44, train loss: 0.04636, val loss: 0.04673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 45, train loss: 0.04643, val loss: 0.04712\n",
      "Interaction tuning epoch: 46, train loss: 0.04656, val loss: 0.04678\n",
      "Interaction tuning epoch: 47, train loss: 0.04635, val loss: 0.04667\n",
      "Interaction tuning epoch: 48, train loss: 0.04816, val loss: 0.04882\n",
      "Interaction tuning epoch: 49, train loss: 0.04683, val loss: 0.04711\n",
      "Interaction tuning epoch: 50, train loss: 0.04845, val loss: 0.04905\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 37.35396099090576\n",
      "After the gam stage, training error is 0.04845 , validation error is 0.04905\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 1.232198\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.034122 validation MAE=0.046426,rank=10\n",
      "[SoftImpute] Iter 2: observed MAE=0.030072 validation MAE=0.045516,rank=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 3: observed MAE=0.026945 validation MAE=0.044771,rank=10\n",
      "[SoftImpute] Iter 4: observed MAE=0.024455 validation MAE=0.044179,rank=10\n",
      "[SoftImpute] Iter 5: observed MAE=0.022426 validation MAE=0.043722,rank=10\n",
      "[SoftImpute] Iter 6: observed MAE=0.020734 validation MAE=0.043355,rank=10\n",
      "[SoftImpute] Iter 7: observed MAE=0.019307 validation MAE=0.043021,rank=10\n",
      "[SoftImpute] Iter 8: observed MAE=0.018090 validation MAE=0.042737,rank=10\n",
      "[SoftImpute] Iter 9: observed MAE=0.017033 validation MAE=0.042474,rank=10\n",
      "[SoftImpute] Iter 10: observed MAE=0.016111 validation MAE=0.042235,rank=10\n",
      "[SoftImpute] Iter 11: observed MAE=0.015307 validation MAE=0.042021,rank=10\n",
      "[SoftImpute] Iter 12: observed MAE=0.014593 validation MAE=0.041821,rank=10\n",
      "[SoftImpute] Iter 13: observed MAE=0.013956 validation MAE=0.041636,rank=10\n",
      "[SoftImpute] Iter 14: observed MAE=0.013382 validation MAE=0.041467,rank=10\n",
      "[SoftImpute] Iter 15: observed MAE=0.012864 validation MAE=0.041312,rank=10\n",
      "[SoftImpute] Iter 16: observed MAE=0.012393 validation MAE=0.041181,rank=10\n",
      "[SoftImpute] Iter 17: observed MAE=0.011964 validation MAE=0.041057,rank=10\n",
      "[SoftImpute] Iter 18: observed MAE=0.011573 validation MAE=0.040939,rank=10\n",
      "[SoftImpute] Iter 19: observed MAE=0.011214 validation MAE=0.040829,rank=10\n",
      "[SoftImpute] Iter 20: observed MAE=0.010882 validation MAE=0.040724,rank=10\n",
      "[SoftImpute] Iter 21: observed MAE=0.010573 validation MAE=0.040628,rank=10\n",
      "[SoftImpute] Iter 22: observed MAE=0.010285 validation MAE=0.040541,rank=10\n",
      "[SoftImpute] Iter 23: observed MAE=0.010017 validation MAE=0.040459,rank=10\n",
      "[SoftImpute] Iter 24: observed MAE=0.009768 validation MAE=0.040381,rank=10\n",
      "[SoftImpute] Iter 25: observed MAE=0.009534 validation MAE=0.040305,rank=10\n",
      "[SoftImpute] Iter 26: observed MAE=0.009315 validation MAE=0.040232,rank=10\n",
      "[SoftImpute] Iter 27: observed MAE=0.009109 validation MAE=0.040163,rank=10\n",
      "[SoftImpute] Iter 28: observed MAE=0.008915 validation MAE=0.040097,rank=10\n",
      "[SoftImpute] Iter 29: observed MAE=0.008732 validation MAE=0.040034,rank=10\n",
      "[SoftImpute] Iter 30: observed MAE=0.008560 validation MAE=0.039973,rank=10\n",
      "[SoftImpute] Iter 31: observed MAE=0.008397 validation MAE=0.039915,rank=10\n",
      "[SoftImpute] Iter 32: observed MAE=0.008243 validation MAE=0.039858,rank=10\n",
      "[SoftImpute] Iter 33: observed MAE=0.008097 validation MAE=0.039801,rank=10\n",
      "[SoftImpute] Iter 34: observed MAE=0.007958 validation MAE=0.039745,rank=10\n",
      "[SoftImpute] Iter 35: observed MAE=0.007825 validation MAE=0.039690,rank=10\n",
      "[SoftImpute] Iter 36: observed MAE=0.007698 validation MAE=0.039636,rank=10\n",
      "[SoftImpute] Iter 37: observed MAE=0.007578 validation MAE=0.039584,rank=10\n",
      "[SoftImpute] Iter 38: observed MAE=0.007462 validation MAE=0.039531,rank=10\n",
      "[SoftImpute] Iter 39: observed MAE=0.007352 validation MAE=0.039478,rank=10\n",
      "[SoftImpute] Iter 40: observed MAE=0.007247 validation MAE=0.039425,rank=10\n",
      "[SoftImpute] Iter 41: observed MAE=0.007146 validation MAE=0.039373,rank=10\n",
      "[SoftImpute] Iter 42: observed MAE=0.007049 validation MAE=0.039321,rank=10\n",
      "[SoftImpute] Iter 43: observed MAE=0.006956 validation MAE=0.039271,rank=10\n",
      "[SoftImpute] Iter 44: observed MAE=0.006867 validation MAE=0.039221,rank=10\n",
      "[SoftImpute] Iter 45: observed MAE=0.006781 validation MAE=0.039172,rank=10\n",
      "[SoftImpute] Iter 46: observed MAE=0.006698 validation MAE=0.039124,rank=10\n",
      "[SoftImpute] Iter 47: observed MAE=0.006619 validation MAE=0.039078,rank=10\n",
      "[SoftImpute] Iter 48: observed MAE=0.006543 validation MAE=0.039032,rank=10\n",
      "[SoftImpute] Iter 49: observed MAE=0.006469 validation MAE=0.038986,rank=10\n",
      "[SoftImpute] Iter 50: observed MAE=0.006398 validation MAE=0.038939,rank=10\n",
      "[SoftImpute] Iter 51: observed MAE=0.006330 validation MAE=0.038893,rank=10\n",
      "[SoftImpute] Iter 52: observed MAE=0.006265 validation MAE=0.038848,rank=10\n",
      "[SoftImpute] Iter 53: observed MAE=0.006201 validation MAE=0.038802,rank=10\n",
      "[SoftImpute] Iter 54: observed MAE=0.006140 validation MAE=0.038757,rank=10\n",
      "[SoftImpute] Iter 55: observed MAE=0.006080 validation MAE=0.038712,rank=10\n",
      "[SoftImpute] Iter 56: observed MAE=0.006023 validation MAE=0.038667,rank=10\n",
      "[SoftImpute] Iter 57: observed MAE=0.005967 validation MAE=0.038622,rank=10\n",
      "[SoftImpute] Iter 58: observed MAE=0.005913 validation MAE=0.038577,rank=10\n",
      "[SoftImpute] Iter 59: observed MAE=0.005861 validation MAE=0.038532,rank=10\n",
      "[SoftImpute] Iter 60: observed MAE=0.005810 validation MAE=0.038487,rank=10\n",
      "[SoftImpute] Iter 61: observed MAE=0.005761 validation MAE=0.038442,rank=10\n",
      "[SoftImpute] Iter 62: observed MAE=0.005713 validation MAE=0.038397,rank=10\n",
      "[SoftImpute] Iter 63: observed MAE=0.005667 validation MAE=0.038351,rank=10\n",
      "[SoftImpute] Iter 64: observed MAE=0.005622 validation MAE=0.038306,rank=10\n",
      "[SoftImpute] Iter 65: observed MAE=0.005578 validation MAE=0.038261,rank=10\n",
      "[SoftImpute] Iter 66: observed MAE=0.005535 validation MAE=0.038217,rank=10\n",
      "[SoftImpute] Iter 67: observed MAE=0.005494 validation MAE=0.038173,rank=10\n",
      "[SoftImpute] Iter 68: observed MAE=0.005454 validation MAE=0.038129,rank=10\n",
      "[SoftImpute] Iter 69: observed MAE=0.005415 validation MAE=0.038085,rank=10\n",
      "[SoftImpute] Iter 70: observed MAE=0.005377 validation MAE=0.038042,rank=10\n",
      "[SoftImpute] Iter 71: observed MAE=0.005341 validation MAE=0.038000,rank=10\n",
      "[SoftImpute] Iter 72: observed MAE=0.005305 validation MAE=0.037959,rank=10\n",
      "[SoftImpute] Iter 73: observed MAE=0.005270 validation MAE=0.037918,rank=10\n",
      "[SoftImpute] Iter 74: observed MAE=0.005236 validation MAE=0.037878,rank=10\n",
      "[SoftImpute] Iter 75: observed MAE=0.005203 validation MAE=0.037838,rank=10\n",
      "[SoftImpute] Iter 76: observed MAE=0.005171 validation MAE=0.037799,rank=10\n",
      "[SoftImpute] Iter 77: observed MAE=0.005140 validation MAE=0.037759,rank=10\n",
      "[SoftImpute] Iter 78: observed MAE=0.005109 validation MAE=0.037719,rank=10\n",
      "[SoftImpute] Iter 79: observed MAE=0.005079 validation MAE=0.037679,rank=10\n",
      "[SoftImpute] Iter 80: observed MAE=0.005050 validation MAE=0.037639,rank=10\n",
      "[SoftImpute] Iter 81: observed MAE=0.005022 validation MAE=0.037600,rank=10\n",
      "[SoftImpute] Iter 82: observed MAE=0.004994 validation MAE=0.037560,rank=10\n",
      "[SoftImpute] Iter 83: observed MAE=0.004967 validation MAE=0.037522,rank=10\n",
      "[SoftImpute] Iter 84: observed MAE=0.004940 validation MAE=0.037483,rank=10\n",
      "[SoftImpute] Iter 85: observed MAE=0.004914 validation MAE=0.037445,rank=10\n",
      "[SoftImpute] Iter 86: observed MAE=0.004889 validation MAE=0.037407,rank=10\n",
      "[SoftImpute] Iter 87: observed MAE=0.004864 validation MAE=0.037369,rank=10\n",
      "[SoftImpute] Iter 88: observed MAE=0.004840 validation MAE=0.037331,rank=10\n",
      "[SoftImpute] Iter 89: observed MAE=0.004816 validation MAE=0.037292,rank=10\n",
      "[SoftImpute] Iter 90: observed MAE=0.004793 validation MAE=0.037254,rank=10\n",
      "[SoftImpute] Iter 91: observed MAE=0.004770 validation MAE=0.037216,rank=10\n",
      "[SoftImpute] Iter 92: observed MAE=0.004748 validation MAE=0.037178,rank=10\n",
      "[SoftImpute] Iter 93: observed MAE=0.004726 validation MAE=0.037141,rank=10\n",
      "[SoftImpute] Iter 94: observed MAE=0.004705 validation MAE=0.037104,rank=10\n",
      "[SoftImpute] Iter 95: observed MAE=0.004684 validation MAE=0.037066,rank=10\n",
      "[SoftImpute] Iter 96: observed MAE=0.004663 validation MAE=0.037029,rank=10\n",
      "[SoftImpute] Iter 97: observed MAE=0.004643 validation MAE=0.036992,rank=10\n",
      "[SoftImpute] Iter 98: observed MAE=0.004624 validation MAE=0.036955,rank=10\n",
      "[SoftImpute] Iter 99: observed MAE=0.004604 validation MAE=0.036918,rank=10\n",
      "[SoftImpute] Iter 100: observed MAE=0.004585 validation MAE=0.036881,rank=10\n",
      "[SoftImpute] Iter 101: observed MAE=0.004567 validation MAE=0.036843,rank=10\n",
      "[SoftImpute] Iter 102: observed MAE=0.004549 validation MAE=0.036806,rank=10\n",
      "[SoftImpute] Iter 103: observed MAE=0.004531 validation MAE=0.036770,rank=10\n",
      "[SoftImpute] Iter 104: observed MAE=0.004513 validation MAE=0.036735,rank=10\n",
      "[SoftImpute] Iter 105: observed MAE=0.004496 validation MAE=0.036700,rank=10\n",
      "[SoftImpute] Iter 106: observed MAE=0.004479 validation MAE=0.036665,rank=10\n",
      "[SoftImpute] Iter 107: observed MAE=0.004463 validation MAE=0.036631,rank=10\n",
      "[SoftImpute] Iter 108: observed MAE=0.004447 validation MAE=0.036597,rank=10\n",
      "[SoftImpute] Iter 109: observed MAE=0.004431 validation MAE=0.036563,rank=10\n",
      "[SoftImpute] Iter 110: observed MAE=0.004415 validation MAE=0.036529,rank=10\n",
      "[SoftImpute] Iter 111: observed MAE=0.004400 validation MAE=0.036495,rank=10\n",
      "[SoftImpute] Iter 112: observed MAE=0.004385 validation MAE=0.036462,rank=10\n",
      "[SoftImpute] Iter 113: observed MAE=0.004370 validation MAE=0.036428,rank=10\n",
      "[SoftImpute] Iter 114: observed MAE=0.004355 validation MAE=0.036395,rank=10\n",
      "[SoftImpute] Iter 115: observed MAE=0.004341 validation MAE=0.036362,rank=10\n",
      "[SoftImpute] Iter 116: observed MAE=0.004327 validation MAE=0.036329,rank=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 117: observed MAE=0.004313 validation MAE=0.036296,rank=10\n",
      "[SoftImpute] Iter 118: observed MAE=0.004300 validation MAE=0.036263,rank=10\n",
      "[SoftImpute] Iter 119: observed MAE=0.004286 validation MAE=0.036230,rank=10\n",
      "[SoftImpute] Iter 120: observed MAE=0.004273 validation MAE=0.036197,rank=10\n",
      "[SoftImpute] Iter 121: observed MAE=0.004260 validation MAE=0.036164,rank=10\n",
      "[SoftImpute] Iter 122: observed MAE=0.004248 validation MAE=0.036131,rank=10\n",
      "[SoftImpute] Iter 123: observed MAE=0.004235 validation MAE=0.036098,rank=10\n",
      "[SoftImpute] Iter 124: observed MAE=0.004223 validation MAE=0.036065,rank=10\n",
      "[SoftImpute] Iter 125: observed MAE=0.004211 validation MAE=0.036033,rank=10\n",
      "[SoftImpute] Iter 126: observed MAE=0.004199 validation MAE=0.036001,rank=10\n",
      "[SoftImpute] Iter 127: observed MAE=0.004187 validation MAE=0.035969,rank=10\n",
      "[SoftImpute] Iter 128: observed MAE=0.004175 validation MAE=0.035936,rank=10\n",
      "[SoftImpute] Iter 129: observed MAE=0.004164 validation MAE=0.035904,rank=10\n",
      "[SoftImpute] Iter 130: observed MAE=0.004153 validation MAE=0.035872,rank=10\n",
      "[SoftImpute] Iter 131: observed MAE=0.004142 validation MAE=0.035840,rank=10\n",
      "[SoftImpute] Iter 132: observed MAE=0.004131 validation MAE=0.035808,rank=10\n",
      "[SoftImpute] Iter 133: observed MAE=0.004120 validation MAE=0.035776,rank=10\n",
      "[SoftImpute] Iter 134: observed MAE=0.004110 validation MAE=0.035744,rank=10\n",
      "[SoftImpute] Iter 135: observed MAE=0.004099 validation MAE=0.035712,rank=10\n",
      "[SoftImpute] Iter 136: observed MAE=0.004089 validation MAE=0.035680,rank=10\n",
      "[SoftImpute] Iter 137: observed MAE=0.004079 validation MAE=0.035648,rank=10\n",
      "[SoftImpute] Iter 138: observed MAE=0.004069 validation MAE=0.035617,rank=10\n",
      "[SoftImpute] Iter 139: observed MAE=0.004059 validation MAE=0.035585,rank=10\n",
      "[SoftImpute] Iter 140: observed MAE=0.004050 validation MAE=0.035554,rank=10\n",
      "[SoftImpute] Iter 141: observed MAE=0.004040 validation MAE=0.035523,rank=10\n",
      "[SoftImpute] Iter 142: observed MAE=0.004031 validation MAE=0.035493,rank=10\n",
      "[SoftImpute] Iter 143: observed MAE=0.004021 validation MAE=0.035462,rank=10\n",
      "[SoftImpute] Iter 144: observed MAE=0.004012 validation MAE=0.035432,rank=10\n",
      "[SoftImpute] Iter 145: observed MAE=0.004003 validation MAE=0.035401,rank=10\n",
      "[SoftImpute] Iter 146: observed MAE=0.003994 validation MAE=0.035371,rank=10\n",
      "[SoftImpute] Iter 147: observed MAE=0.003985 validation MAE=0.035340,rank=10\n",
      "[SoftImpute] Iter 148: observed MAE=0.003977 validation MAE=0.035310,rank=10\n",
      "[SoftImpute] Iter 149: observed MAE=0.003968 validation MAE=0.035279,rank=10\n",
      "[SoftImpute] Iter 150: observed MAE=0.003960 validation MAE=0.035249,rank=10\n",
      "[SoftImpute] Iter 151: observed MAE=0.003951 validation MAE=0.035218,rank=10\n",
      "[SoftImpute] Iter 152: observed MAE=0.003943 validation MAE=0.035188,rank=10\n",
      "[SoftImpute] Iter 153: observed MAE=0.003935 validation MAE=0.035158,rank=10\n",
      "[SoftImpute] Iter 154: observed MAE=0.003927 validation MAE=0.035127,rank=10\n",
      "[SoftImpute] Iter 155: observed MAE=0.003919 validation MAE=0.035097,rank=10\n",
      "[SoftImpute] Iter 156: observed MAE=0.003911 validation MAE=0.035067,rank=10\n",
      "[SoftImpute] Iter 157: observed MAE=0.003904 validation MAE=0.035037,rank=10\n",
      "[SoftImpute] Iter 158: observed MAE=0.003896 validation MAE=0.035008,rank=10\n",
      "[SoftImpute] Iter 159: observed MAE=0.003889 validation MAE=0.034978,rank=10\n",
      "[SoftImpute] Iter 160: observed MAE=0.003881 validation MAE=0.034948,rank=10\n",
      "[SoftImpute] Iter 161: observed MAE=0.003874 validation MAE=0.034918,rank=10\n",
      "[SoftImpute] Iter 162: observed MAE=0.003867 validation MAE=0.034889,rank=10\n",
      "[SoftImpute] Iter 163: observed MAE=0.003860 validation MAE=0.034859,rank=10\n",
      "[SoftImpute] Iter 164: observed MAE=0.003853 validation MAE=0.034829,rank=10\n",
      "[SoftImpute] Iter 165: observed MAE=0.003846 validation MAE=0.034800,rank=10\n",
      "[SoftImpute] Iter 166: observed MAE=0.003839 validation MAE=0.034770,rank=10\n",
      "[SoftImpute] Iter 167: observed MAE=0.003832 validation MAE=0.034740,rank=10\n",
      "[SoftImpute] Iter 168: observed MAE=0.003825 validation MAE=0.034711,rank=10\n",
      "[SoftImpute] Iter 169: observed MAE=0.003819 validation MAE=0.034681,rank=10\n",
      "[SoftImpute] Iter 170: observed MAE=0.003812 validation MAE=0.034652,rank=10\n",
      "[SoftImpute] Iter 171: observed MAE=0.003806 validation MAE=0.034623,rank=10\n",
      "[SoftImpute] Iter 172: observed MAE=0.003799 validation MAE=0.034594,rank=10\n",
      "[SoftImpute] Iter 173: observed MAE=0.003793 validation MAE=0.034565,rank=10\n",
      "[SoftImpute] Iter 174: observed MAE=0.003787 validation MAE=0.034535,rank=10\n",
      "[SoftImpute] Iter 175: observed MAE=0.003781 validation MAE=0.034506,rank=10\n",
      "[SoftImpute] Iter 176: observed MAE=0.003775 validation MAE=0.034478,rank=10\n",
      "[SoftImpute] Iter 177: observed MAE=0.003769 validation MAE=0.034449,rank=10\n",
      "[SoftImpute] Iter 178: observed MAE=0.003763 validation MAE=0.034420,rank=10\n",
      "[SoftImpute] Iter 179: observed MAE=0.003757 validation MAE=0.034391,rank=10\n",
      "[SoftImpute] Iter 180: observed MAE=0.003751 validation MAE=0.034362,rank=10\n",
      "[SoftImpute] Iter 181: observed MAE=0.003745 validation MAE=0.034334,rank=10\n",
      "[SoftImpute] Iter 182: observed MAE=0.003740 validation MAE=0.034305,rank=10\n",
      "[SoftImpute] Iter 183: observed MAE=0.003734 validation MAE=0.034277,rank=10\n",
      "[SoftImpute] Iter 184: observed MAE=0.003729 validation MAE=0.034248,rank=10\n",
      "[SoftImpute] Iter 185: observed MAE=0.003723 validation MAE=0.034220,rank=10\n",
      "[SoftImpute] Iter 186: observed MAE=0.003718 validation MAE=0.034191,rank=10\n",
      "[SoftImpute] Iter 187: observed MAE=0.003712 validation MAE=0.034163,rank=10\n",
      "[SoftImpute] Iter 188: observed MAE=0.003707 validation MAE=0.034135,rank=10\n",
      "[SoftImpute] Iter 189: observed MAE=0.003702 validation MAE=0.034106,rank=10\n",
      "[SoftImpute] Iter 190: observed MAE=0.003697 validation MAE=0.034078,rank=10\n",
      "[SoftImpute] Iter 191: observed MAE=0.003692 validation MAE=0.034050,rank=10\n",
      "[SoftImpute] Iter 192: observed MAE=0.003687 validation MAE=0.034022,rank=10\n",
      "[SoftImpute] Iter 193: observed MAE=0.003682 validation MAE=0.033994,rank=10\n",
      "[SoftImpute] Iter 194: observed MAE=0.003677 validation MAE=0.033966,rank=10\n",
      "[SoftImpute] Iter 195: observed MAE=0.003672 validation MAE=0.033938,rank=10\n",
      "[SoftImpute] Iter 196: observed MAE=0.003667 validation MAE=0.033910,rank=10\n",
      "[SoftImpute] Iter 197: observed MAE=0.003663 validation MAE=0.033882,rank=10\n",
      "[SoftImpute] Iter 198: observed MAE=0.003658 validation MAE=0.033854,rank=10\n",
      "[SoftImpute] Iter 199: observed MAE=0.003654 validation MAE=0.033826,rank=10\n",
      "[SoftImpute] Iter 200: observed MAE=0.003649 validation MAE=0.033798,rank=10\n",
      "[SoftImpute] Iter 201: observed MAE=0.003644 validation MAE=0.033771,rank=10\n",
      "[SoftImpute] Iter 202: observed MAE=0.003640 validation MAE=0.033743,rank=10\n",
      "[SoftImpute] Iter 203: observed MAE=0.003636 validation MAE=0.033715,rank=10\n",
      "[SoftImpute] Iter 204: observed MAE=0.003631 validation MAE=0.033688,rank=10\n",
      "[SoftImpute] Iter 205: observed MAE=0.003627 validation MAE=0.033660,rank=10\n",
      "[SoftImpute] Iter 206: observed MAE=0.003623 validation MAE=0.033633,rank=10\n",
      "[SoftImpute] Iter 207: observed MAE=0.003619 validation MAE=0.033605,rank=10\n",
      "[SoftImpute] Iter 208: observed MAE=0.003614 validation MAE=0.033578,rank=10\n",
      "[SoftImpute] Iter 209: observed MAE=0.003610 validation MAE=0.033550,rank=10\n",
      "[SoftImpute] Iter 210: observed MAE=0.003606 validation MAE=0.033523,rank=10\n",
      "[SoftImpute] Iter 211: observed MAE=0.003602 validation MAE=0.033495,rank=10\n",
      "[SoftImpute] Iter 212: observed MAE=0.003598 validation MAE=0.033468,rank=10\n",
      "[SoftImpute] Iter 213: observed MAE=0.003594 validation MAE=0.033441,rank=10\n",
      "[SoftImpute] Iter 214: observed MAE=0.003590 validation MAE=0.033414,rank=10\n",
      "[SoftImpute] Iter 215: observed MAE=0.003586 validation MAE=0.033387,rank=10\n",
      "[SoftImpute] Iter 216: observed MAE=0.003583 validation MAE=0.033360,rank=10\n",
      "[SoftImpute] Iter 217: observed MAE=0.003579 validation MAE=0.033333,rank=10\n",
      "[SoftImpute] Iter 218: observed MAE=0.003575 validation MAE=0.033307,rank=10\n",
      "[SoftImpute] Iter 219: observed MAE=0.003571 validation MAE=0.033280,rank=10\n",
      "[SoftImpute] Iter 220: observed MAE=0.003568 validation MAE=0.033254,rank=10\n",
      "[SoftImpute] Iter 221: observed MAE=0.003564 validation MAE=0.033228,rank=10\n",
      "[SoftImpute] Iter 222: observed MAE=0.003561 validation MAE=0.033201,rank=10\n",
      "[SoftImpute] Iter 223: observed MAE=0.003557 validation MAE=0.033175,rank=10\n",
      "[SoftImpute] Iter 224: observed MAE=0.003554 validation MAE=0.033149,rank=10\n",
      "[SoftImpute] Iter 225: observed MAE=0.003550 validation MAE=0.033123,rank=10\n",
      "[SoftImpute] Iter 226: observed MAE=0.003547 validation MAE=0.033098,rank=10\n",
      "[SoftImpute] Iter 227: observed MAE=0.003543 validation MAE=0.033072,rank=10\n",
      "[SoftImpute] Iter 228: observed MAE=0.003540 validation MAE=0.033046,rank=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 229: observed MAE=0.003537 validation MAE=0.033021,rank=10\n",
      "[SoftImpute] Iter 230: observed MAE=0.003533 validation MAE=0.032996,rank=10\n",
      "[SoftImpute] Iter 231: observed MAE=0.003530 validation MAE=0.032970,rank=10\n",
      "[SoftImpute] Iter 232: observed MAE=0.003527 validation MAE=0.032945,rank=10\n",
      "[SoftImpute] Iter 233: observed MAE=0.003523 validation MAE=0.032920,rank=10\n",
      "[SoftImpute] Iter 234: observed MAE=0.003520 validation MAE=0.032894,rank=10\n",
      "[SoftImpute] Iter 235: observed MAE=0.003517 validation MAE=0.032869,rank=10\n",
      "[SoftImpute] Iter 236: observed MAE=0.003514 validation MAE=0.032844,rank=10\n",
      "[SoftImpute] Iter 237: observed MAE=0.003511 validation MAE=0.032819,rank=10\n",
      "[SoftImpute] Iter 238: observed MAE=0.003508 validation MAE=0.032794,rank=10\n",
      "[SoftImpute] Iter 239: observed MAE=0.003505 validation MAE=0.032769,rank=10\n",
      "[SoftImpute] Iter 240: observed MAE=0.003502 validation MAE=0.032744,rank=10\n",
      "[SoftImpute] Iter 241: observed MAE=0.003499 validation MAE=0.032719,rank=10\n",
      "[SoftImpute] Iter 242: observed MAE=0.003496 validation MAE=0.032695,rank=10\n",
      "[SoftImpute] Iter 243: observed MAE=0.003493 validation MAE=0.032670,rank=10\n",
      "[SoftImpute] Iter 244: observed MAE=0.003490 validation MAE=0.032645,rank=10\n",
      "[SoftImpute] Iter 245: observed MAE=0.003487 validation MAE=0.032621,rank=10\n",
      "[SoftImpute] Iter 246: observed MAE=0.003484 validation MAE=0.032596,rank=10\n",
      "[SoftImpute] Iter 247: observed MAE=0.003481 validation MAE=0.032571,rank=10\n",
      "[SoftImpute] Iter 248: observed MAE=0.003479 validation MAE=0.032547,rank=10\n",
      "[SoftImpute] Iter 249: observed MAE=0.003476 validation MAE=0.032522,rank=10\n",
      "[SoftImpute] Iter 250: observed MAE=0.003473 validation MAE=0.032498,rank=10\n",
      "[SoftImpute] Iter 251: observed MAE=0.003471 validation MAE=0.032473,rank=10\n",
      "[SoftImpute] Iter 252: observed MAE=0.003468 validation MAE=0.032449,rank=10\n",
      "[SoftImpute] Iter 253: observed MAE=0.003465 validation MAE=0.032425,rank=10\n",
      "[SoftImpute] Iter 254: observed MAE=0.003463 validation MAE=0.032400,rank=10\n",
      "[SoftImpute] Iter 255: observed MAE=0.003460 validation MAE=0.032376,rank=10\n",
      "[SoftImpute] Iter 256: observed MAE=0.003457 validation MAE=0.032352,rank=10\n",
      "[SoftImpute] Iter 257: observed MAE=0.003455 validation MAE=0.032328,rank=10\n",
      "[SoftImpute] Iter 258: observed MAE=0.003452 validation MAE=0.032303,rank=10\n",
      "[SoftImpute] Iter 259: observed MAE=0.003450 validation MAE=0.032279,rank=10\n",
      "[SoftImpute] Iter 260: observed MAE=0.003447 validation MAE=0.032255,rank=10\n",
      "[SoftImpute] Iter 261: observed MAE=0.003445 validation MAE=0.032232,rank=10\n",
      "[SoftImpute] Iter 262: observed MAE=0.003442 validation MAE=0.032208,rank=10\n",
      "[SoftImpute] Iter 263: observed MAE=0.003440 validation MAE=0.032184,rank=10\n",
      "[SoftImpute] Iter 264: observed MAE=0.003438 validation MAE=0.032161,rank=10\n",
      "[SoftImpute] Iter 265: observed MAE=0.003435 validation MAE=0.032137,rank=10\n",
      "[SoftImpute] Iter 266: observed MAE=0.003433 validation MAE=0.032114,rank=10\n",
      "[SoftImpute] Iter 267: observed MAE=0.003430 validation MAE=0.032090,rank=10\n",
      "[SoftImpute] Iter 268: observed MAE=0.003428 validation MAE=0.032067,rank=10\n",
      "[SoftImpute] Iter 269: observed MAE=0.003426 validation MAE=0.032044,rank=10\n",
      "[SoftImpute] Iter 270: observed MAE=0.003424 validation MAE=0.032021,rank=10\n",
      "[SoftImpute] Iter 271: observed MAE=0.003421 validation MAE=0.031998,rank=10\n",
      "[SoftImpute] Iter 272: observed MAE=0.003419 validation MAE=0.031975,rank=10\n",
      "[SoftImpute] Iter 273: observed MAE=0.003417 validation MAE=0.031952,rank=10\n",
      "[SoftImpute] Iter 274: observed MAE=0.003415 validation MAE=0.031929,rank=10\n",
      "[SoftImpute] Iter 275: observed MAE=0.003412 validation MAE=0.031906,rank=10\n",
      "[SoftImpute] Iter 276: observed MAE=0.003410 validation MAE=0.031883,rank=10\n",
      "[SoftImpute] Iter 277: observed MAE=0.003408 validation MAE=0.031860,rank=10\n",
      "[SoftImpute] Iter 278: observed MAE=0.003406 validation MAE=0.031838,rank=10\n",
      "[SoftImpute] Iter 279: observed MAE=0.003404 validation MAE=0.031815,rank=10\n",
      "[SoftImpute] Iter 280: observed MAE=0.003402 validation MAE=0.031792,rank=10\n",
      "[SoftImpute] Iter 281: observed MAE=0.003400 validation MAE=0.031770,rank=10\n",
      "[SoftImpute] Iter 282: observed MAE=0.003398 validation MAE=0.031747,rank=10\n",
      "[SoftImpute] Iter 283: observed MAE=0.003396 validation MAE=0.031725,rank=10\n",
      "[SoftImpute] Iter 284: observed MAE=0.003394 validation MAE=0.031702,rank=10\n",
      "[SoftImpute] Iter 285: observed MAE=0.003391 validation MAE=0.031680,rank=10\n",
      "[SoftImpute] Iter 286: observed MAE=0.003390 validation MAE=0.031658,rank=10\n",
      "[SoftImpute] Iter 287: observed MAE=0.003388 validation MAE=0.031636,rank=10\n",
      "[SoftImpute] Iter 288: observed MAE=0.003386 validation MAE=0.031613,rank=10\n",
      "[SoftImpute] Iter 289: observed MAE=0.003384 validation MAE=0.031591,rank=10\n",
      "[SoftImpute] Iter 290: observed MAE=0.003382 validation MAE=0.031569,rank=10\n",
      "[SoftImpute] Iter 291: observed MAE=0.003380 validation MAE=0.031547,rank=10\n",
      "[SoftImpute] Iter 292: observed MAE=0.003378 validation MAE=0.031525,rank=10\n",
      "[SoftImpute] Iter 293: observed MAE=0.003376 validation MAE=0.031503,rank=10\n",
      "[SoftImpute] Iter 294: observed MAE=0.003374 validation MAE=0.031482,rank=10\n",
      "[SoftImpute] Iter 295: observed MAE=0.003372 validation MAE=0.031460,rank=10\n",
      "[SoftImpute] Iter 296: observed MAE=0.003370 validation MAE=0.031438,rank=10\n",
      "[SoftImpute] Iter 297: observed MAE=0.003369 validation MAE=0.031417,rank=10\n",
      "[SoftImpute] Iter 298: observed MAE=0.003367 validation MAE=0.031395,rank=10\n",
      "[SoftImpute] Iter 299: observed MAE=0.003365 validation MAE=0.031374,rank=10\n",
      "[SoftImpute] Iter 300: observed MAE=0.003363 validation MAE=0.031352,rank=10\n",
      "[SoftImpute] Iter 301: observed MAE=0.003361 validation MAE=0.031331,rank=10\n",
      "[SoftImpute] Iter 302: observed MAE=0.003360 validation MAE=0.031310,rank=10\n",
      "[SoftImpute] Iter 303: observed MAE=0.003358 validation MAE=0.031289,rank=10\n",
      "[SoftImpute] Iter 304: observed MAE=0.003356 validation MAE=0.031268,rank=10\n",
      "[SoftImpute] Iter 305: observed MAE=0.003354 validation MAE=0.031248,rank=10\n",
      "[SoftImpute] Iter 306: observed MAE=0.003353 validation MAE=0.031227,rank=10\n",
      "[SoftImpute] Iter 307: observed MAE=0.003351 validation MAE=0.031207,rank=10\n",
      "[SoftImpute] Iter 308: observed MAE=0.003349 validation MAE=0.031186,rank=10\n",
      "[SoftImpute] Iter 309: observed MAE=0.003348 validation MAE=0.031166,rank=10\n",
      "[SoftImpute] Iter 310: observed MAE=0.003346 validation MAE=0.031146,rank=10\n",
      "[SoftImpute] Iter 311: observed MAE=0.003344 validation MAE=0.031125,rank=10\n",
      "[SoftImpute] Iter 312: observed MAE=0.003343 validation MAE=0.031105,rank=10\n",
      "[SoftImpute] Iter 313: observed MAE=0.003341 validation MAE=0.031085,rank=10\n",
      "[SoftImpute] Iter 314: observed MAE=0.003340 validation MAE=0.031064,rank=10\n",
      "[SoftImpute] Iter 315: observed MAE=0.003338 validation MAE=0.031044,rank=10\n",
      "[SoftImpute] Iter 316: observed MAE=0.003336 validation MAE=0.031024,rank=10\n",
      "[SoftImpute] Iter 317: observed MAE=0.003335 validation MAE=0.031004,rank=10\n",
      "[SoftImpute] Iter 318: observed MAE=0.003333 validation MAE=0.030984,rank=10\n",
      "[SoftImpute] Iter 319: observed MAE=0.003332 validation MAE=0.030964,rank=10\n",
      "[SoftImpute] Iter 320: observed MAE=0.003330 validation MAE=0.030944,rank=10\n",
      "[SoftImpute] Iter 321: observed MAE=0.003329 validation MAE=0.030924,rank=10\n",
      "[SoftImpute] Iter 322: observed MAE=0.003327 validation MAE=0.030904,rank=10\n",
      "[SoftImpute] Iter 323: observed MAE=0.003326 validation MAE=0.030884,rank=10\n",
      "[SoftImpute] Iter 324: observed MAE=0.003324 validation MAE=0.030864,rank=10\n",
      "[SoftImpute] Iter 325: observed MAE=0.003323 validation MAE=0.030844,rank=10\n",
      "[SoftImpute] Iter 326: observed MAE=0.003321 validation MAE=0.030824,rank=10\n",
      "[SoftImpute] Iter 327: observed MAE=0.003320 validation MAE=0.030805,rank=10\n",
      "[SoftImpute] Iter 328: observed MAE=0.003319 validation MAE=0.030785,rank=10\n",
      "[SoftImpute] Iter 329: observed MAE=0.003317 validation MAE=0.030766,rank=10\n",
      "[SoftImpute] Iter 330: observed MAE=0.003316 validation MAE=0.030746,rank=10\n",
      "[SoftImpute] Iter 331: observed MAE=0.003314 validation MAE=0.030727,rank=10\n",
      "[SoftImpute] Iter 332: observed MAE=0.003313 validation MAE=0.030708,rank=10\n",
      "[SoftImpute] Iter 333: observed MAE=0.003311 validation MAE=0.030689,rank=10\n",
      "[SoftImpute] Iter 334: observed MAE=0.003310 validation MAE=0.030670,rank=10\n",
      "[SoftImpute] Iter 335: observed MAE=0.003309 validation MAE=0.030651,rank=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 336: observed MAE=0.003307 validation MAE=0.030633,rank=10\n",
      "[SoftImpute] Iter 337: observed MAE=0.003306 validation MAE=0.030614,rank=10\n",
      "[SoftImpute] Iter 338: observed MAE=0.003305 validation MAE=0.030595,rank=10\n",
      "[SoftImpute] Iter 339: observed MAE=0.003303 validation MAE=0.030576,rank=10\n",
      "[SoftImpute] Iter 340: observed MAE=0.003302 validation MAE=0.030558,rank=10\n",
      "[SoftImpute] Iter 341: observed MAE=0.003301 validation MAE=0.030539,rank=10\n",
      "[SoftImpute] Iter 342: observed MAE=0.003299 validation MAE=0.030521,rank=10\n",
      "[SoftImpute] Iter 343: observed MAE=0.003298 validation MAE=0.030502,rank=10\n",
      "[SoftImpute] Iter 344: observed MAE=0.003297 validation MAE=0.030484,rank=10\n",
      "[SoftImpute] Iter 345: observed MAE=0.003295 validation MAE=0.030465,rank=10\n",
      "[SoftImpute] Iter 346: observed MAE=0.003294 validation MAE=0.030447,rank=10\n",
      "[SoftImpute] Iter 347: observed MAE=0.003293 validation MAE=0.030429,rank=10\n",
      "[SoftImpute] Iter 348: observed MAE=0.003292 validation MAE=0.030410,rank=10\n",
      "[SoftImpute] Iter 349: observed MAE=0.003290 validation MAE=0.030392,rank=10\n",
      "[SoftImpute] Iter 350: observed MAE=0.003289 validation MAE=0.030374,rank=10\n",
      "[SoftImpute] Iter 351: observed MAE=0.003288 validation MAE=0.030356,rank=10\n",
      "[SoftImpute] Iter 352: observed MAE=0.003287 validation MAE=0.030338,rank=10\n",
      "[SoftImpute] Iter 353: observed MAE=0.003285 validation MAE=0.030319,rank=10\n",
      "[SoftImpute] Iter 354: observed MAE=0.003284 validation MAE=0.030301,rank=10\n",
      "[SoftImpute] Iter 355: observed MAE=0.003283 validation MAE=0.030283,rank=10\n",
      "[SoftImpute] Iter 356: observed MAE=0.003282 validation MAE=0.030265,rank=10\n",
      "[SoftImpute] Iter 357: observed MAE=0.003281 validation MAE=0.030248,rank=10\n",
      "[SoftImpute] Iter 358: observed MAE=0.003279 validation MAE=0.030230,rank=10\n",
      "[SoftImpute] Iter 359: observed MAE=0.003278 validation MAE=0.030213,rank=10\n",
      "[SoftImpute] Iter 360: observed MAE=0.003277 validation MAE=0.030195,rank=10\n",
      "[SoftImpute] Iter 361: observed MAE=0.003276 validation MAE=0.030178,rank=10\n",
      "[SoftImpute] Iter 362: observed MAE=0.003275 validation MAE=0.030160,rank=10\n",
      "[SoftImpute] Iter 363: observed MAE=0.003274 validation MAE=0.030143,rank=10\n",
      "[SoftImpute] Iter 364: observed MAE=0.003272 validation MAE=0.030126,rank=10\n",
      "[SoftImpute] Iter 365: observed MAE=0.003271 validation MAE=0.030108,rank=10\n",
      "[SoftImpute] Iter 366: observed MAE=0.003270 validation MAE=0.030091,rank=10\n",
      "[SoftImpute] Iter 367: observed MAE=0.003269 validation MAE=0.030074,rank=10\n",
      "[SoftImpute] Iter 368: observed MAE=0.003268 validation MAE=0.030057,rank=10\n",
      "[SoftImpute] Iter 369: observed MAE=0.003267 validation MAE=0.030039,rank=10\n",
      "[SoftImpute] Iter 370: observed MAE=0.003266 validation MAE=0.030022,rank=10\n",
      "[SoftImpute] Iter 371: observed MAE=0.003265 validation MAE=0.030005,rank=10\n",
      "[SoftImpute] Iter 372: observed MAE=0.003264 validation MAE=0.029988,rank=10\n",
      "[SoftImpute] Iter 373: observed MAE=0.003263 validation MAE=0.029971,rank=10\n",
      "[SoftImpute] Iter 374: observed MAE=0.003261 validation MAE=0.029954,rank=10\n",
      "[SoftImpute] Iter 375: observed MAE=0.003260 validation MAE=0.029937,rank=10\n",
      "[SoftImpute] Iter 376: observed MAE=0.003259 validation MAE=0.029920,rank=10\n",
      "[SoftImpute] Iter 377: observed MAE=0.003258 validation MAE=0.029903,rank=10\n",
      "[SoftImpute] Iter 378: observed MAE=0.003257 validation MAE=0.029886,rank=10\n",
      "[SoftImpute] Iter 379: observed MAE=0.003256 validation MAE=0.029869,rank=10\n",
      "[SoftImpute] Iter 380: observed MAE=0.003255 validation MAE=0.029852,rank=10\n",
      "[SoftImpute] Iter 381: observed MAE=0.003254 validation MAE=0.029836,rank=10\n",
      "[SoftImpute] Iter 382: observed MAE=0.003253 validation MAE=0.029819,rank=10\n",
      "[SoftImpute] Iter 383: observed MAE=0.003252 validation MAE=0.029802,rank=10\n",
      "[SoftImpute] Iter 384: observed MAE=0.003251 validation MAE=0.029786,rank=10\n",
      "[SoftImpute] Iter 385: observed MAE=0.003250 validation MAE=0.029769,rank=10\n",
      "[SoftImpute] Iter 386: observed MAE=0.003249 validation MAE=0.029753,rank=10\n",
      "[SoftImpute] Iter 387: observed MAE=0.003248 validation MAE=0.029736,rank=10\n",
      "[SoftImpute] Iter 388: observed MAE=0.003247 validation MAE=0.029720,rank=10\n",
      "[SoftImpute] Iter 389: observed MAE=0.003246 validation MAE=0.029704,rank=10\n",
      "[SoftImpute] Iter 390: observed MAE=0.003245 validation MAE=0.029687,rank=10\n",
      "[SoftImpute] Iter 391: observed MAE=0.003244 validation MAE=0.029671,rank=10\n",
      "[SoftImpute] Iter 392: observed MAE=0.003243 validation MAE=0.029655,rank=10\n",
      "[SoftImpute] Iter 393: observed MAE=0.003242 validation MAE=0.029638,rank=10\n",
      "[SoftImpute] Iter 394: observed MAE=0.003241 validation MAE=0.029622,rank=10\n",
      "[SoftImpute] Iter 395: observed MAE=0.003240 validation MAE=0.029606,rank=10\n",
      "[SoftImpute] Iter 396: observed MAE=0.003239 validation MAE=0.029590,rank=10\n",
      "[SoftImpute] Iter 397: observed MAE=0.003238 validation MAE=0.029573,rank=10\n",
      "[SoftImpute] Iter 398: observed MAE=0.003237 validation MAE=0.029557,rank=10\n",
      "[SoftImpute] Iter 399: observed MAE=0.003236 validation MAE=0.029541,rank=10\n",
      "[SoftImpute] Iter 400: observed MAE=0.003235 validation MAE=0.029525,rank=10\n",
      "[SoftImpute] Iter 401: observed MAE=0.003234 validation MAE=0.029509,rank=10\n",
      "[SoftImpute] Iter 402: observed MAE=0.003233 validation MAE=0.029493,rank=10\n",
      "[SoftImpute] Stopped after iteration 402 for lambda=0.024644\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 15.048449516296387\n",
      "After the matrix factor stage, training error is 0.00323, validation error is 0.02949\n"
     ]
    }
   ],
   "source": [
    "rank_result = rtest('warm', data, meta_info, task_type=task_type, random_state=0, params=lx_params)\n",
    "rank_result.to_csv('result/rank_test/reg_rank.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
