{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0804 15:06:03.072466 22544 deprecation.py:323] From C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error,roc_auc_score,mean_absolute_error,log_loss\n",
    "import sys\n",
    "sys.path.append('benchmark/')\n",
    "from gammli_test import gammli\n",
    "from xgb_test import xgb\n",
    "from svd_test import svd\n",
    "from deepfm_fm_test import deepfm_fm\n",
    "from rank_test import rtest\n",
    "sys.path.append('../')\n",
    "from gammli.GAMMLI import GAMMLI\n",
    "from gammli.DataReader import data_initialize\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "random_state = 0\n",
    "data= pd.read_csv('data/simulation/simulation_regression.csv')\n",
    "task_type = \"Regression\"\n",
    "\n",
    "meta_info = OrderedDict()\n",
    "\n",
    "meta_info['uf_1']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_2']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_3']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_4']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_5']={'type': 'continues','source':'user'}\n",
    "meta_info['if_1']={'type': 'continues','source':'item'}\n",
    "meta_info['if_2']={'type': 'continues','source':'item'}\n",
    "meta_info['if_3']={'type': 'continues','source':'item'}\n",
    "meta_info['if_4']={'type': 'continues','source':'item'}\n",
    "meta_info['if_5']={'type': 'continues','source':'item'}\n",
    "meta_info['user_id']={\"type\":\"id\",'source':'user'}\n",
    "meta_info['item_id']={\"type\":\"id\",'source':'item'}\n",
    "meta_info['target']={\"type\":\"target\",'source':''}\n",
    "#the best shrinkage is 0.917120\n",
    "#the best combination is 0.600000\n",
    "lx_params = {\n",
    "        \"rank\":3,\n",
    "        \"main_effect_epochs\":300,\n",
    "        \"interaction_epochs\" : 200 ,\n",
    "        \"tuning_epochs\" : 50 , \n",
    "        \"mf_training_iters\": 500,\n",
    "        \"u_group_num\":30,\n",
    "        \"i_group_num\":50,\n",
    "        \"auto_tune\":False,\n",
    "        \"best_shrinkage\":1,\n",
    "        \"best_combination\":0.4,\n",
    "        \"verbose\":True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.86 MB\n",
      "Memory usage after optimization is: 0.26 MB\n",
      "Decreased by 69.6%\n",
      "Memory usage of dataframe is 0.21 MB\n",
      "Memory usage after optimization is: 0.07 MB\n",
      "Decreased by 69.6%\n",
      "test cold start user: 0\n",
      "test cold start item: 3\n",
      "validation cold start user: 0\n",
      "validation cold start item: 1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.34448, val loss: 0.34765\n",
      "Main effects training epoch: 2, train loss: 0.27354, val loss: 0.27853\n",
      "Main effects training epoch: 3, train loss: 0.20823, val loss: 0.21145\n",
      "Main effects training epoch: 4, train loss: 0.16135, val loss: 0.16514\n",
      "Main effects training epoch: 5, train loss: 0.14048, val loss: 0.14025\n",
      "Main effects training epoch: 6, train loss: 0.13280, val loss: 0.13280\n",
      "Main effects training epoch: 7, train loss: 0.13041, val loss: 0.12922\n",
      "Main effects training epoch: 8, train loss: 0.12977, val loss: 0.12861\n",
      "Main effects training epoch: 9, train loss: 0.12914, val loss: 0.12821\n",
      "Main effects training epoch: 10, train loss: 0.12852, val loss: 0.12844\n",
      "Main effects training epoch: 11, train loss: 0.12742, val loss: 0.12590\n",
      "Main effects training epoch: 12, train loss: 0.12612, val loss: 0.12564\n",
      "Main effects training epoch: 13, train loss: 0.12339, val loss: 0.12252\n",
      "Main effects training epoch: 14, train loss: 0.11835, val loss: 0.11883\n",
      "Main effects training epoch: 15, train loss: 0.11578, val loss: 0.11651\n",
      "Main effects training epoch: 16, train loss: 0.11289, val loss: 0.11344\n",
      "Main effects training epoch: 17, train loss: 0.11081, val loss: 0.11186\n",
      "Main effects training epoch: 18, train loss: 0.11088, val loss: 0.11312\n",
      "Main effects training epoch: 19, train loss: 0.11027, val loss: 0.11199\n",
      "Main effects training epoch: 20, train loss: 0.11090, val loss: 0.11238\n",
      "Main effects training epoch: 21, train loss: 0.10922, val loss: 0.11096\n",
      "Main effects training epoch: 22, train loss: 0.10900, val loss: 0.11106\n",
      "Main effects training epoch: 23, train loss: 0.10645, val loss: 0.10812\n",
      "Main effects training epoch: 24, train loss: 0.10707, val loss: 0.10858\n",
      "Main effects training epoch: 25, train loss: 0.10776, val loss: 0.10768\n",
      "Main effects training epoch: 26, train loss: 0.10556, val loss: 0.10715\n",
      "Main effects training epoch: 27, train loss: 0.10595, val loss: 0.10680\n",
      "Main effects training epoch: 28, train loss: 0.10509, val loss: 0.10619\n",
      "Main effects training epoch: 29, train loss: 0.10460, val loss: 0.10568\n",
      "Main effects training epoch: 30, train loss: 0.10512, val loss: 0.10585\n",
      "Main effects training epoch: 31, train loss: 0.10437, val loss: 0.10598\n",
      "Main effects training epoch: 32, train loss: 0.10431, val loss: 0.10577\n",
      "Main effects training epoch: 33, train loss: 0.10423, val loss: 0.10545\n",
      "Main effects training epoch: 34, train loss: 0.10413, val loss: 0.10561\n",
      "Main effects training epoch: 35, train loss: 0.10424, val loss: 0.10579\n",
      "Main effects training epoch: 36, train loss: 0.10444, val loss: 0.10578\n",
      "Main effects training epoch: 37, train loss: 0.10400, val loss: 0.10575\n",
      "Main effects training epoch: 38, train loss: 0.10438, val loss: 0.10591\n",
      "Main effects training epoch: 39, train loss: 0.10416, val loss: 0.10566\n",
      "Main effects training epoch: 40, train loss: 0.10393, val loss: 0.10524\n",
      "Main effects training epoch: 41, train loss: 0.10418, val loss: 0.10579\n",
      "Main effects training epoch: 42, train loss: 0.10392, val loss: 0.10565\n",
      "Main effects training epoch: 43, train loss: 0.10430, val loss: 0.10612\n",
      "Main effects training epoch: 44, train loss: 0.10409, val loss: 0.10553\n",
      "Main effects training epoch: 45, train loss: 0.10403, val loss: 0.10539\n",
      "Main effects training epoch: 46, train loss: 0.10400, val loss: 0.10547\n",
      "Main effects training epoch: 47, train loss: 0.10472, val loss: 0.10625\n",
      "Main effects training epoch: 48, train loss: 0.10410, val loss: 0.10597\n",
      "Main effects training epoch: 49, train loss: 0.10415, val loss: 0.10512\n",
      "Main effects training epoch: 50, train loss: 0.10408, val loss: 0.10560\n",
      "Main effects training epoch: 51, train loss: 0.10392, val loss: 0.10554\n",
      "Main effects training epoch: 52, train loss: 0.10395, val loss: 0.10519\n",
      "Main effects training epoch: 53, train loss: 0.10385, val loss: 0.10520\n",
      "Main effects training epoch: 54, train loss: 0.10398, val loss: 0.10563\n",
      "Main effects training epoch: 55, train loss: 0.10416, val loss: 0.10523\n",
      "Main effects training epoch: 56, train loss: 0.10404, val loss: 0.10586\n",
      "Main effects training epoch: 57, train loss: 0.10399, val loss: 0.10571\n",
      "Main effects training epoch: 58, train loss: 0.10443, val loss: 0.10631\n",
      "Main effects training epoch: 59, train loss: 0.10393, val loss: 0.10549\n",
      "Main effects training epoch: 60, train loss: 0.10386, val loss: 0.10534\n",
      "Main effects training epoch: 61, train loss: 0.10389, val loss: 0.10538\n",
      "Main effects training epoch: 62, train loss: 0.10417, val loss: 0.10552\n",
      "Main effects training epoch: 63, train loss: 0.10396, val loss: 0.10570\n",
      "Main effects training epoch: 64, train loss: 0.10412, val loss: 0.10546\n",
      "Main effects training epoch: 65, train loss: 0.10403, val loss: 0.10549\n",
      "Main effects training epoch: 66, train loss: 0.10398, val loss: 0.10514\n",
      "Main effects training epoch: 67, train loss: 0.10385, val loss: 0.10514\n",
      "Main effects training epoch: 68, train loss: 0.10396, val loss: 0.10561\n",
      "Main effects training epoch: 69, train loss: 0.10397, val loss: 0.10523\n",
      "Main effects training epoch: 70, train loss: 0.10420, val loss: 0.10542\n",
      "Main effects training epoch: 71, train loss: 0.10473, val loss: 0.10671\n",
      "Main effects training epoch: 72, train loss: 0.10418, val loss: 0.10552\n",
      "Main effects training epoch: 73, train loss: 0.10427, val loss: 0.10590\n",
      "Main effects training epoch: 74, train loss: 0.10409, val loss: 0.10540\n",
      "Main effects training epoch: 75, train loss: 0.10401, val loss: 0.10542\n",
      "Main effects training epoch: 76, train loss: 0.10384, val loss: 0.10560\n",
      "Main effects training epoch: 77, train loss: 0.10404, val loss: 0.10542\n",
      "Main effects training epoch: 78, train loss: 0.10394, val loss: 0.10550\n",
      "Main effects training epoch: 79, train loss: 0.10410, val loss: 0.10553\n",
      "Main effects training epoch: 80, train loss: 0.10425, val loss: 0.10542\n",
      "Main effects training epoch: 81, train loss: 0.10387, val loss: 0.10530\n",
      "Main effects training epoch: 82, train loss: 0.10409, val loss: 0.10607\n",
      "Main effects training epoch: 83, train loss: 0.10386, val loss: 0.10559\n",
      "Main effects training epoch: 84, train loss: 0.10405, val loss: 0.10549\n",
      "Main effects training epoch: 85, train loss: 0.10483, val loss: 0.10617\n",
      "Main effects training epoch: 86, train loss: 0.10383, val loss: 0.10551\n",
      "Main effects training epoch: 87, train loss: 0.10415, val loss: 0.10524\n",
      "Main effects training epoch: 88, train loss: 0.10389, val loss: 0.10586\n",
      "Main effects training epoch: 89, train loss: 0.10383, val loss: 0.10548\n",
      "Main effects training epoch: 90, train loss: 0.10400, val loss: 0.10512\n",
      "Main effects training epoch: 91, train loss: 0.10387, val loss: 0.10538\n",
      "Main effects training epoch: 92, train loss: 0.10420, val loss: 0.10635\n",
      "Main effects training epoch: 93, train loss: 0.10396, val loss: 0.10529\n",
      "Main effects training epoch: 94, train loss: 0.10417, val loss: 0.10594\n",
      "Main effects training epoch: 95, train loss: 0.10387, val loss: 0.10562\n",
      "Main effects training epoch: 96, train loss: 0.10378, val loss: 0.10533\n",
      "Main effects training epoch: 97, train loss: 0.10386, val loss: 0.10536\n",
      "Main effects training epoch: 98, train loss: 0.10425, val loss: 0.10630\n",
      "Main effects training epoch: 99, train loss: 0.10426, val loss: 0.10552\n",
      "Main effects training epoch: 100, train loss: 0.10419, val loss: 0.10632\n",
      "Main effects training epoch: 101, train loss: 0.10395, val loss: 0.10553\n",
      "Main effects training epoch: 102, train loss: 0.10397, val loss: 0.10584\n",
      "Main effects training epoch: 103, train loss: 0.10408, val loss: 0.10574\n",
      "Main effects training epoch: 104, train loss: 0.10395, val loss: 0.10576\n",
      "Main effects training epoch: 105, train loss: 0.10380, val loss: 0.10555\n",
      "Main effects training epoch: 106, train loss: 0.10382, val loss: 0.10590\n",
      "Main effects training epoch: 107, train loss: 0.10417, val loss: 0.10600\n",
      "Main effects training epoch: 108, train loss: 0.10410, val loss: 0.10567\n",
      "Main effects training epoch: 109, train loss: 0.10373, val loss: 0.10542\n",
      "Main effects training epoch: 110, train loss: 0.10377, val loss: 0.10525\n",
      "Main effects training epoch: 111, train loss: 0.10405, val loss: 0.10577\n",
      "Main effects training epoch: 112, train loss: 0.10409, val loss: 0.10590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 113, train loss: 0.10474, val loss: 0.10615\n",
      "Main effects training epoch: 114, train loss: 0.10406, val loss: 0.10594\n",
      "Main effects training epoch: 115, train loss: 0.10415, val loss: 0.10553\n",
      "Main effects training epoch: 116, train loss: 0.10437, val loss: 0.10680\n",
      "Main effects training epoch: 117, train loss: 0.10399, val loss: 0.10532\n",
      "Main effects training epoch: 118, train loss: 0.10393, val loss: 0.10583\n",
      "Main effects training epoch: 119, train loss: 0.10410, val loss: 0.10601\n",
      "Main effects training epoch: 120, train loss: 0.10375, val loss: 0.10578\n",
      "Main effects training epoch: 121, train loss: 0.10382, val loss: 0.10529\n",
      "Main effects training epoch: 122, train loss: 0.10391, val loss: 0.10552\n",
      "Main effects training epoch: 123, train loss: 0.10397, val loss: 0.10615\n",
      "Main effects training epoch: 124, train loss: 0.10422, val loss: 0.10572\n",
      "Main effects training epoch: 125, train loss: 0.10414, val loss: 0.10601\n",
      "Main effects training epoch: 126, train loss: 0.10402, val loss: 0.10605\n",
      "Main effects training epoch: 127, train loss: 0.10402, val loss: 0.10531\n",
      "Main effects training epoch: 128, train loss: 0.10395, val loss: 0.10615\n",
      "Main effects training epoch: 129, train loss: 0.10410, val loss: 0.10643\n",
      "Main effects training epoch: 130, train loss: 0.10401, val loss: 0.10526\n",
      "Main effects training epoch: 131, train loss: 0.10380, val loss: 0.10577\n",
      "Main effects training epoch: 132, train loss: 0.10404, val loss: 0.10554\n",
      "Main effects training epoch: 133, train loss: 0.10425, val loss: 0.10648\n",
      "Main effects training epoch: 134, train loss: 0.10454, val loss: 0.10616\n",
      "Main effects training epoch: 135, train loss: 0.10424, val loss: 0.10653\n",
      "Main effects training epoch: 136, train loss: 0.10426, val loss: 0.10565\n",
      "Main effects training epoch: 137, train loss: 0.10444, val loss: 0.10639\n",
      "Main effects training epoch: 138, train loss: 0.10433, val loss: 0.10622\n",
      "Main effects training epoch: 139, train loss: 0.10384, val loss: 0.10565\n",
      "Main effects training epoch: 140, train loss: 0.10392, val loss: 0.10539\n",
      "Main effects training epoch: 141, train loss: 0.10420, val loss: 0.10593\n",
      "Main effects training epoch: 142, train loss: 0.10404, val loss: 0.10617\n",
      "Main effects training epoch: 143, train loss: 0.10420, val loss: 0.10567\n",
      "Main effects training epoch: 144, train loss: 0.10383, val loss: 0.10601\n",
      "Main effects training epoch: 145, train loss: 0.10370, val loss: 0.10530\n",
      "Main effects training epoch: 146, train loss: 0.10381, val loss: 0.10596\n",
      "Main effects training epoch: 147, train loss: 0.10380, val loss: 0.10575\n",
      "Main effects training epoch: 148, train loss: 0.10377, val loss: 0.10571\n",
      "Main effects training epoch: 149, train loss: 0.10383, val loss: 0.10566\n",
      "Main effects training epoch: 150, train loss: 0.10393, val loss: 0.10604\n",
      "Early stop at epoch 150, with validation loss: 0.10604\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.10405, val loss: 0.10498\n",
      "Main effects tuning epoch: 2, train loss: 0.10423, val loss: 0.10552\n",
      "Main effects tuning epoch: 3, train loss: 0.10411, val loss: 0.10505\n",
      "Main effects tuning epoch: 4, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 5, train loss: 0.10382, val loss: 0.10498\n",
      "Main effects tuning epoch: 6, train loss: 0.10425, val loss: 0.10582\n",
      "Main effects tuning epoch: 7, train loss: 0.10408, val loss: 0.10530\n",
      "Main effects tuning epoch: 8, train loss: 0.10397, val loss: 0.10527\n",
      "Main effects tuning epoch: 9, train loss: 0.10401, val loss: 0.10507\n",
      "Main effects tuning epoch: 10, train loss: 0.10407, val loss: 0.10539\n",
      "Main effects tuning epoch: 11, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 12, train loss: 0.10393, val loss: 0.10556\n",
      "Main effects tuning epoch: 13, train loss: 0.10415, val loss: 0.10510\n",
      "Main effects tuning epoch: 14, train loss: 0.10397, val loss: 0.10570\n",
      "Main effects tuning epoch: 15, train loss: 0.10386, val loss: 0.10516\n",
      "Main effects tuning epoch: 16, train loss: 0.10395, val loss: 0.10562\n",
      "Main effects tuning epoch: 17, train loss: 0.10395, val loss: 0.10539\n",
      "Main effects tuning epoch: 18, train loss: 0.10422, val loss: 0.10568\n",
      "Main effects tuning epoch: 19, train loss: 0.10433, val loss: 0.10517\n",
      "Main effects tuning epoch: 20, train loss: 0.10384, val loss: 0.10568\n",
      "Main effects tuning epoch: 21, train loss: 0.10400, val loss: 0.10541\n",
      "Main effects tuning epoch: 22, train loss: 0.10393, val loss: 0.10526\n",
      "Main effects tuning epoch: 23, train loss: 0.10434, val loss: 0.10582\n",
      "Main effects tuning epoch: 24, train loss: 0.10406, val loss: 0.10540\n",
      "Main effects tuning epoch: 25, train loss: 0.10410, val loss: 0.10570\n",
      "Main effects tuning epoch: 26, train loss: 0.10405, val loss: 0.10569\n",
      "Main effects tuning epoch: 27, train loss: 0.10406, val loss: 0.10556\n",
      "Main effects tuning epoch: 28, train loss: 0.10394, val loss: 0.10542\n",
      "Main effects tuning epoch: 29, train loss: 0.10383, val loss: 0.10542\n",
      "Main effects tuning epoch: 30, train loss: 0.10385, val loss: 0.10548\n",
      "Main effects tuning epoch: 31, train loss: 0.10395, val loss: 0.10516\n",
      "Main effects tuning epoch: 32, train loss: 0.10391, val loss: 0.10584\n",
      "Main effects tuning epoch: 33, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 34, train loss: 0.10391, val loss: 0.10561\n",
      "Main effects tuning epoch: 35, train loss: 0.10424, val loss: 0.10531\n",
      "Main effects tuning epoch: 36, train loss: 0.10395, val loss: 0.10554\n",
      "Main effects tuning epoch: 37, train loss: 0.10400, val loss: 0.10600\n",
      "Main effects tuning epoch: 38, train loss: 0.10408, val loss: 0.10530\n",
      "Main effects tuning epoch: 39, train loss: 0.10394, val loss: 0.10565\n",
      "Main effects tuning epoch: 40, train loss: 0.10385, val loss: 0.10569\n",
      "Main effects tuning epoch: 41, train loss: 0.10390, val loss: 0.10539\n",
      "Main effects tuning epoch: 42, train loss: 0.10400, val loss: 0.10587\n",
      "Main effects tuning epoch: 43, train loss: 0.10386, val loss: 0.10547\n",
      "Main effects tuning epoch: 44, train loss: 0.10403, val loss: 0.10597\n",
      "Main effects tuning epoch: 45, train loss: 0.10384, val loss: 0.10541\n",
      "Main effects tuning epoch: 46, train loss: 0.10398, val loss: 0.10582\n",
      "Main effects tuning epoch: 47, train loss: 0.10375, val loss: 0.10527\n",
      "Main effects tuning epoch: 48, train loss: 0.10380, val loss: 0.10549\n",
      "Main effects tuning epoch: 49, train loss: 0.10396, val loss: 0.10566\n",
      "Main effects tuning epoch: 50, train loss: 0.10407, val loss: 0.10549\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.09376, val loss: 0.09232\n",
      "Interaction training epoch: 2, train loss: 0.20140, val loss: 0.20111\n",
      "Interaction training epoch: 3, train loss: 0.06617, val loss: 0.06860\n",
      "Interaction training epoch: 4, train loss: 0.05173, val loss: 0.05197\n",
      "Interaction training epoch: 5, train loss: 0.06497, val loss: 0.06595\n",
      "Interaction training epoch: 6, train loss: 0.04386, val loss: 0.04449\n",
      "Interaction training epoch: 7, train loss: 0.04606, val loss: 0.04529\n",
      "Interaction training epoch: 8, train loss: 0.04492, val loss: 0.04489\n",
      "Interaction training epoch: 9, train loss: 0.04997, val loss: 0.04868\n",
      "Interaction training epoch: 10, train loss: 0.04002, val loss: 0.04133\n",
      "Interaction training epoch: 11, train loss: 0.04269, val loss: 0.04306\n",
      "Interaction training epoch: 12, train loss: 0.04100, val loss: 0.03997\n",
      "Interaction training epoch: 13, train loss: 0.04366, val loss: 0.04360\n",
      "Interaction training epoch: 14, train loss: 0.04928, val loss: 0.05084\n",
      "Interaction training epoch: 15, train loss: 0.04532, val loss: 0.04488\n",
      "Interaction training epoch: 16, train loss: 0.04530, val loss: 0.04524\n",
      "Interaction training epoch: 17, train loss: 0.03784, val loss: 0.03763\n",
      "Interaction training epoch: 18, train loss: 0.04939, val loss: 0.05023\n",
      "Interaction training epoch: 19, train loss: 0.03608, val loss: 0.03597\n",
      "Interaction training epoch: 20, train loss: 0.04344, val loss: 0.04367\n",
      "Interaction training epoch: 21, train loss: 0.04002, val loss: 0.03932\n",
      "Interaction training epoch: 22, train loss: 0.03977, val loss: 0.03977\n",
      "Interaction training epoch: 23, train loss: 0.03777, val loss: 0.03709\n",
      "Interaction training epoch: 24, train loss: 0.03927, val loss: 0.03924\n",
      "Interaction training epoch: 25, train loss: 0.04164, val loss: 0.04112\n",
      "Interaction training epoch: 26, train loss: 0.04429, val loss: 0.04389\n",
      "Interaction training epoch: 27, train loss: 0.04417, val loss: 0.04416\n",
      "Interaction training epoch: 28, train loss: 0.04241, val loss: 0.04112\n",
      "Interaction training epoch: 29, train loss: 0.03548, val loss: 0.03672\n",
      "Interaction training epoch: 30, train loss: 0.04420, val loss: 0.04353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 31, train loss: 0.04020, val loss: 0.04004\n",
      "Interaction training epoch: 32, train loss: 0.03422, val loss: 0.03451\n",
      "Interaction training epoch: 33, train loss: 0.03378, val loss: 0.03407\n",
      "Interaction training epoch: 34, train loss: 0.03925, val loss: 0.03880\n",
      "Interaction training epoch: 35, train loss: 0.03752, val loss: 0.03636\n",
      "Interaction training epoch: 36, train loss: 0.03582, val loss: 0.03564\n",
      "Interaction training epoch: 37, train loss: 0.03069, val loss: 0.03087\n",
      "Interaction training epoch: 38, train loss: 0.03729, val loss: 0.03652\n",
      "Interaction training epoch: 39, train loss: 0.03772, val loss: 0.03772\n",
      "Interaction training epoch: 40, train loss: 0.04332, val loss: 0.04201\n",
      "Interaction training epoch: 41, train loss: 0.03409, val loss: 0.03435\n",
      "Interaction training epoch: 42, train loss: 0.03501, val loss: 0.03419\n",
      "Interaction training epoch: 43, train loss: 0.03630, val loss: 0.03573\n",
      "Interaction training epoch: 44, train loss: 0.03447, val loss: 0.03420\n",
      "Interaction training epoch: 45, train loss: 0.03622, val loss: 0.03541\n",
      "Interaction training epoch: 46, train loss: 0.02991, val loss: 0.03002\n",
      "Interaction training epoch: 47, train loss: 0.02880, val loss: 0.02856\n",
      "Interaction training epoch: 48, train loss: 0.03548, val loss: 0.03469\n",
      "Interaction training epoch: 49, train loss: 0.03399, val loss: 0.03256\n",
      "Interaction training epoch: 50, train loss: 0.03225, val loss: 0.03266\n",
      "Interaction training epoch: 51, train loss: 0.03286, val loss: 0.03176\n",
      "Interaction training epoch: 52, train loss: 0.03084, val loss: 0.03012\n",
      "Interaction training epoch: 53, train loss: 0.03059, val loss: 0.02998\n",
      "Interaction training epoch: 54, train loss: 0.02966, val loss: 0.02932\n",
      "Interaction training epoch: 55, train loss: 0.04122, val loss: 0.04119\n",
      "Interaction training epoch: 56, train loss: 0.03164, val loss: 0.03059\n",
      "Interaction training epoch: 57, train loss: 0.03483, val loss: 0.03469\n",
      "Interaction training epoch: 58, train loss: 0.03320, val loss: 0.03294\n",
      "Interaction training epoch: 59, train loss: 0.02999, val loss: 0.02930\n",
      "Interaction training epoch: 60, train loss: 0.03540, val loss: 0.03496\n",
      "Interaction training epoch: 61, train loss: 0.03142, val loss: 0.02962\n",
      "Interaction training epoch: 62, train loss: 0.03146, val loss: 0.03164\n",
      "Interaction training epoch: 63, train loss: 0.03481, val loss: 0.03421\n",
      "Interaction training epoch: 64, train loss: 0.02946, val loss: 0.02881\n",
      "Interaction training epoch: 65, train loss: 0.03539, val loss: 0.03511\n",
      "Interaction training epoch: 66, train loss: 0.03797, val loss: 0.03805\n",
      "Interaction training epoch: 67, train loss: 0.03669, val loss: 0.03627\n",
      "Interaction training epoch: 68, train loss: 0.04844, val loss: 0.04881\n",
      "Interaction training epoch: 69, train loss: 0.02947, val loss: 0.02941\n",
      "Interaction training epoch: 70, train loss: 0.03805, val loss: 0.03672\n",
      "Interaction training epoch: 71, train loss: 0.02990, val loss: 0.02970\n",
      "Interaction training epoch: 72, train loss: 0.03926, val loss: 0.03952\n",
      "Interaction training epoch: 73, train loss: 0.03713, val loss: 0.03732\n",
      "Interaction training epoch: 74, train loss: 0.04469, val loss: 0.04392\n",
      "Interaction training epoch: 75, train loss: 0.03518, val loss: 0.03491\n",
      "Interaction training epoch: 76, train loss: 0.03954, val loss: 0.03893\n",
      "Interaction training epoch: 77, train loss: 0.04294, val loss: 0.04248\n",
      "Interaction training epoch: 78, train loss: 0.03122, val loss: 0.03170\n",
      "Interaction training epoch: 79, train loss: 0.03577, val loss: 0.03538\n",
      "Interaction training epoch: 80, train loss: 0.04727, val loss: 0.04664\n",
      "Interaction training epoch: 81, train loss: 0.03478, val loss: 0.03412\n",
      "Interaction training epoch: 82, train loss: 0.03736, val loss: 0.03700\n",
      "Interaction training epoch: 83, train loss: 0.03716, val loss: 0.03665\n",
      "Interaction training epoch: 84, train loss: 0.03675, val loss: 0.03575\n",
      "Interaction training epoch: 85, train loss: 0.03815, val loss: 0.03817\n",
      "Interaction training epoch: 86, train loss: 0.03960, val loss: 0.03903\n",
      "Interaction training epoch: 87, train loss: 0.03498, val loss: 0.03402\n",
      "Interaction training epoch: 88, train loss: 0.03630, val loss: 0.03583\n",
      "Interaction training epoch: 89, train loss: 0.03454, val loss: 0.03517\n",
      "Interaction training epoch: 90, train loss: 0.07231, val loss: 0.07223\n",
      "Interaction training epoch: 91, train loss: 0.04800, val loss: 0.04802\n",
      "Interaction training epoch: 92, train loss: 0.05224, val loss: 0.05162\n",
      "Interaction training epoch: 93, train loss: 0.02794, val loss: 0.02804\n",
      "Interaction training epoch: 94, train loss: 0.03585, val loss: 0.03546\n",
      "Interaction training epoch: 95, train loss: 0.04149, val loss: 0.04158\n",
      "Interaction training epoch: 96, train loss: 0.04943, val loss: 0.04909\n",
      "Interaction training epoch: 97, train loss: 0.05722, val loss: 0.05690\n",
      "Interaction training epoch: 98, train loss: 0.04854, val loss: 0.04831\n",
      "Interaction training epoch: 99, train loss: 0.02719, val loss: 0.02716\n",
      "Interaction training epoch: 100, train loss: 0.03689, val loss: 0.03722\n",
      "Interaction training epoch: 101, train loss: 0.04609, val loss: 0.04601\n",
      "Interaction training epoch: 102, train loss: 0.03250, val loss: 0.03231\n",
      "Interaction training epoch: 103, train loss: 0.03006, val loss: 0.02936\n",
      "Interaction training epoch: 104, train loss: 0.03967, val loss: 0.03937\n",
      "Interaction training epoch: 105, train loss: 0.03096, val loss: 0.03100\n",
      "Interaction training epoch: 106, train loss: 0.05099, val loss: 0.05044\n",
      "Interaction training epoch: 107, train loss: 0.03358, val loss: 0.03389\n",
      "Interaction training epoch: 108, train loss: 0.04273, val loss: 0.04236\n",
      "Interaction training epoch: 109, train loss: 0.07259, val loss: 0.07140\n",
      "Interaction training epoch: 110, train loss: 0.03442, val loss: 0.03433\n",
      "Interaction training epoch: 111, train loss: 0.02947, val loss: 0.02893\n",
      "Interaction training epoch: 112, train loss: 0.03413, val loss: 0.03396\n",
      "Interaction training epoch: 113, train loss: 0.03441, val loss: 0.03428\n",
      "Interaction training epoch: 114, train loss: 0.05762, val loss: 0.05645\n",
      "Interaction training epoch: 115, train loss: 0.02938, val loss: 0.02918\n",
      "Interaction training epoch: 116, train loss: 0.04805, val loss: 0.04824\n",
      "Interaction training epoch: 117, train loss: 0.03927, val loss: 0.03835\n",
      "Interaction training epoch: 118, train loss: 0.03949, val loss: 0.03920\n",
      "Interaction training epoch: 119, train loss: 0.03835, val loss: 0.03852\n",
      "Interaction training epoch: 120, train loss: 0.07402, val loss: 0.07267\n",
      "Interaction training epoch: 121, train loss: 0.02786, val loss: 0.02794\n",
      "Interaction training epoch: 122, train loss: 0.02966, val loss: 0.02840\n",
      "Interaction training epoch: 123, train loss: 0.03146, val loss: 0.03080\n",
      "Interaction training epoch: 124, train loss: 0.03135, val loss: 0.03095\n",
      "Interaction training epoch: 125, train loss: 0.02893, val loss: 0.02832\n",
      "Interaction training epoch: 126, train loss: 0.06644, val loss: 0.06554\n",
      "Interaction training epoch: 127, train loss: 0.02439, val loss: 0.02405\n",
      "Interaction training epoch: 128, train loss: 0.02442, val loss: 0.02392\n",
      "Interaction training epoch: 129, train loss: 0.06718, val loss: 0.06662\n",
      "Interaction training epoch: 130, train loss: 0.06873, val loss: 0.06752\n",
      "Interaction training epoch: 131, train loss: 0.02836, val loss: 0.02795\n",
      "Interaction training epoch: 132, train loss: 0.02537, val loss: 0.02462\n",
      "Interaction training epoch: 133, train loss: 0.05578, val loss: 0.05497\n",
      "Interaction training epoch: 134, train loss: 0.02756, val loss: 0.02628\n",
      "Interaction training epoch: 135, train loss: 0.03851, val loss: 0.03792\n",
      "Interaction training epoch: 136, train loss: 0.02792, val loss: 0.02763\n",
      "Interaction training epoch: 137, train loss: 0.02882, val loss: 0.02822\n",
      "Interaction training epoch: 138, train loss: 0.04268, val loss: 0.04230\n",
      "Interaction training epoch: 139, train loss: 0.02999, val loss: 0.03028\n",
      "Interaction training epoch: 140, train loss: 0.02653, val loss: 0.02598\n",
      "Interaction training epoch: 141, train loss: 0.02864, val loss: 0.02740\n",
      "Interaction training epoch: 142, train loss: 0.04031, val loss: 0.03930\n",
      "Interaction training epoch: 143, train loss: 0.08930, val loss: 0.08825\n",
      "Interaction training epoch: 144, train loss: 0.03278, val loss: 0.03201\n",
      "Interaction training epoch: 145, train loss: 0.07595, val loss: 0.07444\n",
      "Interaction training epoch: 146, train loss: 0.04088, val loss: 0.04007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 147, train loss: 0.03063, val loss: 0.02967\n",
      "Interaction training epoch: 148, train loss: 0.08210, val loss: 0.08116\n",
      "Interaction training epoch: 149, train loss: 0.02600, val loss: 0.02595\n",
      "Interaction training epoch: 150, train loss: 0.04451, val loss: 0.04266\n",
      "Interaction training epoch: 151, train loss: 0.05071, val loss: 0.04953\n",
      "Interaction training epoch: 152, train loss: 0.03689, val loss: 0.03657\n",
      "Interaction training epoch: 153, train loss: 0.05108, val loss: 0.05017\n",
      "Interaction training epoch: 154, train loss: 0.03125, val loss: 0.03020\n",
      "Interaction training epoch: 155, train loss: 0.05262, val loss: 0.05160\n",
      "Interaction training epoch: 156, train loss: 0.03189, val loss: 0.03099\n",
      "Interaction training epoch: 157, train loss: 0.03302, val loss: 0.03192\n",
      "Interaction training epoch: 158, train loss: 0.03919, val loss: 0.03867\n",
      "Interaction training epoch: 159, train loss: 0.06294, val loss: 0.06164\n",
      "Interaction training epoch: 160, train loss: 0.02504, val loss: 0.02398\n",
      "Interaction training epoch: 161, train loss: 0.02726, val loss: 0.02622\n",
      "Interaction training epoch: 162, train loss: 0.02696, val loss: 0.02663\n",
      "Interaction training epoch: 163, train loss: 0.04443, val loss: 0.04306\n",
      "Interaction training epoch: 164, train loss: 0.08229, val loss: 0.08080\n",
      "Interaction training epoch: 165, train loss: 0.02497, val loss: 0.02433\n",
      "Interaction training epoch: 166, train loss: 0.02405, val loss: 0.02321\n",
      "Interaction training epoch: 167, train loss: 0.03154, val loss: 0.03087\n",
      "Interaction training epoch: 168, train loss: 0.03079, val loss: 0.02948\n",
      "Interaction training epoch: 169, train loss: 0.02562, val loss: 0.02466\n",
      "Interaction training epoch: 170, train loss: 0.03950, val loss: 0.03811\n",
      "Interaction training epoch: 171, train loss: 0.05102, val loss: 0.05005\n",
      "Interaction training epoch: 172, train loss: 0.07393, val loss: 0.07199\n",
      "Interaction training epoch: 173, train loss: 0.03775, val loss: 0.03659\n",
      "Interaction training epoch: 174, train loss: 0.03283, val loss: 0.03172\n",
      "Interaction training epoch: 175, train loss: 0.02729, val loss: 0.02663\n",
      "Interaction training epoch: 176, train loss: 0.04249, val loss: 0.04090\n",
      "Interaction training epoch: 177, train loss: 0.11371, val loss: 0.11248\n",
      "Interaction training epoch: 178, train loss: 0.05008, val loss: 0.04851\n",
      "Interaction training epoch: 179, train loss: 0.05070, val loss: 0.04903\n",
      "Interaction training epoch: 180, train loss: 0.05192, val loss: 0.05082\n",
      "Interaction training epoch: 181, train loss: 0.03499, val loss: 0.03401\n",
      "Interaction training epoch: 182, train loss: 0.06009, val loss: 0.05850\n",
      "Interaction training epoch: 183, train loss: 0.03517, val loss: 0.03374\n",
      "Interaction training epoch: 184, train loss: 0.04107, val loss: 0.04010\n",
      "Interaction training epoch: 185, train loss: 0.06361, val loss: 0.06162\n",
      "Interaction training epoch: 186, train loss: 0.06349, val loss: 0.06178\n",
      "Interaction training epoch: 187, train loss: 0.05374, val loss: 0.05196\n",
      "Interaction training epoch: 188, train loss: 0.05958, val loss: 0.05804\n",
      "Interaction training epoch: 189, train loss: 0.02604, val loss: 0.02519\n",
      "Interaction training epoch: 190, train loss: 0.04369, val loss: 0.04302\n",
      "Interaction training epoch: 191, train loss: 0.04175, val loss: 0.04116\n",
      "Interaction training epoch: 192, train loss: 0.06782, val loss: 0.06645\n",
      "Interaction training epoch: 193, train loss: 0.06145, val loss: 0.06034\n",
      "Interaction training epoch: 194, train loss: 0.04303, val loss: 0.04151\n",
      "Interaction training epoch: 195, train loss: 0.03319, val loss: 0.03229\n",
      "Interaction training epoch: 196, train loss: 0.03143, val loss: 0.02941\n",
      "Interaction training epoch: 197, train loss: 0.03413, val loss: 0.03311\n",
      "Interaction training epoch: 198, train loss: 0.03045, val loss: 0.02901\n",
      "Interaction training epoch: 199, train loss: 0.04253, val loss: 0.04138\n",
      "Interaction training epoch: 200, train loss: 0.04032, val loss: 0.03845\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########4 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.02853, val loss: 0.02785\n",
      "Interaction tuning epoch: 2, train loss: 0.02401, val loss: 0.02299\n",
      "Interaction tuning epoch: 3, train loss: 0.04478, val loss: 0.04265\n",
      "Interaction tuning epoch: 4, train loss: 0.06395, val loss: 0.06292\n",
      "Interaction tuning epoch: 5, train loss: 0.02356, val loss: 0.02286\n",
      "Interaction tuning epoch: 6, train loss: 0.06369, val loss: 0.06228\n",
      "Interaction tuning epoch: 7, train loss: 0.04154, val loss: 0.04077\n",
      "Interaction tuning epoch: 8, train loss: 0.08354, val loss: 0.08227\n",
      "Interaction tuning epoch: 9, train loss: 0.07421, val loss: 0.07270\n",
      "Interaction tuning epoch: 10, train loss: 0.04015, val loss: 0.03931\n",
      "Interaction tuning epoch: 11, train loss: 0.02840, val loss: 0.02746\n",
      "Interaction tuning epoch: 12, train loss: 0.06172, val loss: 0.06021\n",
      "Interaction tuning epoch: 13, train loss: 0.03776, val loss: 0.03699\n",
      "Interaction tuning epoch: 14, train loss: 0.07388, val loss: 0.07213\n",
      "Interaction tuning epoch: 15, train loss: 0.02628, val loss: 0.02557\n",
      "Interaction tuning epoch: 16, train loss: 0.02860, val loss: 0.02813\n",
      "Interaction tuning epoch: 17, train loss: 0.04973, val loss: 0.04802\n",
      "Interaction tuning epoch: 18, train loss: 0.02869, val loss: 0.02741\n",
      "Interaction tuning epoch: 19, train loss: 0.03914, val loss: 0.03730\n",
      "Interaction tuning epoch: 20, train loss: 0.03596, val loss: 0.03526\n",
      "Interaction tuning epoch: 21, train loss: 0.04924, val loss: 0.04797\n",
      "Interaction tuning epoch: 22, train loss: 0.08235, val loss: 0.08148\n",
      "Interaction tuning epoch: 23, train loss: 0.08430, val loss: 0.08186\n",
      "Interaction tuning epoch: 24, train loss: 0.07758, val loss: 0.07581\n",
      "Interaction tuning epoch: 25, train loss: 0.02468, val loss: 0.02425\n",
      "Interaction tuning epoch: 26, train loss: 0.02729, val loss: 0.02674\n",
      "Interaction tuning epoch: 27, train loss: 0.02677, val loss: 0.02542\n",
      "Interaction tuning epoch: 28, train loss: 0.03303, val loss: 0.03233\n",
      "Interaction tuning epoch: 29, train loss: 0.07628, val loss: 0.07409\n",
      "Interaction tuning epoch: 30, train loss: 0.05354, val loss: 0.05248\n",
      "Interaction tuning epoch: 31, train loss: 0.02819, val loss: 0.02642\n",
      "Interaction tuning epoch: 32, train loss: 0.02994, val loss: 0.02905\n",
      "Interaction tuning epoch: 33, train loss: 0.03903, val loss: 0.03809\n",
      "Interaction tuning epoch: 34, train loss: 0.03406, val loss: 0.03261\n",
      "Interaction tuning epoch: 35, train loss: 0.02886, val loss: 0.02797\n",
      "Interaction tuning epoch: 36, train loss: 0.04728, val loss: 0.04591\n",
      "Interaction tuning epoch: 37, train loss: 0.02745, val loss: 0.02653\n",
      "Interaction tuning epoch: 38, train loss: 0.09613, val loss: 0.09462\n",
      "Interaction tuning epoch: 39, train loss: 0.06430, val loss: 0.06326\n",
      "Interaction tuning epoch: 40, train loss: 0.03564, val loss: 0.03467\n",
      "Interaction tuning epoch: 41, train loss: 0.05842, val loss: 0.05720\n",
      "Interaction tuning epoch: 42, train loss: 0.02719, val loss: 0.02601\n",
      "Interaction tuning epoch: 43, train loss: 0.03689, val loss: 0.03533\n",
      "Interaction tuning epoch: 44, train loss: 0.02563, val loss: 0.02452\n",
      "Interaction tuning epoch: 45, train loss: 0.11515, val loss: 0.11336\n",
      "Interaction tuning epoch: 46, train loss: 0.05383, val loss: 0.05274\n",
      "Interaction tuning epoch: 47, train loss: 0.03183, val loss: 0.02986\n",
      "Interaction tuning epoch: 48, train loss: 0.04755, val loss: 0.04566\n",
      "Interaction tuning epoch: 49, train loss: 0.03504, val loss: 0.03437\n",
      "Interaction tuning epoch: 50, train loss: 0.03646, val loss: 0.03600\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 31.30019974708557\n",
      "After the gam stage, training error is 0.03646 , validation error is 0.03600\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 1.968308\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed MAE=0.034756 validation MAE=0.036085,rank=1\n",
      "[SoftImpute] Iter 2: observed MAE=0.034116 validation MAE=0.036117,rank=1\n",
      "[SoftImpute] Iter 3: observed MAE=0.033614 validation MAE=0.036152,rank=1\n",
      "[SoftImpute] Iter 4: observed MAE=0.033236 validation MAE=0.036173,rank=1\n",
      "[SoftImpute] Iter 5: observed MAE=0.032947 validation MAE=0.036187,rank=1\n",
      "[SoftImpute] Stopped after iteration 5 for lambda=0.039366\n",
      "final num of user group: 2\n",
      "final num of item group: 2\n",
      "change mode state : True\n",
      "time cost: 1.0009925365447998\n",
      "After the matrix factor stage, training error is 0.03295, validation error is 0.03619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.34448, val loss: 0.34765\n",
      "Main effects training epoch: 2, train loss: 0.27354, val loss: 0.27853\n",
      "Main effects training epoch: 3, train loss: 0.20823, val loss: 0.21145\n",
      "Main effects training epoch: 4, train loss: 0.16135, val loss: 0.16514\n",
      "Main effects training epoch: 5, train loss: 0.14048, val loss: 0.14025\n",
      "Main effects training epoch: 6, train loss: 0.13280, val loss: 0.13280\n",
      "Main effects training epoch: 7, train loss: 0.13041, val loss: 0.12922\n",
      "Main effects training epoch: 8, train loss: 0.12977, val loss: 0.12861\n",
      "Main effects training epoch: 9, train loss: 0.12914, val loss: 0.12821\n",
      "Main effects training epoch: 10, train loss: 0.12852, val loss: 0.12844\n",
      "Main effects training epoch: 11, train loss: 0.12742, val loss: 0.12590\n",
      "Main effects training epoch: 12, train loss: 0.12612, val loss: 0.12564\n",
      "Main effects training epoch: 13, train loss: 0.12339, val loss: 0.12252\n",
      "Main effects training epoch: 14, train loss: 0.11835, val loss: 0.11883\n",
      "Main effects training epoch: 15, train loss: 0.11578, val loss: 0.11651\n",
      "Main effects training epoch: 16, train loss: 0.11289, val loss: 0.11344\n",
      "Main effects training epoch: 17, train loss: 0.11081, val loss: 0.11186\n",
      "Main effects training epoch: 18, train loss: 0.11088, val loss: 0.11312\n",
      "Main effects training epoch: 19, train loss: 0.11027, val loss: 0.11199\n",
      "Main effects training epoch: 20, train loss: 0.11090, val loss: 0.11238\n",
      "Main effects training epoch: 21, train loss: 0.10922, val loss: 0.11096\n",
      "Main effects training epoch: 22, train loss: 0.10900, val loss: 0.11106\n",
      "Main effects training epoch: 23, train loss: 0.10645, val loss: 0.10812\n",
      "Main effects training epoch: 24, train loss: 0.10707, val loss: 0.10858\n",
      "Main effects training epoch: 25, train loss: 0.10776, val loss: 0.10768\n",
      "Main effects training epoch: 26, train loss: 0.10556, val loss: 0.10715\n",
      "Main effects training epoch: 27, train loss: 0.10595, val loss: 0.10680\n",
      "Main effects training epoch: 28, train loss: 0.10509, val loss: 0.10619\n",
      "Main effects training epoch: 29, train loss: 0.10460, val loss: 0.10568\n",
      "Main effects training epoch: 30, train loss: 0.10512, val loss: 0.10585\n",
      "Main effects training epoch: 31, train loss: 0.10437, val loss: 0.10598\n",
      "Main effects training epoch: 32, train loss: 0.10431, val loss: 0.10577\n",
      "Main effects training epoch: 33, train loss: 0.10423, val loss: 0.10545\n",
      "Main effects training epoch: 34, train loss: 0.10413, val loss: 0.10561\n",
      "Main effects training epoch: 35, train loss: 0.10424, val loss: 0.10579\n",
      "Main effects training epoch: 36, train loss: 0.10444, val loss: 0.10578\n",
      "Main effects training epoch: 37, train loss: 0.10400, val loss: 0.10575\n",
      "Main effects training epoch: 38, train loss: 0.10438, val loss: 0.10591\n",
      "Main effects training epoch: 39, train loss: 0.10416, val loss: 0.10566\n",
      "Main effects training epoch: 40, train loss: 0.10393, val loss: 0.10524\n",
      "Main effects training epoch: 41, train loss: 0.10418, val loss: 0.10579\n",
      "Main effects training epoch: 42, train loss: 0.10392, val loss: 0.10565\n",
      "Main effects training epoch: 43, train loss: 0.10430, val loss: 0.10612\n",
      "Main effects training epoch: 44, train loss: 0.10409, val loss: 0.10553\n",
      "Main effects training epoch: 45, train loss: 0.10403, val loss: 0.10539\n",
      "Main effects training epoch: 46, train loss: 0.10400, val loss: 0.10547\n",
      "Main effects training epoch: 47, train loss: 0.10472, val loss: 0.10625\n",
      "Main effects training epoch: 48, train loss: 0.10410, val loss: 0.10597\n",
      "Main effects training epoch: 49, train loss: 0.10415, val loss: 0.10512\n",
      "Main effects training epoch: 50, train loss: 0.10408, val loss: 0.10560\n",
      "Main effects training epoch: 51, train loss: 0.10392, val loss: 0.10554\n",
      "Main effects training epoch: 52, train loss: 0.10395, val loss: 0.10519\n",
      "Main effects training epoch: 53, train loss: 0.10385, val loss: 0.10520\n",
      "Main effects training epoch: 54, train loss: 0.10398, val loss: 0.10563\n",
      "Main effects training epoch: 55, train loss: 0.10416, val loss: 0.10523\n",
      "Main effects training epoch: 56, train loss: 0.10404, val loss: 0.10586\n",
      "Main effects training epoch: 57, train loss: 0.10399, val loss: 0.10571\n",
      "Main effects training epoch: 58, train loss: 0.10443, val loss: 0.10631\n",
      "Main effects training epoch: 59, train loss: 0.10393, val loss: 0.10549\n",
      "Main effects training epoch: 60, train loss: 0.10386, val loss: 0.10534\n",
      "Main effects training epoch: 61, train loss: 0.10389, val loss: 0.10538\n",
      "Main effects training epoch: 62, train loss: 0.10417, val loss: 0.10552\n",
      "Main effects training epoch: 63, train loss: 0.10396, val loss: 0.10570\n",
      "Main effects training epoch: 64, train loss: 0.10412, val loss: 0.10546\n",
      "Main effects training epoch: 65, train loss: 0.10403, val loss: 0.10549\n",
      "Main effects training epoch: 66, train loss: 0.10398, val loss: 0.10514\n",
      "Main effects training epoch: 67, train loss: 0.10385, val loss: 0.10514\n",
      "Main effects training epoch: 68, train loss: 0.10396, val loss: 0.10561\n",
      "Main effects training epoch: 69, train loss: 0.10397, val loss: 0.10523\n",
      "Main effects training epoch: 70, train loss: 0.10420, val loss: 0.10542\n",
      "Main effects training epoch: 71, train loss: 0.10473, val loss: 0.10671\n",
      "Main effects training epoch: 72, train loss: 0.10418, val loss: 0.10552\n",
      "Main effects training epoch: 73, train loss: 0.10427, val loss: 0.10590\n",
      "Main effects training epoch: 74, train loss: 0.10409, val loss: 0.10540\n",
      "Main effects training epoch: 75, train loss: 0.10401, val loss: 0.10542\n",
      "Main effects training epoch: 76, train loss: 0.10384, val loss: 0.10560\n",
      "Main effects training epoch: 77, train loss: 0.10404, val loss: 0.10542\n",
      "Main effects training epoch: 78, train loss: 0.10394, val loss: 0.10550\n",
      "Main effects training epoch: 79, train loss: 0.10410, val loss: 0.10553\n",
      "Main effects training epoch: 80, train loss: 0.10425, val loss: 0.10542\n",
      "Main effects training epoch: 81, train loss: 0.10387, val loss: 0.10530\n",
      "Main effects training epoch: 82, train loss: 0.10409, val loss: 0.10607\n",
      "Main effects training epoch: 83, train loss: 0.10386, val loss: 0.10559\n",
      "Main effects training epoch: 84, train loss: 0.10405, val loss: 0.10549\n",
      "Main effects training epoch: 85, train loss: 0.10483, val loss: 0.10617\n",
      "Main effects training epoch: 86, train loss: 0.10383, val loss: 0.10551\n",
      "Main effects training epoch: 87, train loss: 0.10415, val loss: 0.10524\n",
      "Main effects training epoch: 88, train loss: 0.10389, val loss: 0.10586\n",
      "Main effects training epoch: 89, train loss: 0.10383, val loss: 0.10548\n",
      "Main effects training epoch: 90, train loss: 0.10400, val loss: 0.10512\n",
      "Main effects training epoch: 91, train loss: 0.10387, val loss: 0.10538\n",
      "Main effects training epoch: 92, train loss: 0.10420, val loss: 0.10635\n",
      "Main effects training epoch: 93, train loss: 0.10396, val loss: 0.10529\n",
      "Main effects training epoch: 94, train loss: 0.10417, val loss: 0.10594\n",
      "Main effects training epoch: 95, train loss: 0.10387, val loss: 0.10562\n",
      "Main effects training epoch: 96, train loss: 0.10378, val loss: 0.10533\n",
      "Main effects training epoch: 97, train loss: 0.10386, val loss: 0.10536\n",
      "Main effects training epoch: 98, train loss: 0.10425, val loss: 0.10630\n",
      "Main effects training epoch: 99, train loss: 0.10426, val loss: 0.10552\n",
      "Main effects training epoch: 100, train loss: 0.10419, val loss: 0.10632\n",
      "Main effects training epoch: 101, train loss: 0.10395, val loss: 0.10553\n",
      "Main effects training epoch: 102, train loss: 0.10397, val loss: 0.10584\n",
      "Main effects training epoch: 103, train loss: 0.10408, val loss: 0.10574\n",
      "Main effects training epoch: 104, train loss: 0.10395, val loss: 0.10576\n",
      "Main effects training epoch: 105, train loss: 0.10380, val loss: 0.10555\n",
      "Main effects training epoch: 106, train loss: 0.10382, val loss: 0.10590\n",
      "Main effects training epoch: 107, train loss: 0.10417, val loss: 0.10600\n",
      "Main effects training epoch: 108, train loss: 0.10410, val loss: 0.10567\n",
      "Main effects training epoch: 109, train loss: 0.10373, val loss: 0.10542\n",
      "Main effects training epoch: 110, train loss: 0.10377, val loss: 0.10525\n",
      "Main effects training epoch: 111, train loss: 0.10405, val loss: 0.10577\n",
      "Main effects training epoch: 112, train loss: 0.10409, val loss: 0.10590\n",
      "Main effects training epoch: 113, train loss: 0.10474, val loss: 0.10615\n",
      "Main effects training epoch: 114, train loss: 0.10406, val loss: 0.10594\n",
      "Main effects training epoch: 115, train loss: 0.10415, val loss: 0.10553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 116, train loss: 0.10437, val loss: 0.10680\n",
      "Main effects training epoch: 117, train loss: 0.10399, val loss: 0.10532\n",
      "Main effects training epoch: 118, train loss: 0.10393, val loss: 0.10583\n",
      "Main effects training epoch: 119, train loss: 0.10410, val loss: 0.10601\n",
      "Main effects training epoch: 120, train loss: 0.10375, val loss: 0.10578\n",
      "Main effects training epoch: 121, train loss: 0.10382, val loss: 0.10529\n",
      "Main effects training epoch: 122, train loss: 0.10391, val loss: 0.10552\n",
      "Main effects training epoch: 123, train loss: 0.10397, val loss: 0.10615\n",
      "Main effects training epoch: 124, train loss: 0.10422, val loss: 0.10572\n",
      "Main effects training epoch: 125, train loss: 0.10414, val loss: 0.10601\n",
      "Main effects training epoch: 126, train loss: 0.10402, val loss: 0.10605\n",
      "Main effects training epoch: 127, train loss: 0.10402, val loss: 0.10531\n",
      "Main effects training epoch: 128, train loss: 0.10395, val loss: 0.10615\n",
      "Main effects training epoch: 129, train loss: 0.10410, val loss: 0.10643\n",
      "Main effects training epoch: 130, train loss: 0.10401, val loss: 0.10526\n",
      "Main effects training epoch: 131, train loss: 0.10380, val loss: 0.10577\n",
      "Main effects training epoch: 132, train loss: 0.10404, val loss: 0.10554\n",
      "Main effects training epoch: 133, train loss: 0.10425, val loss: 0.10648\n",
      "Main effects training epoch: 134, train loss: 0.10454, val loss: 0.10616\n",
      "Main effects training epoch: 135, train loss: 0.10424, val loss: 0.10653\n",
      "Main effects training epoch: 136, train loss: 0.10426, val loss: 0.10565\n",
      "Main effects training epoch: 137, train loss: 0.10444, val loss: 0.10639\n",
      "Main effects training epoch: 138, train loss: 0.10433, val loss: 0.10622\n",
      "Main effects training epoch: 139, train loss: 0.10384, val loss: 0.10565\n",
      "Main effects training epoch: 140, train loss: 0.10392, val loss: 0.10539\n",
      "Main effects training epoch: 141, train loss: 0.10420, val loss: 0.10593\n",
      "Main effects training epoch: 142, train loss: 0.10404, val loss: 0.10617\n",
      "Main effects training epoch: 143, train loss: 0.10420, val loss: 0.10567\n",
      "Main effects training epoch: 144, train loss: 0.10383, val loss: 0.10601\n",
      "Main effects training epoch: 145, train loss: 0.10370, val loss: 0.10530\n",
      "Main effects training epoch: 146, train loss: 0.10381, val loss: 0.10596\n",
      "Main effects training epoch: 147, train loss: 0.10380, val loss: 0.10575\n",
      "Main effects training epoch: 148, train loss: 0.10377, val loss: 0.10571\n",
      "Main effects training epoch: 149, train loss: 0.10383, val loss: 0.10566\n",
      "Main effects training epoch: 150, train loss: 0.10393, val loss: 0.10604\n",
      "Early stop at epoch 150, with validation loss: 0.10604\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.10405, val loss: 0.10498\n",
      "Main effects tuning epoch: 2, train loss: 0.10423, val loss: 0.10552\n",
      "Main effects tuning epoch: 3, train loss: 0.10411, val loss: 0.10505\n",
      "Main effects tuning epoch: 4, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 5, train loss: 0.10382, val loss: 0.10498\n",
      "Main effects tuning epoch: 6, train loss: 0.10425, val loss: 0.10582\n",
      "Main effects tuning epoch: 7, train loss: 0.10408, val loss: 0.10530\n",
      "Main effects tuning epoch: 8, train loss: 0.10397, val loss: 0.10527\n",
      "Main effects tuning epoch: 9, train loss: 0.10401, val loss: 0.10507\n",
      "Main effects tuning epoch: 10, train loss: 0.10407, val loss: 0.10539\n",
      "Main effects tuning epoch: 11, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 12, train loss: 0.10393, val loss: 0.10556\n",
      "Main effects tuning epoch: 13, train loss: 0.10415, val loss: 0.10510\n",
      "Main effects tuning epoch: 14, train loss: 0.10397, val loss: 0.10570\n",
      "Main effects tuning epoch: 15, train loss: 0.10386, val loss: 0.10516\n",
      "Main effects tuning epoch: 16, train loss: 0.10395, val loss: 0.10562\n",
      "Main effects tuning epoch: 17, train loss: 0.10395, val loss: 0.10539\n",
      "Main effects tuning epoch: 18, train loss: 0.10422, val loss: 0.10568\n",
      "Main effects tuning epoch: 19, train loss: 0.10433, val loss: 0.10517\n",
      "Main effects tuning epoch: 20, train loss: 0.10384, val loss: 0.10568\n",
      "Main effects tuning epoch: 21, train loss: 0.10400, val loss: 0.10541\n",
      "Main effects tuning epoch: 22, train loss: 0.10393, val loss: 0.10526\n",
      "Main effects tuning epoch: 23, train loss: 0.10434, val loss: 0.10582\n",
      "Main effects tuning epoch: 24, train loss: 0.10406, val loss: 0.10540\n",
      "Main effects tuning epoch: 25, train loss: 0.10410, val loss: 0.10570\n",
      "Main effects tuning epoch: 26, train loss: 0.10405, val loss: 0.10569\n",
      "Main effects tuning epoch: 27, train loss: 0.10406, val loss: 0.10556\n",
      "Main effects tuning epoch: 28, train loss: 0.10394, val loss: 0.10542\n",
      "Main effects tuning epoch: 29, train loss: 0.10383, val loss: 0.10542\n",
      "Main effects tuning epoch: 30, train loss: 0.10385, val loss: 0.10548\n",
      "Main effects tuning epoch: 31, train loss: 0.10395, val loss: 0.10516\n",
      "Main effects tuning epoch: 32, train loss: 0.10391, val loss: 0.10584\n",
      "Main effects tuning epoch: 33, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 34, train loss: 0.10391, val loss: 0.10561\n",
      "Main effects tuning epoch: 35, train loss: 0.10424, val loss: 0.10531\n",
      "Main effects tuning epoch: 36, train loss: 0.10395, val loss: 0.10554\n",
      "Main effects tuning epoch: 37, train loss: 0.10400, val loss: 0.10600\n",
      "Main effects tuning epoch: 38, train loss: 0.10408, val loss: 0.10530\n",
      "Main effects tuning epoch: 39, train loss: 0.10394, val loss: 0.10565\n",
      "Main effects tuning epoch: 40, train loss: 0.10385, val loss: 0.10569\n",
      "Main effects tuning epoch: 41, train loss: 0.10390, val loss: 0.10539\n",
      "Main effects tuning epoch: 42, train loss: 0.10400, val loss: 0.10587\n",
      "Main effects tuning epoch: 43, train loss: 0.10386, val loss: 0.10547\n",
      "Main effects tuning epoch: 44, train loss: 0.10403, val loss: 0.10597\n",
      "Main effects tuning epoch: 45, train loss: 0.10384, val loss: 0.10541\n",
      "Main effects tuning epoch: 46, train loss: 0.10398, val loss: 0.10582\n",
      "Main effects tuning epoch: 47, train loss: 0.10375, val loss: 0.10527\n",
      "Main effects tuning epoch: 48, train loss: 0.10380, val loss: 0.10549\n",
      "Main effects tuning epoch: 49, train loss: 0.10396, val loss: 0.10566\n",
      "Main effects tuning epoch: 50, train loss: 0.10407, val loss: 0.10549\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.09376, val loss: 0.09232\n",
      "Interaction training epoch: 2, train loss: 0.20140, val loss: 0.20111\n",
      "Interaction training epoch: 3, train loss: 0.06617, val loss: 0.06860\n",
      "Interaction training epoch: 4, train loss: 0.05173, val loss: 0.05197\n",
      "Interaction training epoch: 5, train loss: 0.06497, val loss: 0.06595\n",
      "Interaction training epoch: 6, train loss: 0.04386, val loss: 0.04449\n",
      "Interaction training epoch: 7, train loss: 0.04606, val loss: 0.04529\n",
      "Interaction training epoch: 8, train loss: 0.04492, val loss: 0.04489\n",
      "Interaction training epoch: 9, train loss: 0.04997, val loss: 0.04868\n",
      "Interaction training epoch: 10, train loss: 0.04002, val loss: 0.04133\n",
      "Interaction training epoch: 11, train loss: 0.04269, val loss: 0.04306\n",
      "Interaction training epoch: 12, train loss: 0.04100, val loss: 0.03997\n",
      "Interaction training epoch: 13, train loss: 0.04366, val loss: 0.04360\n",
      "Interaction training epoch: 14, train loss: 0.04928, val loss: 0.05084\n",
      "Interaction training epoch: 15, train loss: 0.04532, val loss: 0.04488\n",
      "Interaction training epoch: 16, train loss: 0.04530, val loss: 0.04524\n",
      "Interaction training epoch: 17, train loss: 0.03784, val loss: 0.03763\n",
      "Interaction training epoch: 18, train loss: 0.04939, val loss: 0.05023\n",
      "Interaction training epoch: 19, train loss: 0.03608, val loss: 0.03597\n",
      "Interaction training epoch: 20, train loss: 0.04344, val loss: 0.04367\n",
      "Interaction training epoch: 21, train loss: 0.04002, val loss: 0.03932\n",
      "Interaction training epoch: 22, train loss: 0.03977, val loss: 0.03977\n",
      "Interaction training epoch: 23, train loss: 0.03777, val loss: 0.03709\n",
      "Interaction training epoch: 24, train loss: 0.03927, val loss: 0.03924\n",
      "Interaction training epoch: 25, train loss: 0.04164, val loss: 0.04112\n",
      "Interaction training epoch: 26, train loss: 0.04429, val loss: 0.04389\n",
      "Interaction training epoch: 27, train loss: 0.04417, val loss: 0.04416\n",
      "Interaction training epoch: 28, train loss: 0.04241, val loss: 0.04112\n",
      "Interaction training epoch: 29, train loss: 0.03548, val loss: 0.03672\n",
      "Interaction training epoch: 30, train loss: 0.04420, val loss: 0.04353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 31, train loss: 0.04020, val loss: 0.04004\n",
      "Interaction training epoch: 32, train loss: 0.03422, val loss: 0.03451\n",
      "Interaction training epoch: 33, train loss: 0.03378, val loss: 0.03407\n",
      "Interaction training epoch: 34, train loss: 0.03925, val loss: 0.03880\n",
      "Interaction training epoch: 35, train loss: 0.03752, val loss: 0.03636\n",
      "Interaction training epoch: 36, train loss: 0.03582, val loss: 0.03564\n",
      "Interaction training epoch: 37, train loss: 0.03069, val loss: 0.03087\n",
      "Interaction training epoch: 38, train loss: 0.03729, val loss: 0.03652\n",
      "Interaction training epoch: 39, train loss: 0.03772, val loss: 0.03772\n",
      "Interaction training epoch: 40, train loss: 0.04332, val loss: 0.04201\n",
      "Interaction training epoch: 41, train loss: 0.03409, val loss: 0.03435\n",
      "Interaction training epoch: 42, train loss: 0.03501, val loss: 0.03419\n",
      "Interaction training epoch: 43, train loss: 0.03630, val loss: 0.03573\n",
      "Interaction training epoch: 44, train loss: 0.03447, val loss: 0.03420\n",
      "Interaction training epoch: 45, train loss: 0.03622, val loss: 0.03541\n",
      "Interaction training epoch: 46, train loss: 0.02991, val loss: 0.03002\n",
      "Interaction training epoch: 47, train loss: 0.02880, val loss: 0.02856\n",
      "Interaction training epoch: 48, train loss: 0.03548, val loss: 0.03469\n",
      "Interaction training epoch: 49, train loss: 0.03399, val loss: 0.03256\n",
      "Interaction training epoch: 50, train loss: 0.03225, val loss: 0.03266\n",
      "Interaction training epoch: 51, train loss: 0.03286, val loss: 0.03176\n",
      "Interaction training epoch: 52, train loss: 0.03084, val loss: 0.03012\n",
      "Interaction training epoch: 53, train loss: 0.03059, val loss: 0.02998\n",
      "Interaction training epoch: 54, train loss: 0.02966, val loss: 0.02932\n",
      "Interaction training epoch: 55, train loss: 0.04122, val loss: 0.04119\n",
      "Interaction training epoch: 56, train loss: 0.03164, val loss: 0.03059\n",
      "Interaction training epoch: 57, train loss: 0.03483, val loss: 0.03469\n",
      "Interaction training epoch: 58, train loss: 0.03320, val loss: 0.03294\n",
      "Interaction training epoch: 59, train loss: 0.02999, val loss: 0.02930\n",
      "Interaction training epoch: 60, train loss: 0.03540, val loss: 0.03496\n",
      "Interaction training epoch: 61, train loss: 0.03142, val loss: 0.02962\n",
      "Interaction training epoch: 62, train loss: 0.03146, val loss: 0.03164\n",
      "Interaction training epoch: 63, train loss: 0.03481, val loss: 0.03421\n",
      "Interaction training epoch: 64, train loss: 0.02946, val loss: 0.02881\n",
      "Interaction training epoch: 65, train loss: 0.03539, val loss: 0.03511\n",
      "Interaction training epoch: 66, train loss: 0.03797, val loss: 0.03805\n",
      "Interaction training epoch: 67, train loss: 0.03669, val loss: 0.03627\n",
      "Interaction training epoch: 68, train loss: 0.04844, val loss: 0.04881\n",
      "Interaction training epoch: 69, train loss: 0.02947, val loss: 0.02941\n",
      "Interaction training epoch: 70, train loss: 0.03805, val loss: 0.03672\n",
      "Interaction training epoch: 71, train loss: 0.02990, val loss: 0.02970\n",
      "Interaction training epoch: 72, train loss: 0.03926, val loss: 0.03952\n",
      "Interaction training epoch: 73, train loss: 0.03713, val loss: 0.03732\n",
      "Interaction training epoch: 74, train loss: 0.04469, val loss: 0.04392\n",
      "Interaction training epoch: 75, train loss: 0.03518, val loss: 0.03491\n",
      "Interaction training epoch: 76, train loss: 0.03954, val loss: 0.03893\n",
      "Interaction training epoch: 77, train loss: 0.04294, val loss: 0.04248\n",
      "Interaction training epoch: 78, train loss: 0.03122, val loss: 0.03170\n",
      "Interaction training epoch: 79, train loss: 0.03577, val loss: 0.03538\n",
      "Interaction training epoch: 80, train loss: 0.04727, val loss: 0.04664\n",
      "Interaction training epoch: 81, train loss: 0.03478, val loss: 0.03412\n",
      "Interaction training epoch: 82, train loss: 0.03736, val loss: 0.03700\n",
      "Interaction training epoch: 83, train loss: 0.03716, val loss: 0.03665\n",
      "Interaction training epoch: 84, train loss: 0.03675, val loss: 0.03575\n",
      "Interaction training epoch: 85, train loss: 0.03815, val loss: 0.03817\n",
      "Interaction training epoch: 86, train loss: 0.03960, val loss: 0.03903\n",
      "Interaction training epoch: 87, train loss: 0.03498, val loss: 0.03402\n",
      "Interaction training epoch: 88, train loss: 0.03630, val loss: 0.03583\n",
      "Interaction training epoch: 89, train loss: 0.03454, val loss: 0.03517\n",
      "Interaction training epoch: 90, train loss: 0.07231, val loss: 0.07223\n",
      "Interaction training epoch: 91, train loss: 0.04800, val loss: 0.04802\n",
      "Interaction training epoch: 92, train loss: 0.05224, val loss: 0.05162\n",
      "Interaction training epoch: 93, train loss: 0.02794, val loss: 0.02804\n",
      "Interaction training epoch: 94, train loss: 0.03585, val loss: 0.03546\n",
      "Interaction training epoch: 95, train loss: 0.04149, val loss: 0.04158\n",
      "Interaction training epoch: 96, train loss: 0.04943, val loss: 0.04909\n",
      "Interaction training epoch: 97, train loss: 0.05722, val loss: 0.05690\n",
      "Interaction training epoch: 98, train loss: 0.04854, val loss: 0.04831\n",
      "Interaction training epoch: 99, train loss: 0.02719, val loss: 0.02716\n",
      "Interaction training epoch: 100, train loss: 0.03689, val loss: 0.03722\n",
      "Interaction training epoch: 101, train loss: 0.04609, val loss: 0.04601\n",
      "Interaction training epoch: 102, train loss: 0.03250, val loss: 0.03231\n",
      "Interaction training epoch: 103, train loss: 0.03006, val loss: 0.02936\n",
      "Interaction training epoch: 104, train loss: 0.03967, val loss: 0.03937\n",
      "Interaction training epoch: 105, train loss: 0.03096, val loss: 0.03100\n",
      "Interaction training epoch: 106, train loss: 0.05099, val loss: 0.05044\n",
      "Interaction training epoch: 107, train loss: 0.03358, val loss: 0.03389\n",
      "Interaction training epoch: 108, train loss: 0.04273, val loss: 0.04236\n",
      "Interaction training epoch: 109, train loss: 0.07259, val loss: 0.07140\n",
      "Interaction training epoch: 110, train loss: 0.03442, val loss: 0.03433\n",
      "Interaction training epoch: 111, train loss: 0.02947, val loss: 0.02893\n",
      "Interaction training epoch: 112, train loss: 0.03413, val loss: 0.03396\n",
      "Interaction training epoch: 113, train loss: 0.03441, val loss: 0.03428\n",
      "Interaction training epoch: 114, train loss: 0.05762, val loss: 0.05645\n",
      "Interaction training epoch: 115, train loss: 0.02938, val loss: 0.02918\n",
      "Interaction training epoch: 116, train loss: 0.04805, val loss: 0.04824\n",
      "Interaction training epoch: 117, train loss: 0.03927, val loss: 0.03835\n",
      "Interaction training epoch: 118, train loss: 0.03949, val loss: 0.03920\n",
      "Interaction training epoch: 119, train loss: 0.03835, val loss: 0.03852\n",
      "Interaction training epoch: 120, train loss: 0.07402, val loss: 0.07267\n",
      "Interaction training epoch: 121, train loss: 0.02786, val loss: 0.02794\n",
      "Interaction training epoch: 122, train loss: 0.02966, val loss: 0.02840\n",
      "Interaction training epoch: 123, train loss: 0.03146, val loss: 0.03080\n",
      "Interaction training epoch: 124, train loss: 0.03135, val loss: 0.03095\n",
      "Interaction training epoch: 125, train loss: 0.02893, val loss: 0.02832\n",
      "Interaction training epoch: 126, train loss: 0.06644, val loss: 0.06554\n",
      "Interaction training epoch: 127, train loss: 0.02439, val loss: 0.02405\n",
      "Interaction training epoch: 128, train loss: 0.02442, val loss: 0.02392\n",
      "Interaction training epoch: 129, train loss: 0.06718, val loss: 0.06662\n",
      "Interaction training epoch: 130, train loss: 0.06873, val loss: 0.06752\n",
      "Interaction training epoch: 131, train loss: 0.02836, val loss: 0.02795\n",
      "Interaction training epoch: 132, train loss: 0.02537, val loss: 0.02462\n",
      "Interaction training epoch: 133, train loss: 0.05578, val loss: 0.05497\n",
      "Interaction training epoch: 134, train loss: 0.02756, val loss: 0.02628\n",
      "Interaction training epoch: 135, train loss: 0.03851, val loss: 0.03792\n",
      "Interaction training epoch: 136, train loss: 0.02792, val loss: 0.02763\n",
      "Interaction training epoch: 137, train loss: 0.02882, val loss: 0.02822\n",
      "Interaction training epoch: 138, train loss: 0.04268, val loss: 0.04230\n",
      "Interaction training epoch: 139, train loss: 0.02999, val loss: 0.03028\n",
      "Interaction training epoch: 140, train loss: 0.02653, val loss: 0.02598\n",
      "Interaction training epoch: 141, train loss: 0.02864, val loss: 0.02740\n",
      "Interaction training epoch: 142, train loss: 0.04031, val loss: 0.03930\n",
      "Interaction training epoch: 143, train loss: 0.08930, val loss: 0.08825\n",
      "Interaction training epoch: 144, train loss: 0.03278, val loss: 0.03201\n",
      "Interaction training epoch: 145, train loss: 0.07595, val loss: 0.07444\n",
      "Interaction training epoch: 146, train loss: 0.04088, val loss: 0.04007\n",
      "Interaction training epoch: 147, train loss: 0.03063, val loss: 0.02967\n",
      "Interaction training epoch: 148, train loss: 0.08210, val loss: 0.08116\n",
      "Interaction training epoch: 149, train loss: 0.02600, val loss: 0.02595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 150, train loss: 0.04451, val loss: 0.04266\n",
      "Interaction training epoch: 151, train loss: 0.05071, val loss: 0.04953\n",
      "Interaction training epoch: 152, train loss: 0.03689, val loss: 0.03657\n",
      "Interaction training epoch: 153, train loss: 0.05108, val loss: 0.05017\n",
      "Interaction training epoch: 154, train loss: 0.03125, val loss: 0.03020\n",
      "Interaction training epoch: 155, train loss: 0.05262, val loss: 0.05160\n",
      "Interaction training epoch: 156, train loss: 0.03189, val loss: 0.03099\n",
      "Interaction training epoch: 157, train loss: 0.03302, val loss: 0.03192\n",
      "Interaction training epoch: 158, train loss: 0.03919, val loss: 0.03867\n",
      "Interaction training epoch: 159, train loss: 0.06294, val loss: 0.06164\n",
      "Interaction training epoch: 160, train loss: 0.02504, val loss: 0.02398\n",
      "Interaction training epoch: 161, train loss: 0.02726, val loss: 0.02622\n",
      "Interaction training epoch: 162, train loss: 0.02696, val loss: 0.02663\n",
      "Interaction training epoch: 163, train loss: 0.04443, val loss: 0.04306\n",
      "Interaction training epoch: 164, train loss: 0.08229, val loss: 0.08080\n",
      "Interaction training epoch: 165, train loss: 0.02497, val loss: 0.02433\n",
      "Interaction training epoch: 166, train loss: 0.02405, val loss: 0.02321\n",
      "Interaction training epoch: 167, train loss: 0.03154, val loss: 0.03087\n",
      "Interaction training epoch: 168, train loss: 0.03079, val loss: 0.02948\n",
      "Interaction training epoch: 169, train loss: 0.02562, val loss: 0.02466\n",
      "Interaction training epoch: 170, train loss: 0.03950, val loss: 0.03811\n",
      "Interaction training epoch: 171, train loss: 0.05102, val loss: 0.05005\n",
      "Interaction training epoch: 172, train loss: 0.07393, val loss: 0.07199\n",
      "Interaction training epoch: 173, train loss: 0.03775, val loss: 0.03659\n",
      "Interaction training epoch: 174, train loss: 0.03283, val loss: 0.03172\n",
      "Interaction training epoch: 175, train loss: 0.02729, val loss: 0.02663\n",
      "Interaction training epoch: 176, train loss: 0.04249, val loss: 0.04090\n",
      "Interaction training epoch: 177, train loss: 0.11371, val loss: 0.11248\n",
      "Interaction training epoch: 178, train loss: 0.05008, val loss: 0.04851\n",
      "Interaction training epoch: 179, train loss: 0.05070, val loss: 0.04903\n",
      "Interaction training epoch: 180, train loss: 0.05192, val loss: 0.05082\n",
      "Interaction training epoch: 181, train loss: 0.03499, val loss: 0.03401\n",
      "Interaction training epoch: 182, train loss: 0.06009, val loss: 0.05850\n",
      "Interaction training epoch: 183, train loss: 0.03517, val loss: 0.03374\n",
      "Interaction training epoch: 184, train loss: 0.04107, val loss: 0.04010\n",
      "Interaction training epoch: 185, train loss: 0.06361, val loss: 0.06162\n",
      "Interaction training epoch: 186, train loss: 0.06349, val loss: 0.06178\n",
      "Interaction training epoch: 187, train loss: 0.05374, val loss: 0.05196\n",
      "Interaction training epoch: 188, train loss: 0.05958, val loss: 0.05804\n",
      "Interaction training epoch: 189, train loss: 0.02604, val loss: 0.02519\n",
      "Interaction training epoch: 190, train loss: 0.04369, val loss: 0.04302\n",
      "Interaction training epoch: 191, train loss: 0.04175, val loss: 0.04116\n",
      "Interaction training epoch: 192, train loss: 0.06782, val loss: 0.06645\n",
      "Interaction training epoch: 193, train loss: 0.06145, val loss: 0.06034\n",
      "Interaction training epoch: 194, train loss: 0.04303, val loss: 0.04151\n",
      "Interaction training epoch: 195, train loss: 0.03319, val loss: 0.03229\n",
      "Interaction training epoch: 196, train loss: 0.03143, val loss: 0.02941\n",
      "Interaction training epoch: 197, train loss: 0.03413, val loss: 0.03311\n",
      "Interaction training epoch: 198, train loss: 0.03045, val loss: 0.02901\n",
      "Interaction training epoch: 199, train loss: 0.04253, val loss: 0.04138\n",
      "Interaction training epoch: 200, train loss: 0.04032, val loss: 0.03845\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########4 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.02853, val loss: 0.02785\n",
      "Interaction tuning epoch: 2, train loss: 0.02401, val loss: 0.02299\n",
      "Interaction tuning epoch: 3, train loss: 0.04478, val loss: 0.04265\n",
      "Interaction tuning epoch: 4, train loss: 0.06395, val loss: 0.06292\n",
      "Interaction tuning epoch: 5, train loss: 0.02356, val loss: 0.02286\n",
      "Interaction tuning epoch: 6, train loss: 0.06369, val loss: 0.06228\n",
      "Interaction tuning epoch: 7, train loss: 0.04154, val loss: 0.04077\n",
      "Interaction tuning epoch: 8, train loss: 0.08354, val loss: 0.08227\n",
      "Interaction tuning epoch: 9, train loss: 0.07421, val loss: 0.07270\n",
      "Interaction tuning epoch: 10, train loss: 0.04015, val loss: 0.03931\n",
      "Interaction tuning epoch: 11, train loss: 0.02840, val loss: 0.02746\n",
      "Interaction tuning epoch: 12, train loss: 0.06172, val loss: 0.06021\n",
      "Interaction tuning epoch: 13, train loss: 0.03776, val loss: 0.03699\n",
      "Interaction tuning epoch: 14, train loss: 0.07388, val loss: 0.07213\n",
      "Interaction tuning epoch: 15, train loss: 0.02628, val loss: 0.02557\n",
      "Interaction tuning epoch: 16, train loss: 0.02860, val loss: 0.02813\n",
      "Interaction tuning epoch: 17, train loss: 0.04973, val loss: 0.04802\n",
      "Interaction tuning epoch: 18, train loss: 0.02869, val loss: 0.02741\n",
      "Interaction tuning epoch: 19, train loss: 0.03914, val loss: 0.03730\n",
      "Interaction tuning epoch: 20, train loss: 0.03596, val loss: 0.03526\n",
      "Interaction tuning epoch: 21, train loss: 0.04924, val loss: 0.04797\n",
      "Interaction tuning epoch: 22, train loss: 0.08235, val loss: 0.08148\n",
      "Interaction tuning epoch: 23, train loss: 0.08430, val loss: 0.08186\n",
      "Interaction tuning epoch: 24, train loss: 0.07758, val loss: 0.07581\n",
      "Interaction tuning epoch: 25, train loss: 0.02468, val loss: 0.02425\n",
      "Interaction tuning epoch: 26, train loss: 0.02729, val loss: 0.02674\n",
      "Interaction tuning epoch: 27, train loss: 0.02677, val loss: 0.02542\n",
      "Interaction tuning epoch: 28, train loss: 0.03303, val loss: 0.03233\n",
      "Interaction tuning epoch: 29, train loss: 0.07628, val loss: 0.07409\n",
      "Interaction tuning epoch: 30, train loss: 0.05354, val loss: 0.05248\n",
      "Interaction tuning epoch: 31, train loss: 0.02819, val loss: 0.02642\n",
      "Interaction tuning epoch: 32, train loss: 0.02994, val loss: 0.02905\n",
      "Interaction tuning epoch: 33, train loss: 0.03903, val loss: 0.03809\n",
      "Interaction tuning epoch: 34, train loss: 0.03406, val loss: 0.03261\n",
      "Interaction tuning epoch: 35, train loss: 0.02886, val loss: 0.02797\n",
      "Interaction tuning epoch: 36, train loss: 0.04728, val loss: 0.04591\n",
      "Interaction tuning epoch: 37, train loss: 0.02745, val loss: 0.02653\n",
      "Interaction tuning epoch: 38, train loss: 0.09613, val loss: 0.09462\n",
      "Interaction tuning epoch: 39, train loss: 0.06430, val loss: 0.06326\n",
      "Interaction tuning epoch: 40, train loss: 0.03564, val loss: 0.03467\n",
      "Interaction tuning epoch: 41, train loss: 0.05842, val loss: 0.05720\n",
      "Interaction tuning epoch: 42, train loss: 0.02719, val loss: 0.02601\n",
      "Interaction tuning epoch: 43, train loss: 0.03689, val loss: 0.03533\n",
      "Interaction tuning epoch: 44, train loss: 0.02563, val loss: 0.02452\n",
      "Interaction tuning epoch: 45, train loss: 0.11515, val loss: 0.11336\n",
      "Interaction tuning epoch: 46, train loss: 0.05383, val loss: 0.05274\n",
      "Interaction tuning epoch: 47, train loss: 0.03183, val loss: 0.02986\n",
      "Interaction tuning epoch: 48, train loss: 0.04755, val loss: 0.04566\n",
      "Interaction tuning epoch: 49, train loss: 0.03504, val loss: 0.03437\n",
      "Interaction tuning epoch: 50, train loss: 0.03646, val loss: 0.03600\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 27.37300181388855\n",
      "After the gam stage, training error is 0.03646 , validation error is 0.03600\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 1.968308\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.032180 validation MAE=0.035162,rank=2\n",
      "[SoftImpute] Iter 2: observed MAE=0.030799 validation MAE=0.034645,rank=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 3: observed MAE=0.029483 validation MAE=0.034064,rank=2\n",
      "[SoftImpute] Iter 4: observed MAE=0.028256 validation MAE=0.033429,rank=2\n",
      "[SoftImpute] Iter 5: observed MAE=0.027109 validation MAE=0.032743,rank=2\n",
      "[SoftImpute] Iter 6: observed MAE=0.026043 validation MAE=0.032055,rank=2\n",
      "[SoftImpute] Iter 7: observed MAE=0.025050 validation MAE=0.031366,rank=2\n",
      "[SoftImpute] Iter 8: observed MAE=0.024122 validation MAE=0.030693,rank=2\n",
      "[SoftImpute] Iter 9: observed MAE=0.023267 validation MAE=0.030038,rank=2\n",
      "[SoftImpute] Iter 10: observed MAE=0.022482 validation MAE=0.029422,rank=2\n",
      "[SoftImpute] Iter 11: observed MAE=0.021758 validation MAE=0.028871,rank=2\n",
      "[SoftImpute] Iter 12: observed MAE=0.021093 validation MAE=0.028383,rank=2\n",
      "[SoftImpute] Iter 13: observed MAE=0.020482 validation MAE=0.027916,rank=2\n",
      "[SoftImpute] Iter 14: observed MAE=0.019923 validation MAE=0.027471,rank=2\n",
      "[SoftImpute] Iter 15: observed MAE=0.019409 validation MAE=0.027051,rank=2\n",
      "[SoftImpute] Iter 16: observed MAE=0.018936 validation MAE=0.026672,rank=2\n",
      "[SoftImpute] Iter 17: observed MAE=0.018500 validation MAE=0.026317,rank=2\n",
      "[SoftImpute] Iter 18: observed MAE=0.018101 validation MAE=0.025978,rank=2\n",
      "[SoftImpute] Iter 19: observed MAE=0.017735 validation MAE=0.025659,rank=2\n",
      "[SoftImpute] Iter 20: observed MAE=0.017397 validation MAE=0.025363,rank=2\n",
      "[SoftImpute] Iter 21: observed MAE=0.017083 validation MAE=0.025085,rank=2\n",
      "[SoftImpute] Iter 22: observed MAE=0.016793 validation MAE=0.024824,rank=2\n",
      "[SoftImpute] Iter 23: observed MAE=0.016524 validation MAE=0.024576,rank=2\n",
      "[SoftImpute] Iter 24: observed MAE=0.016273 validation MAE=0.024341,rank=2\n",
      "[SoftImpute] Iter 25: observed MAE=0.016040 validation MAE=0.024120,rank=2\n",
      "[SoftImpute] Iter 26: observed MAE=0.015823 validation MAE=0.023910,rank=2\n",
      "[SoftImpute] Iter 27: observed MAE=0.015620 validation MAE=0.023712,rank=2\n",
      "[SoftImpute] Iter 28: observed MAE=0.015429 validation MAE=0.023530,rank=2\n",
      "[SoftImpute] Iter 29: observed MAE=0.015250 validation MAE=0.023358,rank=2\n",
      "[SoftImpute] Iter 30: observed MAE=0.015081 validation MAE=0.023193,rank=2\n",
      "[SoftImpute] Iter 31: observed MAE=0.014923 validation MAE=0.023036,rank=2\n",
      "[SoftImpute] Iter 32: observed MAE=0.014773 validation MAE=0.022888,rank=2\n",
      "[SoftImpute] Iter 33: observed MAE=0.014631 validation MAE=0.022750,rank=2\n",
      "[SoftImpute] Iter 34: observed MAE=0.014497 validation MAE=0.022620,rank=2\n",
      "[SoftImpute] Iter 35: observed MAE=0.014371 validation MAE=0.022495,rank=2\n",
      "[SoftImpute] Iter 36: observed MAE=0.014251 validation MAE=0.022376,rank=2\n",
      "[SoftImpute] Iter 37: observed MAE=0.014138 validation MAE=0.022262,rank=2\n",
      "[SoftImpute] Iter 38: observed MAE=0.014032 validation MAE=0.022153,rank=2\n",
      "[SoftImpute] Iter 39: observed MAE=0.013930 validation MAE=0.022048,rank=2\n",
      "[SoftImpute] Iter 40: observed MAE=0.013833 validation MAE=0.021948,rank=2\n",
      "[SoftImpute] Iter 41: observed MAE=0.013742 validation MAE=0.021852,rank=2\n",
      "[SoftImpute] Iter 42: observed MAE=0.013654 validation MAE=0.021760,rank=2\n",
      "[SoftImpute] Iter 43: observed MAE=0.013571 validation MAE=0.021671,rank=2\n",
      "[SoftImpute] Iter 44: observed MAE=0.013491 validation MAE=0.021584,rank=2\n",
      "[SoftImpute] Iter 45: observed MAE=0.013414 validation MAE=0.021501,rank=2\n",
      "[SoftImpute] Iter 46: observed MAE=0.013341 validation MAE=0.021423,rank=2\n",
      "[SoftImpute] Iter 47: observed MAE=0.013271 validation MAE=0.021346,rank=2\n",
      "[SoftImpute] Iter 48: observed MAE=0.013203 validation MAE=0.021275,rank=2\n",
      "[SoftImpute] Iter 49: observed MAE=0.013137 validation MAE=0.021206,rank=2\n",
      "[SoftImpute] Iter 50: observed MAE=0.013074 validation MAE=0.021139,rank=2\n",
      "[SoftImpute] Iter 51: observed MAE=0.013012 validation MAE=0.021074,rank=2\n",
      "[SoftImpute] Iter 52: observed MAE=0.012953 validation MAE=0.021010,rank=2\n",
      "[SoftImpute] Iter 53: observed MAE=0.012896 validation MAE=0.020948,rank=2\n",
      "[SoftImpute] Iter 54: observed MAE=0.012840 validation MAE=0.020886,rank=2\n",
      "[SoftImpute] Iter 55: observed MAE=0.012787 validation MAE=0.020826,rank=2\n",
      "[SoftImpute] Iter 56: observed MAE=0.012735 validation MAE=0.020766,rank=2\n",
      "[SoftImpute] Iter 57: observed MAE=0.012684 validation MAE=0.020707,rank=2\n",
      "[SoftImpute] Iter 58: observed MAE=0.012634 validation MAE=0.020650,rank=2\n",
      "[SoftImpute] Iter 59: observed MAE=0.012586 validation MAE=0.020594,rank=2\n",
      "[SoftImpute] Iter 60: observed MAE=0.012540 validation MAE=0.020540,rank=2\n",
      "[SoftImpute] Iter 61: observed MAE=0.012494 validation MAE=0.020488,rank=2\n",
      "[SoftImpute] Iter 62: observed MAE=0.012450 validation MAE=0.020437,rank=2\n",
      "[SoftImpute] Iter 63: observed MAE=0.012408 validation MAE=0.020387,rank=2\n",
      "[SoftImpute] Iter 64: observed MAE=0.012366 validation MAE=0.020339,rank=2\n",
      "[SoftImpute] Iter 65: observed MAE=0.012325 validation MAE=0.020290,rank=2\n",
      "[SoftImpute] Iter 66: observed MAE=0.012285 validation MAE=0.020244,rank=2\n",
      "[SoftImpute] Iter 67: observed MAE=0.012247 validation MAE=0.020200,rank=2\n",
      "[SoftImpute] Iter 68: observed MAE=0.012209 validation MAE=0.020157,rank=2\n",
      "[SoftImpute] Iter 69: observed MAE=0.012172 validation MAE=0.020115,rank=2\n",
      "[SoftImpute] Iter 70: observed MAE=0.012137 validation MAE=0.020073,rank=2\n",
      "[SoftImpute] Iter 71: observed MAE=0.012101 validation MAE=0.020033,rank=2\n",
      "[SoftImpute] Iter 72: observed MAE=0.012067 validation MAE=0.019994,rank=2\n",
      "[SoftImpute] Iter 73: observed MAE=0.012034 validation MAE=0.019956,rank=2\n",
      "[SoftImpute] Iter 74: observed MAE=0.012001 validation MAE=0.019918,rank=2\n",
      "[SoftImpute] Iter 75: observed MAE=0.011969 validation MAE=0.019882,rank=2\n",
      "[SoftImpute] Iter 76: observed MAE=0.011938 validation MAE=0.019846,rank=2\n",
      "[SoftImpute] Iter 77: observed MAE=0.011908 validation MAE=0.019811,rank=2\n",
      "[SoftImpute] Iter 78: observed MAE=0.011878 validation MAE=0.019775,rank=2\n",
      "[SoftImpute] Iter 79: observed MAE=0.011850 validation MAE=0.019741,rank=2\n",
      "[SoftImpute] Iter 80: observed MAE=0.011822 validation MAE=0.019706,rank=2\n",
      "[SoftImpute] Iter 81: observed MAE=0.011794 validation MAE=0.019672,rank=2\n",
      "[SoftImpute] Iter 82: observed MAE=0.011767 validation MAE=0.019639,rank=2\n",
      "[SoftImpute] Iter 83: observed MAE=0.011741 validation MAE=0.019606,rank=2\n",
      "[SoftImpute] Iter 84: observed MAE=0.011716 validation MAE=0.019574,rank=2\n",
      "[SoftImpute] Iter 85: observed MAE=0.011691 validation MAE=0.019541,rank=2\n",
      "[SoftImpute] Iter 86: observed MAE=0.011666 validation MAE=0.019510,rank=2\n",
      "[SoftImpute] Iter 87: observed MAE=0.011642 validation MAE=0.019480,rank=2\n",
      "[SoftImpute] Iter 88: observed MAE=0.011619 validation MAE=0.019450,rank=2\n",
      "[SoftImpute] Iter 89: observed MAE=0.011596 validation MAE=0.019421,rank=2\n",
      "[SoftImpute] Iter 90: observed MAE=0.011573 validation MAE=0.019391,rank=2\n",
      "[SoftImpute] Iter 91: observed MAE=0.011551 validation MAE=0.019363,rank=2\n",
      "[SoftImpute] Iter 92: observed MAE=0.011529 validation MAE=0.019334,rank=2\n",
      "[SoftImpute] Iter 93: observed MAE=0.011508 validation MAE=0.019306,rank=2\n",
      "[SoftImpute] Iter 94: observed MAE=0.011486 validation MAE=0.019278,rank=2\n",
      "[SoftImpute] Iter 95: observed MAE=0.011466 validation MAE=0.019252,rank=2\n",
      "[SoftImpute] Iter 96: observed MAE=0.011445 validation MAE=0.019225,rank=2\n",
      "[SoftImpute] Iter 97: observed MAE=0.011425 validation MAE=0.019199,rank=2\n",
      "[SoftImpute] Iter 98: observed MAE=0.011405 validation MAE=0.019174,rank=2\n",
      "[SoftImpute] Iter 99: observed MAE=0.011386 validation MAE=0.019149,rank=2\n",
      "[SoftImpute] Iter 100: observed MAE=0.011367 validation MAE=0.019124,rank=2\n",
      "[SoftImpute] Iter 101: observed MAE=0.011348 validation MAE=0.019100,rank=2\n",
      "[SoftImpute] Iter 102: observed MAE=0.011329 validation MAE=0.019076,rank=2\n",
      "[SoftImpute] Iter 103: observed MAE=0.011311 validation MAE=0.019052,rank=2\n",
      "[SoftImpute] Iter 104: observed MAE=0.011293 validation MAE=0.019029,rank=2\n",
      "[SoftImpute] Iter 105: observed MAE=0.011275 validation MAE=0.019006,rank=2\n",
      "[SoftImpute] Iter 106: observed MAE=0.011258 validation MAE=0.018984,rank=2\n",
      "[SoftImpute] Iter 107: observed MAE=0.011240 validation MAE=0.018962,rank=2\n",
      "[SoftImpute] Iter 108: observed MAE=0.011223 validation MAE=0.018941,rank=2\n",
      "[SoftImpute] Iter 109: observed MAE=0.011207 validation MAE=0.018919,rank=2\n",
      "[SoftImpute] Iter 110: observed MAE=0.011190 validation MAE=0.018898,rank=2\n",
      "[SoftImpute] Iter 111: observed MAE=0.011174 validation MAE=0.018877,rank=2\n",
      "[SoftImpute] Iter 112: observed MAE=0.011158 validation MAE=0.018856,rank=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 113: observed MAE=0.011142 validation MAE=0.018835,rank=2\n",
      "[SoftImpute] Iter 114: observed MAE=0.011127 validation MAE=0.018815,rank=2\n",
      "[SoftImpute] Iter 115: observed MAE=0.011111 validation MAE=0.018794,rank=2\n",
      "[SoftImpute] Iter 116: observed MAE=0.011096 validation MAE=0.018774,rank=2\n",
      "[SoftImpute] Iter 117: observed MAE=0.011081 validation MAE=0.018755,rank=2\n",
      "[SoftImpute] Iter 118: observed MAE=0.011067 validation MAE=0.018735,rank=2\n",
      "[SoftImpute] Iter 119: observed MAE=0.011052 validation MAE=0.018716,rank=2\n",
      "[SoftImpute] Iter 120: observed MAE=0.011038 validation MAE=0.018697,rank=2\n",
      "[SoftImpute] Iter 121: observed MAE=0.011024 validation MAE=0.018679,rank=2\n",
      "[SoftImpute] Iter 122: observed MAE=0.011010 validation MAE=0.018661,rank=2\n",
      "[SoftImpute] Iter 123: observed MAE=0.010997 validation MAE=0.018643,rank=2\n",
      "[SoftImpute] Iter 124: observed MAE=0.010983 validation MAE=0.018626,rank=2\n",
      "[SoftImpute] Iter 125: observed MAE=0.010970 validation MAE=0.018609,rank=2\n",
      "[SoftImpute] Iter 126: observed MAE=0.010957 validation MAE=0.018593,rank=2\n",
      "[SoftImpute] Iter 127: observed MAE=0.010944 validation MAE=0.018576,rank=2\n",
      "[SoftImpute] Iter 128: observed MAE=0.010931 validation MAE=0.018560,rank=2\n",
      "[SoftImpute] Iter 129: observed MAE=0.010918 validation MAE=0.018544,rank=2\n",
      "[SoftImpute] Iter 130: observed MAE=0.010905 validation MAE=0.018528,rank=2\n",
      "[SoftImpute] Iter 131: observed MAE=0.010893 validation MAE=0.018512,rank=2\n",
      "[SoftImpute] Iter 132: observed MAE=0.010881 validation MAE=0.018497,rank=2\n",
      "[SoftImpute] Iter 133: observed MAE=0.010869 validation MAE=0.018482,rank=2\n",
      "[SoftImpute] Iter 134: observed MAE=0.010857 validation MAE=0.018466,rank=2\n",
      "[SoftImpute] Iter 135: observed MAE=0.010845 validation MAE=0.018451,rank=2\n",
      "[SoftImpute] Iter 136: observed MAE=0.010833 validation MAE=0.018437,rank=2\n",
      "[SoftImpute] Iter 137: observed MAE=0.010822 validation MAE=0.018423,rank=2\n",
      "[SoftImpute] Iter 138: observed MAE=0.010811 validation MAE=0.018409,rank=2\n",
      "[SoftImpute] Iter 139: observed MAE=0.010799 validation MAE=0.018396,rank=2\n",
      "[SoftImpute] Iter 140: observed MAE=0.010788 validation MAE=0.018382,rank=2\n",
      "[SoftImpute] Iter 141: observed MAE=0.010777 validation MAE=0.018369,rank=2\n",
      "[SoftImpute] Iter 142: observed MAE=0.010766 validation MAE=0.018355,rank=2\n",
      "[SoftImpute] Iter 143: observed MAE=0.010756 validation MAE=0.018342,rank=2\n",
      "[SoftImpute] Iter 144: observed MAE=0.010745 validation MAE=0.018329,rank=2\n",
      "[SoftImpute] Iter 145: observed MAE=0.010735 validation MAE=0.018316,rank=2\n",
      "[SoftImpute] Iter 146: observed MAE=0.010724 validation MAE=0.018303,rank=2\n",
      "[SoftImpute] Iter 147: observed MAE=0.010714 validation MAE=0.018291,rank=2\n",
      "[SoftImpute] Iter 148: observed MAE=0.010704 validation MAE=0.018278,rank=2\n",
      "[SoftImpute] Iter 149: observed MAE=0.010694 validation MAE=0.018266,rank=2\n",
      "[SoftImpute] Iter 150: observed MAE=0.010684 validation MAE=0.018254,rank=2\n",
      "[SoftImpute] Iter 151: observed MAE=0.010674 validation MAE=0.018242,rank=2\n",
      "[SoftImpute] Iter 152: observed MAE=0.010664 validation MAE=0.018230,rank=2\n",
      "[SoftImpute] Iter 153: observed MAE=0.010655 validation MAE=0.018218,rank=2\n",
      "[SoftImpute] Iter 154: observed MAE=0.010645 validation MAE=0.018207,rank=2\n",
      "[SoftImpute] Iter 155: observed MAE=0.010636 validation MAE=0.018196,rank=2\n",
      "[SoftImpute] Iter 156: observed MAE=0.010627 validation MAE=0.018185,rank=2\n",
      "[SoftImpute] Iter 157: observed MAE=0.010618 validation MAE=0.018174,rank=2\n",
      "[SoftImpute] Iter 158: observed MAE=0.010609 validation MAE=0.018163,rank=2\n",
      "[SoftImpute] Iter 159: observed MAE=0.010600 validation MAE=0.018152,rank=2\n",
      "[SoftImpute] Iter 160: observed MAE=0.010591 validation MAE=0.018141,rank=2\n",
      "[SoftImpute] Iter 161: observed MAE=0.010582 validation MAE=0.018131,rank=2\n",
      "[SoftImpute] Iter 162: observed MAE=0.010574 validation MAE=0.018121,rank=2\n",
      "[SoftImpute] Iter 163: observed MAE=0.010565 validation MAE=0.018111,rank=2\n",
      "[SoftImpute] Iter 164: observed MAE=0.010557 validation MAE=0.018101,rank=2\n",
      "[SoftImpute] Iter 165: observed MAE=0.010549 validation MAE=0.018092,rank=2\n",
      "[SoftImpute] Iter 166: observed MAE=0.010540 validation MAE=0.018082,rank=2\n",
      "[SoftImpute] Iter 167: observed MAE=0.010532 validation MAE=0.018073,rank=2\n",
      "[SoftImpute] Iter 168: observed MAE=0.010524 validation MAE=0.018064,rank=2\n",
      "[SoftImpute] Iter 169: observed MAE=0.010516 validation MAE=0.018055,rank=2\n",
      "[SoftImpute] Iter 170: observed MAE=0.010509 validation MAE=0.018046,rank=2\n",
      "[SoftImpute] Iter 171: observed MAE=0.010501 validation MAE=0.018037,rank=2\n",
      "[SoftImpute] Iter 172: observed MAE=0.010493 validation MAE=0.018028,rank=2\n",
      "[SoftImpute] Iter 173: observed MAE=0.010486 validation MAE=0.018019,rank=2\n",
      "[SoftImpute] Iter 174: observed MAE=0.010478 validation MAE=0.018011,rank=2\n",
      "[SoftImpute] Iter 175: observed MAE=0.010471 validation MAE=0.018002,rank=2\n",
      "[SoftImpute] Iter 176: observed MAE=0.010463 validation MAE=0.017993,rank=2\n",
      "[SoftImpute] Iter 177: observed MAE=0.010456 validation MAE=0.017985,rank=2\n",
      "[SoftImpute] Iter 178: observed MAE=0.010449 validation MAE=0.017976,rank=2\n",
      "[SoftImpute] Iter 179: observed MAE=0.010442 validation MAE=0.017968,rank=2\n",
      "[SoftImpute] Iter 180: observed MAE=0.010435 validation MAE=0.017960,rank=2\n",
      "[SoftImpute] Iter 181: observed MAE=0.010428 validation MAE=0.017951,rank=2\n",
      "[SoftImpute] Iter 182: observed MAE=0.010421 validation MAE=0.017943,rank=2\n",
      "[SoftImpute] Iter 183: observed MAE=0.010415 validation MAE=0.017935,rank=2\n",
      "[SoftImpute] Iter 184: observed MAE=0.010408 validation MAE=0.017927,rank=2\n",
      "[SoftImpute] Iter 185: observed MAE=0.010401 validation MAE=0.017919,rank=2\n",
      "[SoftImpute] Iter 186: observed MAE=0.010395 validation MAE=0.017912,rank=2\n",
      "[SoftImpute] Iter 187: observed MAE=0.010388 validation MAE=0.017904,rank=2\n",
      "[SoftImpute] Iter 188: observed MAE=0.010382 validation MAE=0.017897,rank=2\n",
      "[SoftImpute] Iter 189: observed MAE=0.010375 validation MAE=0.017889,rank=2\n",
      "[SoftImpute] Iter 190: observed MAE=0.010369 validation MAE=0.017882,rank=2\n",
      "[SoftImpute] Iter 191: observed MAE=0.010363 validation MAE=0.017875,rank=2\n",
      "[SoftImpute] Iter 192: observed MAE=0.010356 validation MAE=0.017867,rank=2\n",
      "[SoftImpute] Iter 193: observed MAE=0.010350 validation MAE=0.017860,rank=2\n",
      "[SoftImpute] Iter 194: observed MAE=0.010344 validation MAE=0.017853,rank=2\n",
      "[SoftImpute] Iter 195: observed MAE=0.010338 validation MAE=0.017846,rank=2\n",
      "[SoftImpute] Iter 196: observed MAE=0.010332 validation MAE=0.017839,rank=2\n",
      "[SoftImpute] Iter 197: observed MAE=0.010326 validation MAE=0.017832,rank=2\n",
      "[SoftImpute] Iter 198: observed MAE=0.010320 validation MAE=0.017825,rank=2\n",
      "[SoftImpute] Iter 199: observed MAE=0.010314 validation MAE=0.017818,rank=2\n",
      "[SoftImpute] Iter 200: observed MAE=0.010308 validation MAE=0.017811,rank=2\n",
      "[SoftImpute] Iter 201: observed MAE=0.010302 validation MAE=0.017804,rank=2\n",
      "[SoftImpute] Iter 202: observed MAE=0.010297 validation MAE=0.017798,rank=2\n",
      "[SoftImpute] Iter 203: observed MAE=0.010291 validation MAE=0.017791,rank=2\n",
      "[SoftImpute] Iter 204: observed MAE=0.010285 validation MAE=0.017785,rank=2\n",
      "[SoftImpute] Iter 205: observed MAE=0.010280 validation MAE=0.017779,rank=2\n",
      "[SoftImpute] Iter 206: observed MAE=0.010274 validation MAE=0.017772,rank=2\n",
      "[SoftImpute] Iter 207: observed MAE=0.010269 validation MAE=0.017766,rank=2\n",
      "[SoftImpute] Iter 208: observed MAE=0.010263 validation MAE=0.017760,rank=2\n",
      "[SoftImpute] Iter 209: observed MAE=0.010258 validation MAE=0.017754,rank=2\n",
      "[SoftImpute] Iter 210: observed MAE=0.010253 validation MAE=0.017748,rank=2\n",
      "[SoftImpute] Iter 211: observed MAE=0.010247 validation MAE=0.017741,rank=2\n",
      "[SoftImpute] Iter 212: observed MAE=0.010242 validation MAE=0.017735,rank=2\n",
      "[SoftImpute] Iter 213: observed MAE=0.010237 validation MAE=0.017729,rank=2\n",
      "[SoftImpute] Iter 214: observed MAE=0.010231 validation MAE=0.017723,rank=2\n",
      "[SoftImpute] Iter 215: observed MAE=0.010226 validation MAE=0.017717,rank=2\n",
      "[SoftImpute] Iter 216: observed MAE=0.010221 validation MAE=0.017711,rank=2\n",
      "[SoftImpute] Iter 217: observed MAE=0.010216 validation MAE=0.017705,rank=2\n",
      "[SoftImpute] Iter 218: observed MAE=0.010211 validation MAE=0.017699,rank=2\n",
      "[SoftImpute] Iter 219: observed MAE=0.010206 validation MAE=0.017693,rank=2\n",
      "[SoftImpute] Iter 220: observed MAE=0.010201 validation MAE=0.017688,rank=2\n",
      "[SoftImpute] Iter 221: observed MAE=0.010196 validation MAE=0.017682,rank=2\n",
      "[SoftImpute] Iter 222: observed MAE=0.010191 validation MAE=0.017676,rank=2\n",
      "[SoftImpute] Iter 223: observed MAE=0.010186 validation MAE=0.017670,rank=2\n",
      "[SoftImpute] Iter 224: observed MAE=0.010181 validation MAE=0.017664,rank=2\n",
      "[SoftImpute] Iter 225: observed MAE=0.010177 validation MAE=0.017659,rank=2\n",
      "[SoftImpute] Iter 226: observed MAE=0.010172 validation MAE=0.017653,rank=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 227: observed MAE=0.010167 validation MAE=0.017647,rank=2\n",
      "[SoftImpute] Iter 228: observed MAE=0.010162 validation MAE=0.017641,rank=2\n",
      "[SoftImpute] Iter 229: observed MAE=0.010157 validation MAE=0.017636,rank=2\n",
      "[SoftImpute] Iter 230: observed MAE=0.010152 validation MAE=0.017630,rank=2\n",
      "[SoftImpute] Iter 231: observed MAE=0.010148 validation MAE=0.017624,rank=2\n",
      "[SoftImpute] Iter 232: observed MAE=0.010143 validation MAE=0.017619,rank=2\n",
      "[SoftImpute] Iter 233: observed MAE=0.010138 validation MAE=0.017613,rank=2\n",
      "[SoftImpute] Iter 234: observed MAE=0.010133 validation MAE=0.017608,rank=2\n",
      "[SoftImpute] Iter 235: observed MAE=0.010129 validation MAE=0.017602,rank=2\n",
      "[SoftImpute] Iter 236: observed MAE=0.010124 validation MAE=0.017597,rank=2\n",
      "[SoftImpute] Iter 237: observed MAE=0.010119 validation MAE=0.017592,rank=2\n",
      "[SoftImpute] Iter 238: observed MAE=0.010115 validation MAE=0.017586,rank=2\n",
      "[SoftImpute] Iter 239: observed MAE=0.010110 validation MAE=0.017581,rank=2\n",
      "[SoftImpute] Iter 240: observed MAE=0.010105 validation MAE=0.017576,rank=2\n",
      "[SoftImpute] Iter 241: observed MAE=0.010101 validation MAE=0.017571,rank=2\n",
      "[SoftImpute] Iter 242: observed MAE=0.010096 validation MAE=0.017566,rank=2\n",
      "[SoftImpute] Iter 243: observed MAE=0.010092 validation MAE=0.017561,rank=2\n",
      "[SoftImpute] Iter 244: observed MAE=0.010087 validation MAE=0.017556,rank=2\n",
      "[SoftImpute] Iter 245: observed MAE=0.010082 validation MAE=0.017551,rank=2\n",
      "[SoftImpute] Iter 246: observed MAE=0.010078 validation MAE=0.017546,rank=2\n",
      "[SoftImpute] Iter 247: observed MAE=0.010073 validation MAE=0.017541,rank=2\n",
      "[SoftImpute] Iter 248: observed MAE=0.010069 validation MAE=0.017536,rank=2\n",
      "[SoftImpute] Iter 249: observed MAE=0.010064 validation MAE=0.017531,rank=2\n",
      "[SoftImpute] Iter 250: observed MAE=0.010060 validation MAE=0.017526,rank=2\n",
      "[SoftImpute] Iter 251: observed MAE=0.010055 validation MAE=0.017521,rank=2\n",
      "[SoftImpute] Iter 252: observed MAE=0.010051 validation MAE=0.017516,rank=2\n",
      "[SoftImpute] Iter 253: observed MAE=0.010046 validation MAE=0.017511,rank=2\n",
      "[SoftImpute] Iter 254: observed MAE=0.010042 validation MAE=0.017507,rank=2\n",
      "[SoftImpute] Iter 255: observed MAE=0.010037 validation MAE=0.017502,rank=2\n",
      "[SoftImpute] Iter 256: observed MAE=0.010033 validation MAE=0.017497,rank=2\n",
      "[SoftImpute] Iter 257: observed MAE=0.010028 validation MAE=0.017492,rank=2\n",
      "[SoftImpute] Iter 258: observed MAE=0.010024 validation MAE=0.017487,rank=2\n",
      "[SoftImpute] Iter 259: observed MAE=0.010020 validation MAE=0.017483,rank=2\n",
      "[SoftImpute] Iter 260: observed MAE=0.010015 validation MAE=0.017478,rank=2\n",
      "[SoftImpute] Iter 261: observed MAE=0.010011 validation MAE=0.017473,rank=2\n",
      "[SoftImpute] Iter 262: observed MAE=0.010007 validation MAE=0.017468,rank=2\n",
      "[SoftImpute] Iter 263: observed MAE=0.010003 validation MAE=0.017463,rank=2\n",
      "[SoftImpute] Iter 264: observed MAE=0.009998 validation MAE=0.017458,rank=2\n",
      "[SoftImpute] Iter 265: observed MAE=0.009994 validation MAE=0.017454,rank=2\n",
      "[SoftImpute] Iter 266: observed MAE=0.009990 validation MAE=0.017449,rank=2\n",
      "[SoftImpute] Iter 267: observed MAE=0.009986 validation MAE=0.017444,rank=2\n",
      "[SoftImpute] Iter 268: observed MAE=0.009982 validation MAE=0.017439,rank=2\n",
      "[SoftImpute] Iter 269: observed MAE=0.009978 validation MAE=0.017435,rank=2\n",
      "[SoftImpute] Iter 270: observed MAE=0.009974 validation MAE=0.017430,rank=2\n",
      "[SoftImpute] Iter 271: observed MAE=0.009970 validation MAE=0.017426,rank=2\n",
      "[SoftImpute] Iter 272: observed MAE=0.009966 validation MAE=0.017422,rank=2\n",
      "[SoftImpute] Iter 273: observed MAE=0.009962 validation MAE=0.017417,rank=2\n",
      "[SoftImpute] Iter 274: observed MAE=0.009958 validation MAE=0.017413,rank=2\n",
      "[SoftImpute] Iter 275: observed MAE=0.009954 validation MAE=0.017409,rank=2\n",
      "[SoftImpute] Iter 276: observed MAE=0.009950 validation MAE=0.017405,rank=2\n",
      "[SoftImpute] Iter 277: observed MAE=0.009947 validation MAE=0.017401,rank=2\n",
      "[SoftImpute] Iter 278: observed MAE=0.009943 validation MAE=0.017398,rank=2\n",
      "[SoftImpute] Iter 279: observed MAE=0.009939 validation MAE=0.017394,rank=2\n",
      "[SoftImpute] Iter 280: observed MAE=0.009935 validation MAE=0.017390,rank=2\n",
      "[SoftImpute] Iter 281: observed MAE=0.009932 validation MAE=0.017386,rank=2\n",
      "[SoftImpute] Iter 282: observed MAE=0.009928 validation MAE=0.017382,rank=2\n",
      "[SoftImpute] Iter 283: observed MAE=0.009924 validation MAE=0.017379,rank=2\n",
      "[SoftImpute] Iter 284: observed MAE=0.009921 validation MAE=0.017375,rank=2\n",
      "[SoftImpute] Iter 285: observed MAE=0.009917 validation MAE=0.017371,rank=2\n",
      "[SoftImpute] Iter 286: observed MAE=0.009913 validation MAE=0.017367,rank=2\n",
      "[SoftImpute] Iter 287: observed MAE=0.009910 validation MAE=0.017363,rank=2\n",
      "[SoftImpute] Iter 288: observed MAE=0.009906 validation MAE=0.017360,rank=2\n",
      "[SoftImpute] Iter 289: observed MAE=0.009903 validation MAE=0.017356,rank=2\n",
      "[SoftImpute] Iter 290: observed MAE=0.009899 validation MAE=0.017352,rank=2\n",
      "[SoftImpute] Iter 291: observed MAE=0.009896 validation MAE=0.017348,rank=2\n",
      "[SoftImpute] Iter 292: observed MAE=0.009892 validation MAE=0.017344,rank=2\n",
      "[SoftImpute] Iter 293: observed MAE=0.009889 validation MAE=0.017340,rank=2\n",
      "[SoftImpute] Iter 294: observed MAE=0.009885 validation MAE=0.017336,rank=2\n",
      "[SoftImpute] Iter 295: observed MAE=0.009882 validation MAE=0.017332,rank=2\n",
      "[SoftImpute] Iter 296: observed MAE=0.009879 validation MAE=0.017329,rank=2\n",
      "[SoftImpute] Iter 297: observed MAE=0.009875 validation MAE=0.017325,rank=2\n",
      "[SoftImpute] Iter 298: observed MAE=0.009872 validation MAE=0.017321,rank=2\n",
      "[SoftImpute] Iter 299: observed MAE=0.009868 validation MAE=0.017317,rank=2\n",
      "[SoftImpute] Iter 300: observed MAE=0.009865 validation MAE=0.017313,rank=2\n",
      "[SoftImpute] Iter 301: observed MAE=0.009861 validation MAE=0.017309,rank=2\n",
      "[SoftImpute] Iter 302: observed MAE=0.009858 validation MAE=0.017305,rank=2\n",
      "[SoftImpute] Iter 303: observed MAE=0.009855 validation MAE=0.017302,rank=2\n",
      "[SoftImpute] Iter 304: observed MAE=0.009851 validation MAE=0.017298,rank=2\n",
      "[SoftImpute] Iter 305: observed MAE=0.009848 validation MAE=0.017294,rank=2\n",
      "[SoftImpute] Iter 306: observed MAE=0.009845 validation MAE=0.017291,rank=2\n",
      "[SoftImpute] Iter 307: observed MAE=0.009841 validation MAE=0.017287,rank=2\n",
      "[SoftImpute] Iter 308: observed MAE=0.009838 validation MAE=0.017283,rank=2\n",
      "[SoftImpute] Iter 309: observed MAE=0.009835 validation MAE=0.017280,rank=2\n",
      "[SoftImpute] Iter 310: observed MAE=0.009831 validation MAE=0.017276,rank=2\n",
      "[SoftImpute] Iter 311: observed MAE=0.009828 validation MAE=0.017272,rank=2\n",
      "[SoftImpute] Iter 312: observed MAE=0.009825 validation MAE=0.017269,rank=2\n",
      "[SoftImpute] Iter 313: observed MAE=0.009821 validation MAE=0.017265,rank=2\n",
      "[SoftImpute] Iter 314: observed MAE=0.009818 validation MAE=0.017262,rank=2\n",
      "[SoftImpute] Iter 315: observed MAE=0.009815 validation MAE=0.017258,rank=2\n",
      "[SoftImpute] Iter 316: observed MAE=0.009812 validation MAE=0.017255,rank=2\n",
      "[SoftImpute] Iter 317: observed MAE=0.009808 validation MAE=0.017252,rank=2\n",
      "[SoftImpute] Iter 318: observed MAE=0.009805 validation MAE=0.017249,rank=2\n",
      "[SoftImpute] Iter 319: observed MAE=0.009802 validation MAE=0.017245,rank=2\n",
      "[SoftImpute] Iter 320: observed MAE=0.009799 validation MAE=0.017242,rank=2\n",
      "[SoftImpute] Iter 321: observed MAE=0.009796 validation MAE=0.017239,rank=2\n",
      "[SoftImpute] Iter 322: observed MAE=0.009793 validation MAE=0.017235,rank=2\n",
      "[SoftImpute] Iter 323: observed MAE=0.009790 validation MAE=0.017232,rank=2\n",
      "[SoftImpute] Stopped after iteration 323 for lambda=0.039366\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 8.572381258010864\n",
      "After the matrix factor stage, training error is 0.00979, validation error is 0.01723\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.34448, val loss: 0.34765\n",
      "Main effects training epoch: 2, train loss: 0.27354, val loss: 0.27853\n",
      "Main effects training epoch: 3, train loss: 0.20823, val loss: 0.21145\n",
      "Main effects training epoch: 4, train loss: 0.16135, val loss: 0.16514\n",
      "Main effects training epoch: 5, train loss: 0.14048, val loss: 0.14025\n",
      "Main effects training epoch: 6, train loss: 0.13280, val loss: 0.13280\n",
      "Main effects training epoch: 7, train loss: 0.13041, val loss: 0.12922\n",
      "Main effects training epoch: 8, train loss: 0.12977, val loss: 0.12861\n",
      "Main effects training epoch: 9, train loss: 0.12914, val loss: 0.12821\n",
      "Main effects training epoch: 10, train loss: 0.12852, val loss: 0.12844\n",
      "Main effects training epoch: 11, train loss: 0.12742, val loss: 0.12590\n",
      "Main effects training epoch: 12, train loss: 0.12612, val loss: 0.12564\n",
      "Main effects training epoch: 13, train loss: 0.12339, val loss: 0.12252\n",
      "Main effects training epoch: 14, train loss: 0.11835, val loss: 0.11883\n",
      "Main effects training epoch: 15, train loss: 0.11578, val loss: 0.11651\n",
      "Main effects training epoch: 16, train loss: 0.11289, val loss: 0.11344\n",
      "Main effects training epoch: 17, train loss: 0.11081, val loss: 0.11186\n",
      "Main effects training epoch: 18, train loss: 0.11088, val loss: 0.11312\n",
      "Main effects training epoch: 19, train loss: 0.11027, val loss: 0.11199\n",
      "Main effects training epoch: 20, train loss: 0.11090, val loss: 0.11238\n",
      "Main effects training epoch: 21, train loss: 0.10922, val loss: 0.11096\n",
      "Main effects training epoch: 22, train loss: 0.10900, val loss: 0.11106\n",
      "Main effects training epoch: 23, train loss: 0.10645, val loss: 0.10812\n",
      "Main effects training epoch: 24, train loss: 0.10707, val loss: 0.10858\n",
      "Main effects training epoch: 25, train loss: 0.10776, val loss: 0.10768\n",
      "Main effects training epoch: 26, train loss: 0.10556, val loss: 0.10715\n",
      "Main effects training epoch: 27, train loss: 0.10595, val loss: 0.10680\n",
      "Main effects training epoch: 28, train loss: 0.10509, val loss: 0.10619\n",
      "Main effects training epoch: 29, train loss: 0.10460, val loss: 0.10568\n",
      "Main effects training epoch: 30, train loss: 0.10512, val loss: 0.10585\n",
      "Main effects training epoch: 31, train loss: 0.10437, val loss: 0.10598\n",
      "Main effects training epoch: 32, train loss: 0.10431, val loss: 0.10577\n",
      "Main effects training epoch: 33, train loss: 0.10423, val loss: 0.10545\n",
      "Main effects training epoch: 34, train loss: 0.10413, val loss: 0.10561\n",
      "Main effects training epoch: 35, train loss: 0.10424, val loss: 0.10579\n",
      "Main effects training epoch: 36, train loss: 0.10444, val loss: 0.10578\n",
      "Main effects training epoch: 37, train loss: 0.10400, val loss: 0.10575\n",
      "Main effects training epoch: 38, train loss: 0.10438, val loss: 0.10591\n",
      "Main effects training epoch: 39, train loss: 0.10416, val loss: 0.10566\n",
      "Main effects training epoch: 40, train loss: 0.10393, val loss: 0.10524\n",
      "Main effects training epoch: 41, train loss: 0.10418, val loss: 0.10579\n",
      "Main effects training epoch: 42, train loss: 0.10392, val loss: 0.10565\n",
      "Main effects training epoch: 43, train loss: 0.10430, val loss: 0.10612\n",
      "Main effects training epoch: 44, train loss: 0.10409, val loss: 0.10553\n",
      "Main effects training epoch: 45, train loss: 0.10403, val loss: 0.10539\n",
      "Main effects training epoch: 46, train loss: 0.10400, val loss: 0.10547\n",
      "Main effects training epoch: 47, train loss: 0.10472, val loss: 0.10625\n",
      "Main effects training epoch: 48, train loss: 0.10410, val loss: 0.10597\n",
      "Main effects training epoch: 49, train loss: 0.10415, val loss: 0.10512\n",
      "Main effects training epoch: 50, train loss: 0.10408, val loss: 0.10560\n",
      "Main effects training epoch: 51, train loss: 0.10392, val loss: 0.10554\n",
      "Main effects training epoch: 52, train loss: 0.10395, val loss: 0.10519\n",
      "Main effects training epoch: 53, train loss: 0.10385, val loss: 0.10520\n",
      "Main effects training epoch: 54, train loss: 0.10398, val loss: 0.10563\n",
      "Main effects training epoch: 55, train loss: 0.10416, val loss: 0.10523\n",
      "Main effects training epoch: 56, train loss: 0.10404, val loss: 0.10586\n",
      "Main effects training epoch: 57, train loss: 0.10399, val loss: 0.10571\n",
      "Main effects training epoch: 58, train loss: 0.10443, val loss: 0.10631\n",
      "Main effects training epoch: 59, train loss: 0.10393, val loss: 0.10549\n",
      "Main effects training epoch: 60, train loss: 0.10386, val loss: 0.10534\n",
      "Main effects training epoch: 61, train loss: 0.10389, val loss: 0.10538\n",
      "Main effects training epoch: 62, train loss: 0.10417, val loss: 0.10552\n",
      "Main effects training epoch: 63, train loss: 0.10396, val loss: 0.10570\n",
      "Main effects training epoch: 64, train loss: 0.10412, val loss: 0.10546\n",
      "Main effects training epoch: 65, train loss: 0.10403, val loss: 0.10549\n",
      "Main effects training epoch: 66, train loss: 0.10398, val loss: 0.10514\n",
      "Main effects training epoch: 67, train loss: 0.10385, val loss: 0.10514\n",
      "Main effects training epoch: 68, train loss: 0.10396, val loss: 0.10561\n",
      "Main effects training epoch: 69, train loss: 0.10397, val loss: 0.10523\n",
      "Main effects training epoch: 70, train loss: 0.10420, val loss: 0.10542\n",
      "Main effects training epoch: 71, train loss: 0.10473, val loss: 0.10671\n",
      "Main effects training epoch: 72, train loss: 0.10418, val loss: 0.10552\n",
      "Main effects training epoch: 73, train loss: 0.10427, val loss: 0.10590\n",
      "Main effects training epoch: 74, train loss: 0.10409, val loss: 0.10540\n",
      "Main effects training epoch: 75, train loss: 0.10401, val loss: 0.10542\n",
      "Main effects training epoch: 76, train loss: 0.10384, val loss: 0.10560\n",
      "Main effects training epoch: 77, train loss: 0.10404, val loss: 0.10542\n",
      "Main effects training epoch: 78, train loss: 0.10394, val loss: 0.10550\n",
      "Main effects training epoch: 79, train loss: 0.10410, val loss: 0.10553\n",
      "Main effects training epoch: 80, train loss: 0.10425, val loss: 0.10542\n",
      "Main effects training epoch: 81, train loss: 0.10387, val loss: 0.10530\n",
      "Main effects training epoch: 82, train loss: 0.10409, val loss: 0.10607\n",
      "Main effects training epoch: 83, train loss: 0.10386, val loss: 0.10559\n",
      "Main effects training epoch: 84, train loss: 0.10405, val loss: 0.10549\n",
      "Main effects training epoch: 85, train loss: 0.10483, val loss: 0.10617\n",
      "Main effects training epoch: 86, train loss: 0.10383, val loss: 0.10551\n",
      "Main effects training epoch: 87, train loss: 0.10415, val loss: 0.10524\n",
      "Main effects training epoch: 88, train loss: 0.10389, val loss: 0.10586\n",
      "Main effects training epoch: 89, train loss: 0.10383, val loss: 0.10548\n",
      "Main effects training epoch: 90, train loss: 0.10400, val loss: 0.10512\n",
      "Main effects training epoch: 91, train loss: 0.10387, val loss: 0.10538\n",
      "Main effects training epoch: 92, train loss: 0.10420, val loss: 0.10635\n",
      "Main effects training epoch: 93, train loss: 0.10396, val loss: 0.10529\n",
      "Main effects training epoch: 94, train loss: 0.10417, val loss: 0.10594\n",
      "Main effects training epoch: 95, train loss: 0.10387, val loss: 0.10562\n",
      "Main effects training epoch: 96, train loss: 0.10378, val loss: 0.10533\n",
      "Main effects training epoch: 97, train loss: 0.10386, val loss: 0.10536\n",
      "Main effects training epoch: 98, train loss: 0.10425, val loss: 0.10630\n",
      "Main effects training epoch: 99, train loss: 0.10426, val loss: 0.10552\n",
      "Main effects training epoch: 100, train loss: 0.10419, val loss: 0.10632\n",
      "Main effects training epoch: 101, train loss: 0.10395, val loss: 0.10553\n",
      "Main effects training epoch: 102, train loss: 0.10397, val loss: 0.10584\n",
      "Main effects training epoch: 103, train loss: 0.10408, val loss: 0.10574\n",
      "Main effects training epoch: 104, train loss: 0.10395, val loss: 0.10576\n",
      "Main effects training epoch: 105, train loss: 0.10380, val loss: 0.10555\n",
      "Main effects training epoch: 106, train loss: 0.10382, val loss: 0.10590\n",
      "Main effects training epoch: 107, train loss: 0.10417, val loss: 0.10600\n",
      "Main effects training epoch: 108, train loss: 0.10410, val loss: 0.10567\n",
      "Main effects training epoch: 109, train loss: 0.10373, val loss: 0.10542\n",
      "Main effects training epoch: 110, train loss: 0.10377, val loss: 0.10525\n",
      "Main effects training epoch: 111, train loss: 0.10405, val loss: 0.10577\n",
      "Main effects training epoch: 112, train loss: 0.10409, val loss: 0.10590\n",
      "Main effects training epoch: 113, train loss: 0.10474, val loss: 0.10615\n",
      "Main effects training epoch: 114, train loss: 0.10406, val loss: 0.10594\n",
      "Main effects training epoch: 115, train loss: 0.10415, val loss: 0.10553\n",
      "Main effects training epoch: 116, train loss: 0.10437, val loss: 0.10680\n",
      "Main effects training epoch: 117, train loss: 0.10399, val loss: 0.10532\n",
      "Main effects training epoch: 118, train loss: 0.10393, val loss: 0.10583\n",
      "Main effects training epoch: 119, train loss: 0.10410, val loss: 0.10601\n",
      "Main effects training epoch: 120, train loss: 0.10375, val loss: 0.10578\n",
      "Main effects training epoch: 121, train loss: 0.10382, val loss: 0.10529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 122, train loss: 0.10391, val loss: 0.10552\n",
      "Main effects training epoch: 123, train loss: 0.10397, val loss: 0.10615\n",
      "Main effects training epoch: 124, train loss: 0.10422, val loss: 0.10572\n",
      "Main effects training epoch: 125, train loss: 0.10414, val loss: 0.10601\n",
      "Main effects training epoch: 126, train loss: 0.10402, val loss: 0.10605\n",
      "Main effects training epoch: 127, train loss: 0.10402, val loss: 0.10531\n",
      "Main effects training epoch: 128, train loss: 0.10395, val loss: 0.10615\n",
      "Main effects training epoch: 129, train loss: 0.10410, val loss: 0.10643\n",
      "Main effects training epoch: 130, train loss: 0.10401, val loss: 0.10526\n",
      "Main effects training epoch: 131, train loss: 0.10380, val loss: 0.10577\n",
      "Main effects training epoch: 132, train loss: 0.10404, val loss: 0.10554\n",
      "Main effects training epoch: 133, train loss: 0.10425, val loss: 0.10648\n",
      "Main effects training epoch: 134, train loss: 0.10454, val loss: 0.10616\n",
      "Main effects training epoch: 135, train loss: 0.10424, val loss: 0.10653\n",
      "Main effects training epoch: 136, train loss: 0.10426, val loss: 0.10565\n",
      "Main effects training epoch: 137, train loss: 0.10444, val loss: 0.10639\n",
      "Main effects training epoch: 138, train loss: 0.10433, val loss: 0.10622\n",
      "Main effects training epoch: 139, train loss: 0.10384, val loss: 0.10565\n",
      "Main effects training epoch: 140, train loss: 0.10392, val loss: 0.10539\n",
      "Main effects training epoch: 141, train loss: 0.10420, val loss: 0.10593\n",
      "Main effects training epoch: 142, train loss: 0.10404, val loss: 0.10617\n",
      "Main effects training epoch: 143, train loss: 0.10420, val loss: 0.10567\n",
      "Main effects training epoch: 144, train loss: 0.10383, val loss: 0.10601\n",
      "Main effects training epoch: 145, train loss: 0.10370, val loss: 0.10530\n",
      "Main effects training epoch: 146, train loss: 0.10381, val loss: 0.10596\n",
      "Main effects training epoch: 147, train loss: 0.10380, val loss: 0.10575\n",
      "Main effects training epoch: 148, train loss: 0.10377, val loss: 0.10571\n",
      "Main effects training epoch: 149, train loss: 0.10383, val loss: 0.10566\n",
      "Main effects training epoch: 150, train loss: 0.10393, val loss: 0.10604\n",
      "Early stop at epoch 150, with validation loss: 0.10604\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.10405, val loss: 0.10498\n",
      "Main effects tuning epoch: 2, train loss: 0.10423, val loss: 0.10552\n",
      "Main effects tuning epoch: 3, train loss: 0.10411, val loss: 0.10505\n",
      "Main effects tuning epoch: 4, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 5, train loss: 0.10382, val loss: 0.10498\n",
      "Main effects tuning epoch: 6, train loss: 0.10425, val loss: 0.10582\n",
      "Main effects tuning epoch: 7, train loss: 0.10408, val loss: 0.10530\n",
      "Main effects tuning epoch: 8, train loss: 0.10397, val loss: 0.10527\n",
      "Main effects tuning epoch: 9, train loss: 0.10401, val loss: 0.10507\n",
      "Main effects tuning epoch: 10, train loss: 0.10407, val loss: 0.10539\n",
      "Main effects tuning epoch: 11, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 12, train loss: 0.10393, val loss: 0.10556\n",
      "Main effects tuning epoch: 13, train loss: 0.10415, val loss: 0.10510\n",
      "Main effects tuning epoch: 14, train loss: 0.10397, val loss: 0.10570\n",
      "Main effects tuning epoch: 15, train loss: 0.10386, val loss: 0.10516\n",
      "Main effects tuning epoch: 16, train loss: 0.10395, val loss: 0.10562\n",
      "Main effects tuning epoch: 17, train loss: 0.10395, val loss: 0.10539\n",
      "Main effects tuning epoch: 18, train loss: 0.10422, val loss: 0.10568\n",
      "Main effects tuning epoch: 19, train loss: 0.10433, val loss: 0.10517\n",
      "Main effects tuning epoch: 20, train loss: 0.10384, val loss: 0.10568\n",
      "Main effects tuning epoch: 21, train loss: 0.10400, val loss: 0.10541\n",
      "Main effects tuning epoch: 22, train loss: 0.10393, val loss: 0.10526\n",
      "Main effects tuning epoch: 23, train loss: 0.10434, val loss: 0.10582\n",
      "Main effects tuning epoch: 24, train loss: 0.10406, val loss: 0.10540\n",
      "Main effects tuning epoch: 25, train loss: 0.10410, val loss: 0.10570\n",
      "Main effects tuning epoch: 26, train loss: 0.10405, val loss: 0.10569\n",
      "Main effects tuning epoch: 27, train loss: 0.10406, val loss: 0.10556\n",
      "Main effects tuning epoch: 28, train loss: 0.10394, val loss: 0.10542\n",
      "Main effects tuning epoch: 29, train loss: 0.10383, val loss: 0.10542\n",
      "Main effects tuning epoch: 30, train loss: 0.10385, val loss: 0.10548\n",
      "Main effects tuning epoch: 31, train loss: 0.10395, val loss: 0.10516\n",
      "Main effects tuning epoch: 32, train loss: 0.10391, val loss: 0.10584\n",
      "Main effects tuning epoch: 33, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 34, train loss: 0.10391, val loss: 0.10561\n",
      "Main effects tuning epoch: 35, train loss: 0.10424, val loss: 0.10531\n",
      "Main effects tuning epoch: 36, train loss: 0.10395, val loss: 0.10554\n",
      "Main effects tuning epoch: 37, train loss: 0.10400, val loss: 0.10600\n",
      "Main effects tuning epoch: 38, train loss: 0.10408, val loss: 0.10530\n",
      "Main effects tuning epoch: 39, train loss: 0.10394, val loss: 0.10565\n",
      "Main effects tuning epoch: 40, train loss: 0.10385, val loss: 0.10569\n",
      "Main effects tuning epoch: 41, train loss: 0.10390, val loss: 0.10539\n",
      "Main effects tuning epoch: 42, train loss: 0.10400, val loss: 0.10587\n",
      "Main effects tuning epoch: 43, train loss: 0.10386, val loss: 0.10547\n",
      "Main effects tuning epoch: 44, train loss: 0.10403, val loss: 0.10597\n",
      "Main effects tuning epoch: 45, train loss: 0.10384, val loss: 0.10541\n",
      "Main effects tuning epoch: 46, train loss: 0.10398, val loss: 0.10582\n",
      "Main effects tuning epoch: 47, train loss: 0.10375, val loss: 0.10527\n",
      "Main effects tuning epoch: 48, train loss: 0.10380, val loss: 0.10549\n",
      "Main effects tuning epoch: 49, train loss: 0.10396, val loss: 0.10566\n",
      "Main effects tuning epoch: 50, train loss: 0.10407, val loss: 0.10549\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.09376, val loss: 0.09232\n",
      "Interaction training epoch: 2, train loss: 0.20140, val loss: 0.20111\n",
      "Interaction training epoch: 3, train loss: 0.06617, val loss: 0.06860\n",
      "Interaction training epoch: 4, train loss: 0.05173, val loss: 0.05197\n",
      "Interaction training epoch: 5, train loss: 0.06497, val loss: 0.06595\n",
      "Interaction training epoch: 6, train loss: 0.04386, val loss: 0.04449\n",
      "Interaction training epoch: 7, train loss: 0.04606, val loss: 0.04529\n",
      "Interaction training epoch: 8, train loss: 0.04492, val loss: 0.04489\n",
      "Interaction training epoch: 9, train loss: 0.04997, val loss: 0.04868\n",
      "Interaction training epoch: 10, train loss: 0.04002, val loss: 0.04133\n",
      "Interaction training epoch: 11, train loss: 0.04269, val loss: 0.04306\n",
      "Interaction training epoch: 12, train loss: 0.04100, val loss: 0.03997\n",
      "Interaction training epoch: 13, train loss: 0.04366, val loss: 0.04360\n",
      "Interaction training epoch: 14, train loss: 0.04928, val loss: 0.05084\n",
      "Interaction training epoch: 15, train loss: 0.04532, val loss: 0.04488\n",
      "Interaction training epoch: 16, train loss: 0.04530, val loss: 0.04524\n",
      "Interaction training epoch: 17, train loss: 0.03784, val loss: 0.03763\n",
      "Interaction training epoch: 18, train loss: 0.04939, val loss: 0.05023\n",
      "Interaction training epoch: 19, train loss: 0.03608, val loss: 0.03597\n",
      "Interaction training epoch: 20, train loss: 0.04344, val loss: 0.04367\n",
      "Interaction training epoch: 21, train loss: 0.04002, val loss: 0.03932\n",
      "Interaction training epoch: 22, train loss: 0.03977, val loss: 0.03977\n",
      "Interaction training epoch: 23, train loss: 0.03777, val loss: 0.03709\n",
      "Interaction training epoch: 24, train loss: 0.03927, val loss: 0.03924\n",
      "Interaction training epoch: 25, train loss: 0.04164, val loss: 0.04112\n",
      "Interaction training epoch: 26, train loss: 0.04429, val loss: 0.04389\n",
      "Interaction training epoch: 27, train loss: 0.04417, val loss: 0.04416\n",
      "Interaction training epoch: 28, train loss: 0.04241, val loss: 0.04112\n",
      "Interaction training epoch: 29, train loss: 0.03548, val loss: 0.03672\n",
      "Interaction training epoch: 30, train loss: 0.04420, val loss: 0.04353\n",
      "Interaction training epoch: 31, train loss: 0.04020, val loss: 0.04004\n",
      "Interaction training epoch: 32, train loss: 0.03422, val loss: 0.03451\n",
      "Interaction training epoch: 33, train loss: 0.03378, val loss: 0.03407\n",
      "Interaction training epoch: 34, train loss: 0.03925, val loss: 0.03880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 35, train loss: 0.03752, val loss: 0.03636\n",
      "Interaction training epoch: 36, train loss: 0.03582, val loss: 0.03564\n",
      "Interaction training epoch: 37, train loss: 0.03069, val loss: 0.03087\n",
      "Interaction training epoch: 38, train loss: 0.03729, val loss: 0.03652\n",
      "Interaction training epoch: 39, train loss: 0.03772, val loss: 0.03772\n",
      "Interaction training epoch: 40, train loss: 0.04332, val loss: 0.04201\n",
      "Interaction training epoch: 41, train loss: 0.03409, val loss: 0.03435\n",
      "Interaction training epoch: 42, train loss: 0.03501, val loss: 0.03419\n",
      "Interaction training epoch: 43, train loss: 0.03630, val loss: 0.03573\n",
      "Interaction training epoch: 44, train loss: 0.03447, val loss: 0.03420\n",
      "Interaction training epoch: 45, train loss: 0.03622, val loss: 0.03541\n",
      "Interaction training epoch: 46, train loss: 0.02991, val loss: 0.03002\n",
      "Interaction training epoch: 47, train loss: 0.02880, val loss: 0.02856\n",
      "Interaction training epoch: 48, train loss: 0.03548, val loss: 0.03469\n",
      "Interaction training epoch: 49, train loss: 0.03399, val loss: 0.03256\n",
      "Interaction training epoch: 50, train loss: 0.03225, val loss: 0.03266\n",
      "Interaction training epoch: 51, train loss: 0.03286, val loss: 0.03176\n",
      "Interaction training epoch: 52, train loss: 0.03084, val loss: 0.03012\n",
      "Interaction training epoch: 53, train loss: 0.03059, val loss: 0.02998\n",
      "Interaction training epoch: 54, train loss: 0.02966, val loss: 0.02932\n",
      "Interaction training epoch: 55, train loss: 0.04122, val loss: 0.04119\n",
      "Interaction training epoch: 56, train loss: 0.03164, val loss: 0.03059\n",
      "Interaction training epoch: 57, train loss: 0.03483, val loss: 0.03469\n",
      "Interaction training epoch: 58, train loss: 0.03320, val loss: 0.03294\n",
      "Interaction training epoch: 59, train loss: 0.02999, val loss: 0.02930\n",
      "Interaction training epoch: 60, train loss: 0.03540, val loss: 0.03496\n",
      "Interaction training epoch: 61, train loss: 0.03142, val loss: 0.02962\n",
      "Interaction training epoch: 62, train loss: 0.03146, val loss: 0.03164\n",
      "Interaction training epoch: 63, train loss: 0.03481, val loss: 0.03421\n",
      "Interaction training epoch: 64, train loss: 0.02946, val loss: 0.02881\n",
      "Interaction training epoch: 65, train loss: 0.03539, val loss: 0.03511\n",
      "Interaction training epoch: 66, train loss: 0.03797, val loss: 0.03805\n",
      "Interaction training epoch: 67, train loss: 0.03669, val loss: 0.03627\n",
      "Interaction training epoch: 68, train loss: 0.04844, val loss: 0.04881\n",
      "Interaction training epoch: 69, train loss: 0.02947, val loss: 0.02941\n",
      "Interaction training epoch: 70, train loss: 0.03805, val loss: 0.03672\n",
      "Interaction training epoch: 71, train loss: 0.02990, val loss: 0.02970\n",
      "Interaction training epoch: 72, train loss: 0.03926, val loss: 0.03952\n",
      "Interaction training epoch: 73, train loss: 0.03713, val loss: 0.03732\n",
      "Interaction training epoch: 74, train loss: 0.04469, val loss: 0.04392\n",
      "Interaction training epoch: 75, train loss: 0.03518, val loss: 0.03491\n",
      "Interaction training epoch: 76, train loss: 0.03954, val loss: 0.03893\n",
      "Interaction training epoch: 77, train loss: 0.04294, val loss: 0.04248\n",
      "Interaction training epoch: 78, train loss: 0.03122, val loss: 0.03170\n",
      "Interaction training epoch: 79, train loss: 0.03577, val loss: 0.03538\n",
      "Interaction training epoch: 80, train loss: 0.04727, val loss: 0.04664\n",
      "Interaction training epoch: 81, train loss: 0.03478, val loss: 0.03412\n",
      "Interaction training epoch: 82, train loss: 0.03736, val loss: 0.03700\n",
      "Interaction training epoch: 83, train loss: 0.03716, val loss: 0.03665\n",
      "Interaction training epoch: 84, train loss: 0.03675, val loss: 0.03575\n",
      "Interaction training epoch: 85, train loss: 0.03815, val loss: 0.03817\n",
      "Interaction training epoch: 86, train loss: 0.03960, val loss: 0.03903\n",
      "Interaction training epoch: 87, train loss: 0.03498, val loss: 0.03402\n",
      "Interaction training epoch: 88, train loss: 0.03630, val loss: 0.03583\n",
      "Interaction training epoch: 89, train loss: 0.03454, val loss: 0.03517\n",
      "Interaction training epoch: 90, train loss: 0.07231, val loss: 0.07223\n",
      "Interaction training epoch: 91, train loss: 0.04800, val loss: 0.04802\n",
      "Interaction training epoch: 92, train loss: 0.05224, val loss: 0.05162\n",
      "Interaction training epoch: 93, train loss: 0.02794, val loss: 0.02804\n",
      "Interaction training epoch: 94, train loss: 0.03585, val loss: 0.03546\n",
      "Interaction training epoch: 95, train loss: 0.04149, val loss: 0.04158\n",
      "Interaction training epoch: 96, train loss: 0.04943, val loss: 0.04909\n",
      "Interaction training epoch: 97, train loss: 0.05722, val loss: 0.05690\n",
      "Interaction training epoch: 98, train loss: 0.04854, val loss: 0.04831\n",
      "Interaction training epoch: 99, train loss: 0.02719, val loss: 0.02716\n",
      "Interaction training epoch: 100, train loss: 0.03689, val loss: 0.03722\n",
      "Interaction training epoch: 101, train loss: 0.04609, val loss: 0.04601\n",
      "Interaction training epoch: 102, train loss: 0.03250, val loss: 0.03231\n",
      "Interaction training epoch: 103, train loss: 0.03006, val loss: 0.02936\n",
      "Interaction training epoch: 104, train loss: 0.03967, val loss: 0.03937\n",
      "Interaction training epoch: 105, train loss: 0.03096, val loss: 0.03100\n",
      "Interaction training epoch: 106, train loss: 0.05099, val loss: 0.05044\n",
      "Interaction training epoch: 107, train loss: 0.03358, val loss: 0.03389\n",
      "Interaction training epoch: 108, train loss: 0.04273, val loss: 0.04236\n",
      "Interaction training epoch: 109, train loss: 0.07259, val loss: 0.07140\n",
      "Interaction training epoch: 110, train loss: 0.03442, val loss: 0.03433\n",
      "Interaction training epoch: 111, train loss: 0.02947, val loss: 0.02893\n",
      "Interaction training epoch: 112, train loss: 0.03413, val loss: 0.03396\n",
      "Interaction training epoch: 113, train loss: 0.03441, val loss: 0.03428\n",
      "Interaction training epoch: 114, train loss: 0.05762, val loss: 0.05645\n",
      "Interaction training epoch: 115, train loss: 0.02938, val loss: 0.02918\n",
      "Interaction training epoch: 116, train loss: 0.04805, val loss: 0.04824\n",
      "Interaction training epoch: 117, train loss: 0.03927, val loss: 0.03835\n",
      "Interaction training epoch: 118, train loss: 0.03949, val loss: 0.03920\n",
      "Interaction training epoch: 119, train loss: 0.03835, val loss: 0.03852\n",
      "Interaction training epoch: 120, train loss: 0.07402, val loss: 0.07267\n",
      "Interaction training epoch: 121, train loss: 0.02786, val loss: 0.02794\n",
      "Interaction training epoch: 122, train loss: 0.02966, val loss: 0.02840\n",
      "Interaction training epoch: 123, train loss: 0.03146, val loss: 0.03080\n",
      "Interaction training epoch: 124, train loss: 0.03135, val loss: 0.03095\n",
      "Interaction training epoch: 125, train loss: 0.02893, val loss: 0.02832\n",
      "Interaction training epoch: 126, train loss: 0.06644, val loss: 0.06554\n",
      "Interaction training epoch: 127, train loss: 0.02439, val loss: 0.02405\n",
      "Interaction training epoch: 128, train loss: 0.02442, val loss: 0.02392\n",
      "Interaction training epoch: 129, train loss: 0.06718, val loss: 0.06662\n",
      "Interaction training epoch: 130, train loss: 0.06873, val loss: 0.06752\n",
      "Interaction training epoch: 131, train loss: 0.02836, val loss: 0.02795\n",
      "Interaction training epoch: 132, train loss: 0.02537, val loss: 0.02462\n",
      "Interaction training epoch: 133, train loss: 0.05578, val loss: 0.05497\n",
      "Interaction training epoch: 134, train loss: 0.02756, val loss: 0.02628\n",
      "Interaction training epoch: 135, train loss: 0.03851, val loss: 0.03792\n",
      "Interaction training epoch: 136, train loss: 0.02792, val loss: 0.02763\n",
      "Interaction training epoch: 137, train loss: 0.02882, val loss: 0.02822\n",
      "Interaction training epoch: 138, train loss: 0.04268, val loss: 0.04230\n",
      "Interaction training epoch: 139, train loss: 0.02999, val loss: 0.03028\n",
      "Interaction training epoch: 140, train loss: 0.02653, val loss: 0.02598\n",
      "Interaction training epoch: 141, train loss: 0.02864, val loss: 0.02740\n",
      "Interaction training epoch: 142, train loss: 0.04031, val loss: 0.03930\n",
      "Interaction training epoch: 143, train loss: 0.08930, val loss: 0.08825\n",
      "Interaction training epoch: 144, train loss: 0.03278, val loss: 0.03201\n",
      "Interaction training epoch: 145, train loss: 0.07595, val loss: 0.07444\n",
      "Interaction training epoch: 146, train loss: 0.04088, val loss: 0.04007\n",
      "Interaction training epoch: 147, train loss: 0.03063, val loss: 0.02967\n",
      "Interaction training epoch: 148, train loss: 0.08210, val loss: 0.08116\n",
      "Interaction training epoch: 149, train loss: 0.02600, val loss: 0.02595\n",
      "Interaction training epoch: 150, train loss: 0.04451, val loss: 0.04266\n",
      "Interaction training epoch: 151, train loss: 0.05071, val loss: 0.04953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 152, train loss: 0.03689, val loss: 0.03657\n",
      "Interaction training epoch: 153, train loss: 0.05108, val loss: 0.05017\n",
      "Interaction training epoch: 154, train loss: 0.03125, val loss: 0.03020\n",
      "Interaction training epoch: 155, train loss: 0.05262, val loss: 0.05160\n",
      "Interaction training epoch: 156, train loss: 0.03189, val loss: 0.03099\n",
      "Interaction training epoch: 157, train loss: 0.03302, val loss: 0.03192\n",
      "Interaction training epoch: 158, train loss: 0.03919, val loss: 0.03867\n",
      "Interaction training epoch: 159, train loss: 0.06294, val loss: 0.06164\n",
      "Interaction training epoch: 160, train loss: 0.02504, val loss: 0.02398\n",
      "Interaction training epoch: 161, train loss: 0.02726, val loss: 0.02622\n",
      "Interaction training epoch: 162, train loss: 0.02696, val loss: 0.02663\n",
      "Interaction training epoch: 163, train loss: 0.04443, val loss: 0.04306\n",
      "Interaction training epoch: 164, train loss: 0.08229, val loss: 0.08080\n",
      "Interaction training epoch: 165, train loss: 0.02497, val loss: 0.02433\n",
      "Interaction training epoch: 166, train loss: 0.02405, val loss: 0.02321\n",
      "Interaction training epoch: 167, train loss: 0.03154, val loss: 0.03087\n",
      "Interaction training epoch: 168, train loss: 0.03079, val loss: 0.02948\n",
      "Interaction training epoch: 169, train loss: 0.02562, val loss: 0.02466\n",
      "Interaction training epoch: 170, train loss: 0.03950, val loss: 0.03811\n",
      "Interaction training epoch: 171, train loss: 0.05102, val loss: 0.05005\n",
      "Interaction training epoch: 172, train loss: 0.07393, val loss: 0.07199\n",
      "Interaction training epoch: 173, train loss: 0.03775, val loss: 0.03659\n",
      "Interaction training epoch: 174, train loss: 0.03283, val loss: 0.03172\n",
      "Interaction training epoch: 175, train loss: 0.02729, val loss: 0.02663\n",
      "Interaction training epoch: 176, train loss: 0.04249, val loss: 0.04090\n",
      "Interaction training epoch: 177, train loss: 0.11371, val loss: 0.11248\n",
      "Interaction training epoch: 178, train loss: 0.05008, val loss: 0.04851\n",
      "Interaction training epoch: 179, train loss: 0.05070, val loss: 0.04903\n",
      "Interaction training epoch: 180, train loss: 0.05192, val loss: 0.05082\n",
      "Interaction training epoch: 181, train loss: 0.03499, val loss: 0.03401\n",
      "Interaction training epoch: 182, train loss: 0.06009, val loss: 0.05850\n",
      "Interaction training epoch: 183, train loss: 0.03517, val loss: 0.03374\n",
      "Interaction training epoch: 184, train loss: 0.04107, val loss: 0.04010\n",
      "Interaction training epoch: 185, train loss: 0.06361, val loss: 0.06162\n",
      "Interaction training epoch: 186, train loss: 0.06349, val loss: 0.06178\n",
      "Interaction training epoch: 187, train loss: 0.05374, val loss: 0.05196\n",
      "Interaction training epoch: 188, train loss: 0.05958, val loss: 0.05804\n",
      "Interaction training epoch: 189, train loss: 0.02604, val loss: 0.02519\n",
      "Interaction training epoch: 190, train loss: 0.04369, val loss: 0.04302\n",
      "Interaction training epoch: 191, train loss: 0.04175, val loss: 0.04116\n",
      "Interaction training epoch: 192, train loss: 0.06782, val loss: 0.06645\n",
      "Interaction training epoch: 193, train loss: 0.06145, val loss: 0.06034\n",
      "Interaction training epoch: 194, train loss: 0.04303, val loss: 0.04151\n",
      "Interaction training epoch: 195, train loss: 0.03319, val loss: 0.03229\n",
      "Interaction training epoch: 196, train loss: 0.03143, val loss: 0.02941\n",
      "Interaction training epoch: 197, train loss: 0.03413, val loss: 0.03311\n",
      "Interaction training epoch: 198, train loss: 0.03045, val loss: 0.02901\n",
      "Interaction training epoch: 199, train loss: 0.04253, val loss: 0.04138\n",
      "Interaction training epoch: 200, train loss: 0.04032, val loss: 0.03845\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########4 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.02853, val loss: 0.02785\n",
      "Interaction tuning epoch: 2, train loss: 0.02401, val loss: 0.02299\n",
      "Interaction tuning epoch: 3, train loss: 0.04478, val loss: 0.04265\n",
      "Interaction tuning epoch: 4, train loss: 0.06395, val loss: 0.06292\n",
      "Interaction tuning epoch: 5, train loss: 0.02356, val loss: 0.02286\n",
      "Interaction tuning epoch: 6, train loss: 0.06369, val loss: 0.06228\n",
      "Interaction tuning epoch: 7, train loss: 0.04154, val loss: 0.04077\n",
      "Interaction tuning epoch: 8, train loss: 0.08354, val loss: 0.08227\n",
      "Interaction tuning epoch: 9, train loss: 0.07421, val loss: 0.07270\n",
      "Interaction tuning epoch: 10, train loss: 0.04015, val loss: 0.03931\n",
      "Interaction tuning epoch: 11, train loss: 0.02840, val loss: 0.02746\n",
      "Interaction tuning epoch: 12, train loss: 0.06172, val loss: 0.06021\n",
      "Interaction tuning epoch: 13, train loss: 0.03776, val loss: 0.03699\n",
      "Interaction tuning epoch: 14, train loss: 0.07388, val loss: 0.07213\n",
      "Interaction tuning epoch: 15, train loss: 0.02628, val loss: 0.02557\n",
      "Interaction tuning epoch: 16, train loss: 0.02860, val loss: 0.02813\n",
      "Interaction tuning epoch: 17, train loss: 0.04973, val loss: 0.04802\n",
      "Interaction tuning epoch: 18, train loss: 0.02869, val loss: 0.02741\n",
      "Interaction tuning epoch: 19, train loss: 0.03914, val loss: 0.03730\n",
      "Interaction tuning epoch: 20, train loss: 0.03596, val loss: 0.03526\n",
      "Interaction tuning epoch: 21, train loss: 0.04924, val loss: 0.04797\n",
      "Interaction tuning epoch: 22, train loss: 0.08235, val loss: 0.08148\n",
      "Interaction tuning epoch: 23, train loss: 0.08430, val loss: 0.08186\n",
      "Interaction tuning epoch: 24, train loss: 0.07758, val loss: 0.07581\n",
      "Interaction tuning epoch: 25, train loss: 0.02468, val loss: 0.02425\n",
      "Interaction tuning epoch: 26, train loss: 0.02729, val loss: 0.02674\n",
      "Interaction tuning epoch: 27, train loss: 0.02677, val loss: 0.02542\n",
      "Interaction tuning epoch: 28, train loss: 0.03303, val loss: 0.03233\n",
      "Interaction tuning epoch: 29, train loss: 0.07628, val loss: 0.07409\n",
      "Interaction tuning epoch: 30, train loss: 0.05354, val loss: 0.05248\n",
      "Interaction tuning epoch: 31, train loss: 0.02819, val loss: 0.02642\n",
      "Interaction tuning epoch: 32, train loss: 0.02994, val loss: 0.02905\n",
      "Interaction tuning epoch: 33, train loss: 0.03903, val loss: 0.03809\n",
      "Interaction tuning epoch: 34, train loss: 0.03406, val loss: 0.03261\n",
      "Interaction tuning epoch: 35, train loss: 0.02886, val loss: 0.02797\n",
      "Interaction tuning epoch: 36, train loss: 0.04728, val loss: 0.04591\n",
      "Interaction tuning epoch: 37, train loss: 0.02745, val loss: 0.02653\n",
      "Interaction tuning epoch: 38, train loss: 0.09613, val loss: 0.09462\n",
      "Interaction tuning epoch: 39, train loss: 0.06430, val loss: 0.06326\n",
      "Interaction tuning epoch: 40, train loss: 0.03564, val loss: 0.03467\n",
      "Interaction tuning epoch: 41, train loss: 0.05842, val loss: 0.05720\n",
      "Interaction tuning epoch: 42, train loss: 0.02719, val loss: 0.02601\n",
      "Interaction tuning epoch: 43, train loss: 0.03689, val loss: 0.03533\n",
      "Interaction tuning epoch: 44, train loss: 0.02563, val loss: 0.02452\n",
      "Interaction tuning epoch: 45, train loss: 0.11515, val loss: 0.11336\n",
      "Interaction tuning epoch: 46, train loss: 0.05383, val loss: 0.05274\n",
      "Interaction tuning epoch: 47, train loss: 0.03183, val loss: 0.02986\n",
      "Interaction tuning epoch: 48, train loss: 0.04755, val loss: 0.04566\n",
      "Interaction tuning epoch: 49, train loss: 0.03504, val loss: 0.03437\n",
      "Interaction tuning epoch: 50, train loss: 0.03646, val loss: 0.03600\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 31.666395902633667\n",
      "After the gam stage, training error is 0.03646 , validation error is 0.03600\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 1.968308\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.030510 validation MAE=0.035018,rank=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 2: observed MAE=0.029006 validation MAE=0.034419,rank=3\n",
      "[SoftImpute] Iter 3: observed MAE=0.027664 validation MAE=0.033715,rank=3\n",
      "[SoftImpute] Iter 4: observed MAE=0.026401 validation MAE=0.032934,rank=3\n",
      "[SoftImpute] Iter 5: observed MAE=0.025194 validation MAE=0.032128,rank=3\n",
      "[SoftImpute] Iter 6: observed MAE=0.024044 validation MAE=0.031316,rank=3\n",
      "[SoftImpute] Iter 7: observed MAE=0.022974 validation MAE=0.030549,rank=3\n",
      "[SoftImpute] Iter 8: observed MAE=0.021975 validation MAE=0.029807,rank=3\n",
      "[SoftImpute] Iter 9: observed MAE=0.021051 validation MAE=0.029102,rank=3\n",
      "[SoftImpute] Iter 10: observed MAE=0.020199 validation MAE=0.028464,rank=3\n",
      "[SoftImpute] Iter 11: observed MAE=0.019421 validation MAE=0.027869,rank=3\n",
      "[SoftImpute] Iter 12: observed MAE=0.018712 validation MAE=0.027310,rank=3\n",
      "[SoftImpute] Iter 13: observed MAE=0.018059 validation MAE=0.026792,rank=3\n",
      "[SoftImpute] Iter 14: observed MAE=0.017459 validation MAE=0.026315,rank=3\n",
      "[SoftImpute] Iter 15: observed MAE=0.016911 validation MAE=0.025875,rank=3\n",
      "[SoftImpute] Iter 16: observed MAE=0.016410 validation MAE=0.025470,rank=3\n",
      "[SoftImpute] Iter 17: observed MAE=0.015952 validation MAE=0.025090,rank=3\n",
      "[SoftImpute] Iter 18: observed MAE=0.015530 validation MAE=0.024732,rank=3\n",
      "[SoftImpute] Iter 19: observed MAE=0.015139 validation MAE=0.024394,rank=3\n",
      "[SoftImpute] Iter 20: observed MAE=0.014777 validation MAE=0.024079,rank=3\n",
      "[SoftImpute] Iter 21: observed MAE=0.014440 validation MAE=0.023788,rank=3\n",
      "[SoftImpute] Iter 22: observed MAE=0.014128 validation MAE=0.023515,rank=3\n",
      "[SoftImpute] Iter 23: observed MAE=0.013838 validation MAE=0.023258,rank=3\n",
      "[SoftImpute] Iter 24: observed MAE=0.013566 validation MAE=0.023017,rank=3\n",
      "[SoftImpute] Iter 25: observed MAE=0.013315 validation MAE=0.022792,rank=3\n",
      "[SoftImpute] Iter 26: observed MAE=0.013081 validation MAE=0.022577,rank=3\n",
      "[SoftImpute] Iter 27: observed MAE=0.012862 validation MAE=0.022370,rank=3\n",
      "[SoftImpute] Iter 28: observed MAE=0.012657 validation MAE=0.022173,rank=3\n",
      "[SoftImpute] Iter 29: observed MAE=0.012465 validation MAE=0.021989,rank=3\n",
      "[SoftImpute] Iter 30: observed MAE=0.012286 validation MAE=0.021819,rank=3\n",
      "[SoftImpute] Iter 31: observed MAE=0.012116 validation MAE=0.021659,rank=3\n",
      "[SoftImpute] Iter 32: observed MAE=0.011956 validation MAE=0.021509,rank=3\n",
      "[SoftImpute] Iter 33: observed MAE=0.011806 validation MAE=0.021366,rank=3\n",
      "[SoftImpute] Iter 34: observed MAE=0.011663 validation MAE=0.021230,rank=3\n",
      "[SoftImpute] Iter 35: observed MAE=0.011527 validation MAE=0.021099,rank=3\n",
      "[SoftImpute] Iter 36: observed MAE=0.011399 validation MAE=0.020973,rank=3\n",
      "[SoftImpute] Iter 37: observed MAE=0.011278 validation MAE=0.020854,rank=3\n",
      "[SoftImpute] Iter 38: observed MAE=0.011163 validation MAE=0.020737,rank=3\n",
      "[SoftImpute] Iter 39: observed MAE=0.011053 validation MAE=0.020624,rank=3\n",
      "[SoftImpute] Iter 40: observed MAE=0.010948 validation MAE=0.020515,rank=3\n",
      "[SoftImpute] Iter 41: observed MAE=0.010848 validation MAE=0.020410,rank=3\n",
      "[SoftImpute] Iter 42: observed MAE=0.010752 validation MAE=0.020310,rank=3\n",
      "[SoftImpute] Iter 43: observed MAE=0.010660 validation MAE=0.020215,rank=3\n",
      "[SoftImpute] Iter 44: observed MAE=0.010572 validation MAE=0.020123,rank=3\n",
      "[SoftImpute] Iter 45: observed MAE=0.010488 validation MAE=0.020034,rank=3\n",
      "[SoftImpute] Iter 46: observed MAE=0.010407 validation MAE=0.019950,rank=3\n",
      "[SoftImpute] Iter 47: observed MAE=0.010329 validation MAE=0.019870,rank=3\n",
      "[SoftImpute] Iter 48: observed MAE=0.010254 validation MAE=0.019791,rank=3\n",
      "[SoftImpute] Iter 49: observed MAE=0.010181 validation MAE=0.019715,rank=3\n",
      "[SoftImpute] Iter 50: observed MAE=0.010111 validation MAE=0.019639,rank=3\n",
      "[SoftImpute] Iter 51: observed MAE=0.010043 validation MAE=0.019566,rank=3\n",
      "[SoftImpute] Iter 52: observed MAE=0.009977 validation MAE=0.019495,rank=3\n",
      "[SoftImpute] Iter 53: observed MAE=0.009914 validation MAE=0.019425,rank=3\n",
      "[SoftImpute] Iter 54: observed MAE=0.009852 validation MAE=0.019357,rank=3\n",
      "[SoftImpute] Iter 55: observed MAE=0.009793 validation MAE=0.019293,rank=3\n",
      "[SoftImpute] Iter 56: observed MAE=0.009735 validation MAE=0.019232,rank=3\n",
      "[SoftImpute] Iter 57: observed MAE=0.009679 validation MAE=0.019172,rank=3\n",
      "[SoftImpute] Iter 58: observed MAE=0.009625 validation MAE=0.019113,rank=3\n",
      "[SoftImpute] Iter 59: observed MAE=0.009572 validation MAE=0.019056,rank=3\n",
      "[SoftImpute] Iter 60: observed MAE=0.009521 validation MAE=0.019000,rank=3\n",
      "[SoftImpute] Iter 61: observed MAE=0.009472 validation MAE=0.018943,rank=3\n",
      "[SoftImpute] Iter 62: observed MAE=0.009423 validation MAE=0.018888,rank=3\n",
      "[SoftImpute] Iter 63: observed MAE=0.009376 validation MAE=0.018835,rank=3\n",
      "[SoftImpute] Iter 64: observed MAE=0.009330 validation MAE=0.018782,rank=3\n",
      "[SoftImpute] Iter 65: observed MAE=0.009285 validation MAE=0.018730,rank=3\n",
      "[SoftImpute] Iter 66: observed MAE=0.009241 validation MAE=0.018681,rank=3\n",
      "[SoftImpute] Iter 67: observed MAE=0.009198 validation MAE=0.018632,rank=3\n",
      "[SoftImpute] Iter 68: observed MAE=0.009155 validation MAE=0.018584,rank=3\n",
      "[SoftImpute] Iter 69: observed MAE=0.009114 validation MAE=0.018537,rank=3\n",
      "[SoftImpute] Iter 70: observed MAE=0.009074 validation MAE=0.018491,rank=3\n",
      "[SoftImpute] Iter 71: observed MAE=0.009035 validation MAE=0.018445,rank=3\n",
      "[SoftImpute] Iter 72: observed MAE=0.008997 validation MAE=0.018401,rank=3\n",
      "[SoftImpute] Iter 73: observed MAE=0.008960 validation MAE=0.018356,rank=3\n",
      "[SoftImpute] Iter 74: observed MAE=0.008923 validation MAE=0.018313,rank=3\n",
      "[SoftImpute] Iter 75: observed MAE=0.008888 validation MAE=0.018269,rank=3\n",
      "[SoftImpute] Iter 76: observed MAE=0.008853 validation MAE=0.018227,rank=3\n",
      "[SoftImpute] Iter 77: observed MAE=0.008819 validation MAE=0.018185,rank=3\n",
      "[SoftImpute] Iter 78: observed MAE=0.008786 validation MAE=0.018144,rank=3\n",
      "[SoftImpute] Iter 79: observed MAE=0.008754 validation MAE=0.018103,rank=3\n",
      "[SoftImpute] Iter 80: observed MAE=0.008723 validation MAE=0.018063,rank=3\n",
      "[SoftImpute] Iter 81: observed MAE=0.008692 validation MAE=0.018023,rank=3\n",
      "[SoftImpute] Iter 82: observed MAE=0.008662 validation MAE=0.017984,rank=3\n",
      "[SoftImpute] Iter 83: observed MAE=0.008632 validation MAE=0.017946,rank=3\n",
      "[SoftImpute] Iter 84: observed MAE=0.008603 validation MAE=0.017909,rank=3\n",
      "[SoftImpute] Iter 85: observed MAE=0.008574 validation MAE=0.017871,rank=3\n",
      "[SoftImpute] Iter 86: observed MAE=0.008547 validation MAE=0.017834,rank=3\n",
      "[SoftImpute] Iter 87: observed MAE=0.008519 validation MAE=0.017798,rank=3\n",
      "[SoftImpute] Iter 88: observed MAE=0.008493 validation MAE=0.017761,rank=3\n",
      "[SoftImpute] Iter 89: observed MAE=0.008466 validation MAE=0.017725,rank=3\n",
      "[SoftImpute] Iter 90: observed MAE=0.008441 validation MAE=0.017690,rank=3\n",
      "[SoftImpute] Iter 91: observed MAE=0.008415 validation MAE=0.017655,rank=3\n",
      "[SoftImpute] Iter 92: observed MAE=0.008391 validation MAE=0.017621,rank=3\n",
      "[SoftImpute] Iter 93: observed MAE=0.008366 validation MAE=0.017587,rank=3\n",
      "[SoftImpute] Iter 94: observed MAE=0.008343 validation MAE=0.017553,rank=3\n",
      "[SoftImpute] Iter 95: observed MAE=0.008320 validation MAE=0.017520,rank=3\n",
      "[SoftImpute] Iter 96: observed MAE=0.008297 validation MAE=0.017488,rank=3\n",
      "[SoftImpute] Iter 97: observed MAE=0.008274 validation MAE=0.017455,rank=3\n",
      "[SoftImpute] Iter 98: observed MAE=0.008253 validation MAE=0.017424,rank=3\n",
      "[SoftImpute] Iter 99: observed MAE=0.008231 validation MAE=0.017394,rank=3\n",
      "[SoftImpute] Iter 100: observed MAE=0.008210 validation MAE=0.017364,rank=3\n",
      "[SoftImpute] Iter 101: observed MAE=0.008190 validation MAE=0.017335,rank=3\n",
      "[SoftImpute] Iter 102: observed MAE=0.008170 validation MAE=0.017306,rank=3\n",
      "[SoftImpute] Iter 103: observed MAE=0.008150 validation MAE=0.017278,rank=3\n",
      "[SoftImpute] Iter 104: observed MAE=0.008131 validation MAE=0.017249,rank=3\n",
      "[SoftImpute] Iter 105: observed MAE=0.008112 validation MAE=0.017222,rank=3\n",
      "[SoftImpute] Iter 106: observed MAE=0.008093 validation MAE=0.017194,rank=3\n",
      "[SoftImpute] Iter 107: observed MAE=0.008074 validation MAE=0.017167,rank=3\n",
      "[SoftImpute] Iter 108: observed MAE=0.008056 validation MAE=0.017140,rank=3\n",
      "[SoftImpute] Iter 109: observed MAE=0.008039 validation MAE=0.017114,rank=3\n",
      "[SoftImpute] Iter 110: observed MAE=0.008021 validation MAE=0.017089,rank=3\n",
      "[SoftImpute] Iter 111: observed MAE=0.008004 validation MAE=0.017063,rank=3\n",
      "[SoftImpute] Iter 112: observed MAE=0.007987 validation MAE=0.017039,rank=3\n",
      "[SoftImpute] Iter 113: observed MAE=0.007971 validation MAE=0.017014,rank=3\n",
      "[SoftImpute] Iter 114: observed MAE=0.007954 validation MAE=0.016990,rank=3\n",
      "[SoftImpute] Iter 115: observed MAE=0.007938 validation MAE=0.016966,rank=3\n",
      "[SoftImpute] Iter 116: observed MAE=0.007923 validation MAE=0.016943,rank=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 117: observed MAE=0.007907 validation MAE=0.016919,rank=3\n",
      "[SoftImpute] Iter 118: observed MAE=0.007892 validation MAE=0.016896,rank=3\n",
      "[SoftImpute] Iter 119: observed MAE=0.007877 validation MAE=0.016873,rank=3\n",
      "[SoftImpute] Iter 120: observed MAE=0.007862 validation MAE=0.016850,rank=3\n",
      "[SoftImpute] Iter 121: observed MAE=0.007848 validation MAE=0.016828,rank=3\n",
      "[SoftImpute] Iter 122: observed MAE=0.007833 validation MAE=0.016806,rank=3\n",
      "[SoftImpute] Iter 123: observed MAE=0.007820 validation MAE=0.016784,rank=3\n",
      "[SoftImpute] Iter 124: observed MAE=0.007806 validation MAE=0.016762,rank=3\n",
      "[SoftImpute] Iter 125: observed MAE=0.007792 validation MAE=0.016741,rank=3\n",
      "[SoftImpute] Iter 126: observed MAE=0.007779 validation MAE=0.016720,rank=3\n",
      "[SoftImpute] Iter 127: observed MAE=0.007766 validation MAE=0.016700,rank=3\n",
      "[SoftImpute] Iter 128: observed MAE=0.007753 validation MAE=0.016679,rank=3\n",
      "[SoftImpute] Iter 129: observed MAE=0.007740 validation MAE=0.016659,rank=3\n",
      "[SoftImpute] Iter 130: observed MAE=0.007728 validation MAE=0.016640,rank=3\n",
      "[SoftImpute] Iter 131: observed MAE=0.007715 validation MAE=0.016620,rank=3\n",
      "[SoftImpute] Iter 132: observed MAE=0.007703 validation MAE=0.016601,rank=3\n",
      "[SoftImpute] Iter 133: observed MAE=0.007691 validation MAE=0.016581,rank=3\n",
      "[SoftImpute] Iter 134: observed MAE=0.007679 validation MAE=0.016562,rank=3\n",
      "[SoftImpute] Iter 135: observed MAE=0.007668 validation MAE=0.016543,rank=3\n",
      "[SoftImpute] Iter 136: observed MAE=0.007656 validation MAE=0.016524,rank=3\n",
      "[SoftImpute] Iter 137: observed MAE=0.007645 validation MAE=0.016506,rank=3\n",
      "[SoftImpute] Iter 138: observed MAE=0.007634 validation MAE=0.016488,rank=3\n",
      "[SoftImpute] Iter 139: observed MAE=0.007623 validation MAE=0.016470,rank=3\n",
      "[SoftImpute] Iter 140: observed MAE=0.007612 validation MAE=0.016452,rank=3\n",
      "[SoftImpute] Iter 141: observed MAE=0.007602 validation MAE=0.016435,rank=3\n",
      "[SoftImpute] Iter 142: observed MAE=0.007591 validation MAE=0.016418,rank=3\n",
      "[SoftImpute] Iter 143: observed MAE=0.007581 validation MAE=0.016400,rank=3\n",
      "[SoftImpute] Iter 144: observed MAE=0.007571 validation MAE=0.016383,rank=3\n",
      "[SoftImpute] Iter 145: observed MAE=0.007561 validation MAE=0.016366,rank=3\n",
      "[SoftImpute] Iter 146: observed MAE=0.007551 validation MAE=0.016350,rank=3\n",
      "[SoftImpute] Iter 147: observed MAE=0.007541 validation MAE=0.016333,rank=3\n",
      "[SoftImpute] Iter 148: observed MAE=0.007531 validation MAE=0.016316,rank=3\n",
      "[SoftImpute] Iter 149: observed MAE=0.007522 validation MAE=0.016300,rank=3\n",
      "[SoftImpute] Iter 150: observed MAE=0.007513 validation MAE=0.016284,rank=3\n",
      "[SoftImpute] Iter 151: observed MAE=0.007504 validation MAE=0.016268,rank=3\n",
      "[SoftImpute] Iter 152: observed MAE=0.007494 validation MAE=0.016252,rank=3\n",
      "[SoftImpute] Iter 153: observed MAE=0.007486 validation MAE=0.016237,rank=3\n",
      "[SoftImpute] Iter 154: observed MAE=0.007477 validation MAE=0.016221,rank=3\n",
      "[SoftImpute] Iter 155: observed MAE=0.007468 validation MAE=0.016206,rank=3\n",
      "[SoftImpute] Iter 156: observed MAE=0.007460 validation MAE=0.016191,rank=3\n",
      "[SoftImpute] Iter 157: observed MAE=0.007451 validation MAE=0.016176,rank=3\n",
      "[SoftImpute] Iter 158: observed MAE=0.007443 validation MAE=0.016161,rank=3\n",
      "[SoftImpute] Iter 159: observed MAE=0.007435 validation MAE=0.016146,rank=3\n",
      "[SoftImpute] Iter 160: observed MAE=0.007427 validation MAE=0.016132,rank=3\n",
      "[SoftImpute] Iter 161: observed MAE=0.007419 validation MAE=0.016117,rank=3\n",
      "[SoftImpute] Iter 162: observed MAE=0.007411 validation MAE=0.016103,rank=3\n",
      "[SoftImpute] Iter 163: observed MAE=0.007403 validation MAE=0.016089,rank=3\n",
      "[SoftImpute] Iter 164: observed MAE=0.007395 validation MAE=0.016075,rank=3\n",
      "[SoftImpute] Iter 165: observed MAE=0.007388 validation MAE=0.016061,rank=3\n",
      "[SoftImpute] Iter 166: observed MAE=0.007380 validation MAE=0.016048,rank=3\n",
      "[SoftImpute] Iter 167: observed MAE=0.007373 validation MAE=0.016034,rank=3\n",
      "[SoftImpute] Iter 168: observed MAE=0.007366 validation MAE=0.016021,rank=3\n",
      "[SoftImpute] Iter 169: observed MAE=0.007358 validation MAE=0.016008,rank=3\n",
      "[SoftImpute] Iter 170: observed MAE=0.007351 validation MAE=0.015995,rank=3\n",
      "[SoftImpute] Iter 171: observed MAE=0.007344 validation MAE=0.015983,rank=3\n",
      "[SoftImpute] Iter 172: observed MAE=0.007337 validation MAE=0.015970,rank=3\n",
      "[SoftImpute] Iter 173: observed MAE=0.007330 validation MAE=0.015958,rank=3\n",
      "[SoftImpute] Iter 174: observed MAE=0.007323 validation MAE=0.015946,rank=3\n",
      "[SoftImpute] Iter 175: observed MAE=0.007317 validation MAE=0.015934,rank=3\n",
      "[SoftImpute] Iter 176: observed MAE=0.007310 validation MAE=0.015922,rank=3\n",
      "[SoftImpute] Iter 177: observed MAE=0.007303 validation MAE=0.015910,rank=3\n",
      "[SoftImpute] Iter 178: observed MAE=0.007297 validation MAE=0.015898,rank=3\n",
      "[SoftImpute] Iter 179: observed MAE=0.007290 validation MAE=0.015886,rank=3\n",
      "[SoftImpute] Iter 180: observed MAE=0.007284 validation MAE=0.015874,rank=3\n",
      "[SoftImpute] Iter 181: observed MAE=0.007278 validation MAE=0.015863,rank=3\n",
      "[SoftImpute] Iter 182: observed MAE=0.007272 validation MAE=0.015851,rank=3\n",
      "[SoftImpute] Iter 183: observed MAE=0.007266 validation MAE=0.015840,rank=3\n",
      "[SoftImpute] Iter 184: observed MAE=0.007260 validation MAE=0.015829,rank=3\n",
      "[SoftImpute] Iter 185: observed MAE=0.007254 validation MAE=0.015818,rank=3\n",
      "[SoftImpute] Iter 186: observed MAE=0.007248 validation MAE=0.015807,rank=3\n",
      "[SoftImpute] Iter 187: observed MAE=0.007242 validation MAE=0.015796,rank=3\n",
      "[SoftImpute] Iter 188: observed MAE=0.007236 validation MAE=0.015785,rank=3\n",
      "[SoftImpute] Iter 189: observed MAE=0.007231 validation MAE=0.015774,rank=3\n",
      "[SoftImpute] Iter 190: observed MAE=0.007225 validation MAE=0.015764,rank=3\n",
      "[SoftImpute] Iter 191: observed MAE=0.007220 validation MAE=0.015753,rank=3\n",
      "[SoftImpute] Iter 192: observed MAE=0.007214 validation MAE=0.015743,rank=3\n",
      "[SoftImpute] Iter 193: observed MAE=0.007209 validation MAE=0.015732,rank=3\n",
      "[SoftImpute] Iter 194: observed MAE=0.007204 validation MAE=0.015722,rank=3\n",
      "[SoftImpute] Iter 195: observed MAE=0.007198 validation MAE=0.015712,rank=3\n",
      "[SoftImpute] Iter 196: observed MAE=0.007193 validation MAE=0.015702,rank=3\n",
      "[SoftImpute] Iter 197: observed MAE=0.007188 validation MAE=0.015692,rank=3\n",
      "[SoftImpute] Iter 198: observed MAE=0.007183 validation MAE=0.015683,rank=3\n",
      "[SoftImpute] Iter 199: observed MAE=0.007178 validation MAE=0.015673,rank=3\n",
      "[SoftImpute] Iter 200: observed MAE=0.007173 validation MAE=0.015663,rank=3\n",
      "[SoftImpute] Iter 201: observed MAE=0.007168 validation MAE=0.015654,rank=3\n",
      "[SoftImpute] Iter 202: observed MAE=0.007164 validation MAE=0.015645,rank=3\n",
      "[SoftImpute] Iter 203: observed MAE=0.007159 validation MAE=0.015636,rank=3\n",
      "[SoftImpute] Iter 204: observed MAE=0.007154 validation MAE=0.015628,rank=3\n",
      "[SoftImpute] Iter 205: observed MAE=0.007150 validation MAE=0.015619,rank=3\n",
      "[SoftImpute] Iter 206: observed MAE=0.007145 validation MAE=0.015610,rank=3\n",
      "[SoftImpute] Iter 207: observed MAE=0.007140 validation MAE=0.015602,rank=3\n",
      "[SoftImpute] Iter 208: observed MAE=0.007136 validation MAE=0.015593,rank=3\n",
      "[SoftImpute] Iter 209: observed MAE=0.007131 validation MAE=0.015585,rank=3\n",
      "[SoftImpute] Iter 210: observed MAE=0.007127 validation MAE=0.015576,rank=3\n",
      "[SoftImpute] Iter 211: observed MAE=0.007123 validation MAE=0.015568,rank=3\n",
      "[SoftImpute] Iter 212: observed MAE=0.007118 validation MAE=0.015560,rank=3\n",
      "[SoftImpute] Iter 213: observed MAE=0.007114 validation MAE=0.015552,rank=3\n",
      "[SoftImpute] Iter 214: observed MAE=0.007110 validation MAE=0.015544,rank=3\n",
      "[SoftImpute] Iter 215: observed MAE=0.007106 validation MAE=0.015536,rank=3\n",
      "[SoftImpute] Iter 216: observed MAE=0.007102 validation MAE=0.015528,rank=3\n",
      "[SoftImpute] Iter 217: observed MAE=0.007098 validation MAE=0.015520,rank=3\n",
      "[SoftImpute] Iter 218: observed MAE=0.007094 validation MAE=0.015513,rank=3\n",
      "[SoftImpute] Iter 219: observed MAE=0.007090 validation MAE=0.015506,rank=3\n",
      "[SoftImpute] Iter 220: observed MAE=0.007086 validation MAE=0.015498,rank=3\n",
      "[SoftImpute] Iter 221: observed MAE=0.007082 validation MAE=0.015491,rank=3\n",
      "[SoftImpute] Iter 222: observed MAE=0.007078 validation MAE=0.015484,rank=3\n",
      "[SoftImpute] Iter 223: observed MAE=0.007074 validation MAE=0.015477,rank=3\n",
      "[SoftImpute] Iter 224: observed MAE=0.007070 validation MAE=0.015469,rank=3\n",
      "[SoftImpute] Iter 225: observed MAE=0.007066 validation MAE=0.015462,rank=3\n",
      "[SoftImpute] Iter 226: observed MAE=0.007063 validation MAE=0.015455,rank=3\n",
      "[SoftImpute] Iter 227: observed MAE=0.007059 validation MAE=0.015448,rank=3\n",
      "[SoftImpute] Iter 228: observed MAE=0.007055 validation MAE=0.015442,rank=3\n",
      "[SoftImpute] Iter 229: observed MAE=0.007051 validation MAE=0.015435,rank=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 230: observed MAE=0.007048 validation MAE=0.015428,rank=3\n",
      "[SoftImpute] Iter 231: observed MAE=0.007044 validation MAE=0.015422,rank=3\n",
      "[SoftImpute] Iter 232: observed MAE=0.007041 validation MAE=0.015416,rank=3\n",
      "[SoftImpute] Iter 233: observed MAE=0.007037 validation MAE=0.015410,rank=3\n",
      "[SoftImpute] Iter 234: observed MAE=0.007034 validation MAE=0.015404,rank=3\n",
      "[SoftImpute] Iter 235: observed MAE=0.007030 validation MAE=0.015398,rank=3\n",
      "[SoftImpute] Iter 236: observed MAE=0.007027 validation MAE=0.015392,rank=3\n",
      "[SoftImpute] Iter 237: observed MAE=0.007024 validation MAE=0.015386,rank=3\n",
      "[SoftImpute] Iter 238: observed MAE=0.007020 validation MAE=0.015380,rank=3\n",
      "[SoftImpute] Iter 239: observed MAE=0.007017 validation MAE=0.015375,rank=3\n",
      "[SoftImpute] Iter 240: observed MAE=0.007014 validation MAE=0.015369,rank=3\n",
      "[SoftImpute] Iter 241: observed MAE=0.007010 validation MAE=0.015364,rank=3\n",
      "[SoftImpute] Iter 242: observed MAE=0.007007 validation MAE=0.015358,rank=3\n",
      "[SoftImpute] Iter 243: observed MAE=0.007004 validation MAE=0.015353,rank=3\n",
      "[SoftImpute] Iter 244: observed MAE=0.007001 validation MAE=0.015348,rank=3\n",
      "[SoftImpute] Iter 245: observed MAE=0.006998 validation MAE=0.015343,rank=3\n",
      "[SoftImpute] Iter 246: observed MAE=0.006995 validation MAE=0.015337,rank=3\n",
      "[SoftImpute] Iter 247: observed MAE=0.006992 validation MAE=0.015332,rank=3\n",
      "[SoftImpute] Iter 248: observed MAE=0.006989 validation MAE=0.015327,rank=3\n",
      "[SoftImpute] Iter 249: observed MAE=0.006986 validation MAE=0.015322,rank=3\n",
      "[SoftImpute] Iter 250: observed MAE=0.006983 validation MAE=0.015318,rank=3\n",
      "[SoftImpute] Iter 251: observed MAE=0.006980 validation MAE=0.015313,rank=3\n",
      "[SoftImpute] Iter 252: observed MAE=0.006977 validation MAE=0.015308,rank=3\n",
      "[SoftImpute] Iter 253: observed MAE=0.006974 validation MAE=0.015303,rank=3\n",
      "[SoftImpute] Iter 254: observed MAE=0.006971 validation MAE=0.015299,rank=3\n",
      "[SoftImpute] Iter 255: observed MAE=0.006968 validation MAE=0.015294,rank=3\n",
      "[SoftImpute] Iter 256: observed MAE=0.006966 validation MAE=0.015290,rank=3\n",
      "[SoftImpute] Iter 257: observed MAE=0.006963 validation MAE=0.015285,rank=3\n",
      "[SoftImpute] Iter 258: observed MAE=0.006960 validation MAE=0.015281,rank=3\n",
      "[SoftImpute] Iter 259: observed MAE=0.006957 validation MAE=0.015277,rank=3\n",
      "[SoftImpute] Iter 260: observed MAE=0.006954 validation MAE=0.015273,rank=3\n",
      "[SoftImpute] Iter 261: observed MAE=0.006952 validation MAE=0.015269,rank=3\n",
      "[SoftImpute] Stopped after iteration 261 for lambda=0.039366\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 7.573115110397339\n",
      "After the matrix factor stage, training error is 0.00695, validation error is 0.01527\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.34448, val loss: 0.34765\n",
      "Main effects training epoch: 2, train loss: 0.27354, val loss: 0.27853\n",
      "Main effects training epoch: 3, train loss: 0.20823, val loss: 0.21145\n",
      "Main effects training epoch: 4, train loss: 0.16135, val loss: 0.16514\n",
      "Main effects training epoch: 5, train loss: 0.14048, val loss: 0.14025\n",
      "Main effects training epoch: 6, train loss: 0.13280, val loss: 0.13280\n",
      "Main effects training epoch: 7, train loss: 0.13041, val loss: 0.12922\n",
      "Main effects training epoch: 8, train loss: 0.12977, val loss: 0.12861\n",
      "Main effects training epoch: 9, train loss: 0.12914, val loss: 0.12821\n",
      "Main effects training epoch: 10, train loss: 0.12852, val loss: 0.12844\n",
      "Main effects training epoch: 11, train loss: 0.12742, val loss: 0.12590\n",
      "Main effects training epoch: 12, train loss: 0.12612, val loss: 0.12564\n",
      "Main effects training epoch: 13, train loss: 0.12339, val loss: 0.12252\n",
      "Main effects training epoch: 14, train loss: 0.11835, val loss: 0.11883\n",
      "Main effects training epoch: 15, train loss: 0.11578, val loss: 0.11651\n",
      "Main effects training epoch: 16, train loss: 0.11289, val loss: 0.11344\n",
      "Main effects training epoch: 17, train loss: 0.11081, val loss: 0.11186\n",
      "Main effects training epoch: 18, train loss: 0.11088, val loss: 0.11312\n",
      "Main effects training epoch: 19, train loss: 0.11027, val loss: 0.11199\n",
      "Main effects training epoch: 20, train loss: 0.11090, val loss: 0.11238\n",
      "Main effects training epoch: 21, train loss: 0.10922, val loss: 0.11096\n",
      "Main effects training epoch: 22, train loss: 0.10900, val loss: 0.11106\n",
      "Main effects training epoch: 23, train loss: 0.10645, val loss: 0.10812\n",
      "Main effects training epoch: 24, train loss: 0.10707, val loss: 0.10858\n",
      "Main effects training epoch: 25, train loss: 0.10776, val loss: 0.10768\n",
      "Main effects training epoch: 26, train loss: 0.10556, val loss: 0.10715\n",
      "Main effects training epoch: 27, train loss: 0.10595, val loss: 0.10680\n",
      "Main effects training epoch: 28, train loss: 0.10509, val loss: 0.10619\n",
      "Main effects training epoch: 29, train loss: 0.10460, val loss: 0.10568\n",
      "Main effects training epoch: 30, train loss: 0.10512, val loss: 0.10585\n",
      "Main effects training epoch: 31, train loss: 0.10437, val loss: 0.10598\n",
      "Main effects training epoch: 32, train loss: 0.10431, val loss: 0.10577\n",
      "Main effects training epoch: 33, train loss: 0.10423, val loss: 0.10545\n",
      "Main effects training epoch: 34, train loss: 0.10413, val loss: 0.10561\n",
      "Main effects training epoch: 35, train loss: 0.10424, val loss: 0.10579\n",
      "Main effects training epoch: 36, train loss: 0.10444, val loss: 0.10578\n",
      "Main effects training epoch: 37, train loss: 0.10400, val loss: 0.10575\n",
      "Main effects training epoch: 38, train loss: 0.10438, val loss: 0.10591\n",
      "Main effects training epoch: 39, train loss: 0.10416, val loss: 0.10566\n",
      "Main effects training epoch: 40, train loss: 0.10393, val loss: 0.10524\n",
      "Main effects training epoch: 41, train loss: 0.10418, val loss: 0.10579\n",
      "Main effects training epoch: 42, train loss: 0.10392, val loss: 0.10565\n",
      "Main effects training epoch: 43, train loss: 0.10430, val loss: 0.10612\n",
      "Main effects training epoch: 44, train loss: 0.10409, val loss: 0.10553\n",
      "Main effects training epoch: 45, train loss: 0.10403, val loss: 0.10539\n",
      "Main effects training epoch: 46, train loss: 0.10400, val loss: 0.10547\n",
      "Main effects training epoch: 47, train loss: 0.10472, val loss: 0.10625\n",
      "Main effects training epoch: 48, train loss: 0.10410, val loss: 0.10597\n",
      "Main effects training epoch: 49, train loss: 0.10415, val loss: 0.10512\n",
      "Main effects training epoch: 50, train loss: 0.10408, val loss: 0.10560\n",
      "Main effects training epoch: 51, train loss: 0.10392, val loss: 0.10554\n",
      "Main effects training epoch: 52, train loss: 0.10395, val loss: 0.10519\n",
      "Main effects training epoch: 53, train loss: 0.10385, val loss: 0.10520\n",
      "Main effects training epoch: 54, train loss: 0.10398, val loss: 0.10563\n",
      "Main effects training epoch: 55, train loss: 0.10416, val loss: 0.10523\n",
      "Main effects training epoch: 56, train loss: 0.10404, val loss: 0.10586\n",
      "Main effects training epoch: 57, train loss: 0.10399, val loss: 0.10571\n",
      "Main effects training epoch: 58, train loss: 0.10443, val loss: 0.10631\n",
      "Main effects training epoch: 59, train loss: 0.10393, val loss: 0.10549\n",
      "Main effects training epoch: 60, train loss: 0.10386, val loss: 0.10534\n",
      "Main effects training epoch: 61, train loss: 0.10389, val loss: 0.10538\n",
      "Main effects training epoch: 62, train loss: 0.10417, val loss: 0.10552\n",
      "Main effects training epoch: 63, train loss: 0.10396, val loss: 0.10570\n",
      "Main effects training epoch: 64, train loss: 0.10412, val loss: 0.10546\n",
      "Main effects training epoch: 65, train loss: 0.10403, val loss: 0.10549\n",
      "Main effects training epoch: 66, train loss: 0.10398, val loss: 0.10514\n",
      "Main effects training epoch: 67, train loss: 0.10385, val loss: 0.10514\n",
      "Main effects training epoch: 68, train loss: 0.10396, val loss: 0.10561\n",
      "Main effects training epoch: 69, train loss: 0.10397, val loss: 0.10523\n",
      "Main effects training epoch: 70, train loss: 0.10420, val loss: 0.10542\n",
      "Main effects training epoch: 71, train loss: 0.10473, val loss: 0.10671\n",
      "Main effects training epoch: 72, train loss: 0.10418, val loss: 0.10552\n",
      "Main effects training epoch: 73, train loss: 0.10427, val loss: 0.10590\n",
      "Main effects training epoch: 74, train loss: 0.10409, val loss: 0.10540\n",
      "Main effects training epoch: 75, train loss: 0.10401, val loss: 0.10542\n",
      "Main effects training epoch: 76, train loss: 0.10384, val loss: 0.10560\n",
      "Main effects training epoch: 77, train loss: 0.10404, val loss: 0.10542\n",
      "Main effects training epoch: 78, train loss: 0.10394, val loss: 0.10550\n",
      "Main effects training epoch: 79, train loss: 0.10410, val loss: 0.10553\n",
      "Main effects training epoch: 80, train loss: 0.10425, val loss: 0.10542\n",
      "Main effects training epoch: 81, train loss: 0.10387, val loss: 0.10530\n",
      "Main effects training epoch: 82, train loss: 0.10409, val loss: 0.10607\n",
      "Main effects training epoch: 83, train loss: 0.10386, val loss: 0.10559\n",
      "Main effects training epoch: 84, train loss: 0.10405, val loss: 0.10549\n",
      "Main effects training epoch: 85, train loss: 0.10483, val loss: 0.10617\n",
      "Main effects training epoch: 86, train loss: 0.10383, val loss: 0.10551\n",
      "Main effects training epoch: 87, train loss: 0.10415, val loss: 0.10524\n",
      "Main effects training epoch: 88, train loss: 0.10389, val loss: 0.10586\n",
      "Main effects training epoch: 89, train loss: 0.10383, val loss: 0.10548\n",
      "Main effects training epoch: 90, train loss: 0.10400, val loss: 0.10512\n",
      "Main effects training epoch: 91, train loss: 0.10387, val loss: 0.10538\n",
      "Main effects training epoch: 92, train loss: 0.10420, val loss: 0.10635\n",
      "Main effects training epoch: 93, train loss: 0.10396, val loss: 0.10529\n",
      "Main effects training epoch: 94, train loss: 0.10417, val loss: 0.10594\n",
      "Main effects training epoch: 95, train loss: 0.10387, val loss: 0.10562\n",
      "Main effects training epoch: 96, train loss: 0.10378, val loss: 0.10533\n",
      "Main effects training epoch: 97, train loss: 0.10386, val loss: 0.10536\n",
      "Main effects training epoch: 98, train loss: 0.10425, val loss: 0.10630\n",
      "Main effects training epoch: 99, train loss: 0.10426, val loss: 0.10552\n",
      "Main effects training epoch: 100, train loss: 0.10419, val loss: 0.10632\n",
      "Main effects training epoch: 101, train loss: 0.10395, val loss: 0.10553\n",
      "Main effects training epoch: 102, train loss: 0.10397, val loss: 0.10584\n",
      "Main effects training epoch: 103, train loss: 0.10408, val loss: 0.10574\n",
      "Main effects training epoch: 104, train loss: 0.10395, val loss: 0.10576\n",
      "Main effects training epoch: 105, train loss: 0.10380, val loss: 0.10555\n",
      "Main effects training epoch: 106, train loss: 0.10382, val loss: 0.10590\n",
      "Main effects training epoch: 107, train loss: 0.10417, val loss: 0.10600\n",
      "Main effects training epoch: 108, train loss: 0.10410, val loss: 0.10567\n",
      "Main effects training epoch: 109, train loss: 0.10373, val loss: 0.10542\n",
      "Main effects training epoch: 110, train loss: 0.10377, val loss: 0.10525\n",
      "Main effects training epoch: 111, train loss: 0.10405, val loss: 0.10577\n",
      "Main effects training epoch: 112, train loss: 0.10409, val loss: 0.10590\n",
      "Main effects training epoch: 113, train loss: 0.10474, val loss: 0.10615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 114, train loss: 0.10406, val loss: 0.10594\n",
      "Main effects training epoch: 115, train loss: 0.10415, val loss: 0.10553\n",
      "Main effects training epoch: 116, train loss: 0.10437, val loss: 0.10680\n",
      "Main effects training epoch: 117, train loss: 0.10399, val loss: 0.10532\n",
      "Main effects training epoch: 118, train loss: 0.10393, val loss: 0.10583\n",
      "Main effects training epoch: 119, train loss: 0.10410, val loss: 0.10601\n",
      "Main effects training epoch: 120, train loss: 0.10375, val loss: 0.10578\n",
      "Main effects training epoch: 121, train loss: 0.10382, val loss: 0.10529\n",
      "Main effects training epoch: 122, train loss: 0.10391, val loss: 0.10552\n",
      "Main effects training epoch: 123, train loss: 0.10397, val loss: 0.10615\n",
      "Main effects training epoch: 124, train loss: 0.10422, val loss: 0.10572\n",
      "Main effects training epoch: 125, train loss: 0.10414, val loss: 0.10601\n",
      "Main effects training epoch: 126, train loss: 0.10402, val loss: 0.10605\n",
      "Main effects training epoch: 127, train loss: 0.10402, val loss: 0.10531\n",
      "Main effects training epoch: 128, train loss: 0.10395, val loss: 0.10615\n",
      "Main effects training epoch: 129, train loss: 0.10410, val loss: 0.10643\n",
      "Main effects training epoch: 130, train loss: 0.10401, val loss: 0.10526\n",
      "Main effects training epoch: 131, train loss: 0.10380, val loss: 0.10577\n",
      "Main effects training epoch: 132, train loss: 0.10404, val loss: 0.10554\n",
      "Main effects training epoch: 133, train loss: 0.10425, val loss: 0.10648\n",
      "Main effects training epoch: 134, train loss: 0.10454, val loss: 0.10616\n",
      "Main effects training epoch: 135, train loss: 0.10424, val loss: 0.10653\n",
      "Main effects training epoch: 136, train loss: 0.10426, val loss: 0.10565\n",
      "Main effects training epoch: 137, train loss: 0.10444, val loss: 0.10639\n",
      "Main effects training epoch: 138, train loss: 0.10433, val loss: 0.10622\n",
      "Main effects training epoch: 139, train loss: 0.10384, val loss: 0.10565\n",
      "Main effects training epoch: 140, train loss: 0.10392, val loss: 0.10539\n",
      "Main effects training epoch: 141, train loss: 0.10420, val loss: 0.10593\n",
      "Main effects training epoch: 142, train loss: 0.10404, val loss: 0.10617\n",
      "Main effects training epoch: 143, train loss: 0.10420, val loss: 0.10567\n",
      "Main effects training epoch: 144, train loss: 0.10383, val loss: 0.10601\n",
      "Main effects training epoch: 145, train loss: 0.10370, val loss: 0.10530\n",
      "Main effects training epoch: 146, train loss: 0.10381, val loss: 0.10596\n",
      "Main effects training epoch: 147, train loss: 0.10380, val loss: 0.10575\n",
      "Main effects training epoch: 148, train loss: 0.10377, val loss: 0.10571\n",
      "Main effects training epoch: 149, train loss: 0.10383, val loss: 0.10566\n",
      "Main effects training epoch: 150, train loss: 0.10393, val loss: 0.10604\n",
      "Early stop at epoch 150, with validation loss: 0.10604\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.10405, val loss: 0.10498\n",
      "Main effects tuning epoch: 2, train loss: 0.10423, val loss: 0.10552\n",
      "Main effects tuning epoch: 3, train loss: 0.10411, val loss: 0.10505\n",
      "Main effects tuning epoch: 4, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 5, train loss: 0.10382, val loss: 0.10498\n",
      "Main effects tuning epoch: 6, train loss: 0.10425, val loss: 0.10582\n",
      "Main effects tuning epoch: 7, train loss: 0.10408, val loss: 0.10530\n",
      "Main effects tuning epoch: 8, train loss: 0.10397, val loss: 0.10527\n",
      "Main effects tuning epoch: 9, train loss: 0.10401, val loss: 0.10507\n",
      "Main effects tuning epoch: 10, train loss: 0.10407, val loss: 0.10539\n",
      "Main effects tuning epoch: 11, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 12, train loss: 0.10393, val loss: 0.10556\n",
      "Main effects tuning epoch: 13, train loss: 0.10415, val loss: 0.10510\n",
      "Main effects tuning epoch: 14, train loss: 0.10397, val loss: 0.10570\n",
      "Main effects tuning epoch: 15, train loss: 0.10386, val loss: 0.10516\n",
      "Main effects tuning epoch: 16, train loss: 0.10395, val loss: 0.10562\n",
      "Main effects tuning epoch: 17, train loss: 0.10395, val loss: 0.10539\n",
      "Main effects tuning epoch: 18, train loss: 0.10422, val loss: 0.10568\n",
      "Main effects tuning epoch: 19, train loss: 0.10433, val loss: 0.10517\n",
      "Main effects tuning epoch: 20, train loss: 0.10384, val loss: 0.10568\n",
      "Main effects tuning epoch: 21, train loss: 0.10400, val loss: 0.10541\n",
      "Main effects tuning epoch: 22, train loss: 0.10393, val loss: 0.10526\n",
      "Main effects tuning epoch: 23, train loss: 0.10434, val loss: 0.10582\n",
      "Main effects tuning epoch: 24, train loss: 0.10406, val loss: 0.10540\n",
      "Main effects tuning epoch: 25, train loss: 0.10410, val loss: 0.10570\n",
      "Main effects tuning epoch: 26, train loss: 0.10405, val loss: 0.10569\n",
      "Main effects tuning epoch: 27, train loss: 0.10406, val loss: 0.10556\n",
      "Main effects tuning epoch: 28, train loss: 0.10394, val loss: 0.10542\n",
      "Main effects tuning epoch: 29, train loss: 0.10383, val loss: 0.10542\n",
      "Main effects tuning epoch: 30, train loss: 0.10385, val loss: 0.10548\n",
      "Main effects tuning epoch: 31, train loss: 0.10395, val loss: 0.10516\n",
      "Main effects tuning epoch: 32, train loss: 0.10391, val loss: 0.10584\n",
      "Main effects tuning epoch: 33, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 34, train loss: 0.10391, val loss: 0.10561\n",
      "Main effects tuning epoch: 35, train loss: 0.10424, val loss: 0.10531\n",
      "Main effects tuning epoch: 36, train loss: 0.10395, val loss: 0.10554\n",
      "Main effects tuning epoch: 37, train loss: 0.10400, val loss: 0.10600\n",
      "Main effects tuning epoch: 38, train loss: 0.10408, val loss: 0.10530\n",
      "Main effects tuning epoch: 39, train loss: 0.10394, val loss: 0.10565\n",
      "Main effects tuning epoch: 40, train loss: 0.10385, val loss: 0.10569\n",
      "Main effects tuning epoch: 41, train loss: 0.10390, val loss: 0.10539\n",
      "Main effects tuning epoch: 42, train loss: 0.10400, val loss: 0.10587\n",
      "Main effects tuning epoch: 43, train loss: 0.10386, val loss: 0.10547\n",
      "Main effects tuning epoch: 44, train loss: 0.10403, val loss: 0.10597\n",
      "Main effects tuning epoch: 45, train loss: 0.10384, val loss: 0.10541\n",
      "Main effects tuning epoch: 46, train loss: 0.10398, val loss: 0.10582\n",
      "Main effects tuning epoch: 47, train loss: 0.10375, val loss: 0.10527\n",
      "Main effects tuning epoch: 48, train loss: 0.10380, val loss: 0.10549\n",
      "Main effects tuning epoch: 49, train loss: 0.10396, val loss: 0.10566\n",
      "Main effects tuning epoch: 50, train loss: 0.10407, val loss: 0.10549\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.09376, val loss: 0.09232\n",
      "Interaction training epoch: 2, train loss: 0.20140, val loss: 0.20111\n",
      "Interaction training epoch: 3, train loss: 0.06617, val loss: 0.06860\n",
      "Interaction training epoch: 4, train loss: 0.05173, val loss: 0.05197\n",
      "Interaction training epoch: 5, train loss: 0.06497, val loss: 0.06595\n",
      "Interaction training epoch: 6, train loss: 0.04386, val loss: 0.04449\n",
      "Interaction training epoch: 7, train loss: 0.04606, val loss: 0.04529\n",
      "Interaction training epoch: 8, train loss: 0.04492, val loss: 0.04489\n",
      "Interaction training epoch: 9, train loss: 0.04997, val loss: 0.04868\n",
      "Interaction training epoch: 10, train loss: 0.04002, val loss: 0.04133\n",
      "Interaction training epoch: 11, train loss: 0.04269, val loss: 0.04306\n",
      "Interaction training epoch: 12, train loss: 0.04100, val loss: 0.03997\n",
      "Interaction training epoch: 13, train loss: 0.04366, val loss: 0.04360\n",
      "Interaction training epoch: 14, train loss: 0.04928, val loss: 0.05084\n",
      "Interaction training epoch: 15, train loss: 0.04532, val loss: 0.04488\n",
      "Interaction training epoch: 16, train loss: 0.04530, val loss: 0.04524\n",
      "Interaction training epoch: 17, train loss: 0.03784, val loss: 0.03763\n",
      "Interaction training epoch: 18, train loss: 0.04939, val loss: 0.05023\n",
      "Interaction training epoch: 19, train loss: 0.03608, val loss: 0.03597\n",
      "Interaction training epoch: 20, train loss: 0.04344, val loss: 0.04367\n",
      "Interaction training epoch: 21, train loss: 0.04002, val loss: 0.03932\n",
      "Interaction training epoch: 22, train loss: 0.03977, val loss: 0.03977\n",
      "Interaction training epoch: 23, train loss: 0.03777, val loss: 0.03709\n",
      "Interaction training epoch: 24, train loss: 0.03927, val loss: 0.03924\n",
      "Interaction training epoch: 25, train loss: 0.04164, val loss: 0.04112\n",
      "Interaction training epoch: 26, train loss: 0.04429, val loss: 0.04389\n",
      "Interaction training epoch: 27, train loss: 0.04417, val loss: 0.04416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 28, train loss: 0.04241, val loss: 0.04112\n",
      "Interaction training epoch: 29, train loss: 0.03548, val loss: 0.03672\n",
      "Interaction training epoch: 30, train loss: 0.04420, val loss: 0.04353\n",
      "Interaction training epoch: 31, train loss: 0.04020, val loss: 0.04004\n",
      "Interaction training epoch: 32, train loss: 0.03422, val loss: 0.03451\n",
      "Interaction training epoch: 33, train loss: 0.03378, val loss: 0.03407\n",
      "Interaction training epoch: 34, train loss: 0.03925, val loss: 0.03880\n",
      "Interaction training epoch: 35, train loss: 0.03752, val loss: 0.03636\n",
      "Interaction training epoch: 36, train loss: 0.03582, val loss: 0.03564\n",
      "Interaction training epoch: 37, train loss: 0.03069, val loss: 0.03087\n",
      "Interaction training epoch: 38, train loss: 0.03729, val loss: 0.03652\n",
      "Interaction training epoch: 39, train loss: 0.03772, val loss: 0.03772\n",
      "Interaction training epoch: 40, train loss: 0.04332, val loss: 0.04201\n",
      "Interaction training epoch: 41, train loss: 0.03409, val loss: 0.03435\n",
      "Interaction training epoch: 42, train loss: 0.03501, val loss: 0.03419\n",
      "Interaction training epoch: 43, train loss: 0.03630, val loss: 0.03573\n",
      "Interaction training epoch: 44, train loss: 0.03447, val loss: 0.03420\n",
      "Interaction training epoch: 45, train loss: 0.03622, val loss: 0.03541\n",
      "Interaction training epoch: 46, train loss: 0.02991, val loss: 0.03002\n",
      "Interaction training epoch: 47, train loss: 0.02880, val loss: 0.02856\n",
      "Interaction training epoch: 48, train loss: 0.03548, val loss: 0.03469\n",
      "Interaction training epoch: 49, train loss: 0.03399, val loss: 0.03256\n",
      "Interaction training epoch: 50, train loss: 0.03225, val loss: 0.03266\n",
      "Interaction training epoch: 51, train loss: 0.03286, val loss: 0.03176\n",
      "Interaction training epoch: 52, train loss: 0.03084, val loss: 0.03012\n",
      "Interaction training epoch: 53, train loss: 0.03059, val loss: 0.02998\n",
      "Interaction training epoch: 54, train loss: 0.02966, val loss: 0.02932\n",
      "Interaction training epoch: 55, train loss: 0.04122, val loss: 0.04119\n",
      "Interaction training epoch: 56, train loss: 0.03164, val loss: 0.03059\n",
      "Interaction training epoch: 57, train loss: 0.03483, val loss: 0.03469\n",
      "Interaction training epoch: 58, train loss: 0.03320, val loss: 0.03294\n",
      "Interaction training epoch: 59, train loss: 0.02999, val loss: 0.02930\n",
      "Interaction training epoch: 60, train loss: 0.03540, val loss: 0.03496\n",
      "Interaction training epoch: 61, train loss: 0.03142, val loss: 0.02962\n",
      "Interaction training epoch: 62, train loss: 0.03146, val loss: 0.03164\n",
      "Interaction training epoch: 63, train loss: 0.03481, val loss: 0.03421\n",
      "Interaction training epoch: 64, train loss: 0.02946, val loss: 0.02881\n",
      "Interaction training epoch: 65, train loss: 0.03539, val loss: 0.03511\n",
      "Interaction training epoch: 66, train loss: 0.03797, val loss: 0.03805\n",
      "Interaction training epoch: 67, train loss: 0.03669, val loss: 0.03627\n",
      "Interaction training epoch: 68, train loss: 0.04844, val loss: 0.04881\n",
      "Interaction training epoch: 69, train loss: 0.02947, val loss: 0.02941\n",
      "Interaction training epoch: 70, train loss: 0.03805, val loss: 0.03672\n",
      "Interaction training epoch: 71, train loss: 0.02990, val loss: 0.02970\n",
      "Interaction training epoch: 72, train loss: 0.03926, val loss: 0.03952\n",
      "Interaction training epoch: 73, train loss: 0.03713, val loss: 0.03732\n",
      "Interaction training epoch: 74, train loss: 0.04469, val loss: 0.04392\n",
      "Interaction training epoch: 75, train loss: 0.03518, val loss: 0.03491\n",
      "Interaction training epoch: 76, train loss: 0.03954, val loss: 0.03893\n",
      "Interaction training epoch: 77, train loss: 0.04294, val loss: 0.04248\n",
      "Interaction training epoch: 78, train loss: 0.03122, val loss: 0.03170\n",
      "Interaction training epoch: 79, train loss: 0.03577, val loss: 0.03538\n",
      "Interaction training epoch: 80, train loss: 0.04727, val loss: 0.04664\n",
      "Interaction training epoch: 81, train loss: 0.03478, val loss: 0.03412\n",
      "Interaction training epoch: 82, train loss: 0.03736, val loss: 0.03700\n",
      "Interaction training epoch: 83, train loss: 0.03716, val loss: 0.03665\n",
      "Interaction training epoch: 84, train loss: 0.03675, val loss: 0.03575\n",
      "Interaction training epoch: 85, train loss: 0.03815, val loss: 0.03817\n",
      "Interaction training epoch: 86, train loss: 0.03960, val loss: 0.03903\n",
      "Interaction training epoch: 87, train loss: 0.03498, val loss: 0.03402\n",
      "Interaction training epoch: 88, train loss: 0.03630, val loss: 0.03583\n",
      "Interaction training epoch: 89, train loss: 0.03454, val loss: 0.03517\n",
      "Interaction training epoch: 90, train loss: 0.07231, val loss: 0.07223\n",
      "Interaction training epoch: 91, train loss: 0.04800, val loss: 0.04802\n",
      "Interaction training epoch: 92, train loss: 0.05224, val loss: 0.05162\n",
      "Interaction training epoch: 93, train loss: 0.02794, val loss: 0.02804\n",
      "Interaction training epoch: 94, train loss: 0.03585, val loss: 0.03546\n",
      "Interaction training epoch: 95, train loss: 0.04149, val loss: 0.04158\n",
      "Interaction training epoch: 96, train loss: 0.04943, val loss: 0.04909\n",
      "Interaction training epoch: 97, train loss: 0.05722, val loss: 0.05690\n",
      "Interaction training epoch: 98, train loss: 0.04854, val loss: 0.04831\n",
      "Interaction training epoch: 99, train loss: 0.02719, val loss: 0.02716\n",
      "Interaction training epoch: 100, train loss: 0.03689, val loss: 0.03722\n",
      "Interaction training epoch: 101, train loss: 0.04609, val loss: 0.04601\n",
      "Interaction training epoch: 102, train loss: 0.03250, val loss: 0.03231\n",
      "Interaction training epoch: 103, train loss: 0.03006, val loss: 0.02936\n",
      "Interaction training epoch: 104, train loss: 0.03967, val loss: 0.03937\n",
      "Interaction training epoch: 105, train loss: 0.03096, val loss: 0.03100\n",
      "Interaction training epoch: 106, train loss: 0.05099, val loss: 0.05044\n",
      "Interaction training epoch: 107, train loss: 0.03358, val loss: 0.03389\n",
      "Interaction training epoch: 108, train loss: 0.04273, val loss: 0.04236\n",
      "Interaction training epoch: 109, train loss: 0.07259, val loss: 0.07140\n",
      "Interaction training epoch: 110, train loss: 0.03442, val loss: 0.03433\n",
      "Interaction training epoch: 111, train loss: 0.02947, val loss: 0.02893\n",
      "Interaction training epoch: 112, train loss: 0.03413, val loss: 0.03396\n",
      "Interaction training epoch: 113, train loss: 0.03441, val loss: 0.03428\n",
      "Interaction training epoch: 114, train loss: 0.05762, val loss: 0.05645\n",
      "Interaction training epoch: 115, train loss: 0.02938, val loss: 0.02918\n",
      "Interaction training epoch: 116, train loss: 0.04805, val loss: 0.04824\n",
      "Interaction training epoch: 117, train loss: 0.03927, val loss: 0.03835\n",
      "Interaction training epoch: 118, train loss: 0.03949, val loss: 0.03920\n",
      "Interaction training epoch: 119, train loss: 0.03835, val loss: 0.03852\n",
      "Interaction training epoch: 120, train loss: 0.07402, val loss: 0.07267\n",
      "Interaction training epoch: 121, train loss: 0.02786, val loss: 0.02794\n",
      "Interaction training epoch: 122, train loss: 0.02966, val loss: 0.02840\n",
      "Interaction training epoch: 123, train loss: 0.03146, val loss: 0.03080\n",
      "Interaction training epoch: 124, train loss: 0.03135, val loss: 0.03095\n",
      "Interaction training epoch: 125, train loss: 0.02893, val loss: 0.02832\n",
      "Interaction training epoch: 126, train loss: 0.06644, val loss: 0.06554\n",
      "Interaction training epoch: 127, train loss: 0.02439, val loss: 0.02405\n",
      "Interaction training epoch: 128, train loss: 0.02442, val loss: 0.02392\n",
      "Interaction training epoch: 129, train loss: 0.06718, val loss: 0.06662\n",
      "Interaction training epoch: 130, train loss: 0.06873, val loss: 0.06752\n",
      "Interaction training epoch: 131, train loss: 0.02836, val loss: 0.02795\n",
      "Interaction training epoch: 132, train loss: 0.02537, val loss: 0.02462\n",
      "Interaction training epoch: 133, train loss: 0.05578, val loss: 0.05497\n",
      "Interaction training epoch: 134, train loss: 0.02756, val loss: 0.02628\n",
      "Interaction training epoch: 135, train loss: 0.03851, val loss: 0.03792\n",
      "Interaction training epoch: 136, train loss: 0.02792, val loss: 0.02763\n",
      "Interaction training epoch: 137, train loss: 0.02882, val loss: 0.02822\n",
      "Interaction training epoch: 138, train loss: 0.04268, val loss: 0.04230\n",
      "Interaction training epoch: 139, train loss: 0.02999, val loss: 0.03028\n",
      "Interaction training epoch: 140, train loss: 0.02653, val loss: 0.02598\n",
      "Interaction training epoch: 141, train loss: 0.02864, val loss: 0.02740\n",
      "Interaction training epoch: 142, train loss: 0.04031, val loss: 0.03930\n",
      "Interaction training epoch: 143, train loss: 0.08930, val loss: 0.08825\n",
      "Interaction training epoch: 144, train loss: 0.03278, val loss: 0.03201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 145, train loss: 0.07595, val loss: 0.07444\n",
      "Interaction training epoch: 146, train loss: 0.04088, val loss: 0.04007\n",
      "Interaction training epoch: 147, train loss: 0.03063, val loss: 0.02967\n",
      "Interaction training epoch: 148, train loss: 0.08210, val loss: 0.08116\n",
      "Interaction training epoch: 149, train loss: 0.02600, val loss: 0.02595\n",
      "Interaction training epoch: 150, train loss: 0.04451, val loss: 0.04266\n",
      "Interaction training epoch: 151, train loss: 0.05071, val loss: 0.04953\n",
      "Interaction training epoch: 152, train loss: 0.03689, val loss: 0.03657\n",
      "Interaction training epoch: 153, train loss: 0.05108, val loss: 0.05017\n",
      "Interaction training epoch: 154, train loss: 0.03125, val loss: 0.03020\n",
      "Interaction training epoch: 155, train loss: 0.05262, val loss: 0.05160\n",
      "Interaction training epoch: 156, train loss: 0.03189, val loss: 0.03099\n",
      "Interaction training epoch: 157, train loss: 0.03302, val loss: 0.03192\n",
      "Interaction training epoch: 158, train loss: 0.03919, val loss: 0.03867\n",
      "Interaction training epoch: 159, train loss: 0.06294, val loss: 0.06164\n",
      "Interaction training epoch: 160, train loss: 0.02504, val loss: 0.02398\n",
      "Interaction training epoch: 161, train loss: 0.02726, val loss: 0.02622\n",
      "Interaction training epoch: 162, train loss: 0.02696, val loss: 0.02663\n",
      "Interaction training epoch: 163, train loss: 0.04443, val loss: 0.04306\n",
      "Interaction training epoch: 164, train loss: 0.08229, val loss: 0.08080\n",
      "Interaction training epoch: 165, train loss: 0.02497, val loss: 0.02433\n",
      "Interaction training epoch: 166, train loss: 0.02405, val loss: 0.02321\n",
      "Interaction training epoch: 167, train loss: 0.03154, val loss: 0.03087\n",
      "Interaction training epoch: 168, train loss: 0.03079, val loss: 0.02948\n",
      "Interaction training epoch: 169, train loss: 0.02562, val loss: 0.02466\n",
      "Interaction training epoch: 170, train loss: 0.03950, val loss: 0.03811\n",
      "Interaction training epoch: 171, train loss: 0.05102, val loss: 0.05005\n",
      "Interaction training epoch: 172, train loss: 0.07393, val loss: 0.07199\n",
      "Interaction training epoch: 173, train loss: 0.03775, val loss: 0.03659\n",
      "Interaction training epoch: 174, train loss: 0.03283, val loss: 0.03172\n",
      "Interaction training epoch: 175, train loss: 0.02729, val loss: 0.02663\n",
      "Interaction training epoch: 176, train loss: 0.04249, val loss: 0.04090\n",
      "Interaction training epoch: 177, train loss: 0.11371, val loss: 0.11248\n",
      "Interaction training epoch: 178, train loss: 0.05008, val loss: 0.04851\n",
      "Interaction training epoch: 179, train loss: 0.05070, val loss: 0.04903\n",
      "Interaction training epoch: 180, train loss: 0.05192, val loss: 0.05082\n",
      "Interaction training epoch: 181, train loss: 0.03499, val loss: 0.03401\n",
      "Interaction training epoch: 182, train loss: 0.06009, val loss: 0.05850\n",
      "Interaction training epoch: 183, train loss: 0.03517, val loss: 0.03374\n",
      "Interaction training epoch: 184, train loss: 0.04107, val loss: 0.04010\n",
      "Interaction training epoch: 185, train loss: 0.06361, val loss: 0.06162\n",
      "Interaction training epoch: 186, train loss: 0.06349, val loss: 0.06178\n",
      "Interaction training epoch: 187, train loss: 0.05374, val loss: 0.05196\n",
      "Interaction training epoch: 188, train loss: 0.05958, val loss: 0.05804\n",
      "Interaction training epoch: 189, train loss: 0.02604, val loss: 0.02519\n",
      "Interaction training epoch: 190, train loss: 0.04369, val loss: 0.04302\n",
      "Interaction training epoch: 191, train loss: 0.04175, val loss: 0.04116\n",
      "Interaction training epoch: 192, train loss: 0.06782, val loss: 0.06645\n",
      "Interaction training epoch: 193, train loss: 0.06145, val loss: 0.06034\n",
      "Interaction training epoch: 194, train loss: 0.04303, val loss: 0.04151\n",
      "Interaction training epoch: 195, train loss: 0.03319, val loss: 0.03229\n",
      "Interaction training epoch: 196, train loss: 0.03143, val loss: 0.02941\n",
      "Interaction training epoch: 197, train loss: 0.03413, val loss: 0.03311\n",
      "Interaction training epoch: 198, train loss: 0.03045, val loss: 0.02901\n",
      "Interaction training epoch: 199, train loss: 0.04253, val loss: 0.04138\n",
      "Interaction training epoch: 200, train loss: 0.04032, val loss: 0.03845\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########4 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.02853, val loss: 0.02785\n",
      "Interaction tuning epoch: 2, train loss: 0.02401, val loss: 0.02299\n",
      "Interaction tuning epoch: 3, train loss: 0.04478, val loss: 0.04265\n",
      "Interaction tuning epoch: 4, train loss: 0.06395, val loss: 0.06292\n",
      "Interaction tuning epoch: 5, train loss: 0.02356, val loss: 0.02286\n",
      "Interaction tuning epoch: 6, train loss: 0.06369, val loss: 0.06228\n",
      "Interaction tuning epoch: 7, train loss: 0.04154, val loss: 0.04077\n",
      "Interaction tuning epoch: 8, train loss: 0.08354, val loss: 0.08227\n",
      "Interaction tuning epoch: 9, train loss: 0.07421, val loss: 0.07270\n",
      "Interaction tuning epoch: 10, train loss: 0.04015, val loss: 0.03931\n",
      "Interaction tuning epoch: 11, train loss: 0.02840, val loss: 0.02746\n",
      "Interaction tuning epoch: 12, train loss: 0.06172, val loss: 0.06021\n",
      "Interaction tuning epoch: 13, train loss: 0.03776, val loss: 0.03699\n",
      "Interaction tuning epoch: 14, train loss: 0.07388, val loss: 0.07213\n",
      "Interaction tuning epoch: 15, train loss: 0.02628, val loss: 0.02557\n",
      "Interaction tuning epoch: 16, train loss: 0.02860, val loss: 0.02813\n",
      "Interaction tuning epoch: 17, train loss: 0.04973, val loss: 0.04802\n",
      "Interaction tuning epoch: 18, train loss: 0.02869, val loss: 0.02741\n",
      "Interaction tuning epoch: 19, train loss: 0.03914, val loss: 0.03730\n",
      "Interaction tuning epoch: 20, train loss: 0.03596, val loss: 0.03526\n",
      "Interaction tuning epoch: 21, train loss: 0.04924, val loss: 0.04797\n",
      "Interaction tuning epoch: 22, train loss: 0.08235, val loss: 0.08148\n",
      "Interaction tuning epoch: 23, train loss: 0.08430, val loss: 0.08186\n",
      "Interaction tuning epoch: 24, train loss: 0.07758, val loss: 0.07581\n",
      "Interaction tuning epoch: 25, train loss: 0.02468, val loss: 0.02425\n",
      "Interaction tuning epoch: 26, train loss: 0.02729, val loss: 0.02674\n",
      "Interaction tuning epoch: 27, train loss: 0.02677, val loss: 0.02542\n",
      "Interaction tuning epoch: 28, train loss: 0.03303, val loss: 0.03233\n",
      "Interaction tuning epoch: 29, train loss: 0.07628, val loss: 0.07409\n",
      "Interaction tuning epoch: 30, train loss: 0.05354, val loss: 0.05248\n",
      "Interaction tuning epoch: 31, train loss: 0.02819, val loss: 0.02642\n",
      "Interaction tuning epoch: 32, train loss: 0.02994, val loss: 0.02905\n",
      "Interaction tuning epoch: 33, train loss: 0.03903, val loss: 0.03809\n",
      "Interaction tuning epoch: 34, train loss: 0.03406, val loss: 0.03261\n",
      "Interaction tuning epoch: 35, train loss: 0.02886, val loss: 0.02797\n",
      "Interaction tuning epoch: 36, train loss: 0.04728, val loss: 0.04591\n",
      "Interaction tuning epoch: 37, train loss: 0.02745, val loss: 0.02653\n",
      "Interaction tuning epoch: 38, train loss: 0.09613, val loss: 0.09462\n",
      "Interaction tuning epoch: 39, train loss: 0.06430, val loss: 0.06326\n",
      "Interaction tuning epoch: 40, train loss: 0.03564, val loss: 0.03467\n",
      "Interaction tuning epoch: 41, train loss: 0.05842, val loss: 0.05720\n",
      "Interaction tuning epoch: 42, train loss: 0.02719, val loss: 0.02601\n",
      "Interaction tuning epoch: 43, train loss: 0.03689, val loss: 0.03533\n",
      "Interaction tuning epoch: 44, train loss: 0.02563, val loss: 0.02452\n",
      "Interaction tuning epoch: 45, train loss: 0.11515, val loss: 0.11336\n",
      "Interaction tuning epoch: 46, train loss: 0.05383, val loss: 0.05274\n",
      "Interaction tuning epoch: 47, train loss: 0.03183, val loss: 0.02986\n",
      "Interaction tuning epoch: 48, train loss: 0.04755, val loss: 0.04566\n",
      "Interaction tuning epoch: 49, train loss: 0.03504, val loss: 0.03437\n",
      "Interaction tuning epoch: 50, train loss: 0.03646, val loss: 0.03600\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 36.567344665527344\n",
      "After the gam stage, training error is 0.03646 , validation error is 0.03600\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 1.968308\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.028879 validation MAE=0.034373,rank=4\n",
      "[SoftImpute] Iter 2: observed MAE=0.027006 validation MAE=0.033609,rank=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 3: observed MAE=0.025392 validation MAE=0.032779,rank=4\n",
      "[SoftImpute] Iter 4: observed MAE=0.023965 validation MAE=0.031935,rank=4\n",
      "[SoftImpute] Iter 5: observed MAE=0.022698 validation MAE=0.031105,rank=4\n",
      "[SoftImpute] Iter 6: observed MAE=0.021552 validation MAE=0.030311,rank=4\n",
      "[SoftImpute] Iter 7: observed MAE=0.020499 validation MAE=0.029565,rank=4\n",
      "[SoftImpute] Iter 8: observed MAE=0.019540 validation MAE=0.028876,rank=4\n",
      "[SoftImpute] Iter 9: observed MAE=0.018668 validation MAE=0.028225,rank=4\n",
      "[SoftImpute] Iter 10: observed MAE=0.017874 validation MAE=0.027607,rank=4\n",
      "[SoftImpute] Iter 11: observed MAE=0.017148 validation MAE=0.027037,rank=4\n",
      "[SoftImpute] Iter 12: observed MAE=0.016486 validation MAE=0.026499,rank=4\n",
      "[SoftImpute] Iter 13: observed MAE=0.015883 validation MAE=0.026004,rank=4\n",
      "[SoftImpute] Iter 14: observed MAE=0.015330 validation MAE=0.025549,rank=4\n",
      "[SoftImpute] Iter 15: observed MAE=0.014825 validation MAE=0.025131,rank=4\n",
      "[SoftImpute] Iter 16: observed MAE=0.014361 validation MAE=0.024740,rank=4\n",
      "[SoftImpute] Iter 17: observed MAE=0.013934 validation MAE=0.024373,rank=4\n",
      "[SoftImpute] Iter 18: observed MAE=0.013539 validation MAE=0.024029,rank=4\n",
      "[SoftImpute] Iter 19: observed MAE=0.013175 validation MAE=0.023712,rank=4\n",
      "[SoftImpute] Iter 20: observed MAE=0.012838 validation MAE=0.023425,rank=4\n",
      "[SoftImpute] Iter 21: observed MAE=0.012522 validation MAE=0.023156,rank=4\n",
      "[SoftImpute] Iter 22: observed MAE=0.012226 validation MAE=0.022901,rank=4\n",
      "[SoftImpute] Iter 23: observed MAE=0.011950 validation MAE=0.022661,rank=4\n",
      "[SoftImpute] Iter 24: observed MAE=0.011695 validation MAE=0.022437,rank=4\n",
      "[SoftImpute] Iter 25: observed MAE=0.011455 validation MAE=0.022227,rank=4\n",
      "[SoftImpute] Iter 26: observed MAE=0.011230 validation MAE=0.022032,rank=4\n",
      "[SoftImpute] Iter 27: observed MAE=0.011019 validation MAE=0.021847,rank=4\n",
      "[SoftImpute] Iter 28: observed MAE=0.010819 validation MAE=0.021669,rank=4\n",
      "[SoftImpute] Iter 29: observed MAE=0.010632 validation MAE=0.021500,rank=4\n",
      "[SoftImpute] Iter 30: observed MAE=0.010456 validation MAE=0.021339,rank=4\n",
      "[SoftImpute] Iter 31: observed MAE=0.010290 validation MAE=0.021184,rank=4\n",
      "[SoftImpute] Iter 32: observed MAE=0.010132 validation MAE=0.021039,rank=4\n",
      "[SoftImpute] Iter 33: observed MAE=0.009984 validation MAE=0.020902,rank=4\n",
      "[SoftImpute] Iter 34: observed MAE=0.009844 validation MAE=0.020771,rank=4\n",
      "[SoftImpute] Iter 35: observed MAE=0.009711 validation MAE=0.020646,rank=4\n",
      "[SoftImpute] Iter 36: observed MAE=0.009584 validation MAE=0.020527,rank=4\n",
      "[SoftImpute] Iter 37: observed MAE=0.009463 validation MAE=0.020412,rank=4\n",
      "[SoftImpute] Iter 38: observed MAE=0.009348 validation MAE=0.020301,rank=4\n",
      "[SoftImpute] Iter 39: observed MAE=0.009239 validation MAE=0.020194,rank=4\n",
      "[SoftImpute] Iter 40: observed MAE=0.009134 validation MAE=0.020092,rank=4\n",
      "[SoftImpute] Iter 41: observed MAE=0.009035 validation MAE=0.019994,rank=4\n",
      "[SoftImpute] Iter 42: observed MAE=0.008940 validation MAE=0.019902,rank=4\n",
      "[SoftImpute] Iter 43: observed MAE=0.008850 validation MAE=0.019815,rank=4\n",
      "[SoftImpute] Iter 44: observed MAE=0.008763 validation MAE=0.019731,rank=4\n",
      "[SoftImpute] Iter 45: observed MAE=0.008680 validation MAE=0.019650,rank=4\n",
      "[SoftImpute] Iter 46: observed MAE=0.008600 validation MAE=0.019572,rank=4\n",
      "[SoftImpute] Iter 47: observed MAE=0.008524 validation MAE=0.019495,rank=4\n",
      "[SoftImpute] Iter 48: observed MAE=0.008451 validation MAE=0.019422,rank=4\n",
      "[SoftImpute] Iter 49: observed MAE=0.008380 validation MAE=0.019350,rank=4\n",
      "[SoftImpute] Iter 50: observed MAE=0.008312 validation MAE=0.019280,rank=4\n",
      "[SoftImpute] Iter 51: observed MAE=0.008246 validation MAE=0.019211,rank=4\n",
      "[SoftImpute] Iter 52: observed MAE=0.008183 validation MAE=0.019145,rank=4\n",
      "[SoftImpute] Iter 53: observed MAE=0.008123 validation MAE=0.019079,rank=4\n",
      "[SoftImpute] Iter 54: observed MAE=0.008065 validation MAE=0.019015,rank=4\n",
      "[SoftImpute] Iter 55: observed MAE=0.008009 validation MAE=0.018953,rank=4\n",
      "[SoftImpute] Iter 56: observed MAE=0.007954 validation MAE=0.018892,rank=4\n",
      "[SoftImpute] Iter 57: observed MAE=0.007902 validation MAE=0.018832,rank=4\n",
      "[SoftImpute] Iter 58: observed MAE=0.007851 validation MAE=0.018775,rank=4\n",
      "[SoftImpute] Iter 59: observed MAE=0.007802 validation MAE=0.018719,rank=4\n",
      "[SoftImpute] Iter 60: observed MAE=0.007755 validation MAE=0.018665,rank=4\n",
      "[SoftImpute] Iter 61: observed MAE=0.007709 validation MAE=0.018613,rank=4\n",
      "[SoftImpute] Iter 62: observed MAE=0.007665 validation MAE=0.018562,rank=4\n",
      "[SoftImpute] Iter 63: observed MAE=0.007622 validation MAE=0.018512,rank=4\n",
      "[SoftImpute] Iter 64: observed MAE=0.007580 validation MAE=0.018464,rank=4\n",
      "[SoftImpute] Iter 65: observed MAE=0.007540 validation MAE=0.018418,rank=4\n",
      "[SoftImpute] Iter 66: observed MAE=0.007500 validation MAE=0.018373,rank=4\n",
      "[SoftImpute] Iter 67: observed MAE=0.007462 validation MAE=0.018329,rank=4\n",
      "[SoftImpute] Iter 68: observed MAE=0.007426 validation MAE=0.018286,rank=4\n",
      "[SoftImpute] Iter 69: observed MAE=0.007390 validation MAE=0.018244,rank=4\n",
      "[SoftImpute] Iter 70: observed MAE=0.007355 validation MAE=0.018203,rank=4\n",
      "[SoftImpute] Iter 71: observed MAE=0.007322 validation MAE=0.018162,rank=4\n",
      "[SoftImpute] Iter 72: observed MAE=0.007289 validation MAE=0.018122,rank=4\n",
      "[SoftImpute] Iter 73: observed MAE=0.007257 validation MAE=0.018084,rank=4\n",
      "[SoftImpute] Iter 74: observed MAE=0.007226 validation MAE=0.018047,rank=4\n",
      "[SoftImpute] Iter 75: observed MAE=0.007195 validation MAE=0.018010,rank=4\n",
      "[SoftImpute] Iter 76: observed MAE=0.007166 validation MAE=0.017975,rank=4\n",
      "[SoftImpute] Iter 77: observed MAE=0.007137 validation MAE=0.017940,rank=4\n",
      "[SoftImpute] Iter 78: observed MAE=0.007109 validation MAE=0.017905,rank=4\n",
      "[SoftImpute] Iter 79: observed MAE=0.007082 validation MAE=0.017871,rank=4\n",
      "[SoftImpute] Iter 80: observed MAE=0.007055 validation MAE=0.017837,rank=4\n",
      "[SoftImpute] Iter 81: observed MAE=0.007030 validation MAE=0.017804,rank=4\n",
      "[SoftImpute] Iter 82: observed MAE=0.007004 validation MAE=0.017772,rank=4\n",
      "[SoftImpute] Iter 83: observed MAE=0.006980 validation MAE=0.017740,rank=4\n",
      "[SoftImpute] Iter 84: observed MAE=0.006956 validation MAE=0.017709,rank=4\n",
      "[SoftImpute] Iter 85: observed MAE=0.006932 validation MAE=0.017679,rank=4\n",
      "[SoftImpute] Iter 86: observed MAE=0.006909 validation MAE=0.017650,rank=4\n",
      "[SoftImpute] Iter 87: observed MAE=0.006887 validation MAE=0.017621,rank=4\n",
      "[SoftImpute] Iter 88: observed MAE=0.006865 validation MAE=0.017593,rank=4\n",
      "[SoftImpute] Iter 89: observed MAE=0.006844 validation MAE=0.017565,rank=4\n",
      "[SoftImpute] Iter 90: observed MAE=0.006823 validation MAE=0.017537,rank=4\n",
      "[SoftImpute] Iter 91: observed MAE=0.006803 validation MAE=0.017510,rank=4\n",
      "[SoftImpute] Iter 92: observed MAE=0.006783 validation MAE=0.017484,rank=4\n",
      "[SoftImpute] Iter 93: observed MAE=0.006764 validation MAE=0.017457,rank=4\n",
      "[SoftImpute] Iter 94: observed MAE=0.006745 validation MAE=0.017431,rank=4\n",
      "[SoftImpute] Iter 95: observed MAE=0.006726 validation MAE=0.017406,rank=4\n",
      "[SoftImpute] Iter 96: observed MAE=0.006708 validation MAE=0.017381,rank=4\n",
      "[SoftImpute] Iter 97: observed MAE=0.006690 validation MAE=0.017357,rank=4\n",
      "[SoftImpute] Iter 98: observed MAE=0.006673 validation MAE=0.017333,rank=4\n",
      "[SoftImpute] Iter 99: observed MAE=0.006656 validation MAE=0.017310,rank=4\n",
      "[SoftImpute] Iter 100: observed MAE=0.006640 validation MAE=0.017286,rank=4\n",
      "[SoftImpute] Iter 101: observed MAE=0.006623 validation MAE=0.017263,rank=4\n",
      "[SoftImpute] Iter 102: observed MAE=0.006607 validation MAE=0.017241,rank=4\n",
      "[SoftImpute] Iter 103: observed MAE=0.006592 validation MAE=0.017218,rank=4\n",
      "[SoftImpute] Iter 104: observed MAE=0.006576 validation MAE=0.017196,rank=4\n",
      "[SoftImpute] Iter 105: observed MAE=0.006561 validation MAE=0.017175,rank=4\n",
      "[SoftImpute] Iter 106: observed MAE=0.006547 validation MAE=0.017153,rank=4\n",
      "[SoftImpute] Iter 107: observed MAE=0.006532 validation MAE=0.017132,rank=4\n",
      "[SoftImpute] Iter 108: observed MAE=0.006518 validation MAE=0.017112,rank=4\n",
      "[SoftImpute] Iter 109: observed MAE=0.006504 validation MAE=0.017091,rank=4\n",
      "[SoftImpute] Iter 110: observed MAE=0.006490 validation MAE=0.017071,rank=4\n",
      "[SoftImpute] Iter 111: observed MAE=0.006477 validation MAE=0.017051,rank=4\n",
      "[SoftImpute] Iter 112: observed MAE=0.006464 validation MAE=0.017032,rank=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 113: observed MAE=0.006451 validation MAE=0.017012,rank=4\n",
      "[SoftImpute] Iter 114: observed MAE=0.006439 validation MAE=0.016993,rank=4\n",
      "[SoftImpute] Iter 115: observed MAE=0.006426 validation MAE=0.016974,rank=4\n",
      "[SoftImpute] Iter 116: observed MAE=0.006414 validation MAE=0.016956,rank=4\n",
      "[SoftImpute] Iter 117: observed MAE=0.006402 validation MAE=0.016937,rank=4\n",
      "[SoftImpute] Iter 118: observed MAE=0.006391 validation MAE=0.016919,rank=4\n",
      "[SoftImpute] Iter 119: observed MAE=0.006379 validation MAE=0.016902,rank=4\n",
      "[SoftImpute] Iter 120: observed MAE=0.006368 validation MAE=0.016884,rank=4\n",
      "[SoftImpute] Iter 121: observed MAE=0.006357 validation MAE=0.016867,rank=4\n",
      "[SoftImpute] Iter 122: observed MAE=0.006346 validation MAE=0.016850,rank=4\n",
      "[SoftImpute] Iter 123: observed MAE=0.006335 validation MAE=0.016833,rank=4\n",
      "[SoftImpute] Iter 124: observed MAE=0.006325 validation MAE=0.016816,rank=4\n",
      "[SoftImpute] Iter 125: observed MAE=0.006314 validation MAE=0.016800,rank=4\n",
      "[SoftImpute] Iter 126: observed MAE=0.006304 validation MAE=0.016783,rank=4\n",
      "[SoftImpute] Iter 127: observed MAE=0.006294 validation MAE=0.016767,rank=4\n",
      "[SoftImpute] Iter 128: observed MAE=0.006284 validation MAE=0.016751,rank=4\n",
      "[SoftImpute] Iter 129: observed MAE=0.006275 validation MAE=0.016735,rank=4\n",
      "[SoftImpute] Iter 130: observed MAE=0.006265 validation MAE=0.016719,rank=4\n",
      "[SoftImpute] Iter 131: observed MAE=0.006256 validation MAE=0.016704,rank=4\n",
      "[SoftImpute] Iter 132: observed MAE=0.006247 validation MAE=0.016688,rank=4\n",
      "[SoftImpute] Iter 133: observed MAE=0.006238 validation MAE=0.016673,rank=4\n",
      "[SoftImpute] Iter 134: observed MAE=0.006229 validation MAE=0.016658,rank=4\n",
      "[SoftImpute] Iter 135: observed MAE=0.006220 validation MAE=0.016643,rank=4\n",
      "[SoftImpute] Iter 136: observed MAE=0.006211 validation MAE=0.016628,rank=4\n",
      "[SoftImpute] Iter 137: observed MAE=0.006203 validation MAE=0.016613,rank=4\n",
      "[SoftImpute] Iter 138: observed MAE=0.006194 validation MAE=0.016599,rank=4\n",
      "[SoftImpute] Iter 139: observed MAE=0.006186 validation MAE=0.016584,rank=4\n",
      "[SoftImpute] Iter 140: observed MAE=0.006178 validation MAE=0.016570,rank=4\n",
      "[SoftImpute] Iter 141: observed MAE=0.006170 validation MAE=0.016556,rank=4\n",
      "[SoftImpute] Iter 142: observed MAE=0.006162 validation MAE=0.016542,rank=4\n",
      "[SoftImpute] Iter 143: observed MAE=0.006154 validation MAE=0.016528,rank=4\n",
      "[SoftImpute] Iter 144: observed MAE=0.006146 validation MAE=0.016515,rank=4\n",
      "[SoftImpute] Iter 145: observed MAE=0.006138 validation MAE=0.016501,rank=4\n",
      "[SoftImpute] Iter 146: observed MAE=0.006131 validation MAE=0.016488,rank=4\n",
      "[SoftImpute] Iter 147: observed MAE=0.006124 validation MAE=0.016474,rank=4\n",
      "[SoftImpute] Iter 148: observed MAE=0.006116 validation MAE=0.016461,rank=4\n",
      "[SoftImpute] Iter 149: observed MAE=0.006109 validation MAE=0.016448,rank=4\n",
      "[SoftImpute] Iter 150: observed MAE=0.006102 validation MAE=0.016435,rank=4\n",
      "[SoftImpute] Iter 151: observed MAE=0.006095 validation MAE=0.016422,rank=4\n",
      "[SoftImpute] Iter 152: observed MAE=0.006088 validation MAE=0.016410,rank=4\n",
      "[SoftImpute] Iter 153: observed MAE=0.006082 validation MAE=0.016397,rank=4\n",
      "[SoftImpute] Iter 154: observed MAE=0.006075 validation MAE=0.016385,rank=4\n",
      "[SoftImpute] Iter 155: observed MAE=0.006068 validation MAE=0.016374,rank=4\n",
      "[SoftImpute] Iter 156: observed MAE=0.006062 validation MAE=0.016362,rank=4\n",
      "[SoftImpute] Iter 157: observed MAE=0.006055 validation MAE=0.016351,rank=4\n",
      "[SoftImpute] Iter 158: observed MAE=0.006049 validation MAE=0.016339,rank=4\n",
      "[SoftImpute] Iter 159: observed MAE=0.006043 validation MAE=0.016328,rank=4\n",
      "[SoftImpute] Iter 160: observed MAE=0.006037 validation MAE=0.016317,rank=4\n",
      "[SoftImpute] Iter 161: observed MAE=0.006031 validation MAE=0.016306,rank=4\n",
      "[SoftImpute] Iter 162: observed MAE=0.006025 validation MAE=0.016295,rank=4\n",
      "[SoftImpute] Iter 163: observed MAE=0.006019 validation MAE=0.016284,rank=4\n",
      "[SoftImpute] Iter 164: observed MAE=0.006014 validation MAE=0.016273,rank=4\n",
      "[SoftImpute] Iter 165: observed MAE=0.006008 validation MAE=0.016262,rank=4\n",
      "[SoftImpute] Iter 166: observed MAE=0.006002 validation MAE=0.016252,rank=4\n",
      "[SoftImpute] Iter 167: observed MAE=0.005997 validation MAE=0.016241,rank=4\n",
      "[SoftImpute] Iter 168: observed MAE=0.005991 validation MAE=0.016231,rank=4\n",
      "[SoftImpute] Iter 169: observed MAE=0.005986 validation MAE=0.016220,rank=4\n",
      "[SoftImpute] Iter 170: observed MAE=0.005981 validation MAE=0.016210,rank=4\n",
      "[SoftImpute] Iter 171: observed MAE=0.005976 validation MAE=0.016200,rank=4\n",
      "[SoftImpute] Iter 172: observed MAE=0.005971 validation MAE=0.016190,rank=4\n",
      "[SoftImpute] Iter 173: observed MAE=0.005966 validation MAE=0.016180,rank=4\n",
      "[SoftImpute] Iter 174: observed MAE=0.005961 validation MAE=0.016170,rank=4\n",
      "[SoftImpute] Iter 175: observed MAE=0.005956 validation MAE=0.016160,rank=4\n",
      "[SoftImpute] Iter 176: observed MAE=0.005951 validation MAE=0.016151,rank=4\n",
      "[SoftImpute] Iter 177: observed MAE=0.005946 validation MAE=0.016141,rank=4\n",
      "[SoftImpute] Iter 178: observed MAE=0.005941 validation MAE=0.016132,rank=4\n",
      "[SoftImpute] Iter 179: observed MAE=0.005937 validation MAE=0.016122,rank=4\n",
      "[SoftImpute] Iter 180: observed MAE=0.005932 validation MAE=0.016114,rank=4\n",
      "[SoftImpute] Iter 181: observed MAE=0.005927 validation MAE=0.016105,rank=4\n",
      "[SoftImpute] Iter 182: observed MAE=0.005923 validation MAE=0.016097,rank=4\n",
      "[SoftImpute] Iter 183: observed MAE=0.005919 validation MAE=0.016089,rank=4\n",
      "[SoftImpute] Iter 184: observed MAE=0.005914 validation MAE=0.016081,rank=4\n",
      "[SoftImpute] Iter 185: observed MAE=0.005910 validation MAE=0.016073,rank=4\n",
      "[SoftImpute] Iter 186: observed MAE=0.005906 validation MAE=0.016065,rank=4\n",
      "[SoftImpute] Iter 187: observed MAE=0.005901 validation MAE=0.016057,rank=4\n",
      "[SoftImpute] Iter 188: observed MAE=0.005897 validation MAE=0.016049,rank=4\n",
      "[SoftImpute] Iter 189: observed MAE=0.005893 validation MAE=0.016041,rank=4\n",
      "[SoftImpute] Iter 190: observed MAE=0.005889 validation MAE=0.016033,rank=4\n",
      "[SoftImpute] Iter 191: observed MAE=0.005885 validation MAE=0.016026,rank=4\n",
      "[SoftImpute] Iter 192: observed MAE=0.005881 validation MAE=0.016018,rank=4\n",
      "[SoftImpute] Iter 193: observed MAE=0.005877 validation MAE=0.016010,rank=4\n",
      "[SoftImpute] Iter 194: observed MAE=0.005873 validation MAE=0.016003,rank=4\n",
      "[SoftImpute] Iter 195: observed MAE=0.005869 validation MAE=0.015995,rank=4\n",
      "[SoftImpute] Iter 196: observed MAE=0.005866 validation MAE=0.015988,rank=4\n",
      "[SoftImpute] Iter 197: observed MAE=0.005862 validation MAE=0.015981,rank=4\n",
      "[SoftImpute] Iter 198: observed MAE=0.005858 validation MAE=0.015973,rank=4\n",
      "[SoftImpute] Iter 199: observed MAE=0.005855 validation MAE=0.015966,rank=4\n",
      "[SoftImpute] Iter 200: observed MAE=0.005851 validation MAE=0.015959,rank=4\n",
      "[SoftImpute] Iter 201: observed MAE=0.005847 validation MAE=0.015952,rank=4\n",
      "[SoftImpute] Iter 202: observed MAE=0.005844 validation MAE=0.015945,rank=4\n",
      "[SoftImpute] Iter 203: observed MAE=0.005840 validation MAE=0.015938,rank=4\n",
      "[SoftImpute] Iter 204: observed MAE=0.005837 validation MAE=0.015931,rank=4\n",
      "[SoftImpute] Iter 205: observed MAE=0.005833 validation MAE=0.015924,rank=4\n",
      "[SoftImpute] Iter 206: observed MAE=0.005830 validation MAE=0.015918,rank=4\n",
      "[SoftImpute] Iter 207: observed MAE=0.005827 validation MAE=0.015911,rank=4\n",
      "[SoftImpute] Iter 208: observed MAE=0.005823 validation MAE=0.015904,rank=4\n",
      "[SoftImpute] Iter 209: observed MAE=0.005820 validation MAE=0.015897,rank=4\n",
      "[SoftImpute] Iter 210: observed MAE=0.005817 validation MAE=0.015891,rank=4\n",
      "[SoftImpute] Iter 211: observed MAE=0.005814 validation MAE=0.015884,rank=4\n",
      "[SoftImpute] Iter 212: observed MAE=0.005810 validation MAE=0.015877,rank=4\n",
      "[SoftImpute] Iter 213: observed MAE=0.005807 validation MAE=0.015871,rank=4\n",
      "[SoftImpute] Iter 214: observed MAE=0.005804 validation MAE=0.015864,rank=4\n",
      "[SoftImpute] Iter 215: observed MAE=0.005801 validation MAE=0.015858,rank=4\n",
      "[SoftImpute] Iter 216: observed MAE=0.005798 validation MAE=0.015851,rank=4\n",
      "[SoftImpute] Iter 217: observed MAE=0.005795 validation MAE=0.015845,rank=4\n",
      "[SoftImpute] Iter 218: observed MAE=0.005792 validation MAE=0.015838,rank=4\n",
      "[SoftImpute] Iter 219: observed MAE=0.005789 validation MAE=0.015832,rank=4\n",
      "[SoftImpute] Iter 220: observed MAE=0.005786 validation MAE=0.015826,rank=4\n",
      "[SoftImpute] Iter 221: observed MAE=0.005783 validation MAE=0.015819,rank=4\n",
      "[SoftImpute] Iter 222: observed MAE=0.005780 validation MAE=0.015813,rank=4\n",
      "[SoftImpute] Iter 223: observed MAE=0.005778 validation MAE=0.015807,rank=4\n",
      "[SoftImpute] Iter 224: observed MAE=0.005775 validation MAE=0.015801,rank=4\n",
      "[SoftImpute] Iter 225: observed MAE=0.005772 validation MAE=0.015795,rank=4\n",
      "[SoftImpute] Iter 226: observed MAE=0.005769 validation MAE=0.015789,rank=4\n",
      "[SoftImpute] Iter 227: observed MAE=0.005767 validation MAE=0.015782,rank=4\n",
      "[SoftImpute] Iter 228: observed MAE=0.005764 validation MAE=0.015776,rank=4\n",
      "[SoftImpute] Iter 229: observed MAE=0.005761 validation MAE=0.015770,rank=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 230: observed MAE=0.005759 validation MAE=0.015764,rank=4\n",
      "[SoftImpute] Iter 231: observed MAE=0.005756 validation MAE=0.015758,rank=4\n",
      "[SoftImpute] Iter 232: observed MAE=0.005753 validation MAE=0.015752,rank=4\n",
      "[SoftImpute] Stopped after iteration 232 for lambda=0.039366\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 7.26032829284668\n",
      "After the matrix factor stage, training error is 0.00575, validation error is 0.01575\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.34448, val loss: 0.34765\n",
      "Main effects training epoch: 2, train loss: 0.27354, val loss: 0.27853\n",
      "Main effects training epoch: 3, train loss: 0.20823, val loss: 0.21145\n",
      "Main effects training epoch: 4, train loss: 0.16135, val loss: 0.16514\n",
      "Main effects training epoch: 5, train loss: 0.14048, val loss: 0.14025\n",
      "Main effects training epoch: 6, train loss: 0.13280, val loss: 0.13280\n",
      "Main effects training epoch: 7, train loss: 0.13041, val loss: 0.12922\n",
      "Main effects training epoch: 8, train loss: 0.12977, val loss: 0.12861\n",
      "Main effects training epoch: 9, train loss: 0.12914, val loss: 0.12821\n",
      "Main effects training epoch: 10, train loss: 0.12852, val loss: 0.12844\n",
      "Main effects training epoch: 11, train loss: 0.12742, val loss: 0.12590\n",
      "Main effects training epoch: 12, train loss: 0.12612, val loss: 0.12564\n",
      "Main effects training epoch: 13, train loss: 0.12339, val loss: 0.12252\n",
      "Main effects training epoch: 14, train loss: 0.11835, val loss: 0.11883\n",
      "Main effects training epoch: 15, train loss: 0.11578, val loss: 0.11651\n",
      "Main effects training epoch: 16, train loss: 0.11289, val loss: 0.11344\n",
      "Main effects training epoch: 17, train loss: 0.11081, val loss: 0.11186\n",
      "Main effects training epoch: 18, train loss: 0.11088, val loss: 0.11312\n",
      "Main effects training epoch: 19, train loss: 0.11027, val loss: 0.11199\n",
      "Main effects training epoch: 20, train loss: 0.11090, val loss: 0.11238\n",
      "Main effects training epoch: 21, train loss: 0.10922, val loss: 0.11096\n",
      "Main effects training epoch: 22, train loss: 0.10900, val loss: 0.11106\n",
      "Main effects training epoch: 23, train loss: 0.10645, val loss: 0.10812\n",
      "Main effects training epoch: 24, train loss: 0.10707, val loss: 0.10858\n",
      "Main effects training epoch: 25, train loss: 0.10776, val loss: 0.10768\n",
      "Main effects training epoch: 26, train loss: 0.10556, val loss: 0.10715\n",
      "Main effects training epoch: 27, train loss: 0.10595, val loss: 0.10680\n",
      "Main effects training epoch: 28, train loss: 0.10509, val loss: 0.10619\n",
      "Main effects training epoch: 29, train loss: 0.10460, val loss: 0.10568\n",
      "Main effects training epoch: 30, train loss: 0.10512, val loss: 0.10585\n",
      "Main effects training epoch: 31, train loss: 0.10437, val loss: 0.10598\n",
      "Main effects training epoch: 32, train loss: 0.10431, val loss: 0.10577\n",
      "Main effects training epoch: 33, train loss: 0.10423, val loss: 0.10545\n",
      "Main effects training epoch: 34, train loss: 0.10413, val loss: 0.10561\n",
      "Main effects training epoch: 35, train loss: 0.10424, val loss: 0.10579\n",
      "Main effects training epoch: 36, train loss: 0.10444, val loss: 0.10578\n",
      "Main effects training epoch: 37, train loss: 0.10400, val loss: 0.10575\n",
      "Main effects training epoch: 38, train loss: 0.10438, val loss: 0.10591\n",
      "Main effects training epoch: 39, train loss: 0.10416, val loss: 0.10566\n",
      "Main effects training epoch: 40, train loss: 0.10393, val loss: 0.10524\n",
      "Main effects training epoch: 41, train loss: 0.10418, val loss: 0.10579\n",
      "Main effects training epoch: 42, train loss: 0.10392, val loss: 0.10565\n",
      "Main effects training epoch: 43, train loss: 0.10430, val loss: 0.10612\n",
      "Main effects training epoch: 44, train loss: 0.10409, val loss: 0.10553\n",
      "Main effects training epoch: 45, train loss: 0.10403, val loss: 0.10539\n",
      "Main effects training epoch: 46, train loss: 0.10400, val loss: 0.10547\n",
      "Main effects training epoch: 47, train loss: 0.10472, val loss: 0.10625\n",
      "Main effects training epoch: 48, train loss: 0.10410, val loss: 0.10597\n",
      "Main effects training epoch: 49, train loss: 0.10415, val loss: 0.10512\n",
      "Main effects training epoch: 50, train loss: 0.10408, val loss: 0.10560\n",
      "Main effects training epoch: 51, train loss: 0.10392, val loss: 0.10554\n",
      "Main effects training epoch: 52, train loss: 0.10395, val loss: 0.10519\n",
      "Main effects training epoch: 53, train loss: 0.10385, val loss: 0.10520\n",
      "Main effects training epoch: 54, train loss: 0.10398, val loss: 0.10563\n",
      "Main effects training epoch: 55, train loss: 0.10416, val loss: 0.10523\n",
      "Main effects training epoch: 56, train loss: 0.10404, val loss: 0.10586\n",
      "Main effects training epoch: 57, train loss: 0.10399, val loss: 0.10571\n",
      "Main effects training epoch: 58, train loss: 0.10443, val loss: 0.10631\n",
      "Main effects training epoch: 59, train loss: 0.10393, val loss: 0.10549\n",
      "Main effects training epoch: 60, train loss: 0.10386, val loss: 0.10534\n",
      "Main effects training epoch: 61, train loss: 0.10389, val loss: 0.10538\n",
      "Main effects training epoch: 62, train loss: 0.10417, val loss: 0.10552\n",
      "Main effects training epoch: 63, train loss: 0.10396, val loss: 0.10570\n",
      "Main effects training epoch: 64, train loss: 0.10412, val loss: 0.10546\n",
      "Main effects training epoch: 65, train loss: 0.10403, val loss: 0.10549\n",
      "Main effects training epoch: 66, train loss: 0.10398, val loss: 0.10514\n",
      "Main effects training epoch: 67, train loss: 0.10385, val loss: 0.10514\n",
      "Main effects training epoch: 68, train loss: 0.10396, val loss: 0.10561\n",
      "Main effects training epoch: 69, train loss: 0.10397, val loss: 0.10523\n",
      "Main effects training epoch: 70, train loss: 0.10420, val loss: 0.10542\n",
      "Main effects training epoch: 71, train loss: 0.10473, val loss: 0.10671\n",
      "Main effects training epoch: 72, train loss: 0.10418, val loss: 0.10552\n",
      "Main effects training epoch: 73, train loss: 0.10427, val loss: 0.10590\n",
      "Main effects training epoch: 74, train loss: 0.10409, val loss: 0.10540\n",
      "Main effects training epoch: 75, train loss: 0.10401, val loss: 0.10542\n",
      "Main effects training epoch: 76, train loss: 0.10384, val loss: 0.10560\n",
      "Main effects training epoch: 77, train loss: 0.10404, val loss: 0.10542\n",
      "Main effects training epoch: 78, train loss: 0.10394, val loss: 0.10550\n",
      "Main effects training epoch: 79, train loss: 0.10410, val loss: 0.10553\n",
      "Main effects training epoch: 80, train loss: 0.10425, val loss: 0.10542\n",
      "Main effects training epoch: 81, train loss: 0.10387, val loss: 0.10530\n",
      "Main effects training epoch: 82, train loss: 0.10409, val loss: 0.10607\n",
      "Main effects training epoch: 83, train loss: 0.10386, val loss: 0.10559\n",
      "Main effects training epoch: 84, train loss: 0.10405, val loss: 0.10549\n",
      "Main effects training epoch: 85, train loss: 0.10483, val loss: 0.10617\n",
      "Main effects training epoch: 86, train loss: 0.10383, val loss: 0.10551\n",
      "Main effects training epoch: 87, train loss: 0.10415, val loss: 0.10524\n",
      "Main effects training epoch: 88, train loss: 0.10389, val loss: 0.10586\n",
      "Main effects training epoch: 89, train loss: 0.10383, val loss: 0.10548\n",
      "Main effects training epoch: 90, train loss: 0.10400, val loss: 0.10512\n",
      "Main effects training epoch: 91, train loss: 0.10387, val loss: 0.10538\n",
      "Main effects training epoch: 92, train loss: 0.10420, val loss: 0.10635\n",
      "Main effects training epoch: 93, train loss: 0.10396, val loss: 0.10529\n",
      "Main effects training epoch: 94, train loss: 0.10417, val loss: 0.10594\n",
      "Main effects training epoch: 95, train loss: 0.10387, val loss: 0.10562\n",
      "Main effects training epoch: 96, train loss: 0.10378, val loss: 0.10533\n",
      "Main effects training epoch: 97, train loss: 0.10386, val loss: 0.10536\n",
      "Main effects training epoch: 98, train loss: 0.10425, val loss: 0.10630\n",
      "Main effects training epoch: 99, train loss: 0.10426, val loss: 0.10552\n",
      "Main effects training epoch: 100, train loss: 0.10419, val loss: 0.10632\n",
      "Main effects training epoch: 101, train loss: 0.10395, val loss: 0.10553\n",
      "Main effects training epoch: 102, train loss: 0.10397, val loss: 0.10584\n",
      "Main effects training epoch: 103, train loss: 0.10408, val loss: 0.10574\n",
      "Main effects training epoch: 104, train loss: 0.10395, val loss: 0.10576\n",
      "Main effects training epoch: 105, train loss: 0.10380, val loss: 0.10555\n",
      "Main effects training epoch: 106, train loss: 0.10382, val loss: 0.10590\n",
      "Main effects training epoch: 107, train loss: 0.10417, val loss: 0.10600\n",
      "Main effects training epoch: 108, train loss: 0.10410, val loss: 0.10567\n",
      "Main effects training epoch: 109, train loss: 0.10373, val loss: 0.10542\n",
      "Main effects training epoch: 110, train loss: 0.10377, val loss: 0.10525\n",
      "Main effects training epoch: 111, train loss: 0.10405, val loss: 0.10577\n",
      "Main effects training epoch: 112, train loss: 0.10409, val loss: 0.10590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 113, train loss: 0.10474, val loss: 0.10615\n",
      "Main effects training epoch: 114, train loss: 0.10406, val loss: 0.10594\n",
      "Main effects training epoch: 115, train loss: 0.10415, val loss: 0.10553\n",
      "Main effects training epoch: 116, train loss: 0.10437, val loss: 0.10680\n",
      "Main effects training epoch: 117, train loss: 0.10399, val loss: 0.10532\n",
      "Main effects training epoch: 118, train loss: 0.10393, val loss: 0.10583\n",
      "Main effects training epoch: 119, train loss: 0.10410, val loss: 0.10601\n",
      "Main effects training epoch: 120, train loss: 0.10375, val loss: 0.10578\n",
      "Main effects training epoch: 121, train loss: 0.10382, val loss: 0.10529\n",
      "Main effects training epoch: 122, train loss: 0.10391, val loss: 0.10552\n",
      "Main effects training epoch: 123, train loss: 0.10397, val loss: 0.10615\n",
      "Main effects training epoch: 124, train loss: 0.10422, val loss: 0.10572\n",
      "Main effects training epoch: 125, train loss: 0.10414, val loss: 0.10601\n",
      "Main effects training epoch: 126, train loss: 0.10402, val loss: 0.10605\n",
      "Main effects training epoch: 127, train loss: 0.10402, val loss: 0.10531\n",
      "Main effects training epoch: 128, train loss: 0.10395, val loss: 0.10615\n",
      "Main effects training epoch: 129, train loss: 0.10410, val loss: 0.10643\n",
      "Main effects training epoch: 130, train loss: 0.10401, val loss: 0.10526\n",
      "Main effects training epoch: 131, train loss: 0.10380, val loss: 0.10577\n",
      "Main effects training epoch: 132, train loss: 0.10404, val loss: 0.10554\n",
      "Main effects training epoch: 133, train loss: 0.10425, val loss: 0.10648\n",
      "Main effects training epoch: 134, train loss: 0.10454, val loss: 0.10616\n",
      "Main effects training epoch: 135, train loss: 0.10424, val loss: 0.10653\n",
      "Main effects training epoch: 136, train loss: 0.10426, val loss: 0.10565\n",
      "Main effects training epoch: 137, train loss: 0.10444, val loss: 0.10639\n",
      "Main effects training epoch: 138, train loss: 0.10433, val loss: 0.10622\n",
      "Main effects training epoch: 139, train loss: 0.10384, val loss: 0.10565\n",
      "Main effects training epoch: 140, train loss: 0.10392, val loss: 0.10539\n",
      "Main effects training epoch: 141, train loss: 0.10420, val loss: 0.10593\n",
      "Main effects training epoch: 142, train loss: 0.10404, val loss: 0.10617\n",
      "Main effects training epoch: 143, train loss: 0.10420, val loss: 0.10567\n",
      "Main effects training epoch: 144, train loss: 0.10383, val loss: 0.10601\n",
      "Main effects training epoch: 145, train loss: 0.10370, val loss: 0.10530\n",
      "Main effects training epoch: 146, train loss: 0.10381, val loss: 0.10596\n",
      "Main effects training epoch: 147, train loss: 0.10380, val loss: 0.10575\n",
      "Main effects training epoch: 148, train loss: 0.10377, val loss: 0.10571\n",
      "Main effects training epoch: 149, train loss: 0.10383, val loss: 0.10566\n",
      "Main effects training epoch: 150, train loss: 0.10393, val loss: 0.10604\n",
      "Early stop at epoch 150, with validation loss: 0.10604\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.10405, val loss: 0.10498\n",
      "Main effects tuning epoch: 2, train loss: 0.10423, val loss: 0.10552\n",
      "Main effects tuning epoch: 3, train loss: 0.10411, val loss: 0.10505\n",
      "Main effects tuning epoch: 4, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 5, train loss: 0.10382, val loss: 0.10498\n",
      "Main effects tuning epoch: 6, train loss: 0.10425, val loss: 0.10582\n",
      "Main effects tuning epoch: 7, train loss: 0.10408, val loss: 0.10530\n",
      "Main effects tuning epoch: 8, train loss: 0.10397, val loss: 0.10527\n",
      "Main effects tuning epoch: 9, train loss: 0.10401, val loss: 0.10507\n",
      "Main effects tuning epoch: 10, train loss: 0.10407, val loss: 0.10539\n",
      "Main effects tuning epoch: 11, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 12, train loss: 0.10393, val loss: 0.10556\n",
      "Main effects tuning epoch: 13, train loss: 0.10415, val loss: 0.10510\n",
      "Main effects tuning epoch: 14, train loss: 0.10397, val loss: 0.10570\n",
      "Main effects tuning epoch: 15, train loss: 0.10386, val loss: 0.10516\n",
      "Main effects tuning epoch: 16, train loss: 0.10395, val loss: 0.10562\n",
      "Main effects tuning epoch: 17, train loss: 0.10395, val loss: 0.10539\n",
      "Main effects tuning epoch: 18, train loss: 0.10422, val loss: 0.10568\n",
      "Main effects tuning epoch: 19, train loss: 0.10433, val loss: 0.10517\n",
      "Main effects tuning epoch: 20, train loss: 0.10384, val loss: 0.10568\n",
      "Main effects tuning epoch: 21, train loss: 0.10400, val loss: 0.10541\n",
      "Main effects tuning epoch: 22, train loss: 0.10393, val loss: 0.10526\n",
      "Main effects tuning epoch: 23, train loss: 0.10434, val loss: 0.10582\n",
      "Main effects tuning epoch: 24, train loss: 0.10406, val loss: 0.10540\n",
      "Main effects tuning epoch: 25, train loss: 0.10410, val loss: 0.10570\n",
      "Main effects tuning epoch: 26, train loss: 0.10405, val loss: 0.10569\n",
      "Main effects tuning epoch: 27, train loss: 0.10406, val loss: 0.10556\n",
      "Main effects tuning epoch: 28, train loss: 0.10394, val loss: 0.10542\n",
      "Main effects tuning epoch: 29, train loss: 0.10383, val loss: 0.10542\n",
      "Main effects tuning epoch: 30, train loss: 0.10385, val loss: 0.10548\n",
      "Main effects tuning epoch: 31, train loss: 0.10395, val loss: 0.10516\n",
      "Main effects tuning epoch: 32, train loss: 0.10391, val loss: 0.10584\n",
      "Main effects tuning epoch: 33, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 34, train loss: 0.10391, val loss: 0.10561\n",
      "Main effects tuning epoch: 35, train loss: 0.10424, val loss: 0.10531\n",
      "Main effects tuning epoch: 36, train loss: 0.10395, val loss: 0.10554\n",
      "Main effects tuning epoch: 37, train loss: 0.10400, val loss: 0.10600\n",
      "Main effects tuning epoch: 38, train loss: 0.10408, val loss: 0.10530\n",
      "Main effects tuning epoch: 39, train loss: 0.10394, val loss: 0.10565\n",
      "Main effects tuning epoch: 40, train loss: 0.10385, val loss: 0.10569\n",
      "Main effects tuning epoch: 41, train loss: 0.10390, val loss: 0.10539\n",
      "Main effects tuning epoch: 42, train loss: 0.10400, val loss: 0.10587\n",
      "Main effects tuning epoch: 43, train loss: 0.10386, val loss: 0.10547\n",
      "Main effects tuning epoch: 44, train loss: 0.10403, val loss: 0.10597\n",
      "Main effects tuning epoch: 45, train loss: 0.10384, val loss: 0.10541\n",
      "Main effects tuning epoch: 46, train loss: 0.10398, val loss: 0.10582\n",
      "Main effects tuning epoch: 47, train loss: 0.10375, val loss: 0.10527\n",
      "Main effects tuning epoch: 48, train loss: 0.10380, val loss: 0.10549\n",
      "Main effects tuning epoch: 49, train loss: 0.10396, val loss: 0.10566\n",
      "Main effects tuning epoch: 50, train loss: 0.10407, val loss: 0.10549\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.09376, val loss: 0.09232\n",
      "Interaction training epoch: 2, train loss: 0.20140, val loss: 0.20111\n",
      "Interaction training epoch: 3, train loss: 0.06617, val loss: 0.06860\n",
      "Interaction training epoch: 4, train loss: 0.05173, val loss: 0.05197\n",
      "Interaction training epoch: 5, train loss: 0.06497, val loss: 0.06595\n",
      "Interaction training epoch: 6, train loss: 0.04386, val loss: 0.04449\n",
      "Interaction training epoch: 7, train loss: 0.04606, val loss: 0.04529\n",
      "Interaction training epoch: 8, train loss: 0.04492, val loss: 0.04489\n",
      "Interaction training epoch: 9, train loss: 0.04997, val loss: 0.04868\n",
      "Interaction training epoch: 10, train loss: 0.04002, val loss: 0.04133\n",
      "Interaction training epoch: 11, train loss: 0.04269, val loss: 0.04306\n",
      "Interaction training epoch: 12, train loss: 0.04100, val loss: 0.03997\n",
      "Interaction training epoch: 13, train loss: 0.04366, val loss: 0.04360\n",
      "Interaction training epoch: 14, train loss: 0.04928, val loss: 0.05084\n",
      "Interaction training epoch: 15, train loss: 0.04532, val loss: 0.04488\n",
      "Interaction training epoch: 16, train loss: 0.04530, val loss: 0.04524\n",
      "Interaction training epoch: 17, train loss: 0.03784, val loss: 0.03763\n",
      "Interaction training epoch: 18, train loss: 0.04939, val loss: 0.05023\n",
      "Interaction training epoch: 19, train loss: 0.03608, val loss: 0.03597\n",
      "Interaction training epoch: 20, train loss: 0.04344, val loss: 0.04367\n",
      "Interaction training epoch: 21, train loss: 0.04002, val loss: 0.03932\n",
      "Interaction training epoch: 22, train loss: 0.03977, val loss: 0.03977\n",
      "Interaction training epoch: 23, train loss: 0.03777, val loss: 0.03709\n",
      "Interaction training epoch: 24, train loss: 0.03927, val loss: 0.03924\n",
      "Interaction training epoch: 25, train loss: 0.04164, val loss: 0.04112\n",
      "Interaction training epoch: 26, train loss: 0.04429, val loss: 0.04389\n",
      "Interaction training epoch: 27, train loss: 0.04417, val loss: 0.04416\n",
      "Interaction training epoch: 28, train loss: 0.04241, val loss: 0.04112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 29, train loss: 0.03548, val loss: 0.03672\n",
      "Interaction training epoch: 30, train loss: 0.04420, val loss: 0.04353\n",
      "Interaction training epoch: 31, train loss: 0.04020, val loss: 0.04004\n",
      "Interaction training epoch: 32, train loss: 0.03422, val loss: 0.03451\n",
      "Interaction training epoch: 33, train loss: 0.03378, val loss: 0.03407\n",
      "Interaction training epoch: 34, train loss: 0.03925, val loss: 0.03880\n",
      "Interaction training epoch: 35, train loss: 0.03752, val loss: 0.03636\n",
      "Interaction training epoch: 36, train loss: 0.03582, val loss: 0.03564\n",
      "Interaction training epoch: 37, train loss: 0.03069, val loss: 0.03087\n",
      "Interaction training epoch: 38, train loss: 0.03729, val loss: 0.03652\n",
      "Interaction training epoch: 39, train loss: 0.03772, val loss: 0.03772\n",
      "Interaction training epoch: 40, train loss: 0.04332, val loss: 0.04201\n",
      "Interaction training epoch: 41, train loss: 0.03409, val loss: 0.03435\n",
      "Interaction training epoch: 42, train loss: 0.03501, val loss: 0.03419\n",
      "Interaction training epoch: 43, train loss: 0.03630, val loss: 0.03573\n",
      "Interaction training epoch: 44, train loss: 0.03447, val loss: 0.03420\n",
      "Interaction training epoch: 45, train loss: 0.03622, val loss: 0.03541\n",
      "Interaction training epoch: 46, train loss: 0.02991, val loss: 0.03002\n",
      "Interaction training epoch: 47, train loss: 0.02880, val loss: 0.02856\n",
      "Interaction training epoch: 48, train loss: 0.03548, val loss: 0.03469\n",
      "Interaction training epoch: 49, train loss: 0.03399, val loss: 0.03256\n",
      "Interaction training epoch: 50, train loss: 0.03225, val loss: 0.03266\n",
      "Interaction training epoch: 51, train loss: 0.03286, val loss: 0.03176\n",
      "Interaction training epoch: 52, train loss: 0.03084, val loss: 0.03012\n",
      "Interaction training epoch: 53, train loss: 0.03059, val loss: 0.02998\n",
      "Interaction training epoch: 54, train loss: 0.02966, val loss: 0.02932\n",
      "Interaction training epoch: 55, train loss: 0.04122, val loss: 0.04119\n",
      "Interaction training epoch: 56, train loss: 0.03164, val loss: 0.03059\n",
      "Interaction training epoch: 57, train loss: 0.03483, val loss: 0.03469\n",
      "Interaction training epoch: 58, train loss: 0.03320, val loss: 0.03294\n",
      "Interaction training epoch: 59, train loss: 0.02999, val loss: 0.02930\n",
      "Interaction training epoch: 60, train loss: 0.03540, val loss: 0.03496\n",
      "Interaction training epoch: 61, train loss: 0.03142, val loss: 0.02962\n",
      "Interaction training epoch: 62, train loss: 0.03146, val loss: 0.03164\n",
      "Interaction training epoch: 63, train loss: 0.03481, val loss: 0.03421\n",
      "Interaction training epoch: 64, train loss: 0.02946, val loss: 0.02881\n",
      "Interaction training epoch: 65, train loss: 0.03539, val loss: 0.03511\n",
      "Interaction training epoch: 66, train loss: 0.03797, val loss: 0.03805\n",
      "Interaction training epoch: 67, train loss: 0.03669, val loss: 0.03627\n",
      "Interaction training epoch: 68, train loss: 0.04844, val loss: 0.04881\n",
      "Interaction training epoch: 69, train loss: 0.02947, val loss: 0.02941\n",
      "Interaction training epoch: 70, train loss: 0.03805, val loss: 0.03672\n",
      "Interaction training epoch: 71, train loss: 0.02990, val loss: 0.02970\n",
      "Interaction training epoch: 72, train loss: 0.03926, val loss: 0.03952\n",
      "Interaction training epoch: 73, train loss: 0.03713, val loss: 0.03732\n",
      "Interaction training epoch: 74, train loss: 0.04469, val loss: 0.04392\n",
      "Interaction training epoch: 75, train loss: 0.03518, val loss: 0.03491\n",
      "Interaction training epoch: 76, train loss: 0.03954, val loss: 0.03893\n",
      "Interaction training epoch: 77, train loss: 0.04294, val loss: 0.04248\n",
      "Interaction training epoch: 78, train loss: 0.03122, val loss: 0.03170\n",
      "Interaction training epoch: 79, train loss: 0.03577, val loss: 0.03538\n",
      "Interaction training epoch: 80, train loss: 0.04727, val loss: 0.04664\n",
      "Interaction training epoch: 81, train loss: 0.03478, val loss: 0.03412\n",
      "Interaction training epoch: 82, train loss: 0.03736, val loss: 0.03700\n",
      "Interaction training epoch: 83, train loss: 0.03716, val loss: 0.03665\n",
      "Interaction training epoch: 84, train loss: 0.03675, val loss: 0.03575\n",
      "Interaction training epoch: 85, train loss: 0.03815, val loss: 0.03817\n",
      "Interaction training epoch: 86, train loss: 0.03960, val loss: 0.03903\n",
      "Interaction training epoch: 87, train loss: 0.03498, val loss: 0.03402\n",
      "Interaction training epoch: 88, train loss: 0.03630, val loss: 0.03583\n",
      "Interaction training epoch: 89, train loss: 0.03454, val loss: 0.03517\n",
      "Interaction training epoch: 90, train loss: 0.07231, val loss: 0.07223\n",
      "Interaction training epoch: 91, train loss: 0.04800, val loss: 0.04802\n",
      "Interaction training epoch: 92, train loss: 0.05224, val loss: 0.05162\n",
      "Interaction training epoch: 93, train loss: 0.02794, val loss: 0.02804\n",
      "Interaction training epoch: 94, train loss: 0.03585, val loss: 0.03546\n",
      "Interaction training epoch: 95, train loss: 0.04149, val loss: 0.04158\n",
      "Interaction training epoch: 96, train loss: 0.04943, val loss: 0.04909\n",
      "Interaction training epoch: 97, train loss: 0.05722, val loss: 0.05690\n",
      "Interaction training epoch: 98, train loss: 0.04854, val loss: 0.04831\n",
      "Interaction training epoch: 99, train loss: 0.02719, val loss: 0.02716\n",
      "Interaction training epoch: 100, train loss: 0.03689, val loss: 0.03722\n",
      "Interaction training epoch: 101, train loss: 0.04609, val loss: 0.04601\n",
      "Interaction training epoch: 102, train loss: 0.03250, val loss: 0.03231\n",
      "Interaction training epoch: 103, train loss: 0.03006, val loss: 0.02936\n",
      "Interaction training epoch: 104, train loss: 0.03967, val loss: 0.03937\n",
      "Interaction training epoch: 105, train loss: 0.03096, val loss: 0.03100\n",
      "Interaction training epoch: 106, train loss: 0.05099, val loss: 0.05044\n",
      "Interaction training epoch: 107, train loss: 0.03358, val loss: 0.03389\n",
      "Interaction training epoch: 108, train loss: 0.04273, val loss: 0.04236\n",
      "Interaction training epoch: 109, train loss: 0.07259, val loss: 0.07140\n",
      "Interaction training epoch: 110, train loss: 0.03442, val loss: 0.03433\n",
      "Interaction training epoch: 111, train loss: 0.02947, val loss: 0.02893\n",
      "Interaction training epoch: 112, train loss: 0.03413, val loss: 0.03396\n",
      "Interaction training epoch: 113, train loss: 0.03441, val loss: 0.03428\n",
      "Interaction training epoch: 114, train loss: 0.05762, val loss: 0.05645\n",
      "Interaction training epoch: 115, train loss: 0.02938, val loss: 0.02918\n",
      "Interaction training epoch: 116, train loss: 0.04805, val loss: 0.04824\n",
      "Interaction training epoch: 117, train loss: 0.03927, val loss: 0.03835\n",
      "Interaction training epoch: 118, train loss: 0.03949, val loss: 0.03920\n",
      "Interaction training epoch: 119, train loss: 0.03835, val loss: 0.03852\n",
      "Interaction training epoch: 120, train loss: 0.07402, val loss: 0.07267\n",
      "Interaction training epoch: 121, train loss: 0.02786, val loss: 0.02794\n",
      "Interaction training epoch: 122, train loss: 0.02966, val loss: 0.02840\n",
      "Interaction training epoch: 123, train loss: 0.03146, val loss: 0.03080\n",
      "Interaction training epoch: 124, train loss: 0.03135, val loss: 0.03095\n",
      "Interaction training epoch: 125, train loss: 0.02893, val loss: 0.02832\n",
      "Interaction training epoch: 126, train loss: 0.06644, val loss: 0.06554\n",
      "Interaction training epoch: 127, train loss: 0.02439, val loss: 0.02405\n",
      "Interaction training epoch: 128, train loss: 0.02442, val loss: 0.02392\n",
      "Interaction training epoch: 129, train loss: 0.06718, val loss: 0.06662\n",
      "Interaction training epoch: 130, train loss: 0.06873, val loss: 0.06752\n",
      "Interaction training epoch: 131, train loss: 0.02836, val loss: 0.02795\n",
      "Interaction training epoch: 132, train loss: 0.02537, val loss: 0.02462\n",
      "Interaction training epoch: 133, train loss: 0.05578, val loss: 0.05497\n",
      "Interaction training epoch: 134, train loss: 0.02756, val loss: 0.02628\n",
      "Interaction training epoch: 135, train loss: 0.03851, val loss: 0.03792\n",
      "Interaction training epoch: 136, train loss: 0.02792, val loss: 0.02763\n",
      "Interaction training epoch: 137, train loss: 0.02882, val loss: 0.02822\n",
      "Interaction training epoch: 138, train loss: 0.04268, val loss: 0.04230\n",
      "Interaction training epoch: 139, train loss: 0.02999, val loss: 0.03028\n",
      "Interaction training epoch: 140, train loss: 0.02653, val loss: 0.02598\n",
      "Interaction training epoch: 141, train loss: 0.02864, val loss: 0.02740\n",
      "Interaction training epoch: 142, train loss: 0.04031, val loss: 0.03930\n",
      "Interaction training epoch: 143, train loss: 0.08930, val loss: 0.08825\n",
      "Interaction training epoch: 144, train loss: 0.03278, val loss: 0.03201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 145, train loss: 0.07595, val loss: 0.07444\n",
      "Interaction training epoch: 146, train loss: 0.04088, val loss: 0.04007\n",
      "Interaction training epoch: 147, train loss: 0.03063, val loss: 0.02967\n",
      "Interaction training epoch: 148, train loss: 0.08210, val loss: 0.08116\n",
      "Interaction training epoch: 149, train loss: 0.02600, val loss: 0.02595\n",
      "Interaction training epoch: 150, train loss: 0.04451, val loss: 0.04266\n",
      "Interaction training epoch: 151, train loss: 0.05071, val loss: 0.04953\n",
      "Interaction training epoch: 152, train loss: 0.03689, val loss: 0.03657\n",
      "Interaction training epoch: 153, train loss: 0.05108, val loss: 0.05017\n",
      "Interaction training epoch: 154, train loss: 0.03125, val loss: 0.03020\n",
      "Interaction training epoch: 155, train loss: 0.05262, val loss: 0.05160\n",
      "Interaction training epoch: 156, train loss: 0.03189, val loss: 0.03099\n",
      "Interaction training epoch: 157, train loss: 0.03302, val loss: 0.03192\n",
      "Interaction training epoch: 158, train loss: 0.03919, val loss: 0.03867\n",
      "Interaction training epoch: 159, train loss: 0.06294, val loss: 0.06164\n",
      "Interaction training epoch: 160, train loss: 0.02504, val loss: 0.02398\n",
      "Interaction training epoch: 161, train loss: 0.02726, val loss: 0.02622\n",
      "Interaction training epoch: 162, train loss: 0.02696, val loss: 0.02663\n",
      "Interaction training epoch: 163, train loss: 0.04443, val loss: 0.04306\n",
      "Interaction training epoch: 164, train loss: 0.08229, val loss: 0.08080\n",
      "Interaction training epoch: 165, train loss: 0.02497, val loss: 0.02433\n",
      "Interaction training epoch: 166, train loss: 0.02405, val loss: 0.02321\n",
      "Interaction training epoch: 167, train loss: 0.03154, val loss: 0.03087\n",
      "Interaction training epoch: 168, train loss: 0.03079, val loss: 0.02948\n",
      "Interaction training epoch: 169, train loss: 0.02562, val loss: 0.02466\n",
      "Interaction training epoch: 170, train loss: 0.03950, val loss: 0.03811\n",
      "Interaction training epoch: 171, train loss: 0.05102, val loss: 0.05005\n",
      "Interaction training epoch: 172, train loss: 0.07393, val loss: 0.07199\n",
      "Interaction training epoch: 173, train loss: 0.03775, val loss: 0.03659\n",
      "Interaction training epoch: 174, train loss: 0.03283, val loss: 0.03172\n",
      "Interaction training epoch: 175, train loss: 0.02729, val loss: 0.02663\n",
      "Interaction training epoch: 176, train loss: 0.04249, val loss: 0.04090\n",
      "Interaction training epoch: 177, train loss: 0.11371, val loss: 0.11248\n",
      "Interaction training epoch: 178, train loss: 0.05008, val loss: 0.04851\n",
      "Interaction training epoch: 179, train loss: 0.05070, val loss: 0.04903\n",
      "Interaction training epoch: 180, train loss: 0.05192, val loss: 0.05082\n",
      "Interaction training epoch: 181, train loss: 0.03499, val loss: 0.03401\n",
      "Interaction training epoch: 182, train loss: 0.06009, val loss: 0.05850\n",
      "Interaction training epoch: 183, train loss: 0.03517, val loss: 0.03374\n",
      "Interaction training epoch: 184, train loss: 0.04107, val loss: 0.04010\n",
      "Interaction training epoch: 185, train loss: 0.06361, val loss: 0.06162\n",
      "Interaction training epoch: 186, train loss: 0.06349, val loss: 0.06178\n",
      "Interaction training epoch: 187, train loss: 0.05374, val loss: 0.05196\n",
      "Interaction training epoch: 188, train loss: 0.05958, val loss: 0.05804\n",
      "Interaction training epoch: 189, train loss: 0.02604, val loss: 0.02519\n",
      "Interaction training epoch: 190, train loss: 0.04369, val loss: 0.04302\n",
      "Interaction training epoch: 191, train loss: 0.04175, val loss: 0.04116\n",
      "Interaction training epoch: 192, train loss: 0.06782, val loss: 0.06645\n",
      "Interaction training epoch: 193, train loss: 0.06145, val loss: 0.06034\n",
      "Interaction training epoch: 194, train loss: 0.04303, val loss: 0.04151\n",
      "Interaction training epoch: 195, train loss: 0.03319, val loss: 0.03229\n",
      "Interaction training epoch: 196, train loss: 0.03143, val loss: 0.02941\n",
      "Interaction training epoch: 197, train loss: 0.03413, val loss: 0.03311\n",
      "Interaction training epoch: 198, train loss: 0.03045, val loss: 0.02901\n",
      "Interaction training epoch: 199, train loss: 0.04253, val loss: 0.04138\n",
      "Interaction training epoch: 200, train loss: 0.04032, val loss: 0.03845\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########4 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.02853, val loss: 0.02785\n",
      "Interaction tuning epoch: 2, train loss: 0.02401, val loss: 0.02299\n",
      "Interaction tuning epoch: 3, train loss: 0.04478, val loss: 0.04265\n",
      "Interaction tuning epoch: 4, train loss: 0.06395, val loss: 0.06292\n",
      "Interaction tuning epoch: 5, train loss: 0.02356, val loss: 0.02286\n",
      "Interaction tuning epoch: 6, train loss: 0.06369, val loss: 0.06228\n",
      "Interaction tuning epoch: 7, train loss: 0.04154, val loss: 0.04077\n",
      "Interaction tuning epoch: 8, train loss: 0.08354, val loss: 0.08227\n",
      "Interaction tuning epoch: 9, train loss: 0.07421, val loss: 0.07270\n",
      "Interaction tuning epoch: 10, train loss: 0.04015, val loss: 0.03931\n",
      "Interaction tuning epoch: 11, train loss: 0.02840, val loss: 0.02746\n",
      "Interaction tuning epoch: 12, train loss: 0.06172, val loss: 0.06021\n",
      "Interaction tuning epoch: 13, train loss: 0.03776, val loss: 0.03699\n",
      "Interaction tuning epoch: 14, train loss: 0.07388, val loss: 0.07213\n",
      "Interaction tuning epoch: 15, train loss: 0.02628, val loss: 0.02557\n",
      "Interaction tuning epoch: 16, train loss: 0.02860, val loss: 0.02813\n",
      "Interaction tuning epoch: 17, train loss: 0.04973, val loss: 0.04802\n",
      "Interaction tuning epoch: 18, train loss: 0.02869, val loss: 0.02741\n",
      "Interaction tuning epoch: 19, train loss: 0.03914, val loss: 0.03730\n",
      "Interaction tuning epoch: 20, train loss: 0.03596, val loss: 0.03526\n",
      "Interaction tuning epoch: 21, train loss: 0.04924, val loss: 0.04797\n",
      "Interaction tuning epoch: 22, train loss: 0.08235, val loss: 0.08148\n",
      "Interaction tuning epoch: 23, train loss: 0.08430, val loss: 0.08186\n",
      "Interaction tuning epoch: 24, train loss: 0.07758, val loss: 0.07581\n",
      "Interaction tuning epoch: 25, train loss: 0.02468, val loss: 0.02425\n",
      "Interaction tuning epoch: 26, train loss: 0.02729, val loss: 0.02674\n",
      "Interaction tuning epoch: 27, train loss: 0.02677, val loss: 0.02542\n",
      "Interaction tuning epoch: 28, train loss: 0.03303, val loss: 0.03233\n",
      "Interaction tuning epoch: 29, train loss: 0.07628, val loss: 0.07409\n",
      "Interaction tuning epoch: 30, train loss: 0.05354, val loss: 0.05248\n",
      "Interaction tuning epoch: 31, train loss: 0.02819, val loss: 0.02642\n",
      "Interaction tuning epoch: 32, train loss: 0.02994, val loss: 0.02905\n",
      "Interaction tuning epoch: 33, train loss: 0.03903, val loss: 0.03809\n",
      "Interaction tuning epoch: 34, train loss: 0.03406, val loss: 0.03261\n",
      "Interaction tuning epoch: 35, train loss: 0.02886, val loss: 0.02797\n",
      "Interaction tuning epoch: 36, train loss: 0.04728, val loss: 0.04591\n",
      "Interaction tuning epoch: 37, train loss: 0.02745, val loss: 0.02653\n",
      "Interaction tuning epoch: 38, train loss: 0.09613, val loss: 0.09462\n",
      "Interaction tuning epoch: 39, train loss: 0.06430, val loss: 0.06326\n",
      "Interaction tuning epoch: 40, train loss: 0.03564, val loss: 0.03467\n",
      "Interaction tuning epoch: 41, train loss: 0.05842, val loss: 0.05720\n",
      "Interaction tuning epoch: 42, train loss: 0.02719, val loss: 0.02601\n",
      "Interaction tuning epoch: 43, train loss: 0.03689, val loss: 0.03533\n",
      "Interaction tuning epoch: 44, train loss: 0.02563, val loss: 0.02452\n",
      "Interaction tuning epoch: 45, train loss: 0.11515, val loss: 0.11336\n",
      "Interaction tuning epoch: 46, train loss: 0.05383, val loss: 0.05274\n",
      "Interaction tuning epoch: 47, train loss: 0.03183, val loss: 0.02986\n",
      "Interaction tuning epoch: 48, train loss: 0.04755, val loss: 0.04566\n",
      "Interaction tuning epoch: 49, train loss: 0.03504, val loss: 0.03437\n",
      "Interaction tuning epoch: 50, train loss: 0.03646, val loss: 0.03600\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 32.71964883804321\n",
      "After the gam stage, training error is 0.03646 , validation error is 0.03600\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 1.968308\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.028403 validation MAE=0.034932,rank=5\n",
      "[SoftImpute] Iter 2: observed MAE=0.026450 validation MAE=0.034201,rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 3: observed MAE=0.024705 validation MAE=0.033414,rank=5\n",
      "[SoftImpute] Iter 4: observed MAE=0.023146 validation MAE=0.032640,rank=5\n",
      "[SoftImpute] Iter 5: observed MAE=0.021753 validation MAE=0.031900,rank=5\n",
      "[SoftImpute] Iter 6: observed MAE=0.020496 validation MAE=0.031197,rank=5\n",
      "[SoftImpute] Iter 7: observed MAE=0.019376 validation MAE=0.030548,rank=5\n",
      "[SoftImpute] Iter 8: observed MAE=0.018381 validation MAE=0.029977,rank=5\n",
      "[SoftImpute] Iter 9: observed MAE=0.017494 validation MAE=0.029449,rank=5\n",
      "[SoftImpute] Iter 10: observed MAE=0.016702 validation MAE=0.028963,rank=5\n",
      "[SoftImpute] Iter 11: observed MAE=0.015990 validation MAE=0.028511,rank=5\n",
      "[SoftImpute] Iter 12: observed MAE=0.015350 validation MAE=0.028097,rank=5\n",
      "[SoftImpute] Iter 13: observed MAE=0.014772 validation MAE=0.027727,rank=5\n",
      "[SoftImpute] Iter 14: observed MAE=0.014250 validation MAE=0.027385,rank=5\n",
      "[SoftImpute] Iter 15: observed MAE=0.013776 validation MAE=0.027076,rank=5\n",
      "[SoftImpute] Iter 16: observed MAE=0.013345 validation MAE=0.026798,rank=5\n",
      "[SoftImpute] Iter 17: observed MAE=0.012952 validation MAE=0.026540,rank=5\n",
      "[SoftImpute] Iter 18: observed MAE=0.012592 validation MAE=0.026301,rank=5\n",
      "[SoftImpute] Iter 19: observed MAE=0.012264 validation MAE=0.026080,rank=5\n",
      "[SoftImpute] Iter 20: observed MAE=0.011962 validation MAE=0.025872,rank=5\n",
      "[SoftImpute] Iter 21: observed MAE=0.011682 validation MAE=0.025681,rank=5\n",
      "[SoftImpute] Iter 22: observed MAE=0.011424 validation MAE=0.025503,rank=5\n",
      "[SoftImpute] Iter 23: observed MAE=0.011185 validation MAE=0.025337,rank=5\n",
      "[SoftImpute] Iter 24: observed MAE=0.010963 validation MAE=0.025184,rank=5\n",
      "[SoftImpute] Iter 25: observed MAE=0.010758 validation MAE=0.025042,rank=5\n",
      "[SoftImpute] Iter 26: observed MAE=0.010566 validation MAE=0.024909,rank=5\n",
      "[SoftImpute] Iter 27: observed MAE=0.010389 validation MAE=0.024785,rank=5\n",
      "[SoftImpute] Iter 28: observed MAE=0.010224 validation MAE=0.024666,rank=5\n",
      "[SoftImpute] Iter 29: observed MAE=0.010070 validation MAE=0.024553,rank=5\n",
      "[SoftImpute] Iter 30: observed MAE=0.009925 validation MAE=0.024447,rank=5\n",
      "[SoftImpute] Iter 31: observed MAE=0.009789 validation MAE=0.024347,rank=5\n",
      "[SoftImpute] Iter 32: observed MAE=0.009660 validation MAE=0.024253,rank=5\n",
      "[SoftImpute] Iter 33: observed MAE=0.009539 validation MAE=0.024165,rank=5\n",
      "[SoftImpute] Iter 34: observed MAE=0.009425 validation MAE=0.024080,rank=5\n",
      "[SoftImpute] Iter 35: observed MAE=0.009316 validation MAE=0.023997,rank=5\n",
      "[SoftImpute] Iter 36: observed MAE=0.009213 validation MAE=0.023916,rank=5\n",
      "[SoftImpute] Iter 37: observed MAE=0.009115 validation MAE=0.023836,rank=5\n",
      "[SoftImpute] Iter 38: observed MAE=0.009022 validation MAE=0.023759,rank=5\n",
      "[SoftImpute] Iter 39: observed MAE=0.008933 validation MAE=0.023685,rank=5\n",
      "[SoftImpute] Iter 40: observed MAE=0.008849 validation MAE=0.023614,rank=5\n",
      "[SoftImpute] Iter 41: observed MAE=0.008768 validation MAE=0.023546,rank=5\n",
      "[SoftImpute] Iter 42: observed MAE=0.008690 validation MAE=0.023479,rank=5\n",
      "[SoftImpute] Iter 43: observed MAE=0.008616 validation MAE=0.023415,rank=5\n",
      "[SoftImpute] Iter 44: observed MAE=0.008544 validation MAE=0.023353,rank=5\n",
      "[SoftImpute] Iter 45: observed MAE=0.008475 validation MAE=0.023293,rank=5\n",
      "[SoftImpute] Iter 46: observed MAE=0.008409 validation MAE=0.023233,rank=5\n",
      "[SoftImpute] Iter 47: observed MAE=0.008345 validation MAE=0.023177,rank=5\n",
      "[SoftImpute] Iter 48: observed MAE=0.008284 validation MAE=0.023121,rank=5\n",
      "[SoftImpute] Iter 49: observed MAE=0.008224 validation MAE=0.023066,rank=5\n",
      "[SoftImpute] Iter 50: observed MAE=0.008167 validation MAE=0.023012,rank=5\n",
      "[SoftImpute] Iter 51: observed MAE=0.008112 validation MAE=0.022959,rank=5\n",
      "[SoftImpute] Iter 52: observed MAE=0.008059 validation MAE=0.022907,rank=5\n",
      "[SoftImpute] Iter 53: observed MAE=0.008007 validation MAE=0.022857,rank=5\n",
      "[SoftImpute] Iter 54: observed MAE=0.007958 validation MAE=0.022807,rank=5\n",
      "[SoftImpute] Iter 55: observed MAE=0.007910 validation MAE=0.022759,rank=5\n",
      "[SoftImpute] Iter 56: observed MAE=0.007864 validation MAE=0.022711,rank=5\n",
      "[SoftImpute] Iter 57: observed MAE=0.007819 validation MAE=0.022663,rank=5\n",
      "[SoftImpute] Iter 58: observed MAE=0.007775 validation MAE=0.022616,rank=5\n",
      "[SoftImpute] Iter 59: observed MAE=0.007733 validation MAE=0.022570,rank=5\n",
      "[SoftImpute] Iter 60: observed MAE=0.007691 validation MAE=0.022524,rank=5\n",
      "[SoftImpute] Iter 61: observed MAE=0.007651 validation MAE=0.022480,rank=5\n",
      "[SoftImpute] Iter 62: observed MAE=0.007612 validation MAE=0.022436,rank=5\n",
      "[SoftImpute] Iter 63: observed MAE=0.007574 validation MAE=0.022393,rank=5\n",
      "[SoftImpute] Iter 64: observed MAE=0.007537 validation MAE=0.022350,rank=5\n",
      "[SoftImpute] Iter 65: observed MAE=0.007501 validation MAE=0.022308,rank=5\n",
      "[SoftImpute] Iter 66: observed MAE=0.007465 validation MAE=0.022266,rank=5\n",
      "[SoftImpute] Iter 67: observed MAE=0.007431 validation MAE=0.022225,rank=5\n",
      "[SoftImpute] Iter 68: observed MAE=0.007397 validation MAE=0.022185,rank=5\n",
      "[SoftImpute] Iter 69: observed MAE=0.007364 validation MAE=0.022144,rank=5\n",
      "[SoftImpute] Iter 70: observed MAE=0.007333 validation MAE=0.022104,rank=5\n",
      "[SoftImpute] Iter 71: observed MAE=0.007301 validation MAE=0.022065,rank=5\n",
      "[SoftImpute] Iter 72: observed MAE=0.007271 validation MAE=0.022026,rank=5\n",
      "[SoftImpute] Iter 73: observed MAE=0.007241 validation MAE=0.021988,rank=5\n",
      "[SoftImpute] Iter 74: observed MAE=0.007212 validation MAE=0.021951,rank=5\n",
      "[SoftImpute] Iter 75: observed MAE=0.007184 validation MAE=0.021914,rank=5\n",
      "[SoftImpute] Iter 76: observed MAE=0.007156 validation MAE=0.021877,rank=5\n",
      "[SoftImpute] Iter 77: observed MAE=0.007129 validation MAE=0.021841,rank=5\n",
      "[SoftImpute] Iter 78: observed MAE=0.007103 validation MAE=0.021805,rank=5\n",
      "[SoftImpute] Iter 79: observed MAE=0.007077 validation MAE=0.021769,rank=5\n",
      "[SoftImpute] Iter 80: observed MAE=0.007051 validation MAE=0.021733,rank=5\n",
      "[SoftImpute] Iter 81: observed MAE=0.007027 validation MAE=0.021698,rank=5\n",
      "[SoftImpute] Iter 82: observed MAE=0.007002 validation MAE=0.021663,rank=5\n",
      "[SoftImpute] Iter 83: observed MAE=0.006978 validation MAE=0.021629,rank=5\n",
      "[SoftImpute] Iter 84: observed MAE=0.006955 validation MAE=0.021595,rank=5\n",
      "[SoftImpute] Iter 85: observed MAE=0.006932 validation MAE=0.021561,rank=5\n",
      "[SoftImpute] Iter 86: observed MAE=0.006909 validation MAE=0.021528,rank=5\n",
      "[SoftImpute] Iter 87: observed MAE=0.006887 validation MAE=0.021494,rank=5\n",
      "[SoftImpute] Iter 88: observed MAE=0.006865 validation MAE=0.021461,rank=5\n",
      "[SoftImpute] Iter 89: observed MAE=0.006843 validation MAE=0.021427,rank=5\n",
      "[SoftImpute] Iter 90: observed MAE=0.006822 validation MAE=0.021394,rank=5\n",
      "[SoftImpute] Iter 91: observed MAE=0.006802 validation MAE=0.021361,rank=5\n",
      "[SoftImpute] Iter 92: observed MAE=0.006781 validation MAE=0.021329,rank=5\n",
      "[SoftImpute] Iter 93: observed MAE=0.006761 validation MAE=0.021296,rank=5\n",
      "[SoftImpute] Iter 94: observed MAE=0.006742 validation MAE=0.021264,rank=5\n",
      "[SoftImpute] Iter 95: observed MAE=0.006722 validation MAE=0.021232,rank=5\n",
      "[SoftImpute] Iter 96: observed MAE=0.006703 validation MAE=0.021200,rank=5\n",
      "[SoftImpute] Iter 97: observed MAE=0.006685 validation MAE=0.021168,rank=5\n",
      "[SoftImpute] Iter 98: observed MAE=0.006666 validation MAE=0.021137,rank=5\n",
      "[SoftImpute] Iter 99: observed MAE=0.006648 validation MAE=0.021106,rank=5\n",
      "[SoftImpute] Iter 100: observed MAE=0.006630 validation MAE=0.021076,rank=5\n",
      "[SoftImpute] Iter 101: observed MAE=0.006613 validation MAE=0.021045,rank=5\n",
      "[SoftImpute] Iter 102: observed MAE=0.006596 validation MAE=0.021014,rank=5\n",
      "[SoftImpute] Iter 103: observed MAE=0.006579 validation MAE=0.020984,rank=5\n",
      "[SoftImpute] Iter 104: observed MAE=0.006562 validation MAE=0.020954,rank=5\n",
      "[SoftImpute] Iter 105: observed MAE=0.006546 validation MAE=0.020924,rank=5\n",
      "[SoftImpute] Iter 106: observed MAE=0.006529 validation MAE=0.020894,rank=5\n",
      "[SoftImpute] Iter 107: observed MAE=0.006513 validation MAE=0.020865,rank=5\n",
      "[SoftImpute] Iter 108: observed MAE=0.006497 validation MAE=0.020836,rank=5\n",
      "[SoftImpute] Iter 109: observed MAE=0.006482 validation MAE=0.020806,rank=5\n",
      "[SoftImpute] Iter 110: observed MAE=0.006466 validation MAE=0.020777,rank=5\n",
      "[SoftImpute] Iter 111: observed MAE=0.006451 validation MAE=0.020749,rank=5\n",
      "[SoftImpute] Iter 112: observed MAE=0.006436 validation MAE=0.020720,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 113: observed MAE=0.006422 validation MAE=0.020692,rank=5\n",
      "[SoftImpute] Iter 114: observed MAE=0.006407 validation MAE=0.020665,rank=5\n",
      "[SoftImpute] Iter 115: observed MAE=0.006393 validation MAE=0.020637,rank=5\n",
      "[SoftImpute] Iter 116: observed MAE=0.006379 validation MAE=0.020610,rank=5\n",
      "[SoftImpute] Iter 117: observed MAE=0.006365 validation MAE=0.020582,rank=5\n",
      "[SoftImpute] Iter 118: observed MAE=0.006352 validation MAE=0.020555,rank=5\n",
      "[SoftImpute] Iter 119: observed MAE=0.006338 validation MAE=0.020528,rank=5\n",
      "[SoftImpute] Iter 120: observed MAE=0.006325 validation MAE=0.020502,rank=5\n",
      "[SoftImpute] Iter 121: observed MAE=0.006312 validation MAE=0.020475,rank=5\n",
      "[SoftImpute] Iter 122: observed MAE=0.006299 validation MAE=0.020449,rank=5\n",
      "[SoftImpute] Iter 123: observed MAE=0.006287 validation MAE=0.020423,rank=5\n",
      "[SoftImpute] Iter 124: observed MAE=0.006274 validation MAE=0.020397,rank=5\n",
      "[SoftImpute] Iter 125: observed MAE=0.006262 validation MAE=0.020371,rank=5\n",
      "[SoftImpute] Iter 126: observed MAE=0.006250 validation MAE=0.020346,rank=5\n",
      "[SoftImpute] Iter 127: observed MAE=0.006238 validation MAE=0.020320,rank=5\n",
      "[SoftImpute] Iter 128: observed MAE=0.006227 validation MAE=0.020296,rank=5\n",
      "[SoftImpute] Iter 129: observed MAE=0.006215 validation MAE=0.020271,rank=5\n",
      "[SoftImpute] Iter 130: observed MAE=0.006204 validation MAE=0.020247,rank=5\n",
      "[SoftImpute] Iter 131: observed MAE=0.006193 validation MAE=0.020222,rank=5\n",
      "[SoftImpute] Iter 132: observed MAE=0.006182 validation MAE=0.020198,rank=5\n",
      "[SoftImpute] Iter 133: observed MAE=0.006171 validation MAE=0.020175,rank=5\n",
      "[SoftImpute] Iter 134: observed MAE=0.006160 validation MAE=0.020151,rank=5\n",
      "[SoftImpute] Iter 135: observed MAE=0.006150 validation MAE=0.020127,rank=5\n",
      "[SoftImpute] Iter 136: observed MAE=0.006139 validation MAE=0.020104,rank=5\n",
      "[SoftImpute] Iter 137: observed MAE=0.006129 validation MAE=0.020080,rank=5\n",
      "[SoftImpute] Iter 138: observed MAE=0.006119 validation MAE=0.020057,rank=5\n",
      "[SoftImpute] Iter 139: observed MAE=0.006109 validation MAE=0.020034,rank=5\n",
      "[SoftImpute] Iter 140: observed MAE=0.006099 validation MAE=0.020010,rank=5\n",
      "[SoftImpute] Iter 141: observed MAE=0.006089 validation MAE=0.019988,rank=5\n",
      "[SoftImpute] Iter 142: observed MAE=0.006080 validation MAE=0.019965,rank=5\n",
      "[SoftImpute] Iter 143: observed MAE=0.006071 validation MAE=0.019942,rank=5\n",
      "[SoftImpute] Iter 144: observed MAE=0.006061 validation MAE=0.019920,rank=5\n",
      "[SoftImpute] Iter 145: observed MAE=0.006052 validation MAE=0.019897,rank=5\n",
      "[SoftImpute] Iter 146: observed MAE=0.006043 validation MAE=0.019875,rank=5\n",
      "[SoftImpute] Iter 147: observed MAE=0.006034 validation MAE=0.019853,rank=5\n",
      "[SoftImpute] Iter 148: observed MAE=0.006026 validation MAE=0.019831,rank=5\n",
      "[SoftImpute] Iter 149: observed MAE=0.006017 validation MAE=0.019809,rank=5\n",
      "[SoftImpute] Iter 150: observed MAE=0.006009 validation MAE=0.019788,rank=5\n",
      "[SoftImpute] Iter 151: observed MAE=0.006000 validation MAE=0.019766,rank=5\n",
      "[SoftImpute] Iter 152: observed MAE=0.005992 validation MAE=0.019745,rank=5\n",
      "[SoftImpute] Iter 153: observed MAE=0.005984 validation MAE=0.019724,rank=5\n",
      "[SoftImpute] Iter 154: observed MAE=0.005975 validation MAE=0.019703,rank=5\n",
      "[SoftImpute] Iter 155: observed MAE=0.005967 validation MAE=0.019682,rank=5\n",
      "[SoftImpute] Iter 156: observed MAE=0.005960 validation MAE=0.019661,rank=5\n",
      "[SoftImpute] Iter 157: observed MAE=0.005952 validation MAE=0.019640,rank=5\n",
      "[SoftImpute] Iter 158: observed MAE=0.005944 validation MAE=0.019620,rank=5\n",
      "[SoftImpute] Iter 159: observed MAE=0.005937 validation MAE=0.019599,rank=5\n",
      "[SoftImpute] Iter 160: observed MAE=0.005929 validation MAE=0.019579,rank=5\n",
      "[SoftImpute] Iter 161: observed MAE=0.005922 validation MAE=0.019559,rank=5\n",
      "[SoftImpute] Iter 162: observed MAE=0.005914 validation MAE=0.019539,rank=5\n",
      "[SoftImpute] Iter 163: observed MAE=0.005907 validation MAE=0.019519,rank=5\n",
      "[SoftImpute] Iter 164: observed MAE=0.005900 validation MAE=0.019499,rank=5\n",
      "[SoftImpute] Iter 165: observed MAE=0.005893 validation MAE=0.019479,rank=5\n",
      "[SoftImpute] Iter 166: observed MAE=0.005886 validation MAE=0.019459,rank=5\n",
      "[SoftImpute] Iter 167: observed MAE=0.005879 validation MAE=0.019440,rank=5\n",
      "[SoftImpute] Iter 168: observed MAE=0.005872 validation MAE=0.019422,rank=5\n",
      "[SoftImpute] Iter 169: observed MAE=0.005866 validation MAE=0.019403,rank=5\n",
      "[SoftImpute] Iter 170: observed MAE=0.005859 validation MAE=0.019385,rank=5\n",
      "[SoftImpute] Iter 171: observed MAE=0.005852 validation MAE=0.019366,rank=5\n",
      "[SoftImpute] Iter 172: observed MAE=0.005846 validation MAE=0.019348,rank=5\n",
      "[SoftImpute] Iter 173: observed MAE=0.005839 validation MAE=0.019330,rank=5\n",
      "[SoftImpute] Iter 174: observed MAE=0.005833 validation MAE=0.019312,rank=5\n",
      "[SoftImpute] Iter 175: observed MAE=0.005827 validation MAE=0.019294,rank=5\n",
      "[SoftImpute] Iter 176: observed MAE=0.005820 validation MAE=0.019276,rank=5\n",
      "[SoftImpute] Iter 177: observed MAE=0.005814 validation MAE=0.019258,rank=5\n",
      "[SoftImpute] Iter 178: observed MAE=0.005808 validation MAE=0.019241,rank=5\n",
      "[SoftImpute] Iter 179: observed MAE=0.005802 validation MAE=0.019223,rank=5\n",
      "[SoftImpute] Iter 180: observed MAE=0.005796 validation MAE=0.019206,rank=5\n",
      "[SoftImpute] Iter 181: observed MAE=0.005791 validation MAE=0.019190,rank=5\n",
      "[SoftImpute] Iter 182: observed MAE=0.005785 validation MAE=0.019173,rank=5\n",
      "[SoftImpute] Iter 183: observed MAE=0.005779 validation MAE=0.019157,rank=5\n",
      "[SoftImpute] Iter 184: observed MAE=0.005773 validation MAE=0.019141,rank=5\n",
      "[SoftImpute] Iter 185: observed MAE=0.005768 validation MAE=0.019124,rank=5\n",
      "[SoftImpute] Iter 186: observed MAE=0.005762 validation MAE=0.019108,rank=5\n",
      "[SoftImpute] Iter 187: observed MAE=0.005757 validation MAE=0.019092,rank=5\n",
      "[SoftImpute] Iter 188: observed MAE=0.005752 validation MAE=0.019076,rank=5\n",
      "[SoftImpute] Iter 189: observed MAE=0.005746 validation MAE=0.019060,rank=5\n",
      "[SoftImpute] Iter 190: observed MAE=0.005741 validation MAE=0.019044,rank=5\n",
      "[SoftImpute] Iter 191: observed MAE=0.005736 validation MAE=0.019028,rank=5\n",
      "[SoftImpute] Iter 192: observed MAE=0.005731 validation MAE=0.019012,rank=5\n",
      "[SoftImpute] Iter 193: observed MAE=0.005726 validation MAE=0.018996,rank=5\n",
      "[SoftImpute] Iter 194: observed MAE=0.005721 validation MAE=0.018980,rank=5\n",
      "[SoftImpute] Iter 195: observed MAE=0.005716 validation MAE=0.018965,rank=5\n",
      "[SoftImpute] Iter 196: observed MAE=0.005711 validation MAE=0.018949,rank=5\n",
      "[SoftImpute] Iter 197: observed MAE=0.005706 validation MAE=0.018933,rank=5\n",
      "[SoftImpute] Iter 198: observed MAE=0.005701 validation MAE=0.018918,rank=5\n",
      "[SoftImpute] Iter 199: observed MAE=0.005696 validation MAE=0.018902,rank=5\n",
      "[SoftImpute] Iter 200: observed MAE=0.005692 validation MAE=0.018887,rank=5\n",
      "[SoftImpute] Iter 201: observed MAE=0.005687 validation MAE=0.018872,rank=5\n",
      "[SoftImpute] Iter 202: observed MAE=0.005682 validation MAE=0.018856,rank=5\n",
      "[SoftImpute] Iter 203: observed MAE=0.005678 validation MAE=0.018841,rank=5\n",
      "[SoftImpute] Iter 204: observed MAE=0.005673 validation MAE=0.018826,rank=5\n",
      "[SoftImpute] Iter 205: observed MAE=0.005669 validation MAE=0.018811,rank=5\n",
      "[SoftImpute] Iter 206: observed MAE=0.005664 validation MAE=0.018796,rank=5\n",
      "[SoftImpute] Iter 207: observed MAE=0.005660 validation MAE=0.018781,rank=5\n",
      "[SoftImpute] Iter 208: observed MAE=0.005656 validation MAE=0.018766,rank=5\n",
      "[SoftImpute] Iter 209: observed MAE=0.005651 validation MAE=0.018751,rank=5\n",
      "[SoftImpute] Iter 210: observed MAE=0.005647 validation MAE=0.018736,rank=5\n",
      "[SoftImpute] Iter 211: observed MAE=0.005643 validation MAE=0.018721,rank=5\n",
      "[SoftImpute] Iter 212: observed MAE=0.005638 validation MAE=0.018707,rank=5\n",
      "[SoftImpute] Iter 213: observed MAE=0.005634 validation MAE=0.018692,rank=5\n",
      "[SoftImpute] Iter 214: observed MAE=0.005630 validation MAE=0.018677,rank=5\n",
      "[SoftImpute] Iter 215: observed MAE=0.005626 validation MAE=0.018663,rank=5\n",
      "[SoftImpute] Iter 216: observed MAE=0.005622 validation MAE=0.018648,rank=5\n",
      "[SoftImpute] Iter 217: observed MAE=0.005618 validation MAE=0.018634,rank=5\n",
      "[SoftImpute] Iter 218: observed MAE=0.005614 validation MAE=0.018620,rank=5\n",
      "[SoftImpute] Iter 219: observed MAE=0.005610 validation MAE=0.018605,rank=5\n",
      "[SoftImpute] Iter 220: observed MAE=0.005606 validation MAE=0.018591,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 221: observed MAE=0.005602 validation MAE=0.018577,rank=5\n",
      "[SoftImpute] Iter 222: observed MAE=0.005598 validation MAE=0.018563,rank=5\n",
      "[SoftImpute] Iter 223: observed MAE=0.005594 validation MAE=0.018549,rank=5\n",
      "[SoftImpute] Iter 224: observed MAE=0.005591 validation MAE=0.018535,rank=5\n",
      "[SoftImpute] Iter 225: observed MAE=0.005587 validation MAE=0.018521,rank=5\n",
      "[SoftImpute] Iter 226: observed MAE=0.005583 validation MAE=0.018507,rank=5\n",
      "[SoftImpute] Iter 227: observed MAE=0.005580 validation MAE=0.018493,rank=5\n",
      "[SoftImpute] Iter 228: observed MAE=0.005576 validation MAE=0.018480,rank=5\n",
      "[SoftImpute] Iter 229: observed MAE=0.005572 validation MAE=0.018466,rank=5\n",
      "[SoftImpute] Iter 230: observed MAE=0.005569 validation MAE=0.018453,rank=5\n",
      "[SoftImpute] Iter 231: observed MAE=0.005565 validation MAE=0.018439,rank=5\n",
      "[SoftImpute] Iter 232: observed MAE=0.005562 validation MAE=0.018426,rank=5\n",
      "[SoftImpute] Iter 233: observed MAE=0.005558 validation MAE=0.018413,rank=5\n",
      "[SoftImpute] Iter 234: observed MAE=0.005555 validation MAE=0.018399,rank=5\n",
      "[SoftImpute] Iter 235: observed MAE=0.005551 validation MAE=0.018386,rank=5\n",
      "[SoftImpute] Iter 236: observed MAE=0.005548 validation MAE=0.018373,rank=5\n",
      "[SoftImpute] Iter 237: observed MAE=0.005544 validation MAE=0.018360,rank=5\n",
      "[SoftImpute] Iter 238: observed MAE=0.005541 validation MAE=0.018347,rank=5\n",
      "[SoftImpute] Iter 239: observed MAE=0.005538 validation MAE=0.018334,rank=5\n",
      "[SoftImpute] Iter 240: observed MAE=0.005534 validation MAE=0.018321,rank=5\n",
      "[SoftImpute] Iter 241: observed MAE=0.005531 validation MAE=0.018308,rank=5\n",
      "[SoftImpute] Iter 242: observed MAE=0.005528 validation MAE=0.018295,rank=5\n",
      "[SoftImpute] Iter 243: observed MAE=0.005525 validation MAE=0.018282,rank=5\n",
      "[SoftImpute] Iter 244: observed MAE=0.005521 validation MAE=0.018270,rank=5\n",
      "[SoftImpute] Iter 245: observed MAE=0.005518 validation MAE=0.018257,rank=5\n",
      "[SoftImpute] Iter 246: observed MAE=0.005515 validation MAE=0.018244,rank=5\n",
      "[SoftImpute] Iter 247: observed MAE=0.005512 validation MAE=0.018232,rank=5\n",
      "[SoftImpute] Iter 248: observed MAE=0.005509 validation MAE=0.018219,rank=5\n",
      "[SoftImpute] Iter 249: observed MAE=0.005506 validation MAE=0.018207,rank=5\n",
      "[SoftImpute] Iter 250: observed MAE=0.005502 validation MAE=0.018195,rank=5\n",
      "[SoftImpute] Iter 251: observed MAE=0.005499 validation MAE=0.018182,rank=5\n",
      "[SoftImpute] Iter 252: observed MAE=0.005496 validation MAE=0.018170,rank=5\n",
      "[SoftImpute] Iter 253: observed MAE=0.005493 validation MAE=0.018158,rank=5\n",
      "[SoftImpute] Iter 254: observed MAE=0.005490 validation MAE=0.018146,rank=5\n",
      "[SoftImpute] Iter 255: observed MAE=0.005487 validation MAE=0.018134,rank=5\n",
      "[SoftImpute] Iter 256: observed MAE=0.005484 validation MAE=0.018122,rank=5\n",
      "[SoftImpute] Iter 257: observed MAE=0.005481 validation MAE=0.018110,rank=5\n",
      "[SoftImpute] Iter 258: observed MAE=0.005479 validation MAE=0.018098,rank=5\n",
      "[SoftImpute] Iter 259: observed MAE=0.005476 validation MAE=0.018086,rank=5\n",
      "[SoftImpute] Iter 260: observed MAE=0.005473 validation MAE=0.018074,rank=5\n",
      "[SoftImpute] Iter 261: observed MAE=0.005470 validation MAE=0.018062,rank=5\n",
      "[SoftImpute] Iter 262: observed MAE=0.005467 validation MAE=0.018050,rank=5\n",
      "[SoftImpute] Iter 263: observed MAE=0.005464 validation MAE=0.018038,rank=5\n",
      "[SoftImpute] Iter 264: observed MAE=0.005461 validation MAE=0.018026,rank=5\n",
      "[SoftImpute] Iter 265: observed MAE=0.005459 validation MAE=0.018015,rank=5\n",
      "[SoftImpute] Iter 266: observed MAE=0.005456 validation MAE=0.018003,rank=5\n",
      "[SoftImpute] Iter 267: observed MAE=0.005453 validation MAE=0.017991,rank=5\n",
      "[SoftImpute] Iter 268: observed MAE=0.005450 validation MAE=0.017979,rank=5\n",
      "[SoftImpute] Iter 269: observed MAE=0.005447 validation MAE=0.017968,rank=5\n",
      "[SoftImpute] Iter 270: observed MAE=0.005445 validation MAE=0.017956,rank=5\n",
      "[SoftImpute] Iter 271: observed MAE=0.005442 validation MAE=0.017944,rank=5\n",
      "[SoftImpute] Iter 272: observed MAE=0.005439 validation MAE=0.017933,rank=5\n",
      "[SoftImpute] Iter 273: observed MAE=0.005436 validation MAE=0.017921,rank=5\n",
      "[SoftImpute] Iter 274: observed MAE=0.005434 validation MAE=0.017910,rank=5\n",
      "[SoftImpute] Iter 275: observed MAE=0.005431 validation MAE=0.017899,rank=5\n",
      "[SoftImpute] Iter 276: observed MAE=0.005428 validation MAE=0.017887,rank=5\n",
      "[SoftImpute] Iter 277: observed MAE=0.005426 validation MAE=0.017876,rank=5\n",
      "[SoftImpute] Iter 278: observed MAE=0.005423 validation MAE=0.017865,rank=5\n",
      "[SoftImpute] Iter 279: observed MAE=0.005420 validation MAE=0.017854,rank=5\n",
      "[SoftImpute] Iter 280: observed MAE=0.005418 validation MAE=0.017842,rank=5\n",
      "[SoftImpute] Iter 281: observed MAE=0.005415 validation MAE=0.017831,rank=5\n",
      "[SoftImpute] Iter 282: observed MAE=0.005413 validation MAE=0.017820,rank=5\n",
      "[SoftImpute] Iter 283: observed MAE=0.005410 validation MAE=0.017809,rank=5\n",
      "[SoftImpute] Iter 284: observed MAE=0.005407 validation MAE=0.017798,rank=5\n",
      "[SoftImpute] Iter 285: observed MAE=0.005405 validation MAE=0.017787,rank=5\n",
      "[SoftImpute] Iter 286: observed MAE=0.005402 validation MAE=0.017776,rank=5\n",
      "[SoftImpute] Iter 287: observed MAE=0.005400 validation MAE=0.017765,rank=5\n",
      "[SoftImpute] Iter 288: observed MAE=0.005397 validation MAE=0.017754,rank=5\n",
      "[SoftImpute] Iter 289: observed MAE=0.005395 validation MAE=0.017743,rank=5\n",
      "[SoftImpute] Iter 290: observed MAE=0.005392 validation MAE=0.017732,rank=5\n",
      "[SoftImpute] Iter 291: observed MAE=0.005390 validation MAE=0.017721,rank=5\n",
      "[SoftImpute] Iter 292: observed MAE=0.005387 validation MAE=0.017711,rank=5\n",
      "[SoftImpute] Iter 293: observed MAE=0.005385 validation MAE=0.017700,rank=5\n",
      "[SoftImpute] Iter 294: observed MAE=0.005383 validation MAE=0.017690,rank=5\n",
      "[SoftImpute] Iter 295: observed MAE=0.005380 validation MAE=0.017680,rank=5\n",
      "[SoftImpute] Iter 296: observed MAE=0.005378 validation MAE=0.017670,rank=5\n",
      "[SoftImpute] Iter 297: observed MAE=0.005375 validation MAE=0.017660,rank=5\n",
      "[SoftImpute] Iter 298: observed MAE=0.005373 validation MAE=0.017650,rank=5\n",
      "[SoftImpute] Iter 299: observed MAE=0.005371 validation MAE=0.017640,rank=5\n",
      "[SoftImpute] Iter 300: observed MAE=0.005368 validation MAE=0.017630,rank=5\n",
      "[SoftImpute] Iter 301: observed MAE=0.005366 validation MAE=0.017620,rank=5\n",
      "[SoftImpute] Iter 302: observed MAE=0.005363 validation MAE=0.017610,rank=5\n",
      "[SoftImpute] Iter 303: observed MAE=0.005361 validation MAE=0.017600,rank=5\n",
      "[SoftImpute] Iter 304: observed MAE=0.005359 validation MAE=0.017590,rank=5\n",
      "[SoftImpute] Iter 305: observed MAE=0.005356 validation MAE=0.017580,rank=5\n",
      "[SoftImpute] Iter 306: observed MAE=0.005354 validation MAE=0.017570,rank=5\n",
      "[SoftImpute] Iter 307: observed MAE=0.005352 validation MAE=0.017560,rank=5\n",
      "[SoftImpute] Iter 308: observed MAE=0.005349 validation MAE=0.017550,rank=5\n",
      "[SoftImpute] Iter 309: observed MAE=0.005347 validation MAE=0.017540,rank=5\n",
      "[SoftImpute] Iter 310: observed MAE=0.005345 validation MAE=0.017530,rank=5\n",
      "[SoftImpute] Iter 311: observed MAE=0.005342 validation MAE=0.017520,rank=5\n",
      "[SoftImpute] Iter 312: observed MAE=0.005340 validation MAE=0.017511,rank=5\n",
      "[SoftImpute] Iter 313: observed MAE=0.005338 validation MAE=0.017501,rank=5\n",
      "[SoftImpute] Iter 314: observed MAE=0.005335 validation MAE=0.017491,rank=5\n",
      "[SoftImpute] Iter 315: observed MAE=0.005333 validation MAE=0.017482,rank=5\n",
      "[SoftImpute] Iter 316: observed MAE=0.005331 validation MAE=0.017472,rank=5\n",
      "[SoftImpute] Iter 317: observed MAE=0.005329 validation MAE=0.017463,rank=5\n",
      "[SoftImpute] Iter 318: observed MAE=0.005326 validation MAE=0.017454,rank=5\n",
      "[SoftImpute] Iter 319: observed MAE=0.005324 validation MAE=0.017445,rank=5\n",
      "[SoftImpute] Iter 320: observed MAE=0.005322 validation MAE=0.017436,rank=5\n",
      "[SoftImpute] Iter 321: observed MAE=0.005320 validation MAE=0.017426,rank=5\n",
      "[SoftImpute] Iter 322: observed MAE=0.005317 validation MAE=0.017417,rank=5\n",
      "[SoftImpute] Iter 323: observed MAE=0.005315 validation MAE=0.017408,rank=5\n",
      "[SoftImpute] Iter 324: observed MAE=0.005313 validation MAE=0.017399,rank=5\n",
      "[SoftImpute] Iter 325: observed MAE=0.005311 validation MAE=0.017391,rank=5\n",
      "[SoftImpute] Iter 326: observed MAE=0.005309 validation MAE=0.017382,rank=5\n",
      "[SoftImpute] Iter 327: observed MAE=0.005306 validation MAE=0.017373,rank=5\n",
      "[SoftImpute] Iter 328: observed MAE=0.005304 validation MAE=0.017364,rank=5\n",
      "[SoftImpute] Iter 329: observed MAE=0.005302 validation MAE=0.017355,rank=5\n",
      "[SoftImpute] Iter 330: observed MAE=0.005300 validation MAE=0.017346,rank=5\n",
      "[SoftImpute] Iter 331: observed MAE=0.005298 validation MAE=0.017338,rank=5\n",
      "[SoftImpute] Iter 332: observed MAE=0.005296 validation MAE=0.017329,rank=5\n",
      "[SoftImpute] Iter 333: observed MAE=0.005293 validation MAE=0.017320,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 334: observed MAE=0.005291 validation MAE=0.017311,rank=5\n",
      "[SoftImpute] Iter 335: observed MAE=0.005289 validation MAE=0.017303,rank=5\n",
      "[SoftImpute] Iter 336: observed MAE=0.005287 validation MAE=0.017294,rank=5\n",
      "[SoftImpute] Iter 337: observed MAE=0.005285 validation MAE=0.017285,rank=5\n",
      "[SoftImpute] Iter 338: observed MAE=0.005283 validation MAE=0.017277,rank=5\n",
      "[SoftImpute] Iter 339: observed MAE=0.005281 validation MAE=0.017268,rank=5\n",
      "[SoftImpute] Iter 340: observed MAE=0.005279 validation MAE=0.017260,rank=5\n",
      "[SoftImpute] Iter 341: observed MAE=0.005277 validation MAE=0.017252,rank=5\n",
      "[SoftImpute] Iter 342: observed MAE=0.005275 validation MAE=0.017243,rank=5\n",
      "[SoftImpute] Iter 343: observed MAE=0.005272 validation MAE=0.017235,rank=5\n",
      "[SoftImpute] Iter 344: observed MAE=0.005270 validation MAE=0.017226,rank=5\n",
      "[SoftImpute] Iter 345: observed MAE=0.005268 validation MAE=0.017218,rank=5\n",
      "[SoftImpute] Iter 346: observed MAE=0.005266 validation MAE=0.017210,rank=5\n",
      "[SoftImpute] Iter 347: observed MAE=0.005264 validation MAE=0.017201,rank=5\n",
      "[SoftImpute] Iter 348: observed MAE=0.005262 validation MAE=0.017193,rank=5\n",
      "[SoftImpute] Iter 349: observed MAE=0.005260 validation MAE=0.017185,rank=5\n",
      "[SoftImpute] Iter 350: observed MAE=0.005258 validation MAE=0.017177,rank=5\n",
      "[SoftImpute] Iter 351: observed MAE=0.005256 validation MAE=0.017169,rank=5\n",
      "[SoftImpute] Iter 352: observed MAE=0.005254 validation MAE=0.017161,rank=5\n",
      "[SoftImpute] Iter 353: observed MAE=0.005252 validation MAE=0.017154,rank=5\n",
      "[SoftImpute] Iter 354: observed MAE=0.005250 validation MAE=0.017146,rank=5\n",
      "[SoftImpute] Iter 355: observed MAE=0.005248 validation MAE=0.017138,rank=5\n",
      "[SoftImpute] Iter 356: observed MAE=0.005246 validation MAE=0.017130,rank=5\n",
      "[SoftImpute] Iter 357: observed MAE=0.005244 validation MAE=0.017122,rank=5\n",
      "[SoftImpute] Iter 358: observed MAE=0.005242 validation MAE=0.017114,rank=5\n",
      "[SoftImpute] Iter 359: observed MAE=0.005240 validation MAE=0.017107,rank=5\n",
      "[SoftImpute] Iter 360: observed MAE=0.005238 validation MAE=0.017099,rank=5\n",
      "[SoftImpute] Iter 361: observed MAE=0.005236 validation MAE=0.017091,rank=5\n",
      "[SoftImpute] Iter 362: observed MAE=0.005234 validation MAE=0.017084,rank=5\n",
      "[SoftImpute] Iter 363: observed MAE=0.005232 validation MAE=0.017076,rank=5\n",
      "[SoftImpute] Iter 364: observed MAE=0.005231 validation MAE=0.017068,rank=5\n",
      "[SoftImpute] Iter 365: observed MAE=0.005229 validation MAE=0.017061,rank=5\n",
      "[SoftImpute] Iter 366: observed MAE=0.005227 validation MAE=0.017053,rank=5\n",
      "[SoftImpute] Iter 367: observed MAE=0.005225 validation MAE=0.017046,rank=5\n",
      "[SoftImpute] Iter 368: observed MAE=0.005223 validation MAE=0.017040,rank=5\n",
      "[SoftImpute] Iter 369: observed MAE=0.005221 validation MAE=0.017033,rank=5\n",
      "[SoftImpute] Iter 370: observed MAE=0.005219 validation MAE=0.017027,rank=5\n",
      "[SoftImpute] Iter 371: observed MAE=0.005217 validation MAE=0.017021,rank=5\n",
      "[SoftImpute] Iter 372: observed MAE=0.005215 validation MAE=0.017015,rank=5\n",
      "[SoftImpute] Iter 373: observed MAE=0.005213 validation MAE=0.017009,rank=5\n",
      "[SoftImpute] Iter 374: observed MAE=0.005211 validation MAE=0.017002,rank=5\n",
      "[SoftImpute] Iter 375: observed MAE=0.005209 validation MAE=0.016996,rank=5\n",
      "[SoftImpute] Iter 376: observed MAE=0.005207 validation MAE=0.016990,rank=5\n",
      "[SoftImpute] Iter 377: observed MAE=0.005206 validation MAE=0.016984,rank=5\n",
      "[SoftImpute] Iter 378: observed MAE=0.005204 validation MAE=0.016978,rank=5\n",
      "[SoftImpute] Iter 379: observed MAE=0.005202 validation MAE=0.016972,rank=5\n",
      "[SoftImpute] Iter 380: observed MAE=0.005200 validation MAE=0.016966,rank=5\n",
      "[SoftImpute] Iter 381: observed MAE=0.005198 validation MAE=0.016961,rank=5\n",
      "[SoftImpute] Iter 382: observed MAE=0.005196 validation MAE=0.016955,rank=5\n",
      "[SoftImpute] Iter 383: observed MAE=0.005194 validation MAE=0.016949,rank=5\n",
      "[SoftImpute] Iter 384: observed MAE=0.005192 validation MAE=0.016943,rank=5\n",
      "[SoftImpute] Iter 385: observed MAE=0.005191 validation MAE=0.016938,rank=5\n",
      "[SoftImpute] Iter 386: observed MAE=0.005189 validation MAE=0.016932,rank=5\n",
      "[SoftImpute] Iter 387: observed MAE=0.005187 validation MAE=0.016926,rank=5\n",
      "[SoftImpute] Iter 388: observed MAE=0.005185 validation MAE=0.016921,rank=5\n",
      "[SoftImpute] Iter 389: observed MAE=0.005183 validation MAE=0.016915,rank=5\n",
      "[SoftImpute] Iter 390: observed MAE=0.005181 validation MAE=0.016909,rank=5\n",
      "[SoftImpute] Iter 391: observed MAE=0.005180 validation MAE=0.016903,rank=5\n",
      "[SoftImpute] Iter 392: observed MAE=0.005178 validation MAE=0.016898,rank=5\n",
      "[SoftImpute] Iter 393: observed MAE=0.005176 validation MAE=0.016892,rank=5\n",
      "[SoftImpute] Iter 394: observed MAE=0.005174 validation MAE=0.016886,rank=5\n",
      "[SoftImpute] Iter 395: observed MAE=0.005173 validation MAE=0.016880,rank=5\n",
      "[SoftImpute] Iter 396: observed MAE=0.005171 validation MAE=0.016875,rank=5\n",
      "[SoftImpute] Iter 397: observed MAE=0.005169 validation MAE=0.016869,rank=5\n",
      "[SoftImpute] Iter 398: observed MAE=0.005167 validation MAE=0.016863,rank=5\n",
      "[SoftImpute] Iter 399: observed MAE=0.005165 validation MAE=0.016858,rank=5\n",
      "[SoftImpute] Iter 400: observed MAE=0.005164 validation MAE=0.016852,rank=5\n",
      "[SoftImpute] Iter 401: observed MAE=0.005162 validation MAE=0.016846,rank=5\n",
      "[SoftImpute] Iter 402: observed MAE=0.005160 validation MAE=0.016841,rank=5\n",
      "[SoftImpute] Iter 403: observed MAE=0.005158 validation MAE=0.016835,rank=5\n",
      "[SoftImpute] Iter 404: observed MAE=0.005157 validation MAE=0.016830,rank=5\n",
      "[SoftImpute] Iter 405: observed MAE=0.005155 validation MAE=0.016824,rank=5\n",
      "[SoftImpute] Iter 406: observed MAE=0.005153 validation MAE=0.016818,rank=5\n",
      "[SoftImpute] Iter 407: observed MAE=0.005151 validation MAE=0.016813,rank=5\n",
      "[SoftImpute] Iter 408: observed MAE=0.005150 validation MAE=0.016807,rank=5\n",
      "[SoftImpute] Iter 409: observed MAE=0.005148 validation MAE=0.016802,rank=5\n",
      "[SoftImpute] Iter 410: observed MAE=0.005146 validation MAE=0.016796,rank=5\n",
      "[SoftImpute] Iter 411: observed MAE=0.005145 validation MAE=0.016790,rank=5\n",
      "[SoftImpute] Iter 412: observed MAE=0.005143 validation MAE=0.016785,rank=5\n",
      "[SoftImpute] Iter 413: observed MAE=0.005141 validation MAE=0.016779,rank=5\n",
      "[SoftImpute] Iter 414: observed MAE=0.005140 validation MAE=0.016774,rank=5\n",
      "[SoftImpute] Iter 415: observed MAE=0.005138 validation MAE=0.016768,rank=5\n",
      "[SoftImpute] Iter 416: observed MAE=0.005136 validation MAE=0.016763,rank=5\n",
      "[SoftImpute] Iter 417: observed MAE=0.005134 validation MAE=0.016757,rank=5\n",
      "[SoftImpute] Iter 418: observed MAE=0.005133 validation MAE=0.016752,rank=5\n",
      "[SoftImpute] Iter 419: observed MAE=0.005131 validation MAE=0.016746,rank=5\n",
      "[SoftImpute] Iter 420: observed MAE=0.005129 validation MAE=0.016741,rank=5\n",
      "[SoftImpute] Iter 421: observed MAE=0.005128 validation MAE=0.016736,rank=5\n",
      "[SoftImpute] Iter 422: observed MAE=0.005126 validation MAE=0.016730,rank=5\n",
      "[SoftImpute] Iter 423: observed MAE=0.005124 validation MAE=0.016725,rank=5\n",
      "[SoftImpute] Iter 424: observed MAE=0.005123 validation MAE=0.016719,rank=5\n",
      "[SoftImpute] Iter 425: observed MAE=0.005121 validation MAE=0.016714,rank=5\n",
      "[SoftImpute] Iter 426: observed MAE=0.005120 validation MAE=0.016708,rank=5\n",
      "[SoftImpute] Iter 427: observed MAE=0.005118 validation MAE=0.016703,rank=5\n",
      "[SoftImpute] Iter 428: observed MAE=0.005116 validation MAE=0.016698,rank=5\n",
      "[SoftImpute] Iter 429: observed MAE=0.005115 validation MAE=0.016692,rank=5\n",
      "[SoftImpute] Iter 430: observed MAE=0.005113 validation MAE=0.016687,rank=5\n",
      "[SoftImpute] Iter 431: observed MAE=0.005111 validation MAE=0.016681,rank=5\n",
      "[SoftImpute] Iter 432: observed MAE=0.005110 validation MAE=0.016676,rank=5\n",
      "[SoftImpute] Iter 433: observed MAE=0.005108 validation MAE=0.016671,rank=5\n",
      "[SoftImpute] Iter 434: observed MAE=0.005107 validation MAE=0.016665,rank=5\n",
      "[SoftImpute] Iter 435: observed MAE=0.005105 validation MAE=0.016660,rank=5\n",
      "[SoftImpute] Iter 436: observed MAE=0.005103 validation MAE=0.016654,rank=5\n",
      "[SoftImpute] Iter 437: observed MAE=0.005102 validation MAE=0.016649,rank=5\n",
      "[SoftImpute] Iter 438: observed MAE=0.005100 validation MAE=0.016643,rank=5\n",
      "[SoftImpute] Iter 439: observed MAE=0.005099 validation MAE=0.016638,rank=5\n",
      "[SoftImpute] Iter 440: observed MAE=0.005097 validation MAE=0.016633,rank=5\n",
      "[SoftImpute] Iter 441: observed MAE=0.005095 validation MAE=0.016627,rank=5\n",
      "[SoftImpute] Iter 442: observed MAE=0.005094 validation MAE=0.016622,rank=5\n",
      "[SoftImpute] Iter 443: observed MAE=0.005092 validation MAE=0.016617,rank=5\n",
      "[SoftImpute] Iter 444: observed MAE=0.005091 validation MAE=0.016611,rank=5\n",
      "[SoftImpute] Iter 445: observed MAE=0.005089 validation MAE=0.016606,rank=5\n",
      "[SoftImpute] Iter 446: observed MAE=0.005088 validation MAE=0.016600,rank=5\n",
      "[SoftImpute] Iter 447: observed MAE=0.005086 validation MAE=0.016595,rank=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 448: observed MAE=0.005085 validation MAE=0.016590,rank=5\n",
      "[SoftImpute] Iter 449: observed MAE=0.005083 validation MAE=0.016584,rank=5\n",
      "[SoftImpute] Iter 450: observed MAE=0.005082 validation MAE=0.016579,rank=5\n",
      "[SoftImpute] Iter 451: observed MAE=0.005080 validation MAE=0.016573,rank=5\n",
      "[SoftImpute] Iter 452: observed MAE=0.005079 validation MAE=0.016568,rank=5\n",
      "[SoftImpute] Iter 453: observed MAE=0.005077 validation MAE=0.016562,rank=5\n",
      "[SoftImpute] Iter 454: observed MAE=0.005075 validation MAE=0.016557,rank=5\n",
      "[SoftImpute] Iter 455: observed MAE=0.005074 validation MAE=0.016552,rank=5\n",
      "[SoftImpute] Iter 456: observed MAE=0.005072 validation MAE=0.016546,rank=5\n",
      "[SoftImpute] Iter 457: observed MAE=0.005071 validation MAE=0.016541,rank=5\n",
      "[SoftImpute] Iter 458: observed MAE=0.005069 validation MAE=0.016535,rank=5\n",
      "[SoftImpute] Iter 459: observed MAE=0.005068 validation MAE=0.016530,rank=5\n",
      "[SoftImpute] Iter 460: observed MAE=0.005066 validation MAE=0.016524,rank=5\n",
      "[SoftImpute] Iter 461: observed MAE=0.005065 validation MAE=0.016519,rank=5\n",
      "[SoftImpute] Iter 462: observed MAE=0.005063 validation MAE=0.016514,rank=5\n",
      "[SoftImpute] Iter 463: observed MAE=0.005062 validation MAE=0.016509,rank=5\n",
      "[SoftImpute] Iter 464: observed MAE=0.005061 validation MAE=0.016505,rank=5\n",
      "[SoftImpute] Iter 465: observed MAE=0.005059 validation MAE=0.016500,rank=5\n",
      "[SoftImpute] Iter 466: observed MAE=0.005058 validation MAE=0.016495,rank=5\n",
      "[SoftImpute] Iter 467: observed MAE=0.005056 validation MAE=0.016490,rank=5\n",
      "[SoftImpute] Iter 468: observed MAE=0.005055 validation MAE=0.016485,rank=5\n",
      "[SoftImpute] Iter 469: observed MAE=0.005053 validation MAE=0.016480,rank=5\n",
      "[SoftImpute] Iter 470: observed MAE=0.005052 validation MAE=0.016475,rank=5\n",
      "[SoftImpute] Stopped after iteration 470 for lambda=0.039366\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 13.493860960006714\n",
      "After the matrix factor stage, training error is 0.00505, validation error is 0.01648\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.34448, val loss: 0.34765\n",
      "Main effects training epoch: 2, train loss: 0.27354, val loss: 0.27853\n",
      "Main effects training epoch: 3, train loss: 0.20823, val loss: 0.21145\n",
      "Main effects training epoch: 4, train loss: 0.16135, val loss: 0.16514\n",
      "Main effects training epoch: 5, train loss: 0.14048, val loss: 0.14025\n",
      "Main effects training epoch: 6, train loss: 0.13280, val loss: 0.13280\n",
      "Main effects training epoch: 7, train loss: 0.13041, val loss: 0.12922\n",
      "Main effects training epoch: 8, train loss: 0.12977, val loss: 0.12861\n",
      "Main effects training epoch: 9, train loss: 0.12914, val loss: 0.12821\n",
      "Main effects training epoch: 10, train loss: 0.12852, val loss: 0.12844\n",
      "Main effects training epoch: 11, train loss: 0.12742, val loss: 0.12590\n",
      "Main effects training epoch: 12, train loss: 0.12612, val loss: 0.12564\n",
      "Main effects training epoch: 13, train loss: 0.12339, val loss: 0.12252\n",
      "Main effects training epoch: 14, train loss: 0.11835, val loss: 0.11883\n",
      "Main effects training epoch: 15, train loss: 0.11578, val loss: 0.11651\n",
      "Main effects training epoch: 16, train loss: 0.11289, val loss: 0.11344\n",
      "Main effects training epoch: 17, train loss: 0.11081, val loss: 0.11186\n",
      "Main effects training epoch: 18, train loss: 0.11088, val loss: 0.11312\n",
      "Main effects training epoch: 19, train loss: 0.11027, val loss: 0.11199\n",
      "Main effects training epoch: 20, train loss: 0.11090, val loss: 0.11238\n",
      "Main effects training epoch: 21, train loss: 0.10922, val loss: 0.11096\n",
      "Main effects training epoch: 22, train loss: 0.10900, val loss: 0.11106\n",
      "Main effects training epoch: 23, train loss: 0.10645, val loss: 0.10812\n",
      "Main effects training epoch: 24, train loss: 0.10707, val loss: 0.10858\n",
      "Main effects training epoch: 25, train loss: 0.10776, val loss: 0.10768\n",
      "Main effects training epoch: 26, train loss: 0.10556, val loss: 0.10715\n",
      "Main effects training epoch: 27, train loss: 0.10595, val loss: 0.10680\n",
      "Main effects training epoch: 28, train loss: 0.10509, val loss: 0.10619\n",
      "Main effects training epoch: 29, train loss: 0.10460, val loss: 0.10568\n",
      "Main effects training epoch: 30, train loss: 0.10512, val loss: 0.10585\n",
      "Main effects training epoch: 31, train loss: 0.10437, val loss: 0.10598\n",
      "Main effects training epoch: 32, train loss: 0.10431, val loss: 0.10577\n",
      "Main effects training epoch: 33, train loss: 0.10423, val loss: 0.10545\n",
      "Main effects training epoch: 34, train loss: 0.10413, val loss: 0.10561\n",
      "Main effects training epoch: 35, train loss: 0.10424, val loss: 0.10579\n",
      "Main effects training epoch: 36, train loss: 0.10444, val loss: 0.10578\n",
      "Main effects training epoch: 37, train loss: 0.10400, val loss: 0.10575\n",
      "Main effects training epoch: 38, train loss: 0.10438, val loss: 0.10591\n",
      "Main effects training epoch: 39, train loss: 0.10416, val loss: 0.10566\n",
      "Main effects training epoch: 40, train loss: 0.10393, val loss: 0.10524\n",
      "Main effects training epoch: 41, train loss: 0.10418, val loss: 0.10579\n",
      "Main effects training epoch: 42, train loss: 0.10392, val loss: 0.10565\n",
      "Main effects training epoch: 43, train loss: 0.10430, val loss: 0.10612\n",
      "Main effects training epoch: 44, train loss: 0.10409, val loss: 0.10553\n",
      "Main effects training epoch: 45, train loss: 0.10403, val loss: 0.10539\n",
      "Main effects training epoch: 46, train loss: 0.10400, val loss: 0.10547\n",
      "Main effects training epoch: 47, train loss: 0.10472, val loss: 0.10625\n",
      "Main effects training epoch: 48, train loss: 0.10410, val loss: 0.10597\n",
      "Main effects training epoch: 49, train loss: 0.10415, val loss: 0.10512\n",
      "Main effects training epoch: 50, train loss: 0.10408, val loss: 0.10560\n",
      "Main effects training epoch: 51, train loss: 0.10392, val loss: 0.10554\n",
      "Main effects training epoch: 52, train loss: 0.10395, val loss: 0.10519\n",
      "Main effects training epoch: 53, train loss: 0.10385, val loss: 0.10520\n",
      "Main effects training epoch: 54, train loss: 0.10398, val loss: 0.10563\n",
      "Main effects training epoch: 55, train loss: 0.10416, val loss: 0.10523\n",
      "Main effects training epoch: 56, train loss: 0.10404, val loss: 0.10586\n",
      "Main effects training epoch: 57, train loss: 0.10399, val loss: 0.10571\n",
      "Main effects training epoch: 58, train loss: 0.10443, val loss: 0.10631\n",
      "Main effects training epoch: 59, train loss: 0.10393, val loss: 0.10549\n",
      "Main effects training epoch: 60, train loss: 0.10386, val loss: 0.10534\n",
      "Main effects training epoch: 61, train loss: 0.10389, val loss: 0.10538\n",
      "Main effects training epoch: 62, train loss: 0.10417, val loss: 0.10552\n",
      "Main effects training epoch: 63, train loss: 0.10396, val loss: 0.10570\n",
      "Main effects training epoch: 64, train loss: 0.10412, val loss: 0.10546\n",
      "Main effects training epoch: 65, train loss: 0.10403, val loss: 0.10549\n",
      "Main effects training epoch: 66, train loss: 0.10398, val loss: 0.10514\n",
      "Main effects training epoch: 67, train loss: 0.10385, val loss: 0.10514\n",
      "Main effects training epoch: 68, train loss: 0.10396, val loss: 0.10561\n",
      "Main effects training epoch: 69, train loss: 0.10397, val loss: 0.10523\n",
      "Main effects training epoch: 70, train loss: 0.10420, val loss: 0.10542\n",
      "Main effects training epoch: 71, train loss: 0.10473, val loss: 0.10671\n",
      "Main effects training epoch: 72, train loss: 0.10418, val loss: 0.10552\n",
      "Main effects training epoch: 73, train loss: 0.10427, val loss: 0.10590\n",
      "Main effects training epoch: 74, train loss: 0.10409, val loss: 0.10540\n",
      "Main effects training epoch: 75, train loss: 0.10401, val loss: 0.10542\n",
      "Main effects training epoch: 76, train loss: 0.10384, val loss: 0.10560\n",
      "Main effects training epoch: 77, train loss: 0.10404, val loss: 0.10542\n",
      "Main effects training epoch: 78, train loss: 0.10394, val loss: 0.10550\n",
      "Main effects training epoch: 79, train loss: 0.10410, val loss: 0.10553\n",
      "Main effects training epoch: 80, train loss: 0.10425, val loss: 0.10542\n",
      "Main effects training epoch: 81, train loss: 0.10387, val loss: 0.10530\n",
      "Main effects training epoch: 82, train loss: 0.10409, val loss: 0.10607\n",
      "Main effects training epoch: 83, train loss: 0.10386, val loss: 0.10559\n",
      "Main effects training epoch: 84, train loss: 0.10405, val loss: 0.10549\n",
      "Main effects training epoch: 85, train loss: 0.10483, val loss: 0.10617\n",
      "Main effects training epoch: 86, train loss: 0.10383, val loss: 0.10551\n",
      "Main effects training epoch: 87, train loss: 0.10415, val loss: 0.10524\n",
      "Main effects training epoch: 88, train loss: 0.10389, val loss: 0.10586\n",
      "Main effects training epoch: 89, train loss: 0.10383, val loss: 0.10548\n",
      "Main effects training epoch: 90, train loss: 0.10400, val loss: 0.10512\n",
      "Main effects training epoch: 91, train loss: 0.10387, val loss: 0.10538\n",
      "Main effects training epoch: 92, train loss: 0.10420, val loss: 0.10635\n",
      "Main effects training epoch: 93, train loss: 0.10396, val loss: 0.10529\n",
      "Main effects training epoch: 94, train loss: 0.10417, val loss: 0.10594\n",
      "Main effects training epoch: 95, train loss: 0.10387, val loss: 0.10562\n",
      "Main effects training epoch: 96, train loss: 0.10378, val loss: 0.10533\n",
      "Main effects training epoch: 97, train loss: 0.10386, val loss: 0.10536\n",
      "Main effects training epoch: 98, train loss: 0.10425, val loss: 0.10630\n",
      "Main effects training epoch: 99, train loss: 0.10426, val loss: 0.10552\n",
      "Main effects training epoch: 100, train loss: 0.10419, val loss: 0.10632\n",
      "Main effects training epoch: 101, train loss: 0.10395, val loss: 0.10553\n",
      "Main effects training epoch: 102, train loss: 0.10397, val loss: 0.10584\n",
      "Main effects training epoch: 103, train loss: 0.10408, val loss: 0.10574\n",
      "Main effects training epoch: 104, train loss: 0.10395, val loss: 0.10576\n",
      "Main effects training epoch: 105, train loss: 0.10380, val loss: 0.10555\n",
      "Main effects training epoch: 106, train loss: 0.10382, val loss: 0.10590\n",
      "Main effects training epoch: 107, train loss: 0.10417, val loss: 0.10600\n",
      "Main effects training epoch: 108, train loss: 0.10410, val loss: 0.10567\n",
      "Main effects training epoch: 109, train loss: 0.10373, val loss: 0.10542\n",
      "Main effects training epoch: 110, train loss: 0.10377, val loss: 0.10525\n",
      "Main effects training epoch: 111, train loss: 0.10405, val loss: 0.10577\n",
      "Main effects training epoch: 112, train loss: 0.10409, val loss: 0.10590\n",
      "Main effects training epoch: 113, train loss: 0.10474, val loss: 0.10615\n",
      "Main effects training epoch: 114, train loss: 0.10406, val loss: 0.10594\n",
      "Main effects training epoch: 115, train loss: 0.10415, val loss: 0.10553\n",
      "Main effects training epoch: 116, train loss: 0.10437, val loss: 0.10680\n",
      "Main effects training epoch: 117, train loss: 0.10399, val loss: 0.10532\n",
      "Main effects training epoch: 118, train loss: 0.10393, val loss: 0.10583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 119, train loss: 0.10410, val loss: 0.10601\n",
      "Main effects training epoch: 120, train loss: 0.10375, val loss: 0.10578\n",
      "Main effects training epoch: 121, train loss: 0.10382, val loss: 0.10529\n",
      "Main effects training epoch: 122, train loss: 0.10391, val loss: 0.10552\n",
      "Main effects training epoch: 123, train loss: 0.10397, val loss: 0.10615\n",
      "Main effects training epoch: 124, train loss: 0.10422, val loss: 0.10572\n",
      "Main effects training epoch: 125, train loss: 0.10414, val loss: 0.10601\n",
      "Main effects training epoch: 126, train loss: 0.10402, val loss: 0.10605\n",
      "Main effects training epoch: 127, train loss: 0.10402, val loss: 0.10531\n",
      "Main effects training epoch: 128, train loss: 0.10395, val loss: 0.10615\n",
      "Main effects training epoch: 129, train loss: 0.10410, val loss: 0.10643\n",
      "Main effects training epoch: 130, train loss: 0.10401, val loss: 0.10526\n",
      "Main effects training epoch: 131, train loss: 0.10380, val loss: 0.10577\n",
      "Main effects training epoch: 132, train loss: 0.10404, val loss: 0.10554\n",
      "Main effects training epoch: 133, train loss: 0.10425, val loss: 0.10648\n",
      "Main effects training epoch: 134, train loss: 0.10454, val loss: 0.10616\n",
      "Main effects training epoch: 135, train loss: 0.10424, val loss: 0.10653\n",
      "Main effects training epoch: 136, train loss: 0.10426, val loss: 0.10565\n",
      "Main effects training epoch: 137, train loss: 0.10444, val loss: 0.10639\n",
      "Main effects training epoch: 138, train loss: 0.10433, val loss: 0.10622\n",
      "Main effects training epoch: 139, train loss: 0.10384, val loss: 0.10565\n",
      "Main effects training epoch: 140, train loss: 0.10392, val loss: 0.10539\n",
      "Main effects training epoch: 141, train loss: 0.10420, val loss: 0.10593\n",
      "Main effects training epoch: 142, train loss: 0.10404, val loss: 0.10617\n",
      "Main effects training epoch: 143, train loss: 0.10420, val loss: 0.10567\n",
      "Main effects training epoch: 144, train loss: 0.10383, val loss: 0.10601\n",
      "Main effects training epoch: 145, train loss: 0.10370, val loss: 0.10530\n",
      "Main effects training epoch: 146, train loss: 0.10381, val loss: 0.10596\n",
      "Main effects training epoch: 147, train loss: 0.10380, val loss: 0.10575\n",
      "Main effects training epoch: 148, train loss: 0.10377, val loss: 0.10571\n",
      "Main effects training epoch: 149, train loss: 0.10383, val loss: 0.10566\n",
      "Main effects training epoch: 150, train loss: 0.10393, val loss: 0.10604\n",
      "Early stop at epoch 150, with validation loss: 0.10604\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.10405, val loss: 0.10498\n",
      "Main effects tuning epoch: 2, train loss: 0.10423, val loss: 0.10552\n",
      "Main effects tuning epoch: 3, train loss: 0.10411, val loss: 0.10505\n",
      "Main effects tuning epoch: 4, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 5, train loss: 0.10382, val loss: 0.10498\n",
      "Main effects tuning epoch: 6, train loss: 0.10425, val loss: 0.10582\n",
      "Main effects tuning epoch: 7, train loss: 0.10408, val loss: 0.10530\n",
      "Main effects tuning epoch: 8, train loss: 0.10397, val loss: 0.10527\n",
      "Main effects tuning epoch: 9, train loss: 0.10401, val loss: 0.10507\n",
      "Main effects tuning epoch: 10, train loss: 0.10407, val loss: 0.10539\n",
      "Main effects tuning epoch: 11, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 12, train loss: 0.10393, val loss: 0.10556\n",
      "Main effects tuning epoch: 13, train loss: 0.10415, val loss: 0.10510\n",
      "Main effects tuning epoch: 14, train loss: 0.10397, val loss: 0.10570\n",
      "Main effects tuning epoch: 15, train loss: 0.10386, val loss: 0.10516\n",
      "Main effects tuning epoch: 16, train loss: 0.10395, val loss: 0.10562\n",
      "Main effects tuning epoch: 17, train loss: 0.10395, val loss: 0.10539\n",
      "Main effects tuning epoch: 18, train loss: 0.10422, val loss: 0.10568\n",
      "Main effects tuning epoch: 19, train loss: 0.10433, val loss: 0.10517\n",
      "Main effects tuning epoch: 20, train loss: 0.10384, val loss: 0.10568\n",
      "Main effects tuning epoch: 21, train loss: 0.10400, val loss: 0.10541\n",
      "Main effects tuning epoch: 22, train loss: 0.10393, val loss: 0.10526\n",
      "Main effects tuning epoch: 23, train loss: 0.10434, val loss: 0.10582\n",
      "Main effects tuning epoch: 24, train loss: 0.10406, val loss: 0.10540\n",
      "Main effects tuning epoch: 25, train loss: 0.10410, val loss: 0.10570\n",
      "Main effects tuning epoch: 26, train loss: 0.10405, val loss: 0.10569\n",
      "Main effects tuning epoch: 27, train loss: 0.10406, val loss: 0.10556\n",
      "Main effects tuning epoch: 28, train loss: 0.10394, val loss: 0.10542\n",
      "Main effects tuning epoch: 29, train loss: 0.10383, val loss: 0.10542\n",
      "Main effects tuning epoch: 30, train loss: 0.10385, val loss: 0.10548\n",
      "Main effects tuning epoch: 31, train loss: 0.10395, val loss: 0.10516\n",
      "Main effects tuning epoch: 32, train loss: 0.10391, val loss: 0.10584\n",
      "Main effects tuning epoch: 33, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 34, train loss: 0.10391, val loss: 0.10561\n",
      "Main effects tuning epoch: 35, train loss: 0.10424, val loss: 0.10531\n",
      "Main effects tuning epoch: 36, train loss: 0.10395, val loss: 0.10554\n",
      "Main effects tuning epoch: 37, train loss: 0.10400, val loss: 0.10600\n",
      "Main effects tuning epoch: 38, train loss: 0.10408, val loss: 0.10530\n",
      "Main effects tuning epoch: 39, train loss: 0.10394, val loss: 0.10565\n",
      "Main effects tuning epoch: 40, train loss: 0.10385, val loss: 0.10569\n",
      "Main effects tuning epoch: 41, train loss: 0.10390, val loss: 0.10539\n",
      "Main effects tuning epoch: 42, train loss: 0.10400, val loss: 0.10587\n",
      "Main effects tuning epoch: 43, train loss: 0.10386, val loss: 0.10547\n",
      "Main effects tuning epoch: 44, train loss: 0.10403, val loss: 0.10597\n",
      "Main effects tuning epoch: 45, train loss: 0.10384, val loss: 0.10541\n",
      "Main effects tuning epoch: 46, train loss: 0.10398, val loss: 0.10582\n",
      "Main effects tuning epoch: 47, train loss: 0.10375, val loss: 0.10527\n",
      "Main effects tuning epoch: 48, train loss: 0.10380, val loss: 0.10549\n",
      "Main effects tuning epoch: 49, train loss: 0.10396, val loss: 0.10566\n",
      "Main effects tuning epoch: 50, train loss: 0.10407, val loss: 0.10549\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.09376, val loss: 0.09232\n",
      "Interaction training epoch: 2, train loss: 0.20140, val loss: 0.20111\n",
      "Interaction training epoch: 3, train loss: 0.06617, val loss: 0.06860\n",
      "Interaction training epoch: 4, train loss: 0.05173, val loss: 0.05197\n",
      "Interaction training epoch: 5, train loss: 0.06497, val loss: 0.06595\n",
      "Interaction training epoch: 6, train loss: 0.04386, val loss: 0.04449\n",
      "Interaction training epoch: 7, train loss: 0.04606, val loss: 0.04529\n",
      "Interaction training epoch: 8, train loss: 0.04492, val loss: 0.04489\n",
      "Interaction training epoch: 9, train loss: 0.04997, val loss: 0.04868\n",
      "Interaction training epoch: 10, train loss: 0.04002, val loss: 0.04133\n",
      "Interaction training epoch: 11, train loss: 0.04269, val loss: 0.04306\n",
      "Interaction training epoch: 12, train loss: 0.04100, val loss: 0.03997\n",
      "Interaction training epoch: 13, train loss: 0.04366, val loss: 0.04360\n",
      "Interaction training epoch: 14, train loss: 0.04928, val loss: 0.05084\n",
      "Interaction training epoch: 15, train loss: 0.04532, val loss: 0.04488\n",
      "Interaction training epoch: 16, train loss: 0.04530, val loss: 0.04524\n",
      "Interaction training epoch: 17, train loss: 0.03784, val loss: 0.03763\n",
      "Interaction training epoch: 18, train loss: 0.04939, val loss: 0.05023\n",
      "Interaction training epoch: 19, train loss: 0.03608, val loss: 0.03597\n",
      "Interaction training epoch: 20, train loss: 0.04344, val loss: 0.04367\n",
      "Interaction training epoch: 21, train loss: 0.04002, val loss: 0.03932\n",
      "Interaction training epoch: 22, train loss: 0.03977, val loss: 0.03977\n",
      "Interaction training epoch: 23, train loss: 0.03777, val loss: 0.03709\n",
      "Interaction training epoch: 24, train loss: 0.03927, val loss: 0.03924\n",
      "Interaction training epoch: 25, train loss: 0.04164, val loss: 0.04112\n",
      "Interaction training epoch: 26, train loss: 0.04429, val loss: 0.04389\n",
      "Interaction training epoch: 27, train loss: 0.04417, val loss: 0.04416\n",
      "Interaction training epoch: 28, train loss: 0.04241, val loss: 0.04112\n",
      "Interaction training epoch: 29, train loss: 0.03548, val loss: 0.03672\n",
      "Interaction training epoch: 30, train loss: 0.04420, val loss: 0.04353\n",
      "Interaction training epoch: 31, train loss: 0.04020, val loss: 0.04004\n",
      "Interaction training epoch: 32, train loss: 0.03422, val loss: 0.03451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 33, train loss: 0.03378, val loss: 0.03407\n",
      "Interaction training epoch: 34, train loss: 0.03925, val loss: 0.03880\n",
      "Interaction training epoch: 35, train loss: 0.03752, val loss: 0.03636\n",
      "Interaction training epoch: 36, train loss: 0.03582, val loss: 0.03564\n",
      "Interaction training epoch: 37, train loss: 0.03069, val loss: 0.03087\n",
      "Interaction training epoch: 38, train loss: 0.03729, val loss: 0.03652\n",
      "Interaction training epoch: 39, train loss: 0.03772, val loss: 0.03772\n",
      "Interaction training epoch: 40, train loss: 0.04332, val loss: 0.04201\n",
      "Interaction training epoch: 41, train loss: 0.03409, val loss: 0.03435\n",
      "Interaction training epoch: 42, train loss: 0.03501, val loss: 0.03419\n",
      "Interaction training epoch: 43, train loss: 0.03630, val loss: 0.03573\n",
      "Interaction training epoch: 44, train loss: 0.03447, val loss: 0.03420\n",
      "Interaction training epoch: 45, train loss: 0.03622, val loss: 0.03541\n",
      "Interaction training epoch: 46, train loss: 0.02991, val loss: 0.03002\n",
      "Interaction training epoch: 47, train loss: 0.02880, val loss: 0.02856\n",
      "Interaction training epoch: 48, train loss: 0.03548, val loss: 0.03469\n",
      "Interaction training epoch: 49, train loss: 0.03399, val loss: 0.03256\n",
      "Interaction training epoch: 50, train loss: 0.03225, val loss: 0.03266\n",
      "Interaction training epoch: 51, train loss: 0.03286, val loss: 0.03176\n",
      "Interaction training epoch: 52, train loss: 0.03084, val loss: 0.03012\n",
      "Interaction training epoch: 53, train loss: 0.03059, val loss: 0.02998\n",
      "Interaction training epoch: 54, train loss: 0.02966, val loss: 0.02932\n",
      "Interaction training epoch: 55, train loss: 0.04122, val loss: 0.04119\n",
      "Interaction training epoch: 56, train loss: 0.03164, val loss: 0.03059\n",
      "Interaction training epoch: 57, train loss: 0.03483, val loss: 0.03469\n",
      "Interaction training epoch: 58, train loss: 0.03320, val loss: 0.03294\n",
      "Interaction training epoch: 59, train loss: 0.02999, val loss: 0.02930\n",
      "Interaction training epoch: 60, train loss: 0.03540, val loss: 0.03496\n",
      "Interaction training epoch: 61, train loss: 0.03142, val loss: 0.02962\n",
      "Interaction training epoch: 62, train loss: 0.03146, val loss: 0.03164\n",
      "Interaction training epoch: 63, train loss: 0.03481, val loss: 0.03421\n",
      "Interaction training epoch: 64, train loss: 0.02946, val loss: 0.02881\n",
      "Interaction training epoch: 65, train loss: 0.03539, val loss: 0.03511\n",
      "Interaction training epoch: 66, train loss: 0.03797, val loss: 0.03805\n",
      "Interaction training epoch: 67, train loss: 0.03669, val loss: 0.03627\n",
      "Interaction training epoch: 68, train loss: 0.04844, val loss: 0.04881\n",
      "Interaction training epoch: 69, train loss: 0.02947, val loss: 0.02941\n",
      "Interaction training epoch: 70, train loss: 0.03805, val loss: 0.03672\n",
      "Interaction training epoch: 71, train loss: 0.02990, val loss: 0.02970\n",
      "Interaction training epoch: 72, train loss: 0.03926, val loss: 0.03952\n",
      "Interaction training epoch: 73, train loss: 0.03713, val loss: 0.03732\n",
      "Interaction training epoch: 74, train loss: 0.04469, val loss: 0.04392\n",
      "Interaction training epoch: 75, train loss: 0.03518, val loss: 0.03491\n",
      "Interaction training epoch: 76, train loss: 0.03954, val loss: 0.03893\n",
      "Interaction training epoch: 77, train loss: 0.04294, val loss: 0.04248\n",
      "Interaction training epoch: 78, train loss: 0.03122, val loss: 0.03170\n",
      "Interaction training epoch: 79, train loss: 0.03577, val loss: 0.03538\n",
      "Interaction training epoch: 80, train loss: 0.04727, val loss: 0.04664\n",
      "Interaction training epoch: 81, train loss: 0.03478, val loss: 0.03412\n",
      "Interaction training epoch: 82, train loss: 0.03736, val loss: 0.03700\n",
      "Interaction training epoch: 83, train loss: 0.03716, val loss: 0.03665\n",
      "Interaction training epoch: 84, train loss: 0.03675, val loss: 0.03575\n",
      "Interaction training epoch: 85, train loss: 0.03815, val loss: 0.03817\n",
      "Interaction training epoch: 86, train loss: 0.03960, val loss: 0.03903\n",
      "Interaction training epoch: 87, train loss: 0.03498, val loss: 0.03402\n",
      "Interaction training epoch: 88, train loss: 0.03630, val loss: 0.03583\n",
      "Interaction training epoch: 89, train loss: 0.03454, val loss: 0.03517\n",
      "Interaction training epoch: 90, train loss: 0.07231, val loss: 0.07223\n",
      "Interaction training epoch: 91, train loss: 0.04800, val loss: 0.04802\n",
      "Interaction training epoch: 92, train loss: 0.05224, val loss: 0.05162\n",
      "Interaction training epoch: 93, train loss: 0.02794, val loss: 0.02804\n",
      "Interaction training epoch: 94, train loss: 0.03585, val loss: 0.03546\n",
      "Interaction training epoch: 95, train loss: 0.04149, val loss: 0.04158\n",
      "Interaction training epoch: 96, train loss: 0.04943, val loss: 0.04909\n",
      "Interaction training epoch: 97, train loss: 0.05722, val loss: 0.05690\n",
      "Interaction training epoch: 98, train loss: 0.04854, val loss: 0.04831\n",
      "Interaction training epoch: 99, train loss: 0.02719, val loss: 0.02716\n",
      "Interaction training epoch: 100, train loss: 0.03689, val loss: 0.03722\n",
      "Interaction training epoch: 101, train loss: 0.04609, val loss: 0.04601\n",
      "Interaction training epoch: 102, train loss: 0.03250, val loss: 0.03231\n",
      "Interaction training epoch: 103, train loss: 0.03006, val loss: 0.02936\n",
      "Interaction training epoch: 104, train loss: 0.03967, val loss: 0.03937\n",
      "Interaction training epoch: 105, train loss: 0.03096, val loss: 0.03100\n",
      "Interaction training epoch: 106, train loss: 0.05099, val loss: 0.05044\n",
      "Interaction training epoch: 107, train loss: 0.03358, val loss: 0.03389\n",
      "Interaction training epoch: 108, train loss: 0.04273, val loss: 0.04236\n",
      "Interaction training epoch: 109, train loss: 0.07259, val loss: 0.07140\n",
      "Interaction training epoch: 110, train loss: 0.03442, val loss: 0.03433\n",
      "Interaction training epoch: 111, train loss: 0.02947, val loss: 0.02893\n",
      "Interaction training epoch: 112, train loss: 0.03413, val loss: 0.03396\n",
      "Interaction training epoch: 113, train loss: 0.03441, val loss: 0.03428\n",
      "Interaction training epoch: 114, train loss: 0.05762, val loss: 0.05645\n",
      "Interaction training epoch: 115, train loss: 0.02938, val loss: 0.02918\n",
      "Interaction training epoch: 116, train loss: 0.04805, val loss: 0.04824\n",
      "Interaction training epoch: 117, train loss: 0.03927, val loss: 0.03835\n",
      "Interaction training epoch: 118, train loss: 0.03949, val loss: 0.03920\n",
      "Interaction training epoch: 119, train loss: 0.03835, val loss: 0.03852\n",
      "Interaction training epoch: 120, train loss: 0.07402, val loss: 0.07267\n",
      "Interaction training epoch: 121, train loss: 0.02786, val loss: 0.02794\n",
      "Interaction training epoch: 122, train loss: 0.02966, val loss: 0.02840\n",
      "Interaction training epoch: 123, train loss: 0.03146, val loss: 0.03080\n",
      "Interaction training epoch: 124, train loss: 0.03135, val loss: 0.03095\n",
      "Interaction training epoch: 125, train loss: 0.02893, val loss: 0.02832\n",
      "Interaction training epoch: 126, train loss: 0.06644, val loss: 0.06554\n",
      "Interaction training epoch: 127, train loss: 0.02439, val loss: 0.02405\n",
      "Interaction training epoch: 128, train loss: 0.02442, val loss: 0.02392\n",
      "Interaction training epoch: 129, train loss: 0.06718, val loss: 0.06662\n",
      "Interaction training epoch: 130, train loss: 0.06873, val loss: 0.06752\n",
      "Interaction training epoch: 131, train loss: 0.02836, val loss: 0.02795\n",
      "Interaction training epoch: 132, train loss: 0.02537, val loss: 0.02462\n",
      "Interaction training epoch: 133, train loss: 0.05578, val loss: 0.05497\n",
      "Interaction training epoch: 134, train loss: 0.02756, val loss: 0.02628\n",
      "Interaction training epoch: 135, train loss: 0.03851, val loss: 0.03792\n",
      "Interaction training epoch: 136, train loss: 0.02792, val loss: 0.02763\n",
      "Interaction training epoch: 137, train loss: 0.02882, val loss: 0.02822\n",
      "Interaction training epoch: 138, train loss: 0.04268, val loss: 0.04230\n",
      "Interaction training epoch: 139, train loss: 0.02999, val loss: 0.03028\n",
      "Interaction training epoch: 140, train loss: 0.02653, val loss: 0.02598\n",
      "Interaction training epoch: 141, train loss: 0.02864, val loss: 0.02740\n",
      "Interaction training epoch: 142, train loss: 0.04031, val loss: 0.03930\n",
      "Interaction training epoch: 143, train loss: 0.08930, val loss: 0.08825\n",
      "Interaction training epoch: 144, train loss: 0.03278, val loss: 0.03201\n",
      "Interaction training epoch: 145, train loss: 0.07595, val loss: 0.07444\n",
      "Interaction training epoch: 146, train loss: 0.04088, val loss: 0.04007\n",
      "Interaction training epoch: 147, train loss: 0.03063, val loss: 0.02967\n",
      "Interaction training epoch: 148, train loss: 0.08210, val loss: 0.08116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 149, train loss: 0.02600, val loss: 0.02595\n",
      "Interaction training epoch: 150, train loss: 0.04451, val loss: 0.04266\n",
      "Interaction training epoch: 151, train loss: 0.05071, val loss: 0.04953\n",
      "Interaction training epoch: 152, train loss: 0.03689, val loss: 0.03657\n",
      "Interaction training epoch: 153, train loss: 0.05108, val loss: 0.05017\n",
      "Interaction training epoch: 154, train loss: 0.03125, val loss: 0.03020\n",
      "Interaction training epoch: 155, train loss: 0.05262, val loss: 0.05160\n",
      "Interaction training epoch: 156, train loss: 0.03189, val loss: 0.03099\n",
      "Interaction training epoch: 157, train loss: 0.03302, val loss: 0.03192\n",
      "Interaction training epoch: 158, train loss: 0.03919, val loss: 0.03867\n",
      "Interaction training epoch: 159, train loss: 0.06294, val loss: 0.06164\n",
      "Interaction training epoch: 160, train loss: 0.02504, val loss: 0.02398\n",
      "Interaction training epoch: 161, train loss: 0.02726, val loss: 0.02622\n",
      "Interaction training epoch: 162, train loss: 0.02696, val loss: 0.02663\n",
      "Interaction training epoch: 163, train loss: 0.04443, val loss: 0.04306\n",
      "Interaction training epoch: 164, train loss: 0.08229, val loss: 0.08080\n",
      "Interaction training epoch: 165, train loss: 0.02497, val loss: 0.02433\n",
      "Interaction training epoch: 166, train loss: 0.02405, val loss: 0.02321\n",
      "Interaction training epoch: 167, train loss: 0.03154, val loss: 0.03087\n",
      "Interaction training epoch: 168, train loss: 0.03079, val loss: 0.02948\n",
      "Interaction training epoch: 169, train loss: 0.02562, val loss: 0.02466\n",
      "Interaction training epoch: 170, train loss: 0.03950, val loss: 0.03811\n",
      "Interaction training epoch: 171, train loss: 0.05102, val loss: 0.05005\n",
      "Interaction training epoch: 172, train loss: 0.07393, val loss: 0.07199\n",
      "Interaction training epoch: 173, train loss: 0.03775, val loss: 0.03659\n",
      "Interaction training epoch: 174, train loss: 0.03283, val loss: 0.03172\n",
      "Interaction training epoch: 175, train loss: 0.02729, val loss: 0.02663\n",
      "Interaction training epoch: 176, train loss: 0.04249, val loss: 0.04090\n",
      "Interaction training epoch: 177, train loss: 0.11371, val loss: 0.11248\n",
      "Interaction training epoch: 178, train loss: 0.05008, val loss: 0.04851\n",
      "Interaction training epoch: 179, train loss: 0.05070, val loss: 0.04903\n",
      "Interaction training epoch: 180, train loss: 0.05192, val loss: 0.05082\n",
      "Interaction training epoch: 181, train loss: 0.03499, val loss: 0.03401\n",
      "Interaction training epoch: 182, train loss: 0.06009, val loss: 0.05850\n",
      "Interaction training epoch: 183, train loss: 0.03517, val loss: 0.03374\n",
      "Interaction training epoch: 184, train loss: 0.04107, val loss: 0.04010\n",
      "Interaction training epoch: 185, train loss: 0.06361, val loss: 0.06162\n",
      "Interaction training epoch: 186, train loss: 0.06349, val loss: 0.06178\n",
      "Interaction training epoch: 187, train loss: 0.05374, val loss: 0.05196\n",
      "Interaction training epoch: 188, train loss: 0.05958, val loss: 0.05804\n",
      "Interaction training epoch: 189, train loss: 0.02604, val loss: 0.02519\n",
      "Interaction training epoch: 190, train loss: 0.04369, val loss: 0.04302\n",
      "Interaction training epoch: 191, train loss: 0.04175, val loss: 0.04116\n",
      "Interaction training epoch: 192, train loss: 0.06782, val loss: 0.06645\n",
      "Interaction training epoch: 193, train loss: 0.06145, val loss: 0.06034\n",
      "Interaction training epoch: 194, train loss: 0.04303, val loss: 0.04151\n",
      "Interaction training epoch: 195, train loss: 0.03319, val loss: 0.03229\n",
      "Interaction training epoch: 196, train loss: 0.03143, val loss: 0.02941\n",
      "Interaction training epoch: 197, train loss: 0.03413, val loss: 0.03311\n",
      "Interaction training epoch: 198, train loss: 0.03045, val loss: 0.02901\n",
      "Interaction training epoch: 199, train loss: 0.04253, val loss: 0.04138\n",
      "Interaction training epoch: 200, train loss: 0.04032, val loss: 0.03845\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########4 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.02853, val loss: 0.02785\n",
      "Interaction tuning epoch: 2, train loss: 0.02401, val loss: 0.02299\n",
      "Interaction tuning epoch: 3, train loss: 0.04478, val loss: 0.04265\n",
      "Interaction tuning epoch: 4, train loss: 0.06395, val loss: 0.06292\n",
      "Interaction tuning epoch: 5, train loss: 0.02356, val loss: 0.02286\n",
      "Interaction tuning epoch: 6, train loss: 0.06369, val loss: 0.06228\n",
      "Interaction tuning epoch: 7, train loss: 0.04154, val loss: 0.04077\n",
      "Interaction tuning epoch: 8, train loss: 0.08354, val loss: 0.08227\n",
      "Interaction tuning epoch: 9, train loss: 0.07421, val loss: 0.07270\n",
      "Interaction tuning epoch: 10, train loss: 0.04015, val loss: 0.03931\n",
      "Interaction tuning epoch: 11, train loss: 0.02840, val loss: 0.02746\n",
      "Interaction tuning epoch: 12, train loss: 0.06172, val loss: 0.06021\n",
      "Interaction tuning epoch: 13, train loss: 0.03776, val loss: 0.03699\n",
      "Interaction tuning epoch: 14, train loss: 0.07388, val loss: 0.07213\n",
      "Interaction tuning epoch: 15, train loss: 0.02628, val loss: 0.02557\n",
      "Interaction tuning epoch: 16, train loss: 0.02860, val loss: 0.02813\n",
      "Interaction tuning epoch: 17, train loss: 0.04973, val loss: 0.04802\n",
      "Interaction tuning epoch: 18, train loss: 0.02869, val loss: 0.02741\n",
      "Interaction tuning epoch: 19, train loss: 0.03914, val loss: 0.03730\n",
      "Interaction tuning epoch: 20, train loss: 0.03596, val loss: 0.03526\n",
      "Interaction tuning epoch: 21, train loss: 0.04924, val loss: 0.04797\n",
      "Interaction tuning epoch: 22, train loss: 0.08235, val loss: 0.08148\n",
      "Interaction tuning epoch: 23, train loss: 0.08430, val loss: 0.08186\n",
      "Interaction tuning epoch: 24, train loss: 0.07758, val loss: 0.07581\n",
      "Interaction tuning epoch: 25, train loss: 0.02468, val loss: 0.02425\n",
      "Interaction tuning epoch: 26, train loss: 0.02729, val loss: 0.02674\n",
      "Interaction tuning epoch: 27, train loss: 0.02677, val loss: 0.02542\n",
      "Interaction tuning epoch: 28, train loss: 0.03303, val loss: 0.03233\n",
      "Interaction tuning epoch: 29, train loss: 0.07628, val loss: 0.07409\n",
      "Interaction tuning epoch: 30, train loss: 0.05354, val loss: 0.05248\n",
      "Interaction tuning epoch: 31, train loss: 0.02819, val loss: 0.02642\n",
      "Interaction tuning epoch: 32, train loss: 0.02994, val loss: 0.02905\n",
      "Interaction tuning epoch: 33, train loss: 0.03903, val loss: 0.03809\n",
      "Interaction tuning epoch: 34, train loss: 0.03406, val loss: 0.03261\n",
      "Interaction tuning epoch: 35, train loss: 0.02886, val loss: 0.02797\n",
      "Interaction tuning epoch: 36, train loss: 0.04728, val loss: 0.04591\n",
      "Interaction tuning epoch: 37, train loss: 0.02745, val loss: 0.02653\n",
      "Interaction tuning epoch: 38, train loss: 0.09613, val loss: 0.09462\n",
      "Interaction tuning epoch: 39, train loss: 0.06430, val loss: 0.06326\n",
      "Interaction tuning epoch: 40, train loss: 0.03564, val loss: 0.03467\n",
      "Interaction tuning epoch: 41, train loss: 0.05842, val loss: 0.05720\n",
      "Interaction tuning epoch: 42, train loss: 0.02719, val loss: 0.02601\n",
      "Interaction tuning epoch: 43, train loss: 0.03689, val loss: 0.03533\n",
      "Interaction tuning epoch: 44, train loss: 0.02563, val loss: 0.02452\n",
      "Interaction tuning epoch: 45, train loss: 0.11515, val loss: 0.11336\n",
      "Interaction tuning epoch: 46, train loss: 0.05383, val loss: 0.05274\n",
      "Interaction tuning epoch: 47, train loss: 0.03183, val loss: 0.02986\n",
      "Interaction tuning epoch: 48, train loss: 0.04755, val loss: 0.04566\n",
      "Interaction tuning epoch: 49, train loss: 0.03504, val loss: 0.03437\n",
      "Interaction tuning epoch: 50, train loss: 0.03646, val loss: 0.03600\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 31.85081124305725\n",
      "After the gam stage, training error is 0.03646 , validation error is 0.03600\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 1.968308\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.026707 validation MAE=0.034089,rank=6\n",
      "[SoftImpute] Iter 2: observed MAE=0.024684 validation MAE=0.033041,rank=6\n",
      "[SoftImpute] Iter 3: observed MAE=0.022879 validation MAE=0.032061,rank=6\n",
      "[SoftImpute] Iter 4: observed MAE=0.021280 validation MAE=0.031192,rank=6\n",
      "[SoftImpute] Iter 5: observed MAE=0.019881 validation MAE=0.030404,rank=6\n",
      "[SoftImpute] Iter 6: observed MAE=0.018644 validation MAE=0.029688,rank=6\n",
      "[SoftImpute] Iter 7: observed MAE=0.017542 validation MAE=0.029029,rank=6\n",
      "[SoftImpute] Iter 8: observed MAE=0.016560 validation MAE=0.028422,rank=6\n",
      "[SoftImpute] Iter 9: observed MAE=0.015675 validation MAE=0.027863,rank=6\n",
      "[SoftImpute] Iter 10: observed MAE=0.014887 validation MAE=0.027355,rank=6\n",
      "[SoftImpute] Iter 11: observed MAE=0.014178 validation MAE=0.026900,rank=6\n",
      "[SoftImpute] Iter 12: observed MAE=0.013541 validation MAE=0.026485,rank=6\n",
      "[SoftImpute] Iter 13: observed MAE=0.012965 validation MAE=0.026105,rank=6\n",
      "[SoftImpute] Iter 14: observed MAE=0.012445 validation MAE=0.025762,rank=6\n",
      "[SoftImpute] Iter 15: observed MAE=0.011975 validation MAE=0.025447,rank=6\n",
      "[SoftImpute] Iter 16: observed MAE=0.011547 validation MAE=0.025154,rank=6\n",
      "[SoftImpute] Iter 17: observed MAE=0.011157 validation MAE=0.024882,rank=6\n",
      "[SoftImpute] Iter 18: observed MAE=0.010800 validation MAE=0.024636,rank=6\n",
      "[SoftImpute] Iter 19: observed MAE=0.010473 validation MAE=0.024408,rank=6\n",
      "[SoftImpute] Iter 20: observed MAE=0.010172 validation MAE=0.024196,rank=6\n",
      "[SoftImpute] Iter 21: observed MAE=0.009895 validation MAE=0.023999,rank=6\n",
      "[SoftImpute] Iter 22: observed MAE=0.009639 validation MAE=0.023815,rank=6\n",
      "[SoftImpute] Iter 23: observed MAE=0.009403 validation MAE=0.023645,rank=6\n",
      "[SoftImpute] Iter 24: observed MAE=0.009185 validation MAE=0.023484,rank=6\n",
      "[SoftImpute] Iter 25: observed MAE=0.008984 validation MAE=0.023333,rank=6\n",
      "[SoftImpute] Iter 26: observed MAE=0.008797 validation MAE=0.023190,rank=6\n",
      "[SoftImpute] Iter 27: observed MAE=0.008624 validation MAE=0.023053,rank=6\n",
      "[SoftImpute] Iter 28: observed MAE=0.008463 validation MAE=0.022922,rank=6\n",
      "[SoftImpute] Iter 29: observed MAE=0.008312 validation MAE=0.022795,rank=6\n",
      "[SoftImpute] Iter 30: observed MAE=0.008172 validation MAE=0.022675,rank=6\n",
      "[SoftImpute] Iter 31: observed MAE=0.008041 validation MAE=0.022560,rank=6\n",
      "[SoftImpute] Iter 32: observed MAE=0.007918 validation MAE=0.022450,rank=6\n",
      "[SoftImpute] Iter 33: observed MAE=0.007802 validation MAE=0.022348,rank=6\n",
      "[SoftImpute] Iter 34: observed MAE=0.007693 validation MAE=0.022250,rank=6\n",
      "[SoftImpute] Iter 35: observed MAE=0.007591 validation MAE=0.022157,rank=6\n",
      "[SoftImpute] Iter 36: observed MAE=0.007494 validation MAE=0.022068,rank=6\n",
      "[SoftImpute] Iter 37: observed MAE=0.007403 validation MAE=0.021984,rank=6\n",
      "[SoftImpute] Iter 38: observed MAE=0.007316 validation MAE=0.021902,rank=6\n",
      "[SoftImpute] Iter 39: observed MAE=0.007235 validation MAE=0.021823,rank=6\n",
      "[SoftImpute] Iter 40: observed MAE=0.007157 validation MAE=0.021747,rank=6\n",
      "[SoftImpute] Iter 41: observed MAE=0.007083 validation MAE=0.021673,rank=6\n",
      "[SoftImpute] Iter 42: observed MAE=0.007013 validation MAE=0.021603,rank=6\n",
      "[SoftImpute] Iter 43: observed MAE=0.006946 validation MAE=0.021534,rank=6\n",
      "[SoftImpute] Iter 44: observed MAE=0.006883 validation MAE=0.021466,rank=6\n",
      "[SoftImpute] Iter 45: observed MAE=0.006822 validation MAE=0.021400,rank=6\n",
      "[SoftImpute] Iter 46: observed MAE=0.006764 validation MAE=0.021335,rank=6\n",
      "[SoftImpute] Iter 47: observed MAE=0.006708 validation MAE=0.021273,rank=6\n",
      "[SoftImpute] Iter 48: observed MAE=0.006655 validation MAE=0.021212,rank=6\n",
      "[SoftImpute] Iter 49: observed MAE=0.006603 validation MAE=0.021152,rank=6\n",
      "[SoftImpute] Iter 50: observed MAE=0.006554 validation MAE=0.021093,rank=6\n",
      "[SoftImpute] Iter 51: observed MAE=0.006506 validation MAE=0.021036,rank=6\n",
      "[SoftImpute] Iter 52: observed MAE=0.006461 validation MAE=0.020980,rank=6\n",
      "[SoftImpute] Iter 53: observed MAE=0.006417 validation MAE=0.020926,rank=6\n",
      "[SoftImpute] Iter 54: observed MAE=0.006374 validation MAE=0.020873,rank=6\n",
      "[SoftImpute] Iter 55: observed MAE=0.006334 validation MAE=0.020822,rank=6\n",
      "[SoftImpute] Iter 56: observed MAE=0.006294 validation MAE=0.020774,rank=6\n",
      "[SoftImpute] Iter 57: observed MAE=0.006256 validation MAE=0.020726,rank=6\n",
      "[SoftImpute] Iter 58: observed MAE=0.006219 validation MAE=0.020680,rank=6\n",
      "[SoftImpute] Iter 59: observed MAE=0.006184 validation MAE=0.020633,rank=6\n",
      "[SoftImpute] Iter 60: observed MAE=0.006149 validation MAE=0.020588,rank=6\n",
      "[SoftImpute] Iter 61: observed MAE=0.006116 validation MAE=0.020544,rank=6\n",
      "[SoftImpute] Iter 62: observed MAE=0.006084 validation MAE=0.020500,rank=6\n",
      "[SoftImpute] Iter 63: observed MAE=0.006053 validation MAE=0.020457,rank=6\n",
      "[SoftImpute] Iter 64: observed MAE=0.006023 validation MAE=0.020414,rank=6\n",
      "[SoftImpute] Iter 65: observed MAE=0.005994 validation MAE=0.020372,rank=6\n",
      "[SoftImpute] Iter 66: observed MAE=0.005965 validation MAE=0.020331,rank=6\n",
      "[SoftImpute] Iter 67: observed MAE=0.005938 validation MAE=0.020290,rank=6\n",
      "[SoftImpute] Iter 68: observed MAE=0.005911 validation MAE=0.020251,rank=6\n",
      "[SoftImpute] Iter 69: observed MAE=0.005886 validation MAE=0.020212,rank=6\n",
      "[SoftImpute] Iter 70: observed MAE=0.005861 validation MAE=0.020173,rank=6\n",
      "[SoftImpute] Iter 71: observed MAE=0.005836 validation MAE=0.020136,rank=6\n",
      "[SoftImpute] Iter 72: observed MAE=0.005812 validation MAE=0.020099,rank=6\n",
      "[SoftImpute] Iter 73: observed MAE=0.005789 validation MAE=0.020062,rank=6\n",
      "[SoftImpute] Iter 74: observed MAE=0.005767 validation MAE=0.020026,rank=6\n",
      "[SoftImpute] Iter 75: observed MAE=0.005745 validation MAE=0.019991,rank=6\n",
      "[SoftImpute] Iter 76: observed MAE=0.005724 validation MAE=0.019957,rank=6\n",
      "[SoftImpute] Iter 77: observed MAE=0.005703 validation MAE=0.019924,rank=6\n",
      "[SoftImpute] Iter 78: observed MAE=0.005683 validation MAE=0.019892,rank=6\n",
      "[SoftImpute] Iter 79: observed MAE=0.005663 validation MAE=0.019859,rank=6\n",
      "[SoftImpute] Iter 80: observed MAE=0.005644 validation MAE=0.019828,rank=6\n",
      "[SoftImpute] Iter 81: observed MAE=0.005625 validation MAE=0.019796,rank=6\n",
      "[SoftImpute] Iter 82: observed MAE=0.005606 validation MAE=0.019766,rank=6\n",
      "[SoftImpute] Iter 83: observed MAE=0.005588 validation MAE=0.019735,rank=6\n",
      "[SoftImpute] Iter 84: observed MAE=0.005571 validation MAE=0.019705,rank=6\n",
      "[SoftImpute] Iter 85: observed MAE=0.005554 validation MAE=0.019675,rank=6\n",
      "[SoftImpute] Iter 86: observed MAE=0.005537 validation MAE=0.019645,rank=6\n",
      "[SoftImpute] Iter 87: observed MAE=0.005520 validation MAE=0.019616,rank=6\n",
      "[SoftImpute] Iter 88: observed MAE=0.005504 validation MAE=0.019587,rank=6\n",
      "[SoftImpute] Iter 89: observed MAE=0.005489 validation MAE=0.019558,rank=6\n",
      "[SoftImpute] Iter 90: observed MAE=0.005473 validation MAE=0.019530,rank=6\n",
      "[SoftImpute] Iter 91: observed MAE=0.005458 validation MAE=0.019501,rank=6\n",
      "[SoftImpute] Iter 92: observed MAE=0.005444 validation MAE=0.019473,rank=6\n",
      "[SoftImpute] Iter 93: observed MAE=0.005429 validation MAE=0.019445,rank=6\n",
      "[SoftImpute] Iter 94: observed MAE=0.005415 validation MAE=0.019418,rank=6\n",
      "[SoftImpute] Iter 95: observed MAE=0.005401 validation MAE=0.019390,rank=6\n",
      "[SoftImpute] Iter 96: observed MAE=0.005388 validation MAE=0.019363,rank=6\n",
      "[SoftImpute] Iter 97: observed MAE=0.005375 validation MAE=0.019335,rank=6\n",
      "[SoftImpute] Iter 98: observed MAE=0.005362 validation MAE=0.019308,rank=6\n",
      "[SoftImpute] Iter 99: observed MAE=0.005349 validation MAE=0.019282,rank=6\n",
      "[SoftImpute] Iter 100: observed MAE=0.005337 validation MAE=0.019255,rank=6\n",
      "[SoftImpute] Iter 101: observed MAE=0.005325 validation MAE=0.019228,rank=6\n",
      "[SoftImpute] Iter 102: observed MAE=0.005313 validation MAE=0.019202,rank=6\n",
      "[SoftImpute] Iter 103: observed MAE=0.005302 validation MAE=0.019176,rank=6\n",
      "[SoftImpute] Iter 104: observed MAE=0.005290 validation MAE=0.019151,rank=6\n",
      "[SoftImpute] Iter 105: observed MAE=0.005279 validation MAE=0.019125,rank=6\n",
      "[SoftImpute] Iter 106: observed MAE=0.005268 validation MAE=0.019100,rank=6\n",
      "[SoftImpute] Iter 107: observed MAE=0.005258 validation MAE=0.019075,rank=6\n",
      "[SoftImpute] Iter 108: observed MAE=0.005247 validation MAE=0.019050,rank=6\n",
      "[SoftImpute] Iter 109: observed MAE=0.005237 validation MAE=0.019025,rank=6\n",
      "[SoftImpute] Iter 110: observed MAE=0.005227 validation MAE=0.019001,rank=6\n",
      "[SoftImpute] Iter 111: observed MAE=0.005217 validation MAE=0.018976,rank=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 112: observed MAE=0.005207 validation MAE=0.018952,rank=6\n",
      "[SoftImpute] Iter 113: observed MAE=0.005198 validation MAE=0.018928,rank=6\n",
      "[SoftImpute] Iter 114: observed MAE=0.005188 validation MAE=0.018904,rank=6\n",
      "[SoftImpute] Iter 115: observed MAE=0.005179 validation MAE=0.018880,rank=6\n",
      "[SoftImpute] Iter 116: observed MAE=0.005170 validation MAE=0.018857,rank=6\n",
      "[SoftImpute] Iter 117: observed MAE=0.005161 validation MAE=0.018833,rank=6\n",
      "[SoftImpute] Iter 118: observed MAE=0.005152 validation MAE=0.018810,rank=6\n",
      "[SoftImpute] Iter 119: observed MAE=0.005144 validation MAE=0.018787,rank=6\n",
      "[SoftImpute] Iter 120: observed MAE=0.005135 validation MAE=0.018764,rank=6\n",
      "[SoftImpute] Iter 121: observed MAE=0.005127 validation MAE=0.018742,rank=6\n",
      "[SoftImpute] Iter 122: observed MAE=0.005119 validation MAE=0.018719,rank=6\n",
      "[SoftImpute] Iter 123: observed MAE=0.005111 validation MAE=0.018697,rank=6\n",
      "[SoftImpute] Iter 124: observed MAE=0.005103 validation MAE=0.018675,rank=6\n",
      "[SoftImpute] Iter 125: observed MAE=0.005095 validation MAE=0.018653,rank=6\n",
      "[SoftImpute] Iter 126: observed MAE=0.005088 validation MAE=0.018631,rank=6\n",
      "[SoftImpute] Iter 127: observed MAE=0.005080 validation MAE=0.018609,rank=6\n",
      "[SoftImpute] Iter 128: observed MAE=0.005073 validation MAE=0.018588,rank=6\n",
      "[SoftImpute] Iter 129: observed MAE=0.005066 validation MAE=0.018567,rank=6\n",
      "[SoftImpute] Iter 130: observed MAE=0.005058 validation MAE=0.018546,rank=6\n",
      "[SoftImpute] Iter 131: observed MAE=0.005051 validation MAE=0.018525,rank=6\n",
      "[SoftImpute] Iter 132: observed MAE=0.005045 validation MAE=0.018504,rank=6\n",
      "[SoftImpute] Iter 133: observed MAE=0.005038 validation MAE=0.018484,rank=6\n",
      "[SoftImpute] Iter 134: observed MAE=0.005031 validation MAE=0.018464,rank=6\n",
      "[SoftImpute] Iter 135: observed MAE=0.005025 validation MAE=0.018443,rank=6\n",
      "[SoftImpute] Iter 136: observed MAE=0.005018 validation MAE=0.018423,rank=6\n",
      "[SoftImpute] Iter 137: observed MAE=0.005012 validation MAE=0.018403,rank=6\n",
      "[SoftImpute] Iter 138: observed MAE=0.005005 validation MAE=0.018383,rank=6\n",
      "[SoftImpute] Iter 139: observed MAE=0.004999 validation MAE=0.018364,rank=6\n",
      "[SoftImpute] Iter 140: observed MAE=0.004993 validation MAE=0.018344,rank=6\n",
      "[SoftImpute] Iter 141: observed MAE=0.004987 validation MAE=0.018325,rank=6\n",
      "[SoftImpute] Iter 142: observed MAE=0.004981 validation MAE=0.018306,rank=6\n",
      "[SoftImpute] Iter 143: observed MAE=0.004975 validation MAE=0.018287,rank=6\n",
      "[SoftImpute] Iter 144: observed MAE=0.004969 validation MAE=0.018268,rank=6\n",
      "[SoftImpute] Iter 145: observed MAE=0.004964 validation MAE=0.018249,rank=6\n",
      "[SoftImpute] Iter 146: observed MAE=0.004958 validation MAE=0.018231,rank=6\n",
      "[SoftImpute] Iter 147: observed MAE=0.004953 validation MAE=0.018213,rank=6\n",
      "[SoftImpute] Iter 148: observed MAE=0.004947 validation MAE=0.018195,rank=6\n",
      "[SoftImpute] Iter 149: observed MAE=0.004942 validation MAE=0.018176,rank=6\n",
      "[SoftImpute] Iter 150: observed MAE=0.004937 validation MAE=0.018158,rank=6\n",
      "[SoftImpute] Iter 151: observed MAE=0.004931 validation MAE=0.018141,rank=6\n",
      "[SoftImpute] Iter 152: observed MAE=0.004926 validation MAE=0.018123,rank=6\n",
      "[SoftImpute] Iter 153: observed MAE=0.004921 validation MAE=0.018105,rank=6\n",
      "[SoftImpute] Iter 154: observed MAE=0.004916 validation MAE=0.018087,rank=6\n",
      "[SoftImpute] Iter 155: observed MAE=0.004911 validation MAE=0.018070,rank=6\n",
      "[SoftImpute] Iter 156: observed MAE=0.004907 validation MAE=0.018052,rank=6\n",
      "[SoftImpute] Iter 157: observed MAE=0.004902 validation MAE=0.018035,rank=6\n",
      "[SoftImpute] Iter 158: observed MAE=0.004897 validation MAE=0.018018,rank=6\n",
      "[SoftImpute] Iter 159: observed MAE=0.004893 validation MAE=0.018001,rank=6\n",
      "[SoftImpute] Iter 160: observed MAE=0.004888 validation MAE=0.017984,rank=6\n",
      "[SoftImpute] Iter 161: observed MAE=0.004884 validation MAE=0.017967,rank=6\n",
      "[SoftImpute] Iter 162: observed MAE=0.004879 validation MAE=0.017951,rank=6\n",
      "[SoftImpute] Iter 163: observed MAE=0.004875 validation MAE=0.017934,rank=6\n",
      "[SoftImpute] Iter 164: observed MAE=0.004871 validation MAE=0.017918,rank=6\n",
      "[SoftImpute] Iter 165: observed MAE=0.004866 validation MAE=0.017901,rank=6\n",
      "[SoftImpute] Iter 166: observed MAE=0.004862 validation MAE=0.017885,rank=6\n",
      "[SoftImpute] Iter 167: observed MAE=0.004858 validation MAE=0.017869,rank=6\n",
      "[SoftImpute] Iter 168: observed MAE=0.004854 validation MAE=0.017853,rank=6\n",
      "[SoftImpute] Iter 169: observed MAE=0.004850 validation MAE=0.017837,rank=6\n",
      "[SoftImpute] Iter 170: observed MAE=0.004846 validation MAE=0.017821,rank=6\n",
      "[SoftImpute] Iter 171: observed MAE=0.004842 validation MAE=0.017805,rank=6\n",
      "[SoftImpute] Iter 172: observed MAE=0.004838 validation MAE=0.017789,rank=6\n",
      "[SoftImpute] Iter 173: observed MAE=0.004834 validation MAE=0.017773,rank=6\n",
      "[SoftImpute] Iter 174: observed MAE=0.004830 validation MAE=0.017757,rank=6\n",
      "[SoftImpute] Iter 175: observed MAE=0.004827 validation MAE=0.017742,rank=6\n",
      "[SoftImpute] Iter 176: observed MAE=0.004823 validation MAE=0.017726,rank=6\n",
      "[SoftImpute] Iter 177: observed MAE=0.004819 validation MAE=0.017711,rank=6\n",
      "[SoftImpute] Iter 178: observed MAE=0.004816 validation MAE=0.017696,rank=6\n",
      "[SoftImpute] Iter 179: observed MAE=0.004812 validation MAE=0.017681,rank=6\n",
      "[SoftImpute] Iter 180: observed MAE=0.004809 validation MAE=0.017666,rank=6\n",
      "[SoftImpute] Iter 181: observed MAE=0.004805 validation MAE=0.017651,rank=6\n",
      "[SoftImpute] Iter 182: observed MAE=0.004802 validation MAE=0.017636,rank=6\n",
      "[SoftImpute] Iter 183: observed MAE=0.004798 validation MAE=0.017621,rank=6\n",
      "[SoftImpute] Iter 184: observed MAE=0.004795 validation MAE=0.017607,rank=6\n",
      "[SoftImpute] Iter 185: observed MAE=0.004792 validation MAE=0.017592,rank=6\n",
      "[SoftImpute] Iter 186: observed MAE=0.004789 validation MAE=0.017578,rank=6\n",
      "[SoftImpute] Iter 187: observed MAE=0.004785 validation MAE=0.017564,rank=6\n",
      "[SoftImpute] Iter 188: observed MAE=0.004782 validation MAE=0.017549,rank=6\n",
      "[SoftImpute] Iter 189: observed MAE=0.004779 validation MAE=0.017535,rank=6\n",
      "[SoftImpute] Iter 190: observed MAE=0.004776 validation MAE=0.017521,rank=6\n",
      "[SoftImpute] Iter 191: observed MAE=0.004773 validation MAE=0.017508,rank=6\n",
      "[SoftImpute] Iter 192: observed MAE=0.004770 validation MAE=0.017494,rank=6\n",
      "[SoftImpute] Iter 193: observed MAE=0.004767 validation MAE=0.017480,rank=6\n",
      "[SoftImpute] Iter 194: observed MAE=0.004764 validation MAE=0.017467,rank=6\n",
      "[SoftImpute] Iter 195: observed MAE=0.004761 validation MAE=0.017453,rank=6\n",
      "[SoftImpute] Iter 196: observed MAE=0.004758 validation MAE=0.017439,rank=6\n",
      "[SoftImpute] Iter 197: observed MAE=0.004755 validation MAE=0.017426,rank=6\n",
      "[SoftImpute] Iter 198: observed MAE=0.004752 validation MAE=0.017413,rank=6\n",
      "[SoftImpute] Iter 199: observed MAE=0.004750 validation MAE=0.017400,rank=6\n",
      "[SoftImpute] Iter 200: observed MAE=0.004747 validation MAE=0.017386,rank=6\n",
      "[SoftImpute] Iter 201: observed MAE=0.004744 validation MAE=0.017373,rank=6\n",
      "[SoftImpute] Iter 202: observed MAE=0.004741 validation MAE=0.017360,rank=6\n",
      "[SoftImpute] Iter 203: observed MAE=0.004739 validation MAE=0.017347,rank=6\n",
      "[SoftImpute] Iter 204: observed MAE=0.004736 validation MAE=0.017335,rank=6\n",
      "[SoftImpute] Iter 205: observed MAE=0.004733 validation MAE=0.017322,rank=6\n",
      "[SoftImpute] Iter 206: observed MAE=0.004731 validation MAE=0.017309,rank=6\n",
      "[SoftImpute] Iter 207: observed MAE=0.004728 validation MAE=0.017296,rank=6\n",
      "[SoftImpute] Iter 208: observed MAE=0.004726 validation MAE=0.017283,rank=6\n",
      "[SoftImpute] Iter 209: observed MAE=0.004723 validation MAE=0.017271,rank=6\n",
      "[SoftImpute] Iter 210: observed MAE=0.004721 validation MAE=0.017258,rank=6\n",
      "[SoftImpute] Iter 211: observed MAE=0.004718 validation MAE=0.017246,rank=6\n",
      "[SoftImpute] Iter 212: observed MAE=0.004716 validation MAE=0.017234,rank=6\n",
      "[SoftImpute] Iter 213: observed MAE=0.004714 validation MAE=0.017222,rank=6\n",
      "[SoftImpute] Iter 214: observed MAE=0.004711 validation MAE=0.017210,rank=6\n",
      "[SoftImpute] Iter 215: observed MAE=0.004709 validation MAE=0.017198,rank=6\n",
      "[SoftImpute] Iter 216: observed MAE=0.004707 validation MAE=0.017186,rank=6\n",
      "[SoftImpute] Iter 217: observed MAE=0.004704 validation MAE=0.017174,rank=6\n",
      "[SoftImpute] Iter 218: observed MAE=0.004702 validation MAE=0.017162,rank=6\n",
      "[SoftImpute] Iter 219: observed MAE=0.004700 validation MAE=0.017150,rank=6\n",
      "[SoftImpute] Iter 220: observed MAE=0.004697 validation MAE=0.017138,rank=6\n",
      "[SoftImpute] Iter 221: observed MAE=0.004695 validation MAE=0.017127,rank=6\n",
      "[SoftImpute] Iter 222: observed MAE=0.004693 validation MAE=0.017115,rank=6\n",
      "[SoftImpute] Iter 223: observed MAE=0.004691 validation MAE=0.017104,rank=6\n",
      "[SoftImpute] Iter 224: observed MAE=0.004688 validation MAE=0.017093,rank=6\n",
      "[SoftImpute] Iter 225: observed MAE=0.004686 validation MAE=0.017081,rank=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 226: observed MAE=0.004684 validation MAE=0.017070,rank=6\n",
      "[SoftImpute] Iter 227: observed MAE=0.004682 validation MAE=0.017059,rank=6\n",
      "[SoftImpute] Iter 228: observed MAE=0.004680 validation MAE=0.017048,rank=6\n",
      "[SoftImpute] Iter 229: observed MAE=0.004678 validation MAE=0.017037,rank=6\n",
      "[SoftImpute] Iter 230: observed MAE=0.004676 validation MAE=0.017026,rank=6\n",
      "[SoftImpute] Iter 231: observed MAE=0.004674 validation MAE=0.017015,rank=6\n",
      "[SoftImpute] Iter 232: observed MAE=0.004672 validation MAE=0.017004,rank=6\n",
      "[SoftImpute] Iter 233: observed MAE=0.004670 validation MAE=0.016993,rank=6\n",
      "[SoftImpute] Iter 234: observed MAE=0.004668 validation MAE=0.016982,rank=6\n",
      "[SoftImpute] Iter 235: observed MAE=0.004666 validation MAE=0.016972,rank=6\n",
      "[SoftImpute] Iter 236: observed MAE=0.004664 validation MAE=0.016961,rank=6\n",
      "[SoftImpute] Iter 237: observed MAE=0.004662 validation MAE=0.016951,rank=6\n",
      "[SoftImpute] Iter 238: observed MAE=0.004660 validation MAE=0.016940,rank=6\n",
      "[SoftImpute] Iter 239: observed MAE=0.004658 validation MAE=0.016930,rank=6\n",
      "[SoftImpute] Iter 240: observed MAE=0.004656 validation MAE=0.016919,rank=6\n",
      "[SoftImpute] Iter 241: observed MAE=0.004654 validation MAE=0.016909,rank=6\n",
      "[SoftImpute] Iter 242: observed MAE=0.004652 validation MAE=0.016899,rank=6\n",
      "[SoftImpute] Iter 243: observed MAE=0.004650 validation MAE=0.016888,rank=6\n",
      "[SoftImpute] Iter 244: observed MAE=0.004648 validation MAE=0.016878,rank=6\n",
      "[SoftImpute] Iter 245: observed MAE=0.004646 validation MAE=0.016868,rank=6\n",
      "[SoftImpute] Iter 246: observed MAE=0.004644 validation MAE=0.016858,rank=6\n",
      "[SoftImpute] Iter 247: observed MAE=0.004642 validation MAE=0.016847,rank=6\n",
      "[SoftImpute] Iter 248: observed MAE=0.004641 validation MAE=0.016837,rank=6\n",
      "[SoftImpute] Iter 249: observed MAE=0.004639 validation MAE=0.016827,rank=6\n",
      "[SoftImpute] Iter 250: observed MAE=0.004637 validation MAE=0.016817,rank=6\n",
      "[SoftImpute] Iter 251: observed MAE=0.004635 validation MAE=0.016807,rank=6\n",
      "[SoftImpute] Iter 252: observed MAE=0.004633 validation MAE=0.016797,rank=6\n",
      "[SoftImpute] Iter 253: observed MAE=0.004631 validation MAE=0.016787,rank=6\n",
      "[SoftImpute] Iter 254: observed MAE=0.004630 validation MAE=0.016777,rank=6\n",
      "[SoftImpute] Iter 255: observed MAE=0.004628 validation MAE=0.016768,rank=6\n",
      "[SoftImpute] Iter 256: observed MAE=0.004626 validation MAE=0.016758,rank=6\n",
      "[SoftImpute] Iter 257: observed MAE=0.004624 validation MAE=0.016748,rank=6\n",
      "[SoftImpute] Iter 258: observed MAE=0.004623 validation MAE=0.016738,rank=6\n",
      "[SoftImpute] Iter 259: observed MAE=0.004621 validation MAE=0.016728,rank=6\n",
      "[SoftImpute] Iter 260: observed MAE=0.004619 validation MAE=0.016719,rank=6\n",
      "[SoftImpute] Iter 261: observed MAE=0.004618 validation MAE=0.016709,rank=6\n",
      "[SoftImpute] Iter 262: observed MAE=0.004616 validation MAE=0.016699,rank=6\n",
      "[SoftImpute] Iter 263: observed MAE=0.004614 validation MAE=0.016690,rank=6\n",
      "[SoftImpute] Iter 264: observed MAE=0.004613 validation MAE=0.016680,rank=6\n",
      "[SoftImpute] Iter 265: observed MAE=0.004611 validation MAE=0.016671,rank=6\n",
      "[SoftImpute] Iter 266: observed MAE=0.004609 validation MAE=0.016661,rank=6\n",
      "[SoftImpute] Iter 267: observed MAE=0.004608 validation MAE=0.016652,rank=6\n",
      "[SoftImpute] Iter 268: observed MAE=0.004606 validation MAE=0.016643,rank=6\n",
      "[SoftImpute] Iter 269: observed MAE=0.004604 validation MAE=0.016633,rank=6\n",
      "[SoftImpute] Iter 270: observed MAE=0.004603 validation MAE=0.016624,rank=6\n",
      "[SoftImpute] Iter 271: observed MAE=0.004601 validation MAE=0.016615,rank=6\n",
      "[SoftImpute] Iter 272: observed MAE=0.004600 validation MAE=0.016606,rank=6\n",
      "[SoftImpute] Iter 273: observed MAE=0.004598 validation MAE=0.016597,rank=6\n",
      "[SoftImpute] Iter 274: observed MAE=0.004597 validation MAE=0.016588,rank=6\n",
      "[SoftImpute] Iter 275: observed MAE=0.004595 validation MAE=0.016580,rank=6\n",
      "[SoftImpute] Iter 276: observed MAE=0.004593 validation MAE=0.016571,rank=6\n",
      "[SoftImpute] Iter 277: observed MAE=0.004592 validation MAE=0.016563,rank=6\n",
      "[SoftImpute] Iter 278: observed MAE=0.004590 validation MAE=0.016554,rank=6\n",
      "[SoftImpute] Iter 279: observed MAE=0.004589 validation MAE=0.016546,rank=6\n",
      "[SoftImpute] Iter 280: observed MAE=0.004587 validation MAE=0.016537,rank=6\n",
      "[SoftImpute] Iter 281: observed MAE=0.004586 validation MAE=0.016529,rank=6\n",
      "[SoftImpute] Iter 282: observed MAE=0.004584 validation MAE=0.016521,rank=6\n",
      "[SoftImpute] Iter 283: observed MAE=0.004583 validation MAE=0.016513,rank=6\n",
      "[SoftImpute] Iter 284: observed MAE=0.004581 validation MAE=0.016504,rank=6\n",
      "[SoftImpute] Iter 285: observed MAE=0.004580 validation MAE=0.016496,rank=6\n",
      "[SoftImpute] Iter 286: observed MAE=0.004578 validation MAE=0.016488,rank=6\n",
      "[SoftImpute] Iter 287: observed MAE=0.004577 validation MAE=0.016480,rank=6\n",
      "[SoftImpute] Iter 288: observed MAE=0.004575 validation MAE=0.016472,rank=6\n",
      "[SoftImpute] Iter 289: observed MAE=0.004574 validation MAE=0.016464,rank=6\n",
      "[SoftImpute] Iter 290: observed MAE=0.004572 validation MAE=0.016456,rank=6\n",
      "[SoftImpute] Iter 291: observed MAE=0.004571 validation MAE=0.016449,rank=6\n",
      "[SoftImpute] Stopped after iteration 291 for lambda=0.039366\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 8.937804937362671\n",
      "After the matrix factor stage, training error is 0.00457, validation error is 0.01645\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.34448, val loss: 0.34765\n",
      "Main effects training epoch: 2, train loss: 0.27354, val loss: 0.27853\n",
      "Main effects training epoch: 3, train loss: 0.20823, val loss: 0.21145\n",
      "Main effects training epoch: 4, train loss: 0.16135, val loss: 0.16514\n",
      "Main effects training epoch: 5, train loss: 0.14048, val loss: 0.14025\n",
      "Main effects training epoch: 6, train loss: 0.13280, val loss: 0.13280\n",
      "Main effects training epoch: 7, train loss: 0.13041, val loss: 0.12922\n",
      "Main effects training epoch: 8, train loss: 0.12977, val loss: 0.12861\n",
      "Main effects training epoch: 9, train loss: 0.12914, val loss: 0.12821\n",
      "Main effects training epoch: 10, train loss: 0.12852, val loss: 0.12844\n",
      "Main effects training epoch: 11, train loss: 0.12742, val loss: 0.12590\n",
      "Main effects training epoch: 12, train loss: 0.12612, val loss: 0.12564\n",
      "Main effects training epoch: 13, train loss: 0.12339, val loss: 0.12252\n",
      "Main effects training epoch: 14, train loss: 0.11835, val loss: 0.11883\n",
      "Main effects training epoch: 15, train loss: 0.11578, val loss: 0.11651\n",
      "Main effects training epoch: 16, train loss: 0.11289, val loss: 0.11344\n",
      "Main effects training epoch: 17, train loss: 0.11081, val loss: 0.11186\n",
      "Main effects training epoch: 18, train loss: 0.11088, val loss: 0.11312\n",
      "Main effects training epoch: 19, train loss: 0.11027, val loss: 0.11199\n",
      "Main effects training epoch: 20, train loss: 0.11090, val loss: 0.11238\n",
      "Main effects training epoch: 21, train loss: 0.10922, val loss: 0.11096\n",
      "Main effects training epoch: 22, train loss: 0.10900, val loss: 0.11106\n",
      "Main effects training epoch: 23, train loss: 0.10645, val loss: 0.10812\n",
      "Main effects training epoch: 24, train loss: 0.10707, val loss: 0.10858\n",
      "Main effects training epoch: 25, train loss: 0.10776, val loss: 0.10768\n",
      "Main effects training epoch: 26, train loss: 0.10556, val loss: 0.10715\n",
      "Main effects training epoch: 27, train loss: 0.10595, val loss: 0.10680\n",
      "Main effects training epoch: 28, train loss: 0.10509, val loss: 0.10619\n",
      "Main effects training epoch: 29, train loss: 0.10460, val loss: 0.10568\n",
      "Main effects training epoch: 30, train loss: 0.10512, val loss: 0.10585\n",
      "Main effects training epoch: 31, train loss: 0.10437, val loss: 0.10598\n",
      "Main effects training epoch: 32, train loss: 0.10431, val loss: 0.10577\n",
      "Main effects training epoch: 33, train loss: 0.10423, val loss: 0.10545\n",
      "Main effects training epoch: 34, train loss: 0.10413, val loss: 0.10561\n",
      "Main effects training epoch: 35, train loss: 0.10424, val loss: 0.10579\n",
      "Main effects training epoch: 36, train loss: 0.10444, val loss: 0.10578\n",
      "Main effects training epoch: 37, train loss: 0.10400, val loss: 0.10575\n",
      "Main effects training epoch: 38, train loss: 0.10438, val loss: 0.10591\n",
      "Main effects training epoch: 39, train loss: 0.10416, val loss: 0.10566\n",
      "Main effects training epoch: 40, train loss: 0.10393, val loss: 0.10524\n",
      "Main effects training epoch: 41, train loss: 0.10418, val loss: 0.10579\n",
      "Main effects training epoch: 42, train loss: 0.10392, val loss: 0.10565\n",
      "Main effects training epoch: 43, train loss: 0.10430, val loss: 0.10612\n",
      "Main effects training epoch: 44, train loss: 0.10409, val loss: 0.10553\n",
      "Main effects training epoch: 45, train loss: 0.10403, val loss: 0.10539\n",
      "Main effects training epoch: 46, train loss: 0.10400, val loss: 0.10547\n",
      "Main effects training epoch: 47, train loss: 0.10472, val loss: 0.10625\n",
      "Main effects training epoch: 48, train loss: 0.10410, val loss: 0.10597\n",
      "Main effects training epoch: 49, train loss: 0.10415, val loss: 0.10512\n",
      "Main effects training epoch: 50, train loss: 0.10408, val loss: 0.10560\n",
      "Main effects training epoch: 51, train loss: 0.10392, val loss: 0.10554\n",
      "Main effects training epoch: 52, train loss: 0.10395, val loss: 0.10519\n",
      "Main effects training epoch: 53, train loss: 0.10385, val loss: 0.10520\n",
      "Main effects training epoch: 54, train loss: 0.10398, val loss: 0.10563\n",
      "Main effects training epoch: 55, train loss: 0.10416, val loss: 0.10523\n",
      "Main effects training epoch: 56, train loss: 0.10404, val loss: 0.10586\n",
      "Main effects training epoch: 57, train loss: 0.10399, val loss: 0.10571\n",
      "Main effects training epoch: 58, train loss: 0.10443, val loss: 0.10631\n",
      "Main effects training epoch: 59, train loss: 0.10393, val loss: 0.10549\n",
      "Main effects training epoch: 60, train loss: 0.10386, val loss: 0.10534\n",
      "Main effects training epoch: 61, train loss: 0.10389, val loss: 0.10538\n",
      "Main effects training epoch: 62, train loss: 0.10417, val loss: 0.10552\n",
      "Main effects training epoch: 63, train loss: 0.10396, val loss: 0.10570\n",
      "Main effects training epoch: 64, train loss: 0.10412, val loss: 0.10546\n",
      "Main effects training epoch: 65, train loss: 0.10403, val loss: 0.10549\n",
      "Main effects training epoch: 66, train loss: 0.10398, val loss: 0.10514\n",
      "Main effects training epoch: 67, train loss: 0.10385, val loss: 0.10514\n",
      "Main effects training epoch: 68, train loss: 0.10396, val loss: 0.10561\n",
      "Main effects training epoch: 69, train loss: 0.10397, val loss: 0.10523\n",
      "Main effects training epoch: 70, train loss: 0.10420, val loss: 0.10542\n",
      "Main effects training epoch: 71, train loss: 0.10473, val loss: 0.10671\n",
      "Main effects training epoch: 72, train loss: 0.10418, val loss: 0.10552\n",
      "Main effects training epoch: 73, train loss: 0.10427, val loss: 0.10590\n",
      "Main effects training epoch: 74, train loss: 0.10409, val loss: 0.10540\n",
      "Main effects training epoch: 75, train loss: 0.10401, val loss: 0.10542\n",
      "Main effects training epoch: 76, train loss: 0.10384, val loss: 0.10560\n",
      "Main effects training epoch: 77, train loss: 0.10404, val loss: 0.10542\n",
      "Main effects training epoch: 78, train loss: 0.10394, val loss: 0.10550\n",
      "Main effects training epoch: 79, train loss: 0.10410, val loss: 0.10553\n",
      "Main effects training epoch: 80, train loss: 0.10425, val loss: 0.10542\n",
      "Main effects training epoch: 81, train loss: 0.10387, val loss: 0.10530\n",
      "Main effects training epoch: 82, train loss: 0.10409, val loss: 0.10607\n",
      "Main effects training epoch: 83, train loss: 0.10386, val loss: 0.10559\n",
      "Main effects training epoch: 84, train loss: 0.10405, val loss: 0.10549\n",
      "Main effects training epoch: 85, train loss: 0.10483, val loss: 0.10617\n",
      "Main effects training epoch: 86, train loss: 0.10383, val loss: 0.10551\n",
      "Main effects training epoch: 87, train loss: 0.10415, val loss: 0.10524\n",
      "Main effects training epoch: 88, train loss: 0.10389, val loss: 0.10586\n",
      "Main effects training epoch: 89, train loss: 0.10383, val loss: 0.10548\n",
      "Main effects training epoch: 90, train loss: 0.10400, val loss: 0.10512\n",
      "Main effects training epoch: 91, train loss: 0.10387, val loss: 0.10538\n",
      "Main effects training epoch: 92, train loss: 0.10420, val loss: 0.10635\n",
      "Main effects training epoch: 93, train loss: 0.10396, val loss: 0.10529\n",
      "Main effects training epoch: 94, train loss: 0.10417, val loss: 0.10594\n",
      "Main effects training epoch: 95, train loss: 0.10387, val loss: 0.10562\n",
      "Main effects training epoch: 96, train loss: 0.10378, val loss: 0.10533\n",
      "Main effects training epoch: 97, train loss: 0.10386, val loss: 0.10536\n",
      "Main effects training epoch: 98, train loss: 0.10425, val loss: 0.10630\n",
      "Main effects training epoch: 99, train loss: 0.10426, val loss: 0.10552\n",
      "Main effects training epoch: 100, train loss: 0.10419, val loss: 0.10632\n",
      "Main effects training epoch: 101, train loss: 0.10395, val loss: 0.10553\n",
      "Main effects training epoch: 102, train loss: 0.10397, val loss: 0.10584\n",
      "Main effects training epoch: 103, train loss: 0.10408, val loss: 0.10574\n",
      "Main effects training epoch: 104, train loss: 0.10395, val loss: 0.10576\n",
      "Main effects training epoch: 105, train loss: 0.10380, val loss: 0.10555\n",
      "Main effects training epoch: 106, train loss: 0.10382, val loss: 0.10590\n",
      "Main effects training epoch: 107, train loss: 0.10417, val loss: 0.10600\n",
      "Main effects training epoch: 108, train loss: 0.10410, val loss: 0.10567\n",
      "Main effects training epoch: 109, train loss: 0.10373, val loss: 0.10542\n",
      "Main effects training epoch: 110, train loss: 0.10377, val loss: 0.10525\n",
      "Main effects training epoch: 111, train loss: 0.10405, val loss: 0.10577\n",
      "Main effects training epoch: 112, train loss: 0.10409, val loss: 0.10590\n",
      "Main effects training epoch: 113, train loss: 0.10474, val loss: 0.10615\n",
      "Main effects training epoch: 114, train loss: 0.10406, val loss: 0.10594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 115, train loss: 0.10415, val loss: 0.10553\n",
      "Main effects training epoch: 116, train loss: 0.10437, val loss: 0.10680\n",
      "Main effects training epoch: 117, train loss: 0.10399, val loss: 0.10532\n",
      "Main effects training epoch: 118, train loss: 0.10393, val loss: 0.10583\n",
      "Main effects training epoch: 119, train loss: 0.10410, val loss: 0.10601\n",
      "Main effects training epoch: 120, train loss: 0.10375, val loss: 0.10578\n",
      "Main effects training epoch: 121, train loss: 0.10382, val loss: 0.10529\n",
      "Main effects training epoch: 122, train loss: 0.10391, val loss: 0.10552\n",
      "Main effects training epoch: 123, train loss: 0.10397, val loss: 0.10615\n",
      "Main effects training epoch: 124, train loss: 0.10422, val loss: 0.10572\n",
      "Main effects training epoch: 125, train loss: 0.10414, val loss: 0.10601\n",
      "Main effects training epoch: 126, train loss: 0.10402, val loss: 0.10605\n",
      "Main effects training epoch: 127, train loss: 0.10402, val loss: 0.10531\n",
      "Main effects training epoch: 128, train loss: 0.10395, val loss: 0.10615\n",
      "Main effects training epoch: 129, train loss: 0.10410, val loss: 0.10643\n",
      "Main effects training epoch: 130, train loss: 0.10401, val loss: 0.10526\n",
      "Main effects training epoch: 131, train loss: 0.10380, val loss: 0.10577\n",
      "Main effects training epoch: 132, train loss: 0.10404, val loss: 0.10554\n",
      "Main effects training epoch: 133, train loss: 0.10425, val loss: 0.10648\n",
      "Main effects training epoch: 134, train loss: 0.10454, val loss: 0.10616\n",
      "Main effects training epoch: 135, train loss: 0.10424, val loss: 0.10653\n",
      "Main effects training epoch: 136, train loss: 0.10426, val loss: 0.10565\n",
      "Main effects training epoch: 137, train loss: 0.10444, val loss: 0.10639\n",
      "Main effects training epoch: 138, train loss: 0.10433, val loss: 0.10622\n",
      "Main effects training epoch: 139, train loss: 0.10384, val loss: 0.10565\n",
      "Main effects training epoch: 140, train loss: 0.10392, val loss: 0.10539\n",
      "Main effects training epoch: 141, train loss: 0.10420, val loss: 0.10593\n",
      "Main effects training epoch: 142, train loss: 0.10404, val loss: 0.10617\n",
      "Main effects training epoch: 143, train loss: 0.10420, val loss: 0.10567\n",
      "Main effects training epoch: 144, train loss: 0.10383, val loss: 0.10601\n",
      "Main effects training epoch: 145, train loss: 0.10370, val loss: 0.10530\n",
      "Main effects training epoch: 146, train loss: 0.10381, val loss: 0.10596\n",
      "Main effects training epoch: 147, train loss: 0.10380, val loss: 0.10575\n",
      "Main effects training epoch: 148, train loss: 0.10377, val loss: 0.10571\n",
      "Main effects training epoch: 149, train loss: 0.10383, val loss: 0.10566\n",
      "Main effects training epoch: 150, train loss: 0.10393, val loss: 0.10604\n",
      "Early stop at epoch 150, with validation loss: 0.10604\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.10405, val loss: 0.10498\n",
      "Main effects tuning epoch: 2, train loss: 0.10423, val loss: 0.10552\n",
      "Main effects tuning epoch: 3, train loss: 0.10411, val loss: 0.10505\n",
      "Main effects tuning epoch: 4, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 5, train loss: 0.10382, val loss: 0.10498\n",
      "Main effects tuning epoch: 6, train loss: 0.10425, val loss: 0.10582\n",
      "Main effects tuning epoch: 7, train loss: 0.10408, val loss: 0.10530\n",
      "Main effects tuning epoch: 8, train loss: 0.10397, val loss: 0.10527\n",
      "Main effects tuning epoch: 9, train loss: 0.10401, val loss: 0.10507\n",
      "Main effects tuning epoch: 10, train loss: 0.10407, val loss: 0.10539\n",
      "Main effects tuning epoch: 11, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 12, train loss: 0.10393, val loss: 0.10556\n",
      "Main effects tuning epoch: 13, train loss: 0.10415, val loss: 0.10510\n",
      "Main effects tuning epoch: 14, train loss: 0.10397, val loss: 0.10570\n",
      "Main effects tuning epoch: 15, train loss: 0.10386, val loss: 0.10516\n",
      "Main effects tuning epoch: 16, train loss: 0.10395, val loss: 0.10562\n",
      "Main effects tuning epoch: 17, train loss: 0.10395, val loss: 0.10539\n",
      "Main effects tuning epoch: 18, train loss: 0.10422, val loss: 0.10568\n",
      "Main effects tuning epoch: 19, train loss: 0.10433, val loss: 0.10517\n",
      "Main effects tuning epoch: 20, train loss: 0.10384, val loss: 0.10568\n",
      "Main effects tuning epoch: 21, train loss: 0.10400, val loss: 0.10541\n",
      "Main effects tuning epoch: 22, train loss: 0.10393, val loss: 0.10526\n",
      "Main effects tuning epoch: 23, train loss: 0.10434, val loss: 0.10582\n",
      "Main effects tuning epoch: 24, train loss: 0.10406, val loss: 0.10540\n",
      "Main effects tuning epoch: 25, train loss: 0.10410, val loss: 0.10570\n",
      "Main effects tuning epoch: 26, train loss: 0.10405, val loss: 0.10569\n",
      "Main effects tuning epoch: 27, train loss: 0.10406, val loss: 0.10556\n",
      "Main effects tuning epoch: 28, train loss: 0.10394, val loss: 0.10542\n",
      "Main effects tuning epoch: 29, train loss: 0.10383, val loss: 0.10542\n",
      "Main effects tuning epoch: 30, train loss: 0.10385, val loss: 0.10548\n",
      "Main effects tuning epoch: 31, train loss: 0.10395, val loss: 0.10516\n",
      "Main effects tuning epoch: 32, train loss: 0.10391, val loss: 0.10584\n",
      "Main effects tuning epoch: 33, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 34, train loss: 0.10391, val loss: 0.10561\n",
      "Main effects tuning epoch: 35, train loss: 0.10424, val loss: 0.10531\n",
      "Main effects tuning epoch: 36, train loss: 0.10395, val loss: 0.10554\n",
      "Main effects tuning epoch: 37, train loss: 0.10400, val loss: 0.10600\n",
      "Main effects tuning epoch: 38, train loss: 0.10408, val loss: 0.10530\n",
      "Main effects tuning epoch: 39, train loss: 0.10394, val loss: 0.10565\n",
      "Main effects tuning epoch: 40, train loss: 0.10385, val loss: 0.10569\n",
      "Main effects tuning epoch: 41, train loss: 0.10390, val loss: 0.10539\n",
      "Main effects tuning epoch: 42, train loss: 0.10400, val loss: 0.10587\n",
      "Main effects tuning epoch: 43, train loss: 0.10386, val loss: 0.10547\n",
      "Main effects tuning epoch: 44, train loss: 0.10403, val loss: 0.10597\n",
      "Main effects tuning epoch: 45, train loss: 0.10384, val loss: 0.10541\n",
      "Main effects tuning epoch: 46, train loss: 0.10398, val loss: 0.10582\n",
      "Main effects tuning epoch: 47, train loss: 0.10375, val loss: 0.10527\n",
      "Main effects tuning epoch: 48, train loss: 0.10380, val loss: 0.10549\n",
      "Main effects tuning epoch: 49, train loss: 0.10396, val loss: 0.10566\n",
      "Main effects tuning epoch: 50, train loss: 0.10407, val loss: 0.10549\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.09376, val loss: 0.09232\n",
      "Interaction training epoch: 2, train loss: 0.20140, val loss: 0.20111\n",
      "Interaction training epoch: 3, train loss: 0.06617, val loss: 0.06860\n",
      "Interaction training epoch: 4, train loss: 0.05173, val loss: 0.05197\n",
      "Interaction training epoch: 5, train loss: 0.06497, val loss: 0.06595\n",
      "Interaction training epoch: 6, train loss: 0.04386, val loss: 0.04449\n",
      "Interaction training epoch: 7, train loss: 0.04606, val loss: 0.04529\n",
      "Interaction training epoch: 8, train loss: 0.04492, val loss: 0.04489\n",
      "Interaction training epoch: 9, train loss: 0.04997, val loss: 0.04868\n",
      "Interaction training epoch: 10, train loss: 0.04002, val loss: 0.04133\n",
      "Interaction training epoch: 11, train loss: 0.04269, val loss: 0.04306\n",
      "Interaction training epoch: 12, train loss: 0.04100, val loss: 0.03997\n",
      "Interaction training epoch: 13, train loss: 0.04366, val loss: 0.04360\n",
      "Interaction training epoch: 14, train loss: 0.04928, val loss: 0.05084\n",
      "Interaction training epoch: 15, train loss: 0.04532, val loss: 0.04488\n",
      "Interaction training epoch: 16, train loss: 0.04530, val loss: 0.04524\n",
      "Interaction training epoch: 17, train loss: 0.03784, val loss: 0.03763\n",
      "Interaction training epoch: 18, train loss: 0.04939, val loss: 0.05023\n",
      "Interaction training epoch: 19, train loss: 0.03608, val loss: 0.03597\n",
      "Interaction training epoch: 20, train loss: 0.04344, val loss: 0.04367\n",
      "Interaction training epoch: 21, train loss: 0.04002, val loss: 0.03932\n",
      "Interaction training epoch: 22, train loss: 0.03977, val loss: 0.03977\n",
      "Interaction training epoch: 23, train loss: 0.03777, val loss: 0.03709\n",
      "Interaction training epoch: 24, train loss: 0.03927, val loss: 0.03924\n",
      "Interaction training epoch: 25, train loss: 0.04164, val loss: 0.04112\n",
      "Interaction training epoch: 26, train loss: 0.04429, val loss: 0.04389\n",
      "Interaction training epoch: 27, train loss: 0.04417, val loss: 0.04416\n",
      "Interaction training epoch: 28, train loss: 0.04241, val loss: 0.04112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 29, train loss: 0.03548, val loss: 0.03672\n",
      "Interaction training epoch: 30, train loss: 0.04420, val loss: 0.04353\n",
      "Interaction training epoch: 31, train loss: 0.04020, val loss: 0.04004\n",
      "Interaction training epoch: 32, train loss: 0.03422, val loss: 0.03451\n",
      "Interaction training epoch: 33, train loss: 0.03378, val loss: 0.03407\n",
      "Interaction training epoch: 34, train loss: 0.03925, val loss: 0.03880\n",
      "Interaction training epoch: 35, train loss: 0.03752, val loss: 0.03636\n",
      "Interaction training epoch: 36, train loss: 0.03582, val loss: 0.03564\n",
      "Interaction training epoch: 37, train loss: 0.03069, val loss: 0.03087\n",
      "Interaction training epoch: 38, train loss: 0.03729, val loss: 0.03652\n",
      "Interaction training epoch: 39, train loss: 0.03772, val loss: 0.03772\n",
      "Interaction training epoch: 40, train loss: 0.04332, val loss: 0.04201\n",
      "Interaction training epoch: 41, train loss: 0.03409, val loss: 0.03435\n",
      "Interaction training epoch: 42, train loss: 0.03501, val loss: 0.03419\n",
      "Interaction training epoch: 43, train loss: 0.03630, val loss: 0.03573\n",
      "Interaction training epoch: 44, train loss: 0.03447, val loss: 0.03420\n",
      "Interaction training epoch: 45, train loss: 0.03622, val loss: 0.03541\n",
      "Interaction training epoch: 46, train loss: 0.02991, val loss: 0.03002\n",
      "Interaction training epoch: 47, train loss: 0.02880, val loss: 0.02856\n",
      "Interaction training epoch: 48, train loss: 0.03548, val loss: 0.03469\n",
      "Interaction training epoch: 49, train loss: 0.03399, val loss: 0.03256\n",
      "Interaction training epoch: 50, train loss: 0.03225, val loss: 0.03266\n",
      "Interaction training epoch: 51, train loss: 0.03286, val loss: 0.03176\n",
      "Interaction training epoch: 52, train loss: 0.03084, val loss: 0.03012\n",
      "Interaction training epoch: 53, train loss: 0.03059, val loss: 0.02998\n",
      "Interaction training epoch: 54, train loss: 0.02966, val loss: 0.02932\n",
      "Interaction training epoch: 55, train loss: 0.04122, val loss: 0.04119\n",
      "Interaction training epoch: 56, train loss: 0.03164, val loss: 0.03059\n",
      "Interaction training epoch: 57, train loss: 0.03483, val loss: 0.03469\n",
      "Interaction training epoch: 58, train loss: 0.03320, val loss: 0.03294\n",
      "Interaction training epoch: 59, train loss: 0.02999, val loss: 0.02930\n",
      "Interaction training epoch: 60, train loss: 0.03540, val loss: 0.03496\n",
      "Interaction training epoch: 61, train loss: 0.03142, val loss: 0.02962\n",
      "Interaction training epoch: 62, train loss: 0.03146, val loss: 0.03164\n",
      "Interaction training epoch: 63, train loss: 0.03481, val loss: 0.03421\n",
      "Interaction training epoch: 64, train loss: 0.02946, val loss: 0.02881\n",
      "Interaction training epoch: 65, train loss: 0.03539, val loss: 0.03511\n",
      "Interaction training epoch: 66, train loss: 0.03797, val loss: 0.03805\n",
      "Interaction training epoch: 67, train loss: 0.03669, val loss: 0.03627\n",
      "Interaction training epoch: 68, train loss: 0.04844, val loss: 0.04881\n",
      "Interaction training epoch: 69, train loss: 0.02947, val loss: 0.02941\n",
      "Interaction training epoch: 70, train loss: 0.03805, val loss: 0.03672\n",
      "Interaction training epoch: 71, train loss: 0.02990, val loss: 0.02970\n",
      "Interaction training epoch: 72, train loss: 0.03926, val loss: 0.03952\n",
      "Interaction training epoch: 73, train loss: 0.03713, val loss: 0.03732\n",
      "Interaction training epoch: 74, train loss: 0.04469, val loss: 0.04392\n",
      "Interaction training epoch: 75, train loss: 0.03518, val loss: 0.03491\n",
      "Interaction training epoch: 76, train loss: 0.03954, val loss: 0.03893\n",
      "Interaction training epoch: 77, train loss: 0.04294, val loss: 0.04248\n",
      "Interaction training epoch: 78, train loss: 0.03122, val loss: 0.03170\n",
      "Interaction training epoch: 79, train loss: 0.03577, val loss: 0.03538\n",
      "Interaction training epoch: 80, train loss: 0.04727, val loss: 0.04664\n",
      "Interaction training epoch: 81, train loss: 0.03478, val loss: 0.03412\n",
      "Interaction training epoch: 82, train loss: 0.03736, val loss: 0.03700\n",
      "Interaction training epoch: 83, train loss: 0.03716, val loss: 0.03665\n",
      "Interaction training epoch: 84, train loss: 0.03675, val loss: 0.03575\n",
      "Interaction training epoch: 85, train loss: 0.03815, val loss: 0.03817\n",
      "Interaction training epoch: 86, train loss: 0.03960, val loss: 0.03903\n",
      "Interaction training epoch: 87, train loss: 0.03498, val loss: 0.03402\n",
      "Interaction training epoch: 88, train loss: 0.03630, val loss: 0.03583\n",
      "Interaction training epoch: 89, train loss: 0.03454, val loss: 0.03517\n",
      "Interaction training epoch: 90, train loss: 0.07231, val loss: 0.07223\n",
      "Interaction training epoch: 91, train loss: 0.04800, val loss: 0.04802\n",
      "Interaction training epoch: 92, train loss: 0.05224, val loss: 0.05162\n",
      "Interaction training epoch: 93, train loss: 0.02794, val loss: 0.02804\n",
      "Interaction training epoch: 94, train loss: 0.03585, val loss: 0.03546\n",
      "Interaction training epoch: 95, train loss: 0.04149, val loss: 0.04158\n",
      "Interaction training epoch: 96, train loss: 0.04943, val loss: 0.04909\n",
      "Interaction training epoch: 97, train loss: 0.05722, val loss: 0.05690\n",
      "Interaction training epoch: 98, train loss: 0.04854, val loss: 0.04831\n",
      "Interaction training epoch: 99, train loss: 0.02719, val loss: 0.02716\n",
      "Interaction training epoch: 100, train loss: 0.03689, val loss: 0.03722\n",
      "Interaction training epoch: 101, train loss: 0.04609, val loss: 0.04601\n",
      "Interaction training epoch: 102, train loss: 0.03250, val loss: 0.03231\n",
      "Interaction training epoch: 103, train loss: 0.03006, val loss: 0.02936\n",
      "Interaction training epoch: 104, train loss: 0.03967, val loss: 0.03937\n",
      "Interaction training epoch: 105, train loss: 0.03096, val loss: 0.03100\n",
      "Interaction training epoch: 106, train loss: 0.05099, val loss: 0.05044\n",
      "Interaction training epoch: 107, train loss: 0.03358, val loss: 0.03389\n",
      "Interaction training epoch: 108, train loss: 0.04273, val loss: 0.04236\n",
      "Interaction training epoch: 109, train loss: 0.07259, val loss: 0.07140\n",
      "Interaction training epoch: 110, train loss: 0.03442, val loss: 0.03433\n",
      "Interaction training epoch: 111, train loss: 0.02947, val loss: 0.02893\n",
      "Interaction training epoch: 112, train loss: 0.03413, val loss: 0.03396\n",
      "Interaction training epoch: 113, train loss: 0.03441, val loss: 0.03428\n",
      "Interaction training epoch: 114, train loss: 0.05762, val loss: 0.05645\n",
      "Interaction training epoch: 115, train loss: 0.02938, val loss: 0.02918\n",
      "Interaction training epoch: 116, train loss: 0.04805, val loss: 0.04824\n",
      "Interaction training epoch: 117, train loss: 0.03927, val loss: 0.03835\n",
      "Interaction training epoch: 118, train loss: 0.03949, val loss: 0.03920\n",
      "Interaction training epoch: 119, train loss: 0.03835, val loss: 0.03852\n",
      "Interaction training epoch: 120, train loss: 0.07402, val loss: 0.07267\n",
      "Interaction training epoch: 121, train loss: 0.02786, val loss: 0.02794\n",
      "Interaction training epoch: 122, train loss: 0.02966, val loss: 0.02840\n",
      "Interaction training epoch: 123, train loss: 0.03146, val loss: 0.03080\n",
      "Interaction training epoch: 124, train loss: 0.03135, val loss: 0.03095\n",
      "Interaction training epoch: 125, train loss: 0.02893, val loss: 0.02832\n",
      "Interaction training epoch: 126, train loss: 0.06644, val loss: 0.06554\n",
      "Interaction training epoch: 127, train loss: 0.02439, val loss: 0.02405\n",
      "Interaction training epoch: 128, train loss: 0.02442, val loss: 0.02392\n",
      "Interaction training epoch: 129, train loss: 0.06718, val loss: 0.06662\n",
      "Interaction training epoch: 130, train loss: 0.06873, val loss: 0.06752\n",
      "Interaction training epoch: 131, train loss: 0.02836, val loss: 0.02795\n",
      "Interaction training epoch: 132, train loss: 0.02537, val loss: 0.02462\n",
      "Interaction training epoch: 133, train loss: 0.05578, val loss: 0.05497\n",
      "Interaction training epoch: 134, train loss: 0.02756, val loss: 0.02628\n",
      "Interaction training epoch: 135, train loss: 0.03851, val loss: 0.03792\n",
      "Interaction training epoch: 136, train loss: 0.02792, val loss: 0.02763\n",
      "Interaction training epoch: 137, train loss: 0.02882, val loss: 0.02822\n",
      "Interaction training epoch: 138, train loss: 0.04268, val loss: 0.04230\n",
      "Interaction training epoch: 139, train loss: 0.02999, val loss: 0.03028\n",
      "Interaction training epoch: 140, train loss: 0.02653, val loss: 0.02598\n",
      "Interaction training epoch: 141, train loss: 0.02864, val loss: 0.02740\n",
      "Interaction training epoch: 142, train loss: 0.04031, val loss: 0.03930\n",
      "Interaction training epoch: 143, train loss: 0.08930, val loss: 0.08825\n",
      "Interaction training epoch: 144, train loss: 0.03278, val loss: 0.03201\n",
      "Interaction training epoch: 145, train loss: 0.07595, val loss: 0.07444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 146, train loss: 0.04088, val loss: 0.04007\n",
      "Interaction training epoch: 147, train loss: 0.03063, val loss: 0.02967\n",
      "Interaction training epoch: 148, train loss: 0.08210, val loss: 0.08116\n",
      "Interaction training epoch: 149, train loss: 0.02600, val loss: 0.02595\n",
      "Interaction training epoch: 150, train loss: 0.04451, val loss: 0.04266\n",
      "Interaction training epoch: 151, train loss: 0.05071, val loss: 0.04953\n",
      "Interaction training epoch: 152, train loss: 0.03689, val loss: 0.03657\n",
      "Interaction training epoch: 153, train loss: 0.05108, val loss: 0.05017\n",
      "Interaction training epoch: 154, train loss: 0.03125, val loss: 0.03020\n",
      "Interaction training epoch: 155, train loss: 0.05262, val loss: 0.05160\n",
      "Interaction training epoch: 156, train loss: 0.03189, val loss: 0.03099\n",
      "Interaction training epoch: 157, train loss: 0.03302, val loss: 0.03192\n",
      "Interaction training epoch: 158, train loss: 0.03919, val loss: 0.03867\n",
      "Interaction training epoch: 159, train loss: 0.06294, val loss: 0.06164\n",
      "Interaction training epoch: 160, train loss: 0.02504, val loss: 0.02398\n",
      "Interaction training epoch: 161, train loss: 0.02726, val loss: 0.02622\n",
      "Interaction training epoch: 162, train loss: 0.02696, val loss: 0.02663\n",
      "Interaction training epoch: 163, train loss: 0.04443, val loss: 0.04306\n",
      "Interaction training epoch: 164, train loss: 0.08229, val loss: 0.08080\n",
      "Interaction training epoch: 165, train loss: 0.02497, val loss: 0.02433\n",
      "Interaction training epoch: 166, train loss: 0.02405, val loss: 0.02321\n",
      "Interaction training epoch: 167, train loss: 0.03154, val loss: 0.03087\n",
      "Interaction training epoch: 168, train loss: 0.03079, val loss: 0.02948\n",
      "Interaction training epoch: 169, train loss: 0.02562, val loss: 0.02466\n",
      "Interaction training epoch: 170, train loss: 0.03950, val loss: 0.03811\n",
      "Interaction training epoch: 171, train loss: 0.05102, val loss: 0.05005\n",
      "Interaction training epoch: 172, train loss: 0.07393, val loss: 0.07199\n",
      "Interaction training epoch: 173, train loss: 0.03775, val loss: 0.03659\n",
      "Interaction training epoch: 174, train loss: 0.03283, val loss: 0.03172\n",
      "Interaction training epoch: 175, train loss: 0.02729, val loss: 0.02663\n",
      "Interaction training epoch: 176, train loss: 0.04249, val loss: 0.04090\n",
      "Interaction training epoch: 177, train loss: 0.11371, val loss: 0.11248\n",
      "Interaction training epoch: 178, train loss: 0.05008, val loss: 0.04851\n",
      "Interaction training epoch: 179, train loss: 0.05070, val loss: 0.04903\n",
      "Interaction training epoch: 180, train loss: 0.05192, val loss: 0.05082\n",
      "Interaction training epoch: 181, train loss: 0.03499, val loss: 0.03401\n",
      "Interaction training epoch: 182, train loss: 0.06009, val loss: 0.05850\n",
      "Interaction training epoch: 183, train loss: 0.03517, val loss: 0.03374\n",
      "Interaction training epoch: 184, train loss: 0.04107, val loss: 0.04010\n",
      "Interaction training epoch: 185, train loss: 0.06361, val loss: 0.06162\n",
      "Interaction training epoch: 186, train loss: 0.06349, val loss: 0.06178\n",
      "Interaction training epoch: 187, train loss: 0.05374, val loss: 0.05196\n",
      "Interaction training epoch: 188, train loss: 0.05958, val loss: 0.05804\n",
      "Interaction training epoch: 189, train loss: 0.02604, val loss: 0.02519\n",
      "Interaction training epoch: 190, train loss: 0.04369, val loss: 0.04302\n",
      "Interaction training epoch: 191, train loss: 0.04175, val loss: 0.04116\n",
      "Interaction training epoch: 192, train loss: 0.06782, val loss: 0.06645\n",
      "Interaction training epoch: 193, train loss: 0.06145, val loss: 0.06034\n",
      "Interaction training epoch: 194, train loss: 0.04303, val loss: 0.04151\n",
      "Interaction training epoch: 195, train loss: 0.03319, val loss: 0.03229\n",
      "Interaction training epoch: 196, train loss: 0.03143, val loss: 0.02941\n",
      "Interaction training epoch: 197, train loss: 0.03413, val loss: 0.03311\n",
      "Interaction training epoch: 198, train loss: 0.03045, val loss: 0.02901\n",
      "Interaction training epoch: 199, train loss: 0.04253, val loss: 0.04138\n",
      "Interaction training epoch: 200, train loss: 0.04032, val loss: 0.03845\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########4 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.02853, val loss: 0.02785\n",
      "Interaction tuning epoch: 2, train loss: 0.02401, val loss: 0.02299\n",
      "Interaction tuning epoch: 3, train loss: 0.04478, val loss: 0.04265\n",
      "Interaction tuning epoch: 4, train loss: 0.06395, val loss: 0.06292\n",
      "Interaction tuning epoch: 5, train loss: 0.02356, val loss: 0.02286\n",
      "Interaction tuning epoch: 6, train loss: 0.06369, val loss: 0.06228\n",
      "Interaction tuning epoch: 7, train loss: 0.04154, val loss: 0.04077\n",
      "Interaction tuning epoch: 8, train loss: 0.08354, val loss: 0.08227\n",
      "Interaction tuning epoch: 9, train loss: 0.07421, val loss: 0.07270\n",
      "Interaction tuning epoch: 10, train loss: 0.04015, val loss: 0.03931\n",
      "Interaction tuning epoch: 11, train loss: 0.02840, val loss: 0.02746\n",
      "Interaction tuning epoch: 12, train loss: 0.06172, val loss: 0.06021\n",
      "Interaction tuning epoch: 13, train loss: 0.03776, val loss: 0.03699\n",
      "Interaction tuning epoch: 14, train loss: 0.07388, val loss: 0.07213\n",
      "Interaction tuning epoch: 15, train loss: 0.02628, val loss: 0.02557\n",
      "Interaction tuning epoch: 16, train loss: 0.02860, val loss: 0.02813\n",
      "Interaction tuning epoch: 17, train loss: 0.04973, val loss: 0.04802\n",
      "Interaction tuning epoch: 18, train loss: 0.02869, val loss: 0.02741\n",
      "Interaction tuning epoch: 19, train loss: 0.03914, val loss: 0.03730\n",
      "Interaction tuning epoch: 20, train loss: 0.03596, val loss: 0.03526\n",
      "Interaction tuning epoch: 21, train loss: 0.04924, val loss: 0.04797\n",
      "Interaction tuning epoch: 22, train loss: 0.08235, val loss: 0.08148\n",
      "Interaction tuning epoch: 23, train loss: 0.08430, val loss: 0.08186\n",
      "Interaction tuning epoch: 24, train loss: 0.07758, val loss: 0.07581\n",
      "Interaction tuning epoch: 25, train loss: 0.02468, val loss: 0.02425\n",
      "Interaction tuning epoch: 26, train loss: 0.02729, val loss: 0.02674\n",
      "Interaction tuning epoch: 27, train loss: 0.02677, val loss: 0.02542\n",
      "Interaction tuning epoch: 28, train loss: 0.03303, val loss: 0.03233\n",
      "Interaction tuning epoch: 29, train loss: 0.07628, val loss: 0.07409\n",
      "Interaction tuning epoch: 30, train loss: 0.05354, val loss: 0.05248\n",
      "Interaction tuning epoch: 31, train loss: 0.02819, val loss: 0.02642\n",
      "Interaction tuning epoch: 32, train loss: 0.02994, val loss: 0.02905\n",
      "Interaction tuning epoch: 33, train loss: 0.03903, val loss: 0.03809\n",
      "Interaction tuning epoch: 34, train loss: 0.03406, val loss: 0.03261\n",
      "Interaction tuning epoch: 35, train loss: 0.02886, val loss: 0.02797\n",
      "Interaction tuning epoch: 36, train loss: 0.04728, val loss: 0.04591\n",
      "Interaction tuning epoch: 37, train loss: 0.02745, val loss: 0.02653\n",
      "Interaction tuning epoch: 38, train loss: 0.09613, val loss: 0.09462\n",
      "Interaction tuning epoch: 39, train loss: 0.06430, val loss: 0.06326\n",
      "Interaction tuning epoch: 40, train loss: 0.03564, val loss: 0.03467\n",
      "Interaction tuning epoch: 41, train loss: 0.05842, val loss: 0.05720\n",
      "Interaction tuning epoch: 42, train loss: 0.02719, val loss: 0.02601\n",
      "Interaction tuning epoch: 43, train loss: 0.03689, val loss: 0.03533\n",
      "Interaction tuning epoch: 44, train loss: 0.02563, val loss: 0.02452\n",
      "Interaction tuning epoch: 45, train loss: 0.11515, val loss: 0.11336\n",
      "Interaction tuning epoch: 46, train loss: 0.05383, val loss: 0.05274\n",
      "Interaction tuning epoch: 47, train loss: 0.03183, val loss: 0.02986\n",
      "Interaction tuning epoch: 48, train loss: 0.04755, val loss: 0.04566\n",
      "Interaction tuning epoch: 49, train loss: 0.03504, val loss: 0.03437\n",
      "Interaction tuning epoch: 50, train loss: 0.03646, val loss: 0.03600\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 36.538437843322754\n",
      "After the gam stage, training error is 0.03646 , validation error is 0.03600\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 1.968308\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.025919 validation MAE=0.034367,rank=7\n",
      "[SoftImpute] Iter 2: observed MAE=0.023882 validation MAE=0.033468,rank=7\n",
      "[SoftImpute] Iter 3: observed MAE=0.022090 validation MAE=0.032597,rank=7\n",
      "[SoftImpute] Iter 4: observed MAE=0.020500 validation MAE=0.031820,rank=7\n",
      "[SoftImpute] Iter 5: observed MAE=0.019090 validation MAE=0.031120,rank=7\n",
      "[SoftImpute] Iter 6: observed MAE=0.017847 validation MAE=0.030498,rank=7\n",
      "[SoftImpute] Iter 7: observed MAE=0.016749 validation MAE=0.029925,rank=7\n",
      "[SoftImpute] Iter 8: observed MAE=0.015790 validation MAE=0.029398,rank=7\n",
      "[SoftImpute] Iter 9: observed MAE=0.014938 validation MAE=0.028919,rank=7\n",
      "[SoftImpute] Iter 10: observed MAE=0.014182 validation MAE=0.028486,rank=7\n",
      "[SoftImpute] Iter 11: observed MAE=0.013508 validation MAE=0.028102,rank=7\n",
      "[SoftImpute] Iter 12: observed MAE=0.012903 validation MAE=0.027755,rank=7\n",
      "[SoftImpute] Iter 13: observed MAE=0.012361 validation MAE=0.027430,rank=7\n",
      "[SoftImpute] Iter 14: observed MAE=0.011872 validation MAE=0.027130,rank=7\n",
      "[SoftImpute] Iter 15: observed MAE=0.011429 validation MAE=0.026851,rank=7\n",
      "[SoftImpute] Iter 16: observed MAE=0.011029 validation MAE=0.026594,rank=7\n",
      "[SoftImpute] Iter 17: observed MAE=0.010667 validation MAE=0.026353,rank=7\n",
      "[SoftImpute] Iter 18: observed MAE=0.010335 validation MAE=0.026126,rank=7\n",
      "[SoftImpute] Iter 19: observed MAE=0.010031 validation MAE=0.025910,rank=7\n",
      "[SoftImpute] Iter 20: observed MAE=0.009752 validation MAE=0.025707,rank=7\n",
      "[SoftImpute] Iter 21: observed MAE=0.009493 validation MAE=0.025516,rank=7\n",
      "[SoftImpute] Iter 22: observed MAE=0.009253 validation MAE=0.025337,rank=7\n",
      "[SoftImpute] Iter 23: observed MAE=0.009031 validation MAE=0.025166,rank=7\n",
      "[SoftImpute] Iter 24: observed MAE=0.008825 validation MAE=0.025005,rank=7\n",
      "[SoftImpute] Iter 25: observed MAE=0.008633 validation MAE=0.024850,rank=7\n",
      "[SoftImpute] Iter 26: observed MAE=0.008453 validation MAE=0.024702,rank=7\n",
      "[SoftImpute] Iter 27: observed MAE=0.008284 validation MAE=0.024561,rank=7\n",
      "[SoftImpute] Iter 28: observed MAE=0.008126 validation MAE=0.024431,rank=7\n",
      "[SoftImpute] Iter 29: observed MAE=0.007978 validation MAE=0.024309,rank=7\n",
      "[SoftImpute] Iter 30: observed MAE=0.007839 validation MAE=0.024193,rank=7\n",
      "[SoftImpute] Iter 31: observed MAE=0.007708 validation MAE=0.024081,rank=7\n",
      "[SoftImpute] Iter 32: observed MAE=0.007584 validation MAE=0.023975,rank=7\n",
      "[SoftImpute] Iter 33: observed MAE=0.007467 validation MAE=0.023874,rank=7\n",
      "[SoftImpute] Iter 34: observed MAE=0.007355 validation MAE=0.023778,rank=7\n",
      "[SoftImpute] Iter 35: observed MAE=0.007249 validation MAE=0.023685,rank=7\n",
      "[SoftImpute] Iter 36: observed MAE=0.007150 validation MAE=0.023595,rank=7\n",
      "[SoftImpute] Iter 37: observed MAE=0.007055 validation MAE=0.023507,rank=7\n",
      "[SoftImpute] Iter 38: observed MAE=0.006964 validation MAE=0.023422,rank=7\n",
      "[SoftImpute] Iter 39: observed MAE=0.006878 validation MAE=0.023338,rank=7\n",
      "[SoftImpute] Iter 40: observed MAE=0.006796 validation MAE=0.023258,rank=7\n",
      "[SoftImpute] Iter 41: observed MAE=0.006718 validation MAE=0.023179,rank=7\n",
      "[SoftImpute] Iter 42: observed MAE=0.006643 validation MAE=0.023101,rank=7\n",
      "[SoftImpute] Iter 43: observed MAE=0.006571 validation MAE=0.023026,rank=7\n",
      "[SoftImpute] Iter 44: observed MAE=0.006503 validation MAE=0.022952,rank=7\n",
      "[SoftImpute] Iter 45: observed MAE=0.006438 validation MAE=0.022880,rank=7\n",
      "[SoftImpute] Iter 46: observed MAE=0.006376 validation MAE=0.022810,rank=7\n",
      "[SoftImpute] Iter 47: observed MAE=0.006316 validation MAE=0.022741,rank=7\n",
      "[SoftImpute] Iter 48: observed MAE=0.006258 validation MAE=0.022674,rank=7\n",
      "[SoftImpute] Iter 49: observed MAE=0.006203 validation MAE=0.022609,rank=7\n",
      "[SoftImpute] Iter 50: observed MAE=0.006151 validation MAE=0.022545,rank=7\n",
      "[SoftImpute] Iter 51: observed MAE=0.006100 validation MAE=0.022481,rank=7\n",
      "[SoftImpute] Iter 52: observed MAE=0.006052 validation MAE=0.022419,rank=7\n",
      "[SoftImpute] Iter 53: observed MAE=0.006005 validation MAE=0.022359,rank=7\n",
      "[SoftImpute] Iter 54: observed MAE=0.005960 validation MAE=0.022300,rank=7\n",
      "[SoftImpute] Iter 55: observed MAE=0.005917 validation MAE=0.022242,rank=7\n",
      "[SoftImpute] Iter 56: observed MAE=0.005875 validation MAE=0.022185,rank=7\n",
      "[SoftImpute] Iter 57: observed MAE=0.005834 validation MAE=0.022128,rank=7\n",
      "[SoftImpute] Iter 58: observed MAE=0.005795 validation MAE=0.022073,rank=7\n",
      "[SoftImpute] Iter 59: observed MAE=0.005758 validation MAE=0.022019,rank=7\n",
      "[SoftImpute] Iter 60: observed MAE=0.005722 validation MAE=0.021966,rank=7\n",
      "[SoftImpute] Iter 61: observed MAE=0.005687 validation MAE=0.021914,rank=7\n",
      "[SoftImpute] Iter 62: observed MAE=0.005654 validation MAE=0.021862,rank=7\n",
      "[SoftImpute] Iter 63: observed MAE=0.005621 validation MAE=0.021811,rank=7\n",
      "[SoftImpute] Iter 64: observed MAE=0.005589 validation MAE=0.021761,rank=7\n",
      "[SoftImpute] Iter 65: observed MAE=0.005559 validation MAE=0.021713,rank=7\n",
      "[SoftImpute] Iter 66: observed MAE=0.005529 validation MAE=0.021666,rank=7\n",
      "[SoftImpute] Iter 67: observed MAE=0.005500 validation MAE=0.021620,rank=7\n",
      "[SoftImpute] Iter 68: observed MAE=0.005472 validation MAE=0.021574,rank=7\n",
      "[SoftImpute] Iter 69: observed MAE=0.005445 validation MAE=0.021529,rank=7\n",
      "[SoftImpute] Iter 70: observed MAE=0.005419 validation MAE=0.021485,rank=7\n",
      "[SoftImpute] Iter 71: observed MAE=0.005394 validation MAE=0.021441,rank=7\n",
      "[SoftImpute] Iter 72: observed MAE=0.005369 validation MAE=0.021397,rank=7\n",
      "[SoftImpute] Iter 73: observed MAE=0.005345 validation MAE=0.021354,rank=7\n",
      "[SoftImpute] Iter 74: observed MAE=0.005321 validation MAE=0.021311,rank=7\n",
      "[SoftImpute] Iter 75: observed MAE=0.005299 validation MAE=0.021269,rank=7\n",
      "[SoftImpute] Iter 76: observed MAE=0.005276 validation MAE=0.021227,rank=7\n",
      "[SoftImpute] Iter 77: observed MAE=0.005255 validation MAE=0.021186,rank=7\n",
      "[SoftImpute] Iter 78: observed MAE=0.005234 validation MAE=0.021146,rank=7\n",
      "[SoftImpute] Iter 79: observed MAE=0.005214 validation MAE=0.021107,rank=7\n",
      "[SoftImpute] Iter 80: observed MAE=0.005194 validation MAE=0.021068,rank=7\n",
      "[SoftImpute] Iter 81: observed MAE=0.005175 validation MAE=0.021030,rank=7\n",
      "[SoftImpute] Iter 82: observed MAE=0.005157 validation MAE=0.020992,rank=7\n",
      "[SoftImpute] Iter 83: observed MAE=0.005138 validation MAE=0.020955,rank=7\n",
      "[SoftImpute] Iter 84: observed MAE=0.005121 validation MAE=0.020918,rank=7\n",
      "[SoftImpute] Iter 85: observed MAE=0.005103 validation MAE=0.020881,rank=7\n",
      "[SoftImpute] Iter 86: observed MAE=0.005087 validation MAE=0.020845,rank=7\n",
      "[SoftImpute] Iter 87: observed MAE=0.005070 validation MAE=0.020809,rank=7\n",
      "[SoftImpute] Iter 88: observed MAE=0.005054 validation MAE=0.020773,rank=7\n",
      "[SoftImpute] Iter 89: observed MAE=0.005039 validation MAE=0.020738,rank=7\n",
      "[SoftImpute] Iter 90: observed MAE=0.005023 validation MAE=0.020703,rank=7\n",
      "[SoftImpute] Iter 91: observed MAE=0.005009 validation MAE=0.020669,rank=7\n",
      "[SoftImpute] Iter 92: observed MAE=0.004994 validation MAE=0.020634,rank=7\n",
      "[SoftImpute] Iter 93: observed MAE=0.004980 validation MAE=0.020600,rank=7\n",
      "[SoftImpute] Iter 94: observed MAE=0.004966 validation MAE=0.020567,rank=7\n",
      "[SoftImpute] Iter 95: observed MAE=0.004953 validation MAE=0.020534,rank=7\n",
      "[SoftImpute] Iter 96: observed MAE=0.004940 validation MAE=0.020501,rank=7\n",
      "[SoftImpute] Iter 97: observed MAE=0.004927 validation MAE=0.020468,rank=7\n",
      "[SoftImpute] Iter 98: observed MAE=0.004915 validation MAE=0.020436,rank=7\n",
      "[SoftImpute] Iter 99: observed MAE=0.004903 validation MAE=0.020404,rank=7\n",
      "[SoftImpute] Iter 100: observed MAE=0.004891 validation MAE=0.020372,rank=7\n",
      "[SoftImpute] Iter 101: observed MAE=0.004879 validation MAE=0.020341,rank=7\n",
      "[SoftImpute] Iter 102: observed MAE=0.004867 validation MAE=0.020309,rank=7\n",
      "[SoftImpute] Iter 103: observed MAE=0.004856 validation MAE=0.020278,rank=7\n",
      "[SoftImpute] Iter 104: observed MAE=0.004845 validation MAE=0.020248,rank=7\n",
      "[SoftImpute] Iter 105: observed MAE=0.004835 validation MAE=0.020217,rank=7\n",
      "[SoftImpute] Iter 106: observed MAE=0.004824 validation MAE=0.020187,rank=7\n",
      "[SoftImpute] Iter 107: observed MAE=0.004814 validation MAE=0.020157,rank=7\n",
      "[SoftImpute] Iter 108: observed MAE=0.004804 validation MAE=0.020127,rank=7\n",
      "[SoftImpute] Iter 109: observed MAE=0.004794 validation MAE=0.020097,rank=7\n",
      "[SoftImpute] Iter 110: observed MAE=0.004785 validation MAE=0.020068,rank=7\n",
      "[SoftImpute] Iter 111: observed MAE=0.004775 validation MAE=0.020039,rank=7\n",
      "[SoftImpute] Iter 112: observed MAE=0.004766 validation MAE=0.020010,rank=7\n",
      "[SoftImpute] Iter 113: observed MAE=0.004757 validation MAE=0.019982,rank=7\n",
      "[SoftImpute] Iter 114: observed MAE=0.004748 validation MAE=0.019954,rank=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 115: observed MAE=0.004739 validation MAE=0.019926,rank=7\n",
      "[SoftImpute] Iter 116: observed MAE=0.004731 validation MAE=0.019899,rank=7\n",
      "[SoftImpute] Iter 117: observed MAE=0.004722 validation MAE=0.019871,rank=7\n",
      "[SoftImpute] Iter 118: observed MAE=0.004714 validation MAE=0.019844,rank=7\n",
      "[SoftImpute] Iter 119: observed MAE=0.004706 validation MAE=0.019817,rank=7\n",
      "[SoftImpute] Iter 120: observed MAE=0.004698 validation MAE=0.019790,rank=7\n",
      "[SoftImpute] Iter 121: observed MAE=0.004691 validation MAE=0.019763,rank=7\n",
      "[SoftImpute] Iter 122: observed MAE=0.004683 validation MAE=0.019736,rank=7\n",
      "[SoftImpute] Iter 123: observed MAE=0.004676 validation MAE=0.019709,rank=7\n",
      "[SoftImpute] Iter 124: observed MAE=0.004669 validation MAE=0.019683,rank=7\n",
      "[SoftImpute] Iter 125: observed MAE=0.004662 validation MAE=0.019657,rank=7\n",
      "[SoftImpute] Iter 126: observed MAE=0.004655 validation MAE=0.019631,rank=7\n",
      "[SoftImpute] Iter 127: observed MAE=0.004648 validation MAE=0.019605,rank=7\n",
      "[SoftImpute] Iter 128: observed MAE=0.004641 validation MAE=0.019579,rank=7\n",
      "[SoftImpute] Iter 129: observed MAE=0.004635 validation MAE=0.019554,rank=7\n",
      "[SoftImpute] Iter 130: observed MAE=0.004629 validation MAE=0.019529,rank=7\n",
      "[SoftImpute] Iter 131: observed MAE=0.004622 validation MAE=0.019504,rank=7\n",
      "[SoftImpute] Iter 132: observed MAE=0.004616 validation MAE=0.019479,rank=7\n",
      "[SoftImpute] Iter 133: observed MAE=0.004610 validation MAE=0.019454,rank=7\n",
      "[SoftImpute] Iter 134: observed MAE=0.004604 validation MAE=0.019430,rank=7\n",
      "[SoftImpute] Iter 135: observed MAE=0.004599 validation MAE=0.019405,rank=7\n",
      "[SoftImpute] Iter 136: observed MAE=0.004593 validation MAE=0.019381,rank=7\n",
      "[SoftImpute] Iter 137: observed MAE=0.004587 validation MAE=0.019358,rank=7\n",
      "[SoftImpute] Iter 138: observed MAE=0.004582 validation MAE=0.019334,rank=7\n",
      "[SoftImpute] Iter 139: observed MAE=0.004577 validation MAE=0.019311,rank=7\n",
      "[SoftImpute] Iter 140: observed MAE=0.004571 validation MAE=0.019287,rank=7\n",
      "[SoftImpute] Iter 141: observed MAE=0.004566 validation MAE=0.019264,rank=7\n",
      "[SoftImpute] Iter 142: observed MAE=0.004561 validation MAE=0.019242,rank=7\n",
      "[SoftImpute] Iter 143: observed MAE=0.004556 validation MAE=0.019219,rank=7\n",
      "[SoftImpute] Iter 144: observed MAE=0.004551 validation MAE=0.019197,rank=7\n",
      "[SoftImpute] Iter 145: observed MAE=0.004546 validation MAE=0.019175,rank=7\n",
      "[SoftImpute] Iter 146: observed MAE=0.004542 validation MAE=0.019153,rank=7\n",
      "[SoftImpute] Iter 147: observed MAE=0.004537 validation MAE=0.019132,rank=7\n",
      "[SoftImpute] Iter 148: observed MAE=0.004532 validation MAE=0.019110,rank=7\n",
      "[SoftImpute] Iter 149: observed MAE=0.004528 validation MAE=0.019088,rank=7\n",
      "[SoftImpute] Iter 150: observed MAE=0.004523 validation MAE=0.019067,rank=7\n",
      "[SoftImpute] Iter 151: observed MAE=0.004519 validation MAE=0.019046,rank=7\n",
      "[SoftImpute] Iter 152: observed MAE=0.004515 validation MAE=0.019025,rank=7\n",
      "[SoftImpute] Iter 153: observed MAE=0.004510 validation MAE=0.019004,rank=7\n",
      "[SoftImpute] Iter 154: observed MAE=0.004506 validation MAE=0.018983,rank=7\n",
      "[SoftImpute] Iter 155: observed MAE=0.004502 validation MAE=0.018963,rank=7\n",
      "[SoftImpute] Iter 156: observed MAE=0.004498 validation MAE=0.018943,rank=7\n",
      "[SoftImpute] Iter 157: observed MAE=0.004494 validation MAE=0.018922,rank=7\n",
      "[SoftImpute] Iter 158: observed MAE=0.004490 validation MAE=0.018902,rank=7\n",
      "[SoftImpute] Iter 159: observed MAE=0.004486 validation MAE=0.018883,rank=7\n",
      "[SoftImpute] Iter 160: observed MAE=0.004482 validation MAE=0.018863,rank=7\n",
      "[SoftImpute] Iter 161: observed MAE=0.004478 validation MAE=0.018843,rank=7\n",
      "[SoftImpute] Iter 162: observed MAE=0.004475 validation MAE=0.018824,rank=7\n",
      "[SoftImpute] Iter 163: observed MAE=0.004471 validation MAE=0.018804,rank=7\n",
      "[SoftImpute] Iter 164: observed MAE=0.004467 validation MAE=0.018785,rank=7\n",
      "[SoftImpute] Iter 165: observed MAE=0.004464 validation MAE=0.018766,rank=7\n",
      "[SoftImpute] Iter 166: observed MAE=0.004460 validation MAE=0.018746,rank=7\n",
      "[SoftImpute] Iter 167: observed MAE=0.004457 validation MAE=0.018727,rank=7\n",
      "[SoftImpute] Iter 168: observed MAE=0.004453 validation MAE=0.018708,rank=7\n",
      "[SoftImpute] Iter 169: observed MAE=0.004450 validation MAE=0.018690,rank=7\n",
      "[SoftImpute] Iter 170: observed MAE=0.004447 validation MAE=0.018671,rank=7\n",
      "[SoftImpute] Iter 171: observed MAE=0.004443 validation MAE=0.018652,rank=7\n",
      "[SoftImpute] Iter 172: observed MAE=0.004440 validation MAE=0.018634,rank=7\n",
      "[SoftImpute] Iter 173: observed MAE=0.004437 validation MAE=0.018615,rank=7\n",
      "[SoftImpute] Iter 174: observed MAE=0.004434 validation MAE=0.018597,rank=7\n",
      "[SoftImpute] Iter 175: observed MAE=0.004430 validation MAE=0.018579,rank=7\n",
      "[SoftImpute] Iter 176: observed MAE=0.004427 validation MAE=0.018561,rank=7\n",
      "[SoftImpute] Iter 177: observed MAE=0.004424 validation MAE=0.018543,rank=7\n",
      "[SoftImpute] Iter 178: observed MAE=0.004421 validation MAE=0.018525,rank=7\n",
      "[SoftImpute] Iter 179: observed MAE=0.004418 validation MAE=0.018507,rank=7\n",
      "[SoftImpute] Iter 180: observed MAE=0.004415 validation MAE=0.018489,rank=7\n",
      "[SoftImpute] Iter 181: observed MAE=0.004412 validation MAE=0.018472,rank=7\n",
      "[SoftImpute] Iter 182: observed MAE=0.004409 validation MAE=0.018454,rank=7\n",
      "[SoftImpute] Iter 183: observed MAE=0.004407 validation MAE=0.018437,rank=7\n",
      "[SoftImpute] Iter 184: observed MAE=0.004404 validation MAE=0.018419,rank=7\n",
      "[SoftImpute] Iter 185: observed MAE=0.004401 validation MAE=0.018402,rank=7\n",
      "[SoftImpute] Iter 186: observed MAE=0.004398 validation MAE=0.018385,rank=7\n",
      "[SoftImpute] Iter 187: observed MAE=0.004396 validation MAE=0.018368,rank=7\n",
      "[SoftImpute] Iter 188: observed MAE=0.004393 validation MAE=0.018350,rank=7\n",
      "[SoftImpute] Iter 189: observed MAE=0.004390 validation MAE=0.018334,rank=7\n",
      "[SoftImpute] Iter 190: observed MAE=0.004388 validation MAE=0.018317,rank=7\n",
      "[SoftImpute] Iter 191: observed MAE=0.004385 validation MAE=0.018300,rank=7\n",
      "[SoftImpute] Iter 192: observed MAE=0.004382 validation MAE=0.018283,rank=7\n",
      "[SoftImpute] Iter 193: observed MAE=0.004380 validation MAE=0.018266,rank=7\n",
      "[SoftImpute] Iter 194: observed MAE=0.004377 validation MAE=0.018250,rank=7\n",
      "[SoftImpute] Iter 195: observed MAE=0.004375 validation MAE=0.018233,rank=7\n",
      "[SoftImpute] Iter 196: observed MAE=0.004372 validation MAE=0.018217,rank=7\n",
      "[SoftImpute] Iter 197: observed MAE=0.004370 validation MAE=0.018201,rank=7\n",
      "[SoftImpute] Iter 198: observed MAE=0.004367 validation MAE=0.018185,rank=7\n",
      "[SoftImpute] Iter 199: observed MAE=0.004365 validation MAE=0.018169,rank=7\n",
      "[SoftImpute] Iter 200: observed MAE=0.004362 validation MAE=0.018153,rank=7\n",
      "[SoftImpute] Iter 201: observed MAE=0.004360 validation MAE=0.018137,rank=7\n",
      "[SoftImpute] Iter 202: observed MAE=0.004358 validation MAE=0.018121,rank=7\n",
      "[SoftImpute] Iter 203: observed MAE=0.004355 validation MAE=0.018105,rank=7\n",
      "[SoftImpute] Iter 204: observed MAE=0.004353 validation MAE=0.018090,rank=7\n",
      "[SoftImpute] Iter 205: observed MAE=0.004351 validation MAE=0.018074,rank=7\n",
      "[SoftImpute] Iter 206: observed MAE=0.004349 validation MAE=0.018059,rank=7\n",
      "[SoftImpute] Iter 207: observed MAE=0.004346 validation MAE=0.018043,rank=7\n",
      "[SoftImpute] Iter 208: observed MAE=0.004344 validation MAE=0.018028,rank=7\n",
      "[SoftImpute] Iter 209: observed MAE=0.004342 validation MAE=0.018013,rank=7\n",
      "[SoftImpute] Iter 210: observed MAE=0.004340 validation MAE=0.017998,rank=7\n",
      "[SoftImpute] Iter 211: observed MAE=0.004338 validation MAE=0.017983,rank=7\n",
      "[SoftImpute] Iter 212: observed MAE=0.004335 validation MAE=0.017968,rank=7\n",
      "[SoftImpute] Iter 213: observed MAE=0.004333 validation MAE=0.017953,rank=7\n",
      "[SoftImpute] Iter 214: observed MAE=0.004331 validation MAE=0.017938,rank=7\n",
      "[SoftImpute] Iter 215: observed MAE=0.004329 validation MAE=0.017924,rank=7\n",
      "[SoftImpute] Iter 216: observed MAE=0.004327 validation MAE=0.017910,rank=7\n",
      "[SoftImpute] Iter 217: observed MAE=0.004325 validation MAE=0.017896,rank=7\n",
      "[SoftImpute] Iter 218: observed MAE=0.004323 validation MAE=0.017882,rank=7\n",
      "[SoftImpute] Iter 219: observed MAE=0.004321 validation MAE=0.017868,rank=7\n",
      "[SoftImpute] Iter 220: observed MAE=0.004319 validation MAE=0.017854,rank=7\n",
      "[SoftImpute] Iter 221: observed MAE=0.004317 validation MAE=0.017840,rank=7\n",
      "[SoftImpute] Iter 222: observed MAE=0.004315 validation MAE=0.017826,rank=7\n",
      "[SoftImpute] Iter 223: observed MAE=0.004313 validation MAE=0.017812,rank=7\n",
      "[SoftImpute] Iter 224: observed MAE=0.004311 validation MAE=0.017798,rank=7\n",
      "[SoftImpute] Iter 225: observed MAE=0.004309 validation MAE=0.017785,rank=7\n",
      "[SoftImpute] Iter 226: observed MAE=0.004307 validation MAE=0.017771,rank=7\n",
      "[SoftImpute] Iter 227: observed MAE=0.004305 validation MAE=0.017758,rank=7\n",
      "[SoftImpute] Iter 228: observed MAE=0.004303 validation MAE=0.017744,rank=7\n",
      "[SoftImpute] Iter 229: observed MAE=0.004301 validation MAE=0.017731,rank=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 230: observed MAE=0.004300 validation MAE=0.017718,rank=7\n",
      "[SoftImpute] Iter 231: observed MAE=0.004298 validation MAE=0.017705,rank=7\n",
      "[SoftImpute] Iter 232: observed MAE=0.004296 validation MAE=0.017692,rank=7\n",
      "[SoftImpute] Iter 233: observed MAE=0.004294 validation MAE=0.017679,rank=7\n",
      "[SoftImpute] Iter 234: observed MAE=0.004292 validation MAE=0.017666,rank=7\n",
      "[SoftImpute] Iter 235: observed MAE=0.004290 validation MAE=0.017653,rank=7\n",
      "[SoftImpute] Iter 236: observed MAE=0.004289 validation MAE=0.017640,rank=7\n",
      "[SoftImpute] Iter 237: observed MAE=0.004287 validation MAE=0.017627,rank=7\n",
      "[SoftImpute] Iter 238: observed MAE=0.004285 validation MAE=0.017614,rank=7\n",
      "[SoftImpute] Iter 239: observed MAE=0.004283 validation MAE=0.017601,rank=7\n",
      "[SoftImpute] Iter 240: observed MAE=0.004282 validation MAE=0.017589,rank=7\n",
      "[SoftImpute] Iter 241: observed MAE=0.004280 validation MAE=0.017576,rank=7\n",
      "[SoftImpute] Iter 242: observed MAE=0.004278 validation MAE=0.017563,rank=7\n",
      "[SoftImpute] Iter 243: observed MAE=0.004277 validation MAE=0.017551,rank=7\n",
      "[SoftImpute] Iter 244: observed MAE=0.004275 validation MAE=0.017538,rank=7\n",
      "[SoftImpute] Iter 245: observed MAE=0.004273 validation MAE=0.017526,rank=7\n",
      "[SoftImpute] Iter 246: observed MAE=0.004272 validation MAE=0.017514,rank=7\n",
      "[SoftImpute] Iter 247: observed MAE=0.004270 validation MAE=0.017502,rank=7\n",
      "[SoftImpute] Iter 248: observed MAE=0.004268 validation MAE=0.017490,rank=7\n",
      "[SoftImpute] Iter 249: observed MAE=0.004267 validation MAE=0.017478,rank=7\n",
      "[SoftImpute] Iter 250: observed MAE=0.004265 validation MAE=0.017467,rank=7\n",
      "[SoftImpute] Iter 251: observed MAE=0.004263 validation MAE=0.017455,rank=7\n",
      "[SoftImpute] Iter 252: observed MAE=0.004262 validation MAE=0.017443,rank=7\n",
      "[SoftImpute] Iter 253: observed MAE=0.004260 validation MAE=0.017431,rank=7\n",
      "[SoftImpute] Iter 254: observed MAE=0.004259 validation MAE=0.017420,rank=7\n",
      "[SoftImpute] Iter 255: observed MAE=0.004257 validation MAE=0.017408,rank=7\n",
      "[SoftImpute] Iter 256: observed MAE=0.004256 validation MAE=0.017396,rank=7\n",
      "[SoftImpute] Iter 257: observed MAE=0.004254 validation MAE=0.017385,rank=7\n",
      "[SoftImpute] Iter 258: observed MAE=0.004253 validation MAE=0.017374,rank=7\n",
      "[SoftImpute] Iter 259: observed MAE=0.004251 validation MAE=0.017363,rank=7\n",
      "[SoftImpute] Iter 260: observed MAE=0.004249 validation MAE=0.017351,rank=7\n",
      "[SoftImpute] Iter 261: observed MAE=0.004248 validation MAE=0.017340,rank=7\n",
      "[SoftImpute] Iter 262: observed MAE=0.004246 validation MAE=0.017329,rank=7\n",
      "[SoftImpute] Iter 263: observed MAE=0.004245 validation MAE=0.017318,rank=7\n",
      "[SoftImpute] Iter 264: observed MAE=0.004243 validation MAE=0.017307,rank=7\n",
      "[SoftImpute] Iter 265: observed MAE=0.004242 validation MAE=0.017296,rank=7\n",
      "[SoftImpute] Iter 266: observed MAE=0.004241 validation MAE=0.017285,rank=7\n",
      "[SoftImpute] Iter 267: observed MAE=0.004239 validation MAE=0.017275,rank=7\n",
      "[SoftImpute] Iter 268: observed MAE=0.004238 validation MAE=0.017264,rank=7\n",
      "[SoftImpute] Iter 269: observed MAE=0.004236 validation MAE=0.017253,rank=7\n",
      "[SoftImpute] Iter 270: observed MAE=0.004235 validation MAE=0.017243,rank=7\n",
      "[SoftImpute] Iter 271: observed MAE=0.004233 validation MAE=0.017232,rank=7\n",
      "[SoftImpute] Iter 272: observed MAE=0.004232 validation MAE=0.017222,rank=7\n",
      "[SoftImpute] Iter 273: observed MAE=0.004230 validation MAE=0.017211,rank=7\n",
      "[SoftImpute] Iter 274: observed MAE=0.004229 validation MAE=0.017201,rank=7\n",
      "[SoftImpute] Iter 275: observed MAE=0.004227 validation MAE=0.017191,rank=7\n",
      "[SoftImpute] Iter 276: observed MAE=0.004226 validation MAE=0.017181,rank=7\n",
      "[SoftImpute] Iter 277: observed MAE=0.004225 validation MAE=0.017170,rank=7\n",
      "[SoftImpute] Iter 278: observed MAE=0.004223 validation MAE=0.017160,rank=7\n",
      "[SoftImpute] Iter 279: observed MAE=0.004222 validation MAE=0.017150,rank=7\n",
      "[SoftImpute] Iter 280: observed MAE=0.004220 validation MAE=0.017140,rank=7\n",
      "[SoftImpute] Iter 281: observed MAE=0.004219 validation MAE=0.017130,rank=7\n",
      "[SoftImpute] Iter 282: observed MAE=0.004218 validation MAE=0.017119,rank=7\n",
      "[SoftImpute] Iter 283: observed MAE=0.004216 validation MAE=0.017110,rank=7\n",
      "[SoftImpute] Iter 284: observed MAE=0.004215 validation MAE=0.017100,rank=7\n",
      "[SoftImpute] Iter 285: observed MAE=0.004214 validation MAE=0.017090,rank=7\n",
      "[SoftImpute] Iter 286: observed MAE=0.004212 validation MAE=0.017081,rank=7\n",
      "[SoftImpute] Iter 287: observed MAE=0.004211 validation MAE=0.017071,rank=7\n",
      "[SoftImpute] Iter 288: observed MAE=0.004210 validation MAE=0.017062,rank=7\n",
      "[SoftImpute] Iter 289: observed MAE=0.004208 validation MAE=0.017053,rank=7\n",
      "[SoftImpute] Iter 290: observed MAE=0.004207 validation MAE=0.017043,rank=7\n",
      "[SoftImpute] Iter 291: observed MAE=0.004206 validation MAE=0.017034,rank=7\n",
      "[SoftImpute] Iter 292: observed MAE=0.004204 validation MAE=0.017025,rank=7\n",
      "[SoftImpute] Iter 293: observed MAE=0.004203 validation MAE=0.017016,rank=7\n",
      "[SoftImpute] Iter 294: observed MAE=0.004202 validation MAE=0.017007,rank=7\n",
      "[SoftImpute] Iter 295: observed MAE=0.004200 validation MAE=0.016998,rank=7\n",
      "[SoftImpute] Iter 296: observed MAE=0.004199 validation MAE=0.016989,rank=7\n",
      "[SoftImpute] Iter 297: observed MAE=0.004198 validation MAE=0.016980,rank=7\n",
      "[SoftImpute] Iter 298: observed MAE=0.004197 validation MAE=0.016971,rank=7\n",
      "[SoftImpute] Iter 299: observed MAE=0.004195 validation MAE=0.016961,rank=7\n",
      "[SoftImpute] Iter 300: observed MAE=0.004194 validation MAE=0.016952,rank=7\n",
      "[SoftImpute] Iter 301: observed MAE=0.004193 validation MAE=0.016943,rank=7\n",
      "[SoftImpute] Iter 302: observed MAE=0.004192 validation MAE=0.016935,rank=7\n",
      "[SoftImpute] Iter 303: observed MAE=0.004190 validation MAE=0.016926,rank=7\n",
      "[SoftImpute] Iter 304: observed MAE=0.004189 validation MAE=0.016917,rank=7\n",
      "[SoftImpute] Iter 305: observed MAE=0.004188 validation MAE=0.016908,rank=7\n",
      "[SoftImpute] Iter 306: observed MAE=0.004187 validation MAE=0.016899,rank=7\n",
      "[SoftImpute] Iter 307: observed MAE=0.004186 validation MAE=0.016890,rank=7\n",
      "[SoftImpute] Iter 308: observed MAE=0.004184 validation MAE=0.016881,rank=7\n",
      "[SoftImpute] Iter 309: observed MAE=0.004183 validation MAE=0.016873,rank=7\n",
      "[SoftImpute] Iter 310: observed MAE=0.004182 validation MAE=0.016864,rank=7\n",
      "[SoftImpute] Iter 311: observed MAE=0.004181 validation MAE=0.016855,rank=7\n",
      "[SoftImpute] Iter 312: observed MAE=0.004180 validation MAE=0.016847,rank=7\n",
      "[SoftImpute] Iter 313: observed MAE=0.004178 validation MAE=0.016838,rank=7\n",
      "[SoftImpute] Iter 314: observed MAE=0.004177 validation MAE=0.016830,rank=7\n",
      "[SoftImpute] Iter 315: observed MAE=0.004176 validation MAE=0.016822,rank=7\n",
      "[SoftImpute] Iter 316: observed MAE=0.004175 validation MAE=0.016814,rank=7\n",
      "[SoftImpute] Iter 317: observed MAE=0.004174 validation MAE=0.016806,rank=7\n",
      "[SoftImpute] Iter 318: observed MAE=0.004173 validation MAE=0.016798,rank=7\n",
      "[SoftImpute] Iter 319: observed MAE=0.004171 validation MAE=0.016790,rank=7\n",
      "[SoftImpute] Iter 320: observed MAE=0.004170 validation MAE=0.016782,rank=7\n",
      "[SoftImpute] Iter 321: observed MAE=0.004169 validation MAE=0.016774,rank=7\n",
      "[SoftImpute] Iter 322: observed MAE=0.004168 validation MAE=0.016767,rank=7\n",
      "[SoftImpute] Iter 323: observed MAE=0.004167 validation MAE=0.016759,rank=7\n",
      "[SoftImpute] Iter 324: observed MAE=0.004166 validation MAE=0.016751,rank=7\n",
      "[SoftImpute] Iter 325: observed MAE=0.004165 validation MAE=0.016744,rank=7\n",
      "[SoftImpute] Iter 326: observed MAE=0.004164 validation MAE=0.016737,rank=7\n",
      "[SoftImpute] Iter 327: observed MAE=0.004162 validation MAE=0.016729,rank=7\n",
      "[SoftImpute] Iter 328: observed MAE=0.004161 validation MAE=0.016722,rank=7\n",
      "[SoftImpute] Stopped after iteration 328 for lambda=0.039366\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 8.956103563308716\n",
      "After the matrix factor stage, training error is 0.00416, validation error is 0.01672\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.34448, val loss: 0.34765\n",
      "Main effects training epoch: 2, train loss: 0.27354, val loss: 0.27853\n",
      "Main effects training epoch: 3, train loss: 0.20823, val loss: 0.21145\n",
      "Main effects training epoch: 4, train loss: 0.16135, val loss: 0.16514\n",
      "Main effects training epoch: 5, train loss: 0.14048, val loss: 0.14025\n",
      "Main effects training epoch: 6, train loss: 0.13280, val loss: 0.13280\n",
      "Main effects training epoch: 7, train loss: 0.13041, val loss: 0.12922\n",
      "Main effects training epoch: 8, train loss: 0.12977, val loss: 0.12861\n",
      "Main effects training epoch: 9, train loss: 0.12914, val loss: 0.12821\n",
      "Main effects training epoch: 10, train loss: 0.12852, val loss: 0.12844\n",
      "Main effects training epoch: 11, train loss: 0.12742, val loss: 0.12590\n",
      "Main effects training epoch: 12, train loss: 0.12612, val loss: 0.12564\n",
      "Main effects training epoch: 13, train loss: 0.12339, val loss: 0.12252\n",
      "Main effects training epoch: 14, train loss: 0.11835, val loss: 0.11883\n",
      "Main effects training epoch: 15, train loss: 0.11578, val loss: 0.11651\n",
      "Main effects training epoch: 16, train loss: 0.11289, val loss: 0.11344\n",
      "Main effects training epoch: 17, train loss: 0.11081, val loss: 0.11186\n",
      "Main effects training epoch: 18, train loss: 0.11088, val loss: 0.11312\n",
      "Main effects training epoch: 19, train loss: 0.11027, val loss: 0.11199\n",
      "Main effects training epoch: 20, train loss: 0.11090, val loss: 0.11238\n",
      "Main effects training epoch: 21, train loss: 0.10922, val loss: 0.11096\n",
      "Main effects training epoch: 22, train loss: 0.10900, val loss: 0.11106\n",
      "Main effects training epoch: 23, train loss: 0.10645, val loss: 0.10812\n",
      "Main effects training epoch: 24, train loss: 0.10707, val loss: 0.10858\n",
      "Main effects training epoch: 25, train loss: 0.10776, val loss: 0.10768\n",
      "Main effects training epoch: 26, train loss: 0.10556, val loss: 0.10715\n",
      "Main effects training epoch: 27, train loss: 0.10595, val loss: 0.10680\n",
      "Main effects training epoch: 28, train loss: 0.10509, val loss: 0.10619\n",
      "Main effects training epoch: 29, train loss: 0.10460, val loss: 0.10568\n",
      "Main effects training epoch: 30, train loss: 0.10512, val loss: 0.10585\n",
      "Main effects training epoch: 31, train loss: 0.10437, val loss: 0.10598\n",
      "Main effects training epoch: 32, train loss: 0.10431, val loss: 0.10577\n",
      "Main effects training epoch: 33, train loss: 0.10423, val loss: 0.10545\n",
      "Main effects training epoch: 34, train loss: 0.10413, val loss: 0.10561\n",
      "Main effects training epoch: 35, train loss: 0.10424, val loss: 0.10579\n",
      "Main effects training epoch: 36, train loss: 0.10444, val loss: 0.10578\n",
      "Main effects training epoch: 37, train loss: 0.10400, val loss: 0.10575\n",
      "Main effects training epoch: 38, train loss: 0.10438, val loss: 0.10591\n",
      "Main effects training epoch: 39, train loss: 0.10416, val loss: 0.10566\n",
      "Main effects training epoch: 40, train loss: 0.10393, val loss: 0.10524\n",
      "Main effects training epoch: 41, train loss: 0.10418, val loss: 0.10579\n",
      "Main effects training epoch: 42, train loss: 0.10392, val loss: 0.10565\n",
      "Main effects training epoch: 43, train loss: 0.10430, val loss: 0.10612\n",
      "Main effects training epoch: 44, train loss: 0.10409, val loss: 0.10553\n",
      "Main effects training epoch: 45, train loss: 0.10403, val loss: 0.10539\n",
      "Main effects training epoch: 46, train loss: 0.10400, val loss: 0.10547\n",
      "Main effects training epoch: 47, train loss: 0.10472, val loss: 0.10625\n",
      "Main effects training epoch: 48, train loss: 0.10410, val loss: 0.10597\n",
      "Main effects training epoch: 49, train loss: 0.10415, val loss: 0.10512\n",
      "Main effects training epoch: 50, train loss: 0.10408, val loss: 0.10560\n",
      "Main effects training epoch: 51, train loss: 0.10392, val loss: 0.10554\n",
      "Main effects training epoch: 52, train loss: 0.10395, val loss: 0.10519\n",
      "Main effects training epoch: 53, train loss: 0.10385, val loss: 0.10520\n",
      "Main effects training epoch: 54, train loss: 0.10398, val loss: 0.10563\n",
      "Main effects training epoch: 55, train loss: 0.10416, val loss: 0.10523\n",
      "Main effects training epoch: 56, train loss: 0.10404, val loss: 0.10586\n",
      "Main effects training epoch: 57, train loss: 0.10399, val loss: 0.10571\n",
      "Main effects training epoch: 58, train loss: 0.10443, val loss: 0.10631\n",
      "Main effects training epoch: 59, train loss: 0.10393, val loss: 0.10549\n",
      "Main effects training epoch: 60, train loss: 0.10386, val loss: 0.10534\n",
      "Main effects training epoch: 61, train loss: 0.10389, val loss: 0.10538\n",
      "Main effects training epoch: 62, train loss: 0.10417, val loss: 0.10552\n",
      "Main effects training epoch: 63, train loss: 0.10396, val loss: 0.10570\n",
      "Main effects training epoch: 64, train loss: 0.10412, val loss: 0.10546\n",
      "Main effects training epoch: 65, train loss: 0.10403, val loss: 0.10549\n",
      "Main effects training epoch: 66, train loss: 0.10398, val loss: 0.10514\n",
      "Main effects training epoch: 67, train loss: 0.10385, val loss: 0.10514\n",
      "Main effects training epoch: 68, train loss: 0.10396, val loss: 0.10561\n",
      "Main effects training epoch: 69, train loss: 0.10397, val loss: 0.10523\n",
      "Main effects training epoch: 70, train loss: 0.10420, val loss: 0.10542\n",
      "Main effects training epoch: 71, train loss: 0.10473, val loss: 0.10671\n",
      "Main effects training epoch: 72, train loss: 0.10418, val loss: 0.10552\n",
      "Main effects training epoch: 73, train loss: 0.10427, val loss: 0.10590\n",
      "Main effects training epoch: 74, train loss: 0.10409, val loss: 0.10540\n",
      "Main effects training epoch: 75, train loss: 0.10401, val loss: 0.10542\n",
      "Main effects training epoch: 76, train loss: 0.10384, val loss: 0.10560\n",
      "Main effects training epoch: 77, train loss: 0.10404, val loss: 0.10542\n",
      "Main effects training epoch: 78, train loss: 0.10394, val loss: 0.10550\n",
      "Main effects training epoch: 79, train loss: 0.10410, val loss: 0.10553\n",
      "Main effects training epoch: 80, train loss: 0.10425, val loss: 0.10542\n",
      "Main effects training epoch: 81, train loss: 0.10387, val loss: 0.10530\n",
      "Main effects training epoch: 82, train loss: 0.10409, val loss: 0.10607\n",
      "Main effects training epoch: 83, train loss: 0.10386, val loss: 0.10559\n",
      "Main effects training epoch: 84, train loss: 0.10405, val loss: 0.10549\n",
      "Main effects training epoch: 85, train loss: 0.10483, val loss: 0.10617\n",
      "Main effects training epoch: 86, train loss: 0.10383, val loss: 0.10551\n",
      "Main effects training epoch: 87, train loss: 0.10415, val loss: 0.10524\n",
      "Main effects training epoch: 88, train loss: 0.10389, val loss: 0.10586\n",
      "Main effects training epoch: 89, train loss: 0.10383, val loss: 0.10548\n",
      "Main effects training epoch: 90, train loss: 0.10400, val loss: 0.10512\n",
      "Main effects training epoch: 91, train loss: 0.10387, val loss: 0.10538\n",
      "Main effects training epoch: 92, train loss: 0.10420, val loss: 0.10635\n",
      "Main effects training epoch: 93, train loss: 0.10396, val loss: 0.10529\n",
      "Main effects training epoch: 94, train loss: 0.10417, val loss: 0.10594\n",
      "Main effects training epoch: 95, train loss: 0.10387, val loss: 0.10562\n",
      "Main effects training epoch: 96, train loss: 0.10378, val loss: 0.10533\n",
      "Main effects training epoch: 97, train loss: 0.10386, val loss: 0.10536\n",
      "Main effects training epoch: 98, train loss: 0.10425, val loss: 0.10630\n",
      "Main effects training epoch: 99, train loss: 0.10426, val loss: 0.10552\n",
      "Main effects training epoch: 100, train loss: 0.10419, val loss: 0.10632\n",
      "Main effects training epoch: 101, train loss: 0.10395, val loss: 0.10553\n",
      "Main effects training epoch: 102, train loss: 0.10397, val loss: 0.10584\n",
      "Main effects training epoch: 103, train loss: 0.10408, val loss: 0.10574\n",
      "Main effects training epoch: 104, train loss: 0.10395, val loss: 0.10576\n",
      "Main effects training epoch: 105, train loss: 0.10380, val loss: 0.10555\n",
      "Main effects training epoch: 106, train loss: 0.10382, val loss: 0.10590\n",
      "Main effects training epoch: 107, train loss: 0.10417, val loss: 0.10600\n",
      "Main effects training epoch: 108, train loss: 0.10410, val loss: 0.10567\n",
      "Main effects training epoch: 109, train loss: 0.10373, val loss: 0.10542\n",
      "Main effects training epoch: 110, train loss: 0.10377, val loss: 0.10525\n",
      "Main effects training epoch: 111, train loss: 0.10405, val loss: 0.10577\n",
      "Main effects training epoch: 112, train loss: 0.10409, val loss: 0.10590\n",
      "Main effects training epoch: 113, train loss: 0.10474, val loss: 0.10615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 114, train loss: 0.10406, val loss: 0.10594\n",
      "Main effects training epoch: 115, train loss: 0.10415, val loss: 0.10553\n",
      "Main effects training epoch: 116, train loss: 0.10437, val loss: 0.10680\n",
      "Main effects training epoch: 117, train loss: 0.10399, val loss: 0.10532\n",
      "Main effects training epoch: 118, train loss: 0.10393, val loss: 0.10583\n",
      "Main effects training epoch: 119, train loss: 0.10410, val loss: 0.10601\n",
      "Main effects training epoch: 120, train loss: 0.10375, val loss: 0.10578\n",
      "Main effects training epoch: 121, train loss: 0.10382, val loss: 0.10529\n",
      "Main effects training epoch: 122, train loss: 0.10391, val loss: 0.10552\n",
      "Main effects training epoch: 123, train loss: 0.10397, val loss: 0.10615\n",
      "Main effects training epoch: 124, train loss: 0.10422, val loss: 0.10572\n",
      "Main effects training epoch: 125, train loss: 0.10414, val loss: 0.10601\n",
      "Main effects training epoch: 126, train loss: 0.10402, val loss: 0.10605\n",
      "Main effects training epoch: 127, train loss: 0.10402, val loss: 0.10531\n",
      "Main effects training epoch: 128, train loss: 0.10395, val loss: 0.10615\n",
      "Main effects training epoch: 129, train loss: 0.10410, val loss: 0.10643\n",
      "Main effects training epoch: 130, train loss: 0.10401, val loss: 0.10526\n",
      "Main effects training epoch: 131, train loss: 0.10380, val loss: 0.10577\n",
      "Main effects training epoch: 132, train loss: 0.10404, val loss: 0.10554\n",
      "Main effects training epoch: 133, train loss: 0.10425, val loss: 0.10648\n",
      "Main effects training epoch: 134, train loss: 0.10454, val loss: 0.10616\n",
      "Main effects training epoch: 135, train loss: 0.10424, val loss: 0.10653\n",
      "Main effects training epoch: 136, train loss: 0.10426, val loss: 0.10565\n",
      "Main effects training epoch: 137, train loss: 0.10444, val loss: 0.10639\n",
      "Main effects training epoch: 138, train loss: 0.10433, val loss: 0.10622\n",
      "Main effects training epoch: 139, train loss: 0.10384, val loss: 0.10565\n",
      "Main effects training epoch: 140, train loss: 0.10392, val loss: 0.10539\n",
      "Main effects training epoch: 141, train loss: 0.10420, val loss: 0.10593\n",
      "Main effects training epoch: 142, train loss: 0.10404, val loss: 0.10617\n",
      "Main effects training epoch: 143, train loss: 0.10420, val loss: 0.10567\n",
      "Main effects training epoch: 144, train loss: 0.10383, val loss: 0.10601\n",
      "Main effects training epoch: 145, train loss: 0.10370, val loss: 0.10530\n",
      "Main effects training epoch: 146, train loss: 0.10381, val loss: 0.10596\n",
      "Main effects training epoch: 147, train loss: 0.10380, val loss: 0.10575\n",
      "Main effects training epoch: 148, train loss: 0.10377, val loss: 0.10571\n",
      "Main effects training epoch: 149, train loss: 0.10383, val loss: 0.10566\n",
      "Main effects training epoch: 150, train loss: 0.10393, val loss: 0.10604\n",
      "Early stop at epoch 150, with validation loss: 0.10604\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.10405, val loss: 0.10498\n",
      "Main effects tuning epoch: 2, train loss: 0.10423, val loss: 0.10552\n",
      "Main effects tuning epoch: 3, train loss: 0.10411, val loss: 0.10505\n",
      "Main effects tuning epoch: 4, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 5, train loss: 0.10382, val loss: 0.10498\n",
      "Main effects tuning epoch: 6, train loss: 0.10425, val loss: 0.10582\n",
      "Main effects tuning epoch: 7, train loss: 0.10408, val loss: 0.10530\n",
      "Main effects tuning epoch: 8, train loss: 0.10397, val loss: 0.10527\n",
      "Main effects tuning epoch: 9, train loss: 0.10401, val loss: 0.10507\n",
      "Main effects tuning epoch: 10, train loss: 0.10407, val loss: 0.10539\n",
      "Main effects tuning epoch: 11, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 12, train loss: 0.10393, val loss: 0.10556\n",
      "Main effects tuning epoch: 13, train loss: 0.10415, val loss: 0.10510\n",
      "Main effects tuning epoch: 14, train loss: 0.10397, val loss: 0.10570\n",
      "Main effects tuning epoch: 15, train loss: 0.10386, val loss: 0.10516\n",
      "Main effects tuning epoch: 16, train loss: 0.10395, val loss: 0.10562\n",
      "Main effects tuning epoch: 17, train loss: 0.10395, val loss: 0.10539\n",
      "Main effects tuning epoch: 18, train loss: 0.10422, val loss: 0.10568\n",
      "Main effects tuning epoch: 19, train loss: 0.10433, val loss: 0.10517\n",
      "Main effects tuning epoch: 20, train loss: 0.10384, val loss: 0.10568\n",
      "Main effects tuning epoch: 21, train loss: 0.10400, val loss: 0.10541\n",
      "Main effects tuning epoch: 22, train loss: 0.10393, val loss: 0.10526\n",
      "Main effects tuning epoch: 23, train loss: 0.10434, val loss: 0.10582\n",
      "Main effects tuning epoch: 24, train loss: 0.10406, val loss: 0.10540\n",
      "Main effects tuning epoch: 25, train loss: 0.10410, val loss: 0.10570\n",
      "Main effects tuning epoch: 26, train loss: 0.10405, val loss: 0.10569\n",
      "Main effects tuning epoch: 27, train loss: 0.10406, val loss: 0.10556\n",
      "Main effects tuning epoch: 28, train loss: 0.10394, val loss: 0.10542\n",
      "Main effects tuning epoch: 29, train loss: 0.10383, val loss: 0.10542\n",
      "Main effects tuning epoch: 30, train loss: 0.10385, val loss: 0.10548\n",
      "Main effects tuning epoch: 31, train loss: 0.10395, val loss: 0.10516\n",
      "Main effects tuning epoch: 32, train loss: 0.10391, val loss: 0.10584\n",
      "Main effects tuning epoch: 33, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 34, train loss: 0.10391, val loss: 0.10561\n",
      "Main effects tuning epoch: 35, train loss: 0.10424, val loss: 0.10531\n",
      "Main effects tuning epoch: 36, train loss: 0.10395, val loss: 0.10554\n",
      "Main effects tuning epoch: 37, train loss: 0.10400, val loss: 0.10600\n",
      "Main effects tuning epoch: 38, train loss: 0.10408, val loss: 0.10530\n",
      "Main effects tuning epoch: 39, train loss: 0.10394, val loss: 0.10565\n",
      "Main effects tuning epoch: 40, train loss: 0.10385, val loss: 0.10569\n",
      "Main effects tuning epoch: 41, train loss: 0.10390, val loss: 0.10539\n",
      "Main effects tuning epoch: 42, train loss: 0.10400, val loss: 0.10587\n",
      "Main effects tuning epoch: 43, train loss: 0.10386, val loss: 0.10547\n",
      "Main effects tuning epoch: 44, train loss: 0.10403, val loss: 0.10597\n",
      "Main effects tuning epoch: 45, train loss: 0.10384, val loss: 0.10541\n",
      "Main effects tuning epoch: 46, train loss: 0.10398, val loss: 0.10582\n",
      "Main effects tuning epoch: 47, train loss: 0.10375, val loss: 0.10527\n",
      "Main effects tuning epoch: 48, train loss: 0.10380, val loss: 0.10549\n",
      "Main effects tuning epoch: 49, train loss: 0.10396, val loss: 0.10566\n",
      "Main effects tuning epoch: 50, train loss: 0.10407, val loss: 0.10549\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.09376, val loss: 0.09232\n",
      "Interaction training epoch: 2, train loss: 0.20140, val loss: 0.20111\n",
      "Interaction training epoch: 3, train loss: 0.06617, val loss: 0.06860\n",
      "Interaction training epoch: 4, train loss: 0.05173, val loss: 0.05197\n",
      "Interaction training epoch: 5, train loss: 0.06497, val loss: 0.06595\n",
      "Interaction training epoch: 6, train loss: 0.04386, val loss: 0.04449\n",
      "Interaction training epoch: 7, train loss: 0.04606, val loss: 0.04529\n",
      "Interaction training epoch: 8, train loss: 0.04492, val loss: 0.04489\n",
      "Interaction training epoch: 9, train loss: 0.04997, val loss: 0.04868\n",
      "Interaction training epoch: 10, train loss: 0.04002, val loss: 0.04133\n",
      "Interaction training epoch: 11, train loss: 0.04269, val loss: 0.04306\n",
      "Interaction training epoch: 12, train loss: 0.04100, val loss: 0.03997\n",
      "Interaction training epoch: 13, train loss: 0.04366, val loss: 0.04360\n",
      "Interaction training epoch: 14, train loss: 0.04928, val loss: 0.05084\n",
      "Interaction training epoch: 15, train loss: 0.04532, val loss: 0.04488\n",
      "Interaction training epoch: 16, train loss: 0.04530, val loss: 0.04524\n",
      "Interaction training epoch: 17, train loss: 0.03784, val loss: 0.03763\n",
      "Interaction training epoch: 18, train loss: 0.04939, val loss: 0.05023\n",
      "Interaction training epoch: 19, train loss: 0.03608, val loss: 0.03597\n",
      "Interaction training epoch: 20, train loss: 0.04344, val loss: 0.04367\n",
      "Interaction training epoch: 21, train loss: 0.04002, val loss: 0.03932\n",
      "Interaction training epoch: 22, train loss: 0.03977, val loss: 0.03977\n",
      "Interaction training epoch: 23, train loss: 0.03777, val loss: 0.03709\n",
      "Interaction training epoch: 24, train loss: 0.03927, val loss: 0.03924\n",
      "Interaction training epoch: 25, train loss: 0.04164, val loss: 0.04112\n",
      "Interaction training epoch: 26, train loss: 0.04429, val loss: 0.04389\n",
      "Interaction training epoch: 27, train loss: 0.04417, val loss: 0.04416\n",
      "Interaction training epoch: 28, train loss: 0.04241, val loss: 0.04112\n",
      "Interaction training epoch: 29, train loss: 0.03548, val loss: 0.03672\n",
      "Interaction training epoch: 30, train loss: 0.04420, val loss: 0.04353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 31, train loss: 0.04020, val loss: 0.04004\n",
      "Interaction training epoch: 32, train loss: 0.03422, val loss: 0.03451\n",
      "Interaction training epoch: 33, train loss: 0.03378, val loss: 0.03407\n",
      "Interaction training epoch: 34, train loss: 0.03925, val loss: 0.03880\n",
      "Interaction training epoch: 35, train loss: 0.03752, val loss: 0.03636\n",
      "Interaction training epoch: 36, train loss: 0.03582, val loss: 0.03564\n",
      "Interaction training epoch: 37, train loss: 0.03069, val loss: 0.03087\n",
      "Interaction training epoch: 38, train loss: 0.03729, val loss: 0.03652\n",
      "Interaction training epoch: 39, train loss: 0.03772, val loss: 0.03772\n",
      "Interaction training epoch: 40, train loss: 0.04332, val loss: 0.04201\n",
      "Interaction training epoch: 41, train loss: 0.03409, val loss: 0.03435\n",
      "Interaction training epoch: 42, train loss: 0.03501, val loss: 0.03419\n",
      "Interaction training epoch: 43, train loss: 0.03630, val loss: 0.03573\n",
      "Interaction training epoch: 44, train loss: 0.03447, val loss: 0.03420\n",
      "Interaction training epoch: 45, train loss: 0.03622, val loss: 0.03541\n",
      "Interaction training epoch: 46, train loss: 0.02991, val loss: 0.03002\n",
      "Interaction training epoch: 47, train loss: 0.02880, val loss: 0.02856\n",
      "Interaction training epoch: 48, train loss: 0.03548, val loss: 0.03469\n",
      "Interaction training epoch: 49, train loss: 0.03399, val loss: 0.03256\n",
      "Interaction training epoch: 50, train loss: 0.03225, val loss: 0.03266\n",
      "Interaction training epoch: 51, train loss: 0.03286, val loss: 0.03176\n",
      "Interaction training epoch: 52, train loss: 0.03084, val loss: 0.03012\n",
      "Interaction training epoch: 53, train loss: 0.03059, val loss: 0.02998\n",
      "Interaction training epoch: 54, train loss: 0.02966, val loss: 0.02932\n",
      "Interaction training epoch: 55, train loss: 0.04122, val loss: 0.04119\n",
      "Interaction training epoch: 56, train loss: 0.03164, val loss: 0.03059\n",
      "Interaction training epoch: 57, train loss: 0.03483, val loss: 0.03469\n",
      "Interaction training epoch: 58, train loss: 0.03320, val loss: 0.03294\n",
      "Interaction training epoch: 59, train loss: 0.02999, val loss: 0.02930\n",
      "Interaction training epoch: 60, train loss: 0.03540, val loss: 0.03496\n",
      "Interaction training epoch: 61, train loss: 0.03142, val loss: 0.02962\n",
      "Interaction training epoch: 62, train loss: 0.03146, val loss: 0.03164\n",
      "Interaction training epoch: 63, train loss: 0.03481, val loss: 0.03421\n",
      "Interaction training epoch: 64, train loss: 0.02946, val loss: 0.02881\n",
      "Interaction training epoch: 65, train loss: 0.03539, val loss: 0.03511\n",
      "Interaction training epoch: 66, train loss: 0.03797, val loss: 0.03805\n",
      "Interaction training epoch: 67, train loss: 0.03669, val loss: 0.03627\n",
      "Interaction training epoch: 68, train loss: 0.04844, val loss: 0.04881\n",
      "Interaction training epoch: 69, train loss: 0.02947, val loss: 0.02941\n",
      "Interaction training epoch: 70, train loss: 0.03805, val loss: 0.03672\n",
      "Interaction training epoch: 71, train loss: 0.02990, val loss: 0.02970\n",
      "Interaction training epoch: 72, train loss: 0.03926, val loss: 0.03952\n",
      "Interaction training epoch: 73, train loss: 0.03713, val loss: 0.03732\n",
      "Interaction training epoch: 74, train loss: 0.04469, val loss: 0.04392\n",
      "Interaction training epoch: 75, train loss: 0.03518, val loss: 0.03491\n",
      "Interaction training epoch: 76, train loss: 0.03954, val loss: 0.03893\n",
      "Interaction training epoch: 77, train loss: 0.04294, val loss: 0.04248\n",
      "Interaction training epoch: 78, train loss: 0.03122, val loss: 0.03170\n",
      "Interaction training epoch: 79, train loss: 0.03577, val loss: 0.03538\n",
      "Interaction training epoch: 80, train loss: 0.04727, val loss: 0.04664\n",
      "Interaction training epoch: 81, train loss: 0.03478, val loss: 0.03412\n",
      "Interaction training epoch: 82, train loss: 0.03736, val loss: 0.03700\n",
      "Interaction training epoch: 83, train loss: 0.03716, val loss: 0.03665\n",
      "Interaction training epoch: 84, train loss: 0.03675, val loss: 0.03575\n",
      "Interaction training epoch: 85, train loss: 0.03815, val loss: 0.03817\n",
      "Interaction training epoch: 86, train loss: 0.03960, val loss: 0.03903\n",
      "Interaction training epoch: 87, train loss: 0.03498, val loss: 0.03402\n",
      "Interaction training epoch: 88, train loss: 0.03630, val loss: 0.03583\n",
      "Interaction training epoch: 89, train loss: 0.03454, val loss: 0.03517\n",
      "Interaction training epoch: 90, train loss: 0.07231, val loss: 0.07223\n",
      "Interaction training epoch: 91, train loss: 0.04800, val loss: 0.04802\n",
      "Interaction training epoch: 92, train loss: 0.05224, val loss: 0.05162\n",
      "Interaction training epoch: 93, train loss: 0.02794, val loss: 0.02804\n",
      "Interaction training epoch: 94, train loss: 0.03585, val loss: 0.03546\n",
      "Interaction training epoch: 95, train loss: 0.04149, val loss: 0.04158\n",
      "Interaction training epoch: 96, train loss: 0.04943, val loss: 0.04909\n",
      "Interaction training epoch: 97, train loss: 0.05722, val loss: 0.05690\n",
      "Interaction training epoch: 98, train loss: 0.04854, val loss: 0.04831\n",
      "Interaction training epoch: 99, train loss: 0.02719, val loss: 0.02716\n",
      "Interaction training epoch: 100, train loss: 0.03689, val loss: 0.03722\n",
      "Interaction training epoch: 101, train loss: 0.04609, val loss: 0.04601\n",
      "Interaction training epoch: 102, train loss: 0.03250, val loss: 0.03231\n",
      "Interaction training epoch: 103, train loss: 0.03006, val loss: 0.02936\n",
      "Interaction training epoch: 104, train loss: 0.03967, val loss: 0.03937\n",
      "Interaction training epoch: 105, train loss: 0.03096, val loss: 0.03100\n",
      "Interaction training epoch: 106, train loss: 0.05099, val loss: 0.05044\n",
      "Interaction training epoch: 107, train loss: 0.03358, val loss: 0.03389\n",
      "Interaction training epoch: 108, train loss: 0.04273, val loss: 0.04236\n",
      "Interaction training epoch: 109, train loss: 0.07259, val loss: 0.07140\n",
      "Interaction training epoch: 110, train loss: 0.03442, val loss: 0.03433\n",
      "Interaction training epoch: 111, train loss: 0.02947, val loss: 0.02893\n",
      "Interaction training epoch: 112, train loss: 0.03413, val loss: 0.03396\n",
      "Interaction training epoch: 113, train loss: 0.03441, val loss: 0.03428\n",
      "Interaction training epoch: 114, train loss: 0.05762, val loss: 0.05645\n",
      "Interaction training epoch: 115, train loss: 0.02938, val loss: 0.02918\n",
      "Interaction training epoch: 116, train loss: 0.04805, val loss: 0.04824\n",
      "Interaction training epoch: 117, train loss: 0.03927, val loss: 0.03835\n",
      "Interaction training epoch: 118, train loss: 0.03949, val loss: 0.03920\n",
      "Interaction training epoch: 119, train loss: 0.03835, val loss: 0.03852\n",
      "Interaction training epoch: 120, train loss: 0.07402, val loss: 0.07267\n",
      "Interaction training epoch: 121, train loss: 0.02786, val loss: 0.02794\n",
      "Interaction training epoch: 122, train loss: 0.02966, val loss: 0.02840\n",
      "Interaction training epoch: 123, train loss: 0.03146, val loss: 0.03080\n",
      "Interaction training epoch: 124, train loss: 0.03135, val loss: 0.03095\n",
      "Interaction training epoch: 125, train loss: 0.02893, val loss: 0.02832\n",
      "Interaction training epoch: 126, train loss: 0.06644, val loss: 0.06554\n",
      "Interaction training epoch: 127, train loss: 0.02439, val loss: 0.02405\n",
      "Interaction training epoch: 128, train loss: 0.02442, val loss: 0.02392\n",
      "Interaction training epoch: 129, train loss: 0.06718, val loss: 0.06662\n",
      "Interaction training epoch: 130, train loss: 0.06873, val loss: 0.06752\n",
      "Interaction training epoch: 131, train loss: 0.02836, val loss: 0.02795\n",
      "Interaction training epoch: 132, train loss: 0.02537, val loss: 0.02462\n",
      "Interaction training epoch: 133, train loss: 0.05578, val loss: 0.05497\n",
      "Interaction training epoch: 134, train loss: 0.02756, val loss: 0.02628\n",
      "Interaction training epoch: 135, train loss: 0.03851, val loss: 0.03792\n",
      "Interaction training epoch: 136, train loss: 0.02792, val loss: 0.02763\n",
      "Interaction training epoch: 137, train loss: 0.02882, val loss: 0.02822\n",
      "Interaction training epoch: 138, train loss: 0.04268, val loss: 0.04230\n",
      "Interaction training epoch: 139, train loss: 0.02999, val loss: 0.03028\n",
      "Interaction training epoch: 140, train loss: 0.02653, val loss: 0.02598\n",
      "Interaction training epoch: 141, train loss: 0.02864, val loss: 0.02740\n",
      "Interaction training epoch: 142, train loss: 0.04031, val loss: 0.03930\n",
      "Interaction training epoch: 143, train loss: 0.08930, val loss: 0.08825\n",
      "Interaction training epoch: 144, train loss: 0.03278, val loss: 0.03201\n",
      "Interaction training epoch: 145, train loss: 0.07595, val loss: 0.07444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 146, train loss: 0.04088, val loss: 0.04007\n",
      "Interaction training epoch: 147, train loss: 0.03063, val loss: 0.02967\n",
      "Interaction training epoch: 148, train loss: 0.08210, val loss: 0.08116\n",
      "Interaction training epoch: 149, train loss: 0.02600, val loss: 0.02595\n",
      "Interaction training epoch: 150, train loss: 0.04451, val loss: 0.04266\n",
      "Interaction training epoch: 151, train loss: 0.05071, val loss: 0.04953\n",
      "Interaction training epoch: 152, train loss: 0.03689, val loss: 0.03657\n",
      "Interaction training epoch: 153, train loss: 0.05108, val loss: 0.05017\n",
      "Interaction training epoch: 154, train loss: 0.03125, val loss: 0.03020\n",
      "Interaction training epoch: 155, train loss: 0.05262, val loss: 0.05160\n",
      "Interaction training epoch: 156, train loss: 0.03189, val loss: 0.03099\n",
      "Interaction training epoch: 157, train loss: 0.03302, val loss: 0.03192\n",
      "Interaction training epoch: 158, train loss: 0.03919, val loss: 0.03867\n",
      "Interaction training epoch: 159, train loss: 0.06294, val loss: 0.06164\n",
      "Interaction training epoch: 160, train loss: 0.02504, val loss: 0.02398\n",
      "Interaction training epoch: 161, train loss: 0.02726, val loss: 0.02622\n",
      "Interaction training epoch: 162, train loss: 0.02696, val loss: 0.02663\n",
      "Interaction training epoch: 163, train loss: 0.04443, val loss: 0.04306\n",
      "Interaction training epoch: 164, train loss: 0.08229, val loss: 0.08080\n",
      "Interaction training epoch: 165, train loss: 0.02497, val loss: 0.02433\n",
      "Interaction training epoch: 166, train loss: 0.02405, val loss: 0.02321\n",
      "Interaction training epoch: 167, train loss: 0.03154, val loss: 0.03087\n",
      "Interaction training epoch: 168, train loss: 0.03079, val loss: 0.02948\n",
      "Interaction training epoch: 169, train loss: 0.02562, val loss: 0.02466\n",
      "Interaction training epoch: 170, train loss: 0.03950, val loss: 0.03811\n",
      "Interaction training epoch: 171, train loss: 0.05102, val loss: 0.05005\n",
      "Interaction training epoch: 172, train loss: 0.07393, val loss: 0.07199\n",
      "Interaction training epoch: 173, train loss: 0.03775, val loss: 0.03659\n",
      "Interaction training epoch: 174, train loss: 0.03283, val loss: 0.03172\n",
      "Interaction training epoch: 175, train loss: 0.02729, val loss: 0.02663\n",
      "Interaction training epoch: 176, train loss: 0.04249, val loss: 0.04090\n",
      "Interaction training epoch: 177, train loss: 0.11371, val loss: 0.11248\n",
      "Interaction training epoch: 178, train loss: 0.05008, val loss: 0.04851\n",
      "Interaction training epoch: 179, train loss: 0.05070, val loss: 0.04903\n",
      "Interaction training epoch: 180, train loss: 0.05192, val loss: 0.05082\n",
      "Interaction training epoch: 181, train loss: 0.03499, val loss: 0.03401\n",
      "Interaction training epoch: 182, train loss: 0.06009, val loss: 0.05850\n",
      "Interaction training epoch: 183, train loss: 0.03517, val loss: 0.03374\n",
      "Interaction training epoch: 184, train loss: 0.04107, val loss: 0.04010\n",
      "Interaction training epoch: 185, train loss: 0.06361, val loss: 0.06162\n",
      "Interaction training epoch: 186, train loss: 0.06349, val loss: 0.06178\n",
      "Interaction training epoch: 187, train loss: 0.05374, val loss: 0.05196\n",
      "Interaction training epoch: 188, train loss: 0.05958, val loss: 0.05804\n",
      "Interaction training epoch: 189, train loss: 0.02604, val loss: 0.02519\n",
      "Interaction training epoch: 190, train loss: 0.04369, val loss: 0.04302\n",
      "Interaction training epoch: 191, train loss: 0.04175, val loss: 0.04116\n",
      "Interaction training epoch: 192, train loss: 0.06782, val loss: 0.06645\n",
      "Interaction training epoch: 193, train loss: 0.06145, val loss: 0.06034\n",
      "Interaction training epoch: 194, train loss: 0.04303, val loss: 0.04151\n",
      "Interaction training epoch: 195, train loss: 0.03319, val loss: 0.03229\n",
      "Interaction training epoch: 196, train loss: 0.03143, val loss: 0.02941\n",
      "Interaction training epoch: 197, train loss: 0.03413, val loss: 0.03311\n",
      "Interaction training epoch: 198, train loss: 0.03045, val loss: 0.02901\n",
      "Interaction training epoch: 199, train loss: 0.04253, val loss: 0.04138\n",
      "Interaction training epoch: 200, train loss: 0.04032, val loss: 0.03845\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########4 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.02853, val loss: 0.02785\n",
      "Interaction tuning epoch: 2, train loss: 0.02401, val loss: 0.02299\n",
      "Interaction tuning epoch: 3, train loss: 0.04478, val loss: 0.04265\n",
      "Interaction tuning epoch: 4, train loss: 0.06395, val loss: 0.06292\n",
      "Interaction tuning epoch: 5, train loss: 0.02356, val loss: 0.02286\n",
      "Interaction tuning epoch: 6, train loss: 0.06369, val loss: 0.06228\n",
      "Interaction tuning epoch: 7, train loss: 0.04154, val loss: 0.04077\n",
      "Interaction tuning epoch: 8, train loss: 0.08354, val loss: 0.08227\n",
      "Interaction tuning epoch: 9, train loss: 0.07421, val loss: 0.07270\n",
      "Interaction tuning epoch: 10, train loss: 0.04015, val loss: 0.03931\n",
      "Interaction tuning epoch: 11, train loss: 0.02840, val loss: 0.02746\n",
      "Interaction tuning epoch: 12, train loss: 0.06172, val loss: 0.06021\n",
      "Interaction tuning epoch: 13, train loss: 0.03776, val loss: 0.03699\n",
      "Interaction tuning epoch: 14, train loss: 0.07388, val loss: 0.07213\n",
      "Interaction tuning epoch: 15, train loss: 0.02628, val loss: 0.02557\n",
      "Interaction tuning epoch: 16, train loss: 0.02860, val loss: 0.02813\n",
      "Interaction tuning epoch: 17, train loss: 0.04973, val loss: 0.04802\n",
      "Interaction tuning epoch: 18, train loss: 0.02869, val loss: 0.02741\n",
      "Interaction tuning epoch: 19, train loss: 0.03914, val loss: 0.03730\n",
      "Interaction tuning epoch: 20, train loss: 0.03596, val loss: 0.03526\n",
      "Interaction tuning epoch: 21, train loss: 0.04924, val loss: 0.04797\n",
      "Interaction tuning epoch: 22, train loss: 0.08235, val loss: 0.08148\n",
      "Interaction tuning epoch: 23, train loss: 0.08430, val loss: 0.08186\n",
      "Interaction tuning epoch: 24, train loss: 0.07758, val loss: 0.07581\n",
      "Interaction tuning epoch: 25, train loss: 0.02468, val loss: 0.02425\n",
      "Interaction tuning epoch: 26, train loss: 0.02729, val loss: 0.02674\n",
      "Interaction tuning epoch: 27, train loss: 0.02677, val loss: 0.02542\n",
      "Interaction tuning epoch: 28, train loss: 0.03303, val loss: 0.03233\n",
      "Interaction tuning epoch: 29, train loss: 0.07628, val loss: 0.07409\n",
      "Interaction tuning epoch: 30, train loss: 0.05354, val loss: 0.05248\n",
      "Interaction tuning epoch: 31, train loss: 0.02819, val loss: 0.02642\n",
      "Interaction tuning epoch: 32, train loss: 0.02994, val loss: 0.02905\n",
      "Interaction tuning epoch: 33, train loss: 0.03903, val loss: 0.03809\n",
      "Interaction tuning epoch: 34, train loss: 0.03406, val loss: 0.03261\n",
      "Interaction tuning epoch: 35, train loss: 0.02886, val loss: 0.02797\n",
      "Interaction tuning epoch: 36, train loss: 0.04728, val loss: 0.04591\n",
      "Interaction tuning epoch: 37, train loss: 0.02745, val loss: 0.02653\n",
      "Interaction tuning epoch: 38, train loss: 0.09613, val loss: 0.09462\n",
      "Interaction tuning epoch: 39, train loss: 0.06430, val loss: 0.06326\n",
      "Interaction tuning epoch: 40, train loss: 0.03564, val loss: 0.03467\n",
      "Interaction tuning epoch: 41, train loss: 0.05842, val loss: 0.05720\n",
      "Interaction tuning epoch: 42, train loss: 0.02719, val loss: 0.02601\n",
      "Interaction tuning epoch: 43, train loss: 0.03689, val loss: 0.03533\n",
      "Interaction tuning epoch: 44, train loss: 0.02563, val loss: 0.02452\n",
      "Interaction tuning epoch: 45, train loss: 0.11515, val loss: 0.11336\n",
      "Interaction tuning epoch: 46, train loss: 0.05383, val loss: 0.05274\n",
      "Interaction tuning epoch: 47, train loss: 0.03183, val loss: 0.02986\n",
      "Interaction tuning epoch: 48, train loss: 0.04755, val loss: 0.04566\n",
      "Interaction tuning epoch: 49, train loss: 0.03504, val loss: 0.03437\n",
      "Interaction tuning epoch: 50, train loss: 0.03646, val loss: 0.03600\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 33.001482009887695\n",
      "After the gam stage, training error is 0.03646 , validation error is 0.03600\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 1.968308\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.024751 validation MAE=0.034101,rank=8\n",
      "[SoftImpute] Iter 2: observed MAE=0.022487 validation MAE=0.033165,rank=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 3: observed MAE=0.020586 validation MAE=0.032315,rank=8\n",
      "[SoftImpute] Iter 4: observed MAE=0.018960 validation MAE=0.031547,rank=8\n",
      "[SoftImpute] Iter 5: observed MAE=0.017560 validation MAE=0.030881,rank=8\n",
      "[SoftImpute] Iter 6: observed MAE=0.016351 validation MAE=0.030301,rank=8\n",
      "[SoftImpute] Iter 7: observed MAE=0.015300 validation MAE=0.029780,rank=8\n",
      "[SoftImpute] Iter 8: observed MAE=0.014382 validation MAE=0.029310,rank=8\n",
      "[SoftImpute] Iter 9: observed MAE=0.013575 validation MAE=0.028890,rank=8\n",
      "[SoftImpute] Iter 10: observed MAE=0.012862 validation MAE=0.028516,rank=8\n",
      "[SoftImpute] Iter 11: observed MAE=0.012232 validation MAE=0.028176,rank=8\n",
      "[SoftImpute] Iter 12: observed MAE=0.011672 validation MAE=0.027872,rank=8\n",
      "[SoftImpute] Iter 13: observed MAE=0.011169 validation MAE=0.027591,rank=8\n",
      "[SoftImpute] Iter 14: observed MAE=0.010717 validation MAE=0.027334,rank=8\n",
      "[SoftImpute] Iter 15: observed MAE=0.010308 validation MAE=0.027095,rank=8\n",
      "[SoftImpute] Iter 16: observed MAE=0.009938 validation MAE=0.026873,rank=8\n",
      "[SoftImpute] Iter 17: observed MAE=0.009601 validation MAE=0.026669,rank=8\n",
      "[SoftImpute] Iter 18: observed MAE=0.009294 validation MAE=0.026477,rank=8\n",
      "[SoftImpute] Iter 19: observed MAE=0.009013 validation MAE=0.026295,rank=8\n",
      "[SoftImpute] Iter 20: observed MAE=0.008754 validation MAE=0.026123,rank=8\n",
      "[SoftImpute] Iter 21: observed MAE=0.008518 validation MAE=0.025965,rank=8\n",
      "[SoftImpute] Iter 22: observed MAE=0.008301 validation MAE=0.025816,rank=8\n",
      "[SoftImpute] Iter 23: observed MAE=0.008101 validation MAE=0.025672,rank=8\n",
      "[SoftImpute] Iter 24: observed MAE=0.007916 validation MAE=0.025534,rank=8\n",
      "[SoftImpute] Iter 25: observed MAE=0.007744 validation MAE=0.025402,rank=8\n",
      "[SoftImpute] Iter 26: observed MAE=0.007584 validation MAE=0.025276,rank=8\n",
      "[SoftImpute] Iter 27: observed MAE=0.007434 validation MAE=0.025156,rank=8\n",
      "[SoftImpute] Iter 28: observed MAE=0.007294 validation MAE=0.025040,rank=8\n",
      "[SoftImpute] Iter 29: observed MAE=0.007164 validation MAE=0.024931,rank=8\n",
      "[SoftImpute] Iter 30: observed MAE=0.007041 validation MAE=0.024827,rank=8\n",
      "[SoftImpute] Iter 31: observed MAE=0.006925 validation MAE=0.024727,rank=8\n",
      "[SoftImpute] Iter 32: observed MAE=0.006817 validation MAE=0.024632,rank=8\n",
      "[SoftImpute] Iter 33: observed MAE=0.006715 validation MAE=0.024540,rank=8\n",
      "[SoftImpute] Iter 34: observed MAE=0.006619 validation MAE=0.024451,rank=8\n",
      "[SoftImpute] Iter 35: observed MAE=0.006528 validation MAE=0.024366,rank=8\n",
      "[SoftImpute] Iter 36: observed MAE=0.006443 validation MAE=0.024283,rank=8\n",
      "[SoftImpute] Iter 37: observed MAE=0.006361 validation MAE=0.024203,rank=8\n",
      "[SoftImpute] Iter 38: observed MAE=0.006284 validation MAE=0.024124,rank=8\n",
      "[SoftImpute] Iter 39: observed MAE=0.006211 validation MAE=0.024048,rank=8\n",
      "[SoftImpute] Iter 40: observed MAE=0.006141 validation MAE=0.023973,rank=8\n",
      "[SoftImpute] Iter 41: observed MAE=0.006074 validation MAE=0.023901,rank=8\n",
      "[SoftImpute] Iter 42: observed MAE=0.006010 validation MAE=0.023829,rank=8\n",
      "[SoftImpute] Iter 43: observed MAE=0.005949 validation MAE=0.023759,rank=8\n",
      "[SoftImpute] Iter 44: observed MAE=0.005891 validation MAE=0.023691,rank=8\n",
      "[SoftImpute] Iter 45: observed MAE=0.005836 validation MAE=0.023624,rank=8\n",
      "[SoftImpute] Iter 46: observed MAE=0.005783 validation MAE=0.023558,rank=8\n",
      "[SoftImpute] Iter 47: observed MAE=0.005732 validation MAE=0.023495,rank=8\n",
      "[SoftImpute] Iter 48: observed MAE=0.005683 validation MAE=0.023434,rank=8\n",
      "[SoftImpute] Iter 49: observed MAE=0.005636 validation MAE=0.023374,rank=8\n",
      "[SoftImpute] Iter 50: observed MAE=0.005591 validation MAE=0.023315,rank=8\n",
      "[SoftImpute] Iter 51: observed MAE=0.005547 validation MAE=0.023258,rank=8\n",
      "[SoftImpute] Iter 52: observed MAE=0.005506 validation MAE=0.023202,rank=8\n",
      "[SoftImpute] Iter 53: observed MAE=0.005466 validation MAE=0.023146,rank=8\n",
      "[SoftImpute] Iter 54: observed MAE=0.005427 validation MAE=0.023091,rank=8\n",
      "[SoftImpute] Iter 55: observed MAE=0.005390 validation MAE=0.023038,rank=8\n",
      "[SoftImpute] Iter 56: observed MAE=0.005354 validation MAE=0.022985,rank=8\n",
      "[SoftImpute] Iter 57: observed MAE=0.005320 validation MAE=0.022933,rank=8\n",
      "[SoftImpute] Iter 58: observed MAE=0.005286 validation MAE=0.022882,rank=8\n",
      "[SoftImpute] Iter 59: observed MAE=0.005254 validation MAE=0.022831,rank=8\n",
      "[SoftImpute] Iter 60: observed MAE=0.005223 validation MAE=0.022781,rank=8\n",
      "[SoftImpute] Iter 61: observed MAE=0.005194 validation MAE=0.022732,rank=8\n",
      "[SoftImpute] Iter 62: observed MAE=0.005165 validation MAE=0.022683,rank=8\n",
      "[SoftImpute] Iter 63: observed MAE=0.005137 validation MAE=0.022635,rank=8\n",
      "[SoftImpute] Iter 64: observed MAE=0.005110 validation MAE=0.022588,rank=8\n",
      "[SoftImpute] Iter 65: observed MAE=0.005084 validation MAE=0.022541,rank=8\n",
      "[SoftImpute] Iter 66: observed MAE=0.005058 validation MAE=0.022495,rank=8\n",
      "[SoftImpute] Iter 67: observed MAE=0.005034 validation MAE=0.022449,rank=8\n",
      "[SoftImpute] Iter 68: observed MAE=0.005010 validation MAE=0.022404,rank=8\n",
      "[SoftImpute] Iter 69: observed MAE=0.004987 validation MAE=0.022360,rank=8\n",
      "[SoftImpute] Iter 70: observed MAE=0.004964 validation MAE=0.022316,rank=8\n",
      "[SoftImpute] Iter 71: observed MAE=0.004943 validation MAE=0.022273,rank=8\n",
      "[SoftImpute] Iter 72: observed MAE=0.004922 validation MAE=0.022230,rank=8\n",
      "[SoftImpute] Iter 73: observed MAE=0.004901 validation MAE=0.022187,rank=8\n",
      "[SoftImpute] Iter 74: observed MAE=0.004881 validation MAE=0.022145,rank=8\n",
      "[SoftImpute] Iter 75: observed MAE=0.004862 validation MAE=0.022104,rank=8\n",
      "[SoftImpute] Iter 76: observed MAE=0.004843 validation MAE=0.022063,rank=8\n",
      "[SoftImpute] Iter 77: observed MAE=0.004825 validation MAE=0.022023,rank=8\n",
      "[SoftImpute] Iter 78: observed MAE=0.004807 validation MAE=0.021984,rank=8\n",
      "[SoftImpute] Iter 79: observed MAE=0.004790 validation MAE=0.021944,rank=8\n",
      "[SoftImpute] Iter 80: observed MAE=0.004773 validation MAE=0.021905,rank=8\n",
      "[SoftImpute] Iter 81: observed MAE=0.004757 validation MAE=0.021866,rank=8\n",
      "[SoftImpute] Iter 82: observed MAE=0.004741 validation MAE=0.021828,rank=8\n",
      "[SoftImpute] Iter 83: observed MAE=0.004725 validation MAE=0.021791,rank=8\n",
      "[SoftImpute] Iter 84: observed MAE=0.004710 validation MAE=0.021753,rank=8\n",
      "[SoftImpute] Iter 85: observed MAE=0.004696 validation MAE=0.021716,rank=8\n",
      "[SoftImpute] Iter 86: observed MAE=0.004682 validation MAE=0.021680,rank=8\n",
      "[SoftImpute] Iter 87: observed MAE=0.004668 validation MAE=0.021644,rank=8\n",
      "[SoftImpute] Iter 88: observed MAE=0.004655 validation MAE=0.021608,rank=8\n",
      "[SoftImpute] Iter 89: observed MAE=0.004641 validation MAE=0.021572,rank=8\n",
      "[SoftImpute] Iter 90: observed MAE=0.004629 validation MAE=0.021536,rank=8\n",
      "[SoftImpute] Iter 91: observed MAE=0.004616 validation MAE=0.021501,rank=8\n",
      "[SoftImpute] Iter 92: observed MAE=0.004604 validation MAE=0.021466,rank=8\n",
      "[SoftImpute] Iter 93: observed MAE=0.004593 validation MAE=0.021431,rank=8\n",
      "[SoftImpute] Iter 94: observed MAE=0.004581 validation MAE=0.021397,rank=8\n",
      "[SoftImpute] Iter 95: observed MAE=0.004570 validation MAE=0.021362,rank=8\n",
      "[SoftImpute] Iter 96: observed MAE=0.004559 validation MAE=0.021328,rank=8\n",
      "[SoftImpute] Iter 97: observed MAE=0.004549 validation MAE=0.021295,rank=8\n",
      "[SoftImpute] Iter 98: observed MAE=0.004538 validation MAE=0.021261,rank=8\n",
      "[SoftImpute] Iter 99: observed MAE=0.004528 validation MAE=0.021228,rank=8\n",
      "[SoftImpute] Iter 100: observed MAE=0.004518 validation MAE=0.021196,rank=8\n",
      "[SoftImpute] Iter 101: observed MAE=0.004509 validation MAE=0.021163,rank=8\n",
      "[SoftImpute] Iter 102: observed MAE=0.004499 validation MAE=0.021131,rank=8\n",
      "[SoftImpute] Iter 103: observed MAE=0.004490 validation MAE=0.021099,rank=8\n",
      "[SoftImpute] Iter 104: observed MAE=0.004481 validation MAE=0.021067,rank=8\n",
      "[SoftImpute] Iter 105: observed MAE=0.004472 validation MAE=0.021035,rank=8\n",
      "[SoftImpute] Iter 106: observed MAE=0.004464 validation MAE=0.021004,rank=8\n",
      "[SoftImpute] Iter 107: observed MAE=0.004455 validation MAE=0.020972,rank=8\n",
      "[SoftImpute] Iter 108: observed MAE=0.004447 validation MAE=0.020942,rank=8\n",
      "[SoftImpute] Iter 109: observed MAE=0.004439 validation MAE=0.020911,rank=8\n",
      "[SoftImpute] Iter 110: observed MAE=0.004431 validation MAE=0.020881,rank=8\n",
      "[SoftImpute] Iter 111: observed MAE=0.004423 validation MAE=0.020851,rank=8\n",
      "[SoftImpute] Iter 112: observed MAE=0.004416 validation MAE=0.020821,rank=8\n",
      "[SoftImpute] Iter 113: observed MAE=0.004408 validation MAE=0.020791,rank=8\n",
      "[SoftImpute] Iter 114: observed MAE=0.004401 validation MAE=0.020761,rank=8\n",
      "[SoftImpute] Iter 115: observed MAE=0.004394 validation MAE=0.020732,rank=8\n",
      "[SoftImpute] Iter 116: observed MAE=0.004387 validation MAE=0.020703,rank=8\n",
      "[SoftImpute] Iter 117: observed MAE=0.004380 validation MAE=0.020674,rank=8\n",
      "[SoftImpute] Iter 118: observed MAE=0.004373 validation MAE=0.020645,rank=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 119: observed MAE=0.004367 validation MAE=0.020616,rank=8\n",
      "[SoftImpute] Iter 120: observed MAE=0.004360 validation MAE=0.020588,rank=8\n",
      "[SoftImpute] Iter 121: observed MAE=0.004354 validation MAE=0.020560,rank=8\n",
      "[SoftImpute] Iter 122: observed MAE=0.004348 validation MAE=0.020532,rank=8\n",
      "[SoftImpute] Iter 123: observed MAE=0.004342 validation MAE=0.020504,rank=8\n",
      "[SoftImpute] Iter 124: observed MAE=0.004336 validation MAE=0.020477,rank=8\n",
      "[SoftImpute] Iter 125: observed MAE=0.004330 validation MAE=0.020449,rank=8\n",
      "[SoftImpute] Iter 126: observed MAE=0.004324 validation MAE=0.020422,rank=8\n",
      "[SoftImpute] Iter 127: observed MAE=0.004318 validation MAE=0.020395,rank=8\n",
      "[SoftImpute] Iter 128: observed MAE=0.004312 validation MAE=0.020368,rank=8\n",
      "[SoftImpute] Iter 129: observed MAE=0.004307 validation MAE=0.020341,rank=8\n",
      "[SoftImpute] Iter 130: observed MAE=0.004302 validation MAE=0.020315,rank=8\n",
      "[SoftImpute] Iter 131: observed MAE=0.004296 validation MAE=0.020288,rank=8\n",
      "[SoftImpute] Iter 132: observed MAE=0.004291 validation MAE=0.020262,rank=8\n",
      "[SoftImpute] Iter 133: observed MAE=0.004286 validation MAE=0.020236,rank=8\n",
      "[SoftImpute] Iter 134: observed MAE=0.004281 validation MAE=0.020210,rank=8\n",
      "[SoftImpute] Iter 135: observed MAE=0.004276 validation MAE=0.020185,rank=8\n",
      "[SoftImpute] Iter 136: observed MAE=0.004271 validation MAE=0.020159,rank=8\n",
      "[SoftImpute] Iter 137: observed MAE=0.004266 validation MAE=0.020134,rank=8\n",
      "[SoftImpute] Iter 138: observed MAE=0.004261 validation MAE=0.020109,rank=8\n",
      "[SoftImpute] Iter 139: observed MAE=0.004256 validation MAE=0.020085,rank=8\n",
      "[SoftImpute] Iter 140: observed MAE=0.004252 validation MAE=0.020060,rank=8\n",
      "[SoftImpute] Iter 141: observed MAE=0.004247 validation MAE=0.020036,rank=8\n",
      "[SoftImpute] Iter 142: observed MAE=0.004243 validation MAE=0.020011,rank=8\n",
      "[SoftImpute] Iter 143: observed MAE=0.004238 validation MAE=0.019987,rank=8\n",
      "[SoftImpute] Iter 144: observed MAE=0.004234 validation MAE=0.019963,rank=8\n",
      "[SoftImpute] Iter 145: observed MAE=0.004230 validation MAE=0.019939,rank=8\n",
      "[SoftImpute] Iter 146: observed MAE=0.004226 validation MAE=0.019915,rank=8\n",
      "[SoftImpute] Iter 147: observed MAE=0.004221 validation MAE=0.019892,rank=8\n",
      "[SoftImpute] Iter 148: observed MAE=0.004217 validation MAE=0.019869,rank=8\n",
      "[SoftImpute] Iter 149: observed MAE=0.004213 validation MAE=0.019845,rank=8\n",
      "[SoftImpute] Iter 150: observed MAE=0.004209 validation MAE=0.019822,rank=8\n",
      "[SoftImpute] Iter 151: observed MAE=0.004205 validation MAE=0.019799,rank=8\n",
      "[SoftImpute] Iter 152: observed MAE=0.004201 validation MAE=0.019777,rank=8\n",
      "[SoftImpute] Iter 153: observed MAE=0.004198 validation MAE=0.019754,rank=8\n",
      "[SoftImpute] Iter 154: observed MAE=0.004194 validation MAE=0.019732,rank=8\n",
      "[SoftImpute] Iter 155: observed MAE=0.004190 validation MAE=0.019710,rank=8\n",
      "[SoftImpute] Iter 156: observed MAE=0.004186 validation MAE=0.019688,rank=8\n",
      "[SoftImpute] Iter 157: observed MAE=0.004183 validation MAE=0.019666,rank=8\n",
      "[SoftImpute] Iter 158: observed MAE=0.004179 validation MAE=0.019644,rank=8\n",
      "[SoftImpute] Iter 159: observed MAE=0.004176 validation MAE=0.019622,rank=8\n",
      "[SoftImpute] Iter 160: observed MAE=0.004172 validation MAE=0.019601,rank=8\n",
      "[SoftImpute] Iter 161: observed MAE=0.004169 validation MAE=0.019579,rank=8\n",
      "[SoftImpute] Iter 162: observed MAE=0.004165 validation MAE=0.019558,rank=8\n",
      "[SoftImpute] Iter 163: observed MAE=0.004162 validation MAE=0.019537,rank=8\n",
      "[SoftImpute] Iter 164: observed MAE=0.004159 validation MAE=0.019516,rank=8\n",
      "[SoftImpute] Iter 165: observed MAE=0.004155 validation MAE=0.019495,rank=8\n",
      "[SoftImpute] Iter 166: observed MAE=0.004152 validation MAE=0.019474,rank=8\n",
      "[SoftImpute] Iter 167: observed MAE=0.004149 validation MAE=0.019453,rank=8\n",
      "[SoftImpute] Iter 168: observed MAE=0.004146 validation MAE=0.019432,rank=8\n",
      "[SoftImpute] Iter 169: observed MAE=0.004143 validation MAE=0.019411,rank=8\n",
      "[SoftImpute] Iter 170: observed MAE=0.004140 validation MAE=0.019391,rank=8\n",
      "[SoftImpute] Iter 171: observed MAE=0.004137 validation MAE=0.019370,rank=8\n",
      "[SoftImpute] Iter 172: observed MAE=0.004134 validation MAE=0.019350,rank=8\n",
      "[SoftImpute] Iter 173: observed MAE=0.004131 validation MAE=0.019330,rank=8\n",
      "[SoftImpute] Iter 174: observed MAE=0.004128 validation MAE=0.019310,rank=8\n",
      "[SoftImpute] Iter 175: observed MAE=0.004125 validation MAE=0.019290,rank=8\n",
      "[SoftImpute] Iter 176: observed MAE=0.004122 validation MAE=0.019270,rank=8\n",
      "[SoftImpute] Iter 177: observed MAE=0.004119 validation MAE=0.019250,rank=8\n",
      "[SoftImpute] Iter 178: observed MAE=0.004116 validation MAE=0.019231,rank=8\n",
      "[SoftImpute] Iter 179: observed MAE=0.004114 validation MAE=0.019211,rank=8\n",
      "[SoftImpute] Iter 180: observed MAE=0.004111 validation MAE=0.019192,rank=8\n",
      "[SoftImpute] Iter 181: observed MAE=0.004108 validation MAE=0.019172,rank=8\n",
      "[SoftImpute] Iter 182: observed MAE=0.004106 validation MAE=0.019153,rank=8\n",
      "[SoftImpute] Iter 183: observed MAE=0.004103 validation MAE=0.019134,rank=8\n",
      "[SoftImpute] Iter 184: observed MAE=0.004100 validation MAE=0.019115,rank=8\n",
      "[SoftImpute] Iter 185: observed MAE=0.004098 validation MAE=0.019096,rank=8\n",
      "[SoftImpute] Iter 186: observed MAE=0.004095 validation MAE=0.019077,rank=8\n",
      "[SoftImpute] Iter 187: observed MAE=0.004093 validation MAE=0.019059,rank=8\n",
      "[SoftImpute] Iter 188: observed MAE=0.004090 validation MAE=0.019040,rank=8\n",
      "[SoftImpute] Iter 189: observed MAE=0.004088 validation MAE=0.019022,rank=8\n",
      "[SoftImpute] Iter 190: observed MAE=0.004085 validation MAE=0.019003,rank=8\n",
      "[SoftImpute] Iter 191: observed MAE=0.004083 validation MAE=0.018985,rank=8\n",
      "[SoftImpute] Iter 192: observed MAE=0.004080 validation MAE=0.018966,rank=8\n",
      "[SoftImpute] Iter 193: observed MAE=0.004078 validation MAE=0.018948,rank=8\n",
      "[SoftImpute] Iter 194: observed MAE=0.004076 validation MAE=0.018930,rank=8\n",
      "[SoftImpute] Iter 195: observed MAE=0.004073 validation MAE=0.018912,rank=8\n",
      "[SoftImpute] Iter 196: observed MAE=0.004071 validation MAE=0.018894,rank=8\n",
      "[SoftImpute] Iter 197: observed MAE=0.004069 validation MAE=0.018877,rank=8\n",
      "[SoftImpute] Iter 198: observed MAE=0.004066 validation MAE=0.018859,rank=8\n",
      "[SoftImpute] Iter 199: observed MAE=0.004064 validation MAE=0.018842,rank=8\n",
      "[SoftImpute] Iter 200: observed MAE=0.004062 validation MAE=0.018824,rank=8\n",
      "[SoftImpute] Iter 201: observed MAE=0.004060 validation MAE=0.018807,rank=8\n",
      "[SoftImpute] Iter 202: observed MAE=0.004057 validation MAE=0.018790,rank=8\n",
      "[SoftImpute] Iter 203: observed MAE=0.004055 validation MAE=0.018773,rank=8\n",
      "[SoftImpute] Iter 204: observed MAE=0.004053 validation MAE=0.018756,rank=8\n",
      "[SoftImpute] Iter 205: observed MAE=0.004051 validation MAE=0.018739,rank=8\n",
      "[SoftImpute] Iter 206: observed MAE=0.004049 validation MAE=0.018722,rank=8\n",
      "[SoftImpute] Iter 207: observed MAE=0.004046 validation MAE=0.018705,rank=8\n",
      "[SoftImpute] Iter 208: observed MAE=0.004044 validation MAE=0.018688,rank=8\n",
      "[SoftImpute] Iter 209: observed MAE=0.004042 validation MAE=0.018672,rank=8\n",
      "[SoftImpute] Iter 210: observed MAE=0.004040 validation MAE=0.018655,rank=8\n",
      "[SoftImpute] Iter 211: observed MAE=0.004038 validation MAE=0.018639,rank=8\n",
      "[SoftImpute] Iter 212: observed MAE=0.004036 validation MAE=0.018623,rank=8\n",
      "[SoftImpute] Iter 213: observed MAE=0.004034 validation MAE=0.018607,rank=8\n",
      "[SoftImpute] Iter 214: observed MAE=0.004032 validation MAE=0.018591,rank=8\n",
      "[SoftImpute] Iter 215: observed MAE=0.004030 validation MAE=0.018575,rank=8\n",
      "[SoftImpute] Iter 216: observed MAE=0.004028 validation MAE=0.018559,rank=8\n",
      "[SoftImpute] Iter 217: observed MAE=0.004026 validation MAE=0.018543,rank=8\n",
      "[SoftImpute] Iter 218: observed MAE=0.004024 validation MAE=0.018527,rank=8\n",
      "[SoftImpute] Iter 219: observed MAE=0.004022 validation MAE=0.018512,rank=8\n",
      "[SoftImpute] Iter 220: observed MAE=0.004021 validation MAE=0.018496,rank=8\n",
      "[SoftImpute] Iter 221: observed MAE=0.004019 validation MAE=0.018480,rank=8\n",
      "[SoftImpute] Iter 222: observed MAE=0.004017 validation MAE=0.018465,rank=8\n",
      "[SoftImpute] Iter 223: observed MAE=0.004015 validation MAE=0.018450,rank=8\n",
      "[SoftImpute] Iter 224: observed MAE=0.004013 validation MAE=0.018434,rank=8\n",
      "[SoftImpute] Iter 225: observed MAE=0.004011 validation MAE=0.018419,rank=8\n",
      "[SoftImpute] Iter 226: observed MAE=0.004009 validation MAE=0.018404,rank=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 227: observed MAE=0.004008 validation MAE=0.018389,rank=8\n",
      "[SoftImpute] Iter 228: observed MAE=0.004006 validation MAE=0.018373,rank=8\n",
      "[SoftImpute] Iter 229: observed MAE=0.004004 validation MAE=0.018358,rank=8\n",
      "[SoftImpute] Iter 230: observed MAE=0.004002 validation MAE=0.018343,rank=8\n",
      "[SoftImpute] Iter 231: observed MAE=0.004001 validation MAE=0.018328,rank=8\n",
      "[SoftImpute] Iter 232: observed MAE=0.003999 validation MAE=0.018313,rank=8\n",
      "[SoftImpute] Iter 233: observed MAE=0.003997 validation MAE=0.018298,rank=8\n",
      "[SoftImpute] Iter 234: observed MAE=0.003995 validation MAE=0.018284,rank=8\n",
      "[SoftImpute] Iter 235: observed MAE=0.003994 validation MAE=0.018269,rank=8\n",
      "[SoftImpute] Iter 236: observed MAE=0.003992 validation MAE=0.018254,rank=8\n",
      "[SoftImpute] Iter 237: observed MAE=0.003990 validation MAE=0.018239,rank=8\n",
      "[SoftImpute] Iter 238: observed MAE=0.003989 validation MAE=0.018225,rank=8\n",
      "[SoftImpute] Iter 239: observed MAE=0.003987 validation MAE=0.018210,rank=8\n",
      "[SoftImpute] Iter 240: observed MAE=0.003985 validation MAE=0.018196,rank=8\n",
      "[SoftImpute] Iter 241: observed MAE=0.003984 validation MAE=0.018182,rank=8\n",
      "[SoftImpute] Iter 242: observed MAE=0.003982 validation MAE=0.018167,rank=8\n",
      "[SoftImpute] Iter 243: observed MAE=0.003980 validation MAE=0.018153,rank=8\n",
      "[SoftImpute] Iter 244: observed MAE=0.003979 validation MAE=0.018139,rank=8\n",
      "[SoftImpute] Iter 245: observed MAE=0.003977 validation MAE=0.018125,rank=8\n",
      "[SoftImpute] Iter 246: observed MAE=0.003976 validation MAE=0.018111,rank=8\n",
      "[SoftImpute] Iter 247: observed MAE=0.003974 validation MAE=0.018097,rank=8\n",
      "[SoftImpute] Iter 248: observed MAE=0.003973 validation MAE=0.018083,rank=8\n",
      "[SoftImpute] Iter 249: observed MAE=0.003971 validation MAE=0.018069,rank=8\n",
      "[SoftImpute] Iter 250: observed MAE=0.003970 validation MAE=0.018056,rank=8\n",
      "[SoftImpute] Iter 251: observed MAE=0.003968 validation MAE=0.018042,rank=8\n",
      "[SoftImpute] Iter 252: observed MAE=0.003966 validation MAE=0.018029,rank=8\n",
      "[SoftImpute] Iter 253: observed MAE=0.003965 validation MAE=0.018015,rank=8\n",
      "[SoftImpute] Iter 254: observed MAE=0.003963 validation MAE=0.018002,rank=8\n",
      "[SoftImpute] Iter 255: observed MAE=0.003962 validation MAE=0.017988,rank=8\n",
      "[SoftImpute] Iter 256: observed MAE=0.003960 validation MAE=0.017975,rank=8\n",
      "[SoftImpute] Iter 257: observed MAE=0.003959 validation MAE=0.017962,rank=8\n",
      "[SoftImpute] Iter 258: observed MAE=0.003958 validation MAE=0.017949,rank=8\n",
      "[SoftImpute] Iter 259: observed MAE=0.003956 validation MAE=0.017935,rank=8\n",
      "[SoftImpute] Iter 260: observed MAE=0.003955 validation MAE=0.017922,rank=8\n",
      "[SoftImpute] Iter 261: observed MAE=0.003953 validation MAE=0.017909,rank=8\n",
      "[SoftImpute] Iter 262: observed MAE=0.003952 validation MAE=0.017896,rank=8\n",
      "[SoftImpute] Iter 263: observed MAE=0.003950 validation MAE=0.017883,rank=8\n",
      "[SoftImpute] Iter 264: observed MAE=0.003949 validation MAE=0.017870,rank=8\n",
      "[SoftImpute] Iter 265: observed MAE=0.003948 validation MAE=0.017857,rank=8\n",
      "[SoftImpute] Iter 266: observed MAE=0.003946 validation MAE=0.017844,rank=8\n",
      "[SoftImpute] Iter 267: observed MAE=0.003945 validation MAE=0.017831,rank=8\n",
      "[SoftImpute] Iter 268: observed MAE=0.003943 validation MAE=0.017819,rank=8\n",
      "[SoftImpute] Iter 269: observed MAE=0.003942 validation MAE=0.017806,rank=8\n",
      "[SoftImpute] Iter 270: observed MAE=0.003941 validation MAE=0.017793,rank=8\n",
      "[SoftImpute] Iter 271: observed MAE=0.003939 validation MAE=0.017781,rank=8\n",
      "[SoftImpute] Iter 272: observed MAE=0.003938 validation MAE=0.017768,rank=8\n",
      "[SoftImpute] Iter 273: observed MAE=0.003936 validation MAE=0.017756,rank=8\n",
      "[SoftImpute] Iter 274: observed MAE=0.003935 validation MAE=0.017743,rank=8\n",
      "[SoftImpute] Iter 275: observed MAE=0.003934 validation MAE=0.017731,rank=8\n",
      "[SoftImpute] Iter 276: observed MAE=0.003932 validation MAE=0.017719,rank=8\n",
      "[SoftImpute] Iter 277: observed MAE=0.003931 validation MAE=0.017707,rank=8\n",
      "[SoftImpute] Iter 278: observed MAE=0.003930 validation MAE=0.017694,rank=8\n",
      "[SoftImpute] Iter 279: observed MAE=0.003928 validation MAE=0.017682,rank=8\n",
      "[SoftImpute] Iter 280: observed MAE=0.003927 validation MAE=0.017670,rank=8\n",
      "[SoftImpute] Iter 281: observed MAE=0.003926 validation MAE=0.017658,rank=8\n",
      "[SoftImpute] Iter 282: observed MAE=0.003924 validation MAE=0.017646,rank=8\n",
      "[SoftImpute] Iter 283: observed MAE=0.003923 validation MAE=0.017634,rank=8\n",
      "[SoftImpute] Iter 284: observed MAE=0.003922 validation MAE=0.017623,rank=8\n",
      "[SoftImpute] Iter 285: observed MAE=0.003921 validation MAE=0.017611,rank=8\n",
      "[SoftImpute] Iter 286: observed MAE=0.003919 validation MAE=0.017599,rank=8\n",
      "[SoftImpute] Iter 287: observed MAE=0.003918 validation MAE=0.017587,rank=8\n",
      "[SoftImpute] Iter 288: observed MAE=0.003917 validation MAE=0.017576,rank=8\n",
      "[SoftImpute] Iter 289: observed MAE=0.003916 validation MAE=0.017564,rank=8\n",
      "[SoftImpute] Iter 290: observed MAE=0.003914 validation MAE=0.017553,rank=8\n",
      "[SoftImpute] Iter 291: observed MAE=0.003913 validation MAE=0.017541,rank=8\n",
      "[SoftImpute] Iter 292: observed MAE=0.003912 validation MAE=0.017530,rank=8\n",
      "[SoftImpute] Iter 293: observed MAE=0.003911 validation MAE=0.017518,rank=8\n",
      "[SoftImpute] Iter 294: observed MAE=0.003909 validation MAE=0.017507,rank=8\n",
      "[SoftImpute] Iter 295: observed MAE=0.003908 validation MAE=0.017495,rank=8\n",
      "[SoftImpute] Iter 296: observed MAE=0.003907 validation MAE=0.017484,rank=8\n",
      "[SoftImpute] Iter 297: observed MAE=0.003906 validation MAE=0.017473,rank=8\n",
      "[SoftImpute] Iter 298: observed MAE=0.003904 validation MAE=0.017462,rank=8\n",
      "[SoftImpute] Iter 299: observed MAE=0.003903 validation MAE=0.017451,rank=8\n",
      "[SoftImpute] Iter 300: observed MAE=0.003902 validation MAE=0.017440,rank=8\n",
      "[SoftImpute] Iter 301: observed MAE=0.003901 validation MAE=0.017429,rank=8\n",
      "[SoftImpute] Iter 302: observed MAE=0.003900 validation MAE=0.017419,rank=8\n",
      "[SoftImpute] Iter 303: observed MAE=0.003898 validation MAE=0.017408,rank=8\n",
      "[SoftImpute] Iter 304: observed MAE=0.003897 validation MAE=0.017397,rank=8\n",
      "[SoftImpute] Iter 305: observed MAE=0.003896 validation MAE=0.017386,rank=8\n",
      "[SoftImpute] Iter 306: observed MAE=0.003895 validation MAE=0.017376,rank=8\n",
      "[SoftImpute] Iter 307: observed MAE=0.003894 validation MAE=0.017365,rank=8\n",
      "[SoftImpute] Iter 308: observed MAE=0.003893 validation MAE=0.017355,rank=8\n",
      "[SoftImpute] Iter 309: observed MAE=0.003891 validation MAE=0.017345,rank=8\n",
      "[SoftImpute] Iter 310: observed MAE=0.003890 validation MAE=0.017334,rank=8\n",
      "[SoftImpute] Iter 311: observed MAE=0.003889 validation MAE=0.017324,rank=8\n",
      "[SoftImpute] Iter 312: observed MAE=0.003888 validation MAE=0.017314,rank=8\n",
      "[SoftImpute] Iter 313: observed MAE=0.003887 validation MAE=0.017304,rank=8\n",
      "[SoftImpute] Iter 314: observed MAE=0.003886 validation MAE=0.017293,rank=8\n",
      "[SoftImpute] Iter 315: observed MAE=0.003884 validation MAE=0.017283,rank=8\n",
      "[SoftImpute] Iter 316: observed MAE=0.003883 validation MAE=0.017273,rank=8\n",
      "[SoftImpute] Iter 317: observed MAE=0.003882 validation MAE=0.017263,rank=8\n",
      "[SoftImpute] Iter 318: observed MAE=0.003881 validation MAE=0.017253,rank=8\n",
      "[SoftImpute] Iter 319: observed MAE=0.003880 validation MAE=0.017243,rank=8\n",
      "[SoftImpute] Iter 320: observed MAE=0.003879 validation MAE=0.017234,rank=8\n",
      "[SoftImpute] Iter 321: observed MAE=0.003878 validation MAE=0.017224,rank=8\n",
      "[SoftImpute] Iter 322: observed MAE=0.003877 validation MAE=0.017214,rank=8\n",
      "[SoftImpute] Iter 323: observed MAE=0.003875 validation MAE=0.017204,rank=8\n",
      "[SoftImpute] Iter 324: observed MAE=0.003874 validation MAE=0.017194,rank=8\n",
      "[SoftImpute] Iter 325: observed MAE=0.003873 validation MAE=0.017185,rank=8\n",
      "[SoftImpute] Iter 326: observed MAE=0.003872 validation MAE=0.017175,rank=8\n",
      "[SoftImpute] Iter 327: observed MAE=0.003871 validation MAE=0.017166,rank=8\n",
      "[SoftImpute] Iter 328: observed MAE=0.003870 validation MAE=0.017156,rank=8\n",
      "[SoftImpute] Iter 329: observed MAE=0.003869 validation MAE=0.017147,rank=8\n",
      "[SoftImpute] Iter 330: observed MAE=0.003868 validation MAE=0.017138,rank=8\n",
      "[SoftImpute] Iter 331: observed MAE=0.003867 validation MAE=0.017130,rank=8\n",
      "[SoftImpute] Iter 332: observed MAE=0.003865 validation MAE=0.017121,rank=8\n",
      "[SoftImpute] Iter 333: observed MAE=0.003864 validation MAE=0.017112,rank=8\n",
      "[SoftImpute] Iter 334: observed MAE=0.003863 validation MAE=0.017104,rank=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 335: observed MAE=0.003862 validation MAE=0.017095,rank=8\n",
      "[SoftImpute] Iter 336: observed MAE=0.003861 validation MAE=0.017086,rank=8\n",
      "[SoftImpute] Iter 337: observed MAE=0.003860 validation MAE=0.017078,rank=8\n",
      "[SoftImpute] Iter 338: observed MAE=0.003859 validation MAE=0.017070,rank=8\n",
      "[SoftImpute] Iter 339: observed MAE=0.003858 validation MAE=0.017061,rank=8\n",
      "[SoftImpute] Iter 340: observed MAE=0.003857 validation MAE=0.017053,rank=8\n",
      "[SoftImpute] Iter 341: observed MAE=0.003856 validation MAE=0.017045,rank=8\n",
      "[SoftImpute] Iter 342: observed MAE=0.003855 validation MAE=0.017036,rank=8\n",
      "[SoftImpute] Iter 343: observed MAE=0.003854 validation MAE=0.017028,rank=8\n",
      "[SoftImpute] Iter 344: observed MAE=0.003853 validation MAE=0.017020,rank=8\n",
      "[SoftImpute] Iter 345: observed MAE=0.003852 validation MAE=0.017011,rank=8\n",
      "[SoftImpute] Iter 346: observed MAE=0.003851 validation MAE=0.017003,rank=8\n",
      "[SoftImpute] Iter 347: observed MAE=0.003850 validation MAE=0.016995,rank=8\n",
      "[SoftImpute] Iter 348: observed MAE=0.003849 validation MAE=0.016987,rank=8\n",
      "[SoftImpute] Iter 349: observed MAE=0.003848 validation MAE=0.016979,rank=8\n",
      "[SoftImpute] Iter 350: observed MAE=0.003847 validation MAE=0.016971,rank=8\n",
      "[SoftImpute] Stopped after iteration 350 for lambda=0.039366\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 10.876520872116089\n",
      "After the matrix factor stage, training error is 0.00385, validation error is 0.01697\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.34448, val loss: 0.34765\n",
      "Main effects training epoch: 2, train loss: 0.27354, val loss: 0.27853\n",
      "Main effects training epoch: 3, train loss: 0.20823, val loss: 0.21145\n",
      "Main effects training epoch: 4, train loss: 0.16135, val loss: 0.16514\n",
      "Main effects training epoch: 5, train loss: 0.14048, val loss: 0.14025\n",
      "Main effects training epoch: 6, train loss: 0.13280, val loss: 0.13280\n",
      "Main effects training epoch: 7, train loss: 0.13041, val loss: 0.12922\n",
      "Main effects training epoch: 8, train loss: 0.12977, val loss: 0.12861\n",
      "Main effects training epoch: 9, train loss: 0.12914, val loss: 0.12821\n",
      "Main effects training epoch: 10, train loss: 0.12852, val loss: 0.12844\n",
      "Main effects training epoch: 11, train loss: 0.12742, val loss: 0.12590\n",
      "Main effects training epoch: 12, train loss: 0.12612, val loss: 0.12564\n",
      "Main effects training epoch: 13, train loss: 0.12339, val loss: 0.12252\n",
      "Main effects training epoch: 14, train loss: 0.11835, val loss: 0.11883\n",
      "Main effects training epoch: 15, train loss: 0.11578, val loss: 0.11651\n",
      "Main effects training epoch: 16, train loss: 0.11289, val loss: 0.11344\n",
      "Main effects training epoch: 17, train loss: 0.11081, val loss: 0.11186\n",
      "Main effects training epoch: 18, train loss: 0.11088, val loss: 0.11312\n",
      "Main effects training epoch: 19, train loss: 0.11027, val loss: 0.11199\n",
      "Main effects training epoch: 20, train loss: 0.11090, val loss: 0.11238\n",
      "Main effects training epoch: 21, train loss: 0.10922, val loss: 0.11096\n",
      "Main effects training epoch: 22, train loss: 0.10900, val loss: 0.11106\n",
      "Main effects training epoch: 23, train loss: 0.10645, val loss: 0.10812\n",
      "Main effects training epoch: 24, train loss: 0.10707, val loss: 0.10858\n",
      "Main effects training epoch: 25, train loss: 0.10776, val loss: 0.10768\n",
      "Main effects training epoch: 26, train loss: 0.10556, val loss: 0.10715\n",
      "Main effects training epoch: 27, train loss: 0.10595, val loss: 0.10680\n",
      "Main effects training epoch: 28, train loss: 0.10509, val loss: 0.10619\n",
      "Main effects training epoch: 29, train loss: 0.10460, val loss: 0.10568\n",
      "Main effects training epoch: 30, train loss: 0.10512, val loss: 0.10585\n",
      "Main effects training epoch: 31, train loss: 0.10437, val loss: 0.10598\n",
      "Main effects training epoch: 32, train loss: 0.10431, val loss: 0.10577\n",
      "Main effects training epoch: 33, train loss: 0.10423, val loss: 0.10545\n",
      "Main effects training epoch: 34, train loss: 0.10413, val loss: 0.10561\n",
      "Main effects training epoch: 35, train loss: 0.10424, val loss: 0.10579\n",
      "Main effects training epoch: 36, train loss: 0.10444, val loss: 0.10578\n",
      "Main effects training epoch: 37, train loss: 0.10400, val loss: 0.10575\n",
      "Main effects training epoch: 38, train loss: 0.10438, val loss: 0.10591\n",
      "Main effects training epoch: 39, train loss: 0.10416, val loss: 0.10566\n",
      "Main effects training epoch: 40, train loss: 0.10393, val loss: 0.10524\n",
      "Main effects training epoch: 41, train loss: 0.10418, val loss: 0.10579\n",
      "Main effects training epoch: 42, train loss: 0.10392, val loss: 0.10565\n",
      "Main effects training epoch: 43, train loss: 0.10430, val loss: 0.10612\n",
      "Main effects training epoch: 44, train loss: 0.10409, val loss: 0.10553\n",
      "Main effects training epoch: 45, train loss: 0.10403, val loss: 0.10539\n",
      "Main effects training epoch: 46, train loss: 0.10400, val loss: 0.10547\n",
      "Main effects training epoch: 47, train loss: 0.10472, val loss: 0.10625\n",
      "Main effects training epoch: 48, train loss: 0.10410, val loss: 0.10597\n",
      "Main effects training epoch: 49, train loss: 0.10415, val loss: 0.10512\n",
      "Main effects training epoch: 50, train loss: 0.10408, val loss: 0.10560\n",
      "Main effects training epoch: 51, train loss: 0.10392, val loss: 0.10554\n",
      "Main effects training epoch: 52, train loss: 0.10395, val loss: 0.10519\n",
      "Main effects training epoch: 53, train loss: 0.10385, val loss: 0.10520\n",
      "Main effects training epoch: 54, train loss: 0.10398, val loss: 0.10563\n",
      "Main effects training epoch: 55, train loss: 0.10416, val loss: 0.10523\n",
      "Main effects training epoch: 56, train loss: 0.10404, val loss: 0.10586\n",
      "Main effects training epoch: 57, train loss: 0.10399, val loss: 0.10571\n",
      "Main effects training epoch: 58, train loss: 0.10443, val loss: 0.10631\n",
      "Main effects training epoch: 59, train loss: 0.10393, val loss: 0.10549\n",
      "Main effects training epoch: 60, train loss: 0.10386, val loss: 0.10534\n",
      "Main effects training epoch: 61, train loss: 0.10389, val loss: 0.10538\n",
      "Main effects training epoch: 62, train loss: 0.10417, val loss: 0.10552\n",
      "Main effects training epoch: 63, train loss: 0.10396, val loss: 0.10570\n",
      "Main effects training epoch: 64, train loss: 0.10412, val loss: 0.10546\n",
      "Main effects training epoch: 65, train loss: 0.10403, val loss: 0.10549\n",
      "Main effects training epoch: 66, train loss: 0.10398, val loss: 0.10514\n",
      "Main effects training epoch: 67, train loss: 0.10385, val loss: 0.10514\n",
      "Main effects training epoch: 68, train loss: 0.10396, val loss: 0.10561\n",
      "Main effects training epoch: 69, train loss: 0.10397, val loss: 0.10523\n",
      "Main effects training epoch: 70, train loss: 0.10420, val loss: 0.10542\n",
      "Main effects training epoch: 71, train loss: 0.10473, val loss: 0.10671\n",
      "Main effects training epoch: 72, train loss: 0.10418, val loss: 0.10552\n",
      "Main effects training epoch: 73, train loss: 0.10427, val loss: 0.10590\n",
      "Main effects training epoch: 74, train loss: 0.10409, val loss: 0.10540\n",
      "Main effects training epoch: 75, train loss: 0.10401, val loss: 0.10542\n",
      "Main effects training epoch: 76, train loss: 0.10384, val loss: 0.10560\n",
      "Main effects training epoch: 77, train loss: 0.10404, val loss: 0.10542\n",
      "Main effects training epoch: 78, train loss: 0.10394, val loss: 0.10550\n",
      "Main effects training epoch: 79, train loss: 0.10410, val loss: 0.10553\n",
      "Main effects training epoch: 80, train loss: 0.10425, val loss: 0.10542\n",
      "Main effects training epoch: 81, train loss: 0.10387, val loss: 0.10530\n",
      "Main effects training epoch: 82, train loss: 0.10409, val loss: 0.10607\n",
      "Main effects training epoch: 83, train loss: 0.10386, val loss: 0.10559\n",
      "Main effects training epoch: 84, train loss: 0.10405, val loss: 0.10549\n",
      "Main effects training epoch: 85, train loss: 0.10483, val loss: 0.10617\n",
      "Main effects training epoch: 86, train loss: 0.10383, val loss: 0.10551\n",
      "Main effects training epoch: 87, train loss: 0.10415, val loss: 0.10524\n",
      "Main effects training epoch: 88, train loss: 0.10389, val loss: 0.10586\n",
      "Main effects training epoch: 89, train loss: 0.10383, val loss: 0.10548\n",
      "Main effects training epoch: 90, train loss: 0.10400, val loss: 0.10512\n",
      "Main effects training epoch: 91, train loss: 0.10387, val loss: 0.10538\n",
      "Main effects training epoch: 92, train loss: 0.10420, val loss: 0.10635\n",
      "Main effects training epoch: 93, train loss: 0.10396, val loss: 0.10529\n",
      "Main effects training epoch: 94, train loss: 0.10417, val loss: 0.10594\n",
      "Main effects training epoch: 95, train loss: 0.10387, val loss: 0.10562\n",
      "Main effects training epoch: 96, train loss: 0.10378, val loss: 0.10533\n",
      "Main effects training epoch: 97, train loss: 0.10386, val loss: 0.10536\n",
      "Main effects training epoch: 98, train loss: 0.10425, val loss: 0.10630\n",
      "Main effects training epoch: 99, train loss: 0.10426, val loss: 0.10552\n",
      "Main effects training epoch: 100, train loss: 0.10419, val loss: 0.10632\n",
      "Main effects training epoch: 101, train loss: 0.10395, val loss: 0.10553\n",
      "Main effects training epoch: 102, train loss: 0.10397, val loss: 0.10584\n",
      "Main effects training epoch: 103, train loss: 0.10408, val loss: 0.10574\n",
      "Main effects training epoch: 104, train loss: 0.10395, val loss: 0.10576\n",
      "Main effects training epoch: 105, train loss: 0.10380, val loss: 0.10555\n",
      "Main effects training epoch: 106, train loss: 0.10382, val loss: 0.10590\n",
      "Main effects training epoch: 107, train loss: 0.10417, val loss: 0.10600\n",
      "Main effects training epoch: 108, train loss: 0.10410, val loss: 0.10567\n",
      "Main effects training epoch: 109, train loss: 0.10373, val loss: 0.10542\n",
      "Main effects training epoch: 110, train loss: 0.10377, val loss: 0.10525\n",
      "Main effects training epoch: 111, train loss: 0.10405, val loss: 0.10577\n",
      "Main effects training epoch: 112, train loss: 0.10409, val loss: 0.10590\n",
      "Main effects training epoch: 113, train loss: 0.10474, val loss: 0.10615\n",
      "Main effects training epoch: 114, train loss: 0.10406, val loss: 0.10594\n",
      "Main effects training epoch: 115, train loss: 0.10415, val loss: 0.10553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 116, train loss: 0.10437, val loss: 0.10680\n",
      "Main effects training epoch: 117, train loss: 0.10399, val loss: 0.10532\n",
      "Main effects training epoch: 118, train loss: 0.10393, val loss: 0.10583\n",
      "Main effects training epoch: 119, train loss: 0.10410, val loss: 0.10601\n",
      "Main effects training epoch: 120, train loss: 0.10375, val loss: 0.10578\n",
      "Main effects training epoch: 121, train loss: 0.10382, val loss: 0.10529\n",
      "Main effects training epoch: 122, train loss: 0.10391, val loss: 0.10552\n",
      "Main effects training epoch: 123, train loss: 0.10397, val loss: 0.10615\n",
      "Main effects training epoch: 124, train loss: 0.10422, val loss: 0.10572\n",
      "Main effects training epoch: 125, train loss: 0.10414, val loss: 0.10601\n",
      "Main effects training epoch: 126, train loss: 0.10402, val loss: 0.10605\n",
      "Main effects training epoch: 127, train loss: 0.10402, val loss: 0.10531\n",
      "Main effects training epoch: 128, train loss: 0.10395, val loss: 0.10615\n",
      "Main effects training epoch: 129, train loss: 0.10410, val loss: 0.10643\n",
      "Main effects training epoch: 130, train loss: 0.10401, val loss: 0.10526\n",
      "Main effects training epoch: 131, train loss: 0.10380, val loss: 0.10577\n",
      "Main effects training epoch: 132, train loss: 0.10404, val loss: 0.10554\n",
      "Main effects training epoch: 133, train loss: 0.10425, val loss: 0.10648\n",
      "Main effects training epoch: 134, train loss: 0.10454, val loss: 0.10616\n",
      "Main effects training epoch: 135, train loss: 0.10424, val loss: 0.10653\n",
      "Main effects training epoch: 136, train loss: 0.10426, val loss: 0.10565\n",
      "Main effects training epoch: 137, train loss: 0.10444, val loss: 0.10639\n",
      "Main effects training epoch: 138, train loss: 0.10433, val loss: 0.10622\n",
      "Main effects training epoch: 139, train loss: 0.10384, val loss: 0.10565\n",
      "Main effects training epoch: 140, train loss: 0.10392, val loss: 0.10539\n",
      "Main effects training epoch: 141, train loss: 0.10420, val loss: 0.10593\n",
      "Main effects training epoch: 142, train loss: 0.10404, val loss: 0.10617\n",
      "Main effects training epoch: 143, train loss: 0.10420, val loss: 0.10567\n",
      "Main effects training epoch: 144, train loss: 0.10383, val loss: 0.10601\n",
      "Main effects training epoch: 145, train loss: 0.10370, val loss: 0.10530\n",
      "Main effects training epoch: 146, train loss: 0.10381, val loss: 0.10596\n",
      "Main effects training epoch: 147, train loss: 0.10380, val loss: 0.10575\n",
      "Main effects training epoch: 148, train loss: 0.10377, val loss: 0.10571\n",
      "Main effects training epoch: 149, train loss: 0.10383, val loss: 0.10566\n",
      "Main effects training epoch: 150, train loss: 0.10393, val loss: 0.10604\n",
      "Early stop at epoch 150, with validation loss: 0.10604\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.10405, val loss: 0.10498\n",
      "Main effects tuning epoch: 2, train loss: 0.10423, val loss: 0.10552\n",
      "Main effects tuning epoch: 3, train loss: 0.10411, val loss: 0.10505\n",
      "Main effects tuning epoch: 4, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 5, train loss: 0.10382, val loss: 0.10498\n",
      "Main effects tuning epoch: 6, train loss: 0.10425, val loss: 0.10582\n",
      "Main effects tuning epoch: 7, train loss: 0.10408, val loss: 0.10530\n",
      "Main effects tuning epoch: 8, train loss: 0.10397, val loss: 0.10527\n",
      "Main effects tuning epoch: 9, train loss: 0.10401, val loss: 0.10507\n",
      "Main effects tuning epoch: 10, train loss: 0.10407, val loss: 0.10539\n",
      "Main effects tuning epoch: 11, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 12, train loss: 0.10393, val loss: 0.10556\n",
      "Main effects tuning epoch: 13, train loss: 0.10415, val loss: 0.10510\n",
      "Main effects tuning epoch: 14, train loss: 0.10397, val loss: 0.10570\n",
      "Main effects tuning epoch: 15, train loss: 0.10386, val loss: 0.10516\n",
      "Main effects tuning epoch: 16, train loss: 0.10395, val loss: 0.10562\n",
      "Main effects tuning epoch: 17, train loss: 0.10395, val loss: 0.10539\n",
      "Main effects tuning epoch: 18, train loss: 0.10422, val loss: 0.10568\n",
      "Main effects tuning epoch: 19, train loss: 0.10433, val loss: 0.10517\n",
      "Main effects tuning epoch: 20, train loss: 0.10384, val loss: 0.10568\n",
      "Main effects tuning epoch: 21, train loss: 0.10400, val loss: 0.10541\n",
      "Main effects tuning epoch: 22, train loss: 0.10393, val loss: 0.10526\n",
      "Main effects tuning epoch: 23, train loss: 0.10434, val loss: 0.10582\n",
      "Main effects tuning epoch: 24, train loss: 0.10406, val loss: 0.10540\n",
      "Main effects tuning epoch: 25, train loss: 0.10410, val loss: 0.10570\n",
      "Main effects tuning epoch: 26, train loss: 0.10405, val loss: 0.10569\n",
      "Main effects tuning epoch: 27, train loss: 0.10406, val loss: 0.10556\n",
      "Main effects tuning epoch: 28, train loss: 0.10394, val loss: 0.10542\n",
      "Main effects tuning epoch: 29, train loss: 0.10383, val loss: 0.10542\n",
      "Main effects tuning epoch: 30, train loss: 0.10385, val loss: 0.10548\n",
      "Main effects tuning epoch: 31, train loss: 0.10395, val loss: 0.10516\n",
      "Main effects tuning epoch: 32, train loss: 0.10391, val loss: 0.10584\n",
      "Main effects tuning epoch: 33, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 34, train loss: 0.10391, val loss: 0.10561\n",
      "Main effects tuning epoch: 35, train loss: 0.10424, val loss: 0.10531\n",
      "Main effects tuning epoch: 36, train loss: 0.10395, val loss: 0.10554\n",
      "Main effects tuning epoch: 37, train loss: 0.10400, val loss: 0.10600\n",
      "Main effects tuning epoch: 38, train loss: 0.10408, val loss: 0.10530\n",
      "Main effects tuning epoch: 39, train loss: 0.10394, val loss: 0.10565\n",
      "Main effects tuning epoch: 40, train loss: 0.10385, val loss: 0.10569\n",
      "Main effects tuning epoch: 41, train loss: 0.10390, val loss: 0.10539\n",
      "Main effects tuning epoch: 42, train loss: 0.10400, val loss: 0.10587\n",
      "Main effects tuning epoch: 43, train loss: 0.10386, val loss: 0.10547\n",
      "Main effects tuning epoch: 44, train loss: 0.10403, val loss: 0.10597\n",
      "Main effects tuning epoch: 45, train loss: 0.10384, val loss: 0.10541\n",
      "Main effects tuning epoch: 46, train loss: 0.10398, val loss: 0.10582\n",
      "Main effects tuning epoch: 47, train loss: 0.10375, val loss: 0.10527\n",
      "Main effects tuning epoch: 48, train loss: 0.10380, val loss: 0.10549\n",
      "Main effects tuning epoch: 49, train loss: 0.10396, val loss: 0.10566\n",
      "Main effects tuning epoch: 50, train loss: 0.10407, val loss: 0.10549\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.09376, val loss: 0.09232\n",
      "Interaction training epoch: 2, train loss: 0.20140, val loss: 0.20111\n",
      "Interaction training epoch: 3, train loss: 0.06617, val loss: 0.06860\n",
      "Interaction training epoch: 4, train loss: 0.05173, val loss: 0.05197\n",
      "Interaction training epoch: 5, train loss: 0.06497, val loss: 0.06595\n",
      "Interaction training epoch: 6, train loss: 0.04386, val loss: 0.04449\n",
      "Interaction training epoch: 7, train loss: 0.04606, val loss: 0.04529\n",
      "Interaction training epoch: 8, train loss: 0.04492, val loss: 0.04489\n",
      "Interaction training epoch: 9, train loss: 0.04997, val loss: 0.04868\n",
      "Interaction training epoch: 10, train loss: 0.04002, val loss: 0.04133\n",
      "Interaction training epoch: 11, train loss: 0.04269, val loss: 0.04306\n",
      "Interaction training epoch: 12, train loss: 0.04100, val loss: 0.03997\n",
      "Interaction training epoch: 13, train loss: 0.04366, val loss: 0.04360\n",
      "Interaction training epoch: 14, train loss: 0.04928, val loss: 0.05084\n",
      "Interaction training epoch: 15, train loss: 0.04532, val loss: 0.04488\n",
      "Interaction training epoch: 16, train loss: 0.04530, val loss: 0.04524\n",
      "Interaction training epoch: 17, train loss: 0.03784, val loss: 0.03763\n",
      "Interaction training epoch: 18, train loss: 0.04939, val loss: 0.05023\n",
      "Interaction training epoch: 19, train loss: 0.03608, val loss: 0.03597\n",
      "Interaction training epoch: 20, train loss: 0.04344, val loss: 0.04367\n",
      "Interaction training epoch: 21, train loss: 0.04002, val loss: 0.03932\n",
      "Interaction training epoch: 22, train loss: 0.03977, val loss: 0.03977\n",
      "Interaction training epoch: 23, train loss: 0.03777, val loss: 0.03709\n",
      "Interaction training epoch: 24, train loss: 0.03927, val loss: 0.03924\n",
      "Interaction training epoch: 25, train loss: 0.04164, val loss: 0.04112\n",
      "Interaction training epoch: 26, train loss: 0.04429, val loss: 0.04389\n",
      "Interaction training epoch: 27, train loss: 0.04417, val loss: 0.04416\n",
      "Interaction training epoch: 28, train loss: 0.04241, val loss: 0.04112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 29, train loss: 0.03548, val loss: 0.03672\n",
      "Interaction training epoch: 30, train loss: 0.04420, val loss: 0.04353\n",
      "Interaction training epoch: 31, train loss: 0.04020, val loss: 0.04004\n",
      "Interaction training epoch: 32, train loss: 0.03422, val loss: 0.03451\n",
      "Interaction training epoch: 33, train loss: 0.03378, val loss: 0.03407\n",
      "Interaction training epoch: 34, train loss: 0.03925, val loss: 0.03880\n",
      "Interaction training epoch: 35, train loss: 0.03752, val loss: 0.03636\n",
      "Interaction training epoch: 36, train loss: 0.03582, val loss: 0.03564\n",
      "Interaction training epoch: 37, train loss: 0.03069, val loss: 0.03087\n",
      "Interaction training epoch: 38, train loss: 0.03729, val loss: 0.03652\n",
      "Interaction training epoch: 39, train loss: 0.03772, val loss: 0.03772\n",
      "Interaction training epoch: 40, train loss: 0.04332, val loss: 0.04201\n",
      "Interaction training epoch: 41, train loss: 0.03409, val loss: 0.03435\n",
      "Interaction training epoch: 42, train loss: 0.03501, val loss: 0.03419\n",
      "Interaction training epoch: 43, train loss: 0.03630, val loss: 0.03573\n",
      "Interaction training epoch: 44, train loss: 0.03447, val loss: 0.03420\n",
      "Interaction training epoch: 45, train loss: 0.03622, val loss: 0.03541\n",
      "Interaction training epoch: 46, train loss: 0.02991, val loss: 0.03002\n",
      "Interaction training epoch: 47, train loss: 0.02880, val loss: 0.02856\n",
      "Interaction training epoch: 48, train loss: 0.03548, val loss: 0.03469\n",
      "Interaction training epoch: 49, train loss: 0.03399, val loss: 0.03256\n",
      "Interaction training epoch: 50, train loss: 0.03225, val loss: 0.03266\n",
      "Interaction training epoch: 51, train loss: 0.03286, val loss: 0.03176\n",
      "Interaction training epoch: 52, train loss: 0.03084, val loss: 0.03012\n",
      "Interaction training epoch: 53, train loss: 0.03059, val loss: 0.02998\n",
      "Interaction training epoch: 54, train loss: 0.02966, val loss: 0.02932\n",
      "Interaction training epoch: 55, train loss: 0.04122, val loss: 0.04119\n",
      "Interaction training epoch: 56, train loss: 0.03164, val loss: 0.03059\n",
      "Interaction training epoch: 57, train loss: 0.03483, val loss: 0.03469\n",
      "Interaction training epoch: 58, train loss: 0.03320, val loss: 0.03294\n",
      "Interaction training epoch: 59, train loss: 0.02999, val loss: 0.02930\n",
      "Interaction training epoch: 60, train loss: 0.03540, val loss: 0.03496\n",
      "Interaction training epoch: 61, train loss: 0.03142, val loss: 0.02962\n",
      "Interaction training epoch: 62, train loss: 0.03146, val loss: 0.03164\n",
      "Interaction training epoch: 63, train loss: 0.03481, val loss: 0.03421\n",
      "Interaction training epoch: 64, train loss: 0.02946, val loss: 0.02881\n",
      "Interaction training epoch: 65, train loss: 0.03539, val loss: 0.03511\n",
      "Interaction training epoch: 66, train loss: 0.03797, val loss: 0.03805\n",
      "Interaction training epoch: 67, train loss: 0.03669, val loss: 0.03627\n",
      "Interaction training epoch: 68, train loss: 0.04844, val loss: 0.04881\n",
      "Interaction training epoch: 69, train loss: 0.02947, val loss: 0.02941\n",
      "Interaction training epoch: 70, train loss: 0.03805, val loss: 0.03672\n",
      "Interaction training epoch: 71, train loss: 0.02990, val loss: 0.02970\n",
      "Interaction training epoch: 72, train loss: 0.03926, val loss: 0.03952\n",
      "Interaction training epoch: 73, train loss: 0.03713, val loss: 0.03732\n",
      "Interaction training epoch: 74, train loss: 0.04469, val loss: 0.04392\n",
      "Interaction training epoch: 75, train loss: 0.03518, val loss: 0.03491\n",
      "Interaction training epoch: 76, train loss: 0.03954, val loss: 0.03893\n",
      "Interaction training epoch: 77, train loss: 0.04294, val loss: 0.04248\n",
      "Interaction training epoch: 78, train loss: 0.03122, val loss: 0.03170\n",
      "Interaction training epoch: 79, train loss: 0.03577, val loss: 0.03538\n",
      "Interaction training epoch: 80, train loss: 0.04727, val loss: 0.04664\n",
      "Interaction training epoch: 81, train loss: 0.03478, val loss: 0.03412\n",
      "Interaction training epoch: 82, train loss: 0.03736, val loss: 0.03700\n",
      "Interaction training epoch: 83, train loss: 0.03716, val loss: 0.03665\n",
      "Interaction training epoch: 84, train loss: 0.03675, val loss: 0.03575\n",
      "Interaction training epoch: 85, train loss: 0.03815, val loss: 0.03817\n",
      "Interaction training epoch: 86, train loss: 0.03960, val loss: 0.03903\n",
      "Interaction training epoch: 87, train loss: 0.03498, val loss: 0.03402\n",
      "Interaction training epoch: 88, train loss: 0.03630, val loss: 0.03583\n",
      "Interaction training epoch: 89, train loss: 0.03454, val loss: 0.03517\n",
      "Interaction training epoch: 90, train loss: 0.07231, val loss: 0.07223\n",
      "Interaction training epoch: 91, train loss: 0.04800, val loss: 0.04802\n",
      "Interaction training epoch: 92, train loss: 0.05224, val loss: 0.05162\n",
      "Interaction training epoch: 93, train loss: 0.02794, val loss: 0.02804\n",
      "Interaction training epoch: 94, train loss: 0.03585, val loss: 0.03546\n",
      "Interaction training epoch: 95, train loss: 0.04149, val loss: 0.04158\n",
      "Interaction training epoch: 96, train loss: 0.04943, val loss: 0.04909\n",
      "Interaction training epoch: 97, train loss: 0.05722, val loss: 0.05690\n",
      "Interaction training epoch: 98, train loss: 0.04854, val loss: 0.04831\n",
      "Interaction training epoch: 99, train loss: 0.02719, val loss: 0.02716\n",
      "Interaction training epoch: 100, train loss: 0.03689, val loss: 0.03722\n",
      "Interaction training epoch: 101, train loss: 0.04609, val loss: 0.04601\n",
      "Interaction training epoch: 102, train loss: 0.03250, val loss: 0.03231\n",
      "Interaction training epoch: 103, train loss: 0.03006, val loss: 0.02936\n",
      "Interaction training epoch: 104, train loss: 0.03967, val loss: 0.03937\n",
      "Interaction training epoch: 105, train loss: 0.03096, val loss: 0.03100\n",
      "Interaction training epoch: 106, train loss: 0.05099, val loss: 0.05044\n",
      "Interaction training epoch: 107, train loss: 0.03358, val loss: 0.03389\n",
      "Interaction training epoch: 108, train loss: 0.04273, val loss: 0.04236\n",
      "Interaction training epoch: 109, train loss: 0.07259, val loss: 0.07140\n",
      "Interaction training epoch: 110, train loss: 0.03442, val loss: 0.03433\n",
      "Interaction training epoch: 111, train loss: 0.02947, val loss: 0.02893\n",
      "Interaction training epoch: 112, train loss: 0.03413, val loss: 0.03396\n",
      "Interaction training epoch: 113, train loss: 0.03441, val loss: 0.03428\n",
      "Interaction training epoch: 114, train loss: 0.05762, val loss: 0.05645\n",
      "Interaction training epoch: 115, train loss: 0.02938, val loss: 0.02918\n",
      "Interaction training epoch: 116, train loss: 0.04805, val loss: 0.04824\n",
      "Interaction training epoch: 117, train loss: 0.03927, val loss: 0.03835\n",
      "Interaction training epoch: 118, train loss: 0.03949, val loss: 0.03920\n",
      "Interaction training epoch: 119, train loss: 0.03835, val loss: 0.03852\n",
      "Interaction training epoch: 120, train loss: 0.07402, val loss: 0.07267\n",
      "Interaction training epoch: 121, train loss: 0.02786, val loss: 0.02794\n",
      "Interaction training epoch: 122, train loss: 0.02966, val loss: 0.02840\n",
      "Interaction training epoch: 123, train loss: 0.03146, val loss: 0.03080\n",
      "Interaction training epoch: 124, train loss: 0.03135, val loss: 0.03095\n",
      "Interaction training epoch: 125, train loss: 0.02893, val loss: 0.02832\n",
      "Interaction training epoch: 126, train loss: 0.06644, val loss: 0.06554\n",
      "Interaction training epoch: 127, train loss: 0.02439, val loss: 0.02405\n",
      "Interaction training epoch: 128, train loss: 0.02442, val loss: 0.02392\n",
      "Interaction training epoch: 129, train loss: 0.06718, val loss: 0.06662\n",
      "Interaction training epoch: 130, train loss: 0.06873, val loss: 0.06752\n",
      "Interaction training epoch: 131, train loss: 0.02836, val loss: 0.02795\n",
      "Interaction training epoch: 132, train loss: 0.02537, val loss: 0.02462\n",
      "Interaction training epoch: 133, train loss: 0.05578, val loss: 0.05497\n",
      "Interaction training epoch: 134, train loss: 0.02756, val loss: 0.02628\n",
      "Interaction training epoch: 135, train loss: 0.03851, val loss: 0.03792\n",
      "Interaction training epoch: 136, train loss: 0.02792, val loss: 0.02763\n",
      "Interaction training epoch: 137, train loss: 0.02882, val loss: 0.02822\n",
      "Interaction training epoch: 138, train loss: 0.04268, val loss: 0.04230\n",
      "Interaction training epoch: 139, train loss: 0.02999, val loss: 0.03028\n",
      "Interaction training epoch: 140, train loss: 0.02653, val loss: 0.02598\n",
      "Interaction training epoch: 141, train loss: 0.02864, val loss: 0.02740\n",
      "Interaction training epoch: 142, train loss: 0.04031, val loss: 0.03930\n",
      "Interaction training epoch: 143, train loss: 0.08930, val loss: 0.08825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 144, train loss: 0.03278, val loss: 0.03201\n",
      "Interaction training epoch: 145, train loss: 0.07595, val loss: 0.07444\n",
      "Interaction training epoch: 146, train loss: 0.04088, val loss: 0.04007\n",
      "Interaction training epoch: 147, train loss: 0.03063, val loss: 0.02967\n",
      "Interaction training epoch: 148, train loss: 0.08210, val loss: 0.08116\n",
      "Interaction training epoch: 149, train loss: 0.02600, val loss: 0.02595\n",
      "Interaction training epoch: 150, train loss: 0.04451, val loss: 0.04266\n",
      "Interaction training epoch: 151, train loss: 0.05071, val loss: 0.04953\n",
      "Interaction training epoch: 152, train loss: 0.03689, val loss: 0.03657\n",
      "Interaction training epoch: 153, train loss: 0.05108, val loss: 0.05017\n",
      "Interaction training epoch: 154, train loss: 0.03125, val loss: 0.03020\n",
      "Interaction training epoch: 155, train loss: 0.05262, val loss: 0.05160\n",
      "Interaction training epoch: 156, train loss: 0.03189, val loss: 0.03099\n",
      "Interaction training epoch: 157, train loss: 0.03302, val loss: 0.03192\n",
      "Interaction training epoch: 158, train loss: 0.03919, val loss: 0.03867\n",
      "Interaction training epoch: 159, train loss: 0.06294, val loss: 0.06164\n",
      "Interaction training epoch: 160, train loss: 0.02504, val loss: 0.02398\n",
      "Interaction training epoch: 161, train loss: 0.02726, val loss: 0.02622\n",
      "Interaction training epoch: 162, train loss: 0.02696, val loss: 0.02663\n",
      "Interaction training epoch: 163, train loss: 0.04443, val loss: 0.04306\n",
      "Interaction training epoch: 164, train loss: 0.08229, val loss: 0.08080\n",
      "Interaction training epoch: 165, train loss: 0.02497, val loss: 0.02433\n",
      "Interaction training epoch: 166, train loss: 0.02405, val loss: 0.02321\n",
      "Interaction training epoch: 167, train loss: 0.03154, val loss: 0.03087\n",
      "Interaction training epoch: 168, train loss: 0.03079, val loss: 0.02948\n",
      "Interaction training epoch: 169, train loss: 0.02562, val loss: 0.02466\n",
      "Interaction training epoch: 170, train loss: 0.03950, val loss: 0.03811\n",
      "Interaction training epoch: 171, train loss: 0.05102, val loss: 0.05005\n",
      "Interaction training epoch: 172, train loss: 0.07393, val loss: 0.07199\n",
      "Interaction training epoch: 173, train loss: 0.03775, val loss: 0.03659\n",
      "Interaction training epoch: 174, train loss: 0.03283, val loss: 0.03172\n",
      "Interaction training epoch: 175, train loss: 0.02729, val loss: 0.02663\n",
      "Interaction training epoch: 176, train loss: 0.04249, val loss: 0.04090\n",
      "Interaction training epoch: 177, train loss: 0.11371, val loss: 0.11248\n",
      "Interaction training epoch: 178, train loss: 0.05008, val loss: 0.04851\n",
      "Interaction training epoch: 179, train loss: 0.05070, val loss: 0.04903\n",
      "Interaction training epoch: 180, train loss: 0.05192, val loss: 0.05082\n",
      "Interaction training epoch: 181, train loss: 0.03499, val loss: 0.03401\n",
      "Interaction training epoch: 182, train loss: 0.06009, val loss: 0.05850\n",
      "Interaction training epoch: 183, train loss: 0.03517, val loss: 0.03374\n",
      "Interaction training epoch: 184, train loss: 0.04107, val loss: 0.04010\n",
      "Interaction training epoch: 185, train loss: 0.06361, val loss: 0.06162\n",
      "Interaction training epoch: 186, train loss: 0.06349, val loss: 0.06178\n",
      "Interaction training epoch: 187, train loss: 0.05374, val loss: 0.05196\n",
      "Interaction training epoch: 188, train loss: 0.05958, val loss: 0.05804\n",
      "Interaction training epoch: 189, train loss: 0.02604, val loss: 0.02519\n",
      "Interaction training epoch: 190, train loss: 0.04369, val loss: 0.04302\n",
      "Interaction training epoch: 191, train loss: 0.04175, val loss: 0.04116\n",
      "Interaction training epoch: 192, train loss: 0.06782, val loss: 0.06645\n",
      "Interaction training epoch: 193, train loss: 0.06145, val loss: 0.06034\n",
      "Interaction training epoch: 194, train loss: 0.04303, val loss: 0.04151\n",
      "Interaction training epoch: 195, train loss: 0.03319, val loss: 0.03229\n",
      "Interaction training epoch: 196, train loss: 0.03143, val loss: 0.02941\n",
      "Interaction training epoch: 197, train loss: 0.03413, val loss: 0.03311\n",
      "Interaction training epoch: 198, train loss: 0.03045, val loss: 0.02901\n",
      "Interaction training epoch: 199, train loss: 0.04253, val loss: 0.04138\n",
      "Interaction training epoch: 200, train loss: 0.04032, val loss: 0.03845\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########4 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.02853, val loss: 0.02785\n",
      "Interaction tuning epoch: 2, train loss: 0.02401, val loss: 0.02299\n",
      "Interaction tuning epoch: 3, train loss: 0.04478, val loss: 0.04265\n",
      "Interaction tuning epoch: 4, train loss: 0.06395, val loss: 0.06292\n",
      "Interaction tuning epoch: 5, train loss: 0.02356, val loss: 0.02286\n",
      "Interaction tuning epoch: 6, train loss: 0.06369, val loss: 0.06228\n",
      "Interaction tuning epoch: 7, train loss: 0.04154, val loss: 0.04077\n",
      "Interaction tuning epoch: 8, train loss: 0.08354, val loss: 0.08227\n",
      "Interaction tuning epoch: 9, train loss: 0.07421, val loss: 0.07270\n",
      "Interaction tuning epoch: 10, train loss: 0.04015, val loss: 0.03931\n",
      "Interaction tuning epoch: 11, train loss: 0.02840, val loss: 0.02746\n",
      "Interaction tuning epoch: 12, train loss: 0.06172, val loss: 0.06021\n",
      "Interaction tuning epoch: 13, train loss: 0.03776, val loss: 0.03699\n",
      "Interaction tuning epoch: 14, train loss: 0.07388, val loss: 0.07213\n",
      "Interaction tuning epoch: 15, train loss: 0.02628, val loss: 0.02557\n",
      "Interaction tuning epoch: 16, train loss: 0.02860, val loss: 0.02813\n",
      "Interaction tuning epoch: 17, train loss: 0.04973, val loss: 0.04802\n",
      "Interaction tuning epoch: 18, train loss: 0.02869, val loss: 0.02741\n",
      "Interaction tuning epoch: 19, train loss: 0.03914, val loss: 0.03730\n",
      "Interaction tuning epoch: 20, train loss: 0.03596, val loss: 0.03526\n",
      "Interaction tuning epoch: 21, train loss: 0.04924, val loss: 0.04797\n",
      "Interaction tuning epoch: 22, train loss: 0.08235, val loss: 0.08148\n",
      "Interaction tuning epoch: 23, train loss: 0.08430, val loss: 0.08186\n",
      "Interaction tuning epoch: 24, train loss: 0.07758, val loss: 0.07581\n",
      "Interaction tuning epoch: 25, train loss: 0.02468, val loss: 0.02425\n",
      "Interaction tuning epoch: 26, train loss: 0.02729, val loss: 0.02674\n",
      "Interaction tuning epoch: 27, train loss: 0.02677, val loss: 0.02542\n",
      "Interaction tuning epoch: 28, train loss: 0.03303, val loss: 0.03233\n",
      "Interaction tuning epoch: 29, train loss: 0.07628, val loss: 0.07409\n",
      "Interaction tuning epoch: 30, train loss: 0.05354, val loss: 0.05248\n",
      "Interaction tuning epoch: 31, train loss: 0.02819, val loss: 0.02642\n",
      "Interaction tuning epoch: 32, train loss: 0.02994, val loss: 0.02905\n",
      "Interaction tuning epoch: 33, train loss: 0.03903, val loss: 0.03809\n",
      "Interaction tuning epoch: 34, train loss: 0.03406, val loss: 0.03261\n",
      "Interaction tuning epoch: 35, train loss: 0.02886, val loss: 0.02797\n",
      "Interaction tuning epoch: 36, train loss: 0.04728, val loss: 0.04591\n",
      "Interaction tuning epoch: 37, train loss: 0.02745, val loss: 0.02653\n",
      "Interaction tuning epoch: 38, train loss: 0.09613, val loss: 0.09462\n",
      "Interaction tuning epoch: 39, train loss: 0.06430, val loss: 0.06326\n",
      "Interaction tuning epoch: 40, train loss: 0.03564, val loss: 0.03467\n",
      "Interaction tuning epoch: 41, train loss: 0.05842, val loss: 0.05720\n",
      "Interaction tuning epoch: 42, train loss: 0.02719, val loss: 0.02601\n",
      "Interaction tuning epoch: 43, train loss: 0.03689, val loss: 0.03533\n",
      "Interaction tuning epoch: 44, train loss: 0.02563, val loss: 0.02452\n",
      "Interaction tuning epoch: 45, train loss: 0.11515, val loss: 0.11336\n",
      "Interaction tuning epoch: 46, train loss: 0.05383, val loss: 0.05274\n",
      "Interaction tuning epoch: 47, train loss: 0.03183, val loss: 0.02986\n",
      "Interaction tuning epoch: 48, train loss: 0.04755, val loss: 0.04566\n",
      "Interaction tuning epoch: 49, train loss: 0.03504, val loss: 0.03437\n",
      "Interaction tuning epoch: 50, train loss: 0.03646, val loss: 0.03600\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 36.96691942214966\n",
      "After the gam stage, training error is 0.03646 , validation error is 0.03600\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 1.968308\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.024025 validation MAE=0.033896,rank=9\n",
      "[SoftImpute] Iter 2: observed MAE=0.021677 validation MAE=0.032924,rank=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 3: observed MAE=0.019750 validation MAE=0.032036,rank=9\n",
      "[SoftImpute] Iter 4: observed MAE=0.018142 validation MAE=0.031269,rank=9\n",
      "[SoftImpute] Iter 5: observed MAE=0.016774 validation MAE=0.030584,rank=9\n",
      "[SoftImpute] Iter 6: observed MAE=0.015605 validation MAE=0.029974,rank=9\n",
      "[SoftImpute] Iter 7: observed MAE=0.014587 validation MAE=0.029428,rank=9\n",
      "[SoftImpute] Iter 8: observed MAE=0.013696 validation MAE=0.028941,rank=9\n",
      "[SoftImpute] Iter 9: observed MAE=0.012913 validation MAE=0.028503,rank=9\n",
      "[SoftImpute] Iter 10: observed MAE=0.012223 validation MAE=0.028105,rank=9\n",
      "[SoftImpute] Iter 11: observed MAE=0.011609 validation MAE=0.027748,rank=9\n",
      "[SoftImpute] Iter 12: observed MAE=0.011063 validation MAE=0.027426,rank=9\n",
      "[SoftImpute] Iter 13: observed MAE=0.010572 validation MAE=0.027132,rank=9\n",
      "[SoftImpute] Iter 14: observed MAE=0.010128 validation MAE=0.026868,rank=9\n",
      "[SoftImpute] Iter 15: observed MAE=0.009727 validation MAE=0.026625,rank=9\n",
      "[SoftImpute] Iter 16: observed MAE=0.009363 validation MAE=0.026401,rank=9\n",
      "[SoftImpute] Iter 17: observed MAE=0.009031 validation MAE=0.026196,rank=9\n",
      "[SoftImpute] Iter 18: observed MAE=0.008727 validation MAE=0.026007,rank=9\n",
      "[SoftImpute] Iter 19: observed MAE=0.008450 validation MAE=0.025833,rank=9\n",
      "[SoftImpute] Iter 20: observed MAE=0.008195 validation MAE=0.025674,rank=9\n",
      "[SoftImpute] Iter 21: observed MAE=0.007961 validation MAE=0.025524,rank=9\n",
      "[SoftImpute] Iter 22: observed MAE=0.007747 validation MAE=0.025380,rank=9\n",
      "[SoftImpute] Iter 23: observed MAE=0.007548 validation MAE=0.025245,rank=9\n",
      "[SoftImpute] Iter 24: observed MAE=0.007366 validation MAE=0.025116,rank=9\n",
      "[SoftImpute] Iter 25: observed MAE=0.007196 validation MAE=0.024993,rank=9\n",
      "[SoftImpute] Iter 26: observed MAE=0.007039 validation MAE=0.024876,rank=9\n",
      "[SoftImpute] Iter 27: observed MAE=0.006892 validation MAE=0.024765,rank=9\n",
      "[SoftImpute] Iter 28: observed MAE=0.006756 validation MAE=0.024659,rank=9\n",
      "[SoftImpute] Iter 29: observed MAE=0.006629 validation MAE=0.024559,rank=9\n",
      "[SoftImpute] Iter 30: observed MAE=0.006511 validation MAE=0.024462,rank=9\n",
      "[SoftImpute] Iter 31: observed MAE=0.006400 validation MAE=0.024369,rank=9\n",
      "[SoftImpute] Iter 32: observed MAE=0.006296 validation MAE=0.024280,rank=9\n",
      "[SoftImpute] Iter 33: observed MAE=0.006199 validation MAE=0.024194,rank=9\n",
      "[SoftImpute] Iter 34: observed MAE=0.006107 validation MAE=0.024112,rank=9\n",
      "[SoftImpute] Iter 35: observed MAE=0.006021 validation MAE=0.024032,rank=9\n",
      "[SoftImpute] Iter 36: observed MAE=0.005940 validation MAE=0.023954,rank=9\n",
      "[SoftImpute] Iter 37: observed MAE=0.005863 validation MAE=0.023879,rank=9\n",
      "[SoftImpute] Iter 38: observed MAE=0.005790 validation MAE=0.023806,rank=9\n",
      "[SoftImpute] Iter 39: observed MAE=0.005722 validation MAE=0.023734,rank=9\n",
      "[SoftImpute] Iter 40: observed MAE=0.005657 validation MAE=0.023665,rank=9\n",
      "[SoftImpute] Iter 41: observed MAE=0.005596 validation MAE=0.023598,rank=9\n",
      "[SoftImpute] Iter 42: observed MAE=0.005538 validation MAE=0.023533,rank=9\n",
      "[SoftImpute] Iter 43: observed MAE=0.005483 validation MAE=0.023469,rank=9\n",
      "[SoftImpute] Iter 44: observed MAE=0.005430 validation MAE=0.023407,rank=9\n",
      "[SoftImpute] Iter 45: observed MAE=0.005380 validation MAE=0.023345,rank=9\n",
      "[SoftImpute] Iter 46: observed MAE=0.005333 validation MAE=0.023284,rank=9\n",
      "[SoftImpute] Iter 47: observed MAE=0.005288 validation MAE=0.023225,rank=9\n",
      "[SoftImpute] Iter 48: observed MAE=0.005245 validation MAE=0.023166,rank=9\n",
      "[SoftImpute] Iter 49: observed MAE=0.005203 validation MAE=0.023109,rank=9\n",
      "[SoftImpute] Iter 50: observed MAE=0.005164 validation MAE=0.023053,rank=9\n",
      "[SoftImpute] Iter 51: observed MAE=0.005126 validation MAE=0.022998,rank=9\n",
      "[SoftImpute] Iter 52: observed MAE=0.005089 validation MAE=0.022945,rank=9\n",
      "[SoftImpute] Iter 53: observed MAE=0.005054 validation MAE=0.022892,rank=9\n",
      "[SoftImpute] Iter 54: observed MAE=0.005021 validation MAE=0.022839,rank=9\n",
      "[SoftImpute] Iter 55: observed MAE=0.004989 validation MAE=0.022788,rank=9\n",
      "[SoftImpute] Iter 56: observed MAE=0.004958 validation MAE=0.022737,rank=9\n",
      "[SoftImpute] Iter 57: observed MAE=0.004928 validation MAE=0.022686,rank=9\n",
      "[SoftImpute] Iter 58: observed MAE=0.004899 validation MAE=0.022637,rank=9\n",
      "[SoftImpute] Iter 59: observed MAE=0.004872 validation MAE=0.022587,rank=9\n",
      "[SoftImpute] Iter 60: observed MAE=0.004845 validation MAE=0.022539,rank=9\n",
      "[SoftImpute] Iter 61: observed MAE=0.004820 validation MAE=0.022490,rank=9\n",
      "[SoftImpute] Iter 62: observed MAE=0.004795 validation MAE=0.022442,rank=9\n",
      "[SoftImpute] Iter 63: observed MAE=0.004771 validation MAE=0.022395,rank=9\n",
      "[SoftImpute] Iter 64: observed MAE=0.004748 validation MAE=0.022348,rank=9\n",
      "[SoftImpute] Iter 65: observed MAE=0.004726 validation MAE=0.022302,rank=9\n",
      "[SoftImpute] Iter 66: observed MAE=0.004705 validation MAE=0.022257,rank=9\n",
      "[SoftImpute] Iter 67: observed MAE=0.004684 validation MAE=0.022212,rank=9\n",
      "[SoftImpute] Iter 68: observed MAE=0.004664 validation MAE=0.022167,rank=9\n",
      "[SoftImpute] Iter 69: observed MAE=0.004644 validation MAE=0.022123,rank=9\n",
      "[SoftImpute] Iter 70: observed MAE=0.004625 validation MAE=0.022080,rank=9\n",
      "[SoftImpute] Iter 71: observed MAE=0.004607 validation MAE=0.022036,rank=9\n",
      "[SoftImpute] Iter 72: observed MAE=0.004589 validation MAE=0.021993,rank=9\n",
      "[SoftImpute] Iter 73: observed MAE=0.004572 validation MAE=0.021951,rank=9\n",
      "[SoftImpute] Iter 74: observed MAE=0.004555 validation MAE=0.021909,rank=9\n",
      "[SoftImpute] Iter 75: observed MAE=0.004539 validation MAE=0.021867,rank=9\n",
      "[SoftImpute] Iter 76: observed MAE=0.004523 validation MAE=0.021826,rank=9\n",
      "[SoftImpute] Iter 77: observed MAE=0.004508 validation MAE=0.021785,rank=9\n",
      "[SoftImpute] Iter 78: observed MAE=0.004493 validation MAE=0.021745,rank=9\n",
      "[SoftImpute] Iter 79: observed MAE=0.004478 validation MAE=0.021705,rank=9\n",
      "[SoftImpute] Iter 80: observed MAE=0.004464 validation MAE=0.021665,rank=9\n",
      "[SoftImpute] Iter 81: observed MAE=0.004450 validation MAE=0.021626,rank=9\n",
      "[SoftImpute] Iter 82: observed MAE=0.004437 validation MAE=0.021587,rank=9\n",
      "[SoftImpute] Iter 83: observed MAE=0.004424 validation MAE=0.021548,rank=9\n",
      "[SoftImpute] Iter 84: observed MAE=0.004411 validation MAE=0.021509,rank=9\n",
      "[SoftImpute] Iter 85: observed MAE=0.004399 validation MAE=0.021470,rank=9\n",
      "[SoftImpute] Iter 86: observed MAE=0.004387 validation MAE=0.021432,rank=9\n",
      "[SoftImpute] Iter 87: observed MAE=0.004375 validation MAE=0.021394,rank=9\n",
      "[SoftImpute] Iter 88: observed MAE=0.004364 validation MAE=0.021357,rank=9\n",
      "[SoftImpute] Iter 89: observed MAE=0.004353 validation MAE=0.021320,rank=9\n",
      "[SoftImpute] Iter 90: observed MAE=0.004342 validation MAE=0.021283,rank=9\n",
      "[SoftImpute] Iter 91: observed MAE=0.004331 validation MAE=0.021246,rank=9\n",
      "[SoftImpute] Iter 92: observed MAE=0.004321 validation MAE=0.021209,rank=9\n",
      "[SoftImpute] Iter 93: observed MAE=0.004311 validation MAE=0.021173,rank=9\n",
      "[SoftImpute] Iter 94: observed MAE=0.004301 validation MAE=0.021137,rank=9\n",
      "[SoftImpute] Iter 95: observed MAE=0.004292 validation MAE=0.021102,rank=9\n",
      "[SoftImpute] Iter 96: observed MAE=0.004282 validation MAE=0.021067,rank=9\n",
      "[SoftImpute] Iter 97: observed MAE=0.004273 validation MAE=0.021032,rank=9\n",
      "[SoftImpute] Iter 98: observed MAE=0.004264 validation MAE=0.020997,rank=9\n",
      "[SoftImpute] Iter 99: observed MAE=0.004255 validation MAE=0.020963,rank=9\n",
      "[SoftImpute] Iter 100: observed MAE=0.004246 validation MAE=0.020929,rank=9\n",
      "[SoftImpute] Iter 101: observed MAE=0.004238 validation MAE=0.020895,rank=9\n",
      "[SoftImpute] Iter 102: observed MAE=0.004230 validation MAE=0.020862,rank=9\n",
      "[SoftImpute] Iter 103: observed MAE=0.004222 validation MAE=0.020828,rank=9\n",
      "[SoftImpute] Iter 104: observed MAE=0.004214 validation MAE=0.020795,rank=9\n",
      "[SoftImpute] Iter 105: observed MAE=0.004206 validation MAE=0.020763,rank=9\n",
      "[SoftImpute] Iter 106: observed MAE=0.004198 validation MAE=0.020730,rank=9\n",
      "[SoftImpute] Iter 107: observed MAE=0.004191 validation MAE=0.020698,rank=9\n",
      "[SoftImpute] Iter 108: observed MAE=0.004184 validation MAE=0.020665,rank=9\n",
      "[SoftImpute] Iter 109: observed MAE=0.004176 validation MAE=0.020634,rank=9\n",
      "[SoftImpute] Iter 110: observed MAE=0.004169 validation MAE=0.020602,rank=9\n",
      "[SoftImpute] Iter 111: observed MAE=0.004162 validation MAE=0.020571,rank=9\n",
      "[SoftImpute] Iter 112: observed MAE=0.004156 validation MAE=0.020540,rank=9\n",
      "[SoftImpute] Iter 113: observed MAE=0.004149 validation MAE=0.020509,rank=9\n",
      "[SoftImpute] Iter 114: observed MAE=0.004143 validation MAE=0.020478,rank=9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 115: observed MAE=0.004136 validation MAE=0.020448,rank=9\n",
      "[SoftImpute] Iter 116: observed MAE=0.004130 validation MAE=0.020419,rank=9\n",
      "[SoftImpute] Iter 117: observed MAE=0.004124 validation MAE=0.020389,rank=9\n",
      "[SoftImpute] Iter 118: observed MAE=0.004118 validation MAE=0.020360,rank=9\n",
      "[SoftImpute] Iter 119: observed MAE=0.004112 validation MAE=0.020330,rank=9\n",
      "[SoftImpute] Iter 120: observed MAE=0.004106 validation MAE=0.020301,rank=9\n",
      "[SoftImpute] Iter 121: observed MAE=0.004101 validation MAE=0.020273,rank=9\n",
      "[SoftImpute] Iter 122: observed MAE=0.004095 validation MAE=0.020244,rank=9\n",
      "[SoftImpute] Iter 123: observed MAE=0.004089 validation MAE=0.020216,rank=9\n",
      "[SoftImpute] Iter 124: observed MAE=0.004084 validation MAE=0.020187,rank=9\n",
      "[SoftImpute] Iter 125: observed MAE=0.004079 validation MAE=0.020159,rank=9\n",
      "[SoftImpute] Iter 126: observed MAE=0.004073 validation MAE=0.020131,rank=9\n",
      "[SoftImpute] Iter 127: observed MAE=0.004068 validation MAE=0.020103,rank=9\n",
      "[SoftImpute] Iter 128: observed MAE=0.004063 validation MAE=0.020075,rank=9\n",
      "[SoftImpute] Iter 129: observed MAE=0.004058 validation MAE=0.020048,rank=9\n",
      "[SoftImpute] Iter 130: observed MAE=0.004053 validation MAE=0.020020,rank=9\n",
      "[SoftImpute] Iter 131: observed MAE=0.004048 validation MAE=0.019993,rank=9\n",
      "[SoftImpute] Iter 132: observed MAE=0.004044 validation MAE=0.019966,rank=9\n",
      "[SoftImpute] Iter 133: observed MAE=0.004039 validation MAE=0.019939,rank=9\n",
      "[SoftImpute] Iter 134: observed MAE=0.004034 validation MAE=0.019912,rank=9\n",
      "[SoftImpute] Iter 135: observed MAE=0.004030 validation MAE=0.019885,rank=9\n",
      "[SoftImpute] Iter 136: observed MAE=0.004025 validation MAE=0.019859,rank=9\n",
      "[SoftImpute] Iter 137: observed MAE=0.004021 validation MAE=0.019832,rank=9\n",
      "[SoftImpute] Iter 138: observed MAE=0.004017 validation MAE=0.019806,rank=9\n",
      "[SoftImpute] Iter 139: observed MAE=0.004013 validation MAE=0.019780,rank=9\n",
      "[SoftImpute] Iter 140: observed MAE=0.004008 validation MAE=0.019754,rank=9\n",
      "[SoftImpute] Iter 141: observed MAE=0.004004 validation MAE=0.019728,rank=9\n",
      "[SoftImpute] Iter 142: observed MAE=0.004000 validation MAE=0.019703,rank=9\n",
      "[SoftImpute] Iter 143: observed MAE=0.003996 validation MAE=0.019677,rank=9\n",
      "[SoftImpute] Iter 144: observed MAE=0.003992 validation MAE=0.019652,rank=9\n",
      "[SoftImpute] Iter 145: observed MAE=0.003989 validation MAE=0.019626,rank=9\n",
      "[SoftImpute] Iter 146: observed MAE=0.003985 validation MAE=0.019601,rank=9\n",
      "[SoftImpute] Iter 147: observed MAE=0.003981 validation MAE=0.019576,rank=9\n",
      "[SoftImpute] Iter 148: observed MAE=0.003977 validation MAE=0.019551,rank=9\n",
      "[SoftImpute] Iter 149: observed MAE=0.003974 validation MAE=0.019527,rank=9\n",
      "[SoftImpute] Iter 150: observed MAE=0.003970 validation MAE=0.019502,rank=9\n",
      "[SoftImpute] Iter 151: observed MAE=0.003967 validation MAE=0.019477,rank=9\n",
      "[SoftImpute] Iter 152: observed MAE=0.003963 validation MAE=0.019453,rank=9\n",
      "[SoftImpute] Iter 153: observed MAE=0.003960 validation MAE=0.019429,rank=9\n",
      "[SoftImpute] Iter 154: observed MAE=0.003956 validation MAE=0.019404,rank=9\n",
      "[SoftImpute] Iter 155: observed MAE=0.003953 validation MAE=0.019380,rank=9\n",
      "[SoftImpute] Iter 156: observed MAE=0.003950 validation MAE=0.019356,rank=9\n",
      "[SoftImpute] Iter 157: observed MAE=0.003946 validation MAE=0.019332,rank=9\n",
      "[SoftImpute] Iter 158: observed MAE=0.003943 validation MAE=0.019308,rank=9\n",
      "[SoftImpute] Iter 159: observed MAE=0.003940 validation MAE=0.019284,rank=9\n",
      "[SoftImpute] Iter 160: observed MAE=0.003937 validation MAE=0.019261,rank=9\n",
      "[SoftImpute] Iter 161: observed MAE=0.003934 validation MAE=0.019237,rank=9\n",
      "[SoftImpute] Iter 162: observed MAE=0.003930 validation MAE=0.019214,rank=9\n",
      "[SoftImpute] Iter 163: observed MAE=0.003927 validation MAE=0.019191,rank=9\n",
      "[SoftImpute] Iter 164: observed MAE=0.003924 validation MAE=0.019168,rank=9\n",
      "[SoftImpute] Iter 165: observed MAE=0.003921 validation MAE=0.019146,rank=9\n",
      "[SoftImpute] Iter 166: observed MAE=0.003919 validation MAE=0.019123,rank=9\n",
      "[SoftImpute] Iter 167: observed MAE=0.003916 validation MAE=0.019100,rank=9\n",
      "[SoftImpute] Iter 168: observed MAE=0.003913 validation MAE=0.019078,rank=9\n",
      "[SoftImpute] Iter 169: observed MAE=0.003910 validation MAE=0.019056,rank=9\n",
      "[SoftImpute] Iter 170: observed MAE=0.003907 validation MAE=0.019033,rank=9\n",
      "[SoftImpute] Iter 171: observed MAE=0.003904 validation MAE=0.019011,rank=9\n",
      "[SoftImpute] Iter 172: observed MAE=0.003902 validation MAE=0.018989,rank=9\n",
      "[SoftImpute] Iter 173: observed MAE=0.003899 validation MAE=0.018967,rank=9\n",
      "[SoftImpute] Iter 174: observed MAE=0.003896 validation MAE=0.018946,rank=9\n",
      "[SoftImpute] Iter 175: observed MAE=0.003894 validation MAE=0.018924,rank=9\n",
      "[SoftImpute] Iter 176: observed MAE=0.003891 validation MAE=0.018902,rank=9\n",
      "[SoftImpute] Iter 177: observed MAE=0.003889 validation MAE=0.018881,rank=9\n",
      "[SoftImpute] Iter 178: observed MAE=0.003886 validation MAE=0.018860,rank=9\n",
      "[SoftImpute] Iter 179: observed MAE=0.003884 validation MAE=0.018838,rank=9\n",
      "[SoftImpute] Iter 180: observed MAE=0.003881 validation MAE=0.018817,rank=9\n",
      "[SoftImpute] Iter 181: observed MAE=0.003879 validation MAE=0.018796,rank=9\n",
      "[SoftImpute] Iter 182: observed MAE=0.003876 validation MAE=0.018775,rank=9\n",
      "[SoftImpute] Iter 183: observed MAE=0.003874 validation MAE=0.018755,rank=9\n",
      "[SoftImpute] Iter 184: observed MAE=0.003872 validation MAE=0.018734,rank=9\n",
      "[SoftImpute] Iter 185: observed MAE=0.003869 validation MAE=0.018713,rank=9\n",
      "[SoftImpute] Iter 186: observed MAE=0.003867 validation MAE=0.018693,rank=9\n",
      "[SoftImpute] Iter 187: observed MAE=0.003865 validation MAE=0.018673,rank=9\n",
      "[SoftImpute] Iter 188: observed MAE=0.003862 validation MAE=0.018652,rank=9\n",
      "[SoftImpute] Iter 189: observed MAE=0.003860 validation MAE=0.018632,rank=9\n",
      "[SoftImpute] Iter 190: observed MAE=0.003858 validation MAE=0.018612,rank=9\n",
      "[SoftImpute] Iter 191: observed MAE=0.003856 validation MAE=0.018592,rank=9\n",
      "[SoftImpute] Iter 192: observed MAE=0.003854 validation MAE=0.018572,rank=9\n",
      "[SoftImpute] Iter 193: observed MAE=0.003851 validation MAE=0.018553,rank=9\n",
      "[SoftImpute] Iter 194: observed MAE=0.003849 validation MAE=0.018533,rank=9\n",
      "[SoftImpute] Iter 195: observed MAE=0.003847 validation MAE=0.018513,rank=9\n",
      "[SoftImpute] Iter 196: observed MAE=0.003845 validation MAE=0.018494,rank=9\n",
      "[SoftImpute] Iter 197: observed MAE=0.003843 validation MAE=0.018474,rank=9\n",
      "[SoftImpute] Iter 198: observed MAE=0.003841 validation MAE=0.018455,rank=9\n",
      "[SoftImpute] Iter 199: observed MAE=0.003839 validation MAE=0.018436,rank=9\n",
      "[SoftImpute] Iter 200: observed MAE=0.003837 validation MAE=0.018417,rank=9\n",
      "[SoftImpute] Iter 201: observed MAE=0.003835 validation MAE=0.018398,rank=9\n",
      "[SoftImpute] Iter 202: observed MAE=0.003833 validation MAE=0.018379,rank=9\n",
      "[SoftImpute] Iter 203: observed MAE=0.003831 validation MAE=0.018361,rank=9\n",
      "[SoftImpute] Iter 204: observed MAE=0.003829 validation MAE=0.018342,rank=9\n",
      "[SoftImpute] Iter 205: observed MAE=0.003827 validation MAE=0.018323,rank=9\n",
      "[SoftImpute] Iter 206: observed MAE=0.003825 validation MAE=0.018305,rank=9\n",
      "[SoftImpute] Iter 207: observed MAE=0.003823 validation MAE=0.018287,rank=9\n",
      "[SoftImpute] Iter 208: observed MAE=0.003822 validation MAE=0.018268,rank=9\n",
      "[SoftImpute] Iter 209: observed MAE=0.003820 validation MAE=0.018250,rank=9\n",
      "[SoftImpute] Iter 210: observed MAE=0.003818 validation MAE=0.018232,rank=9\n",
      "[SoftImpute] Iter 211: observed MAE=0.003816 validation MAE=0.018214,rank=9\n",
      "[SoftImpute] Iter 212: observed MAE=0.003814 validation MAE=0.018196,rank=9\n",
      "[SoftImpute] Iter 213: observed MAE=0.003813 validation MAE=0.018179,rank=9\n",
      "[SoftImpute] Iter 214: observed MAE=0.003811 validation MAE=0.018161,rank=9\n",
      "[SoftImpute] Iter 215: observed MAE=0.003809 validation MAE=0.018143,rank=9\n",
      "[SoftImpute] Iter 216: observed MAE=0.003807 validation MAE=0.018126,rank=9\n",
      "[SoftImpute] Iter 217: observed MAE=0.003806 validation MAE=0.018108,rank=9\n",
      "[SoftImpute] Iter 218: observed MAE=0.003804 validation MAE=0.018091,rank=9\n",
      "[SoftImpute] Iter 219: observed MAE=0.003802 validation MAE=0.018074,rank=9\n",
      "[SoftImpute] Iter 220: observed MAE=0.003801 validation MAE=0.018058,rank=9\n",
      "[SoftImpute] Iter 221: observed MAE=0.003799 validation MAE=0.018041,rank=9\n",
      "[SoftImpute] Iter 222: observed MAE=0.003797 validation MAE=0.018024,rank=9\n",
      "[SoftImpute] Iter 223: observed MAE=0.003796 validation MAE=0.018008,rank=9\n",
      "[SoftImpute] Iter 224: observed MAE=0.003794 validation MAE=0.017992,rank=9\n",
      "[SoftImpute] Iter 225: observed MAE=0.003792 validation MAE=0.017976,rank=9\n",
      "[SoftImpute] Iter 226: observed MAE=0.003791 validation MAE=0.017959,rank=9\n",
      "[SoftImpute] Iter 227: observed MAE=0.003789 validation MAE=0.017943,rank=9\n",
      "[SoftImpute] Iter 228: observed MAE=0.003788 validation MAE=0.017927,rank=9\n",
      "[SoftImpute] Iter 229: observed MAE=0.003786 validation MAE=0.017911,rank=9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 230: observed MAE=0.003785 validation MAE=0.017896,rank=9\n",
      "[SoftImpute] Iter 231: observed MAE=0.003783 validation MAE=0.017880,rank=9\n",
      "[SoftImpute] Iter 232: observed MAE=0.003782 validation MAE=0.017864,rank=9\n",
      "[SoftImpute] Iter 233: observed MAE=0.003780 validation MAE=0.017849,rank=9\n",
      "[SoftImpute] Iter 234: observed MAE=0.003779 validation MAE=0.017833,rank=9\n",
      "[SoftImpute] Iter 235: observed MAE=0.003777 validation MAE=0.017818,rank=9\n",
      "[SoftImpute] Iter 236: observed MAE=0.003776 validation MAE=0.017802,rank=9\n",
      "[SoftImpute] Iter 237: observed MAE=0.003774 validation MAE=0.017787,rank=9\n",
      "[SoftImpute] Iter 238: observed MAE=0.003773 validation MAE=0.017772,rank=9\n",
      "[SoftImpute] Iter 239: observed MAE=0.003771 validation MAE=0.017756,rank=9\n",
      "[SoftImpute] Iter 240: observed MAE=0.003770 validation MAE=0.017741,rank=9\n",
      "[SoftImpute] Iter 241: observed MAE=0.003769 validation MAE=0.017726,rank=9\n",
      "[SoftImpute] Iter 242: observed MAE=0.003767 validation MAE=0.017711,rank=9\n",
      "[SoftImpute] Iter 243: observed MAE=0.003766 validation MAE=0.017697,rank=9\n",
      "[SoftImpute] Iter 244: observed MAE=0.003764 validation MAE=0.017682,rank=9\n",
      "[SoftImpute] Iter 245: observed MAE=0.003763 validation MAE=0.017668,rank=9\n",
      "[SoftImpute] Iter 246: observed MAE=0.003762 validation MAE=0.017653,rank=9\n",
      "[SoftImpute] Iter 247: observed MAE=0.003760 validation MAE=0.017639,rank=9\n",
      "[SoftImpute] Iter 248: observed MAE=0.003759 validation MAE=0.017624,rank=9\n",
      "[SoftImpute] Iter 249: observed MAE=0.003758 validation MAE=0.017610,rank=9\n",
      "[SoftImpute] Iter 250: observed MAE=0.003756 validation MAE=0.017596,rank=9\n",
      "[SoftImpute] Iter 251: observed MAE=0.003755 validation MAE=0.017582,rank=9\n",
      "[SoftImpute] Iter 252: observed MAE=0.003754 validation MAE=0.017568,rank=9\n",
      "[SoftImpute] Iter 253: observed MAE=0.003752 validation MAE=0.017554,rank=9\n",
      "[SoftImpute] Iter 254: observed MAE=0.003751 validation MAE=0.017540,rank=9\n",
      "[SoftImpute] Iter 255: observed MAE=0.003750 validation MAE=0.017527,rank=9\n",
      "[SoftImpute] Iter 256: observed MAE=0.003748 validation MAE=0.017513,rank=9\n",
      "[SoftImpute] Iter 257: observed MAE=0.003747 validation MAE=0.017499,rank=9\n",
      "[SoftImpute] Iter 258: observed MAE=0.003746 validation MAE=0.017486,rank=9\n",
      "[SoftImpute] Iter 259: observed MAE=0.003745 validation MAE=0.017473,rank=9\n",
      "[SoftImpute] Iter 260: observed MAE=0.003743 validation MAE=0.017459,rank=9\n",
      "[SoftImpute] Iter 261: observed MAE=0.003742 validation MAE=0.017446,rank=9\n",
      "[SoftImpute] Iter 262: observed MAE=0.003741 validation MAE=0.017433,rank=9\n",
      "[SoftImpute] Iter 263: observed MAE=0.003740 validation MAE=0.017420,rank=9\n",
      "[SoftImpute] Iter 264: observed MAE=0.003738 validation MAE=0.017406,rank=9\n",
      "[SoftImpute] Iter 265: observed MAE=0.003737 validation MAE=0.017394,rank=9\n",
      "[SoftImpute] Iter 266: observed MAE=0.003736 validation MAE=0.017381,rank=9\n",
      "[SoftImpute] Iter 267: observed MAE=0.003735 validation MAE=0.017368,rank=9\n",
      "[SoftImpute] Iter 268: observed MAE=0.003734 validation MAE=0.017355,rank=9\n",
      "[SoftImpute] Iter 269: observed MAE=0.003732 validation MAE=0.017342,rank=9\n",
      "[SoftImpute] Iter 270: observed MAE=0.003731 validation MAE=0.017330,rank=9\n",
      "[SoftImpute] Iter 271: observed MAE=0.003730 validation MAE=0.017317,rank=9\n",
      "[SoftImpute] Iter 272: observed MAE=0.003729 validation MAE=0.017305,rank=9\n",
      "[SoftImpute] Iter 273: observed MAE=0.003728 validation MAE=0.017293,rank=9\n",
      "[SoftImpute] Iter 274: observed MAE=0.003727 validation MAE=0.017280,rank=9\n",
      "[SoftImpute] Iter 275: observed MAE=0.003725 validation MAE=0.017268,rank=9\n",
      "[SoftImpute] Iter 276: observed MAE=0.003724 validation MAE=0.017256,rank=9\n",
      "[SoftImpute] Iter 277: observed MAE=0.003723 validation MAE=0.017244,rank=9\n",
      "[SoftImpute] Iter 278: observed MAE=0.003722 validation MAE=0.017232,rank=9\n",
      "[SoftImpute] Iter 279: observed MAE=0.003721 validation MAE=0.017220,rank=9\n",
      "[SoftImpute] Iter 280: observed MAE=0.003720 validation MAE=0.017208,rank=9\n",
      "[SoftImpute] Iter 281: observed MAE=0.003719 validation MAE=0.017197,rank=9\n",
      "[SoftImpute] Iter 282: observed MAE=0.003718 validation MAE=0.017185,rank=9\n",
      "[SoftImpute] Iter 283: observed MAE=0.003716 validation MAE=0.017174,rank=9\n",
      "[SoftImpute] Iter 284: observed MAE=0.003715 validation MAE=0.017162,rank=9\n",
      "[SoftImpute] Iter 285: observed MAE=0.003714 validation MAE=0.017151,rank=9\n",
      "[SoftImpute] Iter 286: observed MAE=0.003713 validation MAE=0.017139,rank=9\n",
      "[SoftImpute] Iter 287: observed MAE=0.003712 validation MAE=0.017128,rank=9\n",
      "[SoftImpute] Iter 288: observed MAE=0.003711 validation MAE=0.017117,rank=9\n",
      "[SoftImpute] Iter 289: observed MAE=0.003710 validation MAE=0.017105,rank=9\n",
      "[SoftImpute] Iter 290: observed MAE=0.003709 validation MAE=0.017094,rank=9\n",
      "[SoftImpute] Iter 291: observed MAE=0.003708 validation MAE=0.017083,rank=9\n",
      "[SoftImpute] Iter 292: observed MAE=0.003707 validation MAE=0.017072,rank=9\n",
      "[SoftImpute] Iter 293: observed MAE=0.003706 validation MAE=0.017061,rank=9\n",
      "[SoftImpute] Iter 294: observed MAE=0.003705 validation MAE=0.017051,rank=9\n",
      "[SoftImpute] Iter 295: observed MAE=0.003704 validation MAE=0.017040,rank=9\n",
      "[SoftImpute] Iter 296: observed MAE=0.003703 validation MAE=0.017029,rank=9\n",
      "[SoftImpute] Iter 297: observed MAE=0.003701 validation MAE=0.017019,rank=9\n",
      "[SoftImpute] Iter 298: observed MAE=0.003700 validation MAE=0.017008,rank=9\n",
      "[SoftImpute] Iter 299: observed MAE=0.003699 validation MAE=0.016998,rank=9\n",
      "[SoftImpute] Iter 300: observed MAE=0.003698 validation MAE=0.016988,rank=9\n",
      "[SoftImpute] Iter 301: observed MAE=0.003697 validation MAE=0.016977,rank=9\n",
      "[SoftImpute] Iter 302: observed MAE=0.003696 validation MAE=0.016967,rank=9\n",
      "[SoftImpute] Iter 303: observed MAE=0.003695 validation MAE=0.016957,rank=9\n",
      "[SoftImpute] Iter 304: observed MAE=0.003694 validation MAE=0.016947,rank=9\n",
      "[SoftImpute] Iter 305: observed MAE=0.003693 validation MAE=0.016937,rank=9\n",
      "[SoftImpute] Iter 306: observed MAE=0.003692 validation MAE=0.016927,rank=9\n",
      "[SoftImpute] Iter 307: observed MAE=0.003691 validation MAE=0.016917,rank=9\n",
      "[SoftImpute] Iter 308: observed MAE=0.003690 validation MAE=0.016907,rank=9\n",
      "[SoftImpute] Iter 309: observed MAE=0.003690 validation MAE=0.016897,rank=9\n",
      "[SoftImpute] Iter 310: observed MAE=0.003689 validation MAE=0.016887,rank=9\n",
      "[SoftImpute] Iter 311: observed MAE=0.003688 validation MAE=0.016877,rank=9\n",
      "[SoftImpute] Iter 312: observed MAE=0.003687 validation MAE=0.016867,rank=9\n",
      "[SoftImpute] Iter 313: observed MAE=0.003686 validation MAE=0.016858,rank=9\n",
      "[SoftImpute] Iter 314: observed MAE=0.003685 validation MAE=0.016848,rank=9\n",
      "[SoftImpute] Iter 315: observed MAE=0.003684 validation MAE=0.016838,rank=9\n",
      "[SoftImpute] Iter 316: observed MAE=0.003683 validation MAE=0.016829,rank=9\n",
      "[SoftImpute] Iter 317: observed MAE=0.003682 validation MAE=0.016820,rank=9\n",
      "[SoftImpute] Iter 318: observed MAE=0.003681 validation MAE=0.016811,rank=9\n",
      "[SoftImpute] Iter 319: observed MAE=0.003680 validation MAE=0.016801,rank=9\n",
      "[SoftImpute] Iter 320: observed MAE=0.003679 validation MAE=0.016792,rank=9\n",
      "[SoftImpute] Iter 321: observed MAE=0.003678 validation MAE=0.016783,rank=9\n",
      "[SoftImpute] Iter 322: observed MAE=0.003677 validation MAE=0.016774,rank=9\n",
      "[SoftImpute] Iter 323: observed MAE=0.003676 validation MAE=0.016765,rank=9\n",
      "[SoftImpute] Iter 324: observed MAE=0.003675 validation MAE=0.016756,rank=9\n",
      "[SoftImpute] Iter 325: observed MAE=0.003675 validation MAE=0.016747,rank=9\n",
      "[SoftImpute] Iter 326: observed MAE=0.003674 validation MAE=0.016738,rank=9\n",
      "[SoftImpute] Iter 327: observed MAE=0.003673 validation MAE=0.016730,rank=9\n",
      "[SoftImpute] Iter 328: observed MAE=0.003672 validation MAE=0.016721,rank=9\n",
      "[SoftImpute] Iter 329: observed MAE=0.003671 validation MAE=0.016712,rank=9\n",
      "[SoftImpute] Iter 330: observed MAE=0.003670 validation MAE=0.016703,rank=9\n",
      "[SoftImpute] Iter 331: observed MAE=0.003669 validation MAE=0.016694,rank=9\n",
      "[SoftImpute] Iter 332: observed MAE=0.003668 validation MAE=0.016686,rank=9\n",
      "[SoftImpute] Iter 333: observed MAE=0.003667 validation MAE=0.016677,rank=9\n",
      "[SoftImpute] Iter 334: observed MAE=0.003667 validation MAE=0.016668,rank=9\n",
      "[SoftImpute] Iter 335: observed MAE=0.003666 validation MAE=0.016660,rank=9\n",
      "[SoftImpute] Iter 336: observed MAE=0.003665 validation MAE=0.016651,rank=9\n",
      "[SoftImpute] Iter 337: observed MAE=0.003664 validation MAE=0.016643,rank=9\n",
      "[SoftImpute] Iter 338: observed MAE=0.003663 validation MAE=0.016635,rank=9\n",
      "[SoftImpute] Stopped after iteration 338 for lambda=0.039366\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 9.954558610916138\n",
      "After the matrix factor stage, training error is 0.00366, validation error is 0.01663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\gammli\\DataReader.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.34448, val loss: 0.34765\n",
      "Main effects training epoch: 2, train loss: 0.27354, val loss: 0.27853\n",
      "Main effects training epoch: 3, train loss: 0.20823, val loss: 0.21145\n",
      "Main effects training epoch: 4, train loss: 0.16135, val loss: 0.16514\n",
      "Main effects training epoch: 5, train loss: 0.14048, val loss: 0.14025\n",
      "Main effects training epoch: 6, train loss: 0.13280, val loss: 0.13280\n",
      "Main effects training epoch: 7, train loss: 0.13041, val loss: 0.12922\n",
      "Main effects training epoch: 8, train loss: 0.12977, val loss: 0.12861\n",
      "Main effects training epoch: 9, train loss: 0.12914, val loss: 0.12821\n",
      "Main effects training epoch: 10, train loss: 0.12852, val loss: 0.12844\n",
      "Main effects training epoch: 11, train loss: 0.12742, val loss: 0.12590\n",
      "Main effects training epoch: 12, train loss: 0.12612, val loss: 0.12564\n",
      "Main effects training epoch: 13, train loss: 0.12339, val loss: 0.12252\n",
      "Main effects training epoch: 14, train loss: 0.11835, val loss: 0.11883\n",
      "Main effects training epoch: 15, train loss: 0.11578, val loss: 0.11651\n",
      "Main effects training epoch: 16, train loss: 0.11289, val loss: 0.11344\n",
      "Main effects training epoch: 17, train loss: 0.11081, val loss: 0.11186\n",
      "Main effects training epoch: 18, train loss: 0.11088, val loss: 0.11312\n",
      "Main effects training epoch: 19, train loss: 0.11027, val loss: 0.11199\n",
      "Main effects training epoch: 20, train loss: 0.11090, val loss: 0.11238\n",
      "Main effects training epoch: 21, train loss: 0.10922, val loss: 0.11096\n",
      "Main effects training epoch: 22, train loss: 0.10900, val loss: 0.11106\n",
      "Main effects training epoch: 23, train loss: 0.10645, val loss: 0.10812\n",
      "Main effects training epoch: 24, train loss: 0.10707, val loss: 0.10858\n",
      "Main effects training epoch: 25, train loss: 0.10776, val loss: 0.10768\n",
      "Main effects training epoch: 26, train loss: 0.10556, val loss: 0.10715\n",
      "Main effects training epoch: 27, train loss: 0.10595, val loss: 0.10680\n",
      "Main effects training epoch: 28, train loss: 0.10509, val loss: 0.10619\n",
      "Main effects training epoch: 29, train loss: 0.10460, val loss: 0.10568\n",
      "Main effects training epoch: 30, train loss: 0.10512, val loss: 0.10585\n",
      "Main effects training epoch: 31, train loss: 0.10437, val loss: 0.10598\n",
      "Main effects training epoch: 32, train loss: 0.10431, val loss: 0.10577\n",
      "Main effects training epoch: 33, train loss: 0.10423, val loss: 0.10545\n",
      "Main effects training epoch: 34, train loss: 0.10413, val loss: 0.10561\n",
      "Main effects training epoch: 35, train loss: 0.10424, val loss: 0.10579\n",
      "Main effects training epoch: 36, train loss: 0.10444, val loss: 0.10578\n",
      "Main effects training epoch: 37, train loss: 0.10400, val loss: 0.10575\n",
      "Main effects training epoch: 38, train loss: 0.10438, val loss: 0.10591\n",
      "Main effects training epoch: 39, train loss: 0.10416, val loss: 0.10566\n",
      "Main effects training epoch: 40, train loss: 0.10393, val loss: 0.10524\n",
      "Main effects training epoch: 41, train loss: 0.10418, val loss: 0.10579\n",
      "Main effects training epoch: 42, train loss: 0.10392, val loss: 0.10565\n",
      "Main effects training epoch: 43, train loss: 0.10430, val loss: 0.10612\n",
      "Main effects training epoch: 44, train loss: 0.10409, val loss: 0.10553\n",
      "Main effects training epoch: 45, train loss: 0.10403, val loss: 0.10539\n",
      "Main effects training epoch: 46, train loss: 0.10400, val loss: 0.10547\n",
      "Main effects training epoch: 47, train loss: 0.10472, val loss: 0.10625\n",
      "Main effects training epoch: 48, train loss: 0.10410, val loss: 0.10597\n",
      "Main effects training epoch: 49, train loss: 0.10415, val loss: 0.10512\n",
      "Main effects training epoch: 50, train loss: 0.10408, val loss: 0.10560\n",
      "Main effects training epoch: 51, train loss: 0.10392, val loss: 0.10554\n",
      "Main effects training epoch: 52, train loss: 0.10395, val loss: 0.10519\n",
      "Main effects training epoch: 53, train loss: 0.10385, val loss: 0.10520\n",
      "Main effects training epoch: 54, train loss: 0.10398, val loss: 0.10563\n",
      "Main effects training epoch: 55, train loss: 0.10416, val loss: 0.10523\n",
      "Main effects training epoch: 56, train loss: 0.10404, val loss: 0.10586\n",
      "Main effects training epoch: 57, train loss: 0.10399, val loss: 0.10571\n",
      "Main effects training epoch: 58, train loss: 0.10443, val loss: 0.10631\n",
      "Main effects training epoch: 59, train loss: 0.10393, val loss: 0.10549\n",
      "Main effects training epoch: 60, train loss: 0.10386, val loss: 0.10534\n",
      "Main effects training epoch: 61, train loss: 0.10389, val loss: 0.10538\n",
      "Main effects training epoch: 62, train loss: 0.10417, val loss: 0.10552\n",
      "Main effects training epoch: 63, train loss: 0.10396, val loss: 0.10570\n",
      "Main effects training epoch: 64, train loss: 0.10412, val loss: 0.10546\n",
      "Main effects training epoch: 65, train loss: 0.10403, val loss: 0.10549\n",
      "Main effects training epoch: 66, train loss: 0.10398, val loss: 0.10514\n",
      "Main effects training epoch: 67, train loss: 0.10385, val loss: 0.10514\n",
      "Main effects training epoch: 68, train loss: 0.10396, val loss: 0.10561\n",
      "Main effects training epoch: 69, train loss: 0.10397, val loss: 0.10523\n",
      "Main effects training epoch: 70, train loss: 0.10420, val loss: 0.10542\n",
      "Main effects training epoch: 71, train loss: 0.10473, val loss: 0.10671\n",
      "Main effects training epoch: 72, train loss: 0.10418, val loss: 0.10552\n",
      "Main effects training epoch: 73, train loss: 0.10427, val loss: 0.10590\n",
      "Main effects training epoch: 74, train loss: 0.10409, val loss: 0.10540\n",
      "Main effects training epoch: 75, train loss: 0.10401, val loss: 0.10542\n",
      "Main effects training epoch: 76, train loss: 0.10384, val loss: 0.10560\n",
      "Main effects training epoch: 77, train loss: 0.10404, val loss: 0.10542\n",
      "Main effects training epoch: 78, train loss: 0.10394, val loss: 0.10550\n",
      "Main effects training epoch: 79, train loss: 0.10410, val loss: 0.10553\n",
      "Main effects training epoch: 80, train loss: 0.10425, val loss: 0.10542\n",
      "Main effects training epoch: 81, train loss: 0.10387, val loss: 0.10530\n",
      "Main effects training epoch: 82, train loss: 0.10409, val loss: 0.10607\n",
      "Main effects training epoch: 83, train loss: 0.10386, val loss: 0.10559\n",
      "Main effects training epoch: 84, train loss: 0.10405, val loss: 0.10549\n",
      "Main effects training epoch: 85, train loss: 0.10483, val loss: 0.10617\n",
      "Main effects training epoch: 86, train loss: 0.10383, val loss: 0.10551\n",
      "Main effects training epoch: 87, train loss: 0.10415, val loss: 0.10524\n",
      "Main effects training epoch: 88, train loss: 0.10389, val loss: 0.10586\n",
      "Main effects training epoch: 89, train loss: 0.10383, val loss: 0.10548\n",
      "Main effects training epoch: 90, train loss: 0.10400, val loss: 0.10512\n",
      "Main effects training epoch: 91, train loss: 0.10387, val loss: 0.10538\n",
      "Main effects training epoch: 92, train loss: 0.10420, val loss: 0.10635\n",
      "Main effects training epoch: 93, train loss: 0.10396, val loss: 0.10529\n",
      "Main effects training epoch: 94, train loss: 0.10417, val loss: 0.10594\n",
      "Main effects training epoch: 95, train loss: 0.10387, val loss: 0.10562\n",
      "Main effects training epoch: 96, train loss: 0.10378, val loss: 0.10533\n",
      "Main effects training epoch: 97, train loss: 0.10386, val loss: 0.10536\n",
      "Main effects training epoch: 98, train loss: 0.10425, val loss: 0.10630\n",
      "Main effects training epoch: 99, train loss: 0.10426, val loss: 0.10552\n",
      "Main effects training epoch: 100, train loss: 0.10419, val loss: 0.10632\n",
      "Main effects training epoch: 101, train loss: 0.10395, val loss: 0.10553\n",
      "Main effects training epoch: 102, train loss: 0.10397, val loss: 0.10584\n",
      "Main effects training epoch: 103, train loss: 0.10408, val loss: 0.10574\n",
      "Main effects training epoch: 104, train loss: 0.10395, val loss: 0.10576\n",
      "Main effects training epoch: 105, train loss: 0.10380, val loss: 0.10555\n",
      "Main effects training epoch: 106, train loss: 0.10382, val loss: 0.10590\n",
      "Main effects training epoch: 107, train loss: 0.10417, val loss: 0.10600\n",
      "Main effects training epoch: 108, train loss: 0.10410, val loss: 0.10567\n",
      "Main effects training epoch: 109, train loss: 0.10373, val loss: 0.10542\n",
      "Main effects training epoch: 110, train loss: 0.10377, val loss: 0.10525\n",
      "Main effects training epoch: 111, train loss: 0.10405, val loss: 0.10577\n",
      "Main effects training epoch: 112, train loss: 0.10409, val loss: 0.10590\n",
      "Main effects training epoch: 113, train loss: 0.10474, val loss: 0.10615\n",
      "Main effects training epoch: 114, train loss: 0.10406, val loss: 0.10594\n",
      "Main effects training epoch: 115, train loss: 0.10415, val loss: 0.10553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 116, train loss: 0.10437, val loss: 0.10680\n",
      "Main effects training epoch: 117, train loss: 0.10399, val loss: 0.10532\n",
      "Main effects training epoch: 118, train loss: 0.10393, val loss: 0.10583\n",
      "Main effects training epoch: 119, train loss: 0.10410, val loss: 0.10601\n",
      "Main effects training epoch: 120, train loss: 0.10375, val loss: 0.10578\n",
      "Main effects training epoch: 121, train loss: 0.10382, val loss: 0.10529\n",
      "Main effects training epoch: 122, train loss: 0.10391, val loss: 0.10552\n",
      "Main effects training epoch: 123, train loss: 0.10397, val loss: 0.10615\n",
      "Main effects training epoch: 124, train loss: 0.10422, val loss: 0.10572\n",
      "Main effects training epoch: 125, train loss: 0.10414, val loss: 0.10601\n",
      "Main effects training epoch: 126, train loss: 0.10402, val loss: 0.10605\n",
      "Main effects training epoch: 127, train loss: 0.10402, val loss: 0.10531\n",
      "Main effects training epoch: 128, train loss: 0.10395, val loss: 0.10615\n",
      "Main effects training epoch: 129, train loss: 0.10410, val loss: 0.10643\n",
      "Main effects training epoch: 130, train loss: 0.10401, val loss: 0.10526\n",
      "Main effects training epoch: 131, train loss: 0.10380, val loss: 0.10577\n",
      "Main effects training epoch: 132, train loss: 0.10404, val loss: 0.10554\n",
      "Main effects training epoch: 133, train loss: 0.10425, val loss: 0.10648\n",
      "Main effects training epoch: 134, train loss: 0.10454, val loss: 0.10616\n",
      "Main effects training epoch: 135, train loss: 0.10424, val loss: 0.10653\n",
      "Main effects training epoch: 136, train loss: 0.10426, val loss: 0.10565\n",
      "Main effects training epoch: 137, train loss: 0.10444, val loss: 0.10639\n",
      "Main effects training epoch: 138, train loss: 0.10433, val loss: 0.10622\n",
      "Main effects training epoch: 139, train loss: 0.10384, val loss: 0.10565\n",
      "Main effects training epoch: 140, train loss: 0.10392, val loss: 0.10539\n",
      "Main effects training epoch: 141, train loss: 0.10420, val loss: 0.10593\n",
      "Main effects training epoch: 142, train loss: 0.10404, val loss: 0.10617\n",
      "Main effects training epoch: 143, train loss: 0.10420, val loss: 0.10567\n",
      "Main effects training epoch: 144, train loss: 0.10383, val loss: 0.10601\n",
      "Main effects training epoch: 145, train loss: 0.10370, val loss: 0.10530\n",
      "Main effects training epoch: 146, train loss: 0.10381, val loss: 0.10596\n",
      "Main effects training epoch: 147, train loss: 0.10380, val loss: 0.10575\n",
      "Main effects training epoch: 148, train loss: 0.10377, val loss: 0.10571\n",
      "Main effects training epoch: 149, train loss: 0.10383, val loss: 0.10566\n",
      "Main effects training epoch: 150, train loss: 0.10393, val loss: 0.10604\n",
      "Early stop at epoch 150, with validation loss: 0.10604\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.10405, val loss: 0.10498\n",
      "Main effects tuning epoch: 2, train loss: 0.10423, val loss: 0.10552\n",
      "Main effects tuning epoch: 3, train loss: 0.10411, val loss: 0.10505\n",
      "Main effects tuning epoch: 4, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 5, train loss: 0.10382, val loss: 0.10498\n",
      "Main effects tuning epoch: 6, train loss: 0.10425, val loss: 0.10582\n",
      "Main effects tuning epoch: 7, train loss: 0.10408, val loss: 0.10530\n",
      "Main effects tuning epoch: 8, train loss: 0.10397, val loss: 0.10527\n",
      "Main effects tuning epoch: 9, train loss: 0.10401, val loss: 0.10507\n",
      "Main effects tuning epoch: 10, train loss: 0.10407, val loss: 0.10539\n",
      "Main effects tuning epoch: 11, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 12, train loss: 0.10393, val loss: 0.10556\n",
      "Main effects tuning epoch: 13, train loss: 0.10415, val loss: 0.10510\n",
      "Main effects tuning epoch: 14, train loss: 0.10397, val loss: 0.10570\n",
      "Main effects tuning epoch: 15, train loss: 0.10386, val loss: 0.10516\n",
      "Main effects tuning epoch: 16, train loss: 0.10395, val loss: 0.10562\n",
      "Main effects tuning epoch: 17, train loss: 0.10395, val loss: 0.10539\n",
      "Main effects tuning epoch: 18, train loss: 0.10422, val loss: 0.10568\n",
      "Main effects tuning epoch: 19, train loss: 0.10433, val loss: 0.10517\n",
      "Main effects tuning epoch: 20, train loss: 0.10384, val loss: 0.10568\n",
      "Main effects tuning epoch: 21, train loss: 0.10400, val loss: 0.10541\n",
      "Main effects tuning epoch: 22, train loss: 0.10393, val loss: 0.10526\n",
      "Main effects tuning epoch: 23, train loss: 0.10434, val loss: 0.10582\n",
      "Main effects tuning epoch: 24, train loss: 0.10406, val loss: 0.10540\n",
      "Main effects tuning epoch: 25, train loss: 0.10410, val loss: 0.10570\n",
      "Main effects tuning epoch: 26, train loss: 0.10405, val loss: 0.10569\n",
      "Main effects tuning epoch: 27, train loss: 0.10406, val loss: 0.10556\n",
      "Main effects tuning epoch: 28, train loss: 0.10394, val loss: 0.10542\n",
      "Main effects tuning epoch: 29, train loss: 0.10383, val loss: 0.10542\n",
      "Main effects tuning epoch: 30, train loss: 0.10385, val loss: 0.10548\n",
      "Main effects tuning epoch: 31, train loss: 0.10395, val loss: 0.10516\n",
      "Main effects tuning epoch: 32, train loss: 0.10391, val loss: 0.10584\n",
      "Main effects tuning epoch: 33, train loss: 0.10392, val loss: 0.10548\n",
      "Main effects tuning epoch: 34, train loss: 0.10391, val loss: 0.10561\n",
      "Main effects tuning epoch: 35, train loss: 0.10424, val loss: 0.10531\n",
      "Main effects tuning epoch: 36, train loss: 0.10395, val loss: 0.10554\n",
      "Main effects tuning epoch: 37, train loss: 0.10400, val loss: 0.10600\n",
      "Main effects tuning epoch: 38, train loss: 0.10408, val loss: 0.10530\n",
      "Main effects tuning epoch: 39, train loss: 0.10394, val loss: 0.10565\n",
      "Main effects tuning epoch: 40, train loss: 0.10385, val loss: 0.10569\n",
      "Main effects tuning epoch: 41, train loss: 0.10390, val loss: 0.10539\n",
      "Main effects tuning epoch: 42, train loss: 0.10400, val loss: 0.10587\n",
      "Main effects tuning epoch: 43, train loss: 0.10386, val loss: 0.10547\n",
      "Main effects tuning epoch: 44, train loss: 0.10403, val loss: 0.10597\n",
      "Main effects tuning epoch: 45, train loss: 0.10384, val loss: 0.10541\n",
      "Main effects tuning epoch: 46, train loss: 0.10398, val loss: 0.10582\n",
      "Main effects tuning epoch: 47, train loss: 0.10375, val loss: 0.10527\n",
      "Main effects tuning epoch: 48, train loss: 0.10380, val loss: 0.10549\n",
      "Main effects tuning epoch: 49, train loss: 0.10396, val loss: 0.10566\n",
      "Main effects tuning epoch: 50, train loss: 0.10407, val loss: 0.10549\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.09376, val loss: 0.09232\n",
      "Interaction training epoch: 2, train loss: 0.20140, val loss: 0.20111\n",
      "Interaction training epoch: 3, train loss: 0.06617, val loss: 0.06860\n",
      "Interaction training epoch: 4, train loss: 0.05173, val loss: 0.05197\n",
      "Interaction training epoch: 5, train loss: 0.06497, val loss: 0.06595\n",
      "Interaction training epoch: 6, train loss: 0.04386, val loss: 0.04449\n",
      "Interaction training epoch: 7, train loss: 0.04606, val loss: 0.04529\n",
      "Interaction training epoch: 8, train loss: 0.04492, val loss: 0.04489\n",
      "Interaction training epoch: 9, train loss: 0.04997, val loss: 0.04868\n",
      "Interaction training epoch: 10, train loss: 0.04002, val loss: 0.04133\n",
      "Interaction training epoch: 11, train loss: 0.04269, val loss: 0.04306\n",
      "Interaction training epoch: 12, train loss: 0.04100, val loss: 0.03997\n",
      "Interaction training epoch: 13, train loss: 0.04366, val loss: 0.04360\n",
      "Interaction training epoch: 14, train loss: 0.04928, val loss: 0.05084\n",
      "Interaction training epoch: 15, train loss: 0.04532, val loss: 0.04488\n",
      "Interaction training epoch: 16, train loss: 0.04530, val loss: 0.04524\n",
      "Interaction training epoch: 17, train loss: 0.03784, val loss: 0.03763\n",
      "Interaction training epoch: 18, train loss: 0.04939, val loss: 0.05023\n",
      "Interaction training epoch: 19, train loss: 0.03608, val loss: 0.03597\n",
      "Interaction training epoch: 20, train loss: 0.04344, val loss: 0.04367\n",
      "Interaction training epoch: 21, train loss: 0.04002, val loss: 0.03932\n",
      "Interaction training epoch: 22, train loss: 0.03977, val loss: 0.03977\n",
      "Interaction training epoch: 23, train loss: 0.03777, val loss: 0.03709\n",
      "Interaction training epoch: 24, train loss: 0.03927, val loss: 0.03924\n",
      "Interaction training epoch: 25, train loss: 0.04164, val loss: 0.04112\n",
      "Interaction training epoch: 26, train loss: 0.04429, val loss: 0.04389\n",
      "Interaction training epoch: 27, train loss: 0.04417, val loss: 0.04416\n",
      "Interaction training epoch: 28, train loss: 0.04241, val loss: 0.04112\n",
      "Interaction training epoch: 29, train loss: 0.03548, val loss: 0.03672\n",
      "Interaction training epoch: 30, train loss: 0.04420, val loss: 0.04353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 31, train loss: 0.04020, val loss: 0.04004\n",
      "Interaction training epoch: 32, train loss: 0.03422, val loss: 0.03451\n",
      "Interaction training epoch: 33, train loss: 0.03378, val loss: 0.03407\n",
      "Interaction training epoch: 34, train loss: 0.03925, val loss: 0.03880\n",
      "Interaction training epoch: 35, train loss: 0.03752, val loss: 0.03636\n",
      "Interaction training epoch: 36, train loss: 0.03582, val loss: 0.03564\n",
      "Interaction training epoch: 37, train loss: 0.03069, val loss: 0.03087\n",
      "Interaction training epoch: 38, train loss: 0.03729, val loss: 0.03652\n",
      "Interaction training epoch: 39, train loss: 0.03772, val loss: 0.03772\n",
      "Interaction training epoch: 40, train loss: 0.04332, val loss: 0.04201\n",
      "Interaction training epoch: 41, train loss: 0.03409, val loss: 0.03435\n",
      "Interaction training epoch: 42, train loss: 0.03501, val loss: 0.03419\n",
      "Interaction training epoch: 43, train loss: 0.03630, val loss: 0.03573\n",
      "Interaction training epoch: 44, train loss: 0.03447, val loss: 0.03420\n",
      "Interaction training epoch: 45, train loss: 0.03622, val loss: 0.03541\n",
      "Interaction training epoch: 46, train loss: 0.02991, val loss: 0.03002\n",
      "Interaction training epoch: 47, train loss: 0.02880, val loss: 0.02856\n",
      "Interaction training epoch: 48, train loss: 0.03548, val loss: 0.03469\n",
      "Interaction training epoch: 49, train loss: 0.03399, val loss: 0.03256\n",
      "Interaction training epoch: 50, train loss: 0.03225, val loss: 0.03266\n",
      "Interaction training epoch: 51, train loss: 0.03286, val loss: 0.03176\n",
      "Interaction training epoch: 52, train loss: 0.03084, val loss: 0.03012\n",
      "Interaction training epoch: 53, train loss: 0.03059, val loss: 0.02998\n",
      "Interaction training epoch: 54, train loss: 0.02966, val loss: 0.02932\n",
      "Interaction training epoch: 55, train loss: 0.04122, val loss: 0.04119\n",
      "Interaction training epoch: 56, train loss: 0.03164, val loss: 0.03059\n",
      "Interaction training epoch: 57, train loss: 0.03483, val loss: 0.03469\n",
      "Interaction training epoch: 58, train loss: 0.03320, val loss: 0.03294\n",
      "Interaction training epoch: 59, train loss: 0.02999, val loss: 0.02930\n",
      "Interaction training epoch: 60, train loss: 0.03540, val loss: 0.03496\n",
      "Interaction training epoch: 61, train loss: 0.03142, val loss: 0.02962\n",
      "Interaction training epoch: 62, train loss: 0.03146, val loss: 0.03164\n",
      "Interaction training epoch: 63, train loss: 0.03481, val loss: 0.03421\n",
      "Interaction training epoch: 64, train loss: 0.02946, val loss: 0.02881\n",
      "Interaction training epoch: 65, train loss: 0.03539, val loss: 0.03511\n",
      "Interaction training epoch: 66, train loss: 0.03797, val loss: 0.03805\n",
      "Interaction training epoch: 67, train loss: 0.03669, val loss: 0.03627\n",
      "Interaction training epoch: 68, train loss: 0.04844, val loss: 0.04881\n",
      "Interaction training epoch: 69, train loss: 0.02947, val loss: 0.02941\n",
      "Interaction training epoch: 70, train loss: 0.03805, val loss: 0.03672\n",
      "Interaction training epoch: 71, train loss: 0.02990, val loss: 0.02970\n",
      "Interaction training epoch: 72, train loss: 0.03926, val loss: 0.03952\n",
      "Interaction training epoch: 73, train loss: 0.03713, val loss: 0.03732\n",
      "Interaction training epoch: 74, train loss: 0.04469, val loss: 0.04392\n",
      "Interaction training epoch: 75, train loss: 0.03518, val loss: 0.03491\n",
      "Interaction training epoch: 76, train loss: 0.03954, val loss: 0.03893\n",
      "Interaction training epoch: 77, train loss: 0.04294, val loss: 0.04248\n",
      "Interaction training epoch: 78, train loss: 0.03122, val loss: 0.03170\n",
      "Interaction training epoch: 79, train loss: 0.03577, val loss: 0.03538\n",
      "Interaction training epoch: 80, train loss: 0.04727, val loss: 0.04664\n",
      "Interaction training epoch: 81, train loss: 0.03478, val loss: 0.03412\n",
      "Interaction training epoch: 82, train loss: 0.03736, val loss: 0.03700\n",
      "Interaction training epoch: 83, train loss: 0.03716, val loss: 0.03665\n",
      "Interaction training epoch: 84, train loss: 0.03675, val loss: 0.03575\n",
      "Interaction training epoch: 85, train loss: 0.03815, val loss: 0.03817\n",
      "Interaction training epoch: 86, train loss: 0.03960, val loss: 0.03903\n",
      "Interaction training epoch: 87, train loss: 0.03498, val loss: 0.03402\n",
      "Interaction training epoch: 88, train loss: 0.03630, val loss: 0.03583\n",
      "Interaction training epoch: 89, train loss: 0.03454, val loss: 0.03517\n",
      "Interaction training epoch: 90, train loss: 0.07231, val loss: 0.07223\n",
      "Interaction training epoch: 91, train loss: 0.04800, val loss: 0.04802\n",
      "Interaction training epoch: 92, train loss: 0.05224, val loss: 0.05162\n",
      "Interaction training epoch: 93, train loss: 0.02794, val loss: 0.02804\n",
      "Interaction training epoch: 94, train loss: 0.03585, val loss: 0.03546\n",
      "Interaction training epoch: 95, train loss: 0.04149, val loss: 0.04158\n",
      "Interaction training epoch: 96, train loss: 0.04943, val loss: 0.04909\n",
      "Interaction training epoch: 97, train loss: 0.05722, val loss: 0.05690\n",
      "Interaction training epoch: 98, train loss: 0.04854, val loss: 0.04831\n",
      "Interaction training epoch: 99, train loss: 0.02719, val loss: 0.02716\n",
      "Interaction training epoch: 100, train loss: 0.03689, val loss: 0.03722\n",
      "Interaction training epoch: 101, train loss: 0.04609, val loss: 0.04601\n",
      "Interaction training epoch: 102, train loss: 0.03250, val loss: 0.03231\n",
      "Interaction training epoch: 103, train loss: 0.03006, val loss: 0.02936\n",
      "Interaction training epoch: 104, train loss: 0.03967, val loss: 0.03937\n",
      "Interaction training epoch: 105, train loss: 0.03096, val loss: 0.03100\n",
      "Interaction training epoch: 106, train loss: 0.05099, val loss: 0.05044\n",
      "Interaction training epoch: 107, train loss: 0.03358, val loss: 0.03389\n",
      "Interaction training epoch: 108, train loss: 0.04273, val loss: 0.04236\n",
      "Interaction training epoch: 109, train loss: 0.07259, val loss: 0.07140\n",
      "Interaction training epoch: 110, train loss: 0.03442, val loss: 0.03433\n",
      "Interaction training epoch: 111, train loss: 0.02947, val loss: 0.02893\n",
      "Interaction training epoch: 112, train loss: 0.03413, val loss: 0.03396\n",
      "Interaction training epoch: 113, train loss: 0.03441, val loss: 0.03428\n",
      "Interaction training epoch: 114, train loss: 0.05762, val loss: 0.05645\n",
      "Interaction training epoch: 115, train loss: 0.02938, val loss: 0.02918\n",
      "Interaction training epoch: 116, train loss: 0.04805, val loss: 0.04824\n",
      "Interaction training epoch: 117, train loss: 0.03927, val loss: 0.03835\n",
      "Interaction training epoch: 118, train loss: 0.03949, val loss: 0.03920\n",
      "Interaction training epoch: 119, train loss: 0.03835, val loss: 0.03852\n",
      "Interaction training epoch: 120, train loss: 0.07402, val loss: 0.07267\n",
      "Interaction training epoch: 121, train loss: 0.02786, val loss: 0.02794\n",
      "Interaction training epoch: 122, train loss: 0.02966, val loss: 0.02840\n",
      "Interaction training epoch: 123, train loss: 0.03146, val loss: 0.03080\n",
      "Interaction training epoch: 124, train loss: 0.03135, val loss: 0.03095\n",
      "Interaction training epoch: 125, train loss: 0.02893, val loss: 0.02832\n",
      "Interaction training epoch: 126, train loss: 0.06644, val loss: 0.06554\n",
      "Interaction training epoch: 127, train loss: 0.02439, val loss: 0.02405\n",
      "Interaction training epoch: 128, train loss: 0.02442, val loss: 0.02392\n",
      "Interaction training epoch: 129, train loss: 0.06718, val loss: 0.06662\n",
      "Interaction training epoch: 130, train loss: 0.06873, val loss: 0.06752\n",
      "Interaction training epoch: 131, train loss: 0.02836, val loss: 0.02795\n",
      "Interaction training epoch: 132, train loss: 0.02537, val loss: 0.02462\n",
      "Interaction training epoch: 133, train loss: 0.05578, val loss: 0.05497\n",
      "Interaction training epoch: 134, train loss: 0.02756, val loss: 0.02628\n",
      "Interaction training epoch: 135, train loss: 0.03851, val loss: 0.03792\n",
      "Interaction training epoch: 136, train loss: 0.02792, val loss: 0.02763\n",
      "Interaction training epoch: 137, train loss: 0.02882, val loss: 0.02822\n",
      "Interaction training epoch: 138, train loss: 0.04268, val loss: 0.04230\n",
      "Interaction training epoch: 139, train loss: 0.02999, val loss: 0.03028\n",
      "Interaction training epoch: 140, train loss: 0.02653, val loss: 0.02598\n",
      "Interaction training epoch: 141, train loss: 0.02864, val loss: 0.02740\n",
      "Interaction training epoch: 142, train loss: 0.04031, val loss: 0.03930\n",
      "Interaction training epoch: 143, train loss: 0.08930, val loss: 0.08825\n",
      "Interaction training epoch: 144, train loss: 0.03278, val loss: 0.03201\n",
      "Interaction training epoch: 145, train loss: 0.07595, val loss: 0.07444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 146, train loss: 0.04088, val loss: 0.04007\n",
      "Interaction training epoch: 147, train loss: 0.03063, val loss: 0.02967\n",
      "Interaction training epoch: 148, train loss: 0.08210, val loss: 0.08116\n",
      "Interaction training epoch: 149, train loss: 0.02600, val loss: 0.02595\n",
      "Interaction training epoch: 150, train loss: 0.04451, val loss: 0.04266\n",
      "Interaction training epoch: 151, train loss: 0.05071, val loss: 0.04953\n",
      "Interaction training epoch: 152, train loss: 0.03689, val loss: 0.03657\n",
      "Interaction training epoch: 153, train loss: 0.05108, val loss: 0.05017\n",
      "Interaction training epoch: 154, train loss: 0.03125, val loss: 0.03020\n",
      "Interaction training epoch: 155, train loss: 0.05262, val loss: 0.05160\n",
      "Interaction training epoch: 156, train loss: 0.03189, val loss: 0.03099\n",
      "Interaction training epoch: 157, train loss: 0.03302, val loss: 0.03192\n",
      "Interaction training epoch: 158, train loss: 0.03919, val loss: 0.03867\n",
      "Interaction training epoch: 159, train loss: 0.06294, val loss: 0.06164\n",
      "Interaction training epoch: 160, train loss: 0.02504, val loss: 0.02398\n",
      "Interaction training epoch: 161, train loss: 0.02726, val loss: 0.02622\n",
      "Interaction training epoch: 162, train loss: 0.02696, val loss: 0.02663\n",
      "Interaction training epoch: 163, train loss: 0.04443, val loss: 0.04306\n",
      "Interaction training epoch: 164, train loss: 0.08229, val loss: 0.08080\n",
      "Interaction training epoch: 165, train loss: 0.02497, val loss: 0.02433\n",
      "Interaction training epoch: 166, train loss: 0.02405, val loss: 0.02321\n",
      "Interaction training epoch: 167, train loss: 0.03154, val loss: 0.03087\n",
      "Interaction training epoch: 168, train loss: 0.03079, val loss: 0.02948\n",
      "Interaction training epoch: 169, train loss: 0.02562, val loss: 0.02466\n",
      "Interaction training epoch: 170, train loss: 0.03950, val loss: 0.03811\n",
      "Interaction training epoch: 171, train loss: 0.05102, val loss: 0.05005\n",
      "Interaction training epoch: 172, train loss: 0.07393, val loss: 0.07199\n",
      "Interaction training epoch: 173, train loss: 0.03775, val loss: 0.03659\n",
      "Interaction training epoch: 174, train loss: 0.03283, val loss: 0.03172\n",
      "Interaction training epoch: 175, train loss: 0.02729, val loss: 0.02663\n",
      "Interaction training epoch: 176, train loss: 0.04249, val loss: 0.04090\n",
      "Interaction training epoch: 177, train loss: 0.11371, val loss: 0.11248\n",
      "Interaction training epoch: 178, train loss: 0.05008, val loss: 0.04851\n",
      "Interaction training epoch: 179, train loss: 0.05070, val loss: 0.04903\n",
      "Interaction training epoch: 180, train loss: 0.05192, val loss: 0.05082\n",
      "Interaction training epoch: 181, train loss: 0.03499, val loss: 0.03401\n",
      "Interaction training epoch: 182, train loss: 0.06009, val loss: 0.05850\n",
      "Interaction training epoch: 183, train loss: 0.03517, val loss: 0.03374\n",
      "Interaction training epoch: 184, train loss: 0.04107, val loss: 0.04010\n",
      "Interaction training epoch: 185, train loss: 0.06361, val loss: 0.06162\n",
      "Interaction training epoch: 186, train loss: 0.06349, val loss: 0.06178\n",
      "Interaction training epoch: 187, train loss: 0.05374, val loss: 0.05196\n",
      "Interaction training epoch: 188, train loss: 0.05958, val loss: 0.05804\n",
      "Interaction training epoch: 189, train loss: 0.02604, val loss: 0.02519\n",
      "Interaction training epoch: 190, train loss: 0.04369, val loss: 0.04302\n",
      "Interaction training epoch: 191, train loss: 0.04175, val loss: 0.04116\n",
      "Interaction training epoch: 192, train loss: 0.06782, val loss: 0.06645\n",
      "Interaction training epoch: 193, train loss: 0.06145, val loss: 0.06034\n",
      "Interaction training epoch: 194, train loss: 0.04303, val loss: 0.04151\n",
      "Interaction training epoch: 195, train loss: 0.03319, val loss: 0.03229\n",
      "Interaction training epoch: 196, train loss: 0.03143, val loss: 0.02941\n",
      "Interaction training epoch: 197, train loss: 0.03413, val loss: 0.03311\n",
      "Interaction training epoch: 198, train loss: 0.03045, val loss: 0.02901\n",
      "Interaction training epoch: 199, train loss: 0.04253, val loss: 0.04138\n",
      "Interaction training epoch: 200, train loss: 0.04032, val loss: 0.03845\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########4 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.02853, val loss: 0.02785\n",
      "Interaction tuning epoch: 2, train loss: 0.02401, val loss: 0.02299\n",
      "Interaction tuning epoch: 3, train loss: 0.04478, val loss: 0.04265\n",
      "Interaction tuning epoch: 4, train loss: 0.06395, val loss: 0.06292\n",
      "Interaction tuning epoch: 5, train loss: 0.02356, val loss: 0.02286\n",
      "Interaction tuning epoch: 6, train loss: 0.06369, val loss: 0.06228\n",
      "Interaction tuning epoch: 7, train loss: 0.04154, val loss: 0.04077\n",
      "Interaction tuning epoch: 8, train loss: 0.08354, val loss: 0.08227\n",
      "Interaction tuning epoch: 9, train loss: 0.07421, val loss: 0.07270\n",
      "Interaction tuning epoch: 10, train loss: 0.04015, val loss: 0.03931\n",
      "Interaction tuning epoch: 11, train loss: 0.02840, val loss: 0.02746\n",
      "Interaction tuning epoch: 12, train loss: 0.06172, val loss: 0.06021\n",
      "Interaction tuning epoch: 13, train loss: 0.03776, val loss: 0.03699\n",
      "Interaction tuning epoch: 14, train loss: 0.07388, val loss: 0.07213\n",
      "Interaction tuning epoch: 15, train loss: 0.02628, val loss: 0.02557\n",
      "Interaction tuning epoch: 16, train loss: 0.02860, val loss: 0.02813\n",
      "Interaction tuning epoch: 17, train loss: 0.04973, val loss: 0.04802\n",
      "Interaction tuning epoch: 18, train loss: 0.02869, val loss: 0.02741\n",
      "Interaction tuning epoch: 19, train loss: 0.03914, val loss: 0.03730\n",
      "Interaction tuning epoch: 20, train loss: 0.03596, val loss: 0.03526\n",
      "Interaction tuning epoch: 21, train loss: 0.04924, val loss: 0.04797\n",
      "Interaction tuning epoch: 22, train loss: 0.08235, val loss: 0.08148\n",
      "Interaction tuning epoch: 23, train loss: 0.08430, val loss: 0.08186\n",
      "Interaction tuning epoch: 24, train loss: 0.07758, val loss: 0.07581\n",
      "Interaction tuning epoch: 25, train loss: 0.02468, val loss: 0.02425\n",
      "Interaction tuning epoch: 26, train loss: 0.02729, val loss: 0.02674\n",
      "Interaction tuning epoch: 27, train loss: 0.02677, val loss: 0.02542\n",
      "Interaction tuning epoch: 28, train loss: 0.03303, val loss: 0.03233\n",
      "Interaction tuning epoch: 29, train loss: 0.07628, val loss: 0.07409\n",
      "Interaction tuning epoch: 30, train loss: 0.05354, val loss: 0.05248\n",
      "Interaction tuning epoch: 31, train loss: 0.02819, val loss: 0.02642\n",
      "Interaction tuning epoch: 32, train loss: 0.02994, val loss: 0.02905\n",
      "Interaction tuning epoch: 33, train loss: 0.03903, val loss: 0.03809\n",
      "Interaction tuning epoch: 34, train loss: 0.03406, val loss: 0.03261\n",
      "Interaction tuning epoch: 35, train loss: 0.02886, val loss: 0.02797\n",
      "Interaction tuning epoch: 36, train loss: 0.04728, val loss: 0.04591\n",
      "Interaction tuning epoch: 37, train loss: 0.02745, val loss: 0.02653\n",
      "Interaction tuning epoch: 38, train loss: 0.09613, val loss: 0.09462\n",
      "Interaction tuning epoch: 39, train loss: 0.06430, val loss: 0.06326\n",
      "Interaction tuning epoch: 40, train loss: 0.03564, val loss: 0.03467\n",
      "Interaction tuning epoch: 41, train loss: 0.05842, val loss: 0.05720\n",
      "Interaction tuning epoch: 42, train loss: 0.02719, val loss: 0.02601\n",
      "Interaction tuning epoch: 43, train loss: 0.03689, val loss: 0.03533\n",
      "Interaction tuning epoch: 44, train loss: 0.02563, val loss: 0.02452\n",
      "Interaction tuning epoch: 45, train loss: 0.11515, val loss: 0.11336\n",
      "Interaction tuning epoch: 46, train loss: 0.05383, val loss: 0.05274\n",
      "Interaction tuning epoch: 47, train loss: 0.03183, val loss: 0.02986\n",
      "Interaction tuning epoch: 48, train loss: 0.04755, val loss: 0.04566\n",
      "Interaction tuning epoch: 49, train loss: 0.03504, val loss: 0.03437\n",
      "Interaction tuning epoch: 50, train loss: 0.03646, val loss: 0.03600\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 40.71100354194641\n",
      "After the gam stage, training error is 0.03646 , validation error is 0.03600\n",
      "missing value counts: 92783\n",
      "[SoftImpute] Max Singular Value of X_init = 1.968308\n",
      "#####mf_training#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed MAE=0.023506 validation MAE=0.034242,rank=10\n",
      "[SoftImpute] Iter 2: observed MAE=0.021047 validation MAE=0.033399,rank=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 3: observed MAE=0.019023 validation MAE=0.032657,rank=10\n",
      "[SoftImpute] Iter 4: observed MAE=0.017335 validation MAE=0.032019,rank=10\n",
      "[SoftImpute] Iter 5: observed MAE=0.015917 validation MAE=0.031473,rank=10\n",
      "[SoftImpute] Iter 6: observed MAE=0.014719 validation MAE=0.030991,rank=10\n",
      "[SoftImpute] Iter 7: observed MAE=0.013693 validation MAE=0.030546,rank=10\n",
      "[SoftImpute] Iter 8: observed MAE=0.012810 validation MAE=0.030130,rank=10\n",
      "[SoftImpute] Iter 9: observed MAE=0.012044 validation MAE=0.029748,rank=10\n",
      "[SoftImpute] Iter 10: observed MAE=0.011375 validation MAE=0.029400,rank=10\n",
      "[SoftImpute] Iter 11: observed MAE=0.010787 validation MAE=0.029089,rank=10\n",
      "[SoftImpute] Iter 12: observed MAE=0.010265 validation MAE=0.028806,rank=10\n",
      "[SoftImpute] Iter 13: observed MAE=0.009801 validation MAE=0.028543,rank=10\n",
      "[SoftImpute] Iter 14: observed MAE=0.009386 validation MAE=0.028302,rank=10\n",
      "[SoftImpute] Iter 15: observed MAE=0.009014 validation MAE=0.028079,rank=10\n",
      "[SoftImpute] Iter 16: observed MAE=0.008681 validation MAE=0.027872,rank=10\n",
      "[SoftImpute] Iter 17: observed MAE=0.008380 validation MAE=0.027680,rank=10\n",
      "[SoftImpute] Iter 18: observed MAE=0.008107 validation MAE=0.027502,rank=10\n",
      "[SoftImpute] Iter 19: observed MAE=0.007858 validation MAE=0.027340,rank=10\n",
      "[SoftImpute] Iter 20: observed MAE=0.007632 validation MAE=0.027188,rank=10\n",
      "[SoftImpute] Iter 21: observed MAE=0.007425 validation MAE=0.027046,rank=10\n",
      "[SoftImpute] Iter 22: observed MAE=0.007235 validation MAE=0.026909,rank=10\n",
      "[SoftImpute] Iter 23: observed MAE=0.007060 validation MAE=0.026778,rank=10\n",
      "[SoftImpute] Iter 24: observed MAE=0.006898 validation MAE=0.026651,rank=10\n",
      "[SoftImpute] Iter 25: observed MAE=0.006747 validation MAE=0.026530,rank=10\n",
      "[SoftImpute] Iter 26: observed MAE=0.006608 validation MAE=0.026413,rank=10\n",
      "[SoftImpute] Iter 27: observed MAE=0.006478 validation MAE=0.026301,rank=10\n",
      "[SoftImpute] Iter 28: observed MAE=0.006356 validation MAE=0.026193,rank=10\n",
      "[SoftImpute] Iter 29: observed MAE=0.006243 validation MAE=0.026089,rank=10\n",
      "[SoftImpute] Iter 30: observed MAE=0.006137 validation MAE=0.025988,rank=10\n",
      "[SoftImpute] Iter 31: observed MAE=0.006038 validation MAE=0.025890,rank=10\n",
      "[SoftImpute] Iter 32: observed MAE=0.005945 validation MAE=0.025795,rank=10\n",
      "[SoftImpute] Iter 33: observed MAE=0.005858 validation MAE=0.025703,rank=10\n",
      "[SoftImpute] Iter 34: observed MAE=0.005776 validation MAE=0.025613,rank=10\n",
      "[SoftImpute] Iter 35: observed MAE=0.005698 validation MAE=0.025526,rank=10\n",
      "[SoftImpute] Iter 36: observed MAE=0.005625 validation MAE=0.025441,rank=10\n",
      "[SoftImpute] Iter 37: observed MAE=0.005555 validation MAE=0.025359,rank=10\n",
      "[SoftImpute] Iter 38: observed MAE=0.005490 validation MAE=0.025279,rank=10\n",
      "[SoftImpute] Iter 39: observed MAE=0.005428 validation MAE=0.025200,rank=10\n",
      "[SoftImpute] Iter 40: observed MAE=0.005369 validation MAE=0.025122,rank=10\n",
      "[SoftImpute] Iter 41: observed MAE=0.005312 validation MAE=0.025047,rank=10\n",
      "[SoftImpute] Iter 42: observed MAE=0.005259 validation MAE=0.024973,rank=10\n",
      "[SoftImpute] Iter 43: observed MAE=0.005208 validation MAE=0.024900,rank=10\n",
      "[SoftImpute] Iter 44: observed MAE=0.005160 validation MAE=0.024829,rank=10\n",
      "[SoftImpute] Iter 45: observed MAE=0.005114 validation MAE=0.024759,rank=10\n",
      "[SoftImpute] Iter 46: observed MAE=0.005069 validation MAE=0.024690,rank=10\n",
      "[SoftImpute] Iter 47: observed MAE=0.005027 validation MAE=0.024623,rank=10\n",
      "[SoftImpute] Iter 48: observed MAE=0.004987 validation MAE=0.024556,rank=10\n",
      "[SoftImpute] Iter 49: observed MAE=0.004949 validation MAE=0.024491,rank=10\n",
      "[SoftImpute] Iter 50: observed MAE=0.004913 validation MAE=0.024427,rank=10\n",
      "[SoftImpute] Iter 51: observed MAE=0.004878 validation MAE=0.024364,rank=10\n",
      "[SoftImpute] Iter 52: observed MAE=0.004844 validation MAE=0.024302,rank=10\n",
      "[SoftImpute] Iter 53: observed MAE=0.004812 validation MAE=0.024240,rank=10\n",
      "[SoftImpute] Iter 54: observed MAE=0.004781 validation MAE=0.024180,rank=10\n",
      "[SoftImpute] Iter 55: observed MAE=0.004752 validation MAE=0.024121,rank=10\n",
      "[SoftImpute] Iter 56: observed MAE=0.004723 validation MAE=0.024062,rank=10\n",
      "[SoftImpute] Iter 57: observed MAE=0.004696 validation MAE=0.024005,rank=10\n",
      "[SoftImpute] Iter 58: observed MAE=0.004670 validation MAE=0.023948,rank=10\n",
      "[SoftImpute] Iter 59: observed MAE=0.004645 validation MAE=0.023892,rank=10\n",
      "[SoftImpute] Iter 60: observed MAE=0.004620 validation MAE=0.023838,rank=10\n",
      "[SoftImpute] Iter 61: observed MAE=0.004597 validation MAE=0.023785,rank=10\n",
      "[SoftImpute] Iter 62: observed MAE=0.004574 validation MAE=0.023733,rank=10\n",
      "[SoftImpute] Iter 63: observed MAE=0.004553 validation MAE=0.023681,rank=10\n",
      "[SoftImpute] Iter 64: observed MAE=0.004531 validation MAE=0.023631,rank=10\n",
      "[SoftImpute] Iter 65: observed MAE=0.004511 validation MAE=0.023582,rank=10\n",
      "[SoftImpute] Iter 66: observed MAE=0.004491 validation MAE=0.023532,rank=10\n",
      "[SoftImpute] Iter 67: observed MAE=0.004472 validation MAE=0.023484,rank=10\n",
      "[SoftImpute] Iter 68: observed MAE=0.004454 validation MAE=0.023436,rank=10\n",
      "[SoftImpute] Iter 69: observed MAE=0.004436 validation MAE=0.023388,rank=10\n",
      "[SoftImpute] Iter 70: observed MAE=0.004419 validation MAE=0.023341,rank=10\n",
      "[SoftImpute] Iter 71: observed MAE=0.004402 validation MAE=0.023295,rank=10\n",
      "[SoftImpute] Iter 72: observed MAE=0.004386 validation MAE=0.023249,rank=10\n",
      "[SoftImpute] Iter 73: observed MAE=0.004370 validation MAE=0.023203,rank=10\n",
      "[SoftImpute] Iter 74: observed MAE=0.004355 validation MAE=0.023158,rank=10\n",
      "[SoftImpute] Iter 75: observed MAE=0.004340 validation MAE=0.023114,rank=10\n",
      "[SoftImpute] Iter 76: observed MAE=0.004325 validation MAE=0.023070,rank=10\n",
      "[SoftImpute] Iter 77: observed MAE=0.004311 validation MAE=0.023026,rank=10\n",
      "[SoftImpute] Iter 78: observed MAE=0.004298 validation MAE=0.022983,rank=10\n",
      "[SoftImpute] Iter 79: observed MAE=0.004285 validation MAE=0.022940,rank=10\n",
      "[SoftImpute] Iter 80: observed MAE=0.004272 validation MAE=0.022897,rank=10\n",
      "[SoftImpute] Iter 81: observed MAE=0.004260 validation MAE=0.022855,rank=10\n",
      "[SoftImpute] Iter 82: observed MAE=0.004247 validation MAE=0.022814,rank=10\n",
      "[SoftImpute] Iter 83: observed MAE=0.004236 validation MAE=0.022773,rank=10\n",
      "[SoftImpute] Iter 84: observed MAE=0.004224 validation MAE=0.022732,rank=10\n",
      "[SoftImpute] Iter 85: observed MAE=0.004213 validation MAE=0.022691,rank=10\n",
      "[SoftImpute] Iter 86: observed MAE=0.004202 validation MAE=0.022652,rank=10\n",
      "[SoftImpute] Iter 87: observed MAE=0.004192 validation MAE=0.022612,rank=10\n",
      "[SoftImpute] Iter 88: observed MAE=0.004181 validation MAE=0.022573,rank=10\n",
      "[SoftImpute] Iter 89: observed MAE=0.004171 validation MAE=0.022534,rank=10\n",
      "[SoftImpute] Iter 90: observed MAE=0.004162 validation MAE=0.022496,rank=10\n",
      "[SoftImpute] Iter 91: observed MAE=0.004152 validation MAE=0.022457,rank=10\n",
      "[SoftImpute] Iter 92: observed MAE=0.004143 validation MAE=0.022420,rank=10\n",
      "[SoftImpute] Iter 93: observed MAE=0.004134 validation MAE=0.022382,rank=10\n",
      "[SoftImpute] Iter 94: observed MAE=0.004125 validation MAE=0.022345,rank=10\n",
      "[SoftImpute] Iter 95: observed MAE=0.004116 validation MAE=0.022308,rank=10\n",
      "[SoftImpute] Iter 96: observed MAE=0.004108 validation MAE=0.022271,rank=10\n",
      "[SoftImpute] Iter 97: observed MAE=0.004099 validation MAE=0.022235,rank=10\n",
      "[SoftImpute] Iter 98: observed MAE=0.004091 validation MAE=0.022198,rank=10\n",
      "[SoftImpute] Iter 99: observed MAE=0.004083 validation MAE=0.022162,rank=10\n",
      "[SoftImpute] Iter 100: observed MAE=0.004075 validation MAE=0.022127,rank=10\n",
      "[SoftImpute] Iter 101: observed MAE=0.004068 validation MAE=0.022091,rank=10\n",
      "[SoftImpute] Iter 102: observed MAE=0.004060 validation MAE=0.022056,rank=10\n",
      "[SoftImpute] Iter 103: observed MAE=0.004053 validation MAE=0.022021,rank=10\n",
      "[SoftImpute] Iter 104: observed MAE=0.004046 validation MAE=0.021986,rank=10\n",
      "[SoftImpute] Iter 105: observed MAE=0.004039 validation MAE=0.021952,rank=10\n",
      "[SoftImpute] Iter 106: observed MAE=0.004032 validation MAE=0.021918,rank=10\n",
      "[SoftImpute] Iter 107: observed MAE=0.004026 validation MAE=0.021883,rank=10\n",
      "[SoftImpute] Iter 108: observed MAE=0.004019 validation MAE=0.021849,rank=10\n",
      "[SoftImpute] Iter 109: observed MAE=0.004013 validation MAE=0.021816,rank=10\n",
      "[SoftImpute] Iter 110: observed MAE=0.004006 validation MAE=0.021782,rank=10\n",
      "[SoftImpute] Iter 111: observed MAE=0.004000 validation MAE=0.021749,rank=10\n",
      "[SoftImpute] Iter 112: observed MAE=0.003994 validation MAE=0.021716,rank=10\n",
      "[SoftImpute] Iter 113: observed MAE=0.003988 validation MAE=0.021683,rank=10\n",
      "[SoftImpute] Iter 114: observed MAE=0.003982 validation MAE=0.021650,rank=10\n",
      "[SoftImpute] Iter 115: observed MAE=0.003977 validation MAE=0.021617,rank=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 116: observed MAE=0.003971 validation MAE=0.021585,rank=10\n",
      "[SoftImpute] Iter 117: observed MAE=0.003966 validation MAE=0.021553,rank=10\n",
      "[SoftImpute] Iter 118: observed MAE=0.003960 validation MAE=0.021521,rank=10\n",
      "[SoftImpute] Iter 119: observed MAE=0.003955 validation MAE=0.021489,rank=10\n",
      "[SoftImpute] Iter 120: observed MAE=0.003950 validation MAE=0.021458,rank=10\n",
      "[SoftImpute] Iter 121: observed MAE=0.003945 validation MAE=0.021426,rank=10\n",
      "[SoftImpute] Iter 122: observed MAE=0.003940 validation MAE=0.021395,rank=10\n",
      "[SoftImpute] Iter 123: observed MAE=0.003935 validation MAE=0.021365,rank=10\n",
      "[SoftImpute] Iter 124: observed MAE=0.003930 validation MAE=0.021334,rank=10\n",
      "[SoftImpute] Iter 125: observed MAE=0.003925 validation MAE=0.021304,rank=10\n",
      "[SoftImpute] Iter 126: observed MAE=0.003921 validation MAE=0.021273,rank=10\n",
      "[SoftImpute] Iter 127: observed MAE=0.003916 validation MAE=0.021243,rank=10\n",
      "[SoftImpute] Iter 128: observed MAE=0.003911 validation MAE=0.021213,rank=10\n",
      "[SoftImpute] Iter 129: observed MAE=0.003907 validation MAE=0.021184,rank=10\n",
      "[SoftImpute] Iter 130: observed MAE=0.003903 validation MAE=0.021154,rank=10\n",
      "[SoftImpute] Iter 131: observed MAE=0.003898 validation MAE=0.021125,rank=10\n",
      "[SoftImpute] Iter 132: observed MAE=0.003894 validation MAE=0.021096,rank=10\n",
      "[SoftImpute] Iter 133: observed MAE=0.003890 validation MAE=0.021067,rank=10\n",
      "[SoftImpute] Iter 134: observed MAE=0.003886 validation MAE=0.021038,rank=10\n",
      "[SoftImpute] Iter 135: observed MAE=0.003882 validation MAE=0.021009,rank=10\n",
      "[SoftImpute] Iter 136: observed MAE=0.003878 validation MAE=0.020980,rank=10\n",
      "[SoftImpute] Iter 137: observed MAE=0.003874 validation MAE=0.020952,rank=10\n",
      "[SoftImpute] Iter 138: observed MAE=0.003870 validation MAE=0.020923,rank=10\n",
      "[SoftImpute] Iter 139: observed MAE=0.003866 validation MAE=0.020895,rank=10\n",
      "[SoftImpute] Iter 140: observed MAE=0.003862 validation MAE=0.020867,rank=10\n",
      "[SoftImpute] Iter 141: observed MAE=0.003859 validation MAE=0.020839,rank=10\n",
      "[SoftImpute] Iter 142: observed MAE=0.003855 validation MAE=0.020812,rank=10\n",
      "[SoftImpute] Iter 143: observed MAE=0.003851 validation MAE=0.020785,rank=10\n",
      "[SoftImpute] Iter 144: observed MAE=0.003848 validation MAE=0.020757,rank=10\n",
      "[SoftImpute] Iter 145: observed MAE=0.003844 validation MAE=0.020730,rank=10\n",
      "[SoftImpute] Iter 146: observed MAE=0.003841 validation MAE=0.020703,rank=10\n",
      "[SoftImpute] Iter 147: observed MAE=0.003838 validation MAE=0.020677,rank=10\n",
      "[SoftImpute] Iter 148: observed MAE=0.003834 validation MAE=0.020650,rank=10\n",
      "[SoftImpute] Iter 149: observed MAE=0.003831 validation MAE=0.020624,rank=10\n",
      "[SoftImpute] Iter 150: observed MAE=0.003828 validation MAE=0.020597,rank=10\n",
      "[SoftImpute] Iter 151: observed MAE=0.003825 validation MAE=0.020571,rank=10\n",
      "[SoftImpute] Iter 152: observed MAE=0.003821 validation MAE=0.020546,rank=10\n",
      "[SoftImpute] Iter 153: observed MAE=0.003818 validation MAE=0.020520,rank=10\n",
      "[SoftImpute] Iter 154: observed MAE=0.003815 validation MAE=0.020495,rank=10\n",
      "[SoftImpute] Iter 155: observed MAE=0.003812 validation MAE=0.020469,rank=10\n",
      "[SoftImpute] Iter 156: observed MAE=0.003809 validation MAE=0.020444,rank=10\n",
      "[SoftImpute] Iter 157: observed MAE=0.003806 validation MAE=0.020419,rank=10\n",
      "[SoftImpute] Iter 158: observed MAE=0.003803 validation MAE=0.020395,rank=10\n",
      "[SoftImpute] Iter 159: observed MAE=0.003800 validation MAE=0.020370,rank=10\n",
      "[SoftImpute] Iter 160: observed MAE=0.003798 validation MAE=0.020346,rank=10\n",
      "[SoftImpute] Iter 161: observed MAE=0.003795 validation MAE=0.020322,rank=10\n",
      "[SoftImpute] Iter 162: observed MAE=0.003792 validation MAE=0.020298,rank=10\n",
      "[SoftImpute] Iter 163: observed MAE=0.003789 validation MAE=0.020274,rank=10\n",
      "[SoftImpute] Iter 164: observed MAE=0.003786 validation MAE=0.020250,rank=10\n",
      "[SoftImpute] Iter 165: observed MAE=0.003784 validation MAE=0.020227,rank=10\n",
      "[SoftImpute] Iter 166: observed MAE=0.003781 validation MAE=0.020203,rank=10\n",
      "[SoftImpute] Iter 167: observed MAE=0.003778 validation MAE=0.020180,rank=10\n",
      "[SoftImpute] Iter 168: observed MAE=0.003776 validation MAE=0.020157,rank=10\n",
      "[SoftImpute] Iter 169: observed MAE=0.003773 validation MAE=0.020134,rank=10\n",
      "[SoftImpute] Iter 170: observed MAE=0.003771 validation MAE=0.020111,rank=10\n",
      "[SoftImpute] Iter 171: observed MAE=0.003768 validation MAE=0.020089,rank=10\n",
      "[SoftImpute] Iter 172: observed MAE=0.003766 validation MAE=0.020067,rank=10\n",
      "[SoftImpute] Iter 173: observed MAE=0.003763 validation MAE=0.020045,rank=10\n",
      "[SoftImpute] Iter 174: observed MAE=0.003761 validation MAE=0.020023,rank=10\n",
      "[SoftImpute] Iter 175: observed MAE=0.003758 validation MAE=0.020001,rank=10\n",
      "[SoftImpute] Iter 176: observed MAE=0.003756 validation MAE=0.019980,rank=10\n",
      "[SoftImpute] Iter 177: observed MAE=0.003754 validation MAE=0.019958,rank=10\n",
      "[SoftImpute] Iter 178: observed MAE=0.003751 validation MAE=0.019937,rank=10\n",
      "[SoftImpute] Iter 179: observed MAE=0.003749 validation MAE=0.019915,rank=10\n",
      "[SoftImpute] Iter 180: observed MAE=0.003747 validation MAE=0.019894,rank=10\n",
      "[SoftImpute] Iter 181: observed MAE=0.003744 validation MAE=0.019873,rank=10\n",
      "[SoftImpute] Iter 182: observed MAE=0.003742 validation MAE=0.019852,rank=10\n",
      "[SoftImpute] Iter 183: observed MAE=0.003740 validation MAE=0.019831,rank=10\n",
      "[SoftImpute] Iter 184: observed MAE=0.003738 validation MAE=0.019810,rank=10\n",
      "[SoftImpute] Iter 185: observed MAE=0.003735 validation MAE=0.019789,rank=10\n",
      "[SoftImpute] Iter 186: observed MAE=0.003733 validation MAE=0.019768,rank=10\n",
      "[SoftImpute] Iter 187: observed MAE=0.003731 validation MAE=0.019748,rank=10\n",
      "[SoftImpute] Iter 188: observed MAE=0.003729 validation MAE=0.019727,rank=10\n",
      "[SoftImpute] Iter 189: observed MAE=0.003727 validation MAE=0.019707,rank=10\n",
      "[SoftImpute] Iter 190: observed MAE=0.003725 validation MAE=0.019686,rank=10\n",
      "[SoftImpute] Iter 191: observed MAE=0.003723 validation MAE=0.019666,rank=10\n",
      "[SoftImpute] Iter 192: observed MAE=0.003721 validation MAE=0.019646,rank=10\n",
      "[SoftImpute] Iter 193: observed MAE=0.003719 validation MAE=0.019626,rank=10\n",
      "[SoftImpute] Iter 194: observed MAE=0.003717 validation MAE=0.019607,rank=10\n",
      "[SoftImpute] Iter 195: observed MAE=0.003715 validation MAE=0.019587,rank=10\n",
      "[SoftImpute] Iter 196: observed MAE=0.003713 validation MAE=0.019568,rank=10\n",
      "[SoftImpute] Iter 197: observed MAE=0.003711 validation MAE=0.019548,rank=10\n",
      "[SoftImpute] Iter 198: observed MAE=0.003709 validation MAE=0.019529,rank=10\n",
      "[SoftImpute] Iter 199: observed MAE=0.003707 validation MAE=0.019510,rank=10\n",
      "[SoftImpute] Iter 200: observed MAE=0.003705 validation MAE=0.019491,rank=10\n",
      "[SoftImpute] Iter 201: observed MAE=0.003703 validation MAE=0.019472,rank=10\n",
      "[SoftImpute] Iter 202: observed MAE=0.003702 validation MAE=0.019452,rank=10\n",
      "[SoftImpute] Iter 203: observed MAE=0.003700 validation MAE=0.019434,rank=10\n",
      "[SoftImpute] Iter 204: observed MAE=0.003698 validation MAE=0.019415,rank=10\n",
      "[SoftImpute] Iter 205: observed MAE=0.003696 validation MAE=0.019396,rank=10\n",
      "[SoftImpute] Iter 206: observed MAE=0.003694 validation MAE=0.019377,rank=10\n",
      "[SoftImpute] Iter 207: observed MAE=0.003693 validation MAE=0.019358,rank=10\n",
      "[SoftImpute] Iter 208: observed MAE=0.003691 validation MAE=0.019340,rank=10\n",
      "[SoftImpute] Iter 209: observed MAE=0.003689 validation MAE=0.019321,rank=10\n",
      "[SoftImpute] Iter 210: observed MAE=0.003687 validation MAE=0.019302,rank=10\n",
      "[SoftImpute] Iter 211: observed MAE=0.003685 validation MAE=0.019284,rank=10\n",
      "[SoftImpute] Iter 212: observed MAE=0.003684 validation MAE=0.019265,rank=10\n",
      "[SoftImpute] Iter 213: observed MAE=0.003682 validation MAE=0.019247,rank=10\n",
      "[SoftImpute] Iter 214: observed MAE=0.003680 validation MAE=0.019229,rank=10\n",
      "[SoftImpute] Iter 215: observed MAE=0.003679 validation MAE=0.019210,rank=10\n",
      "[SoftImpute] Iter 216: observed MAE=0.003677 validation MAE=0.019192,rank=10\n",
      "[SoftImpute] Iter 217: observed MAE=0.003675 validation MAE=0.019174,rank=10\n",
      "[SoftImpute] Iter 218: observed MAE=0.003674 validation MAE=0.019156,rank=10\n",
      "[SoftImpute] Iter 219: observed MAE=0.003672 validation MAE=0.019138,rank=10\n",
      "[SoftImpute] Iter 220: observed MAE=0.003670 validation MAE=0.019121,rank=10\n",
      "[SoftImpute] Iter 221: observed MAE=0.003669 validation MAE=0.019103,rank=10\n",
      "[SoftImpute] Iter 222: observed MAE=0.003667 validation MAE=0.019085,rank=10\n",
      "[SoftImpute] Iter 223: observed MAE=0.003666 validation MAE=0.019068,rank=10\n",
      "[SoftImpute] Iter 224: observed MAE=0.003664 validation MAE=0.019050,rank=10\n",
      "[SoftImpute] Iter 225: observed MAE=0.003663 validation MAE=0.019033,rank=10\n",
      "[SoftImpute] Iter 226: observed MAE=0.003661 validation MAE=0.019015,rank=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 227: observed MAE=0.003660 validation MAE=0.018998,rank=10\n",
      "[SoftImpute] Iter 228: observed MAE=0.003658 validation MAE=0.018981,rank=10\n",
      "[SoftImpute] Iter 229: observed MAE=0.003657 validation MAE=0.018964,rank=10\n",
      "[SoftImpute] Iter 230: observed MAE=0.003655 validation MAE=0.018947,rank=10\n",
      "[SoftImpute] Iter 231: observed MAE=0.003654 validation MAE=0.018930,rank=10\n",
      "[SoftImpute] Iter 232: observed MAE=0.003652 validation MAE=0.018914,rank=10\n",
      "[SoftImpute] Iter 233: observed MAE=0.003651 validation MAE=0.018897,rank=10\n",
      "[SoftImpute] Iter 234: observed MAE=0.003649 validation MAE=0.018881,rank=10\n",
      "[SoftImpute] Iter 235: observed MAE=0.003648 validation MAE=0.018865,rank=10\n",
      "[SoftImpute] Iter 236: observed MAE=0.003647 validation MAE=0.018848,rank=10\n",
      "[SoftImpute] Iter 237: observed MAE=0.003645 validation MAE=0.018832,rank=10\n",
      "[SoftImpute] Iter 238: observed MAE=0.003644 validation MAE=0.018816,rank=10\n",
      "[SoftImpute] Iter 239: observed MAE=0.003643 validation MAE=0.018800,rank=10\n",
      "[SoftImpute] Iter 240: observed MAE=0.003641 validation MAE=0.018784,rank=10\n",
      "[SoftImpute] Iter 241: observed MAE=0.003640 validation MAE=0.018768,rank=10\n",
      "[SoftImpute] Iter 242: observed MAE=0.003639 validation MAE=0.018752,rank=10\n",
      "[SoftImpute] Iter 243: observed MAE=0.003637 validation MAE=0.018736,rank=10\n",
      "[SoftImpute] Iter 244: observed MAE=0.003636 validation MAE=0.018720,rank=10\n",
      "[SoftImpute] Iter 245: observed MAE=0.003635 validation MAE=0.018704,rank=10\n",
      "[SoftImpute] Iter 246: observed MAE=0.003634 validation MAE=0.018689,rank=10\n",
      "[SoftImpute] Iter 247: observed MAE=0.003632 validation MAE=0.018673,rank=10\n",
      "[SoftImpute] Iter 248: observed MAE=0.003631 validation MAE=0.018658,rank=10\n",
      "[SoftImpute] Iter 249: observed MAE=0.003630 validation MAE=0.018642,rank=10\n",
      "[SoftImpute] Iter 250: observed MAE=0.003629 validation MAE=0.018627,rank=10\n",
      "[SoftImpute] Iter 251: observed MAE=0.003628 validation MAE=0.018611,rank=10\n",
      "[SoftImpute] Iter 252: observed MAE=0.003626 validation MAE=0.018596,rank=10\n",
      "[SoftImpute] Iter 253: observed MAE=0.003625 validation MAE=0.018581,rank=10\n",
      "[SoftImpute] Iter 254: observed MAE=0.003624 validation MAE=0.018566,rank=10\n",
      "[SoftImpute] Iter 255: observed MAE=0.003623 validation MAE=0.018551,rank=10\n",
      "[SoftImpute] Iter 256: observed MAE=0.003622 validation MAE=0.018536,rank=10\n",
      "[SoftImpute] Iter 257: observed MAE=0.003621 validation MAE=0.018521,rank=10\n",
      "[SoftImpute] Iter 258: observed MAE=0.003619 validation MAE=0.018506,rank=10\n",
      "[SoftImpute] Iter 259: observed MAE=0.003618 validation MAE=0.018491,rank=10\n",
      "[SoftImpute] Iter 260: observed MAE=0.003617 validation MAE=0.018476,rank=10\n",
      "[SoftImpute] Iter 261: observed MAE=0.003616 validation MAE=0.018461,rank=10\n",
      "[SoftImpute] Iter 262: observed MAE=0.003615 validation MAE=0.018447,rank=10\n",
      "[SoftImpute] Iter 263: observed MAE=0.003614 validation MAE=0.018432,rank=10\n",
      "[SoftImpute] Iter 264: observed MAE=0.003613 validation MAE=0.018417,rank=10\n",
      "[SoftImpute] Iter 265: observed MAE=0.003612 validation MAE=0.018403,rank=10\n",
      "[SoftImpute] Iter 266: observed MAE=0.003611 validation MAE=0.018389,rank=10\n",
      "[SoftImpute] Iter 267: observed MAE=0.003610 validation MAE=0.018374,rank=10\n",
      "[SoftImpute] Iter 268: observed MAE=0.003609 validation MAE=0.018360,rank=10\n",
      "[SoftImpute] Iter 269: observed MAE=0.003607 validation MAE=0.018346,rank=10\n",
      "[SoftImpute] Iter 270: observed MAE=0.003606 validation MAE=0.018331,rank=10\n",
      "[SoftImpute] Iter 271: observed MAE=0.003605 validation MAE=0.018317,rank=10\n",
      "[SoftImpute] Iter 272: observed MAE=0.003604 validation MAE=0.018303,rank=10\n",
      "[SoftImpute] Iter 273: observed MAE=0.003603 validation MAE=0.018289,rank=10\n",
      "[SoftImpute] Iter 274: observed MAE=0.003602 validation MAE=0.018276,rank=10\n",
      "[SoftImpute] Iter 275: observed MAE=0.003601 validation MAE=0.018262,rank=10\n",
      "[SoftImpute] Iter 276: observed MAE=0.003600 validation MAE=0.018248,rank=10\n",
      "[SoftImpute] Iter 277: observed MAE=0.003599 validation MAE=0.018235,rank=10\n",
      "[SoftImpute] Iter 278: observed MAE=0.003598 validation MAE=0.018221,rank=10\n",
      "[SoftImpute] Iter 279: observed MAE=0.003597 validation MAE=0.018207,rank=10\n",
      "[SoftImpute] Iter 280: observed MAE=0.003596 validation MAE=0.018194,rank=10\n",
      "[SoftImpute] Iter 281: observed MAE=0.003595 validation MAE=0.018181,rank=10\n",
      "[SoftImpute] Iter 282: observed MAE=0.003595 validation MAE=0.018167,rank=10\n",
      "[SoftImpute] Iter 283: observed MAE=0.003594 validation MAE=0.018154,rank=10\n",
      "[SoftImpute] Iter 284: observed MAE=0.003593 validation MAE=0.018141,rank=10\n",
      "[SoftImpute] Iter 285: observed MAE=0.003592 validation MAE=0.018127,rank=10\n",
      "[SoftImpute] Iter 286: observed MAE=0.003591 validation MAE=0.018114,rank=10\n",
      "[SoftImpute] Iter 287: observed MAE=0.003590 validation MAE=0.018101,rank=10\n",
      "[SoftImpute] Iter 288: observed MAE=0.003589 validation MAE=0.018088,rank=10\n",
      "[SoftImpute] Iter 289: observed MAE=0.003588 validation MAE=0.018075,rank=10\n",
      "[SoftImpute] Iter 290: observed MAE=0.003587 validation MAE=0.018063,rank=10\n",
      "[SoftImpute] Iter 291: observed MAE=0.003586 validation MAE=0.018050,rank=10\n",
      "[SoftImpute] Iter 292: observed MAE=0.003585 validation MAE=0.018037,rank=10\n",
      "[SoftImpute] Iter 293: observed MAE=0.003585 validation MAE=0.018024,rank=10\n",
      "[SoftImpute] Iter 294: observed MAE=0.003584 validation MAE=0.018011,rank=10\n",
      "[SoftImpute] Iter 295: observed MAE=0.003583 validation MAE=0.017999,rank=10\n",
      "[SoftImpute] Iter 296: observed MAE=0.003582 validation MAE=0.017986,rank=10\n",
      "[SoftImpute] Iter 297: observed MAE=0.003581 validation MAE=0.017973,rank=10\n",
      "[SoftImpute] Iter 298: observed MAE=0.003580 validation MAE=0.017961,rank=10\n",
      "[SoftImpute] Iter 299: observed MAE=0.003579 validation MAE=0.017948,rank=10\n",
      "[SoftImpute] Iter 300: observed MAE=0.003579 validation MAE=0.017936,rank=10\n",
      "[SoftImpute] Iter 301: observed MAE=0.003578 validation MAE=0.017923,rank=10\n",
      "[SoftImpute] Iter 302: observed MAE=0.003577 validation MAE=0.017911,rank=10\n",
      "[SoftImpute] Iter 303: observed MAE=0.003576 validation MAE=0.017899,rank=10\n",
      "[SoftImpute] Iter 304: observed MAE=0.003575 validation MAE=0.017886,rank=10\n",
      "[SoftImpute] Iter 305: observed MAE=0.003574 validation MAE=0.017874,rank=10\n",
      "[SoftImpute] Iter 306: observed MAE=0.003574 validation MAE=0.017862,rank=10\n",
      "[SoftImpute] Iter 307: observed MAE=0.003573 validation MAE=0.017850,rank=10\n",
      "[SoftImpute] Iter 308: observed MAE=0.003572 validation MAE=0.017838,rank=10\n",
      "[SoftImpute] Iter 309: observed MAE=0.003571 validation MAE=0.017826,rank=10\n",
      "[SoftImpute] Iter 310: observed MAE=0.003570 validation MAE=0.017814,rank=10\n",
      "[SoftImpute] Iter 311: observed MAE=0.003570 validation MAE=0.017802,rank=10\n",
      "[SoftImpute] Iter 312: observed MAE=0.003569 validation MAE=0.017790,rank=10\n",
      "[SoftImpute] Iter 313: observed MAE=0.003568 validation MAE=0.017779,rank=10\n",
      "[SoftImpute] Iter 314: observed MAE=0.003567 validation MAE=0.017767,rank=10\n",
      "[SoftImpute] Iter 315: observed MAE=0.003566 validation MAE=0.017755,rank=10\n",
      "[SoftImpute] Iter 316: observed MAE=0.003566 validation MAE=0.017743,rank=10\n",
      "[SoftImpute] Iter 317: observed MAE=0.003565 validation MAE=0.017731,rank=10\n",
      "[SoftImpute] Iter 318: observed MAE=0.003564 validation MAE=0.017720,rank=10\n",
      "[SoftImpute] Iter 319: observed MAE=0.003563 validation MAE=0.017708,rank=10\n",
      "[SoftImpute] Iter 320: observed MAE=0.003562 validation MAE=0.017696,rank=10\n",
      "[SoftImpute] Iter 321: observed MAE=0.003562 validation MAE=0.017685,rank=10\n",
      "[SoftImpute] Iter 322: observed MAE=0.003561 validation MAE=0.017673,rank=10\n",
      "[SoftImpute] Iter 323: observed MAE=0.003560 validation MAE=0.017662,rank=10\n",
      "[SoftImpute] Iter 324: observed MAE=0.003559 validation MAE=0.017650,rank=10\n",
      "[SoftImpute] Iter 325: observed MAE=0.003558 validation MAE=0.017639,rank=10\n",
      "[SoftImpute] Iter 326: observed MAE=0.003558 validation MAE=0.017627,rank=10\n",
      "[SoftImpute] Iter 327: observed MAE=0.003557 validation MAE=0.017616,rank=10\n",
      "[SoftImpute] Iter 328: observed MAE=0.003556 validation MAE=0.017605,rank=10\n",
      "[SoftImpute] Iter 329: observed MAE=0.003555 validation MAE=0.017594,rank=10\n",
      "[SoftImpute] Iter 330: observed MAE=0.003555 validation MAE=0.017582,rank=10\n",
      "[SoftImpute] Iter 331: observed MAE=0.003554 validation MAE=0.017571,rank=10\n",
      "[SoftImpute] Iter 332: observed MAE=0.003553 validation MAE=0.017560,rank=10\n",
      "[SoftImpute] Iter 333: observed MAE=0.003553 validation MAE=0.017549,rank=10\n",
      "[SoftImpute] Iter 334: observed MAE=0.003552 validation MAE=0.017539,rank=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 335: observed MAE=0.003551 validation MAE=0.017528,rank=10\n",
      "[SoftImpute] Iter 336: observed MAE=0.003550 validation MAE=0.017517,rank=10\n",
      "[SoftImpute] Iter 337: observed MAE=0.003550 validation MAE=0.017506,rank=10\n",
      "[SoftImpute] Iter 338: observed MAE=0.003549 validation MAE=0.017495,rank=10\n",
      "[SoftImpute] Iter 339: observed MAE=0.003548 validation MAE=0.017484,rank=10\n",
      "[SoftImpute] Iter 340: observed MAE=0.003548 validation MAE=0.017474,rank=10\n",
      "[SoftImpute] Iter 341: observed MAE=0.003547 validation MAE=0.017463,rank=10\n",
      "[SoftImpute] Iter 342: observed MAE=0.003546 validation MAE=0.017452,rank=10\n",
      "[SoftImpute] Iter 343: observed MAE=0.003545 validation MAE=0.017442,rank=10\n",
      "[SoftImpute] Iter 344: observed MAE=0.003545 validation MAE=0.017431,rank=10\n",
      "[SoftImpute] Iter 345: observed MAE=0.003544 validation MAE=0.017421,rank=10\n",
      "[SoftImpute] Iter 346: observed MAE=0.003543 validation MAE=0.017410,rank=10\n",
      "[SoftImpute] Iter 347: observed MAE=0.003543 validation MAE=0.017400,rank=10\n",
      "[SoftImpute] Iter 348: observed MAE=0.003542 validation MAE=0.017390,rank=10\n",
      "[SoftImpute] Iter 349: observed MAE=0.003541 validation MAE=0.017379,rank=10\n",
      "[SoftImpute] Iter 350: observed MAE=0.003541 validation MAE=0.017369,rank=10\n",
      "[SoftImpute] Iter 351: observed MAE=0.003540 validation MAE=0.017359,rank=10\n",
      "[SoftImpute] Iter 352: observed MAE=0.003539 validation MAE=0.017349,rank=10\n",
      "[SoftImpute] Iter 353: observed MAE=0.003539 validation MAE=0.017338,rank=10\n",
      "[SoftImpute] Iter 354: observed MAE=0.003538 validation MAE=0.017328,rank=10\n",
      "[SoftImpute] Iter 355: observed MAE=0.003537 validation MAE=0.017318,rank=10\n",
      "[SoftImpute] Iter 356: observed MAE=0.003536 validation MAE=0.017308,rank=10\n",
      "[SoftImpute] Iter 357: observed MAE=0.003536 validation MAE=0.017298,rank=10\n",
      "[SoftImpute] Iter 358: observed MAE=0.003535 validation MAE=0.017288,rank=10\n",
      "[SoftImpute] Iter 359: observed MAE=0.003534 validation MAE=0.017278,rank=10\n",
      "[SoftImpute] Iter 360: observed MAE=0.003534 validation MAE=0.017269,rank=10\n",
      "[SoftImpute] Iter 361: observed MAE=0.003533 validation MAE=0.017259,rank=10\n",
      "[SoftImpute] Iter 362: observed MAE=0.003533 validation MAE=0.017249,rank=10\n",
      "[SoftImpute] Iter 363: observed MAE=0.003532 validation MAE=0.017239,rank=10\n",
      "[SoftImpute] Iter 364: observed MAE=0.003531 validation MAE=0.017229,rank=10\n",
      "[SoftImpute] Iter 365: observed MAE=0.003531 validation MAE=0.017220,rank=10\n",
      "[SoftImpute] Stopped after iteration 365 for lambda=0.039366\n",
      "final num of user group: 1\n",
      "final num of item group: 1\n",
      "change mode state : True\n",
      "time cost: 11.796952724456787\n",
      "After the matrix factor stage, training error is 0.00353, validation error is 0.01722\n"
     ]
    }
   ],
   "source": [
    "rank_result = rtest('warm', data, meta_info, task_type=task_type, random_state=0, params=lx_params)\n",
    "rank_result.to_csv('result/rank_test/reg_rank.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
