{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LVXNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.86 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../..\\lvxnn\\DataReader.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 0.26 MB\n",
      "Decreased by 69.6%\n",
      "Memory usage of dataframe is 0.21 MB\n",
      "Memory usage after optimization is: 0.07 MB\n",
      "Decreased by 69.6%\n",
      "cold start user: 34\n",
      "cold start item: 318\n",
      "0\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68385, val loss: 0.68396\n",
      "Main effects training epoch: 2, train loss: 0.67816, val loss: 0.67880\n",
      "Main effects training epoch: 3, train loss: 0.67233, val loss: 0.67396\n",
      "Main effects training epoch: 4, train loss: 0.66715, val loss: 0.66975\n",
      "Main effects training epoch: 5, train loss: 0.65680, val loss: 0.65979\n",
      "Main effects training epoch: 6, train loss: 0.63304, val loss: 0.63684\n",
      "Main effects training epoch: 7, train loss: 0.58902, val loss: 0.59758\n",
      "Main effects training epoch: 8, train loss: 0.54806, val loss: 0.56842\n",
      "Main effects training epoch: 9, train loss: 0.52861, val loss: 0.56696\n",
      "Main effects training epoch: 10, train loss: 0.53034, val loss: 0.57210\n",
      "Main effects training epoch: 11, train loss: 0.52568, val loss: 0.56563\n",
      "Main effects training epoch: 12, train loss: 0.52430, val loss: 0.55785\n",
      "Main effects training epoch: 13, train loss: 0.52417, val loss: 0.55587\n",
      "Main effects training epoch: 14, train loss: 0.52337, val loss: 0.55852\n",
      "Main effects training epoch: 15, train loss: 0.52283, val loss: 0.55854\n",
      "Main effects training epoch: 16, train loss: 0.52284, val loss: 0.55802\n",
      "Main effects training epoch: 17, train loss: 0.52349, val loss: 0.55887\n",
      "Main effects training epoch: 18, train loss: 0.52358, val loss: 0.55615\n",
      "Main effects training epoch: 19, train loss: 0.52345, val loss: 0.56002\n",
      "Main effects training epoch: 20, train loss: 0.52245, val loss: 0.55694\n",
      "Main effects training epoch: 21, train loss: 0.52248, val loss: 0.55621\n",
      "Main effects training epoch: 22, train loss: 0.52227, val loss: 0.55712\n",
      "Main effects training epoch: 23, train loss: 0.52270, val loss: 0.55718\n",
      "Main effects training epoch: 24, train loss: 0.52307, val loss: 0.55816\n",
      "Main effects training epoch: 25, train loss: 0.52221, val loss: 0.55638\n",
      "Main effects training epoch: 26, train loss: 0.52279, val loss: 0.55839\n",
      "Main effects training epoch: 27, train loss: 0.52219, val loss: 0.55704\n",
      "Main effects training epoch: 28, train loss: 0.52214, val loss: 0.55688\n",
      "Main effects training epoch: 29, train loss: 0.52237, val loss: 0.55666\n",
      "Main effects training epoch: 30, train loss: 0.52228, val loss: 0.55697\n",
      "Main effects training epoch: 31, train loss: 0.52239, val loss: 0.55692\n",
      "Main effects training epoch: 32, train loss: 0.52252, val loss: 0.55658\n",
      "Main effects training epoch: 33, train loss: 0.52256, val loss: 0.55783\n",
      "Main effects training epoch: 34, train loss: 0.52335, val loss: 0.55680\n",
      "Main effects training epoch: 35, train loss: 0.52321, val loss: 0.55933\n",
      "Main effects training epoch: 36, train loss: 0.52223, val loss: 0.55561\n",
      "Main effects training epoch: 37, train loss: 0.52200, val loss: 0.55631\n",
      "Main effects training epoch: 38, train loss: 0.52204, val loss: 0.55707\n",
      "Main effects training epoch: 39, train loss: 0.52217, val loss: 0.55639\n",
      "Main effects training epoch: 40, train loss: 0.52265, val loss: 0.55798\n",
      "Main effects training epoch: 41, train loss: 0.52347, val loss: 0.55684\n",
      "Main effects training epoch: 42, train loss: 0.52264, val loss: 0.55716\n",
      "Main effects training epoch: 43, train loss: 0.52213, val loss: 0.55619\n",
      "Main effects training epoch: 44, train loss: 0.52332, val loss: 0.55736\n",
      "Main effects training epoch: 45, train loss: 0.52319, val loss: 0.55851\n",
      "Main effects training epoch: 46, train loss: 0.52279, val loss: 0.55593\n",
      "Main effects training epoch: 47, train loss: 0.52263, val loss: 0.55760\n",
      "Main effects training epoch: 48, train loss: 0.52352, val loss: 0.55833\n",
      "Main effects training epoch: 49, train loss: 0.52311, val loss: 0.55766\n",
      "Main effects training epoch: 50, train loss: 0.52246, val loss: 0.55647\n",
      "Main effects training epoch: 51, train loss: 0.52193, val loss: 0.55694\n",
      "Main effects training epoch: 52, train loss: 0.52170, val loss: 0.55664\n",
      "Main effects training epoch: 53, train loss: 0.52193, val loss: 0.55511\n",
      "Main effects training epoch: 54, train loss: 0.52217, val loss: 0.55755\n",
      "Main effects training epoch: 55, train loss: 0.52193, val loss: 0.55578\n",
      "Main effects training epoch: 56, train loss: 0.52182, val loss: 0.55753\n",
      "Main effects training epoch: 57, train loss: 0.52175, val loss: 0.55577\n",
      "Main effects training epoch: 58, train loss: 0.52176, val loss: 0.55627\n",
      "Main effects training epoch: 59, train loss: 0.52157, val loss: 0.55599\n",
      "Main effects training epoch: 60, train loss: 0.52205, val loss: 0.55663\n",
      "Main effects training epoch: 61, train loss: 0.52217, val loss: 0.55706\n",
      "Main effects training epoch: 62, train loss: 0.52180, val loss: 0.55604\n",
      "Main effects training epoch: 63, train loss: 0.52176, val loss: 0.55657\n",
      "Main effects training epoch: 64, train loss: 0.52160, val loss: 0.55743\n",
      "Main effects training epoch: 65, train loss: 0.52182, val loss: 0.55632\n",
      "Main effects training epoch: 66, train loss: 0.52189, val loss: 0.55523\n",
      "Main effects training epoch: 67, train loss: 0.52182, val loss: 0.55594\n",
      "Main effects training epoch: 68, train loss: 0.52208, val loss: 0.55694\n",
      "Main effects training epoch: 69, train loss: 0.52262, val loss: 0.55738\n",
      "Main effects training epoch: 70, train loss: 0.52215, val loss: 0.55664\n",
      "Main effects training epoch: 71, train loss: 0.52146, val loss: 0.55614\n",
      "Main effects training epoch: 72, train loss: 0.52142, val loss: 0.55708\n",
      "Main effects training epoch: 73, train loss: 0.52197, val loss: 0.55663\n",
      "Main effects training epoch: 74, train loss: 0.52230, val loss: 0.55769\n",
      "Main effects training epoch: 75, train loss: 0.52200, val loss: 0.55554\n",
      "Main effects training epoch: 76, train loss: 0.52249, val loss: 0.55731\n",
      "Main effects training epoch: 77, train loss: 0.52163, val loss: 0.55574\n",
      "Main effects training epoch: 78, train loss: 0.52136, val loss: 0.55682\n",
      "Main effects training epoch: 79, train loss: 0.52184, val loss: 0.55656\n",
      "Main effects training epoch: 80, train loss: 0.52134, val loss: 0.55555\n",
      "Main effects training epoch: 81, train loss: 0.52176, val loss: 0.55626\n",
      "Main effects training epoch: 82, train loss: 0.52171, val loss: 0.55568\n",
      "Main effects training epoch: 83, train loss: 0.52129, val loss: 0.55576\n",
      "Main effects training epoch: 84, train loss: 0.52201, val loss: 0.55762\n",
      "Main effects training epoch: 85, train loss: 0.52156, val loss: 0.55519\n",
      "Main effects training epoch: 86, train loss: 0.52116, val loss: 0.55664\n",
      "Main effects training epoch: 87, train loss: 0.52105, val loss: 0.55607\n",
      "Main effects training epoch: 88, train loss: 0.52146, val loss: 0.55530\n",
      "Main effects training epoch: 89, train loss: 0.52146, val loss: 0.55725\n",
      "Main effects training epoch: 90, train loss: 0.52240, val loss: 0.55564\n",
      "Main effects training epoch: 91, train loss: 0.52257, val loss: 0.55923\n",
      "Main effects training epoch: 92, train loss: 0.52114, val loss: 0.55522\n",
      "Main effects training epoch: 93, train loss: 0.52104, val loss: 0.55655\n",
      "Main effects training epoch: 94, train loss: 0.52087, val loss: 0.55493\n",
      "Main effects training epoch: 95, train loss: 0.52093, val loss: 0.55549\n",
      "Main effects training epoch: 96, train loss: 0.52081, val loss: 0.55519\n",
      "Main effects training epoch: 97, train loss: 0.52104, val loss: 0.55652\n",
      "Main effects training epoch: 98, train loss: 0.52104, val loss: 0.55550\n",
      "Main effects training epoch: 99, train loss: 0.52092, val loss: 0.55607\n",
      "Main effects training epoch: 100, train loss: 0.52119, val loss: 0.55477\n",
      "Main effects training epoch: 101, train loss: 0.52099, val loss: 0.55581\n",
      "Main effects training epoch: 102, train loss: 0.52127, val loss: 0.55526\n",
      "Main effects training epoch: 103, train loss: 0.52130, val loss: 0.55719\n",
      "Main effects training epoch: 104, train loss: 0.52128, val loss: 0.55647\n",
      "Main effects training epoch: 105, train loss: 0.52100, val loss: 0.55498\n",
      "Main effects training epoch: 106, train loss: 0.52126, val loss: 0.55407\n",
      "Main effects training epoch: 107, train loss: 0.52088, val loss: 0.55604\n",
      "Main effects training epoch: 108, train loss: 0.52062, val loss: 0.55429\n",
      "Main effects training epoch: 109, train loss: 0.52109, val loss: 0.55634\n",
      "Main effects training epoch: 110, train loss: 0.52072, val loss: 0.55552\n",
      "Main effects training epoch: 111, train loss: 0.52116, val loss: 0.55687\n",
      "Main effects training epoch: 112, train loss: 0.52061, val loss: 0.55522\n",
      "Main effects training epoch: 113, train loss: 0.52067, val loss: 0.55486\n",
      "Main effects training epoch: 114, train loss: 0.52124, val loss: 0.55512\n",
      "Main effects training epoch: 115, train loss: 0.52080, val loss: 0.55705\n",
      "Main effects training epoch: 116, train loss: 0.52043, val loss: 0.55454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 117, train loss: 0.52052, val loss: 0.55478\n",
      "Main effects training epoch: 118, train loss: 0.52038, val loss: 0.55520\n",
      "Main effects training epoch: 119, train loss: 0.52061, val loss: 0.55541\n",
      "Main effects training epoch: 120, train loss: 0.52071, val loss: 0.55651\n",
      "Main effects training epoch: 121, train loss: 0.52070, val loss: 0.55586\n",
      "Main effects training epoch: 122, train loss: 0.52035, val loss: 0.55483\n",
      "Main effects training epoch: 123, train loss: 0.52055, val loss: 0.55383\n",
      "Main effects training epoch: 124, train loss: 0.52042, val loss: 0.55565\n",
      "Main effects training epoch: 125, train loss: 0.52083, val loss: 0.55638\n",
      "Main effects training epoch: 126, train loss: 0.52113, val loss: 0.55510\n",
      "Main effects training epoch: 127, train loss: 0.52140, val loss: 0.55588\n",
      "Main effects training epoch: 128, train loss: 0.52066, val loss: 0.55621\n",
      "Main effects training epoch: 129, train loss: 0.52079, val loss: 0.55553\n",
      "Main effects training epoch: 130, train loss: 0.52071, val loss: 0.55551\n",
      "Main effects training epoch: 131, train loss: 0.52076, val loss: 0.55538\n",
      "Main effects training epoch: 132, train loss: 0.52040, val loss: 0.55590\n",
      "Main effects training epoch: 133, train loss: 0.52067, val loss: 0.55533\n",
      "Main effects training epoch: 134, train loss: 0.52030, val loss: 0.55398\n",
      "Main effects training epoch: 135, train loss: 0.52020, val loss: 0.55529\n",
      "Main effects training epoch: 136, train loss: 0.52006, val loss: 0.55561\n",
      "Main effects training epoch: 137, train loss: 0.52024, val loss: 0.55610\n",
      "Main effects training epoch: 138, train loss: 0.52029, val loss: 0.55527\n",
      "Main effects training epoch: 139, train loss: 0.52014, val loss: 0.55514\n",
      "Main effects training epoch: 140, train loss: 0.51997, val loss: 0.55423\n",
      "Main effects training epoch: 141, train loss: 0.52050, val loss: 0.55556\n",
      "Main effects training epoch: 142, train loss: 0.52028, val loss: 0.55505\n",
      "Main effects training epoch: 143, train loss: 0.52011, val loss: 0.55392\n",
      "Main effects training epoch: 144, train loss: 0.52031, val loss: 0.55626\n",
      "Main effects training epoch: 145, train loss: 0.51998, val loss: 0.55433\n",
      "Main effects training epoch: 146, train loss: 0.51981, val loss: 0.55475\n",
      "Main effects training epoch: 147, train loss: 0.51979, val loss: 0.55464\n",
      "Main effects training epoch: 148, train loss: 0.52045, val loss: 0.55675\n",
      "Main effects training epoch: 149, train loss: 0.52026, val loss: 0.55357\n",
      "Main effects training epoch: 150, train loss: 0.52012, val loss: 0.55546\n",
      "Main effects training epoch: 151, train loss: 0.51979, val loss: 0.55403\n",
      "Main effects training epoch: 152, train loss: 0.51994, val loss: 0.55547\n",
      "Main effects training epoch: 153, train loss: 0.52022, val loss: 0.55453\n",
      "Main effects training epoch: 154, train loss: 0.51987, val loss: 0.55490\n",
      "Main effects training epoch: 155, train loss: 0.52006, val loss: 0.55463\n",
      "Main effects training epoch: 156, train loss: 0.52006, val loss: 0.55556\n",
      "Main effects training epoch: 157, train loss: 0.51996, val loss: 0.55452\n",
      "Main effects training epoch: 158, train loss: 0.51980, val loss: 0.55478\n",
      "Main effects training epoch: 159, train loss: 0.52017, val loss: 0.55419\n",
      "Main effects training epoch: 160, train loss: 0.52019, val loss: 0.55629\n",
      "Main effects training epoch: 161, train loss: 0.51994, val loss: 0.55377\n",
      "Main effects training epoch: 162, train loss: 0.52049, val loss: 0.55681\n",
      "Main effects training epoch: 163, train loss: 0.52030, val loss: 0.55439\n",
      "Main effects training epoch: 164, train loss: 0.51984, val loss: 0.55484\n",
      "Main effects training epoch: 165, train loss: 0.51946, val loss: 0.55381\n",
      "Main effects training epoch: 166, train loss: 0.51939, val loss: 0.55434\n",
      "Main effects training epoch: 167, train loss: 0.51956, val loss: 0.55581\n",
      "Main effects training epoch: 168, train loss: 0.51960, val loss: 0.55372\n",
      "Main effects training epoch: 169, train loss: 0.51966, val loss: 0.55469\n",
      "Main effects training epoch: 170, train loss: 0.51968, val loss: 0.55430\n",
      "Main effects training epoch: 171, train loss: 0.51975, val loss: 0.55498\n",
      "Main effects training epoch: 172, train loss: 0.51924, val loss: 0.55469\n",
      "Main effects training epoch: 173, train loss: 0.51949, val loss: 0.55477\n",
      "Main effects training epoch: 174, train loss: 0.51927, val loss: 0.55359\n",
      "Main effects training epoch: 175, train loss: 0.51920, val loss: 0.55442\n",
      "Main effects training epoch: 176, train loss: 0.51953, val loss: 0.55506\n",
      "Main effects training epoch: 177, train loss: 0.51920, val loss: 0.55374\n",
      "Main effects training epoch: 178, train loss: 0.51963, val loss: 0.55524\n",
      "Main effects training epoch: 179, train loss: 0.51922, val loss: 0.55386\n",
      "Main effects training epoch: 180, train loss: 0.51945, val loss: 0.55468\n",
      "Main effects training epoch: 181, train loss: 0.52009, val loss: 0.55552\n",
      "Main effects training epoch: 182, train loss: 0.52069, val loss: 0.55595\n",
      "Main effects training epoch: 183, train loss: 0.51943, val loss: 0.55343\n",
      "Main effects training epoch: 184, train loss: 0.51898, val loss: 0.55423\n",
      "Main effects training epoch: 185, train loss: 0.51921, val loss: 0.55521\n",
      "Main effects training epoch: 186, train loss: 0.51892, val loss: 0.55405\n",
      "Main effects training epoch: 187, train loss: 0.51899, val loss: 0.55543\n",
      "Main effects training epoch: 188, train loss: 0.51894, val loss: 0.55413\n",
      "Main effects training epoch: 189, train loss: 0.51934, val loss: 0.55380\n",
      "Main effects training epoch: 190, train loss: 0.51896, val loss: 0.55389\n",
      "Main effects training epoch: 191, train loss: 0.51905, val loss: 0.55404\n",
      "Main effects training epoch: 192, train loss: 0.51899, val loss: 0.55425\n",
      "Main effects training epoch: 193, train loss: 0.51900, val loss: 0.55497\n",
      "Main effects training epoch: 194, train loss: 0.51883, val loss: 0.55318\n",
      "Main effects training epoch: 195, train loss: 0.51876, val loss: 0.55472\n",
      "Main effects training epoch: 196, train loss: 0.51905, val loss: 0.55512\n",
      "Main effects training epoch: 197, train loss: 0.51878, val loss: 0.55414\n",
      "Main effects training epoch: 198, train loss: 0.51893, val loss: 0.55330\n",
      "Main effects training epoch: 199, train loss: 0.51867, val loss: 0.55318\n",
      "Main effects training epoch: 200, train loss: 0.51891, val loss: 0.55434\n",
      "Main effects training epoch: 201, train loss: 0.51877, val loss: 0.55348\n",
      "Main effects training epoch: 202, train loss: 0.51873, val loss: 0.55401\n",
      "Main effects training epoch: 203, train loss: 0.51882, val loss: 0.55426\n",
      "Main effects training epoch: 204, train loss: 0.51878, val loss: 0.55171\n",
      "Main effects training epoch: 205, train loss: 0.51887, val loss: 0.55509\n",
      "Main effects training epoch: 206, train loss: 0.51859, val loss: 0.55498\n",
      "Main effects training epoch: 207, train loss: 0.51840, val loss: 0.55315\n",
      "Main effects training epoch: 208, train loss: 0.51858, val loss: 0.55314\n",
      "Main effects training epoch: 209, train loss: 0.51935, val loss: 0.55563\n",
      "Main effects training epoch: 210, train loss: 0.51882, val loss: 0.55289\n",
      "Main effects training epoch: 211, train loss: 0.51882, val loss: 0.55477\n",
      "Main effects training epoch: 212, train loss: 0.51864, val loss: 0.55352\n",
      "Main effects training epoch: 213, train loss: 0.51855, val loss: 0.55480\n",
      "Main effects training epoch: 214, train loss: 0.51861, val loss: 0.55304\n",
      "Main effects training epoch: 215, train loss: 0.51843, val loss: 0.55381\n",
      "Main effects training epoch: 216, train loss: 0.51873, val loss: 0.55373\n",
      "Main effects training epoch: 217, train loss: 0.51848, val loss: 0.55246\n",
      "Main effects training epoch: 218, train loss: 0.51878, val loss: 0.55459\n",
      "Main effects training epoch: 219, train loss: 0.51855, val loss: 0.55350\n",
      "Main effects training epoch: 220, train loss: 0.51803, val loss: 0.55346\n",
      "Main effects training epoch: 221, train loss: 0.51802, val loss: 0.55308\n",
      "Main effects training epoch: 222, train loss: 0.51806, val loss: 0.55225\n",
      "Main effects training epoch: 223, train loss: 0.51801, val loss: 0.55325\n",
      "Main effects training epoch: 224, train loss: 0.51792, val loss: 0.55289\n",
      "Main effects training epoch: 225, train loss: 0.51787, val loss: 0.55259\n",
      "Main effects training epoch: 226, train loss: 0.51788, val loss: 0.55184\n",
      "Main effects training epoch: 227, train loss: 0.51782, val loss: 0.55300\n",
      "Main effects training epoch: 228, train loss: 0.51783, val loss: 0.55355\n",
      "Main effects training epoch: 229, train loss: 0.51804, val loss: 0.55126\n",
      "Main effects training epoch: 230, train loss: 0.51776, val loss: 0.55216\n",
      "Main effects training epoch: 231, train loss: 0.51809, val loss: 0.55342\n",
      "Main effects training epoch: 232, train loss: 0.51780, val loss: 0.55274\n",
      "Main effects training epoch: 233, train loss: 0.51782, val loss: 0.55500\n",
      "Main effects training epoch: 234, train loss: 0.51769, val loss: 0.55278\n",
      "Main effects training epoch: 235, train loss: 0.51770, val loss: 0.55155\n",
      "Main effects training epoch: 236, train loss: 0.51767, val loss: 0.55230\n",
      "Main effects training epoch: 237, train loss: 0.51784, val loss: 0.55304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 238, train loss: 0.51758, val loss: 0.55384\n",
      "Main effects training epoch: 239, train loss: 0.51761, val loss: 0.55132\n",
      "Main effects training epoch: 240, train loss: 0.51772, val loss: 0.55170\n",
      "Main effects training epoch: 241, train loss: 0.51739, val loss: 0.55143\n",
      "Main effects training epoch: 242, train loss: 0.51770, val loss: 0.55312\n",
      "Main effects training epoch: 243, train loss: 0.51746, val loss: 0.55218\n",
      "Main effects training epoch: 244, train loss: 0.51727, val loss: 0.55108\n",
      "Main effects training epoch: 245, train loss: 0.51770, val loss: 0.55259\n",
      "Main effects training epoch: 246, train loss: 0.51715, val loss: 0.55245\n",
      "Main effects training epoch: 247, train loss: 0.51730, val loss: 0.55196\n",
      "Main effects training epoch: 248, train loss: 0.51731, val loss: 0.55248\n",
      "Main effects training epoch: 249, train loss: 0.51730, val loss: 0.55235\n",
      "Main effects training epoch: 250, train loss: 0.51717, val loss: 0.55125\n",
      "Main effects training epoch: 251, train loss: 0.51711, val loss: 0.55168\n",
      "Main effects training epoch: 252, train loss: 0.51734, val loss: 0.55179\n",
      "Main effects training epoch: 253, train loss: 0.51718, val loss: 0.55199\n",
      "Main effects training epoch: 254, train loss: 0.51706, val loss: 0.55126\n",
      "Main effects training epoch: 255, train loss: 0.51699, val loss: 0.55102\n",
      "Main effects training epoch: 256, train loss: 0.51693, val loss: 0.55164\n",
      "Main effects training epoch: 257, train loss: 0.51690, val loss: 0.55141\n",
      "Main effects training epoch: 258, train loss: 0.51721, val loss: 0.55013\n",
      "Main effects training epoch: 259, train loss: 0.51754, val loss: 0.55328\n",
      "Main effects training epoch: 260, train loss: 0.51684, val loss: 0.55151\n",
      "Main effects training epoch: 261, train loss: 0.51672, val loss: 0.55168\n",
      "Main effects training epoch: 262, train loss: 0.51685, val loss: 0.55017\n",
      "Main effects training epoch: 263, train loss: 0.51684, val loss: 0.54988\n",
      "Main effects training epoch: 264, train loss: 0.51693, val loss: 0.55335\n",
      "Main effects training epoch: 265, train loss: 0.51656, val loss: 0.55080\n",
      "Main effects training epoch: 266, train loss: 0.51727, val loss: 0.55150\n",
      "Main effects training epoch: 267, train loss: 0.51727, val loss: 0.55015\n",
      "Main effects training epoch: 268, train loss: 0.51684, val loss: 0.55251\n",
      "Main effects training epoch: 269, train loss: 0.51762, val loss: 0.55124\n",
      "Main effects training epoch: 270, train loss: 0.51661, val loss: 0.54927\n",
      "Main effects training epoch: 271, train loss: 0.51635, val loss: 0.54998\n",
      "Main effects training epoch: 272, train loss: 0.51634, val loss: 0.55194\n",
      "Main effects training epoch: 273, train loss: 0.51671, val loss: 0.55019\n",
      "Main effects training epoch: 274, train loss: 0.51653, val loss: 0.55082\n",
      "Main effects training epoch: 275, train loss: 0.51621, val loss: 0.55056\n",
      "Main effects training epoch: 276, train loss: 0.51601, val loss: 0.54996\n",
      "Main effects training epoch: 277, train loss: 0.51610, val loss: 0.54963\n",
      "Main effects training epoch: 278, train loss: 0.51612, val loss: 0.55114\n",
      "Main effects training epoch: 279, train loss: 0.51599, val loss: 0.54932\n",
      "Main effects training epoch: 280, train loss: 0.51592, val loss: 0.54952\n",
      "Main effects training epoch: 281, train loss: 0.51598, val loss: 0.55012\n",
      "Main effects training epoch: 282, train loss: 0.51585, val loss: 0.54992\n",
      "Main effects training epoch: 283, train loss: 0.51613, val loss: 0.54909\n",
      "Main effects training epoch: 284, train loss: 0.51593, val loss: 0.54926\n",
      "Main effects training epoch: 285, train loss: 0.51581, val loss: 0.54996\n",
      "Main effects training epoch: 286, train loss: 0.51603, val loss: 0.54997\n",
      "Main effects training epoch: 287, train loss: 0.51570, val loss: 0.54938\n",
      "Main effects training epoch: 288, train loss: 0.51562, val loss: 0.54933\n",
      "Main effects training epoch: 289, train loss: 0.51571, val loss: 0.54998\n",
      "Main effects training epoch: 290, train loss: 0.51554, val loss: 0.54855\n",
      "Main effects training epoch: 291, train loss: 0.51580, val loss: 0.54960\n",
      "Main effects training epoch: 292, train loss: 0.51595, val loss: 0.55066\n",
      "Main effects training epoch: 293, train loss: 0.51572, val loss: 0.54942\n",
      "Main effects training epoch: 294, train loss: 0.51557, val loss: 0.54915\n",
      "Main effects training epoch: 295, train loss: 0.51569, val loss: 0.54776\n",
      "Main effects training epoch: 296, train loss: 0.51540, val loss: 0.55114\n",
      "Main effects training epoch: 297, train loss: 0.51516, val loss: 0.54871\n",
      "Main effects training epoch: 298, train loss: 0.51512, val loss: 0.54821\n",
      "Main effects training epoch: 299, train loss: 0.51505, val loss: 0.54901\n",
      "Main effects training epoch: 300, train loss: 0.51519, val loss: 0.54891\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51756, val loss: 0.55156\n",
      "Main effects tuning epoch: 2, train loss: 0.51753, val loss: 0.55206\n",
      "Main effects tuning epoch: 3, train loss: 0.51757, val loss: 0.55076\n",
      "Main effects tuning epoch: 4, train loss: 0.51741, val loss: 0.55089\n",
      "Main effects tuning epoch: 5, train loss: 0.51775, val loss: 0.55244\n",
      "Main effects tuning epoch: 6, train loss: 0.51783, val loss: 0.55185\n",
      "Main effects tuning epoch: 7, train loss: 0.51741, val loss: 0.55046\n",
      "Main effects tuning epoch: 8, train loss: 0.51755, val loss: 0.55105\n",
      "Main effects tuning epoch: 9, train loss: 0.51761, val loss: 0.55153\n",
      "Main effects tuning epoch: 10, train loss: 0.51747, val loss: 0.55260\n",
      "Main effects tuning epoch: 11, train loss: 0.51744, val loss: 0.55008\n",
      "Main effects tuning epoch: 12, train loss: 0.51720, val loss: 0.55166\n",
      "Main effects tuning epoch: 13, train loss: 0.51726, val loss: 0.55077\n",
      "Main effects tuning epoch: 14, train loss: 0.51746, val loss: 0.55176\n",
      "Main effects tuning epoch: 15, train loss: 0.51726, val loss: 0.55056\n",
      "Main effects tuning epoch: 16, train loss: 0.51708, val loss: 0.54985\n",
      "Main effects tuning epoch: 17, train loss: 0.51705, val loss: 0.55124\n",
      "Main effects tuning epoch: 18, train loss: 0.51730, val loss: 0.55108\n",
      "Main effects tuning epoch: 19, train loss: 0.51699, val loss: 0.54967\n",
      "Main effects tuning epoch: 20, train loss: 0.51724, val loss: 0.55223\n",
      "Main effects tuning epoch: 21, train loss: 0.51720, val loss: 0.55250\n",
      "Main effects tuning epoch: 22, train loss: 0.51721, val loss: 0.54898\n",
      "Main effects tuning epoch: 23, train loss: 0.51787, val loss: 0.55175\n",
      "Main effects tuning epoch: 24, train loss: 0.51693, val loss: 0.55059\n",
      "Main effects tuning epoch: 25, train loss: 0.51707, val loss: 0.55153\n",
      "Main effects tuning epoch: 26, train loss: 0.51677, val loss: 0.55009\n",
      "Main effects tuning epoch: 27, train loss: 0.51696, val loss: 0.55001\n",
      "Main effects tuning epoch: 28, train loss: 0.51677, val loss: 0.55037\n",
      "Main effects tuning epoch: 29, train loss: 0.51718, val loss: 0.55063\n",
      "Main effects tuning epoch: 30, train loss: 0.51747, val loss: 0.54910\n",
      "Main effects tuning epoch: 31, train loss: 0.51683, val loss: 0.55084\n",
      "Main effects tuning epoch: 32, train loss: 0.51668, val loss: 0.55093\n",
      "Main effects tuning epoch: 33, train loss: 0.51669, val loss: 0.54963\n",
      "Main effects tuning epoch: 34, train loss: 0.51677, val loss: 0.54858\n",
      "Main effects tuning epoch: 35, train loss: 0.51676, val loss: 0.55002\n",
      "Main effects tuning epoch: 36, train loss: 0.51665, val loss: 0.55027\n",
      "Main effects tuning epoch: 37, train loss: 0.51679, val loss: 0.55177\n",
      "Main effects tuning epoch: 38, train loss: 0.51685, val loss: 0.54834\n",
      "Main effects tuning epoch: 39, train loss: 0.51696, val loss: 0.55013\n",
      "Main effects tuning epoch: 40, train loss: 0.51664, val loss: 0.54934\n",
      "Main effects tuning epoch: 41, train loss: 0.51670, val loss: 0.55100\n",
      "Main effects tuning epoch: 42, train loss: 0.51667, val loss: 0.54920\n",
      "Main effects tuning epoch: 43, train loss: 0.51669, val loss: 0.54927\n",
      "Main effects tuning epoch: 44, train loss: 0.51694, val loss: 0.55108\n",
      "Main effects tuning epoch: 45, train loss: 0.51649, val loss: 0.54914\n",
      "Main effects tuning epoch: 46, train loss: 0.51639, val loss: 0.54936\n",
      "Main effects tuning epoch: 47, train loss: 0.51646, val loss: 0.54886\n",
      "Main effects tuning epoch: 48, train loss: 0.51657, val loss: 0.55116\n",
      "Main effects tuning epoch: 49, train loss: 0.51673, val loss: 0.54925\n",
      "Main effects tuning epoch: 50, train loss: 0.51635, val loss: 0.54933\n",
      "##########Stage 2: interaction training start.##########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 1, train loss: 0.48052, val loss: 0.51050\n",
      "Interaction training epoch: 2, train loss: 0.35444, val loss: 0.34672\n",
      "Interaction training epoch: 3, train loss: 0.32906, val loss: 0.34043\n",
      "Interaction training epoch: 4, train loss: 0.30873, val loss: 0.31850\n",
      "Interaction training epoch: 5, train loss: 0.30376, val loss: 0.29936\n",
      "Interaction training epoch: 6, train loss: 0.30252, val loss: 0.29656\n",
      "Interaction training epoch: 7, train loss: 0.29742, val loss: 0.29407\n",
      "Interaction training epoch: 8, train loss: 0.28827, val loss: 0.29319\n",
      "Interaction training epoch: 9, train loss: 0.28950, val loss: 0.29184\n",
      "Interaction training epoch: 10, train loss: 0.28862, val loss: 0.29437\n",
      "Interaction training epoch: 11, train loss: 0.28407, val loss: 0.28838\n",
      "Interaction training epoch: 12, train loss: 0.28852, val loss: 0.29295\n",
      "Interaction training epoch: 13, train loss: 0.28988, val loss: 0.29214\n",
      "Interaction training epoch: 14, train loss: 0.28120, val loss: 0.28183\n",
      "Interaction training epoch: 15, train loss: 0.28054, val loss: 0.28888\n",
      "Interaction training epoch: 16, train loss: 0.28964, val loss: 0.30249\n",
      "Interaction training epoch: 17, train loss: 0.27847, val loss: 0.28427\n",
      "Interaction training epoch: 18, train loss: 0.28477, val loss: 0.29195\n",
      "Interaction training epoch: 19, train loss: 0.28083, val loss: 0.28584\n",
      "Interaction training epoch: 20, train loss: 0.27749, val loss: 0.28311\n",
      "Interaction training epoch: 21, train loss: 0.28353, val loss: 0.29402\n",
      "Interaction training epoch: 22, train loss: 0.28574, val loss: 0.29261\n",
      "Interaction training epoch: 23, train loss: 0.28242, val loss: 0.28878\n",
      "Interaction training epoch: 24, train loss: 0.27648, val loss: 0.28084\n",
      "Interaction training epoch: 25, train loss: 0.28215, val loss: 0.29067\n",
      "Interaction training epoch: 26, train loss: 0.28249, val loss: 0.28287\n",
      "Interaction training epoch: 27, train loss: 0.27915, val loss: 0.29021\n",
      "Interaction training epoch: 28, train loss: 0.28090, val loss: 0.28558\n",
      "Interaction training epoch: 29, train loss: 0.27769, val loss: 0.28359\n",
      "Interaction training epoch: 30, train loss: 0.27632, val loss: 0.28435\n",
      "Interaction training epoch: 31, train loss: 0.27620, val loss: 0.28336\n",
      "Interaction training epoch: 32, train loss: 0.27630, val loss: 0.28396\n",
      "Interaction training epoch: 33, train loss: 0.27468, val loss: 0.28027\n",
      "Interaction training epoch: 34, train loss: 0.28842, val loss: 0.29457\n",
      "Interaction training epoch: 35, train loss: 0.27654, val loss: 0.28216\n",
      "Interaction training epoch: 36, train loss: 0.27546, val loss: 0.28187\n",
      "Interaction training epoch: 37, train loss: 0.27766, val loss: 0.28363\n",
      "Interaction training epoch: 38, train loss: 0.27355, val loss: 0.27957\n",
      "Interaction training epoch: 39, train loss: 0.27248, val loss: 0.28212\n",
      "Interaction training epoch: 40, train loss: 0.27835, val loss: 0.28686\n",
      "Interaction training epoch: 41, train loss: 0.27192, val loss: 0.27652\n",
      "Interaction training epoch: 42, train loss: 0.27348, val loss: 0.28517\n",
      "Interaction training epoch: 43, train loss: 0.27602, val loss: 0.28461\n",
      "Interaction training epoch: 44, train loss: 0.27454, val loss: 0.27962\n",
      "Interaction training epoch: 45, train loss: 0.28229, val loss: 0.29059\n",
      "Interaction training epoch: 46, train loss: 0.27690, val loss: 0.28694\n",
      "Interaction training epoch: 47, train loss: 0.27365, val loss: 0.28545\n",
      "Interaction training epoch: 48, train loss: 0.26803, val loss: 0.27420\n",
      "Interaction training epoch: 49, train loss: 0.27276, val loss: 0.28606\n",
      "Interaction training epoch: 50, train loss: 0.27101, val loss: 0.27457\n",
      "Interaction training epoch: 51, train loss: 0.27121, val loss: 0.28187\n",
      "Interaction training epoch: 52, train loss: 0.27516, val loss: 0.28281\n",
      "Interaction training epoch: 53, train loss: 0.26957, val loss: 0.27883\n",
      "Interaction training epoch: 54, train loss: 0.27326, val loss: 0.28321\n",
      "Interaction training epoch: 55, train loss: 0.27499, val loss: 0.28397\n",
      "Interaction training epoch: 56, train loss: 0.27179, val loss: 0.28125\n",
      "Interaction training epoch: 57, train loss: 0.27065, val loss: 0.28135\n",
      "Interaction training epoch: 58, train loss: 0.27088, val loss: 0.28066\n",
      "Interaction training epoch: 59, train loss: 0.27113, val loss: 0.28848\n",
      "Interaction training epoch: 60, train loss: 0.27030, val loss: 0.27869\n",
      "Interaction training epoch: 61, train loss: 0.26811, val loss: 0.27983\n",
      "Interaction training epoch: 62, train loss: 0.27305, val loss: 0.28840\n",
      "Interaction training epoch: 63, train loss: 0.26685, val loss: 0.27668\n",
      "Interaction training epoch: 64, train loss: 0.26723, val loss: 0.28292\n",
      "Interaction training epoch: 65, train loss: 0.26697, val loss: 0.27879\n",
      "Interaction training epoch: 66, train loss: 0.26740, val loss: 0.28114\n",
      "Interaction training epoch: 67, train loss: 0.26991, val loss: 0.28291\n",
      "Interaction training epoch: 68, train loss: 0.26460, val loss: 0.27419\n",
      "Interaction training epoch: 69, train loss: 0.26578, val loss: 0.27551\n",
      "Interaction training epoch: 70, train loss: 0.26398, val loss: 0.27805\n",
      "Interaction training epoch: 71, train loss: 0.27096, val loss: 0.28282\n",
      "Interaction training epoch: 72, train loss: 0.26214, val loss: 0.27239\n",
      "Interaction training epoch: 73, train loss: 0.27119, val loss: 0.28744\n",
      "Interaction training epoch: 74, train loss: 0.26365, val loss: 0.27154\n",
      "Interaction training epoch: 75, train loss: 0.26308, val loss: 0.27743\n",
      "Interaction training epoch: 76, train loss: 0.26724, val loss: 0.27882\n",
      "Interaction training epoch: 77, train loss: 0.26267, val loss: 0.27242\n",
      "Interaction training epoch: 78, train loss: 0.26616, val loss: 0.28123\n",
      "Interaction training epoch: 79, train loss: 0.26220, val loss: 0.27442\n",
      "Interaction training epoch: 80, train loss: 0.26423, val loss: 0.27775\n",
      "Interaction training epoch: 81, train loss: 0.26642, val loss: 0.28320\n",
      "Interaction training epoch: 82, train loss: 0.26766, val loss: 0.28572\n",
      "Interaction training epoch: 83, train loss: 0.26130, val loss: 0.27598\n",
      "Interaction training epoch: 84, train loss: 0.26336, val loss: 0.27975\n",
      "Interaction training epoch: 85, train loss: 0.26067, val loss: 0.27282\n",
      "Interaction training epoch: 86, train loss: 0.26487, val loss: 0.28589\n",
      "Interaction training epoch: 87, train loss: 0.26548, val loss: 0.27586\n",
      "Interaction training epoch: 88, train loss: 0.25740, val loss: 0.27181\n",
      "Interaction training epoch: 89, train loss: 0.26309, val loss: 0.27994\n",
      "Interaction training epoch: 90, train loss: 0.26315, val loss: 0.28052\n",
      "Interaction training epoch: 91, train loss: 0.25681, val loss: 0.27403\n",
      "Interaction training epoch: 92, train loss: 0.25631, val loss: 0.27445\n",
      "Interaction training epoch: 93, train loss: 0.25952, val loss: 0.27549\n",
      "Interaction training epoch: 94, train loss: 0.25862, val loss: 0.27213\n",
      "Interaction training epoch: 95, train loss: 0.25543, val loss: 0.27688\n",
      "Interaction training epoch: 96, train loss: 0.25966, val loss: 0.27655\n",
      "Interaction training epoch: 97, train loss: 0.25591, val loss: 0.27459\n",
      "Interaction training epoch: 98, train loss: 0.26286, val loss: 0.28306\n",
      "Interaction training epoch: 99, train loss: 0.26157, val loss: 0.28125\n",
      "Interaction training epoch: 100, train loss: 0.25883, val loss: 0.27622\n",
      "Interaction training epoch: 101, train loss: 0.25675, val loss: 0.27656\n",
      "Interaction training epoch: 102, train loss: 0.25654, val loss: 0.27586\n",
      "Interaction training epoch: 103, train loss: 0.25293, val loss: 0.27275\n",
      "Interaction training epoch: 104, train loss: 0.25195, val loss: 0.27150\n",
      "Interaction training epoch: 105, train loss: 0.26654, val loss: 0.28819\n",
      "Interaction training epoch: 106, train loss: 0.25329, val loss: 0.26748\n",
      "Interaction training epoch: 107, train loss: 0.25948, val loss: 0.28212\n",
      "Interaction training epoch: 108, train loss: 0.25381, val loss: 0.27137\n",
      "Interaction training epoch: 109, train loss: 0.25668, val loss: 0.27500\n",
      "Interaction training epoch: 110, train loss: 0.25772, val loss: 0.28160\n",
      "Interaction training epoch: 111, train loss: 0.26158, val loss: 0.28263\n",
      "Interaction training epoch: 112, train loss: 0.25331, val loss: 0.27036\n",
      "Interaction training epoch: 113, train loss: 0.25293, val loss: 0.27558\n",
      "Interaction training epoch: 114, train loss: 0.25884, val loss: 0.28165\n",
      "Interaction training epoch: 115, train loss: 0.25484, val loss: 0.27339\n",
      "Interaction training epoch: 116, train loss: 0.25200, val loss: 0.27550\n",
      "Interaction training epoch: 117, train loss: 0.24919, val loss: 0.26766\n",
      "Interaction training epoch: 118, train loss: 0.25987, val loss: 0.28336\n",
      "Interaction training epoch: 119, train loss: 0.25332, val loss: 0.27333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 120, train loss: 0.25426, val loss: 0.27549\n",
      "Interaction training epoch: 121, train loss: 0.25187, val loss: 0.27621\n",
      "Interaction training epoch: 122, train loss: 0.24995, val loss: 0.26928\n",
      "Interaction training epoch: 123, train loss: 0.24887, val loss: 0.27021\n",
      "Interaction training epoch: 124, train loss: 0.24877, val loss: 0.27201\n",
      "Interaction training epoch: 125, train loss: 0.24583, val loss: 0.26689\n",
      "Interaction training epoch: 126, train loss: 0.24920, val loss: 0.27148\n",
      "Interaction training epoch: 127, train loss: 0.24730, val loss: 0.26833\n",
      "Interaction training epoch: 128, train loss: 0.24592, val loss: 0.27068\n",
      "Interaction training epoch: 129, train loss: 0.24550, val loss: 0.26453\n",
      "Interaction training epoch: 130, train loss: 0.24907, val loss: 0.26862\n",
      "Interaction training epoch: 131, train loss: 0.26205, val loss: 0.29188\n",
      "Interaction training epoch: 132, train loss: 0.25150, val loss: 0.27227\n",
      "Interaction training epoch: 133, train loss: 0.24858, val loss: 0.26701\n",
      "Interaction training epoch: 134, train loss: 0.24966, val loss: 0.27851\n",
      "Interaction training epoch: 135, train loss: 0.25141, val loss: 0.26666\n",
      "Interaction training epoch: 136, train loss: 0.24474, val loss: 0.26914\n",
      "Interaction training epoch: 137, train loss: 0.25634, val loss: 0.27891\n",
      "Interaction training epoch: 138, train loss: 0.24595, val loss: 0.26404\n",
      "Interaction training epoch: 139, train loss: 0.24685, val loss: 0.26823\n",
      "Interaction training epoch: 140, train loss: 0.24788, val loss: 0.27532\n",
      "Interaction training epoch: 141, train loss: 0.24910, val loss: 0.26527\n",
      "Interaction training epoch: 142, train loss: 0.24766, val loss: 0.26908\n",
      "Interaction training epoch: 143, train loss: 0.25728, val loss: 0.28135\n",
      "Interaction training epoch: 144, train loss: 0.24381, val loss: 0.26407\n",
      "Interaction training epoch: 145, train loss: 0.24694, val loss: 0.27098\n",
      "Interaction training epoch: 146, train loss: 0.24143, val loss: 0.26236\n",
      "Interaction training epoch: 147, train loss: 0.25666, val loss: 0.28454\n",
      "Interaction training epoch: 148, train loss: 0.24687, val loss: 0.26607\n",
      "Interaction training epoch: 149, train loss: 0.24145, val loss: 0.26241\n",
      "Interaction training epoch: 150, train loss: 0.25011, val loss: 0.27818\n",
      "Interaction training epoch: 151, train loss: 0.24649, val loss: 0.26656\n",
      "Interaction training epoch: 152, train loss: 0.24091, val loss: 0.26489\n",
      "Interaction training epoch: 153, train loss: 0.24496, val loss: 0.26982\n",
      "Interaction training epoch: 154, train loss: 0.24382, val loss: 0.26224\n",
      "Interaction training epoch: 155, train loss: 0.24875, val loss: 0.27783\n",
      "Interaction training epoch: 156, train loss: 0.24641, val loss: 0.26925\n",
      "Interaction training epoch: 157, train loss: 0.26216, val loss: 0.28038\n",
      "Interaction training epoch: 158, train loss: 0.24415, val loss: 0.27497\n",
      "Interaction training epoch: 159, train loss: 0.25478, val loss: 0.27885\n",
      "Interaction training epoch: 160, train loss: 0.24779, val loss: 0.27241\n",
      "Interaction training epoch: 161, train loss: 0.25215, val loss: 0.27243\n",
      "Interaction training epoch: 162, train loss: 0.24513, val loss: 0.27015\n",
      "Interaction training epoch: 163, train loss: 0.24387, val loss: 0.27188\n",
      "Interaction training epoch: 164, train loss: 0.24246, val loss: 0.26171\n",
      "Interaction training epoch: 165, train loss: 0.24921, val loss: 0.27354\n",
      "Interaction training epoch: 166, train loss: 0.25379, val loss: 0.28450\n",
      "Interaction training epoch: 167, train loss: 0.26609, val loss: 0.29150\n",
      "Interaction training epoch: 168, train loss: 0.24732, val loss: 0.26819\n",
      "Interaction training epoch: 169, train loss: 0.24353, val loss: 0.27341\n",
      "Interaction training epoch: 170, train loss: 0.24540, val loss: 0.26895\n",
      "Interaction training epoch: 171, train loss: 0.24072, val loss: 0.26554\n",
      "Interaction training epoch: 172, train loss: 0.25084, val loss: 0.27771\n",
      "Interaction training epoch: 173, train loss: 0.24079, val loss: 0.26338\n",
      "Interaction training epoch: 174, train loss: 0.24696, val loss: 0.27300\n",
      "Interaction training epoch: 175, train loss: 0.25491, val loss: 0.28438\n",
      "Interaction training epoch: 176, train loss: 0.24205, val loss: 0.26334\n",
      "Interaction training epoch: 177, train loss: 0.24079, val loss: 0.26348\n",
      "Interaction training epoch: 178, train loss: 0.24375, val loss: 0.26825\n",
      "Interaction training epoch: 179, train loss: 0.25140, val loss: 0.27826\n",
      "Interaction training epoch: 180, train loss: 0.24147, val loss: 0.26843\n",
      "Interaction training epoch: 181, train loss: 0.24834, val loss: 0.26986\n",
      "Interaction training epoch: 182, train loss: 0.23907, val loss: 0.26111\n",
      "Interaction training epoch: 183, train loss: 0.24045, val loss: 0.26502\n",
      "Interaction training epoch: 184, train loss: 0.23877, val loss: 0.26035\n",
      "Interaction training epoch: 185, train loss: 0.24102, val loss: 0.26640\n",
      "Interaction training epoch: 186, train loss: 0.25332, val loss: 0.27961\n",
      "Interaction training epoch: 187, train loss: 0.24634, val loss: 0.26618\n",
      "Interaction training epoch: 188, train loss: 0.25396, val loss: 0.27460\n",
      "Interaction training epoch: 189, train loss: 0.24687, val loss: 0.27533\n",
      "Interaction training epoch: 190, train loss: 0.24007, val loss: 0.26171\n",
      "Interaction training epoch: 191, train loss: 0.24286, val loss: 0.26893\n",
      "Interaction training epoch: 192, train loss: 0.24015, val loss: 0.26402\n",
      "Interaction training epoch: 193, train loss: 0.24673, val loss: 0.27249\n",
      "Interaction training epoch: 194, train loss: 0.23991, val loss: 0.26291\n",
      "Interaction training epoch: 195, train loss: 0.24069, val loss: 0.26666\n",
      "Interaction training epoch: 196, train loss: 0.24440, val loss: 0.27369\n",
      "Interaction training epoch: 197, train loss: 0.24082, val loss: 0.26437\n",
      "Interaction training epoch: 198, train loss: 0.24261, val loss: 0.26626\n",
      "Interaction training epoch: 199, train loss: 0.23805, val loss: 0.26115\n",
      "Interaction training epoch: 200, train loss: 0.23991, val loss: 0.26874\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########3 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.26308, val loss: 0.28144\n",
      "Interaction tuning epoch: 2, train loss: 0.24408, val loss: 0.26354\n",
      "Interaction tuning epoch: 3, train loss: 0.24957, val loss: 0.26507\n",
      "Interaction tuning epoch: 4, train loss: 0.24716, val loss: 0.26587\n",
      "Interaction tuning epoch: 5, train loss: 0.24496, val loss: 0.26328\n",
      "Interaction tuning epoch: 6, train loss: 0.24970, val loss: 0.26392\n",
      "Interaction tuning epoch: 7, train loss: 0.24247, val loss: 0.26042\n",
      "Interaction tuning epoch: 8, train loss: 0.24816, val loss: 0.26268\n",
      "Interaction tuning epoch: 9, train loss: 0.24661, val loss: 0.26262\n",
      "Interaction tuning epoch: 10, train loss: 0.24434, val loss: 0.26358\n",
      "Interaction tuning epoch: 11, train loss: 0.25547, val loss: 0.27947\n",
      "Interaction tuning epoch: 12, train loss: 0.24254, val loss: 0.25769\n",
      "Interaction tuning epoch: 13, train loss: 0.25008, val loss: 0.26849\n",
      "Interaction tuning epoch: 14, train loss: 0.24720, val loss: 0.26629\n",
      "Interaction tuning epoch: 15, train loss: 0.24721, val loss: 0.26492\n",
      "Interaction tuning epoch: 16, train loss: 0.24674, val loss: 0.25885\n",
      "Interaction tuning epoch: 17, train loss: 0.25625, val loss: 0.26929\n",
      "Interaction tuning epoch: 18, train loss: 0.25657, val loss: 0.27014\n",
      "Interaction tuning epoch: 19, train loss: 0.24872, val loss: 0.26784\n",
      "Interaction tuning epoch: 20, train loss: 0.24390, val loss: 0.25752\n",
      "Interaction tuning epoch: 21, train loss: 0.24587, val loss: 0.26255\n",
      "Interaction tuning epoch: 22, train loss: 0.24064, val loss: 0.25988\n",
      "Interaction tuning epoch: 23, train loss: 0.24557, val loss: 0.26238\n",
      "Interaction tuning epoch: 24, train loss: 0.26686, val loss: 0.29246\n",
      "Interaction tuning epoch: 25, train loss: 0.30432, val loss: 0.33618\n",
      "Interaction tuning epoch: 26, train loss: 0.25553, val loss: 0.26407\n",
      "Interaction tuning epoch: 27, train loss: 0.24444, val loss: 0.26607\n",
      "Interaction tuning epoch: 28, train loss: 0.27244, val loss: 0.29359\n",
      "Interaction tuning epoch: 29, train loss: 0.24683, val loss: 0.25917\n",
      "Interaction tuning epoch: 30, train loss: 0.24636, val loss: 0.26570\n",
      "Interaction tuning epoch: 31, train loss: 0.25874, val loss: 0.28263\n",
      "Interaction tuning epoch: 32, train loss: 0.24748, val loss: 0.26245\n",
      "Interaction tuning epoch: 33, train loss: 0.24314, val loss: 0.25878\n",
      "Interaction tuning epoch: 34, train loss: 0.24862, val loss: 0.26989\n",
      "Interaction tuning epoch: 35, train loss: 0.24691, val loss: 0.26470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 36, train loss: 0.25080, val loss: 0.26268\n",
      "Interaction tuning epoch: 37, train loss: 0.24886, val loss: 0.27334\n",
      "Interaction tuning epoch: 38, train loss: 0.24592, val loss: 0.26065\n",
      "Interaction tuning epoch: 39, train loss: 0.23914, val loss: 0.25537\n",
      "Interaction tuning epoch: 40, train loss: 0.24266, val loss: 0.25946\n",
      "Interaction tuning epoch: 41, train loss: 0.24290, val loss: 0.25553\n",
      "Interaction tuning epoch: 42, train loss: 0.24239, val loss: 0.25929\n",
      "Interaction tuning epoch: 43, train loss: 0.24551, val loss: 0.26851\n",
      "Interaction tuning epoch: 44, train loss: 0.25022, val loss: 0.27165\n",
      "Interaction tuning epoch: 45, train loss: 0.24356, val loss: 0.25853\n",
      "Interaction tuning epoch: 46, train loss: 0.24359, val loss: 0.26278\n",
      "Interaction tuning epoch: 47, train loss: 0.25347, val loss: 0.27242\n",
      "Interaction tuning epoch: 48, train loss: 0.24295, val loss: 0.25879\n",
      "Interaction tuning epoch: 49, train loss: 0.24922, val loss: 0.26400\n",
      "Interaction tuning epoch: 50, train loss: 0.24491, val loss: 0.26056\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 39.71498680114746\n",
      "After the gam stage, training error is 0.24491 , validation error is 0.26056\n",
      "missing value counts: 99252\n",
      "#####start auto_tuning#####\n",
      "the best shrinkage is 0.812500\n",
      "[SoftImpute] Max Singular Value of X_init = 3.857373\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.223218 validation BCE=0.259284,rank=3\n",
      "[SoftImpute] Iter 1: observed BCE=0.217497 validation BCE=0.258563,rank=3\n",
      "[SoftImpute] Iter 2: observed BCE=0.213555 validation BCE=0.257997,rank=3\n",
      "[SoftImpute] Iter 3: observed BCE=0.211117 validation BCE=0.257292,rank=3\n",
      "[SoftImpute] Iter 4: observed BCE=0.209404 validation BCE=0.256923,rank=3\n",
      "[SoftImpute] Iter 5: observed BCE=0.207993 validation BCE=0.256530,rank=3\n",
      "[SoftImpute] Iter 6: observed BCE=0.207299 validation BCE=0.256274,rank=3\n",
      "[SoftImpute] Iter 7: observed BCE=0.206587 validation BCE=0.256128,rank=3\n",
      "[SoftImpute] Iter 8: observed BCE=0.206266 validation BCE=0.255876,rank=3\n",
      "[SoftImpute] Iter 9: observed BCE=0.206350 validation BCE=0.255653,rank=3\n",
      "[SoftImpute] Iter 10: observed BCE=0.206653 validation BCE=0.255399,rank=3\n",
      "[SoftImpute] Iter 11: observed BCE=0.206643 validation BCE=0.255300,rank=3\n",
      "[SoftImpute] Iter 12: observed BCE=0.206702 validation BCE=0.255265,rank=3\n",
      "[SoftImpute] Iter 13: observed BCE=0.206634 validation BCE=0.255258,rank=3\n",
      "[SoftImpute] Iter 14: observed BCE=0.206435 validation BCE=0.255118,rank=3\n",
      "[SoftImpute] Iter 15: observed BCE=0.206355 validation BCE=0.255441,rank=3\n",
      "[SoftImpute] Iter 16: observed BCE=0.206228 validation BCE=0.255133,rank=3\n",
      "[SoftImpute] Iter 17: observed BCE=0.206123 validation BCE=0.255462,rank=3\n",
      "[SoftImpute] Iter 18: observed BCE=0.206111 validation BCE=0.255689,rank=3\n",
      "[SoftImpute] Iter 19: observed BCE=0.206196 validation BCE=0.255765,rank=3\n",
      "[SoftImpute] Iter 20: observed BCE=0.206062 validation BCE=0.255514,rank=3\n",
      "[SoftImpute] Iter 21: observed BCE=0.205961 validation BCE=0.256019,rank=3\n",
      "[SoftImpute] Iter 22: observed BCE=0.206026 validation BCE=0.255532,rank=3\n",
      "[SoftImpute] Iter 23: observed BCE=0.205852 validation BCE=0.255473,rank=3\n",
      "[SoftImpute] Iter 24: observed BCE=0.205958 validation BCE=0.255652,rank=3\n",
      "[SoftImpute] Iter 25: observed BCE=0.205867 validation BCE=0.255254,rank=3\n",
      "[SoftImpute] Iter 26: observed BCE=0.205639 validation BCE=0.255793,rank=3\n",
      "[SoftImpute] Iter 27: observed BCE=0.205643 validation BCE=0.255190,rank=3\n",
      "[SoftImpute] Iter 28: observed BCE=0.205675 validation BCE=0.255638,rank=3\n",
      "[SoftImpute] Iter 29: observed BCE=0.205705 validation BCE=0.255753,rank=3\n",
      "[SoftImpute] Iter 30: observed BCE=0.205860 validation BCE=0.255392,rank=3\n",
      "[SoftImpute] Iter 31: observed BCE=0.205480 validation BCE=0.256288,rank=3\n",
      "[SoftImpute] Iter 32: observed BCE=0.205580 validation BCE=0.255524,rank=3\n",
      "[SoftImpute] Iter 33: observed BCE=0.205258 validation BCE=0.256339,rank=3\n",
      "[SoftImpute] Iter 34: observed BCE=0.205396 validation BCE=0.255976,rank=3\n",
      "[SoftImpute] Iter 35: observed BCE=0.205306 validation BCE=0.256238,rank=3\n",
      "[SoftImpute] Iter 36: observed BCE=0.205501 validation BCE=0.255986,rank=3\n",
      "[SoftImpute] Iter 37: observed BCE=0.205226 validation BCE=0.256662,rank=3\n",
      "[SoftImpute] Iter 38: observed BCE=0.205291 validation BCE=0.255926,rank=3\n",
      "[SoftImpute] Iter 39: observed BCE=0.205351 validation BCE=0.256403,rank=3\n",
      "[SoftImpute] Iter 40: observed BCE=0.205295 validation BCE=0.256511,rank=3\n",
      "[SoftImpute] Iter 41: observed BCE=0.205285 validation BCE=0.256175,rank=3\n",
      "[SoftImpute] Iter 42: observed BCE=0.205116 validation BCE=0.256423,rank=3\n",
      "[SoftImpute] Iter 43: observed BCE=0.205006 validation BCE=0.256482,rank=3\n",
      "[SoftImpute] Iter 44: observed BCE=0.205103 validation BCE=0.256384,rank=3\n",
      "[SoftImpute] Iter 45: observed BCE=0.205088 validation BCE=0.256473,rank=3\n",
      "[SoftImpute] Iter 46: observed BCE=0.205124 validation BCE=0.256257,rank=3\n",
      "[SoftImpute] Iter 47: observed BCE=0.204675 validation BCE=0.256740,rank=3\n",
      "[SoftImpute] Iter 48: observed BCE=0.204823 validation BCE=0.256140,rank=3\n",
      "[SoftImpute] Iter 49: observed BCE=0.204804 validation BCE=0.256691,rank=3\n",
      "[SoftImpute] Iter 50: observed BCE=0.204880 validation BCE=0.256266,rank=3\n",
      "[SoftImpute] Iter 51: observed BCE=0.204670 validation BCE=0.256541,rank=3\n",
      "[SoftImpute] Iter 52: observed BCE=0.204855 validation BCE=0.256315,rank=3\n",
      "[SoftImpute] Iter 53: observed BCE=0.204744 validation BCE=0.256843,rank=3\n",
      "[SoftImpute] Iter 54: observed BCE=0.204531 validation BCE=0.256837,rank=3\n",
      "[SoftImpute] Iter 55: observed BCE=0.204830 validation BCE=0.256367,rank=3\n",
      "[SoftImpute] Iter 56: observed BCE=0.204821 validation BCE=0.256092,rank=3\n",
      "[SoftImpute] Iter 57: observed BCE=0.204474 validation BCE=0.256652,rank=3\n",
      "[SoftImpute] Iter 58: observed BCE=0.204499 validation BCE=0.256461,rank=3\n",
      "[SoftImpute] Iter 59: observed BCE=0.204759 validation BCE=0.256799,rank=3\n",
      "[SoftImpute] Iter 60: observed BCE=0.204577 validation BCE=0.256673,rank=3\n",
      "[SoftImpute] Iter 61: observed BCE=0.204723 validation BCE=0.256744,rank=3\n",
      "[SoftImpute] Iter 62: observed BCE=0.204752 validation BCE=0.256851,rank=3\n",
      "[SoftImpute] Iter 63: observed BCE=0.204394 validation BCE=0.256913,rank=3\n",
      "[SoftImpute] Iter 64: observed BCE=0.204323 validation BCE=0.256252,rank=3\n",
      "[SoftImpute] Iter 65: observed BCE=0.204525 validation BCE=0.256699,rank=3\n",
      "[SoftImpute] Iter 66: observed BCE=0.204422 validation BCE=0.257011,rank=3\n",
      "[SoftImpute] Iter 67: observed BCE=0.204406 validation BCE=0.256741,rank=3\n",
      "[SoftImpute] Iter 68: observed BCE=0.204536 validation BCE=0.257237,rank=3\n",
      "[SoftImpute] Iter 69: observed BCE=0.204464 validation BCE=0.256468,rank=3\n",
      "[SoftImpute] Iter 70: observed BCE=0.204302 validation BCE=0.257064,rank=3\n",
      "[SoftImpute] Iter 71: observed BCE=0.204051 validation BCE=0.257493,rank=3\n",
      "[SoftImpute] Iter 72: observed BCE=0.204287 validation BCE=0.256686,rank=3\n",
      "[SoftImpute] Iter 73: observed BCE=0.204136 validation BCE=0.257115,rank=3\n",
      "[SoftImpute] Iter 74: observed BCE=0.204280 validation BCE=0.256895,rank=3\n",
      "[SoftImpute] Iter 75: observed BCE=0.204151 validation BCE=0.257273,rank=3\n",
      "[SoftImpute] Iter 76: observed BCE=0.204358 validation BCE=0.256996,rank=3\n",
      "[SoftImpute] Iter 77: observed BCE=0.204165 validation BCE=0.257177,rank=3\n",
      "[SoftImpute] Iter 78: observed BCE=0.204236 validation BCE=0.257087,rank=3\n",
      "[SoftImpute] Iter 79: observed BCE=0.204178 validation BCE=0.257121,rank=3\n",
      "[SoftImpute] Iter 80: observed BCE=0.204083 validation BCE=0.256662,rank=3\n",
      "[SoftImpute] Iter 81: observed BCE=0.204050 validation BCE=0.257573,rank=3\n",
      "[SoftImpute] Iter 82: observed BCE=0.204278 validation BCE=0.256900,rank=3\n",
      "[SoftImpute] Iter 83: observed BCE=0.204110 validation BCE=0.257552,rank=3\n",
      "[SoftImpute] Iter 84: observed BCE=0.204237 validation BCE=0.256934,rank=3\n",
      "[SoftImpute] Iter 85: observed BCE=0.204143 validation BCE=0.257540,rank=3\n",
      "[SoftImpute] Iter 86: observed BCE=0.204102 validation BCE=0.257391,rank=3\n",
      "[SoftImpute] Iter 87: observed BCE=0.203978 validation BCE=0.257124,rank=3\n",
      "[SoftImpute] Iter 88: observed BCE=0.203889 validation BCE=0.257327,rank=3\n",
      "[SoftImpute] Iter 89: observed BCE=0.203858 validation BCE=0.258304,rank=3\n",
      "[SoftImpute] Iter 90: observed BCE=0.203913 validation BCE=0.257043,rank=3\n",
      "[SoftImpute] Iter 91: observed BCE=0.204147 validation BCE=0.257703,rank=3\n",
      "[SoftImpute] Iter 92: observed BCE=0.204245 validation BCE=0.257425,rank=3\n",
      "[SoftImpute] Iter 93: observed BCE=0.203913 validation BCE=0.257312,rank=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 94: observed BCE=0.204120 validation BCE=0.257209,rank=3\n",
      "[SoftImpute] Iter 95: observed BCE=0.203950 validation BCE=0.257911,rank=3\n",
      "[SoftImpute] Iter 96: observed BCE=0.204244 validation BCE=0.257160,rank=3\n",
      "[SoftImpute] Iter 97: observed BCE=0.203929 validation BCE=0.257498,rank=3\n",
      "[SoftImpute] Iter 98: observed BCE=0.203859 validation BCE=0.257835,rank=3\n",
      "[SoftImpute] Iter 99: observed BCE=0.203829 validation BCE=0.257428,rank=3\n",
      "[SoftImpute] Iter 100: observed BCE=0.204171 validation BCE=0.257519,rank=3\n",
      "[SoftImpute] Stopped after iteration 100 for lambda=0.077147\n",
      "final num of user group: 7\n",
      "final num of item group: 14\n",
      "change mode state : True\n",
      "time cost: 29.41809892654419\n",
      "After the matrix factor stage, training error is 0.20417, validation error is 0.25752\n",
      "1\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68146, val loss: 0.68422\n",
      "Main effects training epoch: 2, train loss: 0.67595, val loss: 0.68221\n",
      "Main effects training epoch: 3, train loss: 0.67072, val loss: 0.67819\n",
      "Main effects training epoch: 4, train loss: 0.66503, val loss: 0.67502\n",
      "Main effects training epoch: 5, train loss: 0.65524, val loss: 0.66894\n",
      "Main effects training epoch: 6, train loss: 0.63803, val loss: 0.65218\n",
      "Main effects training epoch: 7, train loss: 0.60631, val loss: 0.62361\n",
      "Main effects training epoch: 8, train loss: 0.56897, val loss: 0.57964\n",
      "Main effects training epoch: 9, train loss: 0.54055, val loss: 0.55346\n",
      "Main effects training epoch: 10, train loss: 0.53431, val loss: 0.54291\n",
      "Main effects training epoch: 11, train loss: 0.53189, val loss: 0.54189\n",
      "Main effects training epoch: 12, train loss: 0.52954, val loss: 0.54691\n",
      "Main effects training epoch: 13, train loss: 0.52744, val loss: 0.53984\n",
      "Main effects training epoch: 14, train loss: 0.52638, val loss: 0.54117\n",
      "Main effects training epoch: 15, train loss: 0.52599, val loss: 0.53909\n",
      "Main effects training epoch: 16, train loss: 0.52564, val loss: 0.54070\n",
      "Main effects training epoch: 17, train loss: 0.52551, val loss: 0.54059\n",
      "Main effects training epoch: 18, train loss: 0.52543, val loss: 0.53860\n",
      "Main effects training epoch: 19, train loss: 0.52614, val loss: 0.54011\n",
      "Main effects training epoch: 20, train loss: 0.52528, val loss: 0.54095\n",
      "Main effects training epoch: 21, train loss: 0.52504, val loss: 0.53870\n",
      "Main effects training epoch: 22, train loss: 0.52495, val loss: 0.54030\n",
      "Main effects training epoch: 23, train loss: 0.52476, val loss: 0.53969\n",
      "Main effects training epoch: 24, train loss: 0.52497, val loss: 0.53885\n",
      "Main effects training epoch: 25, train loss: 0.52546, val loss: 0.54101\n",
      "Main effects training epoch: 26, train loss: 0.52562, val loss: 0.53829\n",
      "Main effects training epoch: 27, train loss: 0.52501, val loss: 0.54017\n",
      "Main effects training epoch: 28, train loss: 0.52509, val loss: 0.53814\n",
      "Main effects training epoch: 29, train loss: 0.52464, val loss: 0.53832\n",
      "Main effects training epoch: 30, train loss: 0.52447, val loss: 0.53899\n",
      "Main effects training epoch: 31, train loss: 0.52455, val loss: 0.53995\n",
      "Main effects training epoch: 32, train loss: 0.52438, val loss: 0.53927\n",
      "Main effects training epoch: 33, train loss: 0.52432, val loss: 0.53792\n",
      "Main effects training epoch: 34, train loss: 0.52508, val loss: 0.54003\n",
      "Main effects training epoch: 35, train loss: 0.52598, val loss: 0.54006\n",
      "Main effects training epoch: 36, train loss: 0.52520, val loss: 0.53966\n",
      "Main effects training epoch: 37, train loss: 0.52511, val loss: 0.54079\n",
      "Main effects training epoch: 38, train loss: 0.52426, val loss: 0.53788\n",
      "Main effects training epoch: 39, train loss: 0.52433, val loss: 0.53750\n",
      "Main effects training epoch: 40, train loss: 0.52415, val loss: 0.53893\n",
      "Main effects training epoch: 41, train loss: 0.52406, val loss: 0.53854\n",
      "Main effects training epoch: 42, train loss: 0.52410, val loss: 0.53863\n",
      "Main effects training epoch: 43, train loss: 0.52417, val loss: 0.53778\n",
      "Main effects training epoch: 44, train loss: 0.52391, val loss: 0.53835\n",
      "Main effects training epoch: 45, train loss: 0.52390, val loss: 0.53774\n",
      "Main effects training epoch: 46, train loss: 0.52393, val loss: 0.53836\n",
      "Main effects training epoch: 47, train loss: 0.52412, val loss: 0.53872\n",
      "Main effects training epoch: 48, train loss: 0.52448, val loss: 0.53895\n",
      "Main effects training epoch: 49, train loss: 0.52396, val loss: 0.53785\n",
      "Main effects training epoch: 50, train loss: 0.52435, val loss: 0.53908\n",
      "Main effects training epoch: 51, train loss: 0.52469, val loss: 0.53964\n",
      "Main effects training epoch: 52, train loss: 0.52388, val loss: 0.53725\n",
      "Main effects training epoch: 53, train loss: 0.52462, val loss: 0.53865\n",
      "Main effects training epoch: 54, train loss: 0.52358, val loss: 0.53807\n",
      "Main effects training epoch: 55, train loss: 0.52393, val loss: 0.53952\n",
      "Main effects training epoch: 56, train loss: 0.52404, val loss: 0.53800\n",
      "Main effects training epoch: 57, train loss: 0.52422, val loss: 0.54028\n",
      "Main effects training epoch: 58, train loss: 0.52450, val loss: 0.53693\n",
      "Main effects training epoch: 59, train loss: 0.52415, val loss: 0.53894\n",
      "Main effects training epoch: 60, train loss: 0.52396, val loss: 0.53707\n",
      "Main effects training epoch: 61, train loss: 0.52410, val loss: 0.54016\n",
      "Main effects training epoch: 62, train loss: 0.52460, val loss: 0.53758\n",
      "Main effects training epoch: 63, train loss: 0.52442, val loss: 0.53857\n",
      "Main effects training epoch: 64, train loss: 0.52386, val loss: 0.53915\n",
      "Main effects training epoch: 65, train loss: 0.52351, val loss: 0.53747\n",
      "Main effects training epoch: 66, train loss: 0.52468, val loss: 0.54229\n",
      "Main effects training epoch: 67, train loss: 0.52402, val loss: 0.53698\n",
      "Main effects training epoch: 68, train loss: 0.52338, val loss: 0.53821\n",
      "Main effects training epoch: 69, train loss: 0.52331, val loss: 0.53847\n",
      "Main effects training epoch: 70, train loss: 0.52346, val loss: 0.53787\n",
      "Main effects training epoch: 71, train loss: 0.52338, val loss: 0.53794\n",
      "Main effects training epoch: 72, train loss: 0.52332, val loss: 0.53662\n",
      "Main effects training epoch: 73, train loss: 0.52317, val loss: 0.53818\n",
      "Main effects training epoch: 74, train loss: 0.52306, val loss: 0.53751\n",
      "Main effects training epoch: 75, train loss: 0.52314, val loss: 0.53800\n",
      "Main effects training epoch: 76, train loss: 0.52329, val loss: 0.53802\n",
      "Main effects training epoch: 77, train loss: 0.52338, val loss: 0.53699\n",
      "Main effects training epoch: 78, train loss: 0.52298, val loss: 0.53742\n",
      "Main effects training epoch: 79, train loss: 0.52320, val loss: 0.53714\n",
      "Main effects training epoch: 80, train loss: 0.52372, val loss: 0.53876\n",
      "Main effects training epoch: 81, train loss: 0.52301, val loss: 0.53660\n",
      "Main effects training epoch: 82, train loss: 0.52330, val loss: 0.53751\n",
      "Main effects training epoch: 83, train loss: 0.52381, val loss: 0.53816\n",
      "Main effects training epoch: 84, train loss: 0.52298, val loss: 0.53777\n",
      "Main effects training epoch: 85, train loss: 0.52329, val loss: 0.53671\n",
      "Main effects training epoch: 86, train loss: 0.52309, val loss: 0.53722\n",
      "Main effects training epoch: 87, train loss: 0.52293, val loss: 0.53703\n",
      "Main effects training epoch: 88, train loss: 0.52275, val loss: 0.53801\n",
      "Main effects training epoch: 89, train loss: 0.52266, val loss: 0.53684\n",
      "Main effects training epoch: 90, train loss: 0.52295, val loss: 0.53870\n",
      "Main effects training epoch: 91, train loss: 0.52292, val loss: 0.53564\n",
      "Main effects training epoch: 92, train loss: 0.52294, val loss: 0.53885\n",
      "Main effects training epoch: 93, train loss: 0.52308, val loss: 0.53654\n",
      "Main effects training epoch: 94, train loss: 0.52304, val loss: 0.53877\n",
      "Main effects training epoch: 95, train loss: 0.52288, val loss: 0.53617\n",
      "Main effects training epoch: 96, train loss: 0.52275, val loss: 0.53792\n",
      "Main effects training epoch: 97, train loss: 0.52246, val loss: 0.53674\n",
      "Main effects training epoch: 98, train loss: 0.52255, val loss: 0.53779\n",
      "Main effects training epoch: 99, train loss: 0.52268, val loss: 0.53720\n",
      "Main effects training epoch: 100, train loss: 0.52247, val loss: 0.53644\n",
      "Main effects training epoch: 101, train loss: 0.52221, val loss: 0.53683\n",
      "Main effects training epoch: 102, train loss: 0.52297, val loss: 0.53655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 103, train loss: 0.52261, val loss: 0.53725\n",
      "Main effects training epoch: 104, train loss: 0.52237, val loss: 0.53792\n",
      "Main effects training epoch: 105, train loss: 0.52288, val loss: 0.53701\n",
      "Main effects training epoch: 106, train loss: 0.52268, val loss: 0.53614\n",
      "Main effects training epoch: 107, train loss: 0.52226, val loss: 0.53676\n",
      "Main effects training epoch: 108, train loss: 0.52203, val loss: 0.53635\n",
      "Main effects training epoch: 109, train loss: 0.52196, val loss: 0.53704\n",
      "Main effects training epoch: 110, train loss: 0.52210, val loss: 0.53665\n",
      "Main effects training epoch: 111, train loss: 0.52195, val loss: 0.53586\n",
      "Main effects training epoch: 112, train loss: 0.52194, val loss: 0.53779\n",
      "Main effects training epoch: 113, train loss: 0.52208, val loss: 0.53588\n",
      "Main effects training epoch: 114, train loss: 0.52176, val loss: 0.53648\n",
      "Main effects training epoch: 115, train loss: 0.52206, val loss: 0.53562\n",
      "Main effects training epoch: 116, train loss: 0.52175, val loss: 0.53701\n",
      "Main effects training epoch: 117, train loss: 0.52190, val loss: 0.53523\n",
      "Main effects training epoch: 118, train loss: 0.52231, val loss: 0.53665\n",
      "Main effects training epoch: 119, train loss: 0.52192, val loss: 0.53638\n",
      "Main effects training epoch: 120, train loss: 0.52164, val loss: 0.53696\n",
      "Main effects training epoch: 121, train loss: 0.52156, val loss: 0.53591\n",
      "Main effects training epoch: 122, train loss: 0.52172, val loss: 0.53669\n",
      "Main effects training epoch: 123, train loss: 0.52152, val loss: 0.53492\n",
      "Main effects training epoch: 124, train loss: 0.52148, val loss: 0.53673\n",
      "Main effects training epoch: 125, train loss: 0.52154, val loss: 0.53527\n",
      "Main effects training epoch: 126, train loss: 0.52132, val loss: 0.53516\n",
      "Main effects training epoch: 127, train loss: 0.52137, val loss: 0.53599\n",
      "Main effects training epoch: 128, train loss: 0.52149, val loss: 0.53556\n",
      "Main effects training epoch: 129, train loss: 0.52140, val loss: 0.53648\n",
      "Main effects training epoch: 130, train loss: 0.52115, val loss: 0.53583\n",
      "Main effects training epoch: 131, train loss: 0.52158, val loss: 0.53429\n",
      "Main effects training epoch: 132, train loss: 0.52193, val loss: 0.53510\n",
      "Main effects training epoch: 133, train loss: 0.52168, val loss: 0.53763\n",
      "Main effects training epoch: 134, train loss: 0.52129, val loss: 0.53529\n",
      "Main effects training epoch: 135, train loss: 0.52112, val loss: 0.53625\n",
      "Main effects training epoch: 136, train loss: 0.52113, val loss: 0.53565\n",
      "Main effects training epoch: 137, train loss: 0.52104, val loss: 0.53447\n",
      "Main effects training epoch: 138, train loss: 0.52110, val loss: 0.53568\n",
      "Main effects training epoch: 139, train loss: 0.52118, val loss: 0.53422\n",
      "Main effects training epoch: 140, train loss: 0.52091, val loss: 0.53583\n",
      "Main effects training epoch: 141, train loss: 0.52100, val loss: 0.53472\n",
      "Main effects training epoch: 142, train loss: 0.52085, val loss: 0.53489\n",
      "Main effects training epoch: 143, train loss: 0.52076, val loss: 0.53559\n",
      "Main effects training epoch: 144, train loss: 0.52082, val loss: 0.53456\n",
      "Main effects training epoch: 145, train loss: 0.52085, val loss: 0.53625\n",
      "Main effects training epoch: 146, train loss: 0.52083, val loss: 0.53565\n",
      "Main effects training epoch: 147, train loss: 0.52108, val loss: 0.53612\n",
      "Main effects training epoch: 148, train loss: 0.52086, val loss: 0.53462\n",
      "Main effects training epoch: 149, train loss: 0.52114, val loss: 0.53554\n",
      "Main effects training epoch: 150, train loss: 0.52102, val loss: 0.53490\n",
      "Main effects training epoch: 151, train loss: 0.52085, val loss: 0.53621\n",
      "Main effects training epoch: 152, train loss: 0.52042, val loss: 0.53486\n",
      "Main effects training epoch: 153, train loss: 0.52034, val loss: 0.53490\n",
      "Main effects training epoch: 154, train loss: 0.52030, val loss: 0.53459\n",
      "Main effects training epoch: 155, train loss: 0.52085, val loss: 0.53371\n",
      "Main effects training epoch: 156, train loss: 0.52068, val loss: 0.53627\n",
      "Main effects training epoch: 157, train loss: 0.52045, val loss: 0.53495\n",
      "Main effects training epoch: 158, train loss: 0.52009, val loss: 0.53486\n",
      "Main effects training epoch: 159, train loss: 0.52011, val loss: 0.53396\n",
      "Main effects training epoch: 160, train loss: 0.52005, val loss: 0.53501\n",
      "Main effects training epoch: 161, train loss: 0.52007, val loss: 0.53490\n",
      "Main effects training epoch: 162, train loss: 0.51996, val loss: 0.53490\n",
      "Main effects training epoch: 163, train loss: 0.51992, val loss: 0.53461\n",
      "Main effects training epoch: 164, train loss: 0.51996, val loss: 0.53501\n",
      "Main effects training epoch: 165, train loss: 0.51991, val loss: 0.53544\n",
      "Main effects training epoch: 166, train loss: 0.51987, val loss: 0.53326\n",
      "Main effects training epoch: 167, train loss: 0.51987, val loss: 0.53420\n",
      "Main effects training epoch: 168, train loss: 0.51975, val loss: 0.53461\n",
      "Main effects training epoch: 169, train loss: 0.51982, val loss: 0.53570\n",
      "Main effects training epoch: 170, train loss: 0.51995, val loss: 0.53395\n",
      "Main effects training epoch: 171, train loss: 0.51984, val loss: 0.53512\n",
      "Main effects training epoch: 172, train loss: 0.51984, val loss: 0.53370\n",
      "Main effects training epoch: 173, train loss: 0.51960, val loss: 0.53425\n",
      "Main effects training epoch: 174, train loss: 0.51957, val loss: 0.53400\n",
      "Main effects training epoch: 175, train loss: 0.51962, val loss: 0.53476\n",
      "Main effects training epoch: 176, train loss: 0.51944, val loss: 0.53417\n",
      "Main effects training epoch: 177, train loss: 0.51941, val loss: 0.53466\n",
      "Main effects training epoch: 178, train loss: 0.51949, val loss: 0.53494\n",
      "Main effects training epoch: 179, train loss: 0.51940, val loss: 0.53356\n",
      "Main effects training epoch: 180, train loss: 0.51927, val loss: 0.53362\n",
      "Main effects training epoch: 181, train loss: 0.51934, val loss: 0.53491\n",
      "Main effects training epoch: 182, train loss: 0.51974, val loss: 0.53388\n",
      "Main effects training epoch: 183, train loss: 0.51978, val loss: 0.53572\n",
      "Main effects training epoch: 184, train loss: 0.51969, val loss: 0.53537\n",
      "Main effects training epoch: 185, train loss: 0.51943, val loss: 0.53291\n",
      "Main effects training epoch: 186, train loss: 0.51939, val loss: 0.53519\n",
      "Main effects training epoch: 187, train loss: 0.51912, val loss: 0.53313\n",
      "Main effects training epoch: 188, train loss: 0.51918, val loss: 0.53387\n",
      "Main effects training epoch: 189, train loss: 0.51910, val loss: 0.53481\n",
      "Main effects training epoch: 190, train loss: 0.51908, val loss: 0.53428\n",
      "Main effects training epoch: 191, train loss: 0.51903, val loss: 0.53357\n",
      "Main effects training epoch: 192, train loss: 0.51906, val loss: 0.53286\n",
      "Main effects training epoch: 193, train loss: 0.51892, val loss: 0.53548\n",
      "Main effects training epoch: 194, train loss: 0.51900, val loss: 0.53380\n",
      "Main effects training epoch: 195, train loss: 0.51886, val loss: 0.53357\n",
      "Main effects training epoch: 196, train loss: 0.51881, val loss: 0.53315\n",
      "Main effects training epoch: 197, train loss: 0.51876, val loss: 0.53409\n",
      "Main effects training epoch: 198, train loss: 0.51881, val loss: 0.53408\n",
      "Main effects training epoch: 199, train loss: 0.51881, val loss: 0.53355\n",
      "Main effects training epoch: 200, train loss: 0.51860, val loss: 0.53359\n",
      "Main effects training epoch: 201, train loss: 0.51871, val loss: 0.53360\n",
      "Main effects training epoch: 202, train loss: 0.51880, val loss: 0.53409\n",
      "Main effects training epoch: 203, train loss: 0.51864, val loss: 0.53488\n",
      "Main effects training epoch: 204, train loss: 0.51900, val loss: 0.53437\n",
      "Main effects training epoch: 205, train loss: 0.51864, val loss: 0.53389\n",
      "Main effects training epoch: 206, train loss: 0.51855, val loss: 0.53409\n",
      "Main effects training epoch: 207, train loss: 0.51845, val loss: 0.53317\n",
      "Main effects training epoch: 208, train loss: 0.51845, val loss: 0.53363\n",
      "Main effects training epoch: 209, train loss: 0.51895, val loss: 0.53498\n",
      "Main effects training epoch: 210, train loss: 0.51860, val loss: 0.53323\n",
      "Main effects training epoch: 211, train loss: 0.51881, val loss: 0.53485\n",
      "Main effects training epoch: 212, train loss: 0.51846, val loss: 0.53267\n",
      "Main effects training epoch: 213, train loss: 0.51827, val loss: 0.53487\n",
      "Main effects training epoch: 214, train loss: 0.51818, val loss: 0.53410\n",
      "Main effects training epoch: 215, train loss: 0.51839, val loss: 0.53348\n",
      "Main effects training epoch: 216, train loss: 0.51841, val loss: 0.53462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 217, train loss: 0.51845, val loss: 0.53263\n",
      "Main effects training epoch: 218, train loss: 0.51832, val loss: 0.53428\n",
      "Main effects training epoch: 219, train loss: 0.51822, val loss: 0.53306\n",
      "Main effects training epoch: 220, train loss: 0.51842, val loss: 0.53432\n",
      "Main effects training epoch: 221, train loss: 0.51862, val loss: 0.53663\n",
      "Main effects training epoch: 222, train loss: 0.51880, val loss: 0.53483\n",
      "Main effects training epoch: 223, train loss: 0.51858, val loss: 0.53406\n",
      "Main effects training epoch: 224, train loss: 0.51839, val loss: 0.53451\n",
      "Main effects training epoch: 225, train loss: 0.51819, val loss: 0.53337\n",
      "Main effects training epoch: 226, train loss: 0.51833, val loss: 0.53274\n",
      "Main effects training epoch: 227, train loss: 0.51811, val loss: 0.53502\n",
      "Main effects training epoch: 228, train loss: 0.51816, val loss: 0.53299\n",
      "Main effects training epoch: 229, train loss: 0.51822, val loss: 0.53394\n",
      "Main effects training epoch: 230, train loss: 0.51830, val loss: 0.53438\n",
      "Main effects training epoch: 231, train loss: 0.51781, val loss: 0.53396\n",
      "Main effects training epoch: 232, train loss: 0.51785, val loss: 0.53403\n",
      "Main effects training epoch: 233, train loss: 0.51785, val loss: 0.53326\n",
      "Main effects training epoch: 234, train loss: 0.51778, val loss: 0.53291\n",
      "Main effects training epoch: 235, train loss: 0.51786, val loss: 0.53415\n",
      "Main effects training epoch: 236, train loss: 0.51803, val loss: 0.53415\n",
      "Main effects training epoch: 237, train loss: 0.51814, val loss: 0.53376\n",
      "Main effects training epoch: 238, train loss: 0.51835, val loss: 0.53677\n",
      "Main effects training epoch: 239, train loss: 0.51788, val loss: 0.53206\n",
      "Main effects training epoch: 240, train loss: 0.51814, val loss: 0.53548\n",
      "Main effects training epoch: 241, train loss: 0.51805, val loss: 0.53174\n",
      "Main effects training epoch: 242, train loss: 0.51773, val loss: 0.53451\n",
      "Main effects training epoch: 243, train loss: 0.51767, val loss: 0.53342\n",
      "Main effects training epoch: 244, train loss: 0.51842, val loss: 0.53473\n",
      "Main effects training epoch: 245, train loss: 0.51815, val loss: 0.53444\n",
      "Main effects training epoch: 246, train loss: 0.51795, val loss: 0.53316\n",
      "Main effects training epoch: 247, train loss: 0.51760, val loss: 0.53410\n",
      "Main effects training epoch: 248, train loss: 0.51805, val loss: 0.53402\n",
      "Main effects training epoch: 249, train loss: 0.51758, val loss: 0.53402\n",
      "Main effects training epoch: 250, train loss: 0.51728, val loss: 0.53296\n",
      "Main effects training epoch: 251, train loss: 0.51776, val loss: 0.53374\n",
      "Main effects training epoch: 252, train loss: 0.51774, val loss: 0.53383\n",
      "Main effects training epoch: 253, train loss: 0.51734, val loss: 0.53465\n",
      "Main effects training epoch: 254, train loss: 0.51738, val loss: 0.53276\n",
      "Main effects training epoch: 255, train loss: 0.51742, val loss: 0.53391\n",
      "Main effects training epoch: 256, train loss: 0.51766, val loss: 0.53330\n",
      "Main effects training epoch: 257, train loss: 0.51760, val loss: 0.53477\n",
      "Main effects training epoch: 258, train loss: 0.51776, val loss: 0.53443\n",
      "Main effects training epoch: 259, train loss: 0.51750, val loss: 0.53526\n",
      "Main effects training epoch: 260, train loss: 0.51763, val loss: 0.53384\n",
      "Main effects training epoch: 261, train loss: 0.51740, val loss: 0.53356\n",
      "Main effects training epoch: 262, train loss: 0.51772, val loss: 0.53395\n",
      "Main effects training epoch: 263, train loss: 0.51767, val loss: 0.53542\n",
      "Main effects training epoch: 264, train loss: 0.51792, val loss: 0.53402\n",
      "Main effects training epoch: 265, train loss: 0.51736, val loss: 0.53444\n",
      "Main effects training epoch: 266, train loss: 0.51722, val loss: 0.53370\n",
      "Main effects training epoch: 267, train loss: 0.51737, val loss: 0.53315\n",
      "Main effects training epoch: 268, train loss: 0.51709, val loss: 0.53455\n",
      "Main effects training epoch: 269, train loss: 0.51727, val loss: 0.53423\n",
      "Main effects training epoch: 270, train loss: 0.51707, val loss: 0.53445\n",
      "Main effects training epoch: 271, train loss: 0.51685, val loss: 0.53295\n",
      "Main effects training epoch: 272, train loss: 0.51695, val loss: 0.53373\n",
      "Main effects training epoch: 273, train loss: 0.51728, val loss: 0.53502\n",
      "Main effects training epoch: 274, train loss: 0.51674, val loss: 0.53308\n",
      "Main effects training epoch: 275, train loss: 0.51694, val loss: 0.53379\n",
      "Main effects training epoch: 276, train loss: 0.51713, val loss: 0.53285\n",
      "Main effects training epoch: 277, train loss: 0.51693, val loss: 0.53539\n",
      "Main effects training epoch: 278, train loss: 0.51675, val loss: 0.53353\n",
      "Main effects training epoch: 279, train loss: 0.51673, val loss: 0.53403\n",
      "Main effects training epoch: 280, train loss: 0.51664, val loss: 0.53331\n",
      "Main effects training epoch: 281, train loss: 0.51698, val loss: 0.53379\n",
      "Main effects training epoch: 282, train loss: 0.51655, val loss: 0.53319\n",
      "Main effects training epoch: 283, train loss: 0.51676, val loss: 0.53402\n",
      "Main effects training epoch: 284, train loss: 0.51678, val loss: 0.53403\n",
      "Main effects training epoch: 285, train loss: 0.51650, val loss: 0.53418\n",
      "Main effects training epoch: 286, train loss: 0.51651, val loss: 0.53269\n",
      "Main effects training epoch: 287, train loss: 0.51682, val loss: 0.53477\n",
      "Main effects training epoch: 288, train loss: 0.51668, val loss: 0.53365\n",
      "Main effects training epoch: 289, train loss: 0.51673, val loss: 0.53456\n",
      "Main effects training epoch: 290, train loss: 0.51680, val loss: 0.53299\n",
      "Main effects training epoch: 291, train loss: 0.51708, val loss: 0.53547\n",
      "Main effects training epoch: 292, train loss: 0.51644, val loss: 0.53264\n",
      "Main effects training epoch: 293, train loss: 0.51650, val loss: 0.53288\n",
      "Main effects training epoch: 294, train loss: 0.51630, val loss: 0.53350\n",
      "Main effects training epoch: 295, train loss: 0.51666, val loss: 0.53292\n",
      "Main effects training epoch: 296, train loss: 0.51642, val loss: 0.53441\n",
      "Main effects training epoch: 297, train loss: 0.51644, val loss: 0.53275\n",
      "Main effects training epoch: 298, train loss: 0.51665, val loss: 0.53570\n",
      "Main effects training epoch: 299, train loss: 0.51677, val loss: 0.53332\n",
      "Main effects training epoch: 300, train loss: 0.51700, val loss: 0.53383\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51930, val loss: 0.53353\n",
      "Main effects tuning epoch: 2, train loss: 0.51923, val loss: 0.53381\n",
      "Main effects tuning epoch: 3, train loss: 0.51919, val loss: 0.53375\n",
      "Main effects tuning epoch: 4, train loss: 0.51905, val loss: 0.53334\n",
      "Main effects tuning epoch: 5, train loss: 0.51923, val loss: 0.53507\n",
      "Main effects tuning epoch: 6, train loss: 0.51922, val loss: 0.53387\n",
      "Main effects tuning epoch: 7, train loss: 0.51903, val loss: 0.53381\n",
      "Main effects tuning epoch: 8, train loss: 0.51907, val loss: 0.53229\n",
      "Main effects tuning epoch: 9, train loss: 0.51896, val loss: 0.53302\n",
      "Main effects tuning epoch: 10, train loss: 0.51924, val loss: 0.53512\n",
      "Main effects tuning epoch: 11, train loss: 0.51888, val loss: 0.53381\n",
      "Main effects tuning epoch: 12, train loss: 0.51899, val loss: 0.53347\n",
      "Main effects tuning epoch: 13, train loss: 0.51907, val loss: 0.53312\n",
      "Main effects tuning epoch: 14, train loss: 0.51895, val loss: 0.53379\n",
      "Main effects tuning epoch: 15, train loss: 0.51889, val loss: 0.53321\n",
      "Main effects tuning epoch: 16, train loss: 0.51887, val loss: 0.53377\n",
      "Main effects tuning epoch: 17, train loss: 0.51947, val loss: 0.53490\n",
      "Main effects tuning epoch: 18, train loss: 0.51959, val loss: 0.53399\n",
      "Main effects tuning epoch: 19, train loss: 0.51918, val loss: 0.53406\n",
      "Main effects tuning epoch: 20, train loss: 0.51936, val loss: 0.53604\n",
      "Main effects tuning epoch: 21, train loss: 0.51912, val loss: 0.53266\n",
      "Main effects tuning epoch: 22, train loss: 0.51905, val loss: 0.53389\n",
      "Main effects tuning epoch: 23, train loss: 0.51885, val loss: 0.53369\n",
      "Main effects tuning epoch: 24, train loss: 0.51879, val loss: 0.53315\n",
      "Main effects tuning epoch: 25, train loss: 0.51867, val loss: 0.53412\n",
      "Main effects tuning epoch: 26, train loss: 0.51867, val loss: 0.53310\n",
      "Main effects tuning epoch: 27, train loss: 0.51875, val loss: 0.53368\n",
      "Main effects tuning epoch: 28, train loss: 0.51905, val loss: 0.53286\n",
      "Main effects tuning epoch: 29, train loss: 0.51910, val loss: 0.53608\n",
      "Main effects tuning epoch: 30, train loss: 0.51868, val loss: 0.53250\n",
      "Main effects tuning epoch: 31, train loss: 0.51857, val loss: 0.53367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects tuning epoch: 32, train loss: 0.51855, val loss: 0.53382\n",
      "Main effects tuning epoch: 33, train loss: 0.51910, val loss: 0.53397\n",
      "Main effects tuning epoch: 34, train loss: 0.51885, val loss: 0.53496\n",
      "Main effects tuning epoch: 35, train loss: 0.51896, val loss: 0.53327\n",
      "Main effects tuning epoch: 36, train loss: 0.51893, val loss: 0.53475\n",
      "Main effects tuning epoch: 37, train loss: 0.51892, val loss: 0.53248\n",
      "Main effects tuning epoch: 38, train loss: 0.51865, val loss: 0.53509\n",
      "Main effects tuning epoch: 39, train loss: 0.51869, val loss: 0.53333\n",
      "Main effects tuning epoch: 40, train loss: 0.51879, val loss: 0.53448\n",
      "Main effects tuning epoch: 41, train loss: 0.51842, val loss: 0.53348\n",
      "Main effects tuning epoch: 42, train loss: 0.51856, val loss: 0.53269\n",
      "Main effects tuning epoch: 43, train loss: 0.51834, val loss: 0.53384\n",
      "Main effects tuning epoch: 44, train loss: 0.51837, val loss: 0.53363\n",
      "Main effects tuning epoch: 45, train loss: 0.51833, val loss: 0.53321\n",
      "Main effects tuning epoch: 46, train loss: 0.51852, val loss: 0.53489\n",
      "Main effects tuning epoch: 47, train loss: 0.51859, val loss: 0.53323\n",
      "Main effects tuning epoch: 48, train loss: 0.51832, val loss: 0.53263\n",
      "Main effects tuning epoch: 49, train loss: 0.51861, val loss: 0.53457\n",
      "Main effects tuning epoch: 50, train loss: 0.51846, val loss: 0.53368\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.50083, val loss: 0.50894\n",
      "Interaction training epoch: 2, train loss: 0.36983, val loss: 0.35572\n",
      "Interaction training epoch: 3, train loss: 0.33576, val loss: 0.33144\n",
      "Interaction training epoch: 4, train loss: 0.31920, val loss: 0.31099\n",
      "Interaction training epoch: 5, train loss: 0.30678, val loss: 0.29925\n",
      "Interaction training epoch: 6, train loss: 0.30115, val loss: 0.29159\n",
      "Interaction training epoch: 7, train loss: 0.30142, val loss: 0.29445\n",
      "Interaction training epoch: 8, train loss: 0.29572, val loss: 0.28241\n",
      "Interaction training epoch: 9, train loss: 0.29471, val loss: 0.28503\n",
      "Interaction training epoch: 10, train loss: 0.28873, val loss: 0.27805\n",
      "Interaction training epoch: 11, train loss: 0.29312, val loss: 0.28521\n",
      "Interaction training epoch: 12, train loss: 0.29171, val loss: 0.28399\n",
      "Interaction training epoch: 13, train loss: 0.29179, val loss: 0.28439\n",
      "Interaction training epoch: 14, train loss: 0.29480, val loss: 0.28711\n",
      "Interaction training epoch: 15, train loss: 0.28830, val loss: 0.28121\n",
      "Interaction training epoch: 16, train loss: 0.29191, val loss: 0.28520\n",
      "Interaction training epoch: 17, train loss: 0.29095, val loss: 0.28487\n",
      "Interaction training epoch: 18, train loss: 0.28913, val loss: 0.28185\n",
      "Interaction training epoch: 19, train loss: 0.28588, val loss: 0.28282\n",
      "Interaction training epoch: 20, train loss: 0.29292, val loss: 0.29100\n",
      "Interaction training epoch: 21, train loss: 0.28724, val loss: 0.28553\n",
      "Interaction training epoch: 22, train loss: 0.28825, val loss: 0.28657\n",
      "Interaction training epoch: 23, train loss: 0.29413, val loss: 0.29613\n",
      "Interaction training epoch: 24, train loss: 0.28769, val loss: 0.28458\n",
      "Interaction training epoch: 25, train loss: 0.28562, val loss: 0.28509\n",
      "Interaction training epoch: 26, train loss: 0.28351, val loss: 0.28196\n",
      "Interaction training epoch: 27, train loss: 0.28464, val loss: 0.28287\n",
      "Interaction training epoch: 28, train loss: 0.28199, val loss: 0.27900\n",
      "Interaction training epoch: 29, train loss: 0.28403, val loss: 0.28398\n",
      "Interaction training epoch: 30, train loss: 0.28522, val loss: 0.28550\n",
      "Interaction training epoch: 31, train loss: 0.28337, val loss: 0.28450\n",
      "Interaction training epoch: 32, train loss: 0.28479, val loss: 0.28617\n",
      "Interaction training epoch: 33, train loss: 0.28630, val loss: 0.28858\n",
      "Interaction training epoch: 34, train loss: 0.28366, val loss: 0.28540\n",
      "Interaction training epoch: 35, train loss: 0.28531, val loss: 0.28944\n",
      "Interaction training epoch: 36, train loss: 0.28235, val loss: 0.28583\n",
      "Interaction training epoch: 37, train loss: 0.28046, val loss: 0.28163\n",
      "Interaction training epoch: 38, train loss: 0.28272, val loss: 0.28681\n",
      "Interaction training epoch: 39, train loss: 0.28035, val loss: 0.28235\n",
      "Interaction training epoch: 40, train loss: 0.27911, val loss: 0.28147\n",
      "Interaction training epoch: 41, train loss: 0.28332, val loss: 0.28844\n",
      "Interaction training epoch: 42, train loss: 0.27727, val loss: 0.28060\n",
      "Interaction training epoch: 43, train loss: 0.28073, val loss: 0.28513\n",
      "Interaction training epoch: 44, train loss: 0.27784, val loss: 0.28273\n",
      "Interaction training epoch: 45, train loss: 0.28349, val loss: 0.28959\n",
      "Interaction training epoch: 46, train loss: 0.28045, val loss: 0.28994\n",
      "Interaction training epoch: 47, train loss: 0.27865, val loss: 0.28281\n",
      "Interaction training epoch: 48, train loss: 0.28352, val loss: 0.28966\n",
      "Interaction training epoch: 49, train loss: 0.27885, val loss: 0.28626\n",
      "Interaction training epoch: 50, train loss: 0.27817, val loss: 0.28464\n",
      "Interaction training epoch: 51, train loss: 0.27831, val loss: 0.28539\n",
      "Interaction training epoch: 52, train loss: 0.27749, val loss: 0.28684\n",
      "Interaction training epoch: 53, train loss: 0.27836, val loss: 0.28503\n",
      "Interaction training epoch: 54, train loss: 0.27792, val loss: 0.28879\n",
      "Interaction training epoch: 55, train loss: 0.27644, val loss: 0.28288\n",
      "Interaction training epoch: 56, train loss: 0.27556, val loss: 0.28660\n",
      "Interaction training epoch: 57, train loss: 0.28078, val loss: 0.28798\n",
      "Interaction training epoch: 58, train loss: 0.27475, val loss: 0.28273\n",
      "Interaction training epoch: 59, train loss: 0.28114, val loss: 0.29334\n",
      "Interaction training epoch: 60, train loss: 0.27724, val loss: 0.28827\n",
      "Interaction training epoch: 61, train loss: 0.27463, val loss: 0.28352\n",
      "Interaction training epoch: 62, train loss: 0.27885, val loss: 0.28751\n",
      "Interaction training epoch: 63, train loss: 0.27499, val loss: 0.28557\n",
      "Interaction training epoch: 64, train loss: 0.27529, val loss: 0.28756\n",
      "Interaction training epoch: 65, train loss: 0.27619, val loss: 0.28416\n",
      "Interaction training epoch: 66, train loss: 0.27352, val loss: 0.28364\n",
      "Interaction training epoch: 67, train loss: 0.27827, val loss: 0.29052\n",
      "Interaction training epoch: 68, train loss: 0.27319, val loss: 0.28426\n",
      "Interaction training epoch: 69, train loss: 0.27448, val loss: 0.28885\n",
      "Interaction training epoch: 70, train loss: 0.27518, val loss: 0.28614\n",
      "Interaction training epoch: 71, train loss: 0.27369, val loss: 0.28504\n",
      "Interaction training epoch: 72, train loss: 0.27752, val loss: 0.29284\n",
      "Interaction training epoch: 73, train loss: 0.27100, val loss: 0.28268\n",
      "Interaction training epoch: 74, train loss: 0.27393, val loss: 0.28780\n",
      "Interaction training epoch: 75, train loss: 0.27351, val loss: 0.28994\n",
      "Interaction training epoch: 76, train loss: 0.27135, val loss: 0.28135\n",
      "Interaction training epoch: 77, train loss: 0.27217, val loss: 0.28505\n",
      "Interaction training epoch: 78, train loss: 0.27417, val loss: 0.28815\n",
      "Interaction training epoch: 79, train loss: 0.27055, val loss: 0.28326\n",
      "Interaction training epoch: 80, train loss: 0.27540, val loss: 0.28952\n",
      "Interaction training epoch: 81, train loss: 0.26854, val loss: 0.27951\n",
      "Interaction training epoch: 82, train loss: 0.27364, val loss: 0.28728\n",
      "Interaction training epoch: 83, train loss: 0.27352, val loss: 0.28758\n",
      "Interaction training epoch: 84, train loss: 0.26783, val loss: 0.27730\n",
      "Interaction training epoch: 85, train loss: 0.27581, val loss: 0.29099\n",
      "Interaction training epoch: 86, train loss: 0.26896, val loss: 0.28185\n",
      "Interaction training epoch: 87, train loss: 0.26854, val loss: 0.28007\n",
      "Interaction training epoch: 88, train loss: 0.27048, val loss: 0.28410\n",
      "Interaction training epoch: 89, train loss: 0.26790, val loss: 0.27935\n",
      "Interaction training epoch: 90, train loss: 0.26783, val loss: 0.28181\n",
      "Interaction training epoch: 91, train loss: 0.27144, val loss: 0.28257\n",
      "Interaction training epoch: 92, train loss: 0.26589, val loss: 0.28015\n",
      "Interaction training epoch: 93, train loss: 0.26801, val loss: 0.28335\n",
      "Interaction training epoch: 94, train loss: 0.26660, val loss: 0.28043\n",
      "Interaction training epoch: 95, train loss: 0.26701, val loss: 0.27807\n",
      "Interaction training epoch: 96, train loss: 0.27041, val loss: 0.28712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 97, train loss: 0.26834, val loss: 0.28166\n",
      "Interaction training epoch: 98, train loss: 0.27182, val loss: 0.28556\n",
      "Interaction training epoch: 99, train loss: 0.26430, val loss: 0.27980\n",
      "Interaction training epoch: 100, train loss: 0.26758, val loss: 0.28206\n",
      "Interaction training epoch: 101, train loss: 0.26558, val loss: 0.27845\n",
      "Interaction training epoch: 102, train loss: 0.26639, val loss: 0.28329\n",
      "Interaction training epoch: 103, train loss: 0.26477, val loss: 0.27917\n",
      "Interaction training epoch: 104, train loss: 0.26556, val loss: 0.28118\n",
      "Interaction training epoch: 105, train loss: 0.26634, val loss: 0.28171\n",
      "Interaction training epoch: 106, train loss: 0.26488, val loss: 0.28131\n",
      "Interaction training epoch: 107, train loss: 0.26471, val loss: 0.27923\n",
      "Interaction training epoch: 108, train loss: 0.26567, val loss: 0.28207\n",
      "Interaction training epoch: 109, train loss: 0.26198, val loss: 0.27659\n",
      "Interaction training epoch: 110, train loss: 0.26867, val loss: 0.28650\n",
      "Interaction training epoch: 111, train loss: 0.26127, val loss: 0.27660\n",
      "Interaction training epoch: 112, train loss: 0.26355, val loss: 0.28023\n",
      "Interaction training epoch: 113, train loss: 0.26463, val loss: 0.28447\n",
      "Interaction training epoch: 114, train loss: 0.26374, val loss: 0.27803\n",
      "Interaction training epoch: 115, train loss: 0.26078, val loss: 0.27817\n",
      "Interaction training epoch: 116, train loss: 0.26280, val loss: 0.27814\n",
      "Interaction training epoch: 117, train loss: 0.26106, val loss: 0.28012\n",
      "Interaction training epoch: 118, train loss: 0.26282, val loss: 0.27777\n",
      "Interaction training epoch: 119, train loss: 0.26506, val loss: 0.28762\n",
      "Interaction training epoch: 120, train loss: 0.25980, val loss: 0.27875\n",
      "Interaction training epoch: 121, train loss: 0.26242, val loss: 0.28082\n",
      "Interaction training epoch: 122, train loss: 0.26330, val loss: 0.28137\n",
      "Interaction training epoch: 123, train loss: 0.26426, val loss: 0.28727\n",
      "Interaction training epoch: 124, train loss: 0.26626, val loss: 0.28516\n",
      "Interaction training epoch: 125, train loss: 0.26332, val loss: 0.28395\n",
      "Interaction training epoch: 126, train loss: 0.25913, val loss: 0.28100\n",
      "Interaction training epoch: 127, train loss: 0.26155, val loss: 0.28088\n",
      "Interaction training epoch: 128, train loss: 0.26128, val loss: 0.28280\n",
      "Interaction training epoch: 129, train loss: 0.25895, val loss: 0.28041\n",
      "Interaction training epoch: 130, train loss: 0.25759, val loss: 0.27778\n",
      "Interaction training epoch: 131, train loss: 0.26223, val loss: 0.28823\n",
      "Interaction training epoch: 132, train loss: 0.25960, val loss: 0.28128\n",
      "Interaction training epoch: 133, train loss: 0.25933, val loss: 0.27846\n",
      "Interaction training epoch: 134, train loss: 0.26152, val loss: 0.28557\n",
      "Interaction training epoch: 135, train loss: 0.26217, val loss: 0.28186\n",
      "Interaction training epoch: 136, train loss: 0.25848, val loss: 0.27913\n",
      "Interaction training epoch: 137, train loss: 0.26334, val loss: 0.28608\n",
      "Interaction training epoch: 138, train loss: 0.25791, val loss: 0.28128\n",
      "Interaction training epoch: 139, train loss: 0.26018, val loss: 0.28403\n",
      "Interaction training epoch: 140, train loss: 0.25600, val loss: 0.27919\n",
      "Interaction training epoch: 141, train loss: 0.25821, val loss: 0.28360\n",
      "Interaction training epoch: 142, train loss: 0.25411, val loss: 0.27851\n",
      "Interaction training epoch: 143, train loss: 0.25917, val loss: 0.28022\n",
      "Interaction training epoch: 144, train loss: 0.25645, val loss: 0.28463\n",
      "Interaction training epoch: 145, train loss: 0.25410, val loss: 0.27723\n",
      "Interaction training epoch: 146, train loss: 0.25808, val loss: 0.27937\n",
      "Interaction training epoch: 147, train loss: 0.25589, val loss: 0.28066\n",
      "Interaction training epoch: 148, train loss: 0.25804, val loss: 0.28333\n",
      "Interaction training epoch: 149, train loss: 0.26111, val loss: 0.29103\n",
      "Interaction training epoch: 150, train loss: 0.25385, val loss: 0.28018\n",
      "Interaction training epoch: 151, train loss: 0.25456, val loss: 0.27847\n",
      "Interaction training epoch: 152, train loss: 0.25387, val loss: 0.27881\n",
      "Interaction training epoch: 153, train loss: 0.25564, val loss: 0.28308\n",
      "Interaction training epoch: 154, train loss: 0.25319, val loss: 0.27808\n",
      "Interaction training epoch: 155, train loss: 0.25534, val loss: 0.28184\n",
      "Interaction training epoch: 156, train loss: 0.25512, val loss: 0.28242\n",
      "Interaction training epoch: 157, train loss: 0.25117, val loss: 0.27963\n",
      "Interaction training epoch: 158, train loss: 0.25752, val loss: 0.28434\n",
      "Interaction training epoch: 159, train loss: 0.25264, val loss: 0.27569\n",
      "Interaction training epoch: 160, train loss: 0.25397, val loss: 0.28116\n",
      "Interaction training epoch: 161, train loss: 0.25317, val loss: 0.28127\n",
      "Interaction training epoch: 162, train loss: 0.25251, val loss: 0.28179\n",
      "Interaction training epoch: 163, train loss: 0.25379, val loss: 0.28026\n",
      "Interaction training epoch: 164, train loss: 0.25473, val loss: 0.28188\n",
      "Interaction training epoch: 165, train loss: 0.25205, val loss: 0.27858\n",
      "Interaction training epoch: 166, train loss: 0.25579, val loss: 0.28495\n",
      "Interaction training epoch: 167, train loss: 0.25376, val loss: 0.28134\n",
      "Interaction training epoch: 168, train loss: 0.25230, val loss: 0.27329\n",
      "Interaction training epoch: 169, train loss: 0.25442, val loss: 0.28639\n",
      "Interaction training epoch: 170, train loss: 0.25322, val loss: 0.28087\n",
      "Interaction training epoch: 171, train loss: 0.25185, val loss: 0.28019\n",
      "Interaction training epoch: 172, train loss: 0.25349, val loss: 0.28195\n",
      "Interaction training epoch: 173, train loss: 0.24988, val loss: 0.27655\n",
      "Interaction training epoch: 174, train loss: 0.25487, val loss: 0.28312\n",
      "Interaction training epoch: 175, train loss: 0.25090, val loss: 0.27658\n",
      "Interaction training epoch: 176, train loss: 0.25322, val loss: 0.28014\n",
      "Interaction training epoch: 177, train loss: 0.24903, val loss: 0.28031\n",
      "Interaction training epoch: 178, train loss: 0.25143, val loss: 0.27852\n",
      "Interaction training epoch: 179, train loss: 0.24950, val loss: 0.27758\n",
      "Interaction training epoch: 180, train loss: 0.25130, val loss: 0.27977\n",
      "Interaction training epoch: 181, train loss: 0.24821, val loss: 0.27632\n",
      "Interaction training epoch: 182, train loss: 0.25209, val loss: 0.28218\n",
      "Interaction training epoch: 183, train loss: 0.24794, val loss: 0.27753\n",
      "Interaction training epoch: 184, train loss: 0.25086, val loss: 0.27428\n",
      "Interaction training epoch: 185, train loss: 0.25597, val loss: 0.28385\n",
      "Interaction training epoch: 186, train loss: 0.25197, val loss: 0.27972\n",
      "Interaction training epoch: 187, train loss: 0.25124, val loss: 0.27564\n",
      "Interaction training epoch: 188, train loss: 0.25278, val loss: 0.28325\n",
      "Interaction training epoch: 189, train loss: 0.24849, val loss: 0.27754\n",
      "Interaction training epoch: 190, train loss: 0.24826, val loss: 0.27648\n",
      "Interaction training epoch: 191, train loss: 0.24876, val loss: 0.27947\n",
      "Interaction training epoch: 192, train loss: 0.25098, val loss: 0.27960\n",
      "Interaction training epoch: 193, train loss: 0.25119, val loss: 0.28213\n",
      "Interaction training epoch: 194, train loss: 0.24983, val loss: 0.27578\n",
      "Interaction training epoch: 195, train loss: 0.25019, val loss: 0.28376\n",
      "Interaction training epoch: 196, train loss: 0.24914, val loss: 0.27699\n",
      "Interaction training epoch: 197, train loss: 0.25246, val loss: 0.27957\n",
      "Interaction training epoch: 198, train loss: 0.24989, val loss: 0.27770\n",
      "Interaction training epoch: 199, train loss: 0.25149, val loss: 0.27724\n",
      "Interaction training epoch: 200, train loss: 0.25012, val loss: 0.27921\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########4 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.25819, val loss: 0.27727\n",
      "Interaction tuning epoch: 2, train loss: 0.26009, val loss: 0.27882\n",
      "Interaction tuning epoch: 3, train loss: 0.25900, val loss: 0.27921\n",
      "Interaction tuning epoch: 4, train loss: 0.25787, val loss: 0.27764\n",
      "Interaction tuning epoch: 5, train loss: 0.26147, val loss: 0.28247\n",
      "Interaction tuning epoch: 6, train loss: 0.25589, val loss: 0.27598\n",
      "Interaction tuning epoch: 7, train loss: 0.25855, val loss: 0.27538\n",
      "Interaction tuning epoch: 8, train loss: 0.25797, val loss: 0.27982\n",
      "Interaction tuning epoch: 9, train loss: 0.26151, val loss: 0.28258\n",
      "Interaction tuning epoch: 10, train loss: 0.25557, val loss: 0.27510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 11, train loss: 0.25747, val loss: 0.27598\n",
      "Interaction tuning epoch: 12, train loss: 0.25824, val loss: 0.27799\n",
      "Interaction tuning epoch: 13, train loss: 0.25704, val loss: 0.27652\n",
      "Interaction tuning epoch: 14, train loss: 0.25572, val loss: 0.27457\n",
      "Interaction tuning epoch: 15, train loss: 0.25447, val loss: 0.27485\n",
      "Interaction tuning epoch: 16, train loss: 0.25630, val loss: 0.27631\n",
      "Interaction tuning epoch: 17, train loss: 0.25169, val loss: 0.27165\n",
      "Interaction tuning epoch: 18, train loss: 0.25222, val loss: 0.27210\n",
      "Interaction tuning epoch: 19, train loss: 0.25442, val loss: 0.27614\n",
      "Interaction tuning epoch: 20, train loss: 0.25007, val loss: 0.27250\n",
      "Interaction tuning epoch: 21, train loss: 0.25546, val loss: 0.27665\n",
      "Interaction tuning epoch: 22, train loss: 0.25010, val loss: 0.27124\n",
      "Interaction tuning epoch: 23, train loss: 0.24991, val loss: 0.26632\n",
      "Interaction tuning epoch: 24, train loss: 0.25179, val loss: 0.27073\n",
      "Interaction tuning epoch: 25, train loss: 0.25429, val loss: 0.27482\n",
      "Interaction tuning epoch: 26, train loss: 0.24710, val loss: 0.26661\n",
      "Interaction tuning epoch: 27, train loss: 0.24802, val loss: 0.26858\n",
      "Interaction tuning epoch: 28, train loss: 0.25176, val loss: 0.27265\n",
      "Interaction tuning epoch: 29, train loss: 0.24838, val loss: 0.27045\n",
      "Interaction tuning epoch: 30, train loss: 0.24878, val loss: 0.27279\n",
      "Interaction tuning epoch: 31, train loss: 0.24740, val loss: 0.27002\n",
      "Interaction tuning epoch: 32, train loss: 0.25252, val loss: 0.27604\n",
      "Interaction tuning epoch: 33, train loss: 0.24879, val loss: 0.27250\n",
      "Interaction tuning epoch: 34, train loss: 0.24822, val loss: 0.27096\n",
      "Interaction tuning epoch: 35, train loss: 0.24835, val loss: 0.27189\n",
      "Interaction tuning epoch: 36, train loss: 0.24582, val loss: 0.26500\n",
      "Interaction tuning epoch: 37, train loss: 0.25146, val loss: 0.27358\n",
      "Interaction tuning epoch: 38, train loss: 0.24497, val loss: 0.26936\n",
      "Interaction tuning epoch: 39, train loss: 0.24536, val loss: 0.26457\n",
      "Interaction tuning epoch: 40, train loss: 0.25008, val loss: 0.27653\n",
      "Interaction tuning epoch: 41, train loss: 0.24404, val loss: 0.26763\n",
      "Interaction tuning epoch: 42, train loss: 0.24562, val loss: 0.26739\n",
      "Interaction tuning epoch: 43, train loss: 0.24576, val loss: 0.27007\n",
      "Interaction tuning epoch: 44, train loss: 0.24209, val loss: 0.26712\n",
      "Interaction tuning epoch: 45, train loss: 0.24743, val loss: 0.26990\n",
      "Interaction tuning epoch: 46, train loss: 0.24585, val loss: 0.27162\n",
      "Interaction tuning epoch: 47, train loss: 0.24508, val loss: 0.27108\n",
      "Interaction tuning epoch: 48, train loss: 0.24735, val loss: 0.27129\n",
      "Interaction tuning epoch: 49, train loss: 0.24380, val loss: 0.27099\n",
      "Interaction tuning epoch: 50, train loss: 0.24219, val loss: 0.26951\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 66.34169268608093\n",
      "After the gam stage, training error is 0.24219 , validation error is 0.26951\n",
      "missing value counts: 99261\n",
      "#####start auto_tuning#####\n",
      "the best shrinkage is 0.687500\n",
      "[SoftImpute] Max Singular Value of X_init = 3.721448\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.222542 validation BCE=0.282569,rank=3\n",
      "[SoftImpute] Iter 1: observed BCE=0.221332 validation BCE=0.281426,rank=3\n",
      "[SoftImpute] Iter 2: observed BCE=0.220577 validation BCE=0.270270,rank=3\n",
      "[SoftImpute] Iter 3: observed BCE=0.219999 validation BCE=0.268319,rank=3\n",
      "[SoftImpute] Iter 4: observed BCE=0.219731 validation BCE=0.267687,rank=3\n",
      "[SoftImpute] Iter 5: observed BCE=0.219283 validation BCE=0.266859,rank=3\n",
      "[SoftImpute] Iter 6: observed BCE=0.219169 validation BCE=0.266496,rank=3\n",
      "[SoftImpute] Iter 7: observed BCE=0.218866 validation BCE=0.266474,rank=3\n",
      "[SoftImpute] Iter 8: observed BCE=0.218661 validation BCE=0.265573,rank=3\n",
      "[SoftImpute] Iter 9: observed BCE=0.218021 validation BCE=0.265539,rank=3\n",
      "[SoftImpute] Iter 10: observed BCE=0.217927 validation BCE=0.265190,rank=3\n",
      "[SoftImpute] Iter 11: observed BCE=0.217795 validation BCE=0.264582,rank=3\n",
      "[SoftImpute] Iter 12: observed BCE=0.217412 validation BCE=0.264949,rank=3\n",
      "[SoftImpute] Iter 13: observed BCE=0.217189 validation BCE=0.264082,rank=3\n",
      "[SoftImpute] Iter 14: observed BCE=0.217359 validation BCE=0.264553,rank=3\n",
      "[SoftImpute] Iter 15: observed BCE=0.217375 validation BCE=0.264298,rank=3\n",
      "[SoftImpute] Iter 16: observed BCE=0.217234 validation BCE=0.264283,rank=3\n",
      "[SoftImpute] Iter 17: observed BCE=0.217395 validation BCE=0.264031,rank=3\n",
      "[SoftImpute] Iter 18: observed BCE=0.217219 validation BCE=0.264651,rank=3\n",
      "[SoftImpute] Iter 19: observed BCE=0.217755 validation BCE=0.264020,rank=3\n",
      "[SoftImpute] Iter 20: observed BCE=0.218116 validation BCE=0.263865,rank=3\n",
      "[SoftImpute] Iter 21: observed BCE=0.218444 validation BCE=0.263811,rank=3\n",
      "[SoftImpute] Iter 22: observed BCE=0.218762 validation BCE=0.263981,rank=3\n",
      "[SoftImpute] Iter 23: observed BCE=0.218384 validation BCE=0.264269,rank=3\n",
      "[SoftImpute] Iter 24: observed BCE=0.218326 validation BCE=0.263899,rank=3\n",
      "[SoftImpute] Iter 25: observed BCE=0.218546 validation BCE=0.264192,rank=3\n",
      "[SoftImpute] Iter 26: observed BCE=0.218436 validation BCE=0.264661,rank=3\n",
      "[SoftImpute] Iter 27: observed BCE=0.218673 validation BCE=0.264030,rank=3\n",
      "[SoftImpute] Iter 28: observed BCE=0.218618 validation BCE=0.263985,rank=3\n",
      "[SoftImpute] Iter 29: observed BCE=0.218677 validation BCE=0.264493,rank=3\n",
      "[SoftImpute] Iter 30: observed BCE=0.219123 validation BCE=0.264160,rank=3\n",
      "[SoftImpute] Iter 31: observed BCE=0.218779 validation BCE=0.264336,rank=3\n",
      "[SoftImpute] Iter 32: observed BCE=0.218733 validation BCE=0.264513,rank=3\n",
      "[SoftImpute] Iter 33: observed BCE=0.218839 validation BCE=0.264115,rank=3\n",
      "[SoftImpute] Iter 34: observed BCE=0.219002 validation BCE=0.264230,rank=3\n",
      "[SoftImpute] Iter 35: observed BCE=0.219043 validation BCE=0.264575,rank=3\n",
      "[SoftImpute] Iter 36: observed BCE=0.218889 validation BCE=0.264257,rank=3\n",
      "[SoftImpute] Iter 37: observed BCE=0.218974 validation BCE=0.264143,rank=3\n",
      "[SoftImpute] Iter 38: observed BCE=0.219020 validation BCE=0.264530,rank=3\n",
      "[SoftImpute] Iter 39: observed BCE=0.218943 validation BCE=0.264108,rank=3\n",
      "[SoftImpute] Iter 40: observed BCE=0.218880 validation BCE=0.264320,rank=3\n",
      "[SoftImpute] Iter 41: observed BCE=0.218991 validation BCE=0.264467,rank=3\n",
      "[SoftImpute] Iter 42: observed BCE=0.219110 validation BCE=0.264318,rank=3\n",
      "[SoftImpute] Iter 43: observed BCE=0.219259 validation BCE=0.264276,rank=3\n",
      "[SoftImpute] Iter 44: observed BCE=0.219228 validation BCE=0.264484,rank=3\n",
      "[SoftImpute] Iter 45: observed BCE=0.219244 validation BCE=0.264320,rank=3\n",
      "[SoftImpute] Iter 46: observed BCE=0.218923 validation BCE=0.264008,rank=3\n",
      "[SoftImpute] Iter 47: observed BCE=0.218797 validation BCE=0.264169,rank=3\n",
      "[SoftImpute] Iter 48: observed BCE=0.219037 validation BCE=0.263909,rank=3\n",
      "[SoftImpute] Iter 49: observed BCE=0.218887 validation BCE=0.263781,rank=3\n",
      "[SoftImpute] Iter 50: observed BCE=0.219151 validation BCE=0.264198,rank=3\n",
      "[SoftImpute] Iter 51: observed BCE=0.219080 validation BCE=0.263597,rank=3\n",
      "[SoftImpute] Iter 52: observed BCE=0.219268 validation BCE=0.263715,rank=3\n",
      "[SoftImpute] Iter 53: observed BCE=0.218975 validation BCE=0.263924,rank=3\n",
      "[SoftImpute] Iter 54: observed BCE=0.219038 validation BCE=0.264237,rank=3\n",
      "[SoftImpute] Iter 55: observed BCE=0.218997 validation BCE=0.263739,rank=3\n",
      "[SoftImpute] Iter 56: observed BCE=0.219078 validation BCE=0.264172,rank=3\n",
      "[SoftImpute] Iter 57: observed BCE=0.218850 validation BCE=0.263852,rank=3\n",
      "[SoftImpute] Iter 58: observed BCE=0.218978 validation BCE=0.263911,rank=3\n",
      "[SoftImpute] Iter 59: observed BCE=0.219598 validation BCE=0.263893,rank=3\n",
      "[SoftImpute] Iter 60: observed BCE=0.219654 validation BCE=0.264063,rank=3\n",
      "[SoftImpute] Iter 61: observed BCE=0.219448 validation BCE=0.263531,rank=3\n",
      "[SoftImpute] Iter 62: observed BCE=0.219422 validation BCE=0.264062,rank=3\n",
      "[SoftImpute] Iter 63: observed BCE=0.219188 validation BCE=0.263999,rank=3\n",
      "[SoftImpute] Iter 64: observed BCE=0.219174 validation BCE=0.264040,rank=3\n",
      "[SoftImpute] Iter 65: observed BCE=0.219244 validation BCE=0.264121,rank=3\n",
      "[SoftImpute] Iter 66: observed BCE=0.219272 validation BCE=0.263819,rank=3\n",
      "[SoftImpute] Iter 67: observed BCE=0.219069 validation BCE=0.264293,rank=3\n",
      "[SoftImpute] Iter 68: observed BCE=0.219235 validation BCE=0.263996,rank=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 69: observed BCE=0.219058 validation BCE=0.263600,rank=3\n",
      "[SoftImpute] Iter 70: observed BCE=0.219074 validation BCE=0.263982,rank=3\n",
      "[SoftImpute] Iter 71: observed BCE=0.219413 validation BCE=0.264033,rank=3\n",
      "[SoftImpute] Iter 72: observed BCE=0.219249 validation BCE=0.263628,rank=3\n",
      "[SoftImpute] Iter 73: observed BCE=0.219205 validation BCE=0.263620,rank=3\n",
      "[SoftImpute] Iter 74: observed BCE=0.219311 validation BCE=0.263987,rank=3\n",
      "[SoftImpute] Iter 75: observed BCE=0.219224 validation BCE=0.263219,rank=3\n",
      "[SoftImpute] Iter 76: observed BCE=0.219243 validation BCE=0.263794,rank=3\n",
      "[SoftImpute] Iter 77: observed BCE=0.219517 validation BCE=0.263948,rank=3\n",
      "[SoftImpute] Iter 78: observed BCE=0.219516 validation BCE=0.263794,rank=3\n",
      "[SoftImpute] Iter 79: observed BCE=0.219162 validation BCE=0.264078,rank=3\n",
      "[SoftImpute] Iter 80: observed BCE=0.219279 validation BCE=0.263900,rank=3\n",
      "[SoftImpute] Iter 81: observed BCE=0.219292 validation BCE=0.263643,rank=3\n",
      "[SoftImpute] Iter 82: observed BCE=0.219385 validation BCE=0.263668,rank=3\n",
      "[SoftImpute] Iter 83: observed BCE=0.219246 validation BCE=0.263655,rank=3\n",
      "[SoftImpute] Iter 84: observed BCE=0.219212 validation BCE=0.263594,rank=3\n",
      "[SoftImpute] Iter 85: observed BCE=0.219097 validation BCE=0.263644,rank=3\n",
      "[SoftImpute] Iter 86: observed BCE=0.219294 validation BCE=0.263701,rank=3\n",
      "[SoftImpute] Iter 87: observed BCE=0.219312 validation BCE=0.263878,rank=3\n",
      "[SoftImpute] Iter 88: observed BCE=0.219142 validation BCE=0.263936,rank=3\n",
      "[SoftImpute] Iter 89: observed BCE=0.219043 validation BCE=0.263396,rank=3\n",
      "[SoftImpute] Iter 90: observed BCE=0.218937 validation BCE=0.263275,rank=3\n",
      "[SoftImpute] Iter 91: observed BCE=0.219067 validation BCE=0.263430,rank=3\n",
      "[SoftImpute] Iter 92: observed BCE=0.219165 validation BCE=0.263621,rank=3\n",
      "[SoftImpute] Iter 93: observed BCE=0.219103 validation BCE=0.263536,rank=3\n",
      "[SoftImpute] Iter 94: observed BCE=0.219193 validation BCE=0.263890,rank=3\n",
      "[SoftImpute] Iter 95: observed BCE=0.219099 validation BCE=0.263550,rank=3\n",
      "[SoftImpute] Iter 96: observed BCE=0.219532 validation BCE=0.263605,rank=3\n",
      "[SoftImpute] Iter 97: observed BCE=0.219240 validation BCE=0.263454,rank=3\n",
      "[SoftImpute] Iter 98: observed BCE=0.219394 validation BCE=0.263476,rank=3\n",
      "[SoftImpute] Iter 99: observed BCE=0.219507 validation BCE=0.263449,rank=3\n",
      "[SoftImpute] Iter 100: observed BCE=0.219405 validation BCE=0.263752,rank=3\n",
      "[SoftImpute] Stopped after iteration 100 for lambda=0.074429\n",
      "final num of user group: 7\n",
      "final num of item group: 11\n",
      "change mode state : True\n",
      "time cost: 28.32820439338684\n",
      "After the matrix factor stage, training error is 0.21940, validation error is 0.26375\n",
      "2\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68264, val loss: 0.68234\n",
      "Main effects training epoch: 2, train loss: 0.67661, val loss: 0.67691\n",
      "Main effects training epoch: 3, train loss: 0.67266, val loss: 0.67288\n",
      "Main effects training epoch: 4, train loss: 0.66553, val loss: 0.66440\n",
      "Main effects training epoch: 5, train loss: 0.65457, val loss: 0.65387\n",
      "Main effects training epoch: 6, train loss: 0.63120, val loss: 0.63077\n",
      "Main effects training epoch: 7, train loss: 0.59724, val loss: 0.59854\n",
      "Main effects training epoch: 8, train loss: 0.55714, val loss: 0.56668\n",
      "Main effects training epoch: 9, train loss: 0.52893, val loss: 0.55200\n",
      "Main effects training epoch: 10, train loss: 0.53330, val loss: 0.55857\n",
      "Main effects training epoch: 11, train loss: 0.52698, val loss: 0.55430\n",
      "Main effects training epoch: 12, train loss: 0.52575, val loss: 0.55313\n",
      "Main effects training epoch: 13, train loss: 0.52455, val loss: 0.54665\n",
      "Main effects training epoch: 14, train loss: 0.52400, val loss: 0.55095\n",
      "Main effects training epoch: 15, train loss: 0.52365, val loss: 0.54830\n",
      "Main effects training epoch: 16, train loss: 0.52339, val loss: 0.55029\n",
      "Main effects training epoch: 17, train loss: 0.52412, val loss: 0.55184\n",
      "Main effects training epoch: 18, train loss: 0.52341, val loss: 0.54962\n",
      "Main effects training epoch: 19, train loss: 0.52367, val loss: 0.55118\n",
      "Main effects training epoch: 20, train loss: 0.52417, val loss: 0.54906\n",
      "Main effects training epoch: 21, train loss: 0.52372, val loss: 0.55159\n",
      "Main effects training epoch: 22, train loss: 0.52318, val loss: 0.54952\n",
      "Main effects training epoch: 23, train loss: 0.52403, val loss: 0.55003\n",
      "Main effects training epoch: 24, train loss: 0.52393, val loss: 0.55038\n",
      "Main effects training epoch: 25, train loss: 0.52379, val loss: 0.55035\n",
      "Main effects training epoch: 26, train loss: 0.52356, val loss: 0.54901\n",
      "Main effects training epoch: 27, train loss: 0.52318, val loss: 0.54834\n",
      "Main effects training epoch: 28, train loss: 0.52314, val loss: 0.55048\n",
      "Main effects training epoch: 29, train loss: 0.52302, val loss: 0.55028\n",
      "Main effects training epoch: 30, train loss: 0.52348, val loss: 0.55012\n",
      "Main effects training epoch: 31, train loss: 0.52331, val loss: 0.54897\n",
      "Main effects training epoch: 32, train loss: 0.52299, val loss: 0.54931\n",
      "Main effects training epoch: 33, train loss: 0.52286, val loss: 0.54917\n",
      "Main effects training epoch: 34, train loss: 0.52284, val loss: 0.54928\n",
      "Main effects training epoch: 35, train loss: 0.52316, val loss: 0.54774\n",
      "Main effects training epoch: 36, train loss: 0.52307, val loss: 0.55095\n",
      "Main effects training epoch: 37, train loss: 0.52360, val loss: 0.55173\n",
      "Main effects training epoch: 38, train loss: 0.52430, val loss: 0.54849\n",
      "Main effects training epoch: 39, train loss: 0.52337, val loss: 0.55053\n",
      "Main effects training epoch: 40, train loss: 0.52287, val loss: 0.54905\n",
      "Main effects training epoch: 41, train loss: 0.52332, val loss: 0.55139\n",
      "Main effects training epoch: 42, train loss: 0.52287, val loss: 0.54831\n",
      "Main effects training epoch: 43, train loss: 0.52292, val loss: 0.55080\n",
      "Main effects training epoch: 44, train loss: 0.52284, val loss: 0.54864\n",
      "Main effects training epoch: 45, train loss: 0.52266, val loss: 0.54932\n",
      "Main effects training epoch: 46, train loss: 0.52285, val loss: 0.55038\n",
      "Main effects training epoch: 47, train loss: 0.52275, val loss: 0.54775\n",
      "Main effects training epoch: 48, train loss: 0.52271, val loss: 0.54754\n",
      "Main effects training epoch: 49, train loss: 0.52302, val loss: 0.55097\n",
      "Main effects training epoch: 50, train loss: 0.52254, val loss: 0.54807\n",
      "Main effects training epoch: 51, train loss: 0.52330, val loss: 0.55130\n",
      "Main effects training epoch: 52, train loss: 0.52308, val loss: 0.55032\n",
      "Main effects training epoch: 53, train loss: 0.52410, val loss: 0.55250\n",
      "Main effects training epoch: 54, train loss: 0.52290, val loss: 0.54714\n",
      "Main effects training epoch: 55, train loss: 0.52308, val loss: 0.55163\n",
      "Main effects training epoch: 56, train loss: 0.52249, val loss: 0.55026\n",
      "Main effects training epoch: 57, train loss: 0.52276, val loss: 0.54779\n",
      "Main effects training epoch: 58, train loss: 0.52268, val loss: 0.55049\n",
      "Main effects training epoch: 59, train loss: 0.52238, val loss: 0.54884\n",
      "Main effects training epoch: 60, train loss: 0.52243, val loss: 0.54820\n",
      "Main effects training epoch: 61, train loss: 0.52253, val loss: 0.55071\n",
      "Main effects training epoch: 62, train loss: 0.52316, val loss: 0.54703\n",
      "Main effects training epoch: 63, train loss: 0.52297, val loss: 0.55180\n",
      "Main effects training epoch: 64, train loss: 0.52323, val loss: 0.54864\n",
      "Main effects training epoch: 65, train loss: 0.52245, val loss: 0.54993\n",
      "Main effects training epoch: 66, train loss: 0.52266, val loss: 0.54881\n",
      "Main effects training epoch: 67, train loss: 0.52255, val loss: 0.55195\n",
      "Main effects training epoch: 68, train loss: 0.52277, val loss: 0.54737\n",
      "Main effects training epoch: 69, train loss: 0.52244, val loss: 0.54995\n",
      "Main effects training epoch: 70, train loss: 0.52304, val loss: 0.55291\n",
      "Main effects training epoch: 71, train loss: 0.52283, val loss: 0.54792\n",
      "Main effects training epoch: 72, train loss: 0.52245, val loss: 0.55003\n",
      "Main effects training epoch: 73, train loss: 0.52271, val loss: 0.54939\n",
      "Main effects training epoch: 74, train loss: 0.52200, val loss: 0.54818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 75, train loss: 0.52207, val loss: 0.54803\n",
      "Main effects training epoch: 76, train loss: 0.52203, val loss: 0.55064\n",
      "Main effects training epoch: 77, train loss: 0.52204, val loss: 0.54905\n",
      "Main effects training epoch: 78, train loss: 0.52231, val loss: 0.55020\n",
      "Main effects training epoch: 79, train loss: 0.52190, val loss: 0.54891\n",
      "Main effects training epoch: 80, train loss: 0.52183, val loss: 0.54969\n",
      "Main effects training epoch: 81, train loss: 0.52182, val loss: 0.54827\n",
      "Main effects training epoch: 82, train loss: 0.52183, val loss: 0.54962\n",
      "Main effects training epoch: 83, train loss: 0.52187, val loss: 0.54883\n",
      "Main effects training epoch: 84, train loss: 0.52180, val loss: 0.54929\n",
      "Main effects training epoch: 85, train loss: 0.52163, val loss: 0.54911\n",
      "Main effects training epoch: 86, train loss: 0.52203, val loss: 0.54875\n",
      "Main effects training epoch: 87, train loss: 0.52243, val loss: 0.54965\n",
      "Main effects training epoch: 88, train loss: 0.52236, val loss: 0.55050\n",
      "Main effects training epoch: 89, train loss: 0.52179, val loss: 0.54917\n",
      "Main effects training epoch: 90, train loss: 0.52168, val loss: 0.54944\n",
      "Main effects training epoch: 91, train loss: 0.52154, val loss: 0.54788\n",
      "Main effects training epoch: 92, train loss: 0.52208, val loss: 0.55132\n",
      "Main effects training epoch: 93, train loss: 0.52175, val loss: 0.54817\n",
      "Main effects training epoch: 94, train loss: 0.52189, val loss: 0.54979\n",
      "Main effects training epoch: 95, train loss: 0.52171, val loss: 0.54887\n",
      "Main effects training epoch: 96, train loss: 0.52169, val loss: 0.54842\n",
      "Main effects training epoch: 97, train loss: 0.52149, val loss: 0.54849\n",
      "Main effects training epoch: 98, train loss: 0.52176, val loss: 0.55117\n",
      "Main effects training epoch: 99, train loss: 0.52156, val loss: 0.54849\n",
      "Main effects training epoch: 100, train loss: 0.52214, val loss: 0.55027\n",
      "Main effects training epoch: 101, train loss: 0.52148, val loss: 0.54840\n",
      "Main effects training epoch: 102, train loss: 0.52169, val loss: 0.55052\n",
      "Main effects training epoch: 103, train loss: 0.52204, val loss: 0.54748\n",
      "Main effects training epoch: 104, train loss: 0.52193, val loss: 0.55038\n",
      "Main effects training epoch: 105, train loss: 0.52186, val loss: 0.54748\n",
      "Main effects training epoch: 106, train loss: 0.52128, val loss: 0.54908\n",
      "Main effects training epoch: 107, train loss: 0.52149, val loss: 0.54855\n",
      "Main effects training epoch: 108, train loss: 0.52135, val loss: 0.54945\n",
      "Main effects training epoch: 109, train loss: 0.52136, val loss: 0.54706\n",
      "Main effects training epoch: 110, train loss: 0.52120, val loss: 0.54976\n",
      "Main effects training epoch: 111, train loss: 0.52189, val loss: 0.54779\n",
      "Main effects training epoch: 112, train loss: 0.52318, val loss: 0.55301\n",
      "Main effects training epoch: 113, train loss: 0.52209, val loss: 0.54775\n",
      "Main effects training epoch: 114, train loss: 0.52153, val loss: 0.55002\n",
      "Early stop at epoch 114, with validation loss: 0.55002\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.52300, val loss: 0.54911\n",
      "Main effects tuning epoch: 2, train loss: 0.52310, val loss: 0.55090\n",
      "Main effects tuning epoch: 3, train loss: 0.52319, val loss: 0.55036\n",
      "Main effects tuning epoch: 4, train loss: 0.52329, val loss: 0.54873\n",
      "Main effects tuning epoch: 5, train loss: 0.52334, val loss: 0.55040\n",
      "Main effects tuning epoch: 6, train loss: 0.52309, val loss: 0.54907\n",
      "Main effects tuning epoch: 7, train loss: 0.52311, val loss: 0.55067\n",
      "Main effects tuning epoch: 8, train loss: 0.52295, val loss: 0.54947\n",
      "Main effects tuning epoch: 9, train loss: 0.52312, val loss: 0.54978\n",
      "Main effects tuning epoch: 10, train loss: 0.52302, val loss: 0.55120\n",
      "Main effects tuning epoch: 11, train loss: 0.52292, val loss: 0.54970\n",
      "Main effects tuning epoch: 12, train loss: 0.52325, val loss: 0.55119\n",
      "Main effects tuning epoch: 13, train loss: 0.52291, val loss: 0.54970\n",
      "Main effects tuning epoch: 14, train loss: 0.52292, val loss: 0.54939\n",
      "Main effects tuning epoch: 15, train loss: 0.52340, val loss: 0.55225\n",
      "Main effects tuning epoch: 16, train loss: 0.52381, val loss: 0.54897\n",
      "Main effects tuning epoch: 17, train loss: 0.52305, val loss: 0.55124\n",
      "Main effects tuning epoch: 18, train loss: 0.52292, val loss: 0.54842\n",
      "Main effects tuning epoch: 19, train loss: 0.52264, val loss: 0.55011\n",
      "Main effects tuning epoch: 20, train loss: 0.52264, val loss: 0.55070\n",
      "Main effects tuning epoch: 21, train loss: 0.52276, val loss: 0.54886\n",
      "Main effects tuning epoch: 22, train loss: 0.52304, val loss: 0.55032\n",
      "Main effects tuning epoch: 23, train loss: 0.52372, val loss: 0.55076\n",
      "Main effects tuning epoch: 24, train loss: 0.52294, val loss: 0.55002\n",
      "Main effects tuning epoch: 25, train loss: 0.52313, val loss: 0.54876\n",
      "Main effects tuning epoch: 26, train loss: 0.52319, val loss: 0.55239\n",
      "Main effects tuning epoch: 27, train loss: 0.52329, val loss: 0.54877\n",
      "Main effects tuning epoch: 28, train loss: 0.52268, val loss: 0.55074\n",
      "Main effects tuning epoch: 29, train loss: 0.52250, val loss: 0.54878\n",
      "Main effects tuning epoch: 30, train loss: 0.52239, val loss: 0.54887\n",
      "Main effects tuning epoch: 31, train loss: 0.52240, val loss: 0.55037\n",
      "Main effects tuning epoch: 32, train loss: 0.52236, val loss: 0.54920\n",
      "Main effects tuning epoch: 33, train loss: 0.52230, val loss: 0.54910\n",
      "Main effects tuning epoch: 34, train loss: 0.52218, val loss: 0.54935\n",
      "Main effects tuning epoch: 35, train loss: 0.52225, val loss: 0.54946\n",
      "Main effects tuning epoch: 36, train loss: 0.52229, val loss: 0.55020\n",
      "Main effects tuning epoch: 37, train loss: 0.52241, val loss: 0.55133\n",
      "Main effects tuning epoch: 38, train loss: 0.52263, val loss: 0.54917\n",
      "Main effects tuning epoch: 39, train loss: 0.52262, val loss: 0.54993\n",
      "Main effects tuning epoch: 40, train loss: 0.52274, val loss: 0.54935\n",
      "Main effects tuning epoch: 41, train loss: 0.52231, val loss: 0.54957\n",
      "Main effects tuning epoch: 42, train loss: 0.52234, val loss: 0.55044\n",
      "Main effects tuning epoch: 43, train loss: 0.52231, val loss: 0.54910\n",
      "Main effects tuning epoch: 44, train loss: 0.52277, val loss: 0.54957\n",
      "Main effects tuning epoch: 45, train loss: 0.52230, val loss: 0.54933\n",
      "Main effects tuning epoch: 46, train loss: 0.52221, val loss: 0.54874\n",
      "Main effects tuning epoch: 47, train loss: 0.52231, val loss: 0.55031\n",
      "Main effects tuning epoch: 48, train loss: 0.52185, val loss: 0.54889\n",
      "Main effects tuning epoch: 49, train loss: 0.52233, val loss: 0.55035\n",
      "Main effects tuning epoch: 50, train loss: 0.52206, val loss: 0.54750\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.51886, val loss: 0.54547\n",
      "Interaction training epoch: 2, train loss: 0.48200, val loss: 0.50869\n",
      "Interaction training epoch: 3, train loss: 0.38976, val loss: 0.40474\n",
      "Interaction training epoch: 4, train loss: 0.32630, val loss: 0.34222\n",
      "Interaction training epoch: 5, train loss: 0.29892, val loss: 0.31798\n",
      "Interaction training epoch: 6, train loss: 0.30105, val loss: 0.31639\n",
      "Interaction training epoch: 7, train loss: 0.29641, val loss: 0.31215\n",
      "Interaction training epoch: 8, train loss: 0.29583, val loss: 0.31279\n",
      "Interaction training epoch: 9, train loss: 0.28547, val loss: 0.30536\n",
      "Interaction training epoch: 10, train loss: 0.28925, val loss: 0.30666\n",
      "Interaction training epoch: 11, train loss: 0.29043, val loss: 0.31020\n",
      "Interaction training epoch: 12, train loss: 0.29284, val loss: 0.31327\n",
      "Interaction training epoch: 13, train loss: 0.28244, val loss: 0.30469\n",
      "Interaction training epoch: 14, train loss: 0.29133, val loss: 0.31279\n",
      "Interaction training epoch: 15, train loss: 0.28913, val loss: 0.31494\n",
      "Interaction training epoch: 16, train loss: 0.28373, val loss: 0.31056\n",
      "Interaction training epoch: 17, train loss: 0.28359, val loss: 0.31262\n",
      "Interaction training epoch: 18, train loss: 0.28400, val loss: 0.30955\n",
      "Interaction training epoch: 19, train loss: 0.28561, val loss: 0.30918\n",
      "Interaction training epoch: 20, train loss: 0.28196, val loss: 0.31121\n",
      "Interaction training epoch: 21, train loss: 0.28395, val loss: 0.30959\n",
      "Interaction training epoch: 22, train loss: 0.27997, val loss: 0.30785\n",
      "Interaction training epoch: 23, train loss: 0.28155, val loss: 0.30780\n",
      "Interaction training epoch: 24, train loss: 0.28209, val loss: 0.30749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 25, train loss: 0.28429, val loss: 0.31416\n",
      "Interaction training epoch: 26, train loss: 0.28073, val loss: 0.30645\n",
      "Interaction training epoch: 27, train loss: 0.28170, val loss: 0.31007\n",
      "Interaction training epoch: 28, train loss: 0.27876, val loss: 0.30709\n",
      "Interaction training epoch: 29, train loss: 0.28492, val loss: 0.31201\n",
      "Interaction training epoch: 30, train loss: 0.28240, val loss: 0.30935\n",
      "Interaction training epoch: 31, train loss: 0.27872, val loss: 0.30724\n",
      "Interaction training epoch: 32, train loss: 0.28499, val loss: 0.31255\n",
      "Interaction training epoch: 33, train loss: 0.28304, val loss: 0.31047\n",
      "Interaction training epoch: 34, train loss: 0.27812, val loss: 0.30305\n",
      "Interaction training epoch: 35, train loss: 0.28037, val loss: 0.30815\n",
      "Interaction training epoch: 36, train loss: 0.27907, val loss: 0.30521\n",
      "Interaction training epoch: 37, train loss: 0.28014, val loss: 0.30691\n",
      "Interaction training epoch: 38, train loss: 0.27646, val loss: 0.30524\n",
      "Interaction training epoch: 39, train loss: 0.27799, val loss: 0.30810\n",
      "Interaction training epoch: 40, train loss: 0.27642, val loss: 0.30324\n",
      "Interaction training epoch: 41, train loss: 0.27876, val loss: 0.30612\n",
      "Interaction training epoch: 42, train loss: 0.27927, val loss: 0.30630\n",
      "Interaction training epoch: 43, train loss: 0.27474, val loss: 0.30472\n",
      "Interaction training epoch: 44, train loss: 0.27753, val loss: 0.30347\n",
      "Interaction training epoch: 45, train loss: 0.27697, val loss: 0.30451\n",
      "Interaction training epoch: 46, train loss: 0.27831, val loss: 0.30735\n",
      "Interaction training epoch: 47, train loss: 0.27658, val loss: 0.30338\n",
      "Interaction training epoch: 48, train loss: 0.27361, val loss: 0.30243\n",
      "Interaction training epoch: 49, train loss: 0.27472, val loss: 0.30149\n",
      "Interaction training epoch: 50, train loss: 0.27489, val loss: 0.30300\n",
      "Interaction training epoch: 51, train loss: 0.27751, val loss: 0.30536\n",
      "Interaction training epoch: 52, train loss: 0.27756, val loss: 0.30476\n",
      "Interaction training epoch: 53, train loss: 0.27349, val loss: 0.30101\n",
      "Interaction training epoch: 54, train loss: 0.27408, val loss: 0.30228\n",
      "Interaction training epoch: 55, train loss: 0.27152, val loss: 0.30118\n",
      "Interaction training epoch: 56, train loss: 0.27136, val loss: 0.30097\n",
      "Interaction training epoch: 57, train loss: 0.28039, val loss: 0.30416\n",
      "Interaction training epoch: 58, train loss: 0.27424, val loss: 0.30402\n",
      "Interaction training epoch: 59, train loss: 0.27512, val loss: 0.30512\n",
      "Interaction training epoch: 60, train loss: 0.27441, val loss: 0.29925\n",
      "Interaction training epoch: 61, train loss: 0.27133, val loss: 0.30052\n",
      "Interaction training epoch: 62, train loss: 0.27267, val loss: 0.29912\n",
      "Interaction training epoch: 63, train loss: 0.27345, val loss: 0.30397\n",
      "Interaction training epoch: 64, train loss: 0.26781, val loss: 0.29699\n",
      "Interaction training epoch: 65, train loss: 0.27707, val loss: 0.30383\n",
      "Interaction training epoch: 66, train loss: 0.27077, val loss: 0.30265\n",
      "Interaction training epoch: 67, train loss: 0.27066, val loss: 0.29908\n",
      "Interaction training epoch: 68, train loss: 0.27590, val loss: 0.30319\n",
      "Interaction training epoch: 69, train loss: 0.27324, val loss: 0.30404\n",
      "Interaction training epoch: 70, train loss: 0.26817, val loss: 0.29721\n",
      "Interaction training epoch: 71, train loss: 0.26599, val loss: 0.29270\n",
      "Interaction training epoch: 72, train loss: 0.26932, val loss: 0.29809\n",
      "Interaction training epoch: 73, train loss: 0.27119, val loss: 0.30048\n",
      "Interaction training epoch: 74, train loss: 0.26602, val loss: 0.29655\n",
      "Interaction training epoch: 75, train loss: 0.26674, val loss: 0.29477\n",
      "Interaction training epoch: 76, train loss: 0.26519, val loss: 0.29909\n",
      "Interaction training epoch: 77, train loss: 0.26585, val loss: 0.29453\n",
      "Interaction training epoch: 78, train loss: 0.26602, val loss: 0.29784\n",
      "Interaction training epoch: 79, train loss: 0.26780, val loss: 0.29724\n",
      "Interaction training epoch: 80, train loss: 0.26673, val loss: 0.30187\n",
      "Interaction training epoch: 81, train loss: 0.26430, val loss: 0.29825\n",
      "Interaction training epoch: 82, train loss: 0.26316, val loss: 0.29718\n",
      "Interaction training epoch: 83, train loss: 0.26455, val loss: 0.29788\n",
      "Interaction training epoch: 84, train loss: 0.26690, val loss: 0.29957\n",
      "Interaction training epoch: 85, train loss: 0.26692, val loss: 0.30228\n",
      "Interaction training epoch: 86, train loss: 0.26260, val loss: 0.29742\n",
      "Interaction training epoch: 87, train loss: 0.27190, val loss: 0.30249\n",
      "Interaction training epoch: 88, train loss: 0.26379, val loss: 0.29898\n",
      "Interaction training epoch: 89, train loss: 0.26538, val loss: 0.29859\n",
      "Interaction training epoch: 90, train loss: 0.26319, val loss: 0.29699\n",
      "Interaction training epoch: 91, train loss: 0.26334, val loss: 0.29898\n",
      "Interaction training epoch: 92, train loss: 0.26199, val loss: 0.29662\n",
      "Interaction training epoch: 93, train loss: 0.26465, val loss: 0.29799\n",
      "Interaction training epoch: 94, train loss: 0.26177, val loss: 0.29870\n",
      "Interaction training epoch: 95, train loss: 0.25651, val loss: 0.29350\n",
      "Interaction training epoch: 96, train loss: 0.26204, val loss: 0.29853\n",
      "Interaction training epoch: 97, train loss: 0.26371, val loss: 0.30156\n",
      "Interaction training epoch: 98, train loss: 0.26184, val loss: 0.29408\n",
      "Interaction training epoch: 99, train loss: 0.26263, val loss: 0.30186\n",
      "Interaction training epoch: 100, train loss: 0.26020, val loss: 0.29356\n",
      "Interaction training epoch: 101, train loss: 0.26302, val loss: 0.29526\n",
      "Interaction training epoch: 102, train loss: 0.26139, val loss: 0.29969\n",
      "Interaction training epoch: 103, train loss: 0.25710, val loss: 0.29315\n",
      "Interaction training epoch: 104, train loss: 0.26572, val loss: 0.30327\n",
      "Interaction training epoch: 105, train loss: 0.25955, val loss: 0.29742\n",
      "Interaction training epoch: 106, train loss: 0.25558, val loss: 0.29322\n",
      "Interaction training epoch: 107, train loss: 0.26381, val loss: 0.30006\n",
      "Interaction training epoch: 108, train loss: 0.25588, val loss: 0.29404\n",
      "Interaction training epoch: 109, train loss: 0.25859, val loss: 0.29656\n",
      "Interaction training epoch: 110, train loss: 0.25756, val loss: 0.29607\n",
      "Interaction training epoch: 111, train loss: 0.25674, val loss: 0.29302\n",
      "Interaction training epoch: 112, train loss: 0.25591, val loss: 0.29787\n",
      "Interaction training epoch: 113, train loss: 0.25717, val loss: 0.29269\n",
      "Interaction training epoch: 114, train loss: 0.25704, val loss: 0.29735\n",
      "Interaction training epoch: 115, train loss: 0.25831, val loss: 0.29789\n",
      "Interaction training epoch: 116, train loss: 0.25452, val loss: 0.29324\n",
      "Interaction training epoch: 117, train loss: 0.25805, val loss: 0.29409\n",
      "Interaction training epoch: 118, train loss: 0.25534, val loss: 0.29722\n",
      "Interaction training epoch: 119, train loss: 0.26387, val loss: 0.29871\n",
      "Interaction training epoch: 120, train loss: 0.25968, val loss: 0.30621\n",
      "Interaction training epoch: 121, train loss: 0.25701, val loss: 0.29598\n",
      "Interaction training epoch: 122, train loss: 0.25730, val loss: 0.29713\n",
      "Interaction training epoch: 123, train loss: 0.25495, val loss: 0.29506\n",
      "Interaction training epoch: 124, train loss: 0.25385, val loss: 0.29287\n",
      "Interaction training epoch: 125, train loss: 0.25332, val loss: 0.29423\n",
      "Interaction training epoch: 126, train loss: 0.25463, val loss: 0.29410\n",
      "Interaction training epoch: 127, train loss: 0.25335, val loss: 0.29306\n",
      "Interaction training epoch: 128, train loss: 0.25324, val loss: 0.29342\n",
      "Interaction training epoch: 129, train loss: 0.25406, val loss: 0.29155\n",
      "Interaction training epoch: 130, train loss: 0.25163, val loss: 0.29520\n",
      "Interaction training epoch: 131, train loss: 0.25217, val loss: 0.29310\n",
      "Interaction training epoch: 132, train loss: 0.25268, val loss: 0.29551\n",
      "Interaction training epoch: 133, train loss: 0.25135, val loss: 0.29424\n",
      "Interaction training epoch: 134, train loss: 0.25494, val loss: 0.29493\n",
      "Interaction training epoch: 135, train loss: 0.25030, val loss: 0.29311\n",
      "Interaction training epoch: 136, train loss: 0.25215, val loss: 0.28948\n",
      "Interaction training epoch: 137, train loss: 0.25210, val loss: 0.29681\n",
      "Interaction training epoch: 138, train loss: 0.25114, val loss: 0.29144\n",
      "Interaction training epoch: 139, train loss: 0.25345, val loss: 0.29572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 140, train loss: 0.24987, val loss: 0.29372\n",
      "Interaction training epoch: 141, train loss: 0.25661, val loss: 0.29913\n",
      "Interaction training epoch: 142, train loss: 0.25167, val loss: 0.29704\n",
      "Interaction training epoch: 143, train loss: 0.25217, val loss: 0.29089\n",
      "Interaction training epoch: 144, train loss: 0.25174, val loss: 0.29749\n",
      "Interaction training epoch: 145, train loss: 0.25215, val loss: 0.29226\n",
      "Interaction training epoch: 146, train loss: 0.25141, val loss: 0.29648\n",
      "Interaction training epoch: 147, train loss: 0.24945, val loss: 0.29100\n",
      "Interaction training epoch: 148, train loss: 0.24764, val loss: 0.29221\n",
      "Interaction training epoch: 149, train loss: 0.25014, val loss: 0.29312\n",
      "Interaction training epoch: 150, train loss: 0.24696, val loss: 0.29495\n",
      "Interaction training epoch: 151, train loss: 0.24998, val loss: 0.29571\n",
      "Interaction training epoch: 152, train loss: 0.24685, val loss: 0.28865\n",
      "Interaction training epoch: 153, train loss: 0.24981, val loss: 0.29538\n",
      "Interaction training epoch: 154, train loss: 0.24680, val loss: 0.29153\n",
      "Interaction training epoch: 155, train loss: 0.24852, val loss: 0.29240\n",
      "Interaction training epoch: 156, train loss: 0.25039, val loss: 0.29553\n",
      "Interaction training epoch: 157, train loss: 0.24938, val loss: 0.29464\n",
      "Interaction training epoch: 158, train loss: 0.24899, val loss: 0.29230\n",
      "Interaction training epoch: 159, train loss: 0.24551, val loss: 0.29128\n",
      "Interaction training epoch: 160, train loss: 0.25137, val loss: 0.30031\n",
      "Interaction training epoch: 161, train loss: 0.24992, val loss: 0.29585\n",
      "Interaction training epoch: 162, train loss: 0.24625, val loss: 0.29247\n",
      "Interaction training epoch: 163, train loss: 0.24885, val loss: 0.29746\n",
      "Interaction training epoch: 164, train loss: 0.24853, val loss: 0.29267\n",
      "Interaction training epoch: 165, train loss: 0.24565, val loss: 0.29383\n",
      "Interaction training epoch: 166, train loss: 0.24617, val loss: 0.29244\n",
      "Interaction training epoch: 167, train loss: 0.24716, val loss: 0.29510\n",
      "Interaction training epoch: 168, train loss: 0.24649, val loss: 0.29407\n",
      "Interaction training epoch: 169, train loss: 0.24791, val loss: 0.29484\n",
      "Interaction training epoch: 170, train loss: 0.24446, val loss: 0.29257\n",
      "Interaction training epoch: 171, train loss: 0.24345, val loss: 0.29008\n",
      "Interaction training epoch: 172, train loss: 0.24826, val loss: 0.29776\n",
      "Interaction training epoch: 173, train loss: 0.24351, val loss: 0.29054\n",
      "Interaction training epoch: 174, train loss: 0.24700, val loss: 0.29607\n",
      "Interaction training epoch: 175, train loss: 0.24426, val loss: 0.29210\n",
      "Interaction training epoch: 176, train loss: 0.24327, val loss: 0.29111\n",
      "Interaction training epoch: 177, train loss: 0.24408, val loss: 0.29241\n",
      "Interaction training epoch: 178, train loss: 0.24419, val loss: 0.29161\n",
      "Interaction training epoch: 179, train loss: 0.24641, val loss: 0.29478\n",
      "Interaction training epoch: 180, train loss: 0.24378, val loss: 0.29573\n",
      "Interaction training epoch: 181, train loss: 0.24447, val loss: 0.29531\n",
      "Interaction training epoch: 182, train loss: 0.24461, val loss: 0.29330\n",
      "Interaction training epoch: 183, train loss: 0.24531, val loss: 0.29056\n",
      "Interaction training epoch: 184, train loss: 0.24816, val loss: 0.30022\n",
      "Interaction training epoch: 185, train loss: 0.24383, val loss: 0.29158\n",
      "Interaction training epoch: 186, train loss: 0.24601, val loss: 0.29715\n",
      "Interaction training epoch: 187, train loss: 0.24132, val loss: 0.28950\n",
      "Interaction training epoch: 188, train loss: 0.24295, val loss: 0.29287\n",
      "Interaction training epoch: 189, train loss: 0.24420, val loss: 0.29461\n",
      "Interaction training epoch: 190, train loss: 0.24231, val loss: 0.28824\n",
      "Interaction training epoch: 191, train loss: 0.24535, val loss: 0.29909\n",
      "Interaction training epoch: 192, train loss: 0.24232, val loss: 0.29071\n",
      "Interaction training epoch: 193, train loss: 0.24447, val loss: 0.29530\n",
      "Interaction training epoch: 194, train loss: 0.24157, val loss: 0.29163\n",
      "Interaction training epoch: 195, train loss: 0.24172, val loss: 0.29403\n",
      "Interaction training epoch: 196, train loss: 0.24225, val loss: 0.29085\n",
      "Interaction training epoch: 197, train loss: 0.24060, val loss: 0.29201\n",
      "Interaction training epoch: 198, train loss: 0.24487, val loss: 0.29530\n",
      "Interaction training epoch: 199, train loss: 0.24205, val loss: 0.29527\n",
      "Interaction training epoch: 200, train loss: 0.24137, val loss: 0.29261\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########3 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.24794, val loss: 0.29077\n",
      "Interaction tuning epoch: 2, train loss: 0.24961, val loss: 0.28882\n",
      "Interaction tuning epoch: 3, train loss: 0.25832, val loss: 0.30712\n",
      "Interaction tuning epoch: 4, train loss: 0.24966, val loss: 0.28776\n",
      "Interaction tuning epoch: 5, train loss: 0.24785, val loss: 0.28586\n",
      "Interaction tuning epoch: 6, train loss: 0.24773, val loss: 0.29054\n",
      "Interaction tuning epoch: 7, train loss: 0.24936, val loss: 0.28917\n",
      "Interaction tuning epoch: 8, train loss: 0.24681, val loss: 0.28484\n",
      "Interaction tuning epoch: 9, train loss: 0.24993, val loss: 0.29083\n",
      "Interaction tuning epoch: 10, train loss: 0.24736, val loss: 0.28495\n",
      "Interaction tuning epoch: 11, train loss: 0.25220, val loss: 0.29198\n",
      "Interaction tuning epoch: 12, train loss: 0.24811, val loss: 0.28821\n",
      "Interaction tuning epoch: 13, train loss: 0.24760, val loss: 0.28881\n",
      "Interaction tuning epoch: 14, train loss: 0.24839, val loss: 0.29163\n",
      "Interaction tuning epoch: 15, train loss: 0.25073, val loss: 0.29006\n",
      "Interaction tuning epoch: 16, train loss: 0.24473, val loss: 0.28263\n",
      "Interaction tuning epoch: 17, train loss: 0.24987, val loss: 0.29033\n",
      "Interaction tuning epoch: 18, train loss: 0.24416, val loss: 0.28317\n",
      "Interaction tuning epoch: 19, train loss: 0.24809, val loss: 0.28737\n",
      "Interaction tuning epoch: 20, train loss: 0.24317, val loss: 0.28388\n",
      "Interaction tuning epoch: 21, train loss: 0.24760, val loss: 0.28872\n",
      "Interaction tuning epoch: 22, train loss: 0.24426, val loss: 0.28225\n",
      "Interaction tuning epoch: 23, train loss: 0.24597, val loss: 0.28674\n",
      "Interaction tuning epoch: 24, train loss: 0.24286, val loss: 0.28480\n",
      "Interaction tuning epoch: 25, train loss: 0.25003, val loss: 0.29447\n",
      "Interaction tuning epoch: 26, train loss: 0.24529, val loss: 0.28452\n",
      "Interaction tuning epoch: 27, train loss: 0.24368, val loss: 0.28348\n",
      "Interaction tuning epoch: 28, train loss: 0.24558, val loss: 0.28488\n",
      "Interaction tuning epoch: 29, train loss: 0.24584, val loss: 0.28529\n",
      "Interaction tuning epoch: 30, train loss: 0.24538, val loss: 0.28641\n",
      "Interaction tuning epoch: 31, train loss: 0.25233, val loss: 0.29355\n",
      "Interaction tuning epoch: 32, train loss: 0.24164, val loss: 0.28136\n",
      "Interaction tuning epoch: 33, train loss: 0.24551, val loss: 0.28579\n",
      "Interaction tuning epoch: 34, train loss: 0.24469, val loss: 0.28291\n",
      "Interaction tuning epoch: 35, train loss: 0.24503, val loss: 0.28773\n",
      "Interaction tuning epoch: 36, train loss: 0.24682, val loss: 0.29100\n",
      "Interaction tuning epoch: 37, train loss: 0.24507, val loss: 0.28328\n",
      "Interaction tuning epoch: 38, train loss: 0.24490, val loss: 0.28589\n",
      "Interaction tuning epoch: 39, train loss: 0.24612, val loss: 0.28699\n",
      "Interaction tuning epoch: 40, train loss: 0.24264, val loss: 0.28541\n",
      "Interaction tuning epoch: 41, train loss: 0.24450, val loss: 0.28468\n",
      "Interaction tuning epoch: 42, train loss: 0.24254, val loss: 0.28270\n",
      "Interaction tuning epoch: 43, train loss: 0.24787, val loss: 0.29082\n",
      "Interaction tuning epoch: 44, train loss: 0.24580, val loss: 0.28484\n",
      "Interaction tuning epoch: 45, train loss: 0.24896, val loss: 0.28674\n",
      "Interaction tuning epoch: 46, train loss: 0.24519, val loss: 0.29260\n",
      "Interaction tuning epoch: 47, train loss: 0.24468, val loss: 0.28210\n",
      "Interaction tuning epoch: 48, train loss: 0.24247, val loss: 0.28327\n",
      "Interaction tuning epoch: 49, train loss: 0.24200, val loss: 0.28344\n",
      "Interaction tuning epoch: 50, train loss: 0.24435, val loss: 0.28225\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 52.93530869483948\n",
      "After the gam stage, training error is 0.24435 , validation error is 0.28225\n",
      "missing value counts: 99238\n",
      "#####start auto_tuning#####\n",
      "the best shrinkage is 0.593750\n",
      "[SoftImpute] Max Singular Value of X_init = 3.670354\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.224518 validation BCE=0.283816,rank=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 1: observed BCE=0.226554 validation BCE=0.281607,rank=3\n",
      "[SoftImpute] Iter 2: observed BCE=0.227090 validation BCE=0.281468,rank=3\n",
      "[SoftImpute] Iter 3: observed BCE=0.226805 validation BCE=0.280999,rank=3\n",
      "[SoftImpute] Iter 4: observed BCE=0.225960 validation BCE=0.280591,rank=3\n",
      "[SoftImpute] Iter 5: observed BCE=0.225580 validation BCE=0.280523,rank=3\n",
      "[SoftImpute] Iter 6: observed BCE=0.224794 validation BCE=0.280852,rank=3\n",
      "[SoftImpute] Iter 7: observed BCE=0.224873 validation BCE=0.279456,rank=3\n",
      "[SoftImpute] Iter 8: observed BCE=0.224637 validation BCE=0.279217,rank=3\n",
      "[SoftImpute] Iter 9: observed BCE=0.224535 validation BCE=0.279002,rank=3\n",
      "[SoftImpute] Iter 10: observed BCE=0.224363 validation BCE=0.279211,rank=3\n",
      "[SoftImpute] Iter 11: observed BCE=0.224207 validation BCE=0.278734,rank=3\n",
      "[SoftImpute] Iter 12: observed BCE=0.224362 validation BCE=0.278916,rank=3\n",
      "[SoftImpute] Iter 13: observed BCE=0.224125 validation BCE=0.279270,rank=3\n",
      "[SoftImpute] Iter 14: observed BCE=0.224091 validation BCE=0.278904,rank=3\n",
      "[SoftImpute] Iter 15: observed BCE=0.224203 validation BCE=0.279076,rank=3\n",
      "[SoftImpute] Iter 16: observed BCE=0.224066 validation BCE=0.279519,rank=3\n",
      "[SoftImpute] Iter 17: observed BCE=0.224082 validation BCE=0.278766,rank=3\n",
      "[SoftImpute] Iter 18: observed BCE=0.223970 validation BCE=0.279232,rank=3\n",
      "[SoftImpute] Iter 19: observed BCE=0.224255 validation BCE=0.279804,rank=3\n",
      "[SoftImpute] Iter 20: observed BCE=0.224436 validation BCE=0.278914,rank=3\n",
      "[SoftImpute] Iter 21: observed BCE=0.224483 validation BCE=0.279044,rank=3\n",
      "[SoftImpute] Iter 22: observed BCE=0.224413 validation BCE=0.279293,rank=3\n",
      "[SoftImpute] Iter 23: observed BCE=0.224353 validation BCE=0.279185,rank=3\n",
      "[SoftImpute] Iter 24: observed BCE=0.224688 validation BCE=0.279483,rank=3\n",
      "[SoftImpute] Iter 25: observed BCE=0.224694 validation BCE=0.279106,rank=3\n",
      "[SoftImpute] Iter 26: observed BCE=0.224528 validation BCE=0.279049,rank=3\n",
      "[SoftImpute] Iter 27: observed BCE=0.224468 validation BCE=0.279215,rank=3\n",
      "[SoftImpute] Iter 28: observed BCE=0.224715 validation BCE=0.279513,rank=3\n",
      "[SoftImpute] Iter 29: observed BCE=0.224905 validation BCE=0.279170,rank=3\n",
      "[SoftImpute] Iter 30: observed BCE=0.224785 validation BCE=0.279046,rank=3\n",
      "[SoftImpute] Iter 31: observed BCE=0.224766 validation BCE=0.279386,rank=3\n",
      "[SoftImpute] Iter 32: observed BCE=0.224905 validation BCE=0.279396,rank=3\n",
      "[SoftImpute] Iter 33: observed BCE=0.224971 validation BCE=0.279348,rank=3\n",
      "[SoftImpute] Iter 34: observed BCE=0.224633 validation BCE=0.279183,rank=3\n",
      "[SoftImpute] Iter 35: observed BCE=0.224982 validation BCE=0.279017,rank=3\n",
      "[SoftImpute] Iter 36: observed BCE=0.225032 validation BCE=0.278972,rank=3\n",
      "[SoftImpute] Iter 37: observed BCE=0.225131 validation BCE=0.279476,rank=3\n",
      "[SoftImpute] Iter 38: observed BCE=0.224969 validation BCE=0.279032,rank=3\n",
      "[SoftImpute] Iter 39: observed BCE=0.224951 validation BCE=0.279007,rank=3\n",
      "[SoftImpute] Iter 40: observed BCE=0.224804 validation BCE=0.279270,rank=3\n",
      "[SoftImpute] Iter 41: observed BCE=0.225102 validation BCE=0.279029,rank=3\n",
      "[SoftImpute] Iter 42: observed BCE=0.225186 validation BCE=0.279091,rank=3\n",
      "[SoftImpute] Iter 43: observed BCE=0.224860 validation BCE=0.279145,rank=3\n",
      "[SoftImpute] Iter 44: observed BCE=0.225041 validation BCE=0.279044,rank=3\n",
      "[SoftImpute] Iter 45: observed BCE=0.224936 validation BCE=0.279106,rank=3\n",
      "[SoftImpute] Iter 46: observed BCE=0.224768 validation BCE=0.278986,rank=3\n",
      "[SoftImpute] Iter 47: observed BCE=0.225245 validation BCE=0.279231,rank=3\n",
      "[SoftImpute] Iter 48: observed BCE=0.225085 validation BCE=0.279149,rank=3\n",
      "[SoftImpute] Iter 49: observed BCE=0.225026 validation BCE=0.279049,rank=3\n",
      "[SoftImpute] Iter 50: observed BCE=0.225067 validation BCE=0.279043,rank=3\n",
      "[SoftImpute] Iter 51: observed BCE=0.225250 validation BCE=0.278853,rank=3\n",
      "[SoftImpute] Iter 52: observed BCE=0.224835 validation BCE=0.279186,rank=3\n",
      "[SoftImpute] Iter 53: observed BCE=0.225213 validation BCE=0.279375,rank=3\n",
      "[SoftImpute] Iter 54: observed BCE=0.224883 validation BCE=0.278821,rank=3\n",
      "[SoftImpute] Iter 55: observed BCE=0.224809 validation BCE=0.279116,rank=3\n",
      "[SoftImpute] Iter 56: observed BCE=0.225041 validation BCE=0.278900,rank=3\n",
      "[SoftImpute] Iter 57: observed BCE=0.224915 validation BCE=0.279183,rank=3\n",
      "[SoftImpute] Iter 58: observed BCE=0.225159 validation BCE=0.279087,rank=3\n",
      "[SoftImpute] Iter 59: observed BCE=0.225195 validation BCE=0.278987,rank=3\n",
      "[SoftImpute] Iter 60: observed BCE=0.225091 validation BCE=0.279045,rank=3\n",
      "[SoftImpute] Iter 61: observed BCE=0.224814 validation BCE=0.279101,rank=3\n",
      "[SoftImpute] Iter 62: observed BCE=0.224974 validation BCE=0.279098,rank=3\n",
      "[SoftImpute] Iter 63: observed BCE=0.225191 validation BCE=0.279012,rank=3\n",
      "[SoftImpute] Iter 64: observed BCE=0.224833 validation BCE=0.278930,rank=3\n",
      "[SoftImpute] Iter 65: observed BCE=0.225165 validation BCE=0.279241,rank=3\n",
      "[SoftImpute] Iter 66: observed BCE=0.224987 validation BCE=0.279156,rank=3\n",
      "[SoftImpute] Iter 67: observed BCE=0.224949 validation BCE=0.278878,rank=3\n",
      "[SoftImpute] Iter 68: observed BCE=0.225165 validation BCE=0.278965,rank=3\n",
      "[SoftImpute] Iter 69: observed BCE=0.225051 validation BCE=0.278930,rank=3\n",
      "[SoftImpute] Iter 70: observed BCE=0.225009 validation BCE=0.279201,rank=3\n",
      "[SoftImpute] Iter 71: observed BCE=0.225165 validation BCE=0.278970,rank=3\n",
      "[SoftImpute] Iter 72: observed BCE=0.225070 validation BCE=0.278836,rank=3\n",
      "[SoftImpute] Iter 73: observed BCE=0.224786 validation BCE=0.279283,rank=3\n",
      "[SoftImpute] Iter 74: observed BCE=0.224978 validation BCE=0.279066,rank=3\n",
      "[SoftImpute] Iter 75: observed BCE=0.225214 validation BCE=0.279089,rank=3\n",
      "[SoftImpute] Iter 76: observed BCE=0.225362 validation BCE=0.278882,rank=3\n",
      "[SoftImpute] Iter 77: observed BCE=0.225119 validation BCE=0.279072,rank=3\n",
      "[SoftImpute] Iter 78: observed BCE=0.224986 validation BCE=0.279044,rank=3\n",
      "[SoftImpute] Iter 79: observed BCE=0.225043 validation BCE=0.279115,rank=3\n",
      "[SoftImpute] Iter 80: observed BCE=0.225313 validation BCE=0.279154,rank=3\n",
      "[SoftImpute] Iter 81: observed BCE=0.225096 validation BCE=0.279047,rank=3\n",
      "[SoftImpute] Iter 82: observed BCE=0.224855 validation BCE=0.278961,rank=3\n",
      "[SoftImpute] Iter 83: observed BCE=0.225068 validation BCE=0.279242,rank=3\n",
      "[SoftImpute] Iter 84: observed BCE=0.225015 validation BCE=0.279114,rank=3\n",
      "[SoftImpute] Iter 85: observed BCE=0.225109 validation BCE=0.279089,rank=3\n",
      "[SoftImpute] Iter 86: observed BCE=0.224924 validation BCE=0.278909,rank=3\n",
      "[SoftImpute] Iter 87: observed BCE=0.225010 validation BCE=0.278919,rank=3\n",
      "[SoftImpute] Iter 88: observed BCE=0.224903 validation BCE=0.279041,rank=3\n",
      "[SoftImpute] Iter 89: observed BCE=0.225289 validation BCE=0.279310,rank=3\n",
      "[SoftImpute] Iter 90: observed BCE=0.225008 validation BCE=0.278715,rank=3\n",
      "[SoftImpute] Iter 91: observed BCE=0.224920 validation BCE=0.279085,rank=3\n",
      "[SoftImpute] Iter 92: observed BCE=0.225040 validation BCE=0.278955,rank=3\n",
      "[SoftImpute] Iter 93: observed BCE=0.225052 validation BCE=0.279014,rank=3\n",
      "[SoftImpute] Iter 94: observed BCE=0.225026 validation BCE=0.279073,rank=3\n",
      "[SoftImpute] Iter 95: observed BCE=0.225011 validation BCE=0.278977,rank=3\n",
      "[SoftImpute] Iter 96: observed BCE=0.225149 validation BCE=0.278886,rank=3\n",
      "[SoftImpute] Iter 97: observed BCE=0.224995 validation BCE=0.279065,rank=3\n",
      "[SoftImpute] Iter 98: observed BCE=0.225232 validation BCE=0.279149,rank=3\n",
      "[SoftImpute] Iter 99: observed BCE=0.225100 validation BCE=0.279072,rank=3\n",
      "[SoftImpute] Iter 100: observed BCE=0.225116 validation BCE=0.278882,rank=3\n",
      "[SoftImpute] Stopped after iteration 100 for lambda=0.073407\n",
      "final num of user group: 6\n",
      "final num of item group: 11\n",
      "change mode state : True\n",
      "time cost: 30.7294762134552\n",
      "After the matrix factor stage, training error is 0.22512, validation error is 0.27888\n",
      "3\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68378, val loss: 0.68553\n",
      "Main effects training epoch: 2, train loss: 0.67745, val loss: 0.67970\n",
      "Main effects training epoch: 3, train loss: 0.67213, val loss: 0.67129\n",
      "Main effects training epoch: 4, train loss: 0.66773, val loss: 0.66633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 5, train loss: 0.65745, val loss: 0.66091\n",
      "Main effects training epoch: 6, train loss: 0.63747, val loss: 0.64405\n",
      "Main effects training epoch: 7, train loss: 0.60599, val loss: 0.61154\n",
      "Main effects training epoch: 8, train loss: 0.57463, val loss: 0.58217\n",
      "Main effects training epoch: 9, train loss: 0.55271, val loss: 0.55738\n",
      "Main effects training epoch: 10, train loss: 0.53476, val loss: 0.54220\n",
      "Main effects training epoch: 11, train loss: 0.53133, val loss: 0.54200\n",
      "Main effects training epoch: 12, train loss: 0.52741, val loss: 0.53679\n",
      "Main effects training epoch: 13, train loss: 0.52675, val loss: 0.54093\n",
      "Main effects training epoch: 14, train loss: 0.52606, val loss: 0.53671\n",
      "Main effects training epoch: 15, train loss: 0.52548, val loss: 0.53840\n",
      "Main effects training epoch: 16, train loss: 0.52634, val loss: 0.53891\n",
      "Main effects training epoch: 17, train loss: 0.52950, val loss: 0.54094\n",
      "Main effects training epoch: 18, train loss: 0.52612, val loss: 0.53870\n",
      "Main effects training epoch: 19, train loss: 0.52588, val loss: 0.53901\n",
      "Main effects training epoch: 20, train loss: 0.52569, val loss: 0.53611\n",
      "Main effects training epoch: 21, train loss: 0.52539, val loss: 0.53993\n",
      "Main effects training epoch: 22, train loss: 0.52533, val loss: 0.53719\n",
      "Main effects training epoch: 23, train loss: 0.52500, val loss: 0.53708\n",
      "Main effects training epoch: 24, train loss: 0.52542, val loss: 0.53821\n",
      "Main effects training epoch: 25, train loss: 0.52501, val loss: 0.53724\n",
      "Main effects training epoch: 26, train loss: 0.52498, val loss: 0.53750\n",
      "Main effects training epoch: 27, train loss: 0.52514, val loss: 0.53840\n",
      "Main effects training epoch: 28, train loss: 0.52559, val loss: 0.53627\n",
      "Main effects training epoch: 29, train loss: 0.52523, val loss: 0.53922\n",
      "Main effects training epoch: 30, train loss: 0.52579, val loss: 0.53582\n",
      "Main effects training epoch: 31, train loss: 0.52667, val loss: 0.54179\n",
      "Main effects training epoch: 32, train loss: 0.52675, val loss: 0.53662\n",
      "Main effects training epoch: 33, train loss: 0.52586, val loss: 0.54063\n",
      "Main effects training epoch: 34, train loss: 0.52517, val loss: 0.53733\n",
      "Main effects training epoch: 35, train loss: 0.52532, val loss: 0.53713\n",
      "Main effects training epoch: 36, train loss: 0.52505, val loss: 0.53803\n",
      "Main effects training epoch: 37, train loss: 0.52496, val loss: 0.53650\n",
      "Main effects training epoch: 38, train loss: 0.52551, val loss: 0.53846\n",
      "Main effects training epoch: 39, train loss: 0.52580, val loss: 0.53816\n",
      "Main effects training epoch: 40, train loss: 0.52487, val loss: 0.53788\n",
      "Main effects training epoch: 41, train loss: 0.52482, val loss: 0.53694\n",
      "Main effects training epoch: 42, train loss: 0.52482, val loss: 0.53755\n",
      "Main effects training epoch: 43, train loss: 0.52464, val loss: 0.53748\n",
      "Main effects training epoch: 44, train loss: 0.52490, val loss: 0.53685\n",
      "Main effects training epoch: 45, train loss: 0.52504, val loss: 0.53854\n",
      "Main effects training epoch: 46, train loss: 0.52474, val loss: 0.53727\n",
      "Main effects training epoch: 47, train loss: 0.52505, val loss: 0.53954\n",
      "Main effects training epoch: 48, train loss: 0.52480, val loss: 0.53586\n",
      "Main effects training epoch: 49, train loss: 0.52513, val loss: 0.53894\n",
      "Main effects training epoch: 50, train loss: 0.52460, val loss: 0.53725\n",
      "Main effects training epoch: 51, train loss: 0.52470, val loss: 0.53581\n",
      "Main effects training epoch: 52, train loss: 0.52527, val loss: 0.53930\n",
      "Main effects training epoch: 53, train loss: 0.52534, val loss: 0.53582\n",
      "Main effects training epoch: 54, train loss: 0.52510, val loss: 0.53965\n",
      "Main effects training epoch: 55, train loss: 0.52525, val loss: 0.53596\n",
      "Main effects training epoch: 56, train loss: 0.52509, val loss: 0.53864\n",
      "Main effects training epoch: 57, train loss: 0.52485, val loss: 0.53593\n",
      "Main effects training epoch: 58, train loss: 0.52610, val loss: 0.54207\n",
      "Main effects training epoch: 59, train loss: 0.52452, val loss: 0.53639\n",
      "Main effects training epoch: 60, train loss: 0.52522, val loss: 0.53807\n",
      "Main effects training epoch: 61, train loss: 0.52496, val loss: 0.53735\n",
      "Main effects training epoch: 62, train loss: 0.52460, val loss: 0.53793\n",
      "Main effects training epoch: 63, train loss: 0.52554, val loss: 0.53718\n",
      "Main effects training epoch: 64, train loss: 0.52517, val loss: 0.53705\n",
      "Main effects training epoch: 65, train loss: 0.52647, val loss: 0.53930\n",
      "Main effects training epoch: 66, train loss: 0.52555, val loss: 0.53618\n",
      "Main effects training epoch: 67, train loss: 0.52501, val loss: 0.53859\n",
      "Main effects training epoch: 68, train loss: 0.52483, val loss: 0.53601\n",
      "Main effects training epoch: 69, train loss: 0.52445, val loss: 0.53714\n",
      "Main effects training epoch: 70, train loss: 0.52472, val loss: 0.53606\n",
      "Main effects training epoch: 71, train loss: 0.52449, val loss: 0.53735\n",
      "Main effects training epoch: 72, train loss: 0.52427, val loss: 0.53718\n",
      "Main effects training epoch: 73, train loss: 0.52471, val loss: 0.53665\n",
      "Main effects training epoch: 74, train loss: 0.52431, val loss: 0.53651\n",
      "Main effects training epoch: 75, train loss: 0.52453, val loss: 0.53712\n",
      "Main effects training epoch: 76, train loss: 0.52494, val loss: 0.53527\n",
      "Main effects training epoch: 77, train loss: 0.52489, val loss: 0.53943\n",
      "Main effects training epoch: 78, train loss: 0.52429, val loss: 0.53599\n",
      "Main effects training epoch: 79, train loss: 0.52432, val loss: 0.53645\n",
      "Main effects training epoch: 80, train loss: 0.52481, val loss: 0.53753\n",
      "Main effects training epoch: 81, train loss: 0.52426, val loss: 0.53685\n",
      "Main effects training epoch: 82, train loss: 0.52439, val loss: 0.53532\n",
      "Main effects training epoch: 83, train loss: 0.52425, val loss: 0.53717\n",
      "Main effects training epoch: 84, train loss: 0.52430, val loss: 0.53671\n",
      "Main effects training epoch: 85, train loss: 0.52472, val loss: 0.53712\n",
      "Main effects training epoch: 86, train loss: 0.52518, val loss: 0.53745\n",
      "Main effects training epoch: 87, train loss: 0.52441, val loss: 0.53724\n",
      "Main effects training epoch: 88, train loss: 0.52473, val loss: 0.53650\n",
      "Main effects training epoch: 89, train loss: 0.52563, val loss: 0.53948\n",
      "Main effects training epoch: 90, train loss: 0.52451, val loss: 0.53633\n",
      "Main effects training epoch: 91, train loss: 0.52429, val loss: 0.53780\n",
      "Main effects training epoch: 92, train loss: 0.52403, val loss: 0.53614\n",
      "Main effects training epoch: 93, train loss: 0.52424, val loss: 0.53744\n",
      "Main effects training epoch: 94, train loss: 0.52441, val loss: 0.53552\n",
      "Main effects training epoch: 95, train loss: 0.52463, val loss: 0.53803\n",
      "Main effects training epoch: 96, train loss: 0.52506, val loss: 0.53550\n",
      "Main effects training epoch: 97, train loss: 0.52552, val loss: 0.53959\n",
      "Main effects training epoch: 98, train loss: 0.52425, val loss: 0.53571\n",
      "Main effects training epoch: 99, train loss: 0.52508, val loss: 0.53922\n",
      "Main effects training epoch: 100, train loss: 0.52399, val loss: 0.53584\n",
      "Main effects training epoch: 101, train loss: 0.52397, val loss: 0.53717\n",
      "Main effects training epoch: 102, train loss: 0.52398, val loss: 0.53543\n",
      "Main effects training epoch: 103, train loss: 0.52388, val loss: 0.53636\n",
      "Main effects training epoch: 104, train loss: 0.52395, val loss: 0.53646\n",
      "Main effects training epoch: 105, train loss: 0.52408, val loss: 0.53660\n",
      "Main effects training epoch: 106, train loss: 0.52416, val loss: 0.53695\n",
      "Main effects training epoch: 107, train loss: 0.52481, val loss: 0.53834\n",
      "Main effects training epoch: 108, train loss: 0.52449, val loss: 0.53528\n",
      "Main effects training epoch: 109, train loss: 0.52423, val loss: 0.53749\n",
      "Main effects training epoch: 110, train loss: 0.52392, val loss: 0.53560\n",
      "Main effects training epoch: 111, train loss: 0.52378, val loss: 0.53663\n",
      "Main effects training epoch: 112, train loss: 0.52394, val loss: 0.53522\n",
      "Main effects training epoch: 113, train loss: 0.52445, val loss: 0.53769\n",
      "Main effects training epoch: 114, train loss: 0.52565, val loss: 0.53477\n",
      "Main effects training epoch: 115, train loss: 0.52436, val loss: 0.53831\n",
      "Main effects training epoch: 116, train loss: 0.52428, val loss: 0.53552\n",
      "Main effects training epoch: 117, train loss: 0.52398, val loss: 0.53617\n",
      "Main effects training epoch: 118, train loss: 0.52374, val loss: 0.53560\n",
      "Main effects training epoch: 119, train loss: 0.52372, val loss: 0.53611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 120, train loss: 0.52383, val loss: 0.53525\n",
      "Main effects training epoch: 121, train loss: 0.52396, val loss: 0.53731\n",
      "Main effects training epoch: 122, train loss: 0.52389, val loss: 0.53522\n",
      "Main effects training epoch: 123, train loss: 0.52364, val loss: 0.53536\n",
      "Main effects training epoch: 124, train loss: 0.52404, val loss: 0.53642\n",
      "Main effects training epoch: 125, train loss: 0.52386, val loss: 0.53492\n",
      "Main effects training epoch: 126, train loss: 0.52408, val loss: 0.53767\n",
      "Main effects training epoch: 127, train loss: 0.52405, val loss: 0.53549\n",
      "Main effects training epoch: 128, train loss: 0.52371, val loss: 0.53628\n",
      "Main effects training epoch: 129, train loss: 0.52389, val loss: 0.53724\n",
      "Main effects training epoch: 130, train loss: 0.52372, val loss: 0.53535\n",
      "Main effects training epoch: 131, train loss: 0.52355, val loss: 0.53495\n",
      "Main effects training epoch: 132, train loss: 0.52377, val loss: 0.53633\n",
      "Main effects training epoch: 133, train loss: 0.52365, val loss: 0.53535\n",
      "Main effects training epoch: 134, train loss: 0.52394, val loss: 0.53664\n",
      "Main effects training epoch: 135, train loss: 0.52377, val loss: 0.53539\n",
      "Main effects training epoch: 136, train loss: 0.52350, val loss: 0.53552\n",
      "Main effects training epoch: 137, train loss: 0.52348, val loss: 0.53508\n",
      "Main effects training epoch: 138, train loss: 0.52389, val loss: 0.53680\n",
      "Main effects training epoch: 139, train loss: 0.52422, val loss: 0.53712\n",
      "Main effects training epoch: 140, train loss: 0.52405, val loss: 0.53616\n",
      "Main effects training epoch: 141, train loss: 0.52519, val loss: 0.53558\n",
      "Main effects training epoch: 142, train loss: 0.52471, val loss: 0.53887\n",
      "Main effects training epoch: 143, train loss: 0.52393, val loss: 0.53475\n",
      "Main effects training epoch: 144, train loss: 0.52368, val loss: 0.53387\n",
      "Main effects training epoch: 145, train loss: 0.52365, val loss: 0.53714\n",
      "Main effects training epoch: 146, train loss: 0.52329, val loss: 0.53448\n",
      "Main effects training epoch: 147, train loss: 0.52333, val loss: 0.53479\n",
      "Main effects training epoch: 148, train loss: 0.52335, val loss: 0.53533\n",
      "Main effects training epoch: 149, train loss: 0.52334, val loss: 0.53650\n",
      "Main effects training epoch: 150, train loss: 0.52370, val loss: 0.53374\n",
      "Main effects training epoch: 151, train loss: 0.52421, val loss: 0.53760\n",
      "Main effects training epoch: 152, train loss: 0.52332, val loss: 0.53567\n",
      "Main effects training epoch: 153, train loss: 0.52318, val loss: 0.53514\n",
      "Main effects training epoch: 154, train loss: 0.52339, val loss: 0.53520\n",
      "Main effects training epoch: 155, train loss: 0.52374, val loss: 0.53436\n",
      "Main effects training epoch: 156, train loss: 0.52341, val loss: 0.53640\n",
      "Main effects training epoch: 157, train loss: 0.52372, val loss: 0.53339\n",
      "Main effects training epoch: 158, train loss: 0.52357, val loss: 0.53750\n",
      "Main effects training epoch: 159, train loss: 0.52383, val loss: 0.53404\n",
      "Main effects training epoch: 160, train loss: 0.52428, val loss: 0.53665\n",
      "Main effects training epoch: 161, train loss: 0.52500, val loss: 0.53428\n",
      "Main effects training epoch: 162, train loss: 0.52379, val loss: 0.53532\n",
      "Main effects training epoch: 163, train loss: 0.52385, val loss: 0.53505\n",
      "Main effects training epoch: 164, train loss: 0.52490, val loss: 0.53888\n",
      "Main effects training epoch: 165, train loss: 0.52342, val loss: 0.53307\n",
      "Main effects training epoch: 166, train loss: 0.52357, val loss: 0.53774\n",
      "Main effects training epoch: 167, train loss: 0.52332, val loss: 0.53404\n",
      "Main effects training epoch: 168, train loss: 0.52288, val loss: 0.53529\n",
      "Main effects training epoch: 169, train loss: 0.52305, val loss: 0.53301\n",
      "Main effects training epoch: 170, train loss: 0.52274, val loss: 0.53485\n",
      "Main effects training epoch: 171, train loss: 0.52279, val loss: 0.53384\n",
      "Main effects training epoch: 172, train loss: 0.52276, val loss: 0.53535\n",
      "Main effects training epoch: 173, train loss: 0.52350, val loss: 0.53447\n",
      "Main effects training epoch: 174, train loss: 0.52358, val loss: 0.53421\n",
      "Main effects training epoch: 175, train loss: 0.52403, val loss: 0.53687\n",
      "Main effects training epoch: 176, train loss: 0.52333, val loss: 0.53524\n",
      "Main effects training epoch: 177, train loss: 0.52331, val loss: 0.53427\n",
      "Main effects training epoch: 178, train loss: 0.52279, val loss: 0.53548\n",
      "Main effects training epoch: 179, train loss: 0.52284, val loss: 0.53481\n",
      "Main effects training epoch: 180, train loss: 0.52263, val loss: 0.53411\n",
      "Main effects training epoch: 181, train loss: 0.52331, val loss: 0.53632\n",
      "Main effects training epoch: 182, train loss: 0.52283, val loss: 0.53218\n",
      "Main effects training epoch: 183, train loss: 0.52282, val loss: 0.53480\n",
      "Main effects training epoch: 184, train loss: 0.52343, val loss: 0.53301\n",
      "Main effects training epoch: 185, train loss: 0.52336, val loss: 0.53693\n",
      "Main effects training epoch: 186, train loss: 0.52314, val loss: 0.53271\n",
      "Main effects training epoch: 187, train loss: 0.52273, val loss: 0.53616\n",
      "Main effects training epoch: 188, train loss: 0.52249, val loss: 0.53292\n",
      "Main effects training epoch: 189, train loss: 0.52240, val loss: 0.53313\n",
      "Main effects training epoch: 190, train loss: 0.52229, val loss: 0.53351\n",
      "Main effects training epoch: 191, train loss: 0.52227, val loss: 0.53397\n",
      "Main effects training epoch: 192, train loss: 0.52238, val loss: 0.53484\n",
      "Main effects training epoch: 193, train loss: 0.52220, val loss: 0.53243\n",
      "Main effects training epoch: 194, train loss: 0.52274, val loss: 0.53561\n",
      "Main effects training epoch: 195, train loss: 0.52243, val loss: 0.53305\n",
      "Main effects training epoch: 196, train loss: 0.52300, val loss: 0.53635\n",
      "Main effects training epoch: 197, train loss: 0.52294, val loss: 0.53189\n",
      "Main effects training epoch: 198, train loss: 0.52317, val loss: 0.53709\n",
      "Main effects training epoch: 199, train loss: 0.52256, val loss: 0.53223\n",
      "Main effects training epoch: 200, train loss: 0.52295, val loss: 0.53624\n",
      "Main effects training epoch: 201, train loss: 0.52259, val loss: 0.53247\n",
      "Main effects training epoch: 202, train loss: 0.52221, val loss: 0.53501\n",
      "Main effects training epoch: 203, train loss: 0.52225, val loss: 0.53156\n",
      "Main effects training epoch: 204, train loss: 0.52242, val loss: 0.53501\n",
      "Main effects training epoch: 205, train loss: 0.52196, val loss: 0.53368\n",
      "Main effects training epoch: 206, train loss: 0.52203, val loss: 0.53241\n",
      "Main effects training epoch: 207, train loss: 0.52254, val loss: 0.53576\n",
      "Main effects training epoch: 208, train loss: 0.52217, val loss: 0.53207\n",
      "Main effects training epoch: 209, train loss: 0.52166, val loss: 0.53282\n",
      "Main effects training epoch: 210, train loss: 0.52172, val loss: 0.53391\n",
      "Main effects training epoch: 211, train loss: 0.52154, val loss: 0.53274\n",
      "Main effects training epoch: 212, train loss: 0.52159, val loss: 0.53152\n",
      "Main effects training epoch: 213, train loss: 0.52219, val loss: 0.53574\n",
      "Main effects training epoch: 214, train loss: 0.52246, val loss: 0.53133\n",
      "Main effects training epoch: 215, train loss: 0.52288, val loss: 0.53628\n",
      "Main effects training epoch: 216, train loss: 0.52220, val loss: 0.53103\n",
      "Main effects training epoch: 217, train loss: 0.52173, val loss: 0.53441\n",
      "Main effects training epoch: 218, train loss: 0.52154, val loss: 0.53132\n",
      "Main effects training epoch: 219, train loss: 0.52121, val loss: 0.53257\n",
      "Main effects training epoch: 220, train loss: 0.52135, val loss: 0.53313\n",
      "Main effects training epoch: 221, train loss: 0.52124, val loss: 0.53150\n",
      "Main effects training epoch: 222, train loss: 0.52125, val loss: 0.53224\n",
      "Main effects training epoch: 223, train loss: 0.52100, val loss: 0.53173\n",
      "Main effects training epoch: 224, train loss: 0.52102, val loss: 0.53224\n",
      "Main effects training epoch: 225, train loss: 0.52090, val loss: 0.53167\n",
      "Main effects training epoch: 226, train loss: 0.52093, val loss: 0.53255\n",
      "Main effects training epoch: 227, train loss: 0.52087, val loss: 0.53094\n",
      "Main effects training epoch: 228, train loss: 0.52106, val loss: 0.53426\n",
      "Main effects training epoch: 229, train loss: 0.52147, val loss: 0.53092\n",
      "Main effects training epoch: 230, train loss: 0.52062, val loss: 0.53139\n",
      "Main effects training epoch: 231, train loss: 0.52071, val loss: 0.53096\n",
      "Main effects training epoch: 232, train loss: 0.52146, val loss: 0.53366\n",
      "Main effects training epoch: 233, train loss: 0.52132, val loss: 0.53273\n",
      "Main effects training epoch: 234, train loss: 0.52121, val loss: 0.53149\n",
      "Main effects training epoch: 235, train loss: 0.52058, val loss: 0.53076\n",
      "Main effects training epoch: 236, train loss: 0.52036, val loss: 0.53156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 237, train loss: 0.52058, val loss: 0.53108\n",
      "Main effects training epoch: 238, train loss: 0.52058, val loss: 0.53145\n",
      "Main effects training epoch: 239, train loss: 0.52044, val loss: 0.53307\n",
      "Main effects training epoch: 240, train loss: 0.52036, val loss: 0.53136\n",
      "Main effects training epoch: 241, train loss: 0.52021, val loss: 0.53063\n",
      "Main effects training epoch: 242, train loss: 0.52042, val loss: 0.52979\n",
      "Main effects training epoch: 243, train loss: 0.52038, val loss: 0.53222\n",
      "Main effects training epoch: 244, train loss: 0.52010, val loss: 0.53119\n",
      "Main effects training epoch: 245, train loss: 0.52007, val loss: 0.53179\n",
      "Main effects training epoch: 246, train loss: 0.52026, val loss: 0.52989\n",
      "Main effects training epoch: 247, train loss: 0.52013, val loss: 0.53177\n",
      "Main effects training epoch: 248, train loss: 0.52057, val loss: 0.52952\n",
      "Main effects training epoch: 249, train loss: 0.52057, val loss: 0.53336\n",
      "Main effects training epoch: 250, train loss: 0.52040, val loss: 0.52992\n",
      "Main effects training epoch: 251, train loss: 0.51993, val loss: 0.53174\n",
      "Main effects training epoch: 252, train loss: 0.52031, val loss: 0.52938\n",
      "Main effects training epoch: 253, train loss: 0.52002, val loss: 0.53241\n",
      "Main effects training epoch: 254, train loss: 0.52011, val loss: 0.52923\n",
      "Main effects training epoch: 255, train loss: 0.51981, val loss: 0.53077\n",
      "Main effects training epoch: 256, train loss: 0.52097, val loss: 0.52992\n",
      "Main effects training epoch: 257, train loss: 0.52058, val loss: 0.53373\n",
      "Main effects training epoch: 258, train loss: 0.52034, val loss: 0.52977\n",
      "Main effects training epoch: 259, train loss: 0.51960, val loss: 0.52954\n",
      "Main effects training epoch: 260, train loss: 0.52021, val loss: 0.53285\n",
      "Main effects training epoch: 261, train loss: 0.52178, val loss: 0.52892\n",
      "Main effects training epoch: 262, train loss: 0.52002, val loss: 0.53337\n",
      "Main effects training epoch: 263, train loss: 0.51967, val loss: 0.52914\n",
      "Main effects training epoch: 264, train loss: 0.51962, val loss: 0.53105\n",
      "Main effects training epoch: 265, train loss: 0.51954, val loss: 0.52883\n",
      "Main effects training epoch: 266, train loss: 0.51941, val loss: 0.53178\n",
      "Main effects training epoch: 267, train loss: 0.52063, val loss: 0.52978\n",
      "Main effects training epoch: 268, train loss: 0.52086, val loss: 0.53351\n",
      "Main effects training epoch: 269, train loss: 0.51985, val loss: 0.52897\n",
      "Main effects training epoch: 270, train loss: 0.51925, val loss: 0.53183\n",
      "Main effects training epoch: 271, train loss: 0.51933, val loss: 0.53133\n",
      "Main effects training epoch: 272, train loss: 0.51936, val loss: 0.53049\n",
      "Main effects training epoch: 273, train loss: 0.51905, val loss: 0.53050\n",
      "Main effects training epoch: 274, train loss: 0.51893, val loss: 0.52940\n",
      "Main effects training epoch: 275, train loss: 0.51921, val loss: 0.53142\n",
      "Main effects training epoch: 276, train loss: 0.51902, val loss: 0.53149\n",
      "Main effects training epoch: 277, train loss: 0.51894, val loss: 0.52972\n",
      "Main effects training epoch: 278, train loss: 0.51977, val loss: 0.53459\n",
      "Main effects training epoch: 279, train loss: 0.51972, val loss: 0.52974\n",
      "Main effects training epoch: 280, train loss: 0.51930, val loss: 0.53121\n",
      "Main effects training epoch: 281, train loss: 0.51898, val loss: 0.53002\n",
      "Main effects training epoch: 282, train loss: 0.51893, val loss: 0.53078\n",
      "Main effects training epoch: 283, train loss: 0.51897, val loss: 0.53022\n",
      "Main effects training epoch: 284, train loss: 0.51926, val loss: 0.53047\n",
      "Main effects training epoch: 285, train loss: 0.51865, val loss: 0.53156\n",
      "Main effects training epoch: 286, train loss: 0.51874, val loss: 0.53052\n",
      "Main effects training epoch: 287, train loss: 0.51893, val loss: 0.53200\n",
      "Main effects training epoch: 288, train loss: 0.51833, val loss: 0.53046\n",
      "Main effects training epoch: 289, train loss: 0.51845, val loss: 0.53190\n",
      "Main effects training epoch: 290, train loss: 0.51857, val loss: 0.52951\n",
      "Main effects training epoch: 291, train loss: 0.51867, val loss: 0.53131\n",
      "Main effects training epoch: 292, train loss: 0.51851, val loss: 0.53018\n",
      "Main effects training epoch: 293, train loss: 0.51896, val loss: 0.53263\n",
      "Main effects training epoch: 294, train loss: 0.51874, val loss: 0.52910\n",
      "Main effects training epoch: 295, train loss: 0.51875, val loss: 0.53090\n",
      "Main effects training epoch: 296, train loss: 0.51835, val loss: 0.53051\n",
      "Main effects training epoch: 297, train loss: 0.51840, val loss: 0.52960\n",
      "Main effects training epoch: 298, train loss: 0.51844, val loss: 0.53134\n",
      "Main effects training epoch: 299, train loss: 0.51819, val loss: 0.53121\n",
      "Main effects training epoch: 300, train loss: 0.51859, val loss: 0.53097\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.52164, val loss: 0.52653\n",
      "Main effects tuning epoch: 2, train loss: 0.52149, val loss: 0.52725\n",
      "Main effects tuning epoch: 3, train loss: 0.52202, val loss: 0.52969\n",
      "Main effects tuning epoch: 4, train loss: 0.52169, val loss: 0.52750\n",
      "Main effects tuning epoch: 5, train loss: 0.52169, val loss: 0.52758\n",
      "Main effects tuning epoch: 6, train loss: 0.52145, val loss: 0.52701\n",
      "Main effects tuning epoch: 7, train loss: 0.52140, val loss: 0.52736\n",
      "Main effects tuning epoch: 8, train loss: 0.52131, val loss: 0.52759\n",
      "Main effects tuning epoch: 9, train loss: 0.52137, val loss: 0.52745\n",
      "Main effects tuning epoch: 10, train loss: 0.52162, val loss: 0.52838\n",
      "Main effects tuning epoch: 11, train loss: 0.52132, val loss: 0.52619\n",
      "Main effects tuning epoch: 12, train loss: 0.52133, val loss: 0.52783\n",
      "Main effects tuning epoch: 13, train loss: 0.52127, val loss: 0.52644\n",
      "Main effects tuning epoch: 14, train loss: 0.52120, val loss: 0.52772\n",
      "Main effects tuning epoch: 15, train loss: 0.52140, val loss: 0.52694\n",
      "Main effects tuning epoch: 16, train loss: 0.52147, val loss: 0.52848\n",
      "Main effects tuning epoch: 17, train loss: 0.52163, val loss: 0.52513\n",
      "Main effects tuning epoch: 18, train loss: 0.52137, val loss: 0.52861\n",
      "Main effects tuning epoch: 19, train loss: 0.52130, val loss: 0.52606\n",
      "Main effects tuning epoch: 20, train loss: 0.52144, val loss: 0.52855\n",
      "Main effects tuning epoch: 21, train loss: 0.52107, val loss: 0.52617\n",
      "Main effects tuning epoch: 22, train loss: 0.52123, val loss: 0.52808\n",
      "Main effects tuning epoch: 23, train loss: 0.52135, val loss: 0.52593\n",
      "Main effects tuning epoch: 24, train loss: 0.52107, val loss: 0.52630\n",
      "Main effects tuning epoch: 25, train loss: 0.52115, val loss: 0.52753\n",
      "Main effects tuning epoch: 26, train loss: 0.52108, val loss: 0.52576\n",
      "Main effects tuning epoch: 27, train loss: 0.52117, val loss: 0.52717\n",
      "Main effects tuning epoch: 28, train loss: 0.52104, val loss: 0.52680\n",
      "Main effects tuning epoch: 29, train loss: 0.52117, val loss: 0.52599\n",
      "Main effects tuning epoch: 30, train loss: 0.52180, val loss: 0.52947\n",
      "Main effects tuning epoch: 31, train loss: 0.52194, val loss: 0.52574\n",
      "Main effects tuning epoch: 32, train loss: 0.52132, val loss: 0.52796\n",
      "Main effects tuning epoch: 33, train loss: 0.52122, val loss: 0.52650\n",
      "Main effects tuning epoch: 34, train loss: 0.52135, val loss: 0.52727\n",
      "Main effects tuning epoch: 35, train loss: 0.52092, val loss: 0.52549\n",
      "Main effects tuning epoch: 36, train loss: 0.52106, val loss: 0.52752\n",
      "Main effects tuning epoch: 37, train loss: 0.52095, val loss: 0.52655\n",
      "Main effects tuning epoch: 38, train loss: 0.52073, val loss: 0.52602\n",
      "Main effects tuning epoch: 39, train loss: 0.52088, val loss: 0.52701\n",
      "Main effects tuning epoch: 40, train loss: 0.52086, val loss: 0.52627\n",
      "Main effects tuning epoch: 41, train loss: 0.52082, val loss: 0.52566\n",
      "Main effects tuning epoch: 42, train loss: 0.52076, val loss: 0.52603\n",
      "Main effects tuning epoch: 43, train loss: 0.52078, val loss: 0.52721\n",
      "Main effects tuning epoch: 44, train loss: 0.52086, val loss: 0.52521\n",
      "Main effects tuning epoch: 45, train loss: 0.52136, val loss: 0.52869\n",
      "Main effects tuning epoch: 46, train loss: 0.52084, val loss: 0.52690\n",
      "Main effects tuning epoch: 47, train loss: 0.52091, val loss: 0.52520\n",
      "Main effects tuning epoch: 48, train loss: 0.52057, val loss: 0.52582\n",
      "Main effects tuning epoch: 49, train loss: 0.52079, val loss: 0.52525\n",
      "Main effects tuning epoch: 50, train loss: 0.52084, val loss: 0.52723\n",
      "##########Stage 2: interaction training start.##########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 1, train loss: 0.47985, val loss: 0.49159\n",
      "Interaction training epoch: 2, train loss: 0.36691, val loss: 0.36008\n",
      "Interaction training epoch: 3, train loss: 0.32147, val loss: 0.30998\n",
      "Interaction training epoch: 4, train loss: 0.31659, val loss: 0.31602\n",
      "Interaction training epoch: 5, train loss: 0.29807, val loss: 0.29777\n",
      "Interaction training epoch: 6, train loss: 0.30306, val loss: 0.29982\n",
      "Interaction training epoch: 7, train loss: 0.29194, val loss: 0.28342\n",
      "Interaction training epoch: 8, train loss: 0.28912, val loss: 0.27529\n",
      "Interaction training epoch: 9, train loss: 0.29104, val loss: 0.28277\n",
      "Interaction training epoch: 10, train loss: 0.29408, val loss: 0.27743\n",
      "Interaction training epoch: 11, train loss: 0.28995, val loss: 0.28151\n",
      "Interaction training epoch: 12, train loss: 0.30453, val loss: 0.30614\n",
      "Interaction training epoch: 13, train loss: 0.30463, val loss: 0.29511\n",
      "Interaction training epoch: 14, train loss: 0.29206, val loss: 0.27928\n",
      "Interaction training epoch: 15, train loss: 0.29263, val loss: 0.28607\n",
      "Interaction training epoch: 16, train loss: 0.29256, val loss: 0.28221\n",
      "Interaction training epoch: 17, train loss: 0.28682, val loss: 0.27324\n",
      "Interaction training epoch: 18, train loss: 0.28633, val loss: 0.27762\n",
      "Interaction training epoch: 19, train loss: 0.28737, val loss: 0.28079\n",
      "Interaction training epoch: 20, train loss: 0.28633, val loss: 0.27513\n",
      "Interaction training epoch: 21, train loss: 0.28541, val loss: 0.27946\n",
      "Interaction training epoch: 22, train loss: 0.29328, val loss: 0.28621\n",
      "Interaction training epoch: 23, train loss: 0.28884, val loss: 0.27968\n",
      "Interaction training epoch: 24, train loss: 0.28743, val loss: 0.27473\n",
      "Interaction training epoch: 25, train loss: 0.28907, val loss: 0.28442\n",
      "Interaction training epoch: 26, train loss: 0.28792, val loss: 0.27614\n",
      "Interaction training epoch: 27, train loss: 0.28692, val loss: 0.27643\n",
      "Interaction training epoch: 28, train loss: 0.28709, val loss: 0.28047\n",
      "Interaction training epoch: 29, train loss: 0.27917, val loss: 0.27248\n",
      "Interaction training epoch: 30, train loss: 0.28995, val loss: 0.28079\n",
      "Interaction training epoch: 31, train loss: 0.29068, val loss: 0.28463\n",
      "Interaction training epoch: 32, train loss: 0.28258, val loss: 0.27713\n",
      "Interaction training epoch: 33, train loss: 0.28329, val loss: 0.27542\n",
      "Interaction training epoch: 34, train loss: 0.28520, val loss: 0.28004\n",
      "Interaction training epoch: 35, train loss: 0.28137, val loss: 0.27322\n",
      "Interaction training epoch: 36, train loss: 0.28362, val loss: 0.27346\n",
      "Interaction training epoch: 37, train loss: 0.28506, val loss: 0.27825\n",
      "Interaction training epoch: 38, train loss: 0.28192, val loss: 0.27479\n",
      "Interaction training epoch: 39, train loss: 0.27896, val loss: 0.27241\n",
      "Interaction training epoch: 40, train loss: 0.28288, val loss: 0.27808\n",
      "Interaction training epoch: 41, train loss: 0.28009, val loss: 0.27381\n",
      "Interaction training epoch: 42, train loss: 0.27945, val loss: 0.27120\n",
      "Interaction training epoch: 43, train loss: 0.27977, val loss: 0.27868\n",
      "Interaction training epoch: 44, train loss: 0.27785, val loss: 0.27102\n",
      "Interaction training epoch: 45, train loss: 0.27742, val loss: 0.27512\n",
      "Interaction training epoch: 46, train loss: 0.28110, val loss: 0.27875\n",
      "Interaction training epoch: 47, train loss: 0.28153, val loss: 0.28047\n",
      "Interaction training epoch: 48, train loss: 0.27717, val loss: 0.27193\n",
      "Interaction training epoch: 49, train loss: 0.27756, val loss: 0.27902\n",
      "Interaction training epoch: 50, train loss: 0.27849, val loss: 0.27654\n",
      "Interaction training epoch: 51, train loss: 0.28025, val loss: 0.28070\n",
      "Interaction training epoch: 52, train loss: 0.27786, val loss: 0.27743\n",
      "Interaction training epoch: 53, train loss: 0.27721, val loss: 0.27389\n",
      "Interaction training epoch: 54, train loss: 0.27627, val loss: 0.27353\n",
      "Interaction training epoch: 55, train loss: 0.27414, val loss: 0.27542\n",
      "Interaction training epoch: 56, train loss: 0.28189, val loss: 0.28402\n",
      "Interaction training epoch: 57, train loss: 0.27603, val loss: 0.27221\n",
      "Interaction training epoch: 58, train loss: 0.27320, val loss: 0.27265\n",
      "Interaction training epoch: 59, train loss: 0.27580, val loss: 0.27828\n",
      "Interaction training epoch: 60, train loss: 0.27409, val loss: 0.27091\n",
      "Interaction training epoch: 61, train loss: 0.27310, val loss: 0.27225\n",
      "Interaction training epoch: 62, train loss: 0.27215, val loss: 0.27288\n",
      "Interaction training epoch: 63, train loss: 0.27210, val loss: 0.27036\n",
      "Interaction training epoch: 64, train loss: 0.27914, val loss: 0.28451\n",
      "Interaction training epoch: 65, train loss: 0.27350, val loss: 0.27049\n",
      "Interaction training epoch: 66, train loss: 0.27100, val loss: 0.27052\n",
      "Interaction training epoch: 67, train loss: 0.27284, val loss: 0.27575\n",
      "Interaction training epoch: 68, train loss: 0.27497, val loss: 0.27559\n",
      "Interaction training epoch: 69, train loss: 0.27100, val loss: 0.26822\n",
      "Interaction training epoch: 70, train loss: 0.27220, val loss: 0.27683\n",
      "Interaction training epoch: 71, train loss: 0.27368, val loss: 0.27436\n",
      "Interaction training epoch: 72, train loss: 0.27069, val loss: 0.27121\n",
      "Interaction training epoch: 73, train loss: 0.27117, val loss: 0.27355\n",
      "Interaction training epoch: 74, train loss: 0.27236, val loss: 0.27570\n",
      "Interaction training epoch: 75, train loss: 0.26984, val loss: 0.26894\n",
      "Interaction training epoch: 76, train loss: 0.27279, val loss: 0.27728\n",
      "Interaction training epoch: 77, train loss: 0.26992, val loss: 0.26804\n",
      "Interaction training epoch: 78, train loss: 0.26794, val loss: 0.26838\n",
      "Interaction training epoch: 79, train loss: 0.26817, val loss: 0.27123\n",
      "Interaction training epoch: 80, train loss: 0.27059, val loss: 0.27263\n",
      "Interaction training epoch: 81, train loss: 0.26992, val loss: 0.27019\n",
      "Interaction training epoch: 82, train loss: 0.26832, val loss: 0.27191\n",
      "Interaction training epoch: 83, train loss: 0.27029, val loss: 0.27091\n",
      "Interaction training epoch: 84, train loss: 0.26912, val loss: 0.27575\n",
      "Interaction training epoch: 85, train loss: 0.27038, val loss: 0.27485\n",
      "Interaction training epoch: 86, train loss: 0.27133, val loss: 0.27417\n",
      "Interaction training epoch: 87, train loss: 0.26762, val loss: 0.26684\n",
      "Interaction training epoch: 88, train loss: 0.26754, val loss: 0.27197\n",
      "Interaction training epoch: 89, train loss: 0.26526, val loss: 0.26508\n",
      "Interaction training epoch: 90, train loss: 0.26936, val loss: 0.27746\n",
      "Interaction training epoch: 91, train loss: 0.28242, val loss: 0.29251\n",
      "Interaction training epoch: 92, train loss: 0.26802, val loss: 0.26681\n",
      "Interaction training epoch: 93, train loss: 0.27967, val loss: 0.28991\n",
      "Interaction training epoch: 94, train loss: 0.26659, val loss: 0.26882\n",
      "Interaction training epoch: 95, train loss: 0.26431, val loss: 0.26927\n",
      "Interaction training epoch: 96, train loss: 0.26732, val loss: 0.27680\n",
      "Interaction training epoch: 97, train loss: 0.26347, val loss: 0.26469\n",
      "Interaction training epoch: 98, train loss: 0.26573, val loss: 0.27212\n",
      "Interaction training epoch: 99, train loss: 0.26642, val loss: 0.27268\n",
      "Interaction training epoch: 100, train loss: 0.26367, val loss: 0.26772\n",
      "Interaction training epoch: 101, train loss: 0.26726, val loss: 0.27682\n",
      "Interaction training epoch: 102, train loss: 0.26579, val loss: 0.27026\n",
      "Interaction training epoch: 103, train loss: 0.26284, val loss: 0.26874\n",
      "Interaction training epoch: 104, train loss: 0.26585, val loss: 0.27516\n",
      "Interaction training epoch: 105, train loss: 0.26230, val loss: 0.26589\n",
      "Interaction training epoch: 106, train loss: 0.26176, val loss: 0.26586\n",
      "Interaction training epoch: 107, train loss: 0.26080, val loss: 0.26689\n",
      "Interaction training epoch: 108, train loss: 0.26133, val loss: 0.26893\n",
      "Interaction training epoch: 109, train loss: 0.26455, val loss: 0.27250\n",
      "Interaction training epoch: 110, train loss: 0.26452, val loss: 0.27071\n",
      "Interaction training epoch: 111, train loss: 0.27053, val loss: 0.28750\n",
      "Interaction training epoch: 112, train loss: 0.26227, val loss: 0.27029\n",
      "Interaction training epoch: 113, train loss: 0.26079, val loss: 0.26680\n",
      "Interaction training epoch: 114, train loss: 0.26247, val loss: 0.27576\n",
      "Interaction training epoch: 115, train loss: 0.26411, val loss: 0.27281\n",
      "Interaction training epoch: 116, train loss: 0.25918, val loss: 0.26840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 117, train loss: 0.26054, val loss: 0.26867\n",
      "Interaction training epoch: 118, train loss: 0.25875, val loss: 0.27011\n",
      "Interaction training epoch: 119, train loss: 0.25867, val loss: 0.26599\n",
      "Interaction training epoch: 120, train loss: 0.25794, val loss: 0.27126\n",
      "Interaction training epoch: 121, train loss: 0.25971, val loss: 0.26775\n",
      "Interaction training epoch: 122, train loss: 0.26151, val loss: 0.27093\n",
      "Interaction training epoch: 123, train loss: 0.25735, val loss: 0.26678\n",
      "Interaction training epoch: 124, train loss: 0.26046, val loss: 0.27291\n",
      "Interaction training epoch: 125, train loss: 0.25945, val loss: 0.26700\n",
      "Interaction training epoch: 126, train loss: 0.25540, val loss: 0.26753\n",
      "Interaction training epoch: 127, train loss: 0.25732, val loss: 0.26989\n",
      "Interaction training epoch: 128, train loss: 0.25657, val loss: 0.26739\n",
      "Interaction training epoch: 129, train loss: 0.25748, val loss: 0.27308\n",
      "Interaction training epoch: 130, train loss: 0.26127, val loss: 0.27735\n",
      "Interaction training epoch: 131, train loss: 0.25477, val loss: 0.27006\n",
      "Interaction training epoch: 132, train loss: 0.25716, val loss: 0.27153\n",
      "Interaction training epoch: 133, train loss: 0.25856, val loss: 0.27402\n",
      "Interaction training epoch: 134, train loss: 0.25436, val loss: 0.26895\n",
      "Interaction training epoch: 135, train loss: 0.25560, val loss: 0.27080\n",
      "Interaction training epoch: 136, train loss: 0.25490, val loss: 0.26751\n",
      "Interaction training epoch: 137, train loss: 0.25500, val loss: 0.27509\n",
      "Interaction training epoch: 138, train loss: 0.25550, val loss: 0.26862\n",
      "Interaction training epoch: 139, train loss: 0.25767, val loss: 0.27788\n",
      "Interaction training epoch: 140, train loss: 0.25399, val loss: 0.26840\n",
      "Interaction training epoch: 141, train loss: 0.25479, val loss: 0.27567\n",
      "Interaction training epoch: 142, train loss: 0.25669, val loss: 0.27183\n",
      "Interaction training epoch: 143, train loss: 0.25643, val loss: 0.27631\n",
      "Interaction training epoch: 144, train loss: 0.25643, val loss: 0.27452\n",
      "Interaction training epoch: 145, train loss: 0.25194, val loss: 0.26987\n",
      "Interaction training epoch: 146, train loss: 0.25203, val loss: 0.26816\n",
      "Interaction training epoch: 147, train loss: 0.25789, val loss: 0.28009\n",
      "Interaction training epoch: 148, train loss: 0.24926, val loss: 0.26511\n",
      "Interaction training epoch: 149, train loss: 0.26132, val loss: 0.28207\n",
      "Interaction training epoch: 150, train loss: 0.25361, val loss: 0.26338\n",
      "Interaction training epoch: 151, train loss: 0.25538, val loss: 0.28399\n",
      "Interaction training epoch: 152, train loss: 0.25090, val loss: 0.26542\n",
      "Interaction training epoch: 153, train loss: 0.25368, val loss: 0.27515\n",
      "Interaction training epoch: 154, train loss: 0.25047, val loss: 0.26686\n",
      "Interaction training epoch: 155, train loss: 0.25653, val loss: 0.28293\n",
      "Interaction training epoch: 156, train loss: 0.25402, val loss: 0.27164\n",
      "Interaction training epoch: 157, train loss: 0.25611, val loss: 0.28058\n",
      "Interaction training epoch: 158, train loss: 0.24970, val loss: 0.26769\n",
      "Interaction training epoch: 159, train loss: 0.25179, val loss: 0.27844\n",
      "Interaction training epoch: 160, train loss: 0.24844, val loss: 0.26896\n",
      "Interaction training epoch: 161, train loss: 0.25386, val loss: 0.28021\n",
      "Interaction training epoch: 162, train loss: 0.25461, val loss: 0.27917\n",
      "Interaction training epoch: 163, train loss: 0.25045, val loss: 0.26787\n",
      "Interaction training epoch: 164, train loss: 0.25301, val loss: 0.27750\n",
      "Interaction training epoch: 165, train loss: 0.25041, val loss: 0.27371\n",
      "Interaction training epoch: 166, train loss: 0.25179, val loss: 0.26864\n",
      "Interaction training epoch: 167, train loss: 0.24699, val loss: 0.27065\n",
      "Interaction training epoch: 168, train loss: 0.24992, val loss: 0.27776\n",
      "Interaction training epoch: 169, train loss: 0.24868, val loss: 0.26811\n",
      "Interaction training epoch: 170, train loss: 0.25128, val loss: 0.27766\n",
      "Interaction training epoch: 171, train loss: 0.24932, val loss: 0.26869\n",
      "Interaction training epoch: 172, train loss: 0.25225, val loss: 0.28328\n",
      "Interaction training epoch: 173, train loss: 0.25044, val loss: 0.27160\n",
      "Interaction training epoch: 174, train loss: 0.24928, val loss: 0.27029\n",
      "Interaction training epoch: 175, train loss: 0.24820, val loss: 0.27102\n",
      "Interaction training epoch: 176, train loss: 0.24983, val loss: 0.27985\n",
      "Interaction training epoch: 177, train loss: 0.24814, val loss: 0.27142\n",
      "Interaction training epoch: 178, train loss: 0.24602, val loss: 0.27195\n",
      "Interaction training epoch: 179, train loss: 0.24975, val loss: 0.28013\n",
      "Interaction training epoch: 180, train loss: 0.24651, val loss: 0.26806\n",
      "Interaction training epoch: 181, train loss: 0.24645, val loss: 0.27461\n",
      "Interaction training epoch: 182, train loss: 0.25189, val loss: 0.28435\n",
      "Interaction training epoch: 183, train loss: 0.24671, val loss: 0.26585\n",
      "Interaction training epoch: 184, train loss: 0.24717, val loss: 0.27734\n",
      "Interaction training epoch: 185, train loss: 0.24723, val loss: 0.27324\n",
      "Interaction training epoch: 186, train loss: 0.24921, val loss: 0.27858\n",
      "Interaction training epoch: 187, train loss: 0.24698, val loss: 0.26822\n",
      "Interaction training epoch: 188, train loss: 0.24834, val loss: 0.27879\n",
      "Interaction training epoch: 189, train loss: 0.24703, val loss: 0.27318\n",
      "Interaction training epoch: 190, train loss: 0.24851, val loss: 0.28380\n",
      "Interaction training epoch: 191, train loss: 0.24518, val loss: 0.26811\n",
      "Interaction training epoch: 192, train loss: 0.24806, val loss: 0.27296\n",
      "Interaction training epoch: 193, train loss: 0.24689, val loss: 0.27659\n",
      "Interaction training epoch: 194, train loss: 0.24588, val loss: 0.27246\n",
      "Interaction training epoch: 195, train loss: 0.24364, val loss: 0.26735\n",
      "Interaction training epoch: 196, train loss: 0.24890, val loss: 0.28143\n",
      "Interaction training epoch: 197, train loss: 0.24637, val loss: 0.27358\n",
      "Interaction training epoch: 198, train loss: 0.24528, val loss: 0.27660\n",
      "Interaction training epoch: 199, train loss: 0.24589, val loss: 0.26788\n",
      "Interaction training epoch: 200, train loss: 0.24502, val loss: 0.27968\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########4 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.25871, val loss: 0.27171\n",
      "Interaction tuning epoch: 2, train loss: 0.25987, val loss: 0.28083\n",
      "Interaction tuning epoch: 3, train loss: 0.25946, val loss: 0.27149\n",
      "Interaction tuning epoch: 4, train loss: 0.25832, val loss: 0.27206\n",
      "Interaction tuning epoch: 5, train loss: 0.26047, val loss: 0.27476\n",
      "Interaction tuning epoch: 6, train loss: 0.26225, val loss: 0.28133\n",
      "Interaction tuning epoch: 7, train loss: 0.25460, val loss: 0.26433\n",
      "Interaction tuning epoch: 8, train loss: 0.25769, val loss: 0.27376\n",
      "Interaction tuning epoch: 9, train loss: 0.25459, val loss: 0.26937\n",
      "Interaction tuning epoch: 10, train loss: 0.25520, val loss: 0.26840\n",
      "Interaction tuning epoch: 11, train loss: 0.25633, val loss: 0.27080\n",
      "Interaction tuning epoch: 12, train loss: 0.25588, val loss: 0.26692\n",
      "Interaction tuning epoch: 13, train loss: 0.25851, val loss: 0.27834\n",
      "Interaction tuning epoch: 14, train loss: 0.25506, val loss: 0.26632\n",
      "Interaction tuning epoch: 15, train loss: 0.25569, val loss: 0.27205\n",
      "Interaction tuning epoch: 16, train loss: 0.25574, val loss: 0.26841\n",
      "Interaction tuning epoch: 17, train loss: 0.25615, val loss: 0.27502\n",
      "Interaction tuning epoch: 18, train loss: 0.25708, val loss: 0.27166\n",
      "Interaction tuning epoch: 19, train loss: 0.25487, val loss: 0.27132\n",
      "Interaction tuning epoch: 20, train loss: 0.25497, val loss: 0.27085\n",
      "Interaction tuning epoch: 21, train loss: 0.25473, val loss: 0.26539\n",
      "Interaction tuning epoch: 22, train loss: 0.25999, val loss: 0.28066\n",
      "Interaction tuning epoch: 23, train loss: 0.25420, val loss: 0.26538\n",
      "Interaction tuning epoch: 24, train loss: 0.25215, val loss: 0.26708\n",
      "Interaction tuning epoch: 25, train loss: 0.25347, val loss: 0.26575\n",
      "Interaction tuning epoch: 26, train loss: 0.25490, val loss: 0.27369\n",
      "Interaction tuning epoch: 27, train loss: 0.25549, val loss: 0.26698\n",
      "Interaction tuning epoch: 28, train loss: 0.25574, val loss: 0.27138\n",
      "Interaction tuning epoch: 29, train loss: 0.25525, val loss: 0.27088\n",
      "Interaction tuning epoch: 30, train loss: 0.25384, val loss: 0.27230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 31, train loss: 0.25233, val loss: 0.26693\n",
      "Interaction tuning epoch: 32, train loss: 0.25670, val loss: 0.27643\n",
      "Interaction tuning epoch: 33, train loss: 0.25550, val loss: 0.27381\n",
      "Interaction tuning epoch: 34, train loss: 0.25239, val loss: 0.26702\n",
      "Interaction tuning epoch: 35, train loss: 0.25240, val loss: 0.27160\n",
      "Interaction tuning epoch: 36, train loss: 0.25304, val loss: 0.26467\n",
      "Interaction tuning epoch: 37, train loss: 0.25491, val loss: 0.27008\n",
      "Interaction tuning epoch: 38, train loss: 0.25415, val loss: 0.26861\n",
      "Interaction tuning epoch: 39, train loss: 0.25377, val loss: 0.27316\n",
      "Interaction tuning epoch: 40, train loss: 0.25243, val loss: 0.26512\n",
      "Interaction tuning epoch: 41, train loss: 0.25298, val loss: 0.27240\n",
      "Interaction tuning epoch: 42, train loss: 0.25377, val loss: 0.27079\n",
      "Interaction tuning epoch: 43, train loss: 0.25349, val loss: 0.26959\n",
      "Interaction tuning epoch: 44, train loss: 0.25692, val loss: 0.27535\n",
      "Interaction tuning epoch: 45, train loss: 0.25293, val loss: 0.26307\n",
      "Interaction tuning epoch: 46, train loss: 0.25487, val loss: 0.27535\n",
      "Interaction tuning epoch: 47, train loss: 0.25203, val loss: 0.26762\n",
      "Interaction tuning epoch: 48, train loss: 0.25213, val loss: 0.27013\n",
      "Interaction tuning epoch: 49, train loss: 0.25380, val loss: 0.27157\n",
      "Interaction tuning epoch: 50, train loss: 0.25172, val loss: 0.27033\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 65.9844982624054\n",
      "After the gam stage, training error is 0.25172 , validation error is 0.27033\n",
      "missing value counts: 99230\n",
      "#####start auto_tuning#####\n",
      "the best shrinkage is 0.687500\n",
      "[SoftImpute] Max Singular Value of X_init = 3.815318\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.231759 validation BCE=0.267535,rank=3\n",
      "[SoftImpute] Iter 1: observed BCE=0.230476 validation BCE=0.267662,rank=3\n",
      "[SoftImpute] Iter 2: observed BCE=0.229320 validation BCE=0.267625,rank=3\n",
      "[SoftImpute] Iter 3: observed BCE=0.228313 validation BCE=0.267417,rank=3\n",
      "[SoftImpute] Iter 4: observed BCE=0.227408 validation BCE=0.266829,rank=3\n",
      "[SoftImpute] Iter 5: observed BCE=0.227012 validation BCE=0.265874,rank=3\n",
      "[SoftImpute] Iter 6: observed BCE=0.226568 validation BCE=0.265884,rank=3\n",
      "[SoftImpute] Iter 7: observed BCE=0.227077 validation BCE=0.265305,rank=3\n",
      "[SoftImpute] Iter 8: observed BCE=0.227529 validation BCE=0.264498,rank=3\n",
      "[SoftImpute] Iter 9: observed BCE=0.227325 validation BCE=0.264544,rank=3\n",
      "[SoftImpute] Iter 10: observed BCE=0.227344 validation BCE=0.264444,rank=3\n",
      "[SoftImpute] Iter 11: observed BCE=0.227044 validation BCE=0.264422,rank=3\n",
      "[SoftImpute] Iter 12: observed BCE=0.226963 validation BCE=0.263668,rank=3\n",
      "[SoftImpute] Iter 13: observed BCE=0.226885 validation BCE=0.263962,rank=3\n",
      "[SoftImpute] Iter 14: observed BCE=0.226984 validation BCE=0.263947,rank=3\n",
      "[SoftImpute] Iter 15: observed BCE=0.226865 validation BCE=0.263957,rank=3\n",
      "[SoftImpute] Iter 16: observed BCE=0.226554 validation BCE=0.263558,rank=3\n",
      "[SoftImpute] Iter 17: observed BCE=0.226335 validation BCE=0.263981,rank=3\n",
      "[SoftImpute] Iter 18: observed BCE=0.226184 validation BCE=0.263765,rank=3\n",
      "[SoftImpute] Iter 19: observed BCE=0.226554 validation BCE=0.263531,rank=3\n",
      "[SoftImpute] Iter 20: observed BCE=0.226305 validation BCE=0.263570,rank=3\n",
      "[SoftImpute] Iter 21: observed BCE=0.226345 validation BCE=0.263423,rank=3\n",
      "[SoftImpute] Iter 22: observed BCE=0.226329 validation BCE=0.263745,rank=3\n",
      "[SoftImpute] Iter 23: observed BCE=0.226331 validation BCE=0.263137,rank=3\n",
      "[SoftImpute] Iter 24: observed BCE=0.226749 validation BCE=0.264063,rank=3\n",
      "[SoftImpute] Iter 25: observed BCE=0.226639 validation BCE=0.263170,rank=3\n",
      "[SoftImpute] Iter 26: observed BCE=0.226841 validation BCE=0.263791,rank=3\n",
      "[SoftImpute] Iter 27: observed BCE=0.226663 validation BCE=0.263569,rank=3\n",
      "[SoftImpute] Iter 28: observed BCE=0.226849 validation BCE=0.263790,rank=3\n",
      "[SoftImpute] Iter 29: observed BCE=0.226597 validation BCE=0.263457,rank=3\n",
      "[SoftImpute] Iter 30: observed BCE=0.226937 validation BCE=0.264001,rank=3\n",
      "[SoftImpute] Iter 31: observed BCE=0.226721 validation BCE=0.263627,rank=3\n",
      "[SoftImpute] Iter 32: observed BCE=0.226752 validation BCE=0.263650,rank=3\n",
      "[SoftImpute] Iter 33: observed BCE=0.226458 validation BCE=0.263653,rank=3\n",
      "[SoftImpute] Iter 34: observed BCE=0.226737 validation BCE=0.263891,rank=3\n",
      "[SoftImpute] Iter 35: observed BCE=0.226627 validation BCE=0.263476,rank=3\n",
      "[SoftImpute] Iter 36: observed BCE=0.226573 validation BCE=0.263833,rank=3\n",
      "[SoftImpute] Iter 37: observed BCE=0.226557 validation BCE=0.263528,rank=3\n",
      "[SoftImpute] Iter 38: observed BCE=0.226531 validation BCE=0.263760,rank=3\n",
      "[SoftImpute] Iter 39: observed BCE=0.226346 validation BCE=0.263625,rank=3\n",
      "[SoftImpute] Iter 40: observed BCE=0.226291 validation BCE=0.263848,rank=3\n",
      "[SoftImpute] Iter 41: observed BCE=0.226326 validation BCE=0.263232,rank=3\n",
      "[SoftImpute] Iter 42: observed BCE=0.226439 validation BCE=0.264057,rank=3\n",
      "[SoftImpute] Iter 43: observed BCE=0.226586 validation BCE=0.263121,rank=3\n",
      "[SoftImpute] Iter 44: observed BCE=0.226295 validation BCE=0.263346,rank=3\n",
      "[SoftImpute] Iter 45: observed BCE=0.226308 validation BCE=0.263288,rank=3\n",
      "[SoftImpute] Iter 46: observed BCE=0.226551 validation BCE=0.263512,rank=3\n",
      "[SoftImpute] Iter 47: observed BCE=0.226469 validation BCE=0.263133,rank=3\n",
      "[SoftImpute] Iter 48: observed BCE=0.226339 validation BCE=0.263482,rank=3\n",
      "[SoftImpute] Iter 49: observed BCE=0.226301 validation BCE=0.263159,rank=3\n",
      "[SoftImpute] Iter 50: observed BCE=0.226345 validation BCE=0.263339,rank=3\n",
      "[SoftImpute] Iter 51: observed BCE=0.226429 validation BCE=0.263278,rank=3\n",
      "[SoftImpute] Iter 52: observed BCE=0.226540 validation BCE=0.263303,rank=3\n",
      "[SoftImpute] Iter 53: observed BCE=0.226618 validation BCE=0.263311,rank=3\n",
      "[SoftImpute] Iter 54: observed BCE=0.226429 validation BCE=0.263611,rank=3\n",
      "[SoftImpute] Iter 55: observed BCE=0.226421 validation BCE=0.263431,rank=3\n",
      "[SoftImpute] Iter 56: observed BCE=0.226374 validation BCE=0.263423,rank=3\n",
      "[SoftImpute] Iter 57: observed BCE=0.226277 validation BCE=0.263607,rank=3\n",
      "[SoftImpute] Iter 58: observed BCE=0.226471 validation BCE=0.263631,rank=3\n",
      "[SoftImpute] Iter 59: observed BCE=0.226583 validation BCE=0.263121,rank=3\n",
      "[SoftImpute] Iter 60: observed BCE=0.226457 validation BCE=0.263271,rank=3\n",
      "[SoftImpute] Iter 61: observed BCE=0.226537 validation BCE=0.263374,rank=3\n",
      "[SoftImpute] Iter 62: observed BCE=0.226521 validation BCE=0.263427,rank=3\n",
      "[SoftImpute] Iter 63: observed BCE=0.226463 validation BCE=0.263347,rank=3\n",
      "[SoftImpute] Iter 64: observed BCE=0.226394 validation BCE=0.263341,rank=3\n",
      "[SoftImpute] Iter 65: observed BCE=0.226488 validation BCE=0.263400,rank=3\n",
      "[SoftImpute] Iter 66: observed BCE=0.226186 validation BCE=0.263542,rank=3\n",
      "[SoftImpute] Iter 67: observed BCE=0.226385 validation BCE=0.263408,rank=3\n",
      "[SoftImpute] Iter 68: observed BCE=0.226401 validation BCE=0.263280,rank=3\n",
      "[SoftImpute] Iter 69: observed BCE=0.226358 validation BCE=0.263262,rank=3\n",
      "[SoftImpute] Iter 70: observed BCE=0.226401 validation BCE=0.263398,rank=3\n",
      "[SoftImpute] Iter 71: observed BCE=0.226382 validation BCE=0.263418,rank=3\n",
      "[SoftImpute] Iter 72: observed BCE=0.226448 validation BCE=0.263399,rank=3\n",
      "[SoftImpute] Iter 73: observed BCE=0.226257 validation BCE=0.263093,rank=3\n",
      "[SoftImpute] Iter 74: observed BCE=0.226352 validation BCE=0.263209,rank=3\n",
      "[SoftImpute] Iter 75: observed BCE=0.226398 validation BCE=0.263261,rank=3\n",
      "[SoftImpute] Iter 76: observed BCE=0.226636 validation BCE=0.263453,rank=3\n",
      "[SoftImpute] Iter 77: observed BCE=0.226372 validation BCE=0.263168,rank=3\n",
      "[SoftImpute] Iter 78: observed BCE=0.226285 validation BCE=0.263230,rank=3\n",
      "[SoftImpute] Iter 79: observed BCE=0.226278 validation BCE=0.263219,rank=3\n",
      "[SoftImpute] Iter 80: observed BCE=0.226465 validation BCE=0.263229,rank=3\n",
      "[SoftImpute] Iter 81: observed BCE=0.226377 validation BCE=0.263493,rank=3\n",
      "[SoftImpute] Iter 82: observed BCE=0.226439 validation BCE=0.263587,rank=3\n",
      "[SoftImpute] Iter 83: observed BCE=0.226051 validation BCE=0.262837,rank=3\n",
      "[SoftImpute] Iter 84: observed BCE=0.226269 validation BCE=0.263087,rank=3\n",
      "[SoftImpute] Iter 85: observed BCE=0.226301 validation BCE=0.263322,rank=3\n",
      "[SoftImpute] Iter 86: observed BCE=0.226470 validation BCE=0.263313,rank=3\n",
      "[SoftImpute] Iter 87: observed BCE=0.226239 validation BCE=0.263258,rank=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 88: observed BCE=0.226377 validation BCE=0.263121,rank=3\n",
      "[SoftImpute] Iter 89: observed BCE=0.226126 validation BCE=0.263323,rank=3\n",
      "[SoftImpute] Iter 90: observed BCE=0.226288 validation BCE=0.263140,rank=3\n",
      "[SoftImpute] Iter 91: observed BCE=0.226191 validation BCE=0.263078,rank=3\n",
      "[SoftImpute] Iter 92: observed BCE=0.226327 validation BCE=0.263367,rank=3\n",
      "[SoftImpute] Iter 93: observed BCE=0.226283 validation BCE=0.263031,rank=3\n",
      "[SoftImpute] Iter 94: observed BCE=0.226481 validation BCE=0.263221,rank=3\n",
      "[SoftImpute] Iter 95: observed BCE=0.226369 validation BCE=0.263202,rank=3\n",
      "[SoftImpute] Iter 96: observed BCE=0.226332 validation BCE=0.263034,rank=3\n",
      "[SoftImpute] Iter 97: observed BCE=0.226422 validation BCE=0.262884,rank=3\n",
      "[SoftImpute] Iter 98: observed BCE=0.226552 validation BCE=0.263322,rank=3\n",
      "[SoftImpute] Iter 99: observed BCE=0.226545 validation BCE=0.263273,rank=3\n",
      "[SoftImpute] Iter 100: observed BCE=0.226713 validation BCE=0.263370,rank=3\n",
      "[SoftImpute] Stopped after iteration 100 for lambda=0.076306\n",
      "final num of user group: 8\n",
      "final num of item group: 11\n",
      "change mode state : True\n",
      "time cost: 30.948795795440674\n",
      "After the matrix factor stage, training error is 0.22671, validation error is 0.26337\n",
      "4\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68123, val loss: 0.67457\n",
      "Main effects training epoch: 2, train loss: 0.67744, val loss: 0.66792\n",
      "Main effects training epoch: 3, train loss: 0.67173, val loss: 0.66418\n",
      "Main effects training epoch: 4, train loss: 0.66365, val loss: 0.65607\n",
      "Main effects training epoch: 5, train loss: 0.64673, val loss: 0.63849\n",
      "Main effects training epoch: 6, train loss: 0.61354, val loss: 0.60500\n",
      "Main effects training epoch: 7, train loss: 0.56854, val loss: 0.55889\n",
      "Main effects training epoch: 8, train loss: 0.53759, val loss: 0.52700\n",
      "Main effects training epoch: 9, train loss: 0.53776, val loss: 0.52520\n",
      "Main effects training epoch: 10, train loss: 0.53512, val loss: 0.52209\n",
      "Main effects training epoch: 11, train loss: 0.53179, val loss: 0.52047\n",
      "Main effects training epoch: 12, train loss: 0.53184, val loss: 0.52026\n",
      "Main effects training epoch: 13, train loss: 0.53041, val loss: 0.51974\n",
      "Main effects training epoch: 14, train loss: 0.52936, val loss: 0.51920\n",
      "Main effects training epoch: 15, train loss: 0.52968, val loss: 0.51921\n",
      "Main effects training epoch: 16, train loss: 0.52908, val loss: 0.51779\n",
      "Main effects training epoch: 17, train loss: 0.52892, val loss: 0.51778\n",
      "Main effects training epoch: 18, train loss: 0.52854, val loss: 0.51846\n",
      "Main effects training epoch: 19, train loss: 0.52858, val loss: 0.51829\n",
      "Main effects training epoch: 20, train loss: 0.52895, val loss: 0.51855\n",
      "Main effects training epoch: 21, train loss: 0.52851, val loss: 0.51869\n",
      "Main effects training epoch: 22, train loss: 0.52893, val loss: 0.51905\n",
      "Main effects training epoch: 23, train loss: 0.52919, val loss: 0.51935\n",
      "Main effects training epoch: 24, train loss: 0.52875, val loss: 0.51914\n",
      "Main effects training epoch: 25, train loss: 0.52873, val loss: 0.51900\n",
      "Main effects training epoch: 26, train loss: 0.52829, val loss: 0.51847\n",
      "Main effects training epoch: 27, train loss: 0.52860, val loss: 0.51924\n",
      "Main effects training epoch: 28, train loss: 0.52841, val loss: 0.51839\n",
      "Main effects training epoch: 29, train loss: 0.52819, val loss: 0.51862\n",
      "Main effects training epoch: 30, train loss: 0.52812, val loss: 0.51897\n",
      "Main effects training epoch: 31, train loss: 0.52794, val loss: 0.51884\n",
      "Main effects training epoch: 32, train loss: 0.52802, val loss: 0.51908\n",
      "Main effects training epoch: 33, train loss: 0.52783, val loss: 0.51835\n",
      "Main effects training epoch: 34, train loss: 0.52840, val loss: 0.51877\n",
      "Main effects training epoch: 35, train loss: 0.52821, val loss: 0.51913\n",
      "Main effects training epoch: 36, train loss: 0.52817, val loss: 0.51908\n",
      "Main effects training epoch: 37, train loss: 0.52786, val loss: 0.51847\n",
      "Main effects training epoch: 38, train loss: 0.52824, val loss: 0.51924\n",
      "Main effects training epoch: 39, train loss: 0.53025, val loss: 0.52124\n",
      "Main effects training epoch: 40, train loss: 0.52952, val loss: 0.51959\n",
      "Main effects training epoch: 41, train loss: 0.52905, val loss: 0.52004\n",
      "Main effects training epoch: 42, train loss: 0.52870, val loss: 0.51916\n",
      "Main effects training epoch: 43, train loss: 0.52816, val loss: 0.51899\n",
      "Main effects training epoch: 44, train loss: 0.52900, val loss: 0.51958\n",
      "Main effects training epoch: 45, train loss: 0.52834, val loss: 0.51944\n",
      "Main effects training epoch: 46, train loss: 0.52823, val loss: 0.51896\n",
      "Main effects training epoch: 47, train loss: 0.52810, val loss: 0.51890\n",
      "Main effects training epoch: 48, train loss: 0.52729, val loss: 0.51864\n",
      "Main effects training epoch: 49, train loss: 0.52748, val loss: 0.51839\n",
      "Main effects training epoch: 50, train loss: 0.52765, val loss: 0.51922\n",
      "Main effects training epoch: 51, train loss: 0.52749, val loss: 0.51861\n",
      "Main effects training epoch: 52, train loss: 0.52775, val loss: 0.51888\n",
      "Main effects training epoch: 53, train loss: 0.52817, val loss: 0.51915\n",
      "Main effects training epoch: 54, train loss: 0.52850, val loss: 0.51993\n",
      "Main effects training epoch: 55, train loss: 0.52767, val loss: 0.51883\n",
      "Main effects training epoch: 56, train loss: 0.52732, val loss: 0.51891\n",
      "Main effects training epoch: 57, train loss: 0.52728, val loss: 0.51881\n",
      "Main effects training epoch: 58, train loss: 0.52726, val loss: 0.51805\n",
      "Main effects training epoch: 59, train loss: 0.52702, val loss: 0.51796\n",
      "Main effects training epoch: 60, train loss: 0.52742, val loss: 0.51899\n",
      "Main effects training epoch: 61, train loss: 0.52703, val loss: 0.51882\n",
      "Main effects training epoch: 62, train loss: 0.52693, val loss: 0.51876\n",
      "Main effects training epoch: 63, train loss: 0.52695, val loss: 0.51860\n",
      "Main effects training epoch: 64, train loss: 0.52701, val loss: 0.51827\n",
      "Main effects training epoch: 65, train loss: 0.52691, val loss: 0.51860\n",
      "Main effects training epoch: 66, train loss: 0.52695, val loss: 0.51897\n",
      "Main effects training epoch: 67, train loss: 0.52705, val loss: 0.51890\n",
      "Main effects training epoch: 68, train loss: 0.52708, val loss: 0.51909\n",
      "Main effects training epoch: 69, train loss: 0.52684, val loss: 0.51781\n",
      "Main effects training epoch: 70, train loss: 0.52794, val loss: 0.51914\n",
      "Main effects training epoch: 71, train loss: 0.52741, val loss: 0.51930\n",
      "Main effects training epoch: 72, train loss: 0.52722, val loss: 0.51896\n",
      "Main effects training epoch: 73, train loss: 0.52745, val loss: 0.51959\n",
      "Main effects training epoch: 74, train loss: 0.52712, val loss: 0.51956\n",
      "Main effects training epoch: 75, train loss: 0.52724, val loss: 0.51924\n",
      "Main effects training epoch: 76, train loss: 0.52690, val loss: 0.51853\n",
      "Main effects training epoch: 77, train loss: 0.52664, val loss: 0.51849\n",
      "Main effects training epoch: 78, train loss: 0.52690, val loss: 0.51847\n",
      "Main effects training epoch: 79, train loss: 0.52677, val loss: 0.51896\n",
      "Main effects training epoch: 80, train loss: 0.52660, val loss: 0.51899\n",
      "Main effects training epoch: 81, train loss: 0.52654, val loss: 0.51837\n",
      "Main effects training epoch: 82, train loss: 0.52793, val loss: 0.51994\n",
      "Main effects training epoch: 83, train loss: 0.52719, val loss: 0.51935\n",
      "Main effects training epoch: 84, train loss: 0.52789, val loss: 0.51984\n",
      "Main effects training epoch: 85, train loss: 0.52764, val loss: 0.52036\n",
      "Main effects training epoch: 86, train loss: 0.52694, val loss: 0.51866\n",
      "Main effects training epoch: 87, train loss: 0.52639, val loss: 0.51880\n",
      "Main effects training epoch: 88, train loss: 0.52645, val loss: 0.51831\n",
      "Main effects training epoch: 89, train loss: 0.52645, val loss: 0.51792\n",
      "Main effects training epoch: 90, train loss: 0.52644, val loss: 0.51912\n",
      "Main effects training epoch: 91, train loss: 0.52639, val loss: 0.51857\n",
      "Main effects training epoch: 92, train loss: 0.52645, val loss: 0.51925\n",
      "Main effects training epoch: 93, train loss: 0.52627, val loss: 0.51901\n",
      "Main effects training epoch: 94, train loss: 0.52659, val loss: 0.51829\n",
      "Main effects training epoch: 95, train loss: 0.52644, val loss: 0.51806\n",
      "Main effects training epoch: 96, train loss: 0.52683, val loss: 0.51801\n",
      "Main effects training epoch: 97, train loss: 0.52643, val loss: 0.51858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 98, train loss: 0.52615, val loss: 0.51848\n",
      "Main effects training epoch: 99, train loss: 0.52599, val loss: 0.51843\n",
      "Main effects training epoch: 100, train loss: 0.52617, val loss: 0.51874\n",
      "Main effects training epoch: 101, train loss: 0.52655, val loss: 0.51897\n",
      "Main effects training epoch: 102, train loss: 0.52625, val loss: 0.51924\n",
      "Main effects training epoch: 103, train loss: 0.52661, val loss: 0.51885\n",
      "Main effects training epoch: 104, train loss: 0.52609, val loss: 0.51906\n",
      "Main effects training epoch: 105, train loss: 0.52584, val loss: 0.51867\n",
      "Main effects training epoch: 106, train loss: 0.52599, val loss: 0.51843\n",
      "Main effects training epoch: 107, train loss: 0.52587, val loss: 0.51835\n",
      "Main effects training epoch: 108, train loss: 0.52605, val loss: 0.51921\n",
      "Main effects training epoch: 109, train loss: 0.52581, val loss: 0.51911\n",
      "Main effects training epoch: 110, train loss: 0.52625, val loss: 0.51957\n",
      "Main effects training epoch: 111, train loss: 0.52566, val loss: 0.51848\n",
      "Main effects training epoch: 112, train loss: 0.52575, val loss: 0.51810\n",
      "Main effects training epoch: 113, train loss: 0.52565, val loss: 0.51826\n",
      "Main effects training epoch: 114, train loss: 0.52575, val loss: 0.51908\n",
      "Main effects training epoch: 115, train loss: 0.52556, val loss: 0.51823\n",
      "Main effects training epoch: 116, train loss: 0.52628, val loss: 0.51988\n",
      "Main effects training epoch: 117, train loss: 0.52579, val loss: 0.51841\n",
      "Main effects training epoch: 118, train loss: 0.52548, val loss: 0.51863\n",
      "Early stop at epoch 118, with validation loss: 0.51863\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.52794, val loss: 0.52080\n",
      "Main effects tuning epoch: 2, train loss: 0.52777, val loss: 0.52005\n",
      "Main effects tuning epoch: 3, train loss: 0.52793, val loss: 0.52071\n",
      "Main effects tuning epoch: 4, train loss: 0.52804, val loss: 0.52104\n",
      "Main effects tuning epoch: 5, train loss: 0.52754, val loss: 0.52023\n",
      "Main effects tuning epoch: 6, train loss: 0.52777, val loss: 0.52012\n",
      "Main effects tuning epoch: 7, train loss: 0.52758, val loss: 0.52066\n",
      "Main effects tuning epoch: 8, train loss: 0.52740, val loss: 0.52007\n",
      "Main effects tuning epoch: 9, train loss: 0.52748, val loss: 0.51990\n",
      "Main effects tuning epoch: 10, train loss: 0.52732, val loss: 0.51988\n",
      "Main effects tuning epoch: 11, train loss: 0.52727, val loss: 0.51984\n",
      "Main effects tuning epoch: 12, train loss: 0.52725, val loss: 0.52000\n",
      "Main effects tuning epoch: 13, train loss: 0.52716, val loss: 0.52018\n",
      "Main effects tuning epoch: 14, train loss: 0.52726, val loss: 0.52001\n",
      "Main effects tuning epoch: 15, train loss: 0.52724, val loss: 0.51996\n",
      "Main effects tuning epoch: 16, train loss: 0.52713, val loss: 0.51998\n",
      "Main effects tuning epoch: 17, train loss: 0.52712, val loss: 0.51984\n",
      "Main effects tuning epoch: 18, train loss: 0.52710, val loss: 0.51976\n",
      "Main effects tuning epoch: 19, train loss: 0.52727, val loss: 0.52025\n",
      "Main effects tuning epoch: 20, train loss: 0.52703, val loss: 0.51989\n",
      "Main effects tuning epoch: 21, train loss: 0.52720, val loss: 0.51980\n",
      "Main effects tuning epoch: 22, train loss: 0.52720, val loss: 0.52038\n",
      "Main effects tuning epoch: 23, train loss: 0.52723, val loss: 0.52025\n",
      "Main effects tuning epoch: 24, train loss: 0.52779, val loss: 0.52090\n",
      "Main effects tuning epoch: 25, train loss: 0.52708, val loss: 0.51943\n",
      "Main effects tuning epoch: 26, train loss: 0.52707, val loss: 0.51962\n",
      "Main effects tuning epoch: 27, train loss: 0.52705, val loss: 0.51945\n",
      "Main effects tuning epoch: 28, train loss: 0.52702, val loss: 0.51924\n",
      "Main effects tuning epoch: 29, train loss: 0.52725, val loss: 0.52022\n",
      "Main effects tuning epoch: 30, train loss: 0.52699, val loss: 0.51993\n",
      "Main effects tuning epoch: 31, train loss: 0.52731, val loss: 0.52044\n",
      "Main effects tuning epoch: 32, train loss: 0.52696, val loss: 0.52022\n",
      "Main effects tuning epoch: 33, train loss: 0.52696, val loss: 0.51974\n",
      "Main effects tuning epoch: 34, train loss: 0.52765, val loss: 0.52000\n",
      "Main effects tuning epoch: 35, train loss: 0.52715, val loss: 0.52064\n",
      "Main effects tuning epoch: 36, train loss: 0.52698, val loss: 0.51970\n",
      "Main effects tuning epoch: 37, train loss: 0.52702, val loss: 0.52006\n",
      "Main effects tuning epoch: 38, train loss: 0.52699, val loss: 0.51966\n",
      "Main effects tuning epoch: 39, train loss: 0.52745, val loss: 0.52038\n",
      "Main effects tuning epoch: 40, train loss: 0.52693, val loss: 0.51920\n",
      "Main effects tuning epoch: 41, train loss: 0.52698, val loss: 0.52000\n",
      "Main effects tuning epoch: 42, train loss: 0.52677, val loss: 0.51970\n",
      "Main effects tuning epoch: 43, train loss: 0.52674, val loss: 0.51988\n",
      "Main effects tuning epoch: 44, train loss: 0.52658, val loss: 0.51963\n",
      "Main effects tuning epoch: 45, train loss: 0.52660, val loss: 0.51948\n",
      "Main effects tuning epoch: 46, train loss: 0.52707, val loss: 0.51984\n",
      "Main effects tuning epoch: 47, train loss: 0.52767, val loss: 0.52054\n",
      "Main effects tuning epoch: 48, train loss: 0.52673, val loss: 0.52013\n",
      "Main effects tuning epoch: 49, train loss: 0.52680, val loss: 0.51906\n",
      "Main effects tuning epoch: 50, train loss: 0.52661, val loss: 0.51931\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.51098, val loss: 0.50375\n",
      "Interaction training epoch: 2, train loss: 0.33282, val loss: 0.33881\n",
      "Interaction training epoch: 3, train loss: 0.33142, val loss: 0.34521\n",
      "Interaction training epoch: 4, train loss: 0.33198, val loss: 0.33838\n",
      "Interaction training epoch: 5, train loss: 0.31515, val loss: 0.32533\n",
      "Interaction training epoch: 6, train loss: 0.30324, val loss: 0.30860\n",
      "Interaction training epoch: 7, train loss: 0.30809, val loss: 0.32432\n",
      "Interaction training epoch: 8, train loss: 0.30198, val loss: 0.31029\n",
      "Interaction training epoch: 9, train loss: 0.29882, val loss: 0.31059\n",
      "Interaction training epoch: 10, train loss: 0.29570, val loss: 0.31016\n",
      "Interaction training epoch: 11, train loss: 0.29304, val loss: 0.30571\n",
      "Interaction training epoch: 12, train loss: 0.29088, val loss: 0.30420\n",
      "Interaction training epoch: 13, train loss: 0.28842, val loss: 0.30250\n",
      "Interaction training epoch: 14, train loss: 0.29329, val loss: 0.31022\n",
      "Interaction training epoch: 15, train loss: 0.28722, val loss: 0.30480\n",
      "Interaction training epoch: 16, train loss: 0.28937, val loss: 0.30461\n",
      "Interaction training epoch: 17, train loss: 0.28881, val loss: 0.30516\n",
      "Interaction training epoch: 18, train loss: 0.28904, val loss: 0.30908\n",
      "Interaction training epoch: 19, train loss: 0.28216, val loss: 0.29734\n",
      "Interaction training epoch: 20, train loss: 0.29444, val loss: 0.30956\n",
      "Interaction training epoch: 21, train loss: 0.28779, val loss: 0.30370\n",
      "Interaction training epoch: 22, train loss: 0.28573, val loss: 0.30295\n",
      "Interaction training epoch: 23, train loss: 0.28868, val loss: 0.30552\n",
      "Interaction training epoch: 24, train loss: 0.28443, val loss: 0.30003\n",
      "Interaction training epoch: 25, train loss: 0.28673, val loss: 0.30335\n",
      "Interaction training epoch: 26, train loss: 0.28534, val loss: 0.30186\n",
      "Interaction training epoch: 27, train loss: 0.28261, val loss: 0.29899\n",
      "Interaction training epoch: 28, train loss: 0.28638, val loss: 0.30257\n",
      "Interaction training epoch: 29, train loss: 0.28285, val loss: 0.30227\n",
      "Interaction training epoch: 30, train loss: 0.28396, val loss: 0.30397\n",
      "Interaction training epoch: 31, train loss: 0.28088, val loss: 0.30192\n",
      "Interaction training epoch: 32, train loss: 0.28271, val loss: 0.29862\n",
      "Interaction training epoch: 33, train loss: 0.28176, val loss: 0.30046\n",
      "Interaction training epoch: 34, train loss: 0.28345, val loss: 0.30416\n",
      "Interaction training epoch: 35, train loss: 0.28383, val loss: 0.30160\n",
      "Interaction training epoch: 36, train loss: 0.28048, val loss: 0.30367\n",
      "Interaction training epoch: 37, train loss: 0.28426, val loss: 0.30122\n",
      "Interaction training epoch: 38, train loss: 0.28054, val loss: 0.30174\n",
      "Interaction training epoch: 39, train loss: 0.28094, val loss: 0.30192\n",
      "Interaction training epoch: 40, train loss: 0.27860, val loss: 0.29759\n",
      "Interaction training epoch: 41, train loss: 0.27986, val loss: 0.30377\n",
      "Interaction training epoch: 42, train loss: 0.27903, val loss: 0.30150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 43, train loss: 0.27850, val loss: 0.30033\n",
      "Interaction training epoch: 44, train loss: 0.28004, val loss: 0.30550\n",
      "Interaction training epoch: 45, train loss: 0.28060, val loss: 0.30455\n",
      "Interaction training epoch: 46, train loss: 0.27921, val loss: 0.30172\n",
      "Interaction training epoch: 47, train loss: 0.27724, val loss: 0.30235\n",
      "Interaction training epoch: 48, train loss: 0.28001, val loss: 0.30625\n",
      "Interaction training epoch: 49, train loss: 0.27771, val loss: 0.29858\n",
      "Interaction training epoch: 50, train loss: 0.27798, val loss: 0.30165\n",
      "Interaction training epoch: 51, train loss: 0.27694, val loss: 0.30323\n",
      "Interaction training epoch: 52, train loss: 0.27505, val loss: 0.29779\n",
      "Interaction training epoch: 53, train loss: 0.27713, val loss: 0.30209\n",
      "Interaction training epoch: 54, train loss: 0.27680, val loss: 0.30180\n",
      "Interaction training epoch: 55, train loss: 0.27385, val loss: 0.29861\n",
      "Interaction training epoch: 56, train loss: 0.27841, val loss: 0.30399\n",
      "Interaction training epoch: 57, train loss: 0.27516, val loss: 0.30230\n",
      "Interaction training epoch: 58, train loss: 0.27830, val loss: 0.30131\n",
      "Interaction training epoch: 59, train loss: 0.28028, val loss: 0.31019\n",
      "Interaction training epoch: 60, train loss: 0.27465, val loss: 0.29948\n",
      "Interaction training epoch: 61, train loss: 0.27634, val loss: 0.29938\n",
      "Interaction training epoch: 62, train loss: 0.27465, val loss: 0.30215\n",
      "Interaction training epoch: 63, train loss: 0.27603, val loss: 0.30147\n",
      "Interaction training epoch: 64, train loss: 0.27705, val loss: 0.30071\n",
      "Interaction training epoch: 65, train loss: 0.27593, val loss: 0.30424\n",
      "Interaction training epoch: 66, train loss: 0.27530, val loss: 0.29967\n",
      "Interaction training epoch: 67, train loss: 0.27350, val loss: 0.30035\n",
      "Interaction training epoch: 68, train loss: 0.27538, val loss: 0.30314\n",
      "Interaction training epoch: 69, train loss: 0.27090, val loss: 0.29443\n",
      "Interaction training epoch: 70, train loss: 0.27149, val loss: 0.29861\n",
      "Interaction training epoch: 71, train loss: 0.27305, val loss: 0.30026\n",
      "Interaction training epoch: 72, train loss: 0.27218, val loss: 0.30145\n",
      "Interaction training epoch: 73, train loss: 0.27285, val loss: 0.29769\n",
      "Interaction training epoch: 74, train loss: 0.27364, val loss: 0.30272\n",
      "Interaction training epoch: 75, train loss: 0.27334, val loss: 0.30280\n",
      "Interaction training epoch: 76, train loss: 0.27280, val loss: 0.29778\n",
      "Interaction training epoch: 77, train loss: 0.27217, val loss: 0.30252\n",
      "Interaction training epoch: 78, train loss: 0.27242, val loss: 0.29853\n",
      "Interaction training epoch: 79, train loss: 0.27184, val loss: 0.29950\n",
      "Interaction training epoch: 80, train loss: 0.27076, val loss: 0.29800\n",
      "Interaction training epoch: 81, train loss: 0.27251, val loss: 0.29857\n",
      "Interaction training epoch: 82, train loss: 0.27021, val loss: 0.29845\n",
      "Interaction training epoch: 83, train loss: 0.27258, val loss: 0.29874\n",
      "Interaction training epoch: 84, train loss: 0.27063, val loss: 0.29868\n",
      "Interaction training epoch: 85, train loss: 0.26939, val loss: 0.29668\n",
      "Interaction training epoch: 86, train loss: 0.26923, val loss: 0.30077\n",
      "Interaction training epoch: 87, train loss: 0.26799, val loss: 0.29762\n",
      "Interaction training epoch: 88, train loss: 0.26963, val loss: 0.29768\n",
      "Interaction training epoch: 89, train loss: 0.26718, val loss: 0.29509\n",
      "Interaction training epoch: 90, train loss: 0.27117, val loss: 0.30011\n",
      "Interaction training epoch: 91, train loss: 0.26724, val loss: 0.29614\n",
      "Interaction training epoch: 92, train loss: 0.27153, val loss: 0.29937\n",
      "Interaction training epoch: 93, train loss: 0.27274, val loss: 0.30057\n",
      "Interaction training epoch: 94, train loss: 0.27205, val loss: 0.30326\n",
      "Interaction training epoch: 95, train loss: 0.26760, val loss: 0.29167\n",
      "Interaction training epoch: 96, train loss: 0.26807, val loss: 0.29751\n",
      "Interaction training epoch: 97, train loss: 0.26662, val loss: 0.30000\n",
      "Interaction training epoch: 98, train loss: 0.26873, val loss: 0.29575\n",
      "Interaction training epoch: 99, train loss: 0.27036, val loss: 0.30289\n",
      "Interaction training epoch: 100, train loss: 0.26634, val loss: 0.29415\n",
      "Interaction training epoch: 101, train loss: 0.26774, val loss: 0.29811\n",
      "Interaction training epoch: 102, train loss: 0.26682, val loss: 0.29449\n",
      "Interaction training epoch: 103, train loss: 0.26755, val loss: 0.29589\n",
      "Interaction training epoch: 104, train loss: 0.26770, val loss: 0.30158\n",
      "Interaction training epoch: 105, train loss: 0.26617, val loss: 0.29108\n",
      "Interaction training epoch: 106, train loss: 0.26615, val loss: 0.29747\n",
      "Interaction training epoch: 107, train loss: 0.26538, val loss: 0.29564\n",
      "Interaction training epoch: 108, train loss: 0.26387, val loss: 0.29356\n",
      "Interaction training epoch: 109, train loss: 0.26662, val loss: 0.29481\n",
      "Interaction training epoch: 110, train loss: 0.26455, val loss: 0.29631\n",
      "Interaction training epoch: 111, train loss: 0.26667, val loss: 0.29779\n",
      "Interaction training epoch: 112, train loss: 0.26555, val loss: 0.29778\n",
      "Interaction training epoch: 113, train loss: 0.26458, val loss: 0.29745\n",
      "Interaction training epoch: 114, train loss: 0.26627, val loss: 0.29603\n",
      "Interaction training epoch: 115, train loss: 0.26527, val loss: 0.29704\n",
      "Interaction training epoch: 116, train loss: 0.26694, val loss: 0.29666\n",
      "Interaction training epoch: 117, train loss: 0.26196, val loss: 0.29194\n",
      "Interaction training epoch: 118, train loss: 0.26650, val loss: 0.29656\n",
      "Interaction training epoch: 119, train loss: 0.26608, val loss: 0.29680\n",
      "Interaction training epoch: 120, train loss: 0.26257, val loss: 0.29711\n",
      "Interaction training epoch: 121, train loss: 0.26323, val loss: 0.29663\n",
      "Interaction training epoch: 122, train loss: 0.26214, val loss: 0.29281\n",
      "Interaction training epoch: 123, train loss: 0.26265, val loss: 0.29402\n",
      "Interaction training epoch: 124, train loss: 0.26003, val loss: 0.29490\n",
      "Interaction training epoch: 125, train loss: 0.26326, val loss: 0.29701\n",
      "Interaction training epoch: 126, train loss: 0.26300, val loss: 0.29345\n",
      "Interaction training epoch: 127, train loss: 0.26156, val loss: 0.29520\n",
      "Interaction training epoch: 128, train loss: 0.26135, val loss: 0.29283\n",
      "Interaction training epoch: 129, train loss: 0.26118, val loss: 0.29490\n",
      "Interaction training epoch: 130, train loss: 0.26074, val loss: 0.29164\n",
      "Interaction training epoch: 131, train loss: 0.26187, val loss: 0.29360\n",
      "Interaction training epoch: 132, train loss: 0.26280, val loss: 0.29647\n",
      "Interaction training epoch: 133, train loss: 0.25956, val loss: 0.29750\n",
      "Interaction training epoch: 134, train loss: 0.26199, val loss: 0.29749\n",
      "Interaction training epoch: 135, train loss: 0.26097, val loss: 0.29338\n",
      "Interaction training epoch: 136, train loss: 0.26154, val loss: 0.29631\n",
      "Interaction training epoch: 137, train loss: 0.25890, val loss: 0.29341\n",
      "Interaction training epoch: 138, train loss: 0.26154, val loss: 0.29529\n",
      "Interaction training epoch: 139, train loss: 0.26084, val loss: 0.29500\n",
      "Interaction training epoch: 140, train loss: 0.25706, val loss: 0.29188\n",
      "Interaction training epoch: 141, train loss: 0.25889, val loss: 0.29390\n",
      "Interaction training epoch: 142, train loss: 0.25705, val loss: 0.29474\n",
      "Interaction training epoch: 143, train loss: 0.25741, val loss: 0.29287\n",
      "Interaction training epoch: 144, train loss: 0.25867, val loss: 0.29679\n",
      "Interaction training epoch: 145, train loss: 0.25882, val loss: 0.29624\n",
      "Interaction training epoch: 146, train loss: 0.25983, val loss: 0.29534\n",
      "Interaction training epoch: 147, train loss: 0.25745, val loss: 0.29607\n",
      "Interaction training epoch: 148, train loss: 0.25947, val loss: 0.29552\n",
      "Interaction training epoch: 149, train loss: 0.25544, val loss: 0.29513\n",
      "Interaction training epoch: 150, train loss: 0.25748, val loss: 0.29314\n",
      "Interaction training epoch: 151, train loss: 0.25909, val loss: 0.29281\n",
      "Interaction training epoch: 152, train loss: 0.25661, val loss: 0.29713\n",
      "Interaction training epoch: 153, train loss: 0.25988, val loss: 0.29633\n",
      "Interaction training epoch: 154, train loss: 0.25148, val loss: 0.29019\n",
      "Interaction training epoch: 155, train loss: 0.26231, val loss: 0.29759\n",
      "Interaction training epoch: 156, train loss: 0.26381, val loss: 0.29958\n",
      "Interaction training epoch: 157, train loss: 0.25268, val loss: 0.28924\n",
      "Interaction training epoch: 158, train loss: 0.25772, val loss: 0.29356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 159, train loss: 0.25944, val loss: 0.29618\n",
      "Interaction training epoch: 160, train loss: 0.25872, val loss: 0.29543\n",
      "Interaction training epoch: 161, train loss: 0.25835, val loss: 0.29367\n",
      "Interaction training epoch: 162, train loss: 0.25469, val loss: 0.29491\n",
      "Interaction training epoch: 163, train loss: 0.25669, val loss: 0.29733\n",
      "Interaction training epoch: 164, train loss: 0.25561, val loss: 0.29398\n",
      "Interaction training epoch: 165, train loss: 0.25096, val loss: 0.28922\n",
      "Interaction training epoch: 166, train loss: 0.25903, val loss: 0.30013\n",
      "Interaction training epoch: 167, train loss: 0.25273, val loss: 0.29385\n",
      "Interaction training epoch: 168, train loss: 0.25607, val loss: 0.28614\n",
      "Interaction training epoch: 169, train loss: 0.25411, val loss: 0.29588\n",
      "Interaction training epoch: 170, train loss: 0.25201, val loss: 0.28768\n",
      "Interaction training epoch: 171, train loss: 0.25378, val loss: 0.29236\n",
      "Interaction training epoch: 172, train loss: 0.25051, val loss: 0.28996\n",
      "Interaction training epoch: 173, train loss: 0.25473, val loss: 0.29102\n",
      "Interaction training epoch: 174, train loss: 0.25210, val loss: 0.28739\n",
      "Interaction training epoch: 175, train loss: 0.24716, val loss: 0.28709\n",
      "Interaction training epoch: 176, train loss: 0.24994, val loss: 0.29143\n",
      "Interaction training epoch: 177, train loss: 0.24859, val loss: 0.28703\n",
      "Interaction training epoch: 178, train loss: 0.24525, val loss: 0.28637\n",
      "Interaction training epoch: 179, train loss: 0.24621, val loss: 0.28748\n",
      "Interaction training epoch: 180, train loss: 0.24616, val loss: 0.28285\n",
      "Interaction training epoch: 181, train loss: 0.24816, val loss: 0.28204\n",
      "Interaction training epoch: 182, train loss: 0.24466, val loss: 0.28900\n",
      "Interaction training epoch: 183, train loss: 0.24945, val loss: 0.29336\n",
      "Interaction training epoch: 184, train loss: 0.24739, val loss: 0.28748\n",
      "Interaction training epoch: 185, train loss: 0.24796, val loss: 0.28604\n",
      "Interaction training epoch: 186, train loss: 0.24455, val loss: 0.28644\n",
      "Interaction training epoch: 187, train loss: 0.24654, val loss: 0.28449\n",
      "Interaction training epoch: 188, train loss: 0.24363, val loss: 0.28598\n",
      "Interaction training epoch: 189, train loss: 0.24233, val loss: 0.28488\n",
      "Interaction training epoch: 190, train loss: 0.25236, val loss: 0.29369\n",
      "Interaction training epoch: 191, train loss: 0.24386, val loss: 0.28764\n",
      "Interaction training epoch: 192, train loss: 0.24374, val loss: 0.28640\n",
      "Interaction training epoch: 193, train loss: 0.24388, val loss: 0.28636\n",
      "Interaction training epoch: 194, train loss: 0.24538, val loss: 0.28825\n",
      "Interaction training epoch: 195, train loss: 0.25023, val loss: 0.28933\n",
      "Interaction training epoch: 196, train loss: 0.23922, val loss: 0.29029\n",
      "Interaction training epoch: 197, train loss: 0.24998, val loss: 0.29550\n",
      "Interaction training epoch: 198, train loss: 0.24442, val loss: 0.28640\n",
      "Interaction training epoch: 199, train loss: 0.23956, val loss: 0.28328\n",
      "Interaction training epoch: 200, train loss: 0.23933, val loss: 0.28613\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########5 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.26116, val loss: 0.28115\n",
      "Interaction tuning epoch: 2, train loss: 0.25533, val loss: 0.28177\n",
      "Interaction tuning epoch: 3, train loss: 0.25503, val loss: 0.27800\n",
      "Interaction tuning epoch: 4, train loss: 0.25306, val loss: 0.27710\n",
      "Interaction tuning epoch: 5, train loss: 0.25475, val loss: 0.27979\n",
      "Interaction tuning epoch: 6, train loss: 0.25544, val loss: 0.28395\n",
      "Interaction tuning epoch: 7, train loss: 0.26621, val loss: 0.29345\n",
      "Interaction tuning epoch: 8, train loss: 0.25628, val loss: 0.27986\n",
      "Interaction tuning epoch: 9, train loss: 0.25608, val loss: 0.28182\n",
      "Interaction tuning epoch: 10, train loss: 0.24949, val loss: 0.27633\n",
      "Interaction tuning epoch: 11, train loss: 0.25397, val loss: 0.28243\n",
      "Interaction tuning epoch: 12, train loss: 0.27057, val loss: 0.29867\n",
      "Interaction tuning epoch: 13, train loss: 0.25792, val loss: 0.27906\n",
      "Interaction tuning epoch: 14, train loss: 0.25399, val loss: 0.28060\n",
      "Interaction tuning epoch: 15, train loss: 0.25249, val loss: 0.28169\n",
      "Interaction tuning epoch: 16, train loss: 0.26041, val loss: 0.29060\n",
      "Interaction tuning epoch: 17, train loss: 0.25416, val loss: 0.28139\n",
      "Interaction tuning epoch: 18, train loss: 0.25929, val loss: 0.28578\n",
      "Interaction tuning epoch: 19, train loss: 0.25110, val loss: 0.28087\n",
      "Interaction tuning epoch: 20, train loss: 0.25360, val loss: 0.28246\n",
      "Interaction tuning epoch: 21, train loss: 0.25119, val loss: 0.28104\n",
      "Interaction tuning epoch: 22, train loss: 0.24774, val loss: 0.27627\n",
      "Interaction tuning epoch: 23, train loss: 0.25120, val loss: 0.28133\n",
      "Interaction tuning epoch: 24, train loss: 0.25188, val loss: 0.28522\n",
      "Interaction tuning epoch: 25, train loss: 0.25352, val loss: 0.28666\n",
      "Interaction tuning epoch: 26, train loss: 0.25117, val loss: 0.27873\n",
      "Interaction tuning epoch: 27, train loss: 0.25421, val loss: 0.28759\n",
      "Interaction tuning epoch: 28, train loss: 0.24916, val loss: 0.27815\n",
      "Interaction tuning epoch: 29, train loss: 0.25214, val loss: 0.28025\n",
      "Interaction tuning epoch: 30, train loss: 0.25107, val loss: 0.27800\n",
      "Interaction tuning epoch: 31, train loss: 0.24901, val loss: 0.28190\n",
      "Interaction tuning epoch: 32, train loss: 0.25065, val loss: 0.28196\n",
      "Interaction tuning epoch: 33, train loss: 0.24638, val loss: 0.27807\n",
      "Interaction tuning epoch: 34, train loss: 0.24997, val loss: 0.28109\n",
      "Interaction tuning epoch: 35, train loss: 0.24845, val loss: 0.28109\n",
      "Interaction tuning epoch: 36, train loss: 0.25221, val loss: 0.28574\n",
      "Interaction tuning epoch: 37, train loss: 0.25041, val loss: 0.28374\n",
      "Interaction tuning epoch: 38, train loss: 0.25125, val loss: 0.28151\n",
      "Interaction tuning epoch: 39, train loss: 0.24438, val loss: 0.27515\n",
      "Interaction tuning epoch: 40, train loss: 0.24984, val loss: 0.28224\n",
      "Interaction tuning epoch: 41, train loss: 0.25470, val loss: 0.28966\n",
      "Interaction tuning epoch: 42, train loss: 0.25115, val loss: 0.28491\n",
      "Interaction tuning epoch: 43, train loss: 0.25207, val loss: 0.28120\n",
      "Interaction tuning epoch: 44, train loss: 0.24854, val loss: 0.27828\n",
      "Interaction tuning epoch: 45, train loss: 0.24848, val loss: 0.28317\n",
      "Interaction tuning epoch: 46, train loss: 0.24765, val loss: 0.28039\n",
      "Interaction tuning epoch: 47, train loss: 0.24686, val loss: 0.28083\n",
      "Interaction tuning epoch: 48, train loss: 0.24563, val loss: 0.27674\n",
      "Interaction tuning epoch: 49, train loss: 0.24760, val loss: 0.28260\n",
      "Interaction tuning epoch: 50, train loss: 0.24515, val loss: 0.27950\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 56.67484188079834\n",
      "After the gam stage, training error is 0.24515 , validation error is 0.27950\n",
      "missing value counts: 99194\n",
      "#####start auto_tuning#####\n",
      "the best shrinkage is 0.562500\n",
      "[SoftImpute] Max Singular Value of X_init = 4.017273\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.223875 validation BCE=0.276710,rank=3\n",
      "[SoftImpute] Iter 1: observed BCE=0.226982 validation BCE=0.275989,rank=3\n",
      "[SoftImpute] Iter 2: observed BCE=0.227725 validation BCE=0.275060,rank=3\n",
      "[SoftImpute] Iter 3: observed BCE=0.227251 validation BCE=0.274809,rank=3\n",
      "[SoftImpute] Iter 4: observed BCE=0.226890 validation BCE=0.274233,rank=3\n",
      "[SoftImpute] Iter 5: observed BCE=0.226672 validation BCE=0.274243,rank=3\n",
      "[SoftImpute] Iter 6: observed BCE=0.226456 validation BCE=0.274016,rank=3\n",
      "[SoftImpute] Iter 7: observed BCE=0.226238 validation BCE=0.273200,rank=3\n",
      "[SoftImpute] Iter 8: observed BCE=0.226256 validation BCE=0.273541,rank=3\n",
      "[SoftImpute] Iter 9: observed BCE=0.225780 validation BCE=0.273678,rank=3\n",
      "[SoftImpute] Iter 10: observed BCE=0.226307 validation BCE=0.273370,rank=3\n",
      "[SoftImpute] Iter 11: observed BCE=0.226415 validation BCE=0.273413,rank=3\n",
      "[SoftImpute] Iter 12: observed BCE=0.226413 validation BCE=0.273314,rank=3\n",
      "[SoftImpute] Iter 13: observed BCE=0.226483 validation BCE=0.273244,rank=3\n",
      "[SoftImpute] Iter 14: observed BCE=0.226121 validation BCE=0.273502,rank=3\n",
      "[SoftImpute] Iter 15: observed BCE=0.226289 validation BCE=0.273019,rank=3\n",
      "[SoftImpute] Iter 16: observed BCE=0.226036 validation BCE=0.273280,rank=3\n",
      "[SoftImpute] Iter 17: observed BCE=0.226198 validation BCE=0.273520,rank=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 18: observed BCE=0.226228 validation BCE=0.273580,rank=3\n",
      "[SoftImpute] Iter 19: observed BCE=0.226476 validation BCE=0.273146,rank=3\n",
      "[SoftImpute] Iter 20: observed BCE=0.225986 validation BCE=0.273236,rank=3\n",
      "[SoftImpute] Iter 21: observed BCE=0.226201 validation BCE=0.273246,rank=3\n",
      "[SoftImpute] Iter 22: observed BCE=0.226043 validation BCE=0.273199,rank=3\n",
      "[SoftImpute] Iter 23: observed BCE=0.226359 validation BCE=0.273498,rank=3\n",
      "[SoftImpute] Iter 24: observed BCE=0.225981 validation BCE=0.273423,rank=3\n",
      "[SoftImpute] Iter 25: observed BCE=0.226042 validation BCE=0.273180,rank=3\n",
      "[SoftImpute] Iter 26: observed BCE=0.226247 validation BCE=0.273703,rank=3\n",
      "[SoftImpute] Iter 27: observed BCE=0.225991 validation BCE=0.273206,rank=3\n",
      "[SoftImpute] Iter 28: observed BCE=0.225992 validation BCE=0.273119,rank=3\n",
      "[SoftImpute] Iter 29: observed BCE=0.226220 validation BCE=0.273576,rank=3\n",
      "[SoftImpute] Iter 30: observed BCE=0.226050 validation BCE=0.273490,rank=3\n",
      "[SoftImpute] Iter 31: observed BCE=0.226201 validation BCE=0.273363,rank=3\n",
      "[SoftImpute] Iter 32: observed BCE=0.225999 validation BCE=0.273562,rank=3\n",
      "[SoftImpute] Iter 33: observed BCE=0.226209 validation BCE=0.273418,rank=3\n",
      "[SoftImpute] Iter 34: observed BCE=0.225932 validation BCE=0.273325,rank=3\n",
      "[SoftImpute] Iter 35: observed BCE=0.226049 validation BCE=0.273277,rank=3\n",
      "[SoftImpute] Iter 36: observed BCE=0.225954 validation BCE=0.273423,rank=3\n",
      "[SoftImpute] Iter 37: observed BCE=0.226230 validation BCE=0.273136,rank=3\n",
      "[SoftImpute] Iter 38: observed BCE=0.226291 validation BCE=0.273584,rank=3\n",
      "[SoftImpute] Iter 39: observed BCE=0.226127 validation BCE=0.273280,rank=3\n",
      "[SoftImpute] Iter 40: observed BCE=0.225967 validation BCE=0.273059,rank=3\n",
      "[SoftImpute] Iter 41: observed BCE=0.226081 validation BCE=0.273285,rank=3\n",
      "[SoftImpute] Iter 42: observed BCE=0.226035 validation BCE=0.273221,rank=3\n",
      "[SoftImpute] Iter 43: observed BCE=0.226197 validation BCE=0.273539,rank=3\n",
      "[SoftImpute] Iter 44: observed BCE=0.226021 validation BCE=0.273494,rank=3\n",
      "[SoftImpute] Iter 45: observed BCE=0.226130 validation BCE=0.273096,rank=3\n",
      "[SoftImpute] Iter 46: observed BCE=0.226026 validation BCE=0.273415,rank=3\n",
      "[SoftImpute] Iter 47: observed BCE=0.226181 validation BCE=0.273426,rank=3\n",
      "[SoftImpute] Iter 48: observed BCE=0.225988 validation BCE=0.273149,rank=3\n",
      "[SoftImpute] Iter 49: observed BCE=0.226085 validation BCE=0.273489,rank=3\n",
      "[SoftImpute] Iter 50: observed BCE=0.226152 validation BCE=0.273709,rank=3\n",
      "[SoftImpute] Iter 51: observed BCE=0.225917 validation BCE=0.273173,rank=3\n",
      "[SoftImpute] Iter 52: observed BCE=0.225964 validation BCE=0.273487,rank=3\n",
      "[SoftImpute] Iter 53: observed BCE=0.226167 validation BCE=0.273664,rank=3\n",
      "[SoftImpute] Iter 54: observed BCE=0.226090 validation BCE=0.273208,rank=3\n",
      "[SoftImpute] Iter 55: observed BCE=0.226059 validation BCE=0.273244,rank=3\n",
      "[SoftImpute] Iter 56: observed BCE=0.225873 validation BCE=0.273725,rank=3\n",
      "[SoftImpute] Iter 57: observed BCE=0.226181 validation BCE=0.273379,rank=3\n",
      "[SoftImpute] Iter 58: observed BCE=0.226142 validation BCE=0.273525,rank=3\n",
      "[SoftImpute] Iter 59: observed BCE=0.226191 validation BCE=0.273315,rank=3\n",
      "[SoftImpute] Iter 60: observed BCE=0.225830 validation BCE=0.272868,rank=3\n",
      "[SoftImpute] Iter 61: observed BCE=0.226204 validation BCE=0.273359,rank=3\n",
      "[SoftImpute] Iter 62: observed BCE=0.226111 validation BCE=0.273683,rank=3\n",
      "[SoftImpute] Iter 63: observed BCE=0.226097 validation BCE=0.273054,rank=3\n",
      "[SoftImpute] Iter 64: observed BCE=0.226022 validation BCE=0.273240,rank=3\n",
      "[SoftImpute] Iter 65: observed BCE=0.226115 validation BCE=0.273577,rank=3\n",
      "[SoftImpute] Iter 66: observed BCE=0.226165 validation BCE=0.272995,rank=3\n",
      "[SoftImpute] Iter 67: observed BCE=0.226206 validation BCE=0.273482,rank=3\n",
      "[SoftImpute] Iter 68: observed BCE=0.226023 validation BCE=0.273661,rank=3\n",
      "[SoftImpute] Iter 69: observed BCE=0.226150 validation BCE=0.272986,rank=3\n",
      "[SoftImpute] Iter 70: observed BCE=0.226016 validation BCE=0.273499,rank=3\n",
      "[SoftImpute] Iter 71: observed BCE=0.225983 validation BCE=0.273535,rank=3\n",
      "[SoftImpute] Iter 72: observed BCE=0.226103 validation BCE=0.273243,rank=3\n",
      "[SoftImpute] Iter 73: observed BCE=0.225975 validation BCE=0.273352,rank=3\n",
      "[SoftImpute] Iter 74: observed BCE=0.226051 validation BCE=0.273636,rank=3\n",
      "[SoftImpute] Iter 75: observed BCE=0.226179 validation BCE=0.273136,rank=3\n",
      "[SoftImpute] Iter 76: observed BCE=0.226057 validation BCE=0.273367,rank=3\n",
      "[SoftImpute] Iter 77: observed BCE=0.226143 validation BCE=0.273424,rank=3\n",
      "[SoftImpute] Iter 78: observed BCE=0.225987 validation BCE=0.273408,rank=3\n",
      "[SoftImpute] Iter 79: observed BCE=0.226242 validation BCE=0.273466,rank=3\n",
      "[SoftImpute] Iter 80: observed BCE=0.226067 validation BCE=0.273756,rank=3\n",
      "[SoftImpute] Iter 81: observed BCE=0.226107 validation BCE=0.273023,rank=3\n",
      "[SoftImpute] Iter 82: observed BCE=0.225952 validation BCE=0.273424,rank=3\n",
      "[SoftImpute] Iter 83: observed BCE=0.226138 validation BCE=0.273349,rank=3\n",
      "[SoftImpute] Iter 84: observed BCE=0.226003 validation BCE=0.273007,rank=3\n",
      "[SoftImpute] Iter 85: observed BCE=0.226123 validation BCE=0.273420,rank=3\n",
      "[SoftImpute] Iter 86: observed BCE=0.226151 validation BCE=0.273621,rank=3\n",
      "[SoftImpute] Iter 87: observed BCE=0.225970 validation BCE=0.273139,rank=3\n",
      "[SoftImpute] Iter 88: observed BCE=0.225980 validation BCE=0.273362,rank=3\n",
      "[SoftImpute] Iter 89: observed BCE=0.226184 validation BCE=0.273238,rank=3\n",
      "[SoftImpute] Iter 90: observed BCE=0.225958 validation BCE=0.273191,rank=3\n",
      "[SoftImpute] Iter 91: observed BCE=0.226165 validation BCE=0.273448,rank=3\n",
      "[SoftImpute] Iter 92: observed BCE=0.225795 validation BCE=0.273689,rank=3\n",
      "[SoftImpute] Iter 93: observed BCE=0.225986 validation BCE=0.273017,rank=3\n",
      "[SoftImpute] Iter 94: observed BCE=0.226267 validation BCE=0.273620,rank=3\n",
      "[SoftImpute] Iter 95: observed BCE=0.226044 validation BCE=0.273387,rank=3\n",
      "[SoftImpute] Iter 96: observed BCE=0.226114 validation BCE=0.273215,rank=3\n",
      "[SoftImpute] Iter 97: observed BCE=0.226126 validation BCE=0.273336,rank=3\n",
      "[SoftImpute] Iter 98: observed BCE=0.226039 validation BCE=0.273479,rank=3\n",
      "[SoftImpute] Iter 99: observed BCE=0.226143 validation BCE=0.273399,rank=3\n",
      "[SoftImpute] Iter 100: observed BCE=0.225972 validation BCE=0.273635,rank=3\n",
      "[SoftImpute] Stopped after iteration 100 for lambda=0.080345\n",
      "final num of user group: 6\n",
      "final num of item group: 21\n",
      "change mode state : True\n",
      "time cost: 35.80016827583313\n",
      "After the matrix factor stage, training error is 0.22597, validation error is 0.27363\n",
      "5\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68084, val loss: 0.68149\n",
      "Main effects training epoch: 2, train loss: 0.67603, val loss: 0.67698\n",
      "Main effects training epoch: 3, train loss: 0.67233, val loss: 0.67229\n",
      "Main effects training epoch: 4, train loss: 0.66606, val loss: 0.66398\n",
      "Main effects training epoch: 5, train loss: 0.65496, val loss: 0.65104\n",
      "Main effects training epoch: 6, train loss: 0.63158, val loss: 0.62511\n",
      "Main effects training epoch: 7, train loss: 0.59160, val loss: 0.58298\n",
      "Main effects training epoch: 8, train loss: 0.55171, val loss: 0.53857\n",
      "Main effects training epoch: 9, train loss: 0.53834, val loss: 0.52309\n",
      "Main effects training epoch: 10, train loss: 0.53865, val loss: 0.52579\n",
      "Main effects training epoch: 11, train loss: 0.53383, val loss: 0.51552\n",
      "Main effects training epoch: 12, train loss: 0.53131, val loss: 0.52066\n",
      "Main effects training epoch: 13, train loss: 0.53030, val loss: 0.51587\n",
      "Main effects training epoch: 14, train loss: 0.53005, val loss: 0.51906\n",
      "Main effects training epoch: 15, train loss: 0.52883, val loss: 0.51660\n",
      "Main effects training epoch: 16, train loss: 0.52882, val loss: 0.51610\n",
      "Main effects training epoch: 17, train loss: 0.52939, val loss: 0.51653\n",
      "Main effects training epoch: 18, train loss: 0.52977, val loss: 0.51804\n",
      "Main effects training epoch: 19, train loss: 0.52869, val loss: 0.51685\n",
      "Main effects training epoch: 20, train loss: 0.52828, val loss: 0.51577\n",
      "Main effects training epoch: 21, train loss: 0.52841, val loss: 0.51710\n",
      "Main effects training epoch: 22, train loss: 0.52927, val loss: 0.51739\n",
      "Main effects training epoch: 23, train loss: 0.52820, val loss: 0.51704\n",
      "Main effects training epoch: 24, train loss: 0.52801, val loss: 0.51648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 25, train loss: 0.52825, val loss: 0.51537\n",
      "Main effects training epoch: 26, train loss: 0.52798, val loss: 0.51626\n",
      "Main effects training epoch: 27, train loss: 0.52793, val loss: 0.51674\n",
      "Main effects training epoch: 28, train loss: 0.52793, val loss: 0.51566\n",
      "Main effects training epoch: 29, train loss: 0.52811, val loss: 0.51761\n",
      "Main effects training epoch: 30, train loss: 0.52785, val loss: 0.51632\n",
      "Main effects training epoch: 31, train loss: 0.52795, val loss: 0.51610\n",
      "Main effects training epoch: 32, train loss: 0.52767, val loss: 0.51633\n",
      "Main effects training epoch: 33, train loss: 0.52781, val loss: 0.51611\n",
      "Main effects training epoch: 34, train loss: 0.52761, val loss: 0.51616\n",
      "Main effects training epoch: 35, train loss: 0.52762, val loss: 0.51598\n",
      "Main effects training epoch: 36, train loss: 0.52813, val loss: 0.51691\n",
      "Main effects training epoch: 37, train loss: 0.52783, val loss: 0.51738\n",
      "Main effects training epoch: 38, train loss: 0.52757, val loss: 0.51663\n",
      "Main effects training epoch: 39, train loss: 0.52773, val loss: 0.51655\n",
      "Main effects training epoch: 40, train loss: 0.52749, val loss: 0.51661\n",
      "Main effects training epoch: 41, train loss: 0.52848, val loss: 0.51748\n",
      "Main effects training epoch: 42, train loss: 0.52746, val loss: 0.51628\n",
      "Main effects training epoch: 43, train loss: 0.52776, val loss: 0.51817\n",
      "Main effects training epoch: 44, train loss: 0.52729, val loss: 0.51723\n",
      "Main effects training epoch: 45, train loss: 0.52756, val loss: 0.51723\n",
      "Main effects training epoch: 46, train loss: 0.52723, val loss: 0.51627\n",
      "Main effects training epoch: 47, train loss: 0.52712, val loss: 0.51717\n",
      "Main effects training epoch: 48, train loss: 0.52707, val loss: 0.51708\n",
      "Main effects training epoch: 49, train loss: 0.52711, val loss: 0.51734\n",
      "Main effects training epoch: 50, train loss: 0.52712, val loss: 0.51845\n",
      "Main effects training epoch: 51, train loss: 0.52824, val loss: 0.51790\n",
      "Main effects training epoch: 52, train loss: 0.52749, val loss: 0.51763\n",
      "Main effects training epoch: 53, train loss: 0.52700, val loss: 0.51724\n",
      "Main effects training epoch: 54, train loss: 0.52762, val loss: 0.51860\n",
      "Main effects training epoch: 55, train loss: 0.52801, val loss: 0.51749\n",
      "Main effects training epoch: 56, train loss: 0.52770, val loss: 0.51822\n",
      "Main effects training epoch: 57, train loss: 0.52762, val loss: 0.51862\n",
      "Main effects training epoch: 58, train loss: 0.52780, val loss: 0.51740\n",
      "Main effects training epoch: 59, train loss: 0.52735, val loss: 0.51912\n",
      "Main effects training epoch: 60, train loss: 0.52700, val loss: 0.51784\n",
      "Main effects training epoch: 61, train loss: 0.52674, val loss: 0.51764\n",
      "Main effects training epoch: 62, train loss: 0.52695, val loss: 0.51813\n",
      "Main effects training epoch: 63, train loss: 0.52687, val loss: 0.51784\n",
      "Main effects training epoch: 64, train loss: 0.52685, val loss: 0.51726\n",
      "Main effects training epoch: 65, train loss: 0.52667, val loss: 0.51751\n",
      "Main effects training epoch: 66, train loss: 0.52671, val loss: 0.51756\n",
      "Main effects training epoch: 67, train loss: 0.52654, val loss: 0.51753\n",
      "Main effects training epoch: 68, train loss: 0.52674, val loss: 0.51737\n",
      "Main effects training epoch: 69, train loss: 0.52689, val loss: 0.51703\n",
      "Main effects training epoch: 70, train loss: 0.52690, val loss: 0.51659\n",
      "Main effects training epoch: 71, train loss: 0.52686, val loss: 0.51761\n",
      "Main effects training epoch: 72, train loss: 0.52679, val loss: 0.51740\n",
      "Main effects training epoch: 73, train loss: 0.52700, val loss: 0.51866\n",
      "Main effects training epoch: 74, train loss: 0.52680, val loss: 0.51712\n",
      "Main effects training epoch: 75, train loss: 0.52687, val loss: 0.51742\n",
      "Main effects training epoch: 76, train loss: 0.52672, val loss: 0.51746\n",
      "Main effects training epoch: 77, train loss: 0.52653, val loss: 0.51722\n",
      "Main effects training epoch: 78, train loss: 0.52662, val loss: 0.51694\n",
      "Main effects training epoch: 79, train loss: 0.52689, val loss: 0.51753\n",
      "Main effects training epoch: 80, train loss: 0.52718, val loss: 0.51766\n",
      "Main effects training epoch: 81, train loss: 0.52724, val loss: 0.51874\n",
      "Main effects training epoch: 82, train loss: 0.52676, val loss: 0.51775\n",
      "Main effects training epoch: 83, train loss: 0.52669, val loss: 0.51826\n",
      "Main effects training epoch: 84, train loss: 0.52680, val loss: 0.51628\n",
      "Main effects training epoch: 85, train loss: 0.52651, val loss: 0.51772\n",
      "Main effects training epoch: 86, train loss: 0.52650, val loss: 0.51783\n",
      "Main effects training epoch: 87, train loss: 0.52609, val loss: 0.51726\n",
      "Main effects training epoch: 88, train loss: 0.52623, val loss: 0.51800\n",
      "Main effects training epoch: 89, train loss: 0.52626, val loss: 0.51666\n",
      "Main effects training epoch: 90, train loss: 0.52629, val loss: 0.51721\n",
      "Main effects training epoch: 91, train loss: 0.52619, val loss: 0.51740\n",
      "Main effects training epoch: 92, train loss: 0.52616, val loss: 0.51641\n",
      "Main effects training epoch: 93, train loss: 0.52613, val loss: 0.51834\n",
      "Main effects training epoch: 94, train loss: 0.52593, val loss: 0.51660\n",
      "Main effects training epoch: 95, train loss: 0.52603, val loss: 0.51760\n",
      "Main effects training epoch: 96, train loss: 0.52593, val loss: 0.51751\n",
      "Main effects training epoch: 97, train loss: 0.52604, val loss: 0.51728\n",
      "Main effects training epoch: 98, train loss: 0.52604, val loss: 0.51660\n",
      "Main effects training epoch: 99, train loss: 0.52604, val loss: 0.51755\n",
      "Main effects training epoch: 100, train loss: 0.52614, val loss: 0.51661\n",
      "Main effects training epoch: 101, train loss: 0.52612, val loss: 0.51749\n",
      "Main effects training epoch: 102, train loss: 0.52619, val loss: 0.51827\n",
      "Main effects training epoch: 103, train loss: 0.52661, val loss: 0.51702\n",
      "Main effects training epoch: 104, train loss: 0.52635, val loss: 0.51813\n",
      "Main effects training epoch: 105, train loss: 0.52695, val loss: 0.51782\n",
      "Main effects training epoch: 106, train loss: 0.52912, val loss: 0.52154\n",
      "Main effects training epoch: 107, train loss: 0.52953, val loss: 0.52068\n",
      "Main effects training epoch: 108, train loss: 0.52823, val loss: 0.52010\n",
      "Main effects training epoch: 109, train loss: 0.52741, val loss: 0.51873\n",
      "Main effects training epoch: 110, train loss: 0.52667, val loss: 0.51677\n",
      "Main effects training epoch: 111, train loss: 0.52574, val loss: 0.51724\n",
      "Main effects training epoch: 112, train loss: 0.52581, val loss: 0.51699\n",
      "Main effects training epoch: 113, train loss: 0.52577, val loss: 0.51803\n",
      "Main effects training epoch: 114, train loss: 0.52569, val loss: 0.51647\n",
      "Main effects training epoch: 115, train loss: 0.52558, val loss: 0.51843\n",
      "Main effects training epoch: 116, train loss: 0.52572, val loss: 0.51610\n",
      "Main effects training epoch: 117, train loss: 0.52546, val loss: 0.51682\n",
      "Main effects training epoch: 118, train loss: 0.52537, val loss: 0.51758\n",
      "Main effects training epoch: 119, train loss: 0.52551, val loss: 0.51785\n",
      "Main effects training epoch: 120, train loss: 0.52558, val loss: 0.51687\n",
      "Main effects training epoch: 121, train loss: 0.52638, val loss: 0.51817\n",
      "Main effects training epoch: 122, train loss: 0.52577, val loss: 0.51640\n",
      "Main effects training epoch: 123, train loss: 0.52568, val loss: 0.51806\n",
      "Main effects training epoch: 124, train loss: 0.52562, val loss: 0.51612\n",
      "Main effects training epoch: 125, train loss: 0.52541, val loss: 0.51796\n",
      "Main effects training epoch: 126, train loss: 0.52585, val loss: 0.51728\n",
      "Early stop at epoch 126, with validation loss: 0.51728\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.52835, val loss: 0.51446\n",
      "Main effects tuning epoch: 2, train loss: 0.52903, val loss: 0.51382\n",
      "Main effects tuning epoch: 3, train loss: 0.52802, val loss: 0.51400\n",
      "Main effects tuning epoch: 4, train loss: 0.52768, val loss: 0.51335\n",
      "Main effects tuning epoch: 5, train loss: 0.52771, val loss: 0.51376\n",
      "Main effects tuning epoch: 6, train loss: 0.52783, val loss: 0.51338\n",
      "Main effects tuning epoch: 7, train loss: 0.52767, val loss: 0.51387\n",
      "Main effects tuning epoch: 8, train loss: 0.52766, val loss: 0.51351\n",
      "Main effects tuning epoch: 9, train loss: 0.52751, val loss: 0.51359\n",
      "Main effects tuning epoch: 10, train loss: 0.52769, val loss: 0.51361\n",
      "Main effects tuning epoch: 11, train loss: 0.52752, val loss: 0.51369\n",
      "Main effects tuning epoch: 12, train loss: 0.52750, val loss: 0.51315\n",
      "Main effects tuning epoch: 13, train loss: 0.52769, val loss: 0.51372\n",
      "Main effects tuning epoch: 14, train loss: 0.52743, val loss: 0.51277\n",
      "Main effects tuning epoch: 15, train loss: 0.52742, val loss: 0.51339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects tuning epoch: 16, train loss: 0.52735, val loss: 0.51340\n",
      "Main effects tuning epoch: 17, train loss: 0.52748, val loss: 0.51291\n",
      "Main effects tuning epoch: 18, train loss: 0.52744, val loss: 0.51330\n",
      "Main effects tuning epoch: 19, train loss: 0.52735, val loss: 0.51358\n",
      "Main effects tuning epoch: 20, train loss: 0.52762, val loss: 0.51294\n",
      "Main effects tuning epoch: 21, train loss: 0.52776, val loss: 0.51438\n",
      "Main effects tuning epoch: 22, train loss: 0.52793, val loss: 0.51267\n",
      "Main effects tuning epoch: 23, train loss: 0.52729, val loss: 0.51387\n",
      "Main effects tuning epoch: 24, train loss: 0.52737, val loss: 0.51299\n",
      "Main effects tuning epoch: 25, train loss: 0.52726, val loss: 0.51323\n",
      "Main effects tuning epoch: 26, train loss: 0.52718, val loss: 0.51301\n",
      "Main effects tuning epoch: 27, train loss: 0.52711, val loss: 0.51332\n",
      "Main effects tuning epoch: 28, train loss: 0.52738, val loss: 0.51295\n",
      "Main effects tuning epoch: 29, train loss: 0.52702, val loss: 0.51304\n",
      "Main effects tuning epoch: 30, train loss: 0.52713, val loss: 0.51389\n",
      "Main effects tuning epoch: 31, train loss: 0.52703, val loss: 0.51311\n",
      "Main effects tuning epoch: 32, train loss: 0.52710, val loss: 0.51323\n",
      "Main effects tuning epoch: 33, train loss: 0.52716, val loss: 0.51320\n",
      "Main effects tuning epoch: 34, train loss: 0.52685, val loss: 0.51264\n",
      "Main effects tuning epoch: 35, train loss: 0.52690, val loss: 0.51271\n",
      "Main effects tuning epoch: 36, train loss: 0.52712, val loss: 0.51277\n",
      "Main effects tuning epoch: 37, train loss: 0.52716, val loss: 0.51325\n",
      "Main effects tuning epoch: 38, train loss: 0.52708, val loss: 0.51313\n",
      "Main effects tuning epoch: 39, train loss: 0.52717, val loss: 0.51244\n",
      "Main effects tuning epoch: 40, train loss: 0.52686, val loss: 0.51283\n",
      "Main effects tuning epoch: 41, train loss: 0.52676, val loss: 0.51361\n",
      "Main effects tuning epoch: 42, train loss: 0.52667, val loss: 0.51310\n",
      "Main effects tuning epoch: 43, train loss: 0.52667, val loss: 0.51253\n",
      "Main effects tuning epoch: 44, train loss: 0.52668, val loss: 0.51303\n",
      "Main effects tuning epoch: 45, train loss: 0.52658, val loss: 0.51246\n",
      "Main effects tuning epoch: 46, train loss: 0.52654, val loss: 0.51276\n",
      "Main effects tuning epoch: 47, train loss: 0.52661, val loss: 0.51253\n",
      "Main effects tuning epoch: 48, train loss: 0.52667, val loss: 0.51299\n",
      "Main effects tuning epoch: 49, train loss: 0.52639, val loss: 0.51278\n",
      "Main effects tuning epoch: 50, train loss: 0.52641, val loss: 0.51283\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.51636, val loss: 0.50188\n",
      "Interaction training epoch: 2, train loss: 0.36466, val loss: 0.34940\n",
      "Interaction training epoch: 3, train loss: 0.33428, val loss: 0.32449\n",
      "Interaction training epoch: 4, train loss: 0.32422, val loss: 0.30564\n",
      "Interaction training epoch: 5, train loss: 0.31747, val loss: 0.31233\n",
      "Interaction training epoch: 6, train loss: 0.29700, val loss: 0.28838\n",
      "Interaction training epoch: 7, train loss: 0.30994, val loss: 0.30858\n",
      "Interaction training epoch: 8, train loss: 0.30163, val loss: 0.29858\n",
      "Interaction training epoch: 9, train loss: 0.29416, val loss: 0.29285\n",
      "Interaction training epoch: 10, train loss: 0.28687, val loss: 0.28284\n",
      "Interaction training epoch: 11, train loss: 0.28682, val loss: 0.28091\n",
      "Interaction training epoch: 12, train loss: 0.28644, val loss: 0.28391\n",
      "Interaction training epoch: 13, train loss: 0.28644, val loss: 0.28483\n",
      "Interaction training epoch: 14, train loss: 0.28709, val loss: 0.28551\n",
      "Interaction training epoch: 15, train loss: 0.28344, val loss: 0.28044\n",
      "Interaction training epoch: 16, train loss: 0.28001, val loss: 0.27851\n",
      "Interaction training epoch: 17, train loss: 0.27756, val loss: 0.27272\n",
      "Interaction training epoch: 18, train loss: 0.29083, val loss: 0.28913\n",
      "Interaction training epoch: 19, train loss: 0.29313, val loss: 0.29278\n",
      "Interaction training epoch: 20, train loss: 0.28364, val loss: 0.28996\n",
      "Interaction training epoch: 21, train loss: 0.28062, val loss: 0.27618\n",
      "Interaction training epoch: 22, train loss: 0.28029, val loss: 0.27786\n",
      "Interaction training epoch: 23, train loss: 0.28235, val loss: 0.28546\n",
      "Interaction training epoch: 24, train loss: 0.28513, val loss: 0.28531\n",
      "Interaction training epoch: 25, train loss: 0.27956, val loss: 0.28226\n",
      "Interaction training epoch: 26, train loss: 0.28005, val loss: 0.28139\n",
      "Interaction training epoch: 27, train loss: 0.27697, val loss: 0.27862\n",
      "Interaction training epoch: 28, train loss: 0.28213, val loss: 0.28720\n",
      "Interaction training epoch: 29, train loss: 0.27615, val loss: 0.27509\n",
      "Interaction training epoch: 30, train loss: 0.28065, val loss: 0.28213\n",
      "Interaction training epoch: 31, train loss: 0.28273, val loss: 0.28836\n",
      "Interaction training epoch: 32, train loss: 0.27586, val loss: 0.27623\n",
      "Interaction training epoch: 33, train loss: 0.27657, val loss: 0.28082\n",
      "Interaction training epoch: 34, train loss: 0.28033, val loss: 0.28479\n",
      "Interaction training epoch: 35, train loss: 0.27794, val loss: 0.27707\n",
      "Interaction training epoch: 36, train loss: 0.27689, val loss: 0.28257\n",
      "Interaction training epoch: 37, train loss: 0.27876, val loss: 0.28122\n",
      "Interaction training epoch: 38, train loss: 0.27541, val loss: 0.27759\n",
      "Interaction training epoch: 39, train loss: 0.27310, val loss: 0.27374\n",
      "Interaction training epoch: 40, train loss: 0.27631, val loss: 0.28766\n",
      "Interaction training epoch: 41, train loss: 0.28094, val loss: 0.28207\n",
      "Interaction training epoch: 42, train loss: 0.26942, val loss: 0.27153\n",
      "Interaction training epoch: 43, train loss: 0.27850, val loss: 0.28802\n",
      "Interaction training epoch: 44, train loss: 0.28038, val loss: 0.28476\n",
      "Interaction training epoch: 45, train loss: 0.27181, val loss: 0.27543\n",
      "Interaction training epoch: 46, train loss: 0.27220, val loss: 0.27963\n",
      "Interaction training epoch: 47, train loss: 0.27297, val loss: 0.27429\n",
      "Interaction training epoch: 48, train loss: 0.27711, val loss: 0.28665\n",
      "Interaction training epoch: 49, train loss: 0.26829, val loss: 0.27207\n",
      "Interaction training epoch: 50, train loss: 0.27504, val loss: 0.27604\n",
      "Interaction training epoch: 51, train loss: 0.27253, val loss: 0.28436\n",
      "Interaction training epoch: 52, train loss: 0.27598, val loss: 0.28475\n",
      "Interaction training epoch: 53, train loss: 0.26878, val loss: 0.27444\n",
      "Interaction training epoch: 54, train loss: 0.27002, val loss: 0.28135\n",
      "Interaction training epoch: 55, train loss: 0.27083, val loss: 0.27511\n",
      "Interaction training epoch: 56, train loss: 0.26919, val loss: 0.28215\n",
      "Interaction training epoch: 57, train loss: 0.26904, val loss: 0.27729\n",
      "Interaction training epoch: 58, train loss: 0.26872, val loss: 0.27502\n",
      "Interaction training epoch: 59, train loss: 0.27376, val loss: 0.28819\n",
      "Interaction training epoch: 60, train loss: 0.27060, val loss: 0.28077\n",
      "Interaction training epoch: 61, train loss: 0.26525, val loss: 0.27101\n",
      "Interaction training epoch: 62, train loss: 0.27094, val loss: 0.28523\n",
      "Interaction training epoch: 63, train loss: 0.26672, val loss: 0.27598\n",
      "Interaction training epoch: 64, train loss: 0.26659, val loss: 0.27840\n",
      "Interaction training epoch: 65, train loss: 0.27047, val loss: 0.28179\n",
      "Interaction training epoch: 66, train loss: 0.26740, val loss: 0.27986\n",
      "Interaction training epoch: 67, train loss: 0.26837, val loss: 0.28560\n",
      "Interaction training epoch: 68, train loss: 0.26636, val loss: 0.27771\n",
      "Interaction training epoch: 69, train loss: 0.26678, val loss: 0.27518\n",
      "Interaction training epoch: 70, train loss: 0.26835, val loss: 0.28370\n",
      "Interaction training epoch: 71, train loss: 0.26436, val loss: 0.26738\n",
      "Interaction training epoch: 72, train loss: 0.26640, val loss: 0.28531\n",
      "Interaction training epoch: 73, train loss: 0.26138, val loss: 0.27117\n",
      "Interaction training epoch: 74, train loss: 0.27100, val loss: 0.28281\n",
      "Interaction training epoch: 75, train loss: 0.26850, val loss: 0.28602\n",
      "Interaction training epoch: 76, train loss: 0.26312, val loss: 0.27667\n",
      "Interaction training epoch: 77, train loss: 0.26260, val loss: 0.27905\n",
      "Interaction training epoch: 78, train loss: 0.26499, val loss: 0.28090\n",
      "Interaction training epoch: 79, train loss: 0.26237, val loss: 0.27448\n",
      "Interaction training epoch: 80, train loss: 0.26341, val loss: 0.28331\n",
      "Interaction training epoch: 81, train loss: 0.26059, val loss: 0.27317\n",
      "Interaction training epoch: 82, train loss: 0.26865, val loss: 0.28701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 83, train loss: 0.26122, val loss: 0.27918\n",
      "Interaction training epoch: 84, train loss: 0.26602, val loss: 0.28483\n",
      "Interaction training epoch: 85, train loss: 0.26504, val loss: 0.27491\n",
      "Interaction training epoch: 86, train loss: 0.26864, val loss: 0.29061\n",
      "Interaction training epoch: 87, train loss: 0.26240, val loss: 0.28197\n",
      "Interaction training epoch: 88, train loss: 0.26374, val loss: 0.27324\n",
      "Interaction training epoch: 89, train loss: 0.25777, val loss: 0.27724\n",
      "Interaction training epoch: 90, train loss: 0.26056, val loss: 0.27895\n",
      "Interaction training epoch: 91, train loss: 0.26313, val loss: 0.28305\n",
      "Interaction training epoch: 92, train loss: 0.26189, val loss: 0.28317\n",
      "Interaction training epoch: 93, train loss: 0.26770, val loss: 0.28296\n",
      "Interaction training epoch: 94, train loss: 0.25861, val loss: 0.28240\n",
      "Interaction training epoch: 95, train loss: 0.26103, val loss: 0.27927\n",
      "Interaction training epoch: 96, train loss: 0.25911, val loss: 0.27668\n",
      "Interaction training epoch: 97, train loss: 0.26230, val loss: 0.28064\n",
      "Interaction training epoch: 98, train loss: 0.26235, val loss: 0.28514\n",
      "Interaction training epoch: 99, train loss: 0.25660, val loss: 0.27554\n",
      "Interaction training epoch: 100, train loss: 0.25968, val loss: 0.27602\n",
      "Interaction training epoch: 101, train loss: 0.25738, val loss: 0.28149\n",
      "Interaction training epoch: 102, train loss: 0.26141, val loss: 0.28097\n",
      "Interaction training epoch: 103, train loss: 0.25871, val loss: 0.27878\n",
      "Interaction training epoch: 104, train loss: 0.25892, val loss: 0.27937\n",
      "Interaction training epoch: 105, train loss: 0.26156, val loss: 0.28447\n",
      "Interaction training epoch: 106, train loss: 0.26233, val loss: 0.28324\n",
      "Interaction training epoch: 107, train loss: 0.25896, val loss: 0.28127\n",
      "Interaction training epoch: 108, train loss: 0.25636, val loss: 0.27786\n",
      "Interaction training epoch: 109, train loss: 0.25782, val loss: 0.27876\n",
      "Interaction training epoch: 110, train loss: 0.25673, val loss: 0.27489\n",
      "Interaction training epoch: 111, train loss: 0.25660, val loss: 0.27958\n",
      "Interaction training epoch: 112, train loss: 0.25969, val loss: 0.27959\n",
      "Interaction training epoch: 113, train loss: 0.25988, val loss: 0.29143\n",
      "Interaction training epoch: 114, train loss: 0.25617, val loss: 0.26888\n",
      "Interaction training epoch: 115, train loss: 0.26420, val loss: 0.28652\n",
      "Interaction training epoch: 116, train loss: 0.25771, val loss: 0.27845\n",
      "Interaction training epoch: 117, train loss: 0.25701, val loss: 0.27575\n",
      "Interaction training epoch: 118, train loss: 0.25423, val loss: 0.27370\n",
      "Interaction training epoch: 119, train loss: 0.25800, val loss: 0.28421\n",
      "Interaction training epoch: 120, train loss: 0.25670, val loss: 0.27834\n",
      "Interaction training epoch: 121, train loss: 0.25712, val loss: 0.27982\n",
      "Interaction training epoch: 122, train loss: 0.25309, val loss: 0.27466\n",
      "Interaction training epoch: 123, train loss: 0.25421, val loss: 0.27511\n",
      "Interaction training epoch: 124, train loss: 0.25685, val loss: 0.27943\n",
      "Interaction training epoch: 125, train loss: 0.25809, val loss: 0.28591\n",
      "Interaction training epoch: 126, train loss: 0.25378, val loss: 0.27209\n",
      "Interaction training epoch: 127, train loss: 0.25171, val loss: 0.27160\n",
      "Interaction training epoch: 128, train loss: 0.25407, val loss: 0.27858\n",
      "Interaction training epoch: 129, train loss: 0.25561, val loss: 0.27918\n",
      "Interaction training epoch: 130, train loss: 0.25402, val loss: 0.27525\n",
      "Interaction training epoch: 131, train loss: 0.26017, val loss: 0.28937\n",
      "Interaction training epoch: 132, train loss: 0.25637, val loss: 0.27757\n",
      "Interaction training epoch: 133, train loss: 0.25521, val loss: 0.27483\n",
      "Interaction training epoch: 134, train loss: 0.25561, val loss: 0.27883\n",
      "Interaction training epoch: 135, train loss: 0.25012, val loss: 0.27563\n",
      "Interaction training epoch: 136, train loss: 0.25177, val loss: 0.26798\n",
      "Interaction training epoch: 137, train loss: 0.25455, val loss: 0.28207\n",
      "Interaction training epoch: 138, train loss: 0.25415, val loss: 0.28236\n",
      "Interaction training epoch: 139, train loss: 0.25352, val loss: 0.27173\n",
      "Interaction training epoch: 140, train loss: 0.25121, val loss: 0.27336\n",
      "Interaction training epoch: 141, train loss: 0.25662, val loss: 0.28392\n",
      "Interaction training epoch: 142, train loss: 0.25107, val loss: 0.27813\n",
      "Interaction training epoch: 143, train loss: 0.25178, val loss: 0.27275\n",
      "Interaction training epoch: 144, train loss: 0.25259, val loss: 0.27220\n",
      "Interaction training epoch: 145, train loss: 0.25051, val loss: 0.27856\n",
      "Interaction training epoch: 146, train loss: 0.25913, val loss: 0.28435\n",
      "Interaction training epoch: 147, train loss: 0.24666, val loss: 0.26819\n",
      "Interaction training epoch: 148, train loss: 0.25437, val loss: 0.28091\n",
      "Interaction training epoch: 149, train loss: 0.24942, val loss: 0.27060\n",
      "Interaction training epoch: 150, train loss: 0.25092, val loss: 0.27598\n",
      "Interaction training epoch: 151, train loss: 0.25212, val loss: 0.27783\n",
      "Interaction training epoch: 152, train loss: 0.24692, val loss: 0.26738\n",
      "Interaction training epoch: 153, train loss: 0.25142, val loss: 0.27707\n",
      "Interaction training epoch: 154, train loss: 0.25179, val loss: 0.27550\n",
      "Interaction training epoch: 155, train loss: 0.25195, val loss: 0.27571\n",
      "Interaction training epoch: 156, train loss: 0.24887, val loss: 0.27200\n",
      "Interaction training epoch: 157, train loss: 0.25639, val loss: 0.28840\n",
      "Interaction training epoch: 158, train loss: 0.25155, val loss: 0.27433\n",
      "Interaction training epoch: 159, train loss: 0.25153, val loss: 0.27897\n",
      "Interaction training epoch: 160, train loss: 0.24996, val loss: 0.27134\n",
      "Interaction training epoch: 161, train loss: 0.24711, val loss: 0.27037\n",
      "Interaction training epoch: 162, train loss: 0.25305, val loss: 0.28158\n",
      "Interaction training epoch: 163, train loss: 0.26154, val loss: 0.29106\n",
      "Interaction training epoch: 164, train loss: 0.24474, val loss: 0.26423\n",
      "Interaction training epoch: 165, train loss: 0.25036, val loss: 0.27743\n",
      "Interaction training epoch: 166, train loss: 0.24800, val loss: 0.27470\n",
      "Interaction training epoch: 167, train loss: 0.24938, val loss: 0.27143\n",
      "Interaction training epoch: 168, train loss: 0.25061, val loss: 0.27475\n",
      "Interaction training epoch: 169, train loss: 0.24751, val loss: 0.27507\n",
      "Interaction training epoch: 170, train loss: 0.24860, val loss: 0.27369\n",
      "Interaction training epoch: 171, train loss: 0.24548, val loss: 0.26615\n",
      "Interaction training epoch: 172, train loss: 0.24885, val loss: 0.27862\n",
      "Interaction training epoch: 173, train loss: 0.24834, val loss: 0.27211\n",
      "Interaction training epoch: 174, train loss: 0.24643, val loss: 0.27121\n",
      "Interaction training epoch: 175, train loss: 0.24800, val loss: 0.27104\n",
      "Interaction training epoch: 176, train loss: 0.24672, val loss: 0.27244\n",
      "Interaction training epoch: 177, train loss: 0.24545, val loss: 0.27394\n",
      "Interaction training epoch: 178, train loss: 0.24707, val loss: 0.27290\n",
      "Interaction training epoch: 179, train loss: 0.24548, val loss: 0.27297\n",
      "Interaction training epoch: 180, train loss: 0.24813, val loss: 0.27734\n",
      "Interaction training epoch: 181, train loss: 0.24533, val loss: 0.26863\n",
      "Interaction training epoch: 182, train loss: 0.24644, val loss: 0.26975\n",
      "Interaction training epoch: 183, train loss: 0.24467, val loss: 0.27079\n",
      "Interaction training epoch: 184, train loss: 0.24687, val loss: 0.27450\n",
      "Interaction training epoch: 185, train loss: 0.24738, val loss: 0.27330\n",
      "Interaction training epoch: 186, train loss: 0.24385, val loss: 0.27134\n",
      "Interaction training epoch: 187, train loss: 0.24370, val loss: 0.26632\n",
      "Interaction training epoch: 188, train loss: 0.24544, val loss: 0.27520\n",
      "Interaction training epoch: 189, train loss: 0.24249, val loss: 0.27097\n",
      "Interaction training epoch: 190, train loss: 0.24556, val loss: 0.27050\n",
      "Interaction training epoch: 191, train loss: 0.24586, val loss: 0.27235\n",
      "Interaction training epoch: 192, train loss: 0.24756, val loss: 0.27425\n",
      "Interaction training epoch: 193, train loss: 0.24657, val loss: 0.27208\n",
      "Interaction training epoch: 194, train loss: 0.25139, val loss: 0.27931\n",
      "Interaction training epoch: 195, train loss: 0.24672, val loss: 0.26835\n",
      "Interaction training epoch: 196, train loss: 0.24319, val loss: 0.26857\n",
      "Interaction training epoch: 197, train loss: 0.24817, val loss: 0.27594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 198, train loss: 0.24243, val loss: 0.26969\n",
      "Interaction training epoch: 199, train loss: 0.24568, val loss: 0.27388\n",
      "Interaction training epoch: 200, train loss: 0.24468, val loss: 0.27376\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########1 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.25035, val loss: 0.26705\n",
      "Interaction tuning epoch: 2, train loss: 0.24449, val loss: 0.26793\n",
      "Interaction tuning epoch: 3, train loss: 0.24753, val loss: 0.27739\n",
      "Interaction tuning epoch: 4, train loss: 0.24487, val loss: 0.26581\n",
      "Interaction tuning epoch: 5, train loss: 0.24713, val loss: 0.27026\n",
      "Interaction tuning epoch: 6, train loss: 0.24295, val loss: 0.26830\n",
      "Interaction tuning epoch: 7, train loss: 0.24236, val loss: 0.26803\n",
      "Interaction tuning epoch: 8, train loss: 0.24452, val loss: 0.26549\n",
      "Interaction tuning epoch: 9, train loss: 0.24123, val loss: 0.26370\n",
      "Interaction tuning epoch: 10, train loss: 0.24493, val loss: 0.27190\n",
      "Interaction tuning epoch: 11, train loss: 0.24324, val loss: 0.26709\n",
      "Interaction tuning epoch: 12, train loss: 0.24367, val loss: 0.26310\n",
      "Interaction tuning epoch: 13, train loss: 0.24145, val loss: 0.27087\n",
      "Interaction tuning epoch: 14, train loss: 0.24445, val loss: 0.26261\n",
      "Interaction tuning epoch: 15, train loss: 0.25058, val loss: 0.27729\n",
      "Interaction tuning epoch: 16, train loss: 0.24317, val loss: 0.26889\n",
      "Interaction tuning epoch: 17, train loss: 0.24676, val loss: 0.26702\n",
      "Interaction tuning epoch: 18, train loss: 0.24427, val loss: 0.27572\n",
      "Interaction tuning epoch: 19, train loss: 0.24576, val loss: 0.26652\n",
      "Interaction tuning epoch: 20, train loss: 0.24382, val loss: 0.27220\n",
      "Interaction tuning epoch: 21, train loss: 0.24339, val loss: 0.26433\n",
      "Interaction tuning epoch: 22, train loss: 0.24529, val loss: 0.26868\n",
      "Interaction tuning epoch: 23, train loss: 0.24072, val loss: 0.26740\n",
      "Interaction tuning epoch: 24, train loss: 0.24447, val loss: 0.26977\n",
      "Interaction tuning epoch: 25, train loss: 0.24621, val loss: 0.27219\n",
      "Interaction tuning epoch: 26, train loss: 0.24174, val loss: 0.26337\n",
      "Interaction tuning epoch: 27, train loss: 0.24065, val loss: 0.26519\n",
      "Interaction tuning epoch: 28, train loss: 0.24256, val loss: 0.26935\n",
      "Interaction tuning epoch: 29, train loss: 0.24048, val loss: 0.26869\n",
      "Interaction tuning epoch: 30, train loss: 0.24175, val loss: 0.26438\n",
      "Interaction tuning epoch: 31, train loss: 0.24531, val loss: 0.27033\n",
      "Interaction tuning epoch: 32, train loss: 0.23846, val loss: 0.26289\n",
      "Interaction tuning epoch: 33, train loss: 0.23989, val loss: 0.26622\n",
      "Interaction tuning epoch: 34, train loss: 0.24204, val loss: 0.26753\n",
      "Interaction tuning epoch: 35, train loss: 0.23929, val loss: 0.26632\n",
      "Interaction tuning epoch: 36, train loss: 0.24283, val loss: 0.26579\n",
      "Interaction tuning epoch: 37, train loss: 0.24117, val loss: 0.26766\n",
      "Interaction tuning epoch: 38, train loss: 0.23925, val loss: 0.26036\n",
      "Interaction tuning epoch: 39, train loss: 0.24016, val loss: 0.26290\n",
      "Interaction tuning epoch: 40, train loss: 0.23725, val loss: 0.26480\n",
      "Interaction tuning epoch: 41, train loss: 0.24259, val loss: 0.26887\n",
      "Interaction tuning epoch: 42, train loss: 0.23928, val loss: 0.26395\n",
      "Interaction tuning epoch: 43, train loss: 0.24514, val loss: 0.27467\n",
      "Interaction tuning epoch: 44, train loss: 0.23958, val loss: 0.25790\n",
      "Interaction tuning epoch: 45, train loss: 0.24025, val loss: 0.26834\n",
      "Interaction tuning epoch: 46, train loss: 0.23749, val loss: 0.26097\n",
      "Interaction tuning epoch: 47, train loss: 0.23924, val loss: 0.26978\n",
      "Interaction tuning epoch: 48, train loss: 0.24403, val loss: 0.26482\n",
      "Interaction tuning epoch: 49, train loss: 0.24254, val loss: 0.26383\n",
      "Interaction tuning epoch: 50, train loss: 0.24286, val loss: 0.26921\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 46.961986780166626\n",
      "After the gam stage, training error is 0.24286 , validation error is 0.26920\n",
      "missing value counts: 99219\n",
      "#####start auto_tuning#####\n",
      "the best shrinkage is 0.250000\n",
      "[SoftImpute] Max Singular Value of X_init = 3.784096\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.222148 validation BCE=0.267035,rank=3\n",
      "[SoftImpute] Iter 1: observed BCE=0.236263 validation BCE=0.267628,rank=3\n",
      "[SoftImpute] Iter 2: observed BCE=0.237609 validation BCE=0.267945,rank=3\n",
      "[SoftImpute] Iter 3: observed BCE=0.237915 validation BCE=0.268056,rank=3\n",
      "[SoftImpute] Iter 4: observed BCE=0.238116 validation BCE=0.267985,rank=3\n",
      "[SoftImpute] Iter 5: observed BCE=0.238111 validation BCE=0.267951,rank=3\n",
      "[SoftImpute] Iter 6: observed BCE=0.238174 validation BCE=0.268003,rank=3\n",
      "[SoftImpute] Iter 7: observed BCE=0.238199 validation BCE=0.268006,rank=3\n",
      "[SoftImpute] Iter 8: observed BCE=0.238246 validation BCE=0.268049,rank=3\n",
      "[SoftImpute] Iter 9: observed BCE=0.238213 validation BCE=0.268018,rank=3\n",
      "[SoftImpute] Iter 10: observed BCE=0.238249 validation BCE=0.268015,rank=3\n",
      "[SoftImpute] Iter 11: observed BCE=0.238222 validation BCE=0.268018,rank=3\n",
      "[SoftImpute] Iter 12: observed BCE=0.238251 validation BCE=0.267976,rank=3\n",
      "[SoftImpute] Iter 13: observed BCE=0.238224 validation BCE=0.268020,rank=3\n",
      "[SoftImpute] Iter 14: observed BCE=0.238257 validation BCE=0.268018,rank=3\n",
      "[SoftImpute] Iter 15: observed BCE=0.238220 validation BCE=0.267967,rank=3\n",
      "[SoftImpute] Iter 16: observed BCE=0.238290 validation BCE=0.268061,rank=3\n",
      "[SoftImpute] Iter 17: observed BCE=0.238268 validation BCE=0.267997,rank=3\n",
      "[SoftImpute] Iter 18: observed BCE=0.238282 validation BCE=0.268035,rank=3\n",
      "[SoftImpute] Iter 19: observed BCE=0.238271 validation BCE=0.268028,rank=3\n",
      "[SoftImpute] Iter 20: observed BCE=0.238329 validation BCE=0.268013,rank=3\n",
      "[SoftImpute] Iter 21: observed BCE=0.238265 validation BCE=0.268007,rank=3\n",
      "[SoftImpute] Iter 22: observed BCE=0.238320 validation BCE=0.268015,rank=3\n",
      "[SoftImpute] Iter 23: observed BCE=0.238280 validation BCE=0.268031,rank=3\n",
      "[SoftImpute] Iter 24: observed BCE=0.238298 validation BCE=0.268039,rank=3\n",
      "[SoftImpute] Iter 25: observed BCE=0.238289 validation BCE=0.268007,rank=3\n",
      "[SoftImpute] Iter 26: observed BCE=0.238317 validation BCE=0.268037,rank=3\n",
      "[SoftImpute] Iter 27: observed BCE=0.238270 validation BCE=0.267971,rank=3\n",
      "[SoftImpute] Iter 28: observed BCE=0.238327 validation BCE=0.268061,rank=3\n",
      "[SoftImpute] Iter 29: observed BCE=0.238270 validation BCE=0.268024,rank=3\n",
      "[SoftImpute] Iter 30: observed BCE=0.238302 validation BCE=0.268001,rank=3\n",
      "[SoftImpute] Iter 31: observed BCE=0.238285 validation BCE=0.268041,rank=3\n",
      "[SoftImpute] Iter 32: observed BCE=0.238321 validation BCE=0.268003,rank=3\n",
      "[SoftImpute] Iter 33: observed BCE=0.238267 validation BCE=0.268004,rank=3\n",
      "[SoftImpute] Iter 34: observed BCE=0.238330 validation BCE=0.268065,rank=3\n",
      "[SoftImpute] Iter 35: observed BCE=0.238274 validation BCE=0.267989,rank=3\n",
      "[SoftImpute] Iter 36: observed BCE=0.238297 validation BCE=0.268035,rank=3\n",
      "[SoftImpute] Iter 37: observed BCE=0.238290 validation BCE=0.268006,rank=3\n",
      "[SoftImpute] Iter 38: observed BCE=0.238317 validation BCE=0.268035,rank=3\n",
      "[SoftImpute] Iter 39: observed BCE=0.238267 validation BCE=0.268009,rank=3\n",
      "[SoftImpute] Iter 40: observed BCE=0.238333 validation BCE=0.268030,rank=3\n",
      "[SoftImpute] Iter 41: observed BCE=0.238269 validation BCE=0.268021,rank=3\n",
      "[SoftImpute] Iter 42: observed BCE=0.238301 validation BCE=0.267999,rank=3\n",
      "[SoftImpute] Iter 43: observed BCE=0.238287 validation BCE=0.268040,rank=3\n",
      "[SoftImpute] Iter 44: observed BCE=0.238318 validation BCE=0.268040,rank=3\n",
      "[SoftImpute] Iter 45: observed BCE=0.238270 validation BCE=0.267972,rank=3\n",
      "[SoftImpute] Iter 46: observed BCE=0.238330 validation BCE=0.268062,rank=3\n",
      "[SoftImpute] Iter 47: observed BCE=0.238275 validation BCE=0.267987,rank=3\n",
      "[SoftImpute] Iter 48: observed BCE=0.238296 validation BCE=0.268033,rank=3\n",
      "[SoftImpute] Iter 49: observed BCE=0.238287 validation BCE=0.268043,rank=3\n",
      "[SoftImpute] Iter 50: observed BCE=0.238322 validation BCE=0.268004,rank=3\n",
      "[SoftImpute] Iter 51: observed BCE=0.238265 validation BCE=0.268006,rank=3\n",
      "[SoftImpute] Iter 52: observed BCE=0.238332 validation BCE=0.268028,rank=3\n",
      "[SoftImpute] Iter 53: observed BCE=0.238271 validation BCE=0.268020,rank=3\n",
      "[SoftImpute] Iter 54: observed BCE=0.238299 validation BCE=0.268037,rank=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 55: observed BCE=0.238290 validation BCE=0.268008,rank=3\n",
      "[SoftImpute] Iter 56: observed BCE=0.238317 validation BCE=0.268037,rank=3\n",
      "[SoftImpute] Iter 57: observed BCE=0.238271 validation BCE=0.267971,rank=3\n",
      "[SoftImpute] Iter 58: observed BCE=0.238330 validation BCE=0.268061,rank=3\n",
      "[SoftImpute] Iter 59: observed BCE=0.238270 validation BCE=0.268024,rank=3\n",
      "[SoftImpute] Iter 60: observed BCE=0.238301 validation BCE=0.268001,rank=3\n",
      "[SoftImpute] Iter 61: observed BCE=0.238286 validation BCE=0.268040,rank=3\n",
      "[SoftImpute] Iter 62: observed BCE=0.238322 validation BCE=0.268002,rank=3\n",
      "[SoftImpute] Iter 63: observed BCE=0.238266 validation BCE=0.268005,rank=3\n",
      "[SoftImpute] Iter 64: observed BCE=0.238329 validation BCE=0.268066,rank=3\n",
      "[SoftImpute] Iter 65: observed BCE=0.238275 validation BCE=0.267988,rank=3\n",
      "[SoftImpute] Iter 66: observed BCE=0.238297 validation BCE=0.268034,rank=3\n",
      "[SoftImpute] Iter 67: observed BCE=0.238290 validation BCE=0.268007,rank=3\n",
      "[SoftImpute] Iter 68: observed BCE=0.238318 validation BCE=0.268036,rank=3\n",
      "[SoftImpute] Iter 69: observed BCE=0.238266 validation BCE=0.268008,rank=3\n",
      "[SoftImpute] Iter 70: observed BCE=0.238334 validation BCE=0.268030,rank=3\n",
      "[SoftImpute] Iter 71: observed BCE=0.238270 validation BCE=0.268022,rank=3\n",
      "[SoftImpute] Iter 72: observed BCE=0.238301 validation BCE=0.268000,rank=3\n",
      "[SoftImpute] Iter 73: observed BCE=0.238285 validation BCE=0.268039,rank=3\n",
      "[SoftImpute] Iter 74: observed BCE=0.238319 validation BCE=0.268040,rank=3\n",
      "[SoftImpute] Iter 75: observed BCE=0.238271 validation BCE=0.267973,rank=3\n",
      "[SoftImpute] Iter 76: observed BCE=0.238329 validation BCE=0.268063,rank=3\n",
      "[SoftImpute] Iter 77: observed BCE=0.238273 validation BCE=0.267986,rank=3\n",
      "[SoftImpute] Iter 78: observed BCE=0.238298 validation BCE=0.268033,rank=3\n",
      "[SoftImpute] Iter 79: observed BCE=0.238287 validation BCE=0.268044,rank=3\n",
      "[SoftImpute] Iter 80: observed BCE=0.238321 validation BCE=0.268004,rank=3\n",
      "[SoftImpute] Iter 81: observed BCE=0.238265 validation BCE=0.268005,rank=3\n",
      "[SoftImpute] Iter 82: observed BCE=0.238333 validation BCE=0.268028,rank=3\n",
      "[SoftImpute] Iter 83: observed BCE=0.238271 validation BCE=0.268020,rank=3\n",
      "[SoftImpute] Iter 84: observed BCE=0.238298 validation BCE=0.268037,rank=3\n",
      "[SoftImpute] Iter 85: observed BCE=0.238290 validation BCE=0.268007,rank=3\n",
      "[SoftImpute] Iter 86: observed BCE=0.238318 validation BCE=0.268037,rank=3\n",
      "[SoftImpute] Iter 87: observed BCE=0.238271 validation BCE=0.267972,rank=3\n",
      "[SoftImpute] Iter 88: observed BCE=0.238329 validation BCE=0.268062,rank=3\n",
      "[SoftImpute] Iter 89: observed BCE=0.238270 validation BCE=0.268024,rank=3\n",
      "[SoftImpute] Iter 90: observed BCE=0.238302 validation BCE=0.268001,rank=3\n",
      "[SoftImpute] Iter 91: observed BCE=0.238286 validation BCE=0.268041,rank=3\n",
      "[SoftImpute] Iter 92: observed BCE=0.238321 validation BCE=0.268003,rank=3\n",
      "[SoftImpute] Iter 93: observed BCE=0.238266 validation BCE=0.268004,rank=3\n",
      "[SoftImpute] Iter 94: observed BCE=0.238330 validation BCE=0.268066,rank=3\n",
      "[SoftImpute] Iter 95: observed BCE=0.238275 validation BCE=0.267989,rank=3\n",
      "[SoftImpute] Iter 96: observed BCE=0.238296 validation BCE=0.268035,rank=3\n",
      "[SoftImpute] Iter 97: observed BCE=0.238290 validation BCE=0.268006,rank=3\n",
      "[SoftImpute] Iter 98: observed BCE=0.238319 validation BCE=0.268035,rank=3\n",
      "[SoftImpute] Iter 99: observed BCE=0.238267 validation BCE=0.268009,rank=3\n",
      "[SoftImpute] Iter 100: observed BCE=0.238333 validation BCE=0.268030,rank=3\n",
      "[SoftImpute] Stopped after iteration 100 for lambda=0.075682\n",
      "final num of user group: 9\n",
      "final num of item group: 20\n",
      "change mode state : True\n",
      "time cost: 36.72870421409607\n",
      "After the matrix factor stage, training error is 0.23833, validation error is 0.26803\n",
      "6\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68035, val loss: 0.68575\n",
      "Main effects training epoch: 2, train loss: 0.67613, val loss: 0.68261\n",
      "Main effects training epoch: 3, train loss: 0.67003, val loss: 0.67417\n",
      "Main effects training epoch: 4, train loss: 0.66284, val loss: 0.66598\n",
      "Main effects training epoch: 5, train loss: 0.64922, val loss: 0.65283\n",
      "Main effects training epoch: 6, train loss: 0.62580, val loss: 0.62907\n",
      "Main effects training epoch: 7, train loss: 0.59291, val loss: 0.59453\n",
      "Main effects training epoch: 8, train loss: 0.55448, val loss: 0.55672\n",
      "Main effects training epoch: 9, train loss: 0.53828, val loss: 0.53983\n",
      "Main effects training epoch: 10, train loss: 0.54261, val loss: 0.54586\n",
      "Main effects training epoch: 11, train loss: 0.53026, val loss: 0.52826\n",
      "Main effects training epoch: 12, train loss: 0.52833, val loss: 0.52367\n",
      "Main effects training epoch: 13, train loss: 0.52814, val loss: 0.52445\n",
      "Main effects training epoch: 14, train loss: 0.52753, val loss: 0.52251\n",
      "Main effects training epoch: 15, train loss: 0.52816, val loss: 0.52452\n",
      "Main effects training epoch: 16, train loss: 0.52765, val loss: 0.52251\n",
      "Main effects training epoch: 17, train loss: 0.52713, val loss: 0.52236\n",
      "Main effects training epoch: 18, train loss: 0.52757, val loss: 0.52234\n",
      "Main effects training epoch: 19, train loss: 0.52721, val loss: 0.52228\n",
      "Main effects training epoch: 20, train loss: 0.52791, val loss: 0.52246\n",
      "Main effects training epoch: 21, train loss: 0.52731, val loss: 0.52357\n",
      "Main effects training epoch: 22, train loss: 0.52749, val loss: 0.52206\n",
      "Main effects training epoch: 23, train loss: 0.52756, val loss: 0.52308\n",
      "Main effects training epoch: 24, train loss: 0.52782, val loss: 0.52173\n",
      "Main effects training epoch: 25, train loss: 0.52744, val loss: 0.52277\n",
      "Main effects training epoch: 26, train loss: 0.52754, val loss: 0.52276\n",
      "Main effects training epoch: 27, train loss: 0.52850, val loss: 0.52334\n",
      "Main effects training epoch: 28, train loss: 0.52712, val loss: 0.52116\n",
      "Main effects training epoch: 29, train loss: 0.52677, val loss: 0.52216\n",
      "Main effects training epoch: 30, train loss: 0.52738, val loss: 0.52290\n",
      "Main effects training epoch: 31, train loss: 0.52728, val loss: 0.52127\n",
      "Main effects training epoch: 32, train loss: 0.52766, val loss: 0.52350\n",
      "Main effects training epoch: 33, train loss: 0.52723, val loss: 0.52170\n",
      "Main effects training epoch: 34, train loss: 0.52746, val loss: 0.52193\n",
      "Main effects training epoch: 35, train loss: 0.52669, val loss: 0.52210\n",
      "Main effects training epoch: 36, train loss: 0.52676, val loss: 0.52190\n",
      "Main effects training epoch: 37, train loss: 0.52664, val loss: 0.52131\n",
      "Main effects training epoch: 38, train loss: 0.52668, val loss: 0.52245\n",
      "Main effects training epoch: 39, train loss: 0.52672, val loss: 0.52179\n",
      "Main effects training epoch: 40, train loss: 0.52645, val loss: 0.52153\n",
      "Main effects training epoch: 41, train loss: 0.52654, val loss: 0.52186\n",
      "Main effects training epoch: 42, train loss: 0.52638, val loss: 0.52120\n",
      "Main effects training epoch: 43, train loss: 0.52667, val loss: 0.52082\n",
      "Main effects training epoch: 44, train loss: 0.52632, val loss: 0.52078\n",
      "Main effects training epoch: 45, train loss: 0.52627, val loss: 0.52178\n",
      "Main effects training epoch: 46, train loss: 0.52624, val loss: 0.52135\n",
      "Main effects training epoch: 47, train loss: 0.52679, val loss: 0.52114\n",
      "Main effects training epoch: 48, train loss: 0.52733, val loss: 0.52313\n",
      "Main effects training epoch: 49, train loss: 0.52724, val loss: 0.52011\n",
      "Main effects training epoch: 50, train loss: 0.52657, val loss: 0.52274\n",
      "Main effects training epoch: 51, train loss: 0.52613, val loss: 0.52078\n",
      "Main effects training epoch: 52, train loss: 0.52617, val loss: 0.52101\n",
      "Main effects training epoch: 53, train loss: 0.52675, val loss: 0.52113\n",
      "Main effects training epoch: 54, train loss: 0.52672, val loss: 0.52309\n",
      "Main effects training epoch: 55, train loss: 0.52822, val loss: 0.52107\n",
      "Main effects training epoch: 56, train loss: 0.52839, val loss: 0.52481\n",
      "Main effects training epoch: 57, train loss: 0.52696, val loss: 0.52098\n",
      "Main effects training epoch: 58, train loss: 0.52703, val loss: 0.52263\n",
      "Main effects training epoch: 59, train loss: 0.52666, val loss: 0.52161\n",
      "Main effects training epoch: 60, train loss: 0.52669, val loss: 0.52223\n",
      "Main effects training epoch: 61, train loss: 0.52645, val loss: 0.52165\n",
      "Main effects training epoch: 62, train loss: 0.52634, val loss: 0.52195\n",
      "Main effects training epoch: 63, train loss: 0.52596, val loss: 0.52089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 64, train loss: 0.52594, val loss: 0.52109\n",
      "Main effects training epoch: 65, train loss: 0.52610, val loss: 0.52212\n",
      "Main effects training epoch: 66, train loss: 0.52633, val loss: 0.52117\n",
      "Main effects training epoch: 67, train loss: 0.52558, val loss: 0.52129\n",
      "Main effects training epoch: 68, train loss: 0.52583, val loss: 0.52065\n",
      "Main effects training epoch: 69, train loss: 0.52559, val loss: 0.52090\n",
      "Main effects training epoch: 70, train loss: 0.52564, val loss: 0.52151\n",
      "Main effects training epoch: 71, train loss: 0.52613, val loss: 0.52005\n",
      "Main effects training epoch: 72, train loss: 0.52688, val loss: 0.52402\n",
      "Main effects training epoch: 73, train loss: 0.52677, val loss: 0.52114\n",
      "Main effects training epoch: 74, train loss: 0.52629, val loss: 0.52245\n",
      "Main effects training epoch: 75, train loss: 0.52586, val loss: 0.52042\n",
      "Main effects training epoch: 76, train loss: 0.52553, val loss: 0.52096\n",
      "Main effects training epoch: 77, train loss: 0.52537, val loss: 0.52104\n",
      "Main effects training epoch: 78, train loss: 0.52571, val loss: 0.52072\n",
      "Main effects training epoch: 79, train loss: 0.52539, val loss: 0.52104\n",
      "Main effects training epoch: 80, train loss: 0.52541, val loss: 0.52043\n",
      "Main effects training epoch: 81, train loss: 0.52553, val loss: 0.52132\n",
      "Main effects training epoch: 82, train loss: 0.52535, val loss: 0.52220\n",
      "Main effects training epoch: 83, train loss: 0.52552, val loss: 0.52037\n",
      "Main effects training epoch: 84, train loss: 0.52575, val loss: 0.52234\n",
      "Main effects training epoch: 85, train loss: 0.52595, val loss: 0.52118\n",
      "Main effects training epoch: 86, train loss: 0.52668, val loss: 0.52193\n",
      "Main effects training epoch: 87, train loss: 0.52516, val loss: 0.52114\n",
      "Main effects training epoch: 88, train loss: 0.52522, val loss: 0.52135\n",
      "Main effects training epoch: 89, train loss: 0.52600, val loss: 0.52206\n",
      "Main effects training epoch: 90, train loss: 0.52538, val loss: 0.52067\n",
      "Main effects training epoch: 91, train loss: 0.52540, val loss: 0.52025\n",
      "Main effects training epoch: 92, train loss: 0.52537, val loss: 0.52110\n",
      "Main effects training epoch: 93, train loss: 0.52528, val loss: 0.52211\n",
      "Main effects training epoch: 94, train loss: 0.52577, val loss: 0.51997\n",
      "Main effects training epoch: 95, train loss: 0.52522, val loss: 0.52174\n",
      "Main effects training epoch: 96, train loss: 0.52489, val loss: 0.52087\n",
      "Main effects training epoch: 97, train loss: 0.52494, val loss: 0.52117\n",
      "Main effects training epoch: 98, train loss: 0.52501, val loss: 0.52061\n",
      "Main effects training epoch: 99, train loss: 0.52503, val loss: 0.52145\n",
      "Main effects training epoch: 100, train loss: 0.52586, val loss: 0.52185\n",
      "Main effects training epoch: 101, train loss: 0.52485, val loss: 0.52104\n",
      "Main effects training epoch: 102, train loss: 0.52482, val loss: 0.52118\n",
      "Main effects training epoch: 103, train loss: 0.52517, val loss: 0.52065\n",
      "Main effects training epoch: 104, train loss: 0.52528, val loss: 0.52023\n",
      "Main effects training epoch: 105, train loss: 0.52509, val loss: 0.52187\n",
      "Main effects training epoch: 106, train loss: 0.52488, val loss: 0.52040\n",
      "Main effects training epoch: 107, train loss: 0.52517, val loss: 0.52172\n",
      "Main effects training epoch: 108, train loss: 0.52525, val loss: 0.52063\n",
      "Main effects training epoch: 109, train loss: 0.52518, val loss: 0.52182\n",
      "Main effects training epoch: 110, train loss: 0.52479, val loss: 0.52068\n",
      "Main effects training epoch: 111, train loss: 0.52481, val loss: 0.52152\n",
      "Main effects training epoch: 112, train loss: 0.52501, val loss: 0.52069\n",
      "Main effects training epoch: 113, train loss: 0.52520, val loss: 0.52208\n",
      "Main effects training epoch: 114, train loss: 0.52480, val loss: 0.52105\n",
      "Main effects training epoch: 115, train loss: 0.52466, val loss: 0.52034\n",
      "Main effects training epoch: 116, train loss: 0.52494, val loss: 0.52049\n",
      "Main effects training epoch: 117, train loss: 0.52498, val loss: 0.52191\n",
      "Main effects training epoch: 118, train loss: 0.52576, val loss: 0.52200\n",
      "Main effects training epoch: 119, train loss: 0.52493, val loss: 0.52121\n",
      "Main effects training epoch: 120, train loss: 0.52521, val loss: 0.52118\n",
      "Main effects training epoch: 121, train loss: 0.52471, val loss: 0.52112\n",
      "Main effects training epoch: 122, train loss: 0.52423, val loss: 0.52021\n",
      "Main effects training epoch: 123, train loss: 0.52415, val loss: 0.52075\n",
      "Main effects training epoch: 124, train loss: 0.52433, val loss: 0.52144\n",
      "Main effects training epoch: 125, train loss: 0.52463, val loss: 0.51982\n",
      "Main effects training epoch: 126, train loss: 0.52458, val loss: 0.52201\n",
      "Main effects training epoch: 127, train loss: 0.52490, val loss: 0.52076\n",
      "Main effects training epoch: 128, train loss: 0.52466, val loss: 0.52034\n",
      "Main effects training epoch: 129, train loss: 0.52465, val loss: 0.52085\n",
      "Main effects training epoch: 130, train loss: 0.52427, val loss: 0.52137\n",
      "Main effects training epoch: 131, train loss: 0.52424, val loss: 0.51914\n",
      "Main effects training epoch: 132, train loss: 0.52400, val loss: 0.52076\n",
      "Main effects training epoch: 133, train loss: 0.52376, val loss: 0.52054\n",
      "Main effects training epoch: 134, train loss: 0.52359, val loss: 0.52002\n",
      "Main effects training epoch: 135, train loss: 0.52362, val loss: 0.51977\n",
      "Main effects training epoch: 136, train loss: 0.52375, val loss: 0.52029\n",
      "Main effects training epoch: 137, train loss: 0.52375, val loss: 0.52005\n",
      "Main effects training epoch: 138, train loss: 0.52407, val loss: 0.52003\n",
      "Main effects training epoch: 139, train loss: 0.52350, val loss: 0.51998\n",
      "Main effects training epoch: 140, train loss: 0.52343, val loss: 0.52067\n",
      "Main effects training epoch: 141, train loss: 0.52401, val loss: 0.52047\n",
      "Main effects training epoch: 142, train loss: 0.52311, val loss: 0.51885\n",
      "Main effects training epoch: 143, train loss: 0.52348, val loss: 0.52046\n",
      "Main effects training epoch: 144, train loss: 0.52306, val loss: 0.51964\n",
      "Main effects training epoch: 145, train loss: 0.52275, val loss: 0.51941\n",
      "Main effects training epoch: 146, train loss: 0.52335, val loss: 0.51917\n",
      "Main effects training epoch: 147, train loss: 0.52322, val loss: 0.51951\n",
      "Main effects training epoch: 148, train loss: 0.52286, val loss: 0.51864\n",
      "Main effects training epoch: 149, train loss: 0.52230, val loss: 0.51844\n",
      "Main effects training epoch: 150, train loss: 0.52216, val loss: 0.51878\n",
      "Main effects training epoch: 151, train loss: 0.52203, val loss: 0.51721\n",
      "Main effects training epoch: 152, train loss: 0.52206, val loss: 0.51791\n",
      "Main effects training epoch: 153, train loss: 0.52191, val loss: 0.51836\n",
      "Main effects training epoch: 154, train loss: 0.52208, val loss: 0.51707\n",
      "Main effects training epoch: 155, train loss: 0.52238, val loss: 0.51937\n",
      "Main effects training epoch: 156, train loss: 0.52210, val loss: 0.51724\n",
      "Main effects training epoch: 157, train loss: 0.52229, val loss: 0.51814\n",
      "Main effects training epoch: 158, train loss: 0.52158, val loss: 0.51822\n",
      "Main effects training epoch: 159, train loss: 0.52158, val loss: 0.51709\n",
      "Main effects training epoch: 160, train loss: 0.52156, val loss: 0.51735\n",
      "Main effects training epoch: 161, train loss: 0.52222, val loss: 0.51888\n",
      "Main effects training epoch: 162, train loss: 0.52160, val loss: 0.51720\n",
      "Main effects training epoch: 163, train loss: 0.52231, val loss: 0.51830\n",
      "Main effects training epoch: 164, train loss: 0.52170, val loss: 0.51743\n",
      "Main effects training epoch: 165, train loss: 0.52148, val loss: 0.51785\n",
      "Main effects training epoch: 166, train loss: 0.52162, val loss: 0.51690\n",
      "Main effects training epoch: 167, train loss: 0.52187, val loss: 0.51927\n",
      "Main effects training epoch: 168, train loss: 0.52141, val loss: 0.51736\n",
      "Main effects training epoch: 169, train loss: 0.52123, val loss: 0.51666\n",
      "Main effects training epoch: 170, train loss: 0.52172, val loss: 0.51743\n",
      "Main effects training epoch: 171, train loss: 0.52218, val loss: 0.51859\n",
      "Main effects training epoch: 172, train loss: 0.52188, val loss: 0.51706\n",
      "Main effects training epoch: 173, train loss: 0.52148, val loss: 0.51830\n",
      "Main effects training epoch: 174, train loss: 0.52156, val loss: 0.51693\n",
      "Main effects training epoch: 175, train loss: 0.52157, val loss: 0.51688\n",
      "Main effects training epoch: 176, train loss: 0.52188, val loss: 0.51787\n",
      "Main effects training epoch: 177, train loss: 0.52183, val loss: 0.51765\n",
      "Main effects training epoch: 178, train loss: 0.52104, val loss: 0.51677\n",
      "Main effects training epoch: 179, train loss: 0.52098, val loss: 0.51752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 180, train loss: 0.52129, val loss: 0.51632\n",
      "Main effects training epoch: 181, train loss: 0.52076, val loss: 0.51659\n",
      "Main effects training epoch: 182, train loss: 0.52104, val loss: 0.51676\n",
      "Main effects training epoch: 183, train loss: 0.52134, val loss: 0.51777\n",
      "Main effects training epoch: 184, train loss: 0.52107, val loss: 0.51632\n",
      "Main effects training epoch: 185, train loss: 0.52065, val loss: 0.51616\n",
      "Main effects training epoch: 186, train loss: 0.52092, val loss: 0.51764\n",
      "Main effects training epoch: 187, train loss: 0.52172, val loss: 0.51625\n",
      "Main effects training epoch: 188, train loss: 0.52112, val loss: 0.51747\n",
      "Main effects training epoch: 189, train loss: 0.52085, val loss: 0.51631\n",
      "Main effects training epoch: 190, train loss: 0.52062, val loss: 0.51706\n",
      "Main effects training epoch: 191, train loss: 0.52090, val loss: 0.51614\n",
      "Main effects training epoch: 192, train loss: 0.52145, val loss: 0.51796\n",
      "Main effects training epoch: 193, train loss: 0.52090, val loss: 0.51693\n",
      "Main effects training epoch: 194, train loss: 0.52038, val loss: 0.51662\n",
      "Main effects training epoch: 195, train loss: 0.52084, val loss: 0.51587\n",
      "Main effects training epoch: 196, train loss: 0.52055, val loss: 0.51666\n",
      "Main effects training epoch: 197, train loss: 0.52047, val loss: 0.51667\n",
      "Main effects training epoch: 198, train loss: 0.52050, val loss: 0.51644\n",
      "Main effects training epoch: 199, train loss: 0.52053, val loss: 0.51689\n",
      "Main effects training epoch: 200, train loss: 0.52070, val loss: 0.51709\n",
      "Main effects training epoch: 201, train loss: 0.52081, val loss: 0.51625\n",
      "Main effects training epoch: 202, train loss: 0.52024, val loss: 0.51621\n",
      "Main effects training epoch: 203, train loss: 0.52051, val loss: 0.51719\n",
      "Main effects training epoch: 204, train loss: 0.52054, val loss: 0.51699\n",
      "Main effects training epoch: 205, train loss: 0.52103, val loss: 0.51719\n",
      "Main effects training epoch: 206, train loss: 0.52138, val loss: 0.51731\n",
      "Main effects training epoch: 207, train loss: 0.52033, val loss: 0.51500\n",
      "Main effects training epoch: 208, train loss: 0.52107, val loss: 0.51780\n",
      "Main effects training epoch: 209, train loss: 0.52059, val loss: 0.51596\n",
      "Main effects training epoch: 210, train loss: 0.52039, val loss: 0.51853\n",
      "Main effects training epoch: 211, train loss: 0.52042, val loss: 0.51530\n",
      "Main effects training epoch: 212, train loss: 0.52007, val loss: 0.51641\n",
      "Main effects training epoch: 213, train loss: 0.52002, val loss: 0.51606\n",
      "Main effects training epoch: 214, train loss: 0.52000, val loss: 0.51661\n",
      "Main effects training epoch: 215, train loss: 0.51975, val loss: 0.51517\n",
      "Main effects training epoch: 216, train loss: 0.51974, val loss: 0.51603\n",
      "Main effects training epoch: 217, train loss: 0.51970, val loss: 0.51610\n",
      "Main effects training epoch: 218, train loss: 0.51963, val loss: 0.51537\n",
      "Main effects training epoch: 219, train loss: 0.51969, val loss: 0.51580\n",
      "Main effects training epoch: 220, train loss: 0.52002, val loss: 0.51697\n",
      "Main effects training epoch: 221, train loss: 0.52001, val loss: 0.51495\n",
      "Main effects training epoch: 222, train loss: 0.52026, val loss: 0.51631\n",
      "Main effects training epoch: 223, train loss: 0.51968, val loss: 0.51573\n",
      "Main effects training epoch: 224, train loss: 0.52021, val loss: 0.51752\n",
      "Main effects training epoch: 225, train loss: 0.52017, val loss: 0.51562\n",
      "Main effects training epoch: 226, train loss: 0.52035, val loss: 0.51814\n",
      "Main effects training epoch: 227, train loss: 0.52015, val loss: 0.51458\n",
      "Main effects training epoch: 228, train loss: 0.52038, val loss: 0.51696\n",
      "Main effects training epoch: 229, train loss: 0.51975, val loss: 0.51654\n",
      "Main effects training epoch: 230, train loss: 0.52032, val loss: 0.51624\n",
      "Main effects training epoch: 231, train loss: 0.52021, val loss: 0.51622\n",
      "Main effects training epoch: 232, train loss: 0.52005, val loss: 0.51569\n",
      "Main effects training epoch: 233, train loss: 0.51971, val loss: 0.51536\n",
      "Main effects training epoch: 234, train loss: 0.51963, val loss: 0.51630\n",
      "Main effects training epoch: 235, train loss: 0.51958, val loss: 0.51502\n",
      "Main effects training epoch: 236, train loss: 0.51998, val loss: 0.51689\n",
      "Main effects training epoch: 237, train loss: 0.51946, val loss: 0.51635\n",
      "Main effects training epoch: 238, train loss: 0.51972, val loss: 0.51590\n",
      "Main effects training epoch: 239, train loss: 0.51972, val loss: 0.51487\n",
      "Main effects training epoch: 240, train loss: 0.52045, val loss: 0.51742\n",
      "Main effects training epoch: 241, train loss: 0.52001, val loss: 0.51588\n",
      "Main effects training epoch: 242, train loss: 0.52028, val loss: 0.51793\n",
      "Main effects training epoch: 243, train loss: 0.51960, val loss: 0.51560\n",
      "Main effects training epoch: 244, train loss: 0.52032, val loss: 0.51573\n",
      "Main effects training epoch: 245, train loss: 0.52004, val loss: 0.51833\n",
      "Main effects training epoch: 246, train loss: 0.51997, val loss: 0.51578\n",
      "Main effects training epoch: 247, train loss: 0.51973, val loss: 0.51546\n",
      "Main effects training epoch: 248, train loss: 0.51981, val loss: 0.51847\n",
      "Main effects training epoch: 249, train loss: 0.52000, val loss: 0.51570\n",
      "Main effects training epoch: 250, train loss: 0.52011, val loss: 0.51738\n",
      "Main effects training epoch: 251, train loss: 0.52000, val loss: 0.51525\n",
      "Main effects training epoch: 252, train loss: 0.51973, val loss: 0.51726\n",
      "Main effects training epoch: 253, train loss: 0.51925, val loss: 0.51524\n",
      "Main effects training epoch: 254, train loss: 0.51909, val loss: 0.51573\n",
      "Main effects training epoch: 255, train loss: 0.51930, val loss: 0.51694\n",
      "Main effects training epoch: 256, train loss: 0.51918, val loss: 0.51539\n",
      "Main effects training epoch: 257, train loss: 0.51904, val loss: 0.51627\n",
      "Main effects training epoch: 258, train loss: 0.51918, val loss: 0.51621\n",
      "Main effects training epoch: 259, train loss: 0.51923, val loss: 0.51450\n",
      "Main effects training epoch: 260, train loss: 0.51896, val loss: 0.51580\n",
      "Main effects training epoch: 261, train loss: 0.51919, val loss: 0.51543\n",
      "Main effects training epoch: 262, train loss: 0.51902, val loss: 0.51717\n",
      "Main effects training epoch: 263, train loss: 0.51905, val loss: 0.51548\n",
      "Main effects training epoch: 264, train loss: 0.51903, val loss: 0.51598\n",
      "Main effects training epoch: 265, train loss: 0.51952, val loss: 0.51612\n",
      "Main effects training epoch: 266, train loss: 0.51900, val loss: 0.51765\n",
      "Main effects training epoch: 267, train loss: 0.51880, val loss: 0.51521\n",
      "Main effects training epoch: 268, train loss: 0.51888, val loss: 0.51689\n",
      "Main effects training epoch: 269, train loss: 0.51875, val loss: 0.51586\n",
      "Main effects training epoch: 270, train loss: 0.51852, val loss: 0.51576\n",
      "Main effects training epoch: 271, train loss: 0.51865, val loss: 0.51614\n",
      "Main effects training epoch: 272, train loss: 0.51893, val loss: 0.51505\n",
      "Main effects training epoch: 273, train loss: 0.51954, val loss: 0.51813\n",
      "Main effects training epoch: 274, train loss: 0.52099, val loss: 0.51741\n",
      "Main effects training epoch: 275, train loss: 0.51900, val loss: 0.51778\n",
      "Main effects training epoch: 276, train loss: 0.51866, val loss: 0.51703\n",
      "Main effects training epoch: 277, train loss: 0.51872, val loss: 0.51624\n",
      "Main effects training epoch: 278, train loss: 0.51869, val loss: 0.51570\n",
      "Main effects training epoch: 279, train loss: 0.51848, val loss: 0.51686\n",
      "Main effects training epoch: 280, train loss: 0.51867, val loss: 0.51523\n",
      "Main effects training epoch: 281, train loss: 0.51850, val loss: 0.51519\n",
      "Main effects training epoch: 282, train loss: 0.51842, val loss: 0.51654\n",
      "Main effects training epoch: 283, train loss: 0.51879, val loss: 0.51658\n",
      "Main effects training epoch: 284, train loss: 0.51889, val loss: 0.51631\n",
      "Main effects training epoch: 285, train loss: 0.51863, val loss: 0.51738\n",
      "Main effects training epoch: 286, train loss: 0.51929, val loss: 0.51638\n",
      "Main effects training epoch: 287, train loss: 0.51902, val loss: 0.51828\n",
      "Main effects training epoch: 288, train loss: 0.51840, val loss: 0.51605\n",
      "Main effects training epoch: 289, train loss: 0.51824, val loss: 0.51617\n",
      "Main effects training epoch: 290, train loss: 0.51810, val loss: 0.51623\n",
      "Main effects training epoch: 291, train loss: 0.51820, val loss: 0.51662\n",
      "Main effects training epoch: 292, train loss: 0.51823, val loss: 0.51590\n",
      "Main effects training epoch: 293, train loss: 0.51826, val loss: 0.51638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 294, train loss: 0.51830, val loss: 0.51594\n",
      "Main effects training epoch: 295, train loss: 0.51812, val loss: 0.51672\n",
      "Main effects training epoch: 296, train loss: 0.51844, val loss: 0.51559\n",
      "Main effects training epoch: 297, train loss: 0.51880, val loss: 0.51778\n",
      "Main effects training epoch: 298, train loss: 0.51815, val loss: 0.51577\n",
      "Main effects training epoch: 299, train loss: 0.51840, val loss: 0.51673\n",
      "Main effects training epoch: 300, train loss: 0.51804, val loss: 0.51744\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.52332, val loss: 0.51537\n",
      "Main effects tuning epoch: 2, train loss: 0.52311, val loss: 0.51597\n",
      "Main effects tuning epoch: 3, train loss: 0.52313, val loss: 0.51703\n",
      "Main effects tuning epoch: 4, train loss: 0.52311, val loss: 0.51600\n",
      "Main effects tuning epoch: 5, train loss: 0.52311, val loss: 0.51672\n",
      "Main effects tuning epoch: 6, train loss: 0.52329, val loss: 0.51745\n",
      "Main effects tuning epoch: 7, train loss: 0.52314, val loss: 0.51576\n",
      "Main effects tuning epoch: 8, train loss: 0.52309, val loss: 0.51723\n",
      "Main effects tuning epoch: 9, train loss: 0.52301, val loss: 0.51556\n",
      "Main effects tuning epoch: 10, train loss: 0.52301, val loss: 0.51551\n",
      "Main effects tuning epoch: 11, train loss: 0.52293, val loss: 0.51651\n",
      "Main effects tuning epoch: 12, train loss: 0.52296, val loss: 0.51506\n",
      "Main effects tuning epoch: 13, train loss: 0.52277, val loss: 0.51638\n",
      "Main effects tuning epoch: 14, train loss: 0.52305, val loss: 0.51687\n",
      "Main effects tuning epoch: 15, train loss: 0.52323, val loss: 0.51482\n",
      "Main effects tuning epoch: 16, train loss: 0.52335, val loss: 0.51828\n",
      "Main effects tuning epoch: 17, train loss: 0.52322, val loss: 0.51590\n",
      "Main effects tuning epoch: 18, train loss: 0.52310, val loss: 0.51632\n",
      "Main effects tuning epoch: 19, train loss: 0.52265, val loss: 0.51654\n",
      "Main effects tuning epoch: 20, train loss: 0.52288, val loss: 0.51650\n",
      "Main effects tuning epoch: 21, train loss: 0.52279, val loss: 0.51594\n",
      "Main effects tuning epoch: 22, train loss: 0.52260, val loss: 0.51567\n",
      "Main effects tuning epoch: 23, train loss: 0.52267, val loss: 0.51632\n",
      "Main effects tuning epoch: 24, train loss: 0.52260, val loss: 0.51599\n",
      "Main effects tuning epoch: 25, train loss: 0.52277, val loss: 0.51581\n",
      "Main effects tuning epoch: 26, train loss: 0.52287, val loss: 0.51747\n",
      "Main effects tuning epoch: 27, train loss: 0.52309, val loss: 0.51537\n",
      "Main effects tuning epoch: 28, train loss: 0.52310, val loss: 0.51759\n",
      "Main effects tuning epoch: 29, train loss: 0.52292, val loss: 0.51510\n",
      "Main effects tuning epoch: 30, train loss: 0.52284, val loss: 0.51713\n",
      "Main effects tuning epoch: 31, train loss: 0.52275, val loss: 0.51493\n",
      "Main effects tuning epoch: 32, train loss: 0.52256, val loss: 0.51703\n",
      "Main effects tuning epoch: 33, train loss: 0.52260, val loss: 0.51486\n",
      "Main effects tuning epoch: 34, train loss: 0.52261, val loss: 0.51694\n",
      "Main effects tuning epoch: 35, train loss: 0.52260, val loss: 0.51475\n",
      "Main effects tuning epoch: 36, train loss: 0.52273, val loss: 0.51739\n",
      "Main effects tuning epoch: 37, train loss: 0.52257, val loss: 0.51476\n",
      "Main effects tuning epoch: 38, train loss: 0.52243, val loss: 0.51720\n",
      "Main effects tuning epoch: 39, train loss: 0.52257, val loss: 0.51544\n",
      "Main effects tuning epoch: 40, train loss: 0.52274, val loss: 0.51684\n",
      "Main effects tuning epoch: 41, train loss: 0.52200, val loss: 0.51531\n",
      "Main effects tuning epoch: 42, train loss: 0.52195, val loss: 0.51571\n",
      "Main effects tuning epoch: 43, train loss: 0.52188, val loss: 0.51567\n",
      "Main effects tuning epoch: 44, train loss: 0.52193, val loss: 0.51486\n",
      "Main effects tuning epoch: 45, train loss: 0.52190, val loss: 0.51592\n",
      "Main effects tuning epoch: 46, train loss: 0.52200, val loss: 0.51530\n",
      "Main effects tuning epoch: 47, train loss: 0.52190, val loss: 0.51450\n",
      "Main effects tuning epoch: 48, train loss: 0.52190, val loss: 0.51607\n",
      "Main effects tuning epoch: 49, train loss: 0.52188, val loss: 0.51561\n",
      "Main effects tuning epoch: 50, train loss: 0.52183, val loss: 0.51532\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.48800, val loss: 0.48467\n",
      "Interaction training epoch: 2, train loss: 0.47012, val loss: 0.41470\n",
      "Interaction training epoch: 3, train loss: 0.31597, val loss: 0.28573\n",
      "Interaction training epoch: 4, train loss: 0.31991, val loss: 0.28615\n",
      "Interaction training epoch: 5, train loss: 0.31414, val loss: 0.27985\n",
      "Interaction training epoch: 6, train loss: 0.31054, val loss: 0.27603\n",
      "Interaction training epoch: 7, train loss: 0.30381, val loss: 0.27431\n",
      "Interaction training epoch: 8, train loss: 0.29470, val loss: 0.26068\n",
      "Interaction training epoch: 9, train loss: 0.29337, val loss: 0.26137\n",
      "Interaction training epoch: 10, train loss: 0.29371, val loss: 0.26780\n",
      "Interaction training epoch: 11, train loss: 0.30225, val loss: 0.27350\n",
      "Interaction training epoch: 12, train loss: 0.29267, val loss: 0.26107\n",
      "Interaction training epoch: 13, train loss: 0.29476, val loss: 0.26772\n",
      "Interaction training epoch: 14, train loss: 0.29047, val loss: 0.25832\n",
      "Interaction training epoch: 15, train loss: 0.29629, val loss: 0.26392\n",
      "Interaction training epoch: 16, train loss: 0.29176, val loss: 0.26395\n",
      "Interaction training epoch: 17, train loss: 0.29687, val loss: 0.26901\n",
      "Interaction training epoch: 18, train loss: 0.30102, val loss: 0.26405\n",
      "Interaction training epoch: 19, train loss: 0.28932, val loss: 0.25679\n",
      "Interaction training epoch: 20, train loss: 0.30043, val loss: 0.27435\n",
      "Interaction training epoch: 21, train loss: 0.29863, val loss: 0.26749\n",
      "Interaction training epoch: 22, train loss: 0.28768, val loss: 0.25408\n",
      "Interaction training epoch: 23, train loss: 0.29103, val loss: 0.26159\n",
      "Interaction training epoch: 24, train loss: 0.28905, val loss: 0.25422\n",
      "Interaction training epoch: 25, train loss: 0.28717, val loss: 0.25730\n",
      "Interaction training epoch: 26, train loss: 0.28838, val loss: 0.25611\n",
      "Interaction training epoch: 27, train loss: 0.28997, val loss: 0.25855\n",
      "Interaction training epoch: 28, train loss: 0.28840, val loss: 0.25606\n",
      "Interaction training epoch: 29, train loss: 0.28541, val loss: 0.25422\n",
      "Interaction training epoch: 30, train loss: 0.28621, val loss: 0.25659\n",
      "Interaction training epoch: 31, train loss: 0.28889, val loss: 0.25804\n",
      "Interaction training epoch: 32, train loss: 0.28560, val loss: 0.25494\n",
      "Interaction training epoch: 33, train loss: 0.28743, val loss: 0.25603\n",
      "Interaction training epoch: 34, train loss: 0.29092, val loss: 0.25956\n",
      "Interaction training epoch: 35, train loss: 0.28755, val loss: 0.25893\n",
      "Interaction training epoch: 36, train loss: 0.28255, val loss: 0.25239\n",
      "Interaction training epoch: 37, train loss: 0.28876, val loss: 0.26136\n",
      "Interaction training epoch: 38, train loss: 0.29380, val loss: 0.25764\n",
      "Interaction training epoch: 39, train loss: 0.28261, val loss: 0.25445\n",
      "Interaction training epoch: 40, train loss: 0.28382, val loss: 0.25367\n",
      "Interaction training epoch: 41, train loss: 0.28487, val loss: 0.25522\n",
      "Interaction training epoch: 42, train loss: 0.28628, val loss: 0.25716\n",
      "Interaction training epoch: 43, train loss: 0.28564, val loss: 0.25435\n",
      "Interaction training epoch: 44, train loss: 0.28526, val loss: 0.25706\n",
      "Interaction training epoch: 45, train loss: 0.28329, val loss: 0.25619\n",
      "Interaction training epoch: 46, train loss: 0.28150, val loss: 0.25317\n",
      "Interaction training epoch: 47, train loss: 0.28322, val loss: 0.25632\n",
      "Interaction training epoch: 48, train loss: 0.28305, val loss: 0.25358\n",
      "Interaction training epoch: 49, train loss: 0.28030, val loss: 0.25526\n",
      "Interaction training epoch: 50, train loss: 0.28975, val loss: 0.26273\n",
      "Interaction training epoch: 51, train loss: 0.28562, val loss: 0.25202\n",
      "Interaction training epoch: 52, train loss: 0.27923, val loss: 0.25234\n",
      "Interaction training epoch: 53, train loss: 0.28064, val loss: 0.25526\n",
      "Interaction training epoch: 54, train loss: 0.27934, val loss: 0.25063\n",
      "Interaction training epoch: 55, train loss: 0.28130, val loss: 0.25606\n",
      "Interaction training epoch: 56, train loss: 0.27965, val loss: 0.24726\n",
      "Interaction training epoch: 57, train loss: 0.28036, val loss: 0.25731\n",
      "Interaction training epoch: 58, train loss: 0.27877, val loss: 0.24984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 59, train loss: 0.27786, val loss: 0.25356\n",
      "Interaction training epoch: 60, train loss: 0.27729, val loss: 0.25240\n",
      "Interaction training epoch: 61, train loss: 0.27908, val loss: 0.25224\n",
      "Interaction training epoch: 62, train loss: 0.28095, val loss: 0.25629\n",
      "Interaction training epoch: 63, train loss: 0.27796, val loss: 0.25040\n",
      "Interaction training epoch: 64, train loss: 0.27689, val loss: 0.25335\n",
      "Interaction training epoch: 65, train loss: 0.27988, val loss: 0.25399\n",
      "Interaction training epoch: 66, train loss: 0.27568, val loss: 0.24978\n",
      "Interaction training epoch: 67, train loss: 0.27809, val loss: 0.25968\n",
      "Interaction training epoch: 68, train loss: 0.27789, val loss: 0.25290\n",
      "Interaction training epoch: 69, train loss: 0.28350, val loss: 0.26254\n",
      "Interaction training epoch: 70, train loss: 0.27668, val loss: 0.25253\n",
      "Interaction training epoch: 71, train loss: 0.27860, val loss: 0.25253\n",
      "Interaction training epoch: 72, train loss: 0.27257, val loss: 0.25172\n",
      "Interaction training epoch: 73, train loss: 0.27579, val loss: 0.25231\n",
      "Interaction training epoch: 74, train loss: 0.27348, val loss: 0.25295\n",
      "Interaction training epoch: 75, train loss: 0.27490, val loss: 0.25226\n",
      "Interaction training epoch: 76, train loss: 0.27452, val loss: 0.25408\n",
      "Interaction training epoch: 77, train loss: 0.27384, val loss: 0.25126\n",
      "Interaction training epoch: 78, train loss: 0.27166, val loss: 0.25122\n",
      "Interaction training epoch: 79, train loss: 0.27137, val loss: 0.25138\n",
      "Interaction training epoch: 80, train loss: 0.27109, val loss: 0.24974\n",
      "Interaction training epoch: 81, train loss: 0.27288, val loss: 0.25548\n",
      "Interaction training epoch: 82, train loss: 0.27389, val loss: 0.25448\n",
      "Interaction training epoch: 83, train loss: 0.27231, val loss: 0.25326\n",
      "Interaction training epoch: 84, train loss: 0.27290, val loss: 0.25361\n",
      "Interaction training epoch: 85, train loss: 0.27461, val loss: 0.25570\n",
      "Interaction training epoch: 86, train loss: 0.27219, val loss: 0.25338\n",
      "Interaction training epoch: 87, train loss: 0.27161, val loss: 0.25289\n",
      "Interaction training epoch: 88, train loss: 0.26990, val loss: 0.25427\n",
      "Interaction training epoch: 89, train loss: 0.27065, val loss: 0.25206\n",
      "Interaction training epoch: 90, train loss: 0.26857, val loss: 0.25272\n",
      "Interaction training epoch: 91, train loss: 0.27056, val loss: 0.25429\n",
      "Interaction training epoch: 92, train loss: 0.27159, val loss: 0.25216\n",
      "Interaction training epoch: 93, train loss: 0.27320, val loss: 0.25625\n",
      "Interaction training epoch: 94, train loss: 0.26974, val loss: 0.25020\n",
      "Interaction training epoch: 95, train loss: 0.26823, val loss: 0.24982\n",
      "Interaction training epoch: 96, train loss: 0.26829, val loss: 0.25491\n",
      "Interaction training epoch: 97, train loss: 0.26758, val loss: 0.24676\n",
      "Interaction training epoch: 98, train loss: 0.26757, val loss: 0.25029\n",
      "Interaction training epoch: 99, train loss: 0.26576, val loss: 0.25180\n",
      "Interaction training epoch: 100, train loss: 0.26531, val loss: 0.24655\n",
      "Interaction training epoch: 101, train loss: 0.26566, val loss: 0.25213\n",
      "Interaction training epoch: 102, train loss: 0.26591, val loss: 0.24871\n",
      "Interaction training epoch: 103, train loss: 0.26625, val loss: 0.25396\n",
      "Interaction training epoch: 104, train loss: 0.26487, val loss: 0.24978\n",
      "Interaction training epoch: 105, train loss: 0.26589, val loss: 0.25564\n",
      "Interaction training epoch: 106, train loss: 0.27000, val loss: 0.25019\n",
      "Interaction training epoch: 107, train loss: 0.26649, val loss: 0.25173\n",
      "Interaction training epoch: 108, train loss: 0.26701, val loss: 0.25531\n",
      "Interaction training epoch: 109, train loss: 0.26602, val loss: 0.25168\n",
      "Interaction training epoch: 110, train loss: 0.26247, val loss: 0.24641\n",
      "Interaction training epoch: 111, train loss: 0.26418, val loss: 0.25186\n",
      "Interaction training epoch: 112, train loss: 0.26640, val loss: 0.25352\n",
      "Interaction training epoch: 113, train loss: 0.26258, val loss: 0.24908\n",
      "Interaction training epoch: 114, train loss: 0.26503, val loss: 0.24866\n",
      "Interaction training epoch: 115, train loss: 0.26772, val loss: 0.25577\n",
      "Interaction training epoch: 116, train loss: 0.26340, val loss: 0.25296\n",
      "Interaction training epoch: 117, train loss: 0.26275, val loss: 0.24603\n",
      "Interaction training epoch: 118, train loss: 0.26360, val loss: 0.24780\n",
      "Interaction training epoch: 119, train loss: 0.26400, val loss: 0.25103\n",
      "Interaction training epoch: 120, train loss: 0.26054, val loss: 0.24890\n",
      "Interaction training epoch: 121, train loss: 0.26255, val loss: 0.25083\n",
      "Interaction training epoch: 122, train loss: 0.26049, val loss: 0.24709\n",
      "Interaction training epoch: 123, train loss: 0.26349, val loss: 0.25210\n",
      "Interaction training epoch: 124, train loss: 0.26353, val loss: 0.25374\n",
      "Interaction training epoch: 125, train loss: 0.26045, val loss: 0.24434\n",
      "Interaction training epoch: 126, train loss: 0.26051, val loss: 0.25016\n",
      "Interaction training epoch: 127, train loss: 0.26180, val loss: 0.25169\n",
      "Interaction training epoch: 128, train loss: 0.26096, val loss: 0.24468\n",
      "Interaction training epoch: 129, train loss: 0.25803, val loss: 0.24292\n",
      "Interaction training epoch: 130, train loss: 0.26116, val loss: 0.25165\n",
      "Interaction training epoch: 131, train loss: 0.26565, val loss: 0.25136\n",
      "Interaction training epoch: 132, train loss: 0.25991, val loss: 0.24664\n",
      "Interaction training epoch: 133, train loss: 0.26052, val loss: 0.24558\n",
      "Interaction training epoch: 134, train loss: 0.26343, val loss: 0.25736\n",
      "Interaction training epoch: 135, train loss: 0.26037, val loss: 0.24582\n",
      "Interaction training epoch: 136, train loss: 0.25924, val loss: 0.25148\n",
      "Interaction training epoch: 137, train loss: 0.26167, val loss: 0.25277\n",
      "Interaction training epoch: 138, train loss: 0.25662, val loss: 0.24715\n",
      "Interaction training epoch: 139, train loss: 0.25750, val loss: 0.24839\n",
      "Interaction training epoch: 140, train loss: 0.25748, val loss: 0.24408\n",
      "Interaction training epoch: 141, train loss: 0.25787, val loss: 0.25121\n",
      "Interaction training epoch: 142, train loss: 0.25954, val loss: 0.24750\n",
      "Interaction training epoch: 143, train loss: 0.25986, val loss: 0.24598\n",
      "Interaction training epoch: 144, train loss: 0.25754, val loss: 0.24693\n",
      "Interaction training epoch: 145, train loss: 0.25687, val loss: 0.24828\n",
      "Interaction training epoch: 146, train loss: 0.25789, val loss: 0.24607\n",
      "Interaction training epoch: 147, train loss: 0.25697, val loss: 0.25096\n",
      "Interaction training epoch: 148, train loss: 0.25999, val loss: 0.25070\n",
      "Interaction training epoch: 149, train loss: 0.25632, val loss: 0.24713\n",
      "Interaction training epoch: 150, train loss: 0.26130, val loss: 0.25044\n",
      "Interaction training epoch: 151, train loss: 0.25198, val loss: 0.24162\n",
      "Interaction training epoch: 152, train loss: 0.25863, val loss: 0.25128\n",
      "Interaction training epoch: 153, train loss: 0.25492, val loss: 0.24265\n",
      "Interaction training epoch: 154, train loss: 0.25364, val loss: 0.24920\n",
      "Interaction training epoch: 155, train loss: 0.25622, val loss: 0.24658\n",
      "Interaction training epoch: 156, train loss: 0.25604, val loss: 0.24671\n",
      "Interaction training epoch: 157, train loss: 0.25957, val loss: 0.25750\n",
      "Interaction training epoch: 158, train loss: 0.25671, val loss: 0.24347\n",
      "Interaction training epoch: 159, train loss: 0.25233, val loss: 0.24469\n",
      "Interaction training epoch: 160, train loss: 0.25295, val loss: 0.24510\n",
      "Interaction training epoch: 161, train loss: 0.25503, val loss: 0.25482\n",
      "Interaction training epoch: 162, train loss: 0.25282, val loss: 0.24582\n",
      "Interaction training epoch: 163, train loss: 0.25369, val loss: 0.24806\n",
      "Interaction training epoch: 164, train loss: 0.25536, val loss: 0.24783\n",
      "Interaction training epoch: 165, train loss: 0.25151, val loss: 0.24433\n",
      "Interaction training epoch: 166, train loss: 0.25118, val loss: 0.24685\n",
      "Interaction training epoch: 167, train loss: 0.25273, val loss: 0.24897\n",
      "Interaction training epoch: 168, train loss: 0.25395, val loss: 0.24494\n",
      "Interaction training epoch: 169, train loss: 0.25063, val loss: 0.24665\n",
      "Interaction training epoch: 170, train loss: 0.25306, val loss: 0.25023\n",
      "Interaction training epoch: 171, train loss: 0.25021, val loss: 0.24380\n",
      "Interaction training epoch: 172, train loss: 0.25617, val loss: 0.25236\n",
      "Interaction training epoch: 173, train loss: 0.25170, val loss: 0.24595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 174, train loss: 0.25336, val loss: 0.25031\n",
      "Interaction training epoch: 175, train loss: 0.25159, val loss: 0.24679\n",
      "Interaction training epoch: 176, train loss: 0.25210, val loss: 0.25023\n",
      "Interaction training epoch: 177, train loss: 0.25391, val loss: 0.25305\n",
      "Interaction training epoch: 178, train loss: 0.25333, val loss: 0.24613\n",
      "Interaction training epoch: 179, train loss: 0.25436, val loss: 0.24788\n",
      "Interaction training epoch: 180, train loss: 0.25628, val loss: 0.25779\n",
      "Interaction training epoch: 181, train loss: 0.25415, val loss: 0.24726\n",
      "Interaction training epoch: 182, train loss: 0.25540, val loss: 0.25193\n",
      "Interaction training epoch: 183, train loss: 0.24959, val loss: 0.24719\n",
      "Interaction training epoch: 184, train loss: 0.24926, val loss: 0.24635\n",
      "Interaction training epoch: 185, train loss: 0.25172, val loss: 0.25095\n",
      "Interaction training epoch: 186, train loss: 0.24985, val loss: 0.24653\n",
      "Interaction training epoch: 187, train loss: 0.25158, val loss: 0.24454\n",
      "Interaction training epoch: 188, train loss: 0.24636, val loss: 0.24360\n",
      "Interaction training epoch: 189, train loss: 0.24924, val loss: 0.24817\n",
      "Interaction training epoch: 190, train loss: 0.25266, val loss: 0.25238\n",
      "Interaction training epoch: 191, train loss: 0.24803, val loss: 0.24584\n",
      "Interaction training epoch: 192, train loss: 0.24874, val loss: 0.24193\n",
      "Interaction training epoch: 193, train loss: 0.24978, val loss: 0.25348\n",
      "Interaction training epoch: 194, train loss: 0.24860, val loss: 0.24276\n",
      "Interaction training epoch: 195, train loss: 0.25166, val loss: 0.25280\n",
      "Interaction training epoch: 196, train loss: 0.24907, val loss: 0.24549\n",
      "Interaction training epoch: 197, train loss: 0.25744, val loss: 0.26181\n",
      "Interaction training epoch: 198, train loss: 0.25024, val loss: 0.24641\n",
      "Interaction training epoch: 199, train loss: 0.25062, val loss: 0.25020\n",
      "Interaction training epoch: 200, train loss: 0.24742, val loss: 0.24600\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########3 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.25330, val loss: 0.25125\n",
      "Interaction tuning epoch: 2, train loss: 0.25226, val loss: 0.24605\n",
      "Interaction tuning epoch: 3, train loss: 0.25236, val loss: 0.24451\n",
      "Interaction tuning epoch: 4, train loss: 0.25165, val loss: 0.24555\n",
      "Interaction tuning epoch: 5, train loss: 0.25076, val loss: 0.24419\n",
      "Interaction tuning epoch: 6, train loss: 0.25526, val loss: 0.25114\n",
      "Interaction tuning epoch: 7, train loss: 0.25035, val loss: 0.24082\n",
      "Interaction tuning epoch: 8, train loss: 0.25200, val loss: 0.24711\n",
      "Interaction tuning epoch: 9, train loss: 0.25057, val loss: 0.24528\n",
      "Interaction tuning epoch: 10, train loss: 0.25129, val loss: 0.24519\n",
      "Interaction tuning epoch: 11, train loss: 0.25237, val loss: 0.24861\n",
      "Interaction tuning epoch: 12, train loss: 0.24956, val loss: 0.24431\n",
      "Interaction tuning epoch: 13, train loss: 0.25170, val loss: 0.24543\n",
      "Interaction tuning epoch: 14, train loss: 0.24843, val loss: 0.24323\n",
      "Interaction tuning epoch: 15, train loss: 0.25166, val loss: 0.24762\n",
      "Interaction tuning epoch: 16, train loss: 0.25220, val loss: 0.24170\n",
      "Interaction tuning epoch: 17, train loss: 0.25394, val loss: 0.25469\n",
      "Interaction tuning epoch: 18, train loss: 0.25333, val loss: 0.24708\n",
      "Interaction tuning epoch: 19, train loss: 0.25060, val loss: 0.24378\n",
      "Interaction tuning epoch: 20, train loss: 0.24761, val loss: 0.24261\n",
      "Interaction tuning epoch: 21, train loss: 0.25359, val loss: 0.24824\n",
      "Interaction tuning epoch: 22, train loss: 0.25014, val loss: 0.24475\n",
      "Interaction tuning epoch: 23, train loss: 0.24967, val loss: 0.24582\n",
      "Interaction tuning epoch: 24, train loss: 0.24869, val loss: 0.24297\n",
      "Interaction tuning epoch: 25, train loss: 0.25263, val loss: 0.25169\n",
      "Interaction tuning epoch: 26, train loss: 0.24971, val loss: 0.24566\n",
      "Interaction tuning epoch: 27, train loss: 0.24929, val loss: 0.24124\n",
      "Interaction tuning epoch: 28, train loss: 0.24987, val loss: 0.24763\n",
      "Interaction tuning epoch: 29, train loss: 0.24830, val loss: 0.24148\n",
      "Interaction tuning epoch: 30, train loss: 0.24997, val loss: 0.24649\n",
      "Interaction tuning epoch: 31, train loss: 0.24833, val loss: 0.24763\n",
      "Interaction tuning epoch: 32, train loss: 0.24887, val loss: 0.24254\n",
      "Interaction tuning epoch: 33, train loss: 0.25160, val loss: 0.25023\n",
      "Interaction tuning epoch: 34, train loss: 0.25533, val loss: 0.25375\n",
      "Interaction tuning epoch: 35, train loss: 0.24810, val loss: 0.24631\n",
      "Interaction tuning epoch: 36, train loss: 0.25047, val loss: 0.24655\n",
      "Interaction tuning epoch: 37, train loss: 0.24666, val loss: 0.24020\n",
      "Interaction tuning epoch: 38, train loss: 0.24922, val loss: 0.24611\n",
      "Interaction tuning epoch: 39, train loss: 0.24825, val loss: 0.24266\n",
      "Interaction tuning epoch: 40, train loss: 0.24662, val loss: 0.24323\n",
      "Interaction tuning epoch: 41, train loss: 0.24913, val loss: 0.24322\n",
      "Interaction tuning epoch: 42, train loss: 0.25104, val loss: 0.24883\n",
      "Interaction tuning epoch: 43, train loss: 0.24661, val loss: 0.24396\n",
      "Interaction tuning epoch: 44, train loss: 0.24748, val loss: 0.24355\n",
      "Interaction tuning epoch: 45, train loss: 0.24827, val loss: 0.24717\n",
      "Interaction tuning epoch: 46, train loss: 0.24888, val loss: 0.24136\n",
      "Interaction tuning epoch: 47, train loss: 0.24845, val loss: 0.24636\n",
      "Interaction tuning epoch: 48, train loss: 0.24823, val loss: 0.24444\n",
      "Interaction tuning epoch: 49, train loss: 0.24598, val loss: 0.24020\n",
      "Interaction tuning epoch: 50, train loss: 0.24781, val loss: 0.24227\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 57.91761660575867\n",
      "After the gam stage, training error is 0.24780 , validation error is 0.24227\n",
      "missing value counts: 99216\n",
      "#####start auto_tuning#####\n",
      "the best shrinkage is 0.656250\n",
      "[SoftImpute] Max Singular Value of X_init = 3.965428\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.224862 validation BCE=0.238584,rank=3\n",
      "[SoftImpute] Iter 1: observed BCE=0.224342 validation BCE=0.239246,rank=3\n",
      "[SoftImpute] Iter 2: observed BCE=0.223998 validation BCE=0.239030,rank=3\n",
      "[SoftImpute] Iter 3: observed BCE=0.223446 validation BCE=0.238682,rank=3\n",
      "[SoftImpute] Iter 4: observed BCE=0.222982 validation BCE=0.238391,rank=3\n",
      "[SoftImpute] Iter 5: observed BCE=0.222223 validation BCE=0.238005,rank=3\n",
      "[SoftImpute] Iter 6: observed BCE=0.222079 validation BCE=0.237896,rank=3\n",
      "[SoftImpute] Iter 7: observed BCE=0.221426 validation BCE=0.237974,rank=3\n",
      "[SoftImpute] Iter 8: observed BCE=0.221633 validation BCE=0.237906,rank=3\n",
      "[SoftImpute] Iter 9: observed BCE=0.221282 validation BCE=0.237771,rank=3\n",
      "[SoftImpute] Iter 10: observed BCE=0.221585 validation BCE=0.237670,rank=3\n",
      "[SoftImpute] Iter 11: observed BCE=0.221465 validation BCE=0.237771,rank=3\n",
      "[SoftImpute] Iter 12: observed BCE=0.221412 validation BCE=0.238028,rank=3\n",
      "[SoftImpute] Iter 13: observed BCE=0.220936 validation BCE=0.238106,rank=3\n",
      "[SoftImpute] Iter 14: observed BCE=0.221025 validation BCE=0.237765,rank=3\n",
      "[SoftImpute] Iter 15: observed BCE=0.221192 validation BCE=0.237529,rank=3\n",
      "[SoftImpute] Iter 16: observed BCE=0.221711 validation BCE=0.237697,rank=3\n",
      "[SoftImpute] Iter 17: observed BCE=0.221564 validation BCE=0.237263,rank=3\n",
      "[SoftImpute] Iter 18: observed BCE=0.221567 validation BCE=0.237211,rank=3\n",
      "[SoftImpute] Iter 19: observed BCE=0.221751 validation BCE=0.237136,rank=3\n",
      "[SoftImpute] Iter 20: observed BCE=0.221618 validation BCE=0.237109,rank=3\n",
      "[SoftImpute] Iter 21: observed BCE=0.221695 validation BCE=0.237247,rank=3\n",
      "[SoftImpute] Iter 22: observed BCE=0.222311 validation BCE=0.237517,rank=3\n",
      "[SoftImpute] Iter 23: observed BCE=0.221916 validation BCE=0.237335,rank=3\n",
      "[SoftImpute] Iter 24: observed BCE=0.222151 validation BCE=0.237416,rank=3\n",
      "[SoftImpute] Iter 25: observed BCE=0.222239 validation BCE=0.237562,rank=3\n",
      "[SoftImpute] Iter 26: observed BCE=0.222000 validation BCE=0.237480,rank=3\n",
      "[SoftImpute] Iter 27: observed BCE=0.222010 validation BCE=0.237451,rank=3\n",
      "[SoftImpute] Iter 28: observed BCE=0.222008 validation BCE=0.237361,rank=3\n",
      "[SoftImpute] Iter 29: observed BCE=0.222064 validation BCE=0.237639,rank=3\n",
      "[SoftImpute] Iter 30: observed BCE=0.222201 validation BCE=0.237379,rank=3\n",
      "[SoftImpute] Iter 31: observed BCE=0.222090 validation BCE=0.237145,rank=3\n",
      "[SoftImpute] Iter 32: observed BCE=0.222093 validation BCE=0.237433,rank=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 33: observed BCE=0.222104 validation BCE=0.237261,rank=3\n",
      "[SoftImpute] Iter 34: observed BCE=0.221975 validation BCE=0.237106,rank=3\n",
      "[SoftImpute] Iter 35: observed BCE=0.222061 validation BCE=0.237150,rank=3\n",
      "[SoftImpute] Iter 36: observed BCE=0.222035 validation BCE=0.237055,rank=3\n",
      "[SoftImpute] Iter 37: observed BCE=0.221659 validation BCE=0.237314,rank=3\n",
      "[SoftImpute] Iter 38: observed BCE=0.221744 validation BCE=0.237127,rank=3\n",
      "[SoftImpute] Iter 39: observed BCE=0.222077 validation BCE=0.236771,rank=3\n",
      "[SoftImpute] Iter 40: observed BCE=0.222182 validation BCE=0.237001,rank=3\n",
      "[SoftImpute] Iter 41: observed BCE=0.222106 validation BCE=0.236866,rank=3\n",
      "[SoftImpute] Iter 42: observed BCE=0.222145 validation BCE=0.236875,rank=3\n",
      "[SoftImpute] Iter 43: observed BCE=0.221868 validation BCE=0.237019,rank=3\n",
      "[SoftImpute] Iter 44: observed BCE=0.221891 validation BCE=0.236651,rank=3\n",
      "[SoftImpute] Iter 45: observed BCE=0.221838 validation BCE=0.236741,rank=3\n",
      "[SoftImpute] Iter 46: observed BCE=0.221986 validation BCE=0.236727,rank=3\n",
      "[SoftImpute] Iter 47: observed BCE=0.221979 validation BCE=0.236973,rank=3\n",
      "[SoftImpute] Iter 48: observed BCE=0.222208 validation BCE=0.236922,rank=3\n",
      "[SoftImpute] Iter 49: observed BCE=0.222072 validation BCE=0.236676,rank=3\n",
      "[SoftImpute] Iter 50: observed BCE=0.222168 validation BCE=0.236914,rank=3\n",
      "[SoftImpute] Iter 51: observed BCE=0.221976 validation BCE=0.236684,rank=3\n",
      "[SoftImpute] Iter 52: observed BCE=0.221929 validation BCE=0.236849,rank=3\n",
      "[SoftImpute] Iter 53: observed BCE=0.221901 validation BCE=0.236933,rank=3\n",
      "[SoftImpute] Iter 54: observed BCE=0.222010 validation BCE=0.236726,rank=3\n",
      "[SoftImpute] Iter 55: observed BCE=0.222105 validation BCE=0.236814,rank=3\n",
      "[SoftImpute] Iter 56: observed BCE=0.221993 validation BCE=0.236832,rank=3\n",
      "[SoftImpute] Iter 57: observed BCE=0.221985 validation BCE=0.236779,rank=3\n",
      "[SoftImpute] Iter 58: observed BCE=0.222055 validation BCE=0.236731,rank=3\n",
      "[SoftImpute] Iter 59: observed BCE=0.222045 validation BCE=0.236792,rank=3\n",
      "[SoftImpute] Iter 60: observed BCE=0.222146 validation BCE=0.236761,rank=3\n",
      "[SoftImpute] Iter 61: observed BCE=0.221963 validation BCE=0.236794,rank=3\n",
      "[SoftImpute] Iter 62: observed BCE=0.221885 validation BCE=0.236618,rank=3\n",
      "[SoftImpute] Iter 63: observed BCE=0.221854 validation BCE=0.236629,rank=3\n",
      "[SoftImpute] Iter 64: observed BCE=0.221989 validation BCE=0.236706,rank=3\n",
      "[SoftImpute] Iter 65: observed BCE=0.222151 validation BCE=0.236621,rank=3\n",
      "[SoftImpute] Iter 66: observed BCE=0.221914 validation BCE=0.236498,rank=3\n",
      "[SoftImpute] Iter 67: observed BCE=0.222064 validation BCE=0.236833,rank=3\n",
      "[SoftImpute] Iter 68: observed BCE=0.221938 validation BCE=0.236619,rank=3\n",
      "[SoftImpute] Iter 69: observed BCE=0.221746 validation BCE=0.236727,rank=3\n",
      "[SoftImpute] Iter 70: observed BCE=0.222060 validation BCE=0.236688,rank=3\n",
      "[SoftImpute] Iter 71: observed BCE=0.221915 validation BCE=0.236515,rank=3\n",
      "[SoftImpute] Iter 72: observed BCE=0.221879 validation BCE=0.236605,rank=3\n",
      "[SoftImpute] Iter 73: observed BCE=0.222065 validation BCE=0.236462,rank=3\n",
      "[SoftImpute] Iter 74: observed BCE=0.222061 validation BCE=0.236610,rank=3\n",
      "[SoftImpute] Iter 75: observed BCE=0.221996 validation BCE=0.236701,rank=3\n",
      "[SoftImpute] Iter 76: observed BCE=0.222148 validation BCE=0.236758,rank=3\n",
      "[SoftImpute] Iter 77: observed BCE=0.221989 validation BCE=0.236532,rank=3\n",
      "[SoftImpute] Iter 78: observed BCE=0.221699 validation BCE=0.236466,rank=3\n",
      "[SoftImpute] Iter 79: observed BCE=0.221845 validation BCE=0.236702,rank=3\n",
      "[SoftImpute] Iter 80: observed BCE=0.222013 validation BCE=0.236385,rank=3\n",
      "[SoftImpute] Iter 81: observed BCE=0.221964 validation BCE=0.236487,rank=3\n",
      "[SoftImpute] Iter 82: observed BCE=0.222246 validation BCE=0.236635,rank=3\n",
      "[SoftImpute] Iter 83: observed BCE=0.221969 validation BCE=0.236677,rank=3\n",
      "[SoftImpute] Iter 84: observed BCE=0.221913 validation BCE=0.236893,rank=3\n",
      "[SoftImpute] Iter 85: observed BCE=0.221776 validation BCE=0.236280,rank=3\n",
      "[SoftImpute] Iter 86: observed BCE=0.221887 validation BCE=0.236467,rank=3\n",
      "[SoftImpute] Iter 87: observed BCE=0.222032 validation BCE=0.236650,rank=3\n",
      "[SoftImpute] Iter 88: observed BCE=0.221766 validation BCE=0.236358,rank=3\n",
      "[SoftImpute] Iter 89: observed BCE=0.222058 validation BCE=0.236606,rank=3\n",
      "[SoftImpute] Iter 90: observed BCE=0.221908 validation BCE=0.236553,rank=3\n",
      "[SoftImpute] Iter 91: observed BCE=0.221866 validation BCE=0.236431,rank=3\n",
      "[SoftImpute] Iter 92: observed BCE=0.222076 validation BCE=0.236583,rank=3\n",
      "[SoftImpute] Iter 93: observed BCE=0.221815 validation BCE=0.236511,rank=3\n",
      "[SoftImpute] Iter 94: observed BCE=0.222040 validation BCE=0.236358,rank=3\n",
      "[SoftImpute] Iter 95: observed BCE=0.222024 validation BCE=0.236677,rank=3\n",
      "[SoftImpute] Iter 96: observed BCE=0.222039 validation BCE=0.236446,rank=3\n",
      "[SoftImpute] Iter 97: observed BCE=0.221903 validation BCE=0.236492,rank=3\n",
      "[SoftImpute] Iter 98: observed BCE=0.221863 validation BCE=0.236653,rank=3\n",
      "[SoftImpute] Iter 99: observed BCE=0.222152 validation BCE=0.236529,rank=3\n",
      "[SoftImpute] Iter 100: observed BCE=0.221805 validation BCE=0.236546,rank=3\n",
      "[SoftImpute] Stopped after iteration 100 for lambda=0.079309\n",
      "final num of user group: 7\n",
      "final num of item group: 8\n",
      "change mode state : True\n",
      "time cost: 28.60881996154785\n",
      "After the matrix factor stage, training error is 0.22180, validation error is 0.23655\n",
      "7\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.67896, val loss: 0.67841\n",
      "Main effects training epoch: 2, train loss: 0.67307, val loss: 0.67145\n",
      "Main effects training epoch: 3, train loss: 0.66703, val loss: 0.66620\n",
      "Main effects training epoch: 4, train loss: 0.65738, val loss: 0.65780\n",
      "Main effects training epoch: 5, train loss: 0.63804, val loss: 0.63861\n",
      "Main effects training epoch: 6, train loss: 0.60693, val loss: 0.61026\n",
      "Main effects training epoch: 7, train loss: 0.57111, val loss: 0.57768\n",
      "Main effects training epoch: 8, train loss: 0.53996, val loss: 0.54767\n",
      "Main effects training epoch: 9, train loss: 0.53347, val loss: 0.54529\n",
      "Main effects training epoch: 10, train loss: 0.53383, val loss: 0.54141\n",
      "Main effects training epoch: 11, train loss: 0.52988, val loss: 0.53683\n",
      "Main effects training epoch: 12, train loss: 0.52844, val loss: 0.53454\n",
      "Main effects training epoch: 13, train loss: 0.52796, val loss: 0.53449\n",
      "Main effects training epoch: 14, train loss: 0.52768, val loss: 0.53370\n",
      "Main effects training epoch: 15, train loss: 0.52734, val loss: 0.53435\n",
      "Main effects training epoch: 16, train loss: 0.52746, val loss: 0.53295\n",
      "Main effects training epoch: 17, train loss: 0.52717, val loss: 0.53483\n",
      "Main effects training epoch: 18, train loss: 0.52717, val loss: 0.53456\n",
      "Main effects training epoch: 19, train loss: 0.52703, val loss: 0.53441\n",
      "Main effects training epoch: 20, train loss: 0.52688, val loss: 0.53343\n",
      "Main effects training epoch: 21, train loss: 0.52739, val loss: 0.53560\n",
      "Main effects training epoch: 22, train loss: 0.52679, val loss: 0.53407\n",
      "Main effects training epoch: 23, train loss: 0.52721, val loss: 0.53495\n",
      "Main effects training epoch: 24, train loss: 0.52773, val loss: 0.53669\n",
      "Main effects training epoch: 25, train loss: 0.52663, val loss: 0.53290\n",
      "Main effects training epoch: 26, train loss: 0.52640, val loss: 0.53525\n",
      "Main effects training epoch: 27, train loss: 0.52635, val loss: 0.53468\n",
      "Main effects training epoch: 28, train loss: 0.52656, val loss: 0.53327\n",
      "Main effects training epoch: 29, train loss: 0.52662, val loss: 0.53441\n",
      "Main effects training epoch: 30, train loss: 0.52771, val loss: 0.53670\n",
      "Main effects training epoch: 31, train loss: 0.52710, val loss: 0.53432\n",
      "Main effects training epoch: 32, train loss: 0.52645, val loss: 0.53371\n",
      "Main effects training epoch: 33, train loss: 0.52600, val loss: 0.53329\n",
      "Main effects training epoch: 34, train loss: 0.52585, val loss: 0.53429\n",
      "Main effects training epoch: 35, train loss: 0.52574, val loss: 0.53369\n",
      "Main effects training epoch: 36, train loss: 0.52597, val loss: 0.53546\n",
      "Main effects training epoch: 37, train loss: 0.52689, val loss: 0.53311\n",
      "Main effects training epoch: 38, train loss: 0.52587, val loss: 0.53470\n",
      "Main effects training epoch: 39, train loss: 0.52564, val loss: 0.53361\n",
      "Main effects training epoch: 40, train loss: 0.52582, val loss: 0.53391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 41, train loss: 0.52602, val loss: 0.53477\n",
      "Main effects training epoch: 42, train loss: 0.52587, val loss: 0.53333\n",
      "Main effects training epoch: 43, train loss: 0.52567, val loss: 0.53479\n",
      "Main effects training epoch: 44, train loss: 0.52592, val loss: 0.53325\n",
      "Main effects training epoch: 45, train loss: 0.52571, val loss: 0.53444\n",
      "Main effects training epoch: 46, train loss: 0.52542, val loss: 0.53350\n",
      "Main effects training epoch: 47, train loss: 0.52581, val loss: 0.53377\n",
      "Main effects training epoch: 48, train loss: 0.52622, val loss: 0.53454\n",
      "Main effects training epoch: 49, train loss: 0.52608, val loss: 0.53471\n",
      "Main effects training epoch: 50, train loss: 0.52606, val loss: 0.53552\n",
      "Main effects training epoch: 51, train loss: 0.52572, val loss: 0.53320\n",
      "Main effects training epoch: 52, train loss: 0.52580, val loss: 0.53538\n",
      "Main effects training epoch: 53, train loss: 0.52648, val loss: 0.53461\n",
      "Main effects training epoch: 54, train loss: 0.52691, val loss: 0.53608\n",
      "Main effects training epoch: 55, train loss: 0.52885, val loss: 0.53605\n",
      "Main effects training epoch: 56, train loss: 0.52754, val loss: 0.53746\n",
      "Main effects training epoch: 57, train loss: 0.52653, val loss: 0.53430\n",
      "Main effects training epoch: 58, train loss: 0.52562, val loss: 0.53454\n",
      "Main effects training epoch: 59, train loss: 0.52523, val loss: 0.53343\n",
      "Main effects training epoch: 60, train loss: 0.52508, val loss: 0.53415\n",
      "Main effects training epoch: 61, train loss: 0.52500, val loss: 0.53321\n",
      "Main effects training epoch: 62, train loss: 0.52513, val loss: 0.53312\n",
      "Main effects training epoch: 63, train loss: 0.52530, val loss: 0.53547\n",
      "Main effects training epoch: 64, train loss: 0.52530, val loss: 0.53353\n",
      "Main effects training epoch: 65, train loss: 0.52554, val loss: 0.53595\n",
      "Main effects training epoch: 66, train loss: 0.52517, val loss: 0.53394\n",
      "Main effects training epoch: 67, train loss: 0.52513, val loss: 0.53372\n",
      "Main effects training epoch: 68, train loss: 0.52513, val loss: 0.53292\n",
      "Main effects training epoch: 69, train loss: 0.52531, val loss: 0.53524\n",
      "Main effects training epoch: 70, train loss: 0.52569, val loss: 0.53539\n",
      "Main effects training epoch: 71, train loss: 0.52544, val loss: 0.53452\n",
      "Main effects training epoch: 72, train loss: 0.52554, val loss: 0.53406\n",
      "Main effects training epoch: 73, train loss: 0.52522, val loss: 0.53556\n",
      "Main effects training epoch: 74, train loss: 0.52531, val loss: 0.53249\n",
      "Main effects training epoch: 75, train loss: 0.52493, val loss: 0.53414\n",
      "Main effects training epoch: 76, train loss: 0.52547, val loss: 0.53373\n",
      "Main effects training epoch: 77, train loss: 0.52507, val loss: 0.53542\n",
      "Main effects training epoch: 78, train loss: 0.52570, val loss: 0.53457\n",
      "Main effects training epoch: 79, train loss: 0.52523, val loss: 0.53496\n",
      "Main effects training epoch: 80, train loss: 0.52465, val loss: 0.53364\n",
      "Main effects training epoch: 81, train loss: 0.52460, val loss: 0.53462\n",
      "Main effects training epoch: 82, train loss: 0.52448, val loss: 0.53398\n",
      "Main effects training epoch: 83, train loss: 0.52453, val loss: 0.53367\n",
      "Main effects training epoch: 84, train loss: 0.52439, val loss: 0.53341\n",
      "Main effects training epoch: 85, train loss: 0.52427, val loss: 0.53385\n",
      "Main effects training epoch: 86, train loss: 0.52424, val loss: 0.53345\n",
      "Main effects training epoch: 87, train loss: 0.52433, val loss: 0.53269\n",
      "Main effects training epoch: 88, train loss: 0.52425, val loss: 0.53422\n",
      "Main effects training epoch: 89, train loss: 0.52427, val loss: 0.53351\n",
      "Main effects training epoch: 90, train loss: 0.52426, val loss: 0.53323\n",
      "Main effects training epoch: 91, train loss: 0.52427, val loss: 0.53438\n",
      "Main effects training epoch: 92, train loss: 0.52434, val loss: 0.53376\n",
      "Main effects training epoch: 93, train loss: 0.52501, val loss: 0.53420\n",
      "Main effects training epoch: 94, train loss: 0.52462, val loss: 0.53476\n",
      "Main effects training epoch: 95, train loss: 0.52423, val loss: 0.53380\n",
      "Main effects training epoch: 96, train loss: 0.52453, val loss: 0.53465\n",
      "Main effects training epoch: 97, train loss: 0.52440, val loss: 0.53222\n",
      "Main effects training epoch: 98, train loss: 0.52464, val loss: 0.53579\n",
      "Main effects training epoch: 99, train loss: 0.52451, val loss: 0.53282\n",
      "Main effects training epoch: 100, train loss: 0.52437, val loss: 0.53518\n",
      "Main effects training epoch: 101, train loss: 0.52447, val loss: 0.53255\n",
      "Main effects training epoch: 102, train loss: 0.52401, val loss: 0.53332\n",
      "Main effects training epoch: 103, train loss: 0.52398, val loss: 0.53347\n",
      "Main effects training epoch: 104, train loss: 0.52382, val loss: 0.53342\n",
      "Main effects training epoch: 105, train loss: 0.52407, val loss: 0.53335\n",
      "Main effects training epoch: 106, train loss: 0.52410, val loss: 0.53316\n",
      "Main effects training epoch: 107, train loss: 0.52422, val loss: 0.53338\n",
      "Main effects training epoch: 108, train loss: 0.52481, val loss: 0.53388\n",
      "Main effects training epoch: 109, train loss: 0.52489, val loss: 0.53404\n",
      "Main effects training epoch: 110, train loss: 0.52389, val loss: 0.53243\n",
      "Main effects training epoch: 111, train loss: 0.52387, val loss: 0.53386\n",
      "Main effects training epoch: 112, train loss: 0.52382, val loss: 0.53296\n",
      "Main effects training epoch: 113, train loss: 0.52381, val loss: 0.53335\n",
      "Main effects training epoch: 114, train loss: 0.52413, val loss: 0.53367\n",
      "Main effects training epoch: 115, train loss: 0.52395, val loss: 0.53376\n",
      "Main effects training epoch: 116, train loss: 0.52410, val loss: 0.53244\n",
      "Main effects training epoch: 117, train loss: 0.52398, val loss: 0.53487\n",
      "Main effects training epoch: 118, train loss: 0.52409, val loss: 0.53243\n",
      "Main effects training epoch: 119, train loss: 0.52414, val loss: 0.53282\n",
      "Main effects training epoch: 120, train loss: 0.52396, val loss: 0.53395\n",
      "Main effects training epoch: 121, train loss: 0.52406, val loss: 0.53303\n",
      "Main effects training epoch: 122, train loss: 0.52374, val loss: 0.53338\n",
      "Main effects training epoch: 123, train loss: 0.52348, val loss: 0.53263\n",
      "Main effects training epoch: 124, train loss: 0.52346, val loss: 0.53230\n",
      "Main effects training epoch: 125, train loss: 0.52372, val loss: 0.53328\n",
      "Main effects training epoch: 126, train loss: 0.52392, val loss: 0.53202\n",
      "Main effects training epoch: 127, train loss: 0.52403, val loss: 0.53342\n",
      "Main effects training epoch: 128, train loss: 0.52447, val loss: 0.53265\n",
      "Main effects training epoch: 129, train loss: 0.52493, val loss: 0.53576\n",
      "Main effects training epoch: 130, train loss: 0.52508, val loss: 0.53268\n",
      "Main effects training epoch: 131, train loss: 0.52354, val loss: 0.53305\n",
      "Main effects training epoch: 132, train loss: 0.52352, val loss: 0.53189\n",
      "Main effects training epoch: 133, train loss: 0.52362, val loss: 0.53283\n",
      "Main effects training epoch: 134, train loss: 0.52315, val loss: 0.53258\n",
      "Main effects training epoch: 135, train loss: 0.52330, val loss: 0.53375\n",
      "Main effects training epoch: 136, train loss: 0.52316, val loss: 0.53201\n",
      "Main effects training epoch: 137, train loss: 0.52328, val loss: 0.53272\n",
      "Main effects training epoch: 138, train loss: 0.52314, val loss: 0.53244\n",
      "Main effects training epoch: 139, train loss: 0.52329, val loss: 0.53221\n",
      "Main effects training epoch: 140, train loss: 0.52401, val loss: 0.53271\n",
      "Main effects training epoch: 141, train loss: 0.52390, val loss: 0.53442\n",
      "Main effects training epoch: 142, train loss: 0.52340, val loss: 0.53170\n",
      "Main effects training epoch: 143, train loss: 0.52326, val loss: 0.53141\n",
      "Main effects training epoch: 144, train loss: 0.52322, val loss: 0.53311\n",
      "Main effects training epoch: 145, train loss: 0.52330, val loss: 0.53184\n",
      "Main effects training epoch: 146, train loss: 0.52349, val loss: 0.53215\n",
      "Main effects training epoch: 147, train loss: 0.52337, val loss: 0.53210\n",
      "Main effects training epoch: 148, train loss: 0.52339, val loss: 0.53412\n",
      "Main effects training epoch: 149, train loss: 0.52335, val loss: 0.53122\n",
      "Main effects training epoch: 150, train loss: 0.52306, val loss: 0.53215\n",
      "Main effects training epoch: 151, train loss: 0.52318, val loss: 0.53216\n",
      "Main effects training epoch: 152, train loss: 0.52289, val loss: 0.53170\n",
      "Main effects training epoch: 153, train loss: 0.52311, val loss: 0.53277\n",
      "Main effects training epoch: 154, train loss: 0.52308, val loss: 0.53057\n",
      "Main effects training epoch: 155, train loss: 0.52293, val loss: 0.53223\n",
      "Main effects training epoch: 156, train loss: 0.52304, val loss: 0.53206\n",
      "Main effects training epoch: 157, train loss: 0.52377, val loss: 0.53441\n",
      "Main effects training epoch: 158, train loss: 0.52289, val loss: 0.53125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 159, train loss: 0.52276, val loss: 0.53162\n",
      "Main effects training epoch: 160, train loss: 0.52271, val loss: 0.53150\n",
      "Main effects training epoch: 161, train loss: 0.52278, val loss: 0.53106\n",
      "Main effects training epoch: 162, train loss: 0.52298, val loss: 0.53203\n",
      "Main effects training epoch: 163, train loss: 0.52275, val loss: 0.53186\n",
      "Main effects training epoch: 164, train loss: 0.52264, val loss: 0.53116\n",
      "Main effects training epoch: 165, train loss: 0.52287, val loss: 0.53322\n",
      "Main effects training epoch: 166, train loss: 0.52251, val loss: 0.53081\n",
      "Main effects training epoch: 167, train loss: 0.52265, val loss: 0.53267\n",
      "Main effects training epoch: 168, train loss: 0.52271, val loss: 0.53049\n",
      "Main effects training epoch: 169, train loss: 0.52254, val loss: 0.53233\n",
      "Main effects training epoch: 170, train loss: 0.52243, val loss: 0.53162\n",
      "Main effects training epoch: 171, train loss: 0.52237, val loss: 0.53041\n",
      "Main effects training epoch: 172, train loss: 0.52240, val loss: 0.53127\n",
      "Main effects training epoch: 173, train loss: 0.52239, val loss: 0.53118\n",
      "Main effects training epoch: 174, train loss: 0.52268, val loss: 0.53063\n",
      "Main effects training epoch: 175, train loss: 0.52284, val loss: 0.53127\n",
      "Main effects training epoch: 176, train loss: 0.52297, val loss: 0.53188\n",
      "Main effects training epoch: 177, train loss: 0.52283, val loss: 0.53194\n",
      "Main effects training epoch: 178, train loss: 0.52282, val loss: 0.53188\n",
      "Main effects training epoch: 179, train loss: 0.52259, val loss: 0.53187\n",
      "Main effects training epoch: 180, train loss: 0.52269, val loss: 0.53088\n",
      "Main effects training epoch: 181, train loss: 0.52223, val loss: 0.53043\n",
      "Main effects training epoch: 182, train loss: 0.52208, val loss: 0.53111\n",
      "Main effects training epoch: 183, train loss: 0.52226, val loss: 0.53084\n",
      "Main effects training epoch: 184, train loss: 0.52206, val loss: 0.53030\n",
      "Main effects training epoch: 185, train loss: 0.52205, val loss: 0.53120\n",
      "Main effects training epoch: 186, train loss: 0.52187, val loss: 0.53065\n",
      "Main effects training epoch: 187, train loss: 0.52169, val loss: 0.53013\n",
      "Main effects training epoch: 188, train loss: 0.52171, val loss: 0.53081\n",
      "Main effects training epoch: 189, train loss: 0.52194, val loss: 0.53015\n",
      "Main effects training epoch: 190, train loss: 0.52135, val loss: 0.52961\n",
      "Main effects training epoch: 191, train loss: 0.52125, val loss: 0.53086\n",
      "Main effects training epoch: 192, train loss: 0.52157, val loss: 0.52953\n",
      "Main effects training epoch: 193, train loss: 0.52179, val loss: 0.53049\n",
      "Main effects training epoch: 194, train loss: 0.52135, val loss: 0.52930\n",
      "Main effects training epoch: 195, train loss: 0.52097, val loss: 0.52877\n",
      "Main effects training epoch: 196, train loss: 0.52142, val loss: 0.52830\n",
      "Main effects training epoch: 197, train loss: 0.52185, val loss: 0.52906\n",
      "Main effects training epoch: 198, train loss: 0.52124, val loss: 0.52938\n",
      "Main effects training epoch: 199, train loss: 0.52089, val loss: 0.52996\n",
      "Main effects training epoch: 200, train loss: 0.52087, val loss: 0.52829\n",
      "Main effects training epoch: 201, train loss: 0.52078, val loss: 0.52995\n",
      "Main effects training epoch: 202, train loss: 0.52076, val loss: 0.52958\n",
      "Main effects training epoch: 203, train loss: 0.52068, val loss: 0.52813\n",
      "Main effects training epoch: 204, train loss: 0.52060, val loss: 0.52940\n",
      "Main effects training epoch: 205, train loss: 0.52034, val loss: 0.52754\n",
      "Main effects training epoch: 206, train loss: 0.52048, val loss: 0.52852\n",
      "Main effects training epoch: 207, train loss: 0.52046, val loss: 0.52758\n",
      "Main effects training epoch: 208, train loss: 0.52058, val loss: 0.52941\n",
      "Main effects training epoch: 209, train loss: 0.52051, val loss: 0.52862\n",
      "Main effects training epoch: 210, train loss: 0.52022, val loss: 0.52728\n",
      "Main effects training epoch: 211, train loss: 0.52077, val loss: 0.52890\n",
      "Main effects training epoch: 212, train loss: 0.52033, val loss: 0.52874\n",
      "Main effects training epoch: 213, train loss: 0.52056, val loss: 0.52865\n",
      "Main effects training epoch: 214, train loss: 0.52047, val loss: 0.52787\n",
      "Main effects training epoch: 215, train loss: 0.52042, val loss: 0.53003\n",
      "Main effects training epoch: 216, train loss: 0.52062, val loss: 0.52701\n",
      "Main effects training epoch: 217, train loss: 0.52127, val loss: 0.52928\n",
      "Main effects training epoch: 218, train loss: 0.52084, val loss: 0.52932\n",
      "Main effects training epoch: 219, train loss: 0.52049, val loss: 0.52831\n",
      "Main effects training epoch: 220, train loss: 0.52025, val loss: 0.52829\n",
      "Main effects training epoch: 221, train loss: 0.52069, val loss: 0.53041\n",
      "Main effects training epoch: 222, train loss: 0.52029, val loss: 0.52631\n",
      "Main effects training epoch: 223, train loss: 0.52001, val loss: 0.52880\n",
      "Main effects training epoch: 224, train loss: 0.51966, val loss: 0.52754\n",
      "Main effects training epoch: 225, train loss: 0.51988, val loss: 0.52823\n",
      "Main effects training epoch: 226, train loss: 0.51956, val loss: 0.52711\n",
      "Main effects training epoch: 227, train loss: 0.51983, val loss: 0.52742\n",
      "Main effects training epoch: 228, train loss: 0.51977, val loss: 0.52812\n",
      "Main effects training epoch: 229, train loss: 0.51997, val loss: 0.52796\n",
      "Main effects training epoch: 230, train loss: 0.51958, val loss: 0.52682\n",
      "Main effects training epoch: 231, train loss: 0.51973, val loss: 0.52891\n",
      "Main effects training epoch: 232, train loss: 0.51964, val loss: 0.52701\n",
      "Main effects training epoch: 233, train loss: 0.51984, val loss: 0.52774\n",
      "Main effects training epoch: 234, train loss: 0.51936, val loss: 0.52604\n",
      "Main effects training epoch: 235, train loss: 0.51950, val loss: 0.52804\n",
      "Main effects training epoch: 236, train loss: 0.51928, val loss: 0.52674\n",
      "Main effects training epoch: 237, train loss: 0.51923, val loss: 0.52699\n",
      "Main effects training epoch: 238, train loss: 0.51966, val loss: 0.52768\n",
      "Main effects training epoch: 239, train loss: 0.51943, val loss: 0.52680\n",
      "Main effects training epoch: 240, train loss: 0.51944, val loss: 0.52685\n",
      "Main effects training epoch: 241, train loss: 0.51935, val loss: 0.52812\n",
      "Main effects training epoch: 242, train loss: 0.51922, val loss: 0.52626\n",
      "Main effects training epoch: 243, train loss: 0.51897, val loss: 0.52723\n",
      "Main effects training epoch: 244, train loss: 0.51893, val loss: 0.52722\n",
      "Main effects training epoch: 245, train loss: 0.51910, val loss: 0.52635\n",
      "Main effects training epoch: 246, train loss: 0.51895, val loss: 0.52612\n",
      "Main effects training epoch: 247, train loss: 0.51891, val loss: 0.52587\n",
      "Main effects training epoch: 248, train loss: 0.51870, val loss: 0.52603\n",
      "Main effects training epoch: 249, train loss: 0.51876, val loss: 0.52617\n",
      "Main effects training epoch: 250, train loss: 0.51875, val loss: 0.52703\n",
      "Main effects training epoch: 251, train loss: 0.51912, val loss: 0.52635\n",
      "Main effects training epoch: 252, train loss: 0.51926, val loss: 0.52528\n",
      "Main effects training epoch: 253, train loss: 0.51992, val loss: 0.52950\n",
      "Main effects training epoch: 254, train loss: 0.51944, val loss: 0.52559\n",
      "Main effects training epoch: 255, train loss: 0.51863, val loss: 0.52567\n",
      "Main effects training epoch: 256, train loss: 0.51851, val loss: 0.52715\n",
      "Main effects training epoch: 257, train loss: 0.51851, val loss: 0.52605\n",
      "Main effects training epoch: 258, train loss: 0.51858, val loss: 0.52533\n",
      "Main effects training epoch: 259, train loss: 0.51854, val loss: 0.52538\n",
      "Main effects training epoch: 260, train loss: 0.51854, val loss: 0.52586\n",
      "Main effects training epoch: 261, train loss: 0.51888, val loss: 0.52662\n",
      "Main effects training epoch: 262, train loss: 0.51853, val loss: 0.52579\n",
      "Main effects training epoch: 263, train loss: 0.51848, val loss: 0.52517\n",
      "Main effects training epoch: 264, train loss: 0.51821, val loss: 0.52581\n",
      "Main effects training epoch: 265, train loss: 0.51812, val loss: 0.52505\n",
      "Main effects training epoch: 266, train loss: 0.51820, val loss: 0.52552\n",
      "Main effects training epoch: 267, train loss: 0.51820, val loss: 0.52516\n",
      "Main effects training epoch: 268, train loss: 0.51845, val loss: 0.52791\n",
      "Main effects training epoch: 269, train loss: 0.51821, val loss: 0.52405\n",
      "Main effects training epoch: 270, train loss: 0.51802, val loss: 0.52529\n",
      "Main effects training epoch: 271, train loss: 0.51803, val loss: 0.52490\n",
      "Main effects training epoch: 272, train loss: 0.51778, val loss: 0.52492\n",
      "Main effects training epoch: 273, train loss: 0.51856, val loss: 0.52758\n",
      "Main effects training epoch: 274, train loss: 0.51828, val loss: 0.52524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 275, train loss: 0.51801, val loss: 0.52542\n",
      "Main effects training epoch: 276, train loss: 0.51779, val loss: 0.52558\n",
      "Main effects training epoch: 277, train loss: 0.51758, val loss: 0.52483\n",
      "Main effects training epoch: 278, train loss: 0.51764, val loss: 0.52583\n",
      "Main effects training epoch: 279, train loss: 0.51755, val loss: 0.52372\n",
      "Main effects training epoch: 280, train loss: 0.51744, val loss: 0.52538\n",
      "Main effects training epoch: 281, train loss: 0.51753, val loss: 0.52573\n",
      "Main effects training epoch: 282, train loss: 0.51755, val loss: 0.52472\n",
      "Main effects training epoch: 283, train loss: 0.51757, val loss: 0.52386\n",
      "Main effects training epoch: 284, train loss: 0.51764, val loss: 0.52571\n",
      "Main effects training epoch: 285, train loss: 0.51743, val loss: 0.52375\n",
      "Main effects training epoch: 286, train loss: 0.51787, val loss: 0.52581\n",
      "Main effects training epoch: 287, train loss: 0.51720, val loss: 0.52503\n",
      "Main effects training epoch: 288, train loss: 0.51763, val loss: 0.52520\n",
      "Main effects training epoch: 289, train loss: 0.51729, val loss: 0.52390\n",
      "Main effects training epoch: 290, train loss: 0.51723, val loss: 0.52434\n",
      "Main effects training epoch: 291, train loss: 0.51717, val loss: 0.52443\n",
      "Main effects training epoch: 292, train loss: 0.51706, val loss: 0.52404\n",
      "Main effects training epoch: 293, train loss: 0.51723, val loss: 0.52410\n",
      "Main effects training epoch: 294, train loss: 0.51707, val loss: 0.52484\n",
      "Main effects training epoch: 295, train loss: 0.51698, val loss: 0.52363\n",
      "Main effects training epoch: 296, train loss: 0.51719, val loss: 0.52395\n",
      "Main effects training epoch: 297, train loss: 0.51738, val loss: 0.52484\n",
      "Main effects training epoch: 298, train loss: 0.51721, val loss: 0.52414\n",
      "Main effects training epoch: 299, train loss: 0.51669, val loss: 0.52446\n",
      "Main effects training epoch: 300, train loss: 0.51707, val loss: 0.52494\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.52168, val loss: 0.52457\n",
      "Main effects tuning epoch: 2, train loss: 0.52171, val loss: 0.52683\n",
      "Main effects tuning epoch: 3, train loss: 0.52169, val loss: 0.52610\n",
      "Main effects tuning epoch: 4, train loss: 0.52166, val loss: 0.52650\n",
      "Main effects tuning epoch: 5, train loss: 0.52183, val loss: 0.52519\n",
      "Main effects tuning epoch: 6, train loss: 0.52157, val loss: 0.52559\n",
      "Main effects tuning epoch: 7, train loss: 0.52160, val loss: 0.52585\n",
      "Main effects tuning epoch: 8, train loss: 0.52149, val loss: 0.52542\n",
      "Main effects tuning epoch: 9, train loss: 0.52126, val loss: 0.52563\n",
      "Main effects tuning epoch: 10, train loss: 0.52132, val loss: 0.52578\n",
      "Main effects tuning epoch: 11, train loss: 0.52118, val loss: 0.52559\n",
      "Main effects tuning epoch: 12, train loss: 0.52111, val loss: 0.52505\n",
      "Main effects tuning epoch: 13, train loss: 0.52109, val loss: 0.52435\n",
      "Main effects tuning epoch: 14, train loss: 0.52134, val loss: 0.52648\n",
      "Main effects tuning epoch: 15, train loss: 0.52098, val loss: 0.52541\n",
      "Main effects tuning epoch: 16, train loss: 0.52099, val loss: 0.52479\n",
      "Main effects tuning epoch: 17, train loss: 0.52099, val loss: 0.52439\n",
      "Main effects tuning epoch: 18, train loss: 0.52127, val loss: 0.52543\n",
      "Main effects tuning epoch: 19, train loss: 0.52111, val loss: 0.52532\n",
      "Main effects tuning epoch: 20, train loss: 0.52097, val loss: 0.52498\n",
      "Main effects tuning epoch: 21, train loss: 0.52120, val loss: 0.52533\n",
      "Main effects tuning epoch: 22, train loss: 0.52181, val loss: 0.52683\n",
      "Main effects tuning epoch: 23, train loss: 0.52159, val loss: 0.52413\n",
      "Main effects tuning epoch: 24, train loss: 0.52142, val loss: 0.52711\n",
      "Main effects tuning epoch: 25, train loss: 0.52148, val loss: 0.52397\n",
      "Main effects tuning epoch: 26, train loss: 0.52107, val loss: 0.52555\n",
      "Main effects tuning epoch: 27, train loss: 0.52111, val loss: 0.52452\n",
      "Main effects tuning epoch: 28, train loss: 0.52053, val loss: 0.52521\n",
      "Main effects tuning epoch: 29, train loss: 0.52072, val loss: 0.52479\n",
      "Main effects tuning epoch: 30, train loss: 0.52073, val loss: 0.52433\n",
      "Main effects tuning epoch: 31, train loss: 0.52048, val loss: 0.52465\n",
      "Main effects tuning epoch: 32, train loss: 0.52045, val loss: 0.52508\n",
      "Main effects tuning epoch: 33, train loss: 0.52056, val loss: 0.52461\n",
      "Main effects tuning epoch: 34, train loss: 0.52079, val loss: 0.52415\n",
      "Main effects tuning epoch: 35, train loss: 0.52078, val loss: 0.52546\n",
      "Main effects tuning epoch: 36, train loss: 0.52053, val loss: 0.52491\n",
      "Main effects tuning epoch: 37, train loss: 0.52050, val loss: 0.52508\n",
      "Main effects tuning epoch: 38, train loss: 0.52036, val loss: 0.52379\n",
      "Main effects tuning epoch: 39, train loss: 0.52065, val loss: 0.52489\n",
      "Main effects tuning epoch: 40, train loss: 0.52041, val loss: 0.52504\n",
      "Main effects tuning epoch: 41, train loss: 0.52043, val loss: 0.52483\n",
      "Main effects tuning epoch: 42, train loss: 0.52040, val loss: 0.52434\n",
      "Main effects tuning epoch: 43, train loss: 0.52020, val loss: 0.52404\n",
      "Main effects tuning epoch: 44, train loss: 0.52016, val loss: 0.52480\n",
      "Main effects tuning epoch: 45, train loss: 0.52008, val loss: 0.52434\n",
      "Main effects tuning epoch: 46, train loss: 0.52030, val loss: 0.52395\n",
      "Main effects tuning epoch: 47, train loss: 0.52004, val loss: 0.52389\n",
      "Main effects tuning epoch: 48, train loss: 0.52046, val loss: 0.52501\n",
      "Main effects tuning epoch: 49, train loss: 0.52029, val loss: 0.52435\n",
      "Main effects tuning epoch: 50, train loss: 0.52030, val loss: 0.52490\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.45204, val loss: 0.45395\n",
      "Interaction training epoch: 2, train loss: 0.37282, val loss: 0.38496\n",
      "Interaction training epoch: 3, train loss: 0.30659, val loss: 0.31461\n",
      "Interaction training epoch: 4, train loss: 0.32583, val loss: 0.34408\n",
      "Interaction training epoch: 5, train loss: 0.30009, val loss: 0.31767\n",
      "Interaction training epoch: 6, train loss: 0.29098, val loss: 0.30780\n",
      "Interaction training epoch: 7, train loss: 0.29071, val loss: 0.30792\n",
      "Interaction training epoch: 8, train loss: 0.29199, val loss: 0.30897\n",
      "Interaction training epoch: 9, train loss: 0.29180, val loss: 0.30909\n",
      "Interaction training epoch: 10, train loss: 0.29743, val loss: 0.31280\n",
      "Interaction training epoch: 11, train loss: 0.29236, val loss: 0.30901\n",
      "Interaction training epoch: 12, train loss: 0.28835, val loss: 0.30990\n",
      "Interaction training epoch: 13, train loss: 0.28071, val loss: 0.29636\n",
      "Interaction training epoch: 14, train loss: 0.28513, val loss: 0.30455\n",
      "Interaction training epoch: 15, train loss: 0.28207, val loss: 0.30168\n",
      "Interaction training epoch: 16, train loss: 0.28197, val loss: 0.30140\n",
      "Interaction training epoch: 17, train loss: 0.28651, val loss: 0.30349\n",
      "Interaction training epoch: 18, train loss: 0.28385, val loss: 0.30595\n",
      "Interaction training epoch: 19, train loss: 0.28186, val loss: 0.30247\n",
      "Interaction training epoch: 20, train loss: 0.27859, val loss: 0.29816\n",
      "Interaction training epoch: 21, train loss: 0.28140, val loss: 0.30248\n",
      "Interaction training epoch: 22, train loss: 0.28398, val loss: 0.30572\n",
      "Interaction training epoch: 23, train loss: 0.27984, val loss: 0.29905\n",
      "Interaction training epoch: 24, train loss: 0.28206, val loss: 0.30167\n",
      "Interaction training epoch: 25, train loss: 0.27933, val loss: 0.30118\n",
      "Interaction training epoch: 26, train loss: 0.28149, val loss: 0.30405\n",
      "Interaction training epoch: 27, train loss: 0.27974, val loss: 0.29985\n",
      "Interaction training epoch: 28, train loss: 0.27896, val loss: 0.30160\n",
      "Interaction training epoch: 29, train loss: 0.27551, val loss: 0.29647\n",
      "Interaction training epoch: 30, train loss: 0.27770, val loss: 0.29798\n",
      "Interaction training epoch: 31, train loss: 0.28090, val loss: 0.30412\n",
      "Interaction training epoch: 32, train loss: 0.27902, val loss: 0.30454\n",
      "Interaction training epoch: 33, train loss: 0.27663, val loss: 0.29839\n",
      "Interaction training epoch: 34, train loss: 0.27986, val loss: 0.30066\n",
      "Interaction training epoch: 35, train loss: 0.27561, val loss: 0.29829\n",
      "Interaction training epoch: 36, train loss: 0.27673, val loss: 0.29616\n",
      "Interaction training epoch: 37, train loss: 0.27953, val loss: 0.30224\n",
      "Interaction training epoch: 38, train loss: 0.27362, val loss: 0.29742\n",
      "Interaction training epoch: 39, train loss: 0.27903, val loss: 0.30261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 40, train loss: 0.27686, val loss: 0.29861\n",
      "Interaction training epoch: 41, train loss: 0.27562, val loss: 0.29511\n",
      "Interaction training epoch: 42, train loss: 0.27596, val loss: 0.29967\n",
      "Interaction training epoch: 43, train loss: 0.27719, val loss: 0.29824\n",
      "Interaction training epoch: 44, train loss: 0.27910, val loss: 0.29944\n",
      "Interaction training epoch: 45, train loss: 0.27265, val loss: 0.29264\n",
      "Interaction training epoch: 46, train loss: 0.27612, val loss: 0.29783\n",
      "Interaction training epoch: 47, train loss: 0.27424, val loss: 0.29661\n",
      "Interaction training epoch: 48, train loss: 0.27348, val loss: 0.29460\n",
      "Interaction training epoch: 49, train loss: 0.27283, val loss: 0.29695\n",
      "Interaction training epoch: 50, train loss: 0.27328, val loss: 0.29388\n",
      "Interaction training epoch: 51, train loss: 0.27029, val loss: 0.29284\n",
      "Interaction training epoch: 52, train loss: 0.27868, val loss: 0.30079\n",
      "Interaction training epoch: 53, train loss: 0.26909, val loss: 0.29218\n",
      "Interaction training epoch: 54, train loss: 0.27631, val loss: 0.29812\n",
      "Interaction training epoch: 55, train loss: 0.27924, val loss: 0.30198\n",
      "Interaction training epoch: 56, train loss: 0.27129, val loss: 0.29363\n",
      "Interaction training epoch: 57, train loss: 0.27475, val loss: 0.29845\n",
      "Interaction training epoch: 58, train loss: 0.27223, val loss: 0.29578\n",
      "Interaction training epoch: 59, train loss: 0.27037, val loss: 0.29252\n",
      "Interaction training epoch: 60, train loss: 0.27018, val loss: 0.29273\n",
      "Interaction training epoch: 61, train loss: 0.27332, val loss: 0.29772\n",
      "Interaction training epoch: 62, train loss: 0.27043, val loss: 0.29555\n",
      "Interaction training epoch: 63, train loss: 0.27124, val loss: 0.29698\n",
      "Interaction training epoch: 64, train loss: 0.26587, val loss: 0.29014\n",
      "Interaction training epoch: 65, train loss: 0.27291, val loss: 0.29486\n",
      "Interaction training epoch: 66, train loss: 0.26870, val loss: 0.29716\n",
      "Interaction training epoch: 67, train loss: 0.27162, val loss: 0.29260\n",
      "Interaction training epoch: 68, train loss: 0.26836, val loss: 0.29479\n",
      "Interaction training epoch: 69, train loss: 0.26955, val loss: 0.29280\n",
      "Interaction training epoch: 70, train loss: 0.26729, val loss: 0.29282\n",
      "Interaction training epoch: 71, train loss: 0.26627, val loss: 0.29129\n",
      "Interaction training epoch: 72, train loss: 0.26560, val loss: 0.29124\n",
      "Interaction training epoch: 73, train loss: 0.27474, val loss: 0.29679\n",
      "Interaction training epoch: 74, train loss: 0.26722, val loss: 0.29468\n",
      "Interaction training epoch: 75, train loss: 0.27207, val loss: 0.30030\n",
      "Interaction training epoch: 76, train loss: 0.26922, val loss: 0.29247\n",
      "Interaction training epoch: 77, train loss: 0.26513, val loss: 0.29331\n",
      "Interaction training epoch: 78, train loss: 0.26705, val loss: 0.29621\n",
      "Interaction training epoch: 79, train loss: 0.26397, val loss: 0.28989\n",
      "Interaction training epoch: 80, train loss: 0.26584, val loss: 0.29317\n",
      "Interaction training epoch: 81, train loss: 0.26496, val loss: 0.29413\n",
      "Interaction training epoch: 82, train loss: 0.26500, val loss: 0.29254\n",
      "Interaction training epoch: 83, train loss: 0.26507, val loss: 0.29004\n",
      "Interaction training epoch: 84, train loss: 0.26838, val loss: 0.29611\n",
      "Interaction training epoch: 85, train loss: 0.26540, val loss: 0.29570\n",
      "Interaction training epoch: 86, train loss: 0.26154, val loss: 0.28731\n",
      "Interaction training epoch: 87, train loss: 0.26493, val loss: 0.29456\n",
      "Interaction training epoch: 88, train loss: 0.26409, val loss: 0.29138\n",
      "Interaction training epoch: 89, train loss: 0.26009, val loss: 0.28726\n",
      "Interaction training epoch: 90, train loss: 0.26628, val loss: 0.29190\n",
      "Interaction training epoch: 91, train loss: 0.26028, val loss: 0.28843\n",
      "Interaction training epoch: 92, train loss: 0.26211, val loss: 0.29053\n",
      "Interaction training epoch: 93, train loss: 0.26226, val loss: 0.29106\n",
      "Interaction training epoch: 94, train loss: 0.26282, val loss: 0.28963\n",
      "Interaction training epoch: 95, train loss: 0.26222, val loss: 0.29190\n",
      "Interaction training epoch: 96, train loss: 0.26184, val loss: 0.28672\n",
      "Interaction training epoch: 97, train loss: 0.26543, val loss: 0.29279\n",
      "Interaction training epoch: 98, train loss: 0.25919, val loss: 0.28753\n",
      "Interaction training epoch: 99, train loss: 0.26539, val loss: 0.29367\n",
      "Interaction training epoch: 100, train loss: 0.26059, val loss: 0.28999\n",
      "Interaction training epoch: 101, train loss: 0.26087, val loss: 0.28736\n",
      "Interaction training epoch: 102, train loss: 0.25916, val loss: 0.28858\n",
      "Interaction training epoch: 103, train loss: 0.25912, val loss: 0.28772\n",
      "Interaction training epoch: 104, train loss: 0.25772, val loss: 0.28759\n",
      "Interaction training epoch: 105, train loss: 0.25955, val loss: 0.28802\n",
      "Interaction training epoch: 106, train loss: 0.25805, val loss: 0.28966\n",
      "Interaction training epoch: 107, train loss: 0.25951, val loss: 0.28987\n",
      "Interaction training epoch: 108, train loss: 0.25781, val loss: 0.28544\n",
      "Interaction training epoch: 109, train loss: 0.25890, val loss: 0.28847\n",
      "Interaction training epoch: 110, train loss: 0.25858, val loss: 0.29143\n",
      "Interaction training epoch: 111, train loss: 0.25863, val loss: 0.28769\n",
      "Interaction training epoch: 112, train loss: 0.25921, val loss: 0.29177\n",
      "Interaction training epoch: 113, train loss: 0.25905, val loss: 0.29416\n",
      "Interaction training epoch: 114, train loss: 0.25915, val loss: 0.28658\n",
      "Interaction training epoch: 115, train loss: 0.25892, val loss: 0.29107\n",
      "Interaction training epoch: 116, train loss: 0.25517, val loss: 0.28903\n",
      "Interaction training epoch: 117, train loss: 0.25726, val loss: 0.29027\n",
      "Interaction training epoch: 118, train loss: 0.25545, val loss: 0.28758\n",
      "Interaction training epoch: 119, train loss: 0.25741, val loss: 0.28840\n",
      "Interaction training epoch: 120, train loss: 0.25868, val loss: 0.29283\n",
      "Interaction training epoch: 121, train loss: 0.25663, val loss: 0.29005\n",
      "Interaction training epoch: 122, train loss: 0.25640, val loss: 0.28713\n",
      "Interaction training epoch: 123, train loss: 0.25320, val loss: 0.28731\n",
      "Interaction training epoch: 124, train loss: 0.25729, val loss: 0.29017\n",
      "Interaction training epoch: 125, train loss: 0.25177, val loss: 0.28471\n",
      "Interaction training epoch: 126, train loss: 0.25461, val loss: 0.28814\n",
      "Interaction training epoch: 127, train loss: 0.25397, val loss: 0.29123\n",
      "Interaction training epoch: 128, train loss: 0.25180, val loss: 0.28406\n",
      "Interaction training epoch: 129, train loss: 0.25340, val loss: 0.28747\n",
      "Interaction training epoch: 130, train loss: 0.25263, val loss: 0.28868\n",
      "Interaction training epoch: 131, train loss: 0.25390, val loss: 0.28839\n",
      "Interaction training epoch: 132, train loss: 0.25242, val loss: 0.28553\n",
      "Interaction training epoch: 133, train loss: 0.25422, val loss: 0.28996\n",
      "Interaction training epoch: 134, train loss: 0.25107, val loss: 0.28724\n",
      "Interaction training epoch: 135, train loss: 0.25138, val loss: 0.28680\n",
      "Interaction training epoch: 136, train loss: 0.25082, val loss: 0.28578\n",
      "Interaction training epoch: 137, train loss: 0.24831, val loss: 0.28034\n",
      "Interaction training epoch: 138, train loss: 0.25391, val loss: 0.29396\n",
      "Interaction training epoch: 139, train loss: 0.25162, val loss: 0.29169\n",
      "Interaction training epoch: 140, train loss: 0.25173, val loss: 0.28311\n",
      "Interaction training epoch: 141, train loss: 0.25014, val loss: 0.28851\n",
      "Interaction training epoch: 142, train loss: 0.25183, val loss: 0.28778\n",
      "Interaction training epoch: 143, train loss: 0.24976, val loss: 0.28615\n",
      "Interaction training epoch: 144, train loss: 0.25083, val loss: 0.28719\n",
      "Interaction training epoch: 145, train loss: 0.25137, val loss: 0.28881\n",
      "Interaction training epoch: 146, train loss: 0.24768, val loss: 0.28231\n",
      "Interaction training epoch: 147, train loss: 0.25094, val loss: 0.28971\n",
      "Interaction training epoch: 148, train loss: 0.25132, val loss: 0.29174\n",
      "Interaction training epoch: 149, train loss: 0.24692, val loss: 0.28043\n",
      "Interaction training epoch: 150, train loss: 0.25312, val loss: 0.29276\n",
      "Interaction training epoch: 151, train loss: 0.24841, val loss: 0.28813\n",
      "Interaction training epoch: 152, train loss: 0.24972, val loss: 0.28316\n",
      "Interaction training epoch: 153, train loss: 0.24894, val loss: 0.29079\n",
      "Interaction training epoch: 154, train loss: 0.24961, val loss: 0.28810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 155, train loss: 0.25012, val loss: 0.28780\n",
      "Interaction training epoch: 156, train loss: 0.24684, val loss: 0.28653\n",
      "Interaction training epoch: 157, train loss: 0.24968, val loss: 0.28723\n",
      "Interaction training epoch: 158, train loss: 0.24758, val loss: 0.28293\n",
      "Interaction training epoch: 159, train loss: 0.25349, val loss: 0.29141\n",
      "Interaction training epoch: 160, train loss: 0.24694, val loss: 0.28752\n",
      "Interaction training epoch: 161, train loss: 0.24893, val loss: 0.29001\n",
      "Interaction training epoch: 162, train loss: 0.24834, val loss: 0.28421\n",
      "Interaction training epoch: 163, train loss: 0.24636, val loss: 0.28154\n",
      "Interaction training epoch: 164, train loss: 0.24619, val loss: 0.28455\n",
      "Interaction training epoch: 165, train loss: 0.24732, val loss: 0.28835\n",
      "Interaction training epoch: 166, train loss: 0.24590, val loss: 0.28247\n",
      "Interaction training epoch: 167, train loss: 0.24533, val loss: 0.28652\n",
      "Interaction training epoch: 168, train loss: 0.24630, val loss: 0.28310\n",
      "Interaction training epoch: 169, train loss: 0.24633, val loss: 0.28852\n",
      "Interaction training epoch: 170, train loss: 0.24578, val loss: 0.28307\n",
      "Interaction training epoch: 171, train loss: 0.24656, val loss: 0.28332\n",
      "Interaction training epoch: 172, train loss: 0.24511, val loss: 0.28174\n",
      "Interaction training epoch: 173, train loss: 0.24490, val loss: 0.28471\n",
      "Interaction training epoch: 174, train loss: 0.24508, val loss: 0.28186\n",
      "Interaction training epoch: 175, train loss: 0.24463, val loss: 0.28516\n",
      "Interaction training epoch: 176, train loss: 0.24465, val loss: 0.28565\n",
      "Interaction training epoch: 177, train loss: 0.24528, val loss: 0.28648\n",
      "Interaction training epoch: 178, train loss: 0.24311, val loss: 0.28243\n",
      "Interaction training epoch: 179, train loss: 0.24319, val loss: 0.28510\n",
      "Interaction training epoch: 180, train loss: 0.24391, val loss: 0.28393\n",
      "Interaction training epoch: 181, train loss: 0.24319, val loss: 0.28256\n",
      "Interaction training epoch: 182, train loss: 0.24058, val loss: 0.28193\n",
      "Interaction training epoch: 183, train loss: 0.24722, val loss: 0.28832\n",
      "Interaction training epoch: 184, train loss: 0.24257, val loss: 0.28091\n",
      "Interaction training epoch: 185, train loss: 0.24084, val loss: 0.28158\n",
      "Interaction training epoch: 186, train loss: 0.24321, val loss: 0.28278\n",
      "Interaction training epoch: 187, train loss: 0.24303, val loss: 0.28575\n",
      "Interaction training epoch: 188, train loss: 0.24153, val loss: 0.28257\n",
      "Interaction training epoch: 189, train loss: 0.24501, val loss: 0.28640\n",
      "Interaction training epoch: 190, train loss: 0.24050, val loss: 0.28001\n",
      "Interaction training epoch: 191, train loss: 0.24200, val loss: 0.28205\n",
      "Interaction training epoch: 192, train loss: 0.24272, val loss: 0.28272\n",
      "Interaction training epoch: 193, train loss: 0.24074, val loss: 0.27962\n",
      "Interaction training epoch: 194, train loss: 0.24007, val loss: 0.28076\n",
      "Interaction training epoch: 195, train loss: 0.24048, val loss: 0.27784\n",
      "Interaction training epoch: 196, train loss: 0.24201, val loss: 0.28130\n",
      "Interaction training epoch: 197, train loss: 0.24100, val loss: 0.28213\n",
      "Interaction training epoch: 198, train loss: 0.24075, val loss: 0.28105\n",
      "Interaction training epoch: 199, train loss: 0.24256, val loss: 0.28305\n",
      "Interaction training epoch: 200, train loss: 0.23828, val loss: 0.27869\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########2 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.24820, val loss: 0.28686\n",
      "Interaction tuning epoch: 2, train loss: 0.24350, val loss: 0.27825\n",
      "Interaction tuning epoch: 3, train loss: 0.24495, val loss: 0.27844\n",
      "Interaction tuning epoch: 4, train loss: 0.24465, val loss: 0.27821\n",
      "Interaction tuning epoch: 5, train loss: 0.24469, val loss: 0.27809\n",
      "Interaction tuning epoch: 6, train loss: 0.24361, val loss: 0.27788\n",
      "Interaction tuning epoch: 7, train loss: 0.24731, val loss: 0.28149\n",
      "Interaction tuning epoch: 8, train loss: 0.24206, val loss: 0.27625\n",
      "Interaction tuning epoch: 9, train loss: 0.24389, val loss: 0.27896\n",
      "Interaction tuning epoch: 10, train loss: 0.24349, val loss: 0.27962\n",
      "Interaction tuning epoch: 11, train loss: 0.24398, val loss: 0.27295\n",
      "Interaction tuning epoch: 12, train loss: 0.24441, val loss: 0.27684\n",
      "Interaction tuning epoch: 13, train loss: 0.24334, val loss: 0.27744\n",
      "Interaction tuning epoch: 14, train loss: 0.24809, val loss: 0.28152\n",
      "Interaction tuning epoch: 15, train loss: 0.24179, val loss: 0.27720\n",
      "Interaction tuning epoch: 16, train loss: 0.24143, val loss: 0.27433\n",
      "Interaction tuning epoch: 17, train loss: 0.24201, val loss: 0.27956\n",
      "Interaction tuning epoch: 18, train loss: 0.24168, val loss: 0.27551\n",
      "Interaction tuning epoch: 19, train loss: 0.24144, val loss: 0.27545\n",
      "Interaction tuning epoch: 20, train loss: 0.24270, val loss: 0.27511\n",
      "Interaction tuning epoch: 21, train loss: 0.24018, val loss: 0.27163\n",
      "Interaction tuning epoch: 22, train loss: 0.23999, val loss: 0.27522\n",
      "Interaction tuning epoch: 23, train loss: 0.24181, val loss: 0.27809\n",
      "Interaction tuning epoch: 24, train loss: 0.24220, val loss: 0.27599\n",
      "Interaction tuning epoch: 25, train loss: 0.24341, val loss: 0.27804\n",
      "Interaction tuning epoch: 26, train loss: 0.24430, val loss: 0.27799\n",
      "Interaction tuning epoch: 27, train loss: 0.24250, val loss: 0.27597\n",
      "Interaction tuning epoch: 28, train loss: 0.24169, val loss: 0.27732\n",
      "Interaction tuning epoch: 29, train loss: 0.24073, val loss: 0.27366\n",
      "Interaction tuning epoch: 30, train loss: 0.24090, val loss: 0.27365\n",
      "Interaction tuning epoch: 31, train loss: 0.24012, val loss: 0.27605\n",
      "Interaction tuning epoch: 32, train loss: 0.24157, val loss: 0.27605\n",
      "Interaction tuning epoch: 33, train loss: 0.24022, val loss: 0.27602\n",
      "Interaction tuning epoch: 34, train loss: 0.24250, val loss: 0.27615\n",
      "Interaction tuning epoch: 35, train loss: 0.23801, val loss: 0.27373\n",
      "Interaction tuning epoch: 36, train loss: 0.23925, val loss: 0.27367\n",
      "Interaction tuning epoch: 37, train loss: 0.23858, val loss: 0.27438\n",
      "Interaction tuning epoch: 38, train loss: 0.23872, val loss: 0.26968\n",
      "Interaction tuning epoch: 39, train loss: 0.24110, val loss: 0.27556\n",
      "Interaction tuning epoch: 40, train loss: 0.24296, val loss: 0.27745\n",
      "Interaction tuning epoch: 41, train loss: 0.24450, val loss: 0.27920\n",
      "Interaction tuning epoch: 42, train loss: 0.24322, val loss: 0.28132\n",
      "Interaction tuning epoch: 43, train loss: 0.24094, val loss: 0.27343\n",
      "Interaction tuning epoch: 44, train loss: 0.23906, val loss: 0.27102\n",
      "Interaction tuning epoch: 45, train loss: 0.23996, val loss: 0.27727\n",
      "Interaction tuning epoch: 46, train loss: 0.24029, val loss: 0.27430\n",
      "Interaction tuning epoch: 47, train loss: 0.24032, val loss: 0.27424\n",
      "Interaction tuning epoch: 48, train loss: 0.24024, val loss: 0.27578\n",
      "Interaction tuning epoch: 49, train loss: 0.23966, val loss: 0.27597\n",
      "Interaction tuning epoch: 50, train loss: 0.24012, val loss: 0.27763\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 64.20892786979675\n",
      "After the gam stage, training error is 0.24012 , validation error is 0.27763\n",
      "missing value counts: 99269\n",
      "#####start auto_tuning#####\n",
      "the best shrinkage is 0.656250\n",
      "[SoftImpute] Max Singular Value of X_init = 3.682169\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.219046 validation BCE=0.292847,rank=3\n",
      "[SoftImpute] Iter 1: observed BCE=0.218594 validation BCE=0.271833,rank=3\n",
      "[SoftImpute] Iter 2: observed BCE=0.218670 validation BCE=0.271453,rank=3\n",
      "[SoftImpute] Iter 3: observed BCE=0.218473 validation BCE=0.268602,rank=3\n",
      "[SoftImpute] Iter 4: observed BCE=0.217680 validation BCE=0.267479,rank=3\n",
      "[SoftImpute] Iter 5: observed BCE=0.216914 validation BCE=0.266176,rank=3\n",
      "[SoftImpute] Iter 6: observed BCE=0.216802 validation BCE=0.266338,rank=3\n",
      "[SoftImpute] Iter 7: observed BCE=0.216201 validation BCE=0.265429,rank=3\n",
      "[SoftImpute] Iter 8: observed BCE=0.215762 validation BCE=0.264533,rank=3\n",
      "[SoftImpute] Iter 9: observed BCE=0.215558 validation BCE=0.264670,rank=3\n",
      "[SoftImpute] Iter 10: observed BCE=0.215337 validation BCE=0.264291,rank=3\n",
      "[SoftImpute] Iter 11: observed BCE=0.215361 validation BCE=0.263496,rank=3\n",
      "[SoftImpute] Iter 12: observed BCE=0.215142 validation BCE=0.263823,rank=3\n",
      "[SoftImpute] Iter 13: observed BCE=0.215302 validation BCE=0.264073,rank=3\n",
      "[SoftImpute] Iter 14: observed BCE=0.215299 validation BCE=0.263859,rank=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 15: observed BCE=0.216053 validation BCE=0.264312,rank=3\n",
      "[SoftImpute] Iter 16: observed BCE=0.216015 validation BCE=0.264853,rank=3\n",
      "[SoftImpute] Iter 17: observed BCE=0.216176 validation BCE=0.264399,rank=3\n",
      "[SoftImpute] Iter 18: observed BCE=0.215868 validation BCE=0.264940,rank=3\n",
      "[SoftImpute] Iter 19: observed BCE=0.215812 validation BCE=0.264928,rank=3\n",
      "[SoftImpute] Iter 20: observed BCE=0.215976 validation BCE=0.264815,rank=3\n",
      "[SoftImpute] Iter 21: observed BCE=0.215935 validation BCE=0.264948,rank=3\n",
      "[SoftImpute] Iter 22: observed BCE=0.215660 validation BCE=0.265538,rank=3\n",
      "[SoftImpute] Iter 23: observed BCE=0.215678 validation BCE=0.264895,rank=3\n",
      "[SoftImpute] Iter 24: observed BCE=0.215601 validation BCE=0.265142,rank=3\n",
      "[SoftImpute] Iter 25: observed BCE=0.215790 validation BCE=0.266037,rank=3\n",
      "[SoftImpute] Iter 26: observed BCE=0.215619 validation BCE=0.266850,rank=3\n",
      "[SoftImpute] Iter 27: observed BCE=0.215504 validation BCE=0.265731,rank=3\n",
      "[SoftImpute] Iter 28: observed BCE=0.215555 validation BCE=0.266240,rank=3\n",
      "[SoftImpute] Iter 29: observed BCE=0.215843 validation BCE=0.267203,rank=3\n",
      "[SoftImpute] Iter 30: observed BCE=0.215625 validation BCE=0.266463,rank=3\n",
      "[SoftImpute] Iter 31: observed BCE=0.215803 validation BCE=0.267580,rank=3\n",
      "[SoftImpute] Iter 32: observed BCE=0.215641 validation BCE=0.277973,rank=3\n",
      "[SoftImpute] Iter 33: observed BCE=0.215729 validation BCE=0.267375,rank=3\n",
      "[SoftImpute] Iter 34: observed BCE=0.215971 validation BCE=0.268845,rank=3\n",
      "[SoftImpute] Iter 35: observed BCE=0.215626 validation BCE=0.278305,rank=3\n",
      "[SoftImpute] Iter 36: observed BCE=0.215297 validation BCE=0.278176,rank=3\n",
      "[SoftImpute] Iter 37: observed BCE=0.215185 validation BCE=0.278360,rank=3\n",
      "[SoftImpute] Iter 38: observed BCE=0.215279 validation BCE=0.268207,rank=3\n",
      "[SoftImpute] Iter 39: observed BCE=0.215306 validation BCE=0.268589,rank=3\n",
      "[SoftImpute] Iter 40: observed BCE=0.215016 validation BCE=0.269840,rank=3\n",
      "[SoftImpute] Iter 41: observed BCE=0.214816 validation BCE=0.278713,rank=3\n",
      "[SoftImpute] Iter 42: observed BCE=0.215221 validation BCE=0.278864,rank=3\n",
      "[SoftImpute] Iter 43: observed BCE=0.215462 validation BCE=0.279209,rank=3\n",
      "[SoftImpute] Iter 44: observed BCE=0.215225 validation BCE=0.267843,rank=3\n",
      "[SoftImpute] Iter 45: observed BCE=0.215030 validation BCE=0.268573,rank=3\n",
      "[SoftImpute] Iter 46: observed BCE=0.215125 validation BCE=0.269296,rank=3\n",
      "[SoftImpute] Iter 47: observed BCE=0.214927 validation BCE=0.278683,rank=3\n",
      "[SoftImpute] Iter 48: observed BCE=0.215045 validation BCE=0.279210,rank=3\n",
      "[SoftImpute] Iter 49: observed BCE=0.215137 validation BCE=0.279131,rank=3\n",
      "[SoftImpute] Iter 50: observed BCE=0.215238 validation BCE=0.268151,rank=3\n",
      "[SoftImpute] Iter 51: observed BCE=0.214868 validation BCE=0.268778,rank=3\n",
      "[SoftImpute] Iter 52: observed BCE=0.215016 validation BCE=0.269045,rank=3\n",
      "[SoftImpute] Iter 53: observed BCE=0.214602 validation BCE=0.272281,rank=3\n",
      "[SoftImpute] Iter 54: observed BCE=0.214820 validation BCE=0.272684,rank=3\n",
      "[SoftImpute] Iter 55: observed BCE=0.215039 validation BCE=0.279398,rank=3\n",
      "[SoftImpute] Iter 56: observed BCE=0.215157 validation BCE=0.267869,rank=3\n",
      "[SoftImpute] Iter 57: observed BCE=0.215097 validation BCE=0.268025,rank=3\n",
      "[SoftImpute] Iter 58: observed BCE=0.215093 validation BCE=0.268708,rank=3\n",
      "[SoftImpute] Iter 59: observed BCE=0.214958 validation BCE=0.270333,rank=3\n",
      "[SoftImpute] Iter 60: observed BCE=0.215250 validation BCE=0.270513,rank=3\n",
      "[SoftImpute] Iter 61: observed BCE=0.214786 validation BCE=0.278557,rank=3\n",
      "[SoftImpute] Iter 62: observed BCE=0.214963 validation BCE=0.278994,rank=3\n",
      "[SoftImpute] Iter 63: observed BCE=0.214892 validation BCE=0.279696,rank=3\n",
      "[SoftImpute] Iter 64: observed BCE=0.214997 validation BCE=0.267685,rank=3\n",
      "[SoftImpute] Iter 65: observed BCE=0.214712 validation BCE=0.268434,rank=3\n",
      "[SoftImpute] Iter 66: observed BCE=0.214584 validation BCE=0.269517,rank=3\n",
      "[SoftImpute] Iter 67: observed BCE=0.214656 validation BCE=0.269055,rank=3\n",
      "[SoftImpute] Iter 68: observed BCE=0.214927 validation BCE=0.270414,rank=3\n",
      "[SoftImpute] Iter 69: observed BCE=0.214445 validation BCE=0.269894,rank=3\n",
      "[SoftImpute] Iter 70: observed BCE=0.214652 validation BCE=0.270461,rank=3\n",
      "[SoftImpute] Iter 71: observed BCE=0.214750 validation BCE=0.279436,rank=3\n",
      "[SoftImpute] Iter 72: observed BCE=0.214901 validation BCE=0.268246,rank=3\n",
      "[SoftImpute] Iter 73: observed BCE=0.215057 validation BCE=0.268274,rank=3\n",
      "[SoftImpute] Iter 74: observed BCE=0.214932 validation BCE=0.268267,rank=3\n",
      "[SoftImpute] Iter 75: observed BCE=0.214909 validation BCE=0.268995,rank=3\n",
      "[SoftImpute] Iter 76: observed BCE=0.214706 validation BCE=0.269260,rank=3\n",
      "[SoftImpute] Iter 77: observed BCE=0.214613 validation BCE=0.270238,rank=3\n",
      "[SoftImpute] Iter 78: observed BCE=0.214842 validation BCE=0.269565,rank=3\n",
      "[SoftImpute] Iter 79: observed BCE=0.214666 validation BCE=0.269714,rank=3\n",
      "[SoftImpute] Iter 80: observed BCE=0.214245 validation BCE=0.271160,rank=3\n",
      "[SoftImpute] Iter 81: observed BCE=0.214871 validation BCE=0.267798,rank=3\n",
      "[SoftImpute] Iter 82: observed BCE=0.214590 validation BCE=0.268232,rank=3\n",
      "[SoftImpute] Iter 83: observed BCE=0.214632 validation BCE=0.268646,rank=3\n",
      "[SoftImpute] Iter 84: observed BCE=0.214604 validation BCE=0.268910,rank=3\n",
      "[SoftImpute] Iter 85: observed BCE=0.214672 validation BCE=0.268732,rank=3\n",
      "[SoftImpute] Iter 86: observed BCE=0.214413 validation BCE=0.269380,rank=3\n",
      "[SoftImpute] Iter 87: observed BCE=0.214526 validation BCE=0.268835,rank=3\n",
      "[SoftImpute] Iter 88: observed BCE=0.214433 validation BCE=0.268981,rank=3\n",
      "[SoftImpute] Iter 89: observed BCE=0.214510 validation BCE=0.269428,rank=3\n",
      "[SoftImpute] Iter 90: observed BCE=0.214406 validation BCE=0.268907,rank=3\n",
      "[SoftImpute] Iter 91: observed BCE=0.214767 validation BCE=0.269074,rank=3\n",
      "[SoftImpute] Iter 92: observed BCE=0.214735 validation BCE=0.269822,rank=3\n",
      "[SoftImpute] Iter 93: observed BCE=0.214882 validation BCE=0.267595,rank=3\n",
      "[SoftImpute] Iter 94: observed BCE=0.214715 validation BCE=0.267658,rank=3\n",
      "[SoftImpute] Iter 95: observed BCE=0.214546 validation BCE=0.268262,rank=3\n",
      "[SoftImpute] Iter 96: observed BCE=0.214735 validation BCE=0.267786,rank=3\n",
      "[SoftImpute] Iter 97: observed BCE=0.214649 validation BCE=0.267943,rank=3\n",
      "[SoftImpute] Iter 98: observed BCE=0.214550 validation BCE=0.268803,rank=3\n",
      "[SoftImpute] Iter 99: observed BCE=0.214912 validation BCE=0.268554,rank=3\n",
      "[SoftImpute] Iter 100: observed BCE=0.214759 validation BCE=0.267931,rank=3\n",
      "[SoftImpute] Stopped after iteration 100 for lambda=0.073643\n",
      "final num of user group: 8\n",
      "final num of item group: 9\n",
      "change mode state : True\n",
      "time cost: 28.498278617858887\n",
      "After the matrix factor stage, training error is 0.21476, validation error is 0.26793\n",
      "8\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68444, val loss: 0.68654\n",
      "Main effects training epoch: 2, train loss: 0.67673, val loss: 0.68007\n",
      "Main effects training epoch: 3, train loss: 0.67252, val loss: 0.67613\n",
      "Main effects training epoch: 4, train loss: 0.66677, val loss: 0.66980\n",
      "Main effects training epoch: 5, train loss: 0.65588, val loss: 0.65821\n",
      "Main effects training epoch: 6, train loss: 0.63060, val loss: 0.63187\n",
      "Main effects training epoch: 7, train loss: 0.58279, val loss: 0.58270\n",
      "Main effects training epoch: 8, train loss: 0.53858, val loss: 0.53955\n",
      "Main effects training epoch: 9, train loss: 0.53685, val loss: 0.54538\n",
      "Main effects training epoch: 10, train loss: 0.53480, val loss: 0.53561\n",
      "Main effects training epoch: 11, train loss: 0.52952, val loss: 0.53262\n",
      "Main effects training epoch: 12, train loss: 0.52895, val loss: 0.52717\n",
      "Main effects training epoch: 13, train loss: 0.52833, val loss: 0.53114\n",
      "Main effects training epoch: 14, train loss: 0.52701, val loss: 0.52759\n",
      "Main effects training epoch: 15, train loss: 0.52751, val loss: 0.52959\n",
      "Main effects training epoch: 16, train loss: 0.52710, val loss: 0.52922\n",
      "Main effects training epoch: 17, train loss: 0.52670, val loss: 0.52682\n",
      "Main effects training epoch: 18, train loss: 0.52702, val loss: 0.52810\n",
      "Main effects training epoch: 19, train loss: 0.52700, val loss: 0.52745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 20, train loss: 0.52679, val loss: 0.52871\n",
      "Main effects training epoch: 21, train loss: 0.52662, val loss: 0.52842\n",
      "Main effects training epoch: 22, train loss: 0.52694, val loss: 0.52702\n",
      "Main effects training epoch: 23, train loss: 0.52700, val loss: 0.52873\n",
      "Main effects training epoch: 24, train loss: 0.52692, val loss: 0.52833\n",
      "Main effects training epoch: 25, train loss: 0.52638, val loss: 0.52796\n",
      "Main effects training epoch: 26, train loss: 0.52653, val loss: 0.52655\n",
      "Main effects training epoch: 27, train loss: 0.52691, val loss: 0.52909\n",
      "Main effects training epoch: 28, train loss: 0.52710, val loss: 0.52600\n",
      "Main effects training epoch: 29, train loss: 0.52708, val loss: 0.53002\n",
      "Main effects training epoch: 30, train loss: 0.52846, val loss: 0.52616\n",
      "Main effects training epoch: 31, train loss: 0.52705, val loss: 0.52977\n",
      "Main effects training epoch: 32, train loss: 0.52662, val loss: 0.52618\n",
      "Main effects training epoch: 33, train loss: 0.52619, val loss: 0.52678\n",
      "Main effects training epoch: 34, train loss: 0.52645, val loss: 0.52679\n",
      "Main effects training epoch: 35, train loss: 0.52679, val loss: 0.52856\n",
      "Main effects training epoch: 36, train loss: 0.52629, val loss: 0.52711\n",
      "Main effects training epoch: 37, train loss: 0.52643, val loss: 0.52774\n",
      "Main effects training epoch: 38, train loss: 0.52616, val loss: 0.52663\n",
      "Main effects training epoch: 39, train loss: 0.52613, val loss: 0.52652\n",
      "Main effects training epoch: 40, train loss: 0.52613, val loss: 0.52807\n",
      "Main effects training epoch: 41, train loss: 0.52623, val loss: 0.52585\n",
      "Main effects training epoch: 42, train loss: 0.52619, val loss: 0.52797\n",
      "Main effects training epoch: 43, train loss: 0.52618, val loss: 0.52685\n",
      "Main effects training epoch: 44, train loss: 0.52639, val loss: 0.52779\n",
      "Main effects training epoch: 45, train loss: 0.52593, val loss: 0.52682\n",
      "Main effects training epoch: 46, train loss: 0.52616, val loss: 0.52702\n",
      "Main effects training epoch: 47, train loss: 0.52664, val loss: 0.52672\n",
      "Main effects training epoch: 48, train loss: 0.52619, val loss: 0.52653\n",
      "Main effects training epoch: 49, train loss: 0.52605, val loss: 0.52732\n",
      "Main effects training epoch: 50, train loss: 0.52602, val loss: 0.52608\n",
      "Main effects training epoch: 51, train loss: 0.52601, val loss: 0.52700\n",
      "Main effects training epoch: 52, train loss: 0.52600, val loss: 0.52575\n",
      "Main effects training epoch: 53, train loss: 0.52611, val loss: 0.52716\n",
      "Main effects training epoch: 54, train loss: 0.52607, val loss: 0.52713\n",
      "Main effects training epoch: 55, train loss: 0.52595, val loss: 0.52781\n",
      "Main effects training epoch: 56, train loss: 0.52614, val loss: 0.52659\n",
      "Main effects training epoch: 57, train loss: 0.52590, val loss: 0.52730\n",
      "Main effects training epoch: 58, train loss: 0.52601, val loss: 0.52774\n",
      "Main effects training epoch: 59, train loss: 0.52571, val loss: 0.52643\n",
      "Main effects training epoch: 60, train loss: 0.52608, val loss: 0.52630\n",
      "Main effects training epoch: 61, train loss: 0.52610, val loss: 0.52558\n",
      "Main effects training epoch: 62, train loss: 0.52616, val loss: 0.52762\n",
      "Main effects training epoch: 63, train loss: 0.52585, val loss: 0.52628\n",
      "Main effects training epoch: 64, train loss: 0.52568, val loss: 0.52698\n",
      "Main effects training epoch: 65, train loss: 0.52581, val loss: 0.52765\n",
      "Main effects training epoch: 66, train loss: 0.52607, val loss: 0.52566\n",
      "Main effects training epoch: 67, train loss: 0.52648, val loss: 0.52897\n",
      "Main effects training epoch: 68, train loss: 0.52598, val loss: 0.52499\n",
      "Main effects training epoch: 69, train loss: 0.52645, val loss: 0.52820\n",
      "Main effects training epoch: 70, train loss: 0.52686, val loss: 0.52618\n",
      "Main effects training epoch: 71, train loss: 0.52635, val loss: 0.52832\n",
      "Main effects training epoch: 72, train loss: 0.52636, val loss: 0.52498\n",
      "Main effects training epoch: 73, train loss: 0.52583, val loss: 0.52809\n",
      "Main effects training epoch: 74, train loss: 0.52612, val loss: 0.52732\n",
      "Main effects training epoch: 75, train loss: 0.52584, val loss: 0.52553\n",
      "Main effects training epoch: 76, train loss: 0.52588, val loss: 0.52690\n",
      "Main effects training epoch: 77, train loss: 0.52611, val loss: 0.52535\n",
      "Main effects training epoch: 78, train loss: 0.52595, val loss: 0.52731\n",
      "Main effects training epoch: 79, train loss: 0.52554, val loss: 0.52692\n",
      "Main effects training epoch: 80, train loss: 0.52539, val loss: 0.52564\n",
      "Main effects training epoch: 81, train loss: 0.52539, val loss: 0.52691\n",
      "Main effects training epoch: 82, train loss: 0.52539, val loss: 0.52705\n",
      "Main effects training epoch: 83, train loss: 0.52539, val loss: 0.52592\n",
      "Main effects training epoch: 84, train loss: 0.52559, val loss: 0.52622\n",
      "Main effects training epoch: 85, train loss: 0.52564, val loss: 0.52567\n",
      "Main effects training epoch: 86, train loss: 0.52570, val loss: 0.52790\n",
      "Main effects training epoch: 87, train loss: 0.52549, val loss: 0.52570\n",
      "Main effects training epoch: 88, train loss: 0.52536, val loss: 0.52601\n",
      "Main effects training epoch: 89, train loss: 0.52538, val loss: 0.52594\n",
      "Main effects training epoch: 90, train loss: 0.52608, val loss: 0.52534\n",
      "Main effects training epoch: 91, train loss: 0.52577, val loss: 0.52751\n",
      "Main effects training epoch: 92, train loss: 0.52570, val loss: 0.52650\n",
      "Main effects training epoch: 93, train loss: 0.52548, val loss: 0.52731\n",
      "Main effects training epoch: 94, train loss: 0.52625, val loss: 0.52529\n",
      "Main effects training epoch: 95, train loss: 0.52571, val loss: 0.52790\n",
      "Main effects training epoch: 96, train loss: 0.52534, val loss: 0.52564\n",
      "Main effects training epoch: 97, train loss: 0.52526, val loss: 0.52546\n",
      "Main effects training epoch: 98, train loss: 0.52579, val loss: 0.52796\n",
      "Main effects training epoch: 99, train loss: 0.52584, val loss: 0.52591\n",
      "Main effects training epoch: 100, train loss: 0.52533, val loss: 0.52626\n",
      "Main effects training epoch: 101, train loss: 0.52529, val loss: 0.52647\n",
      "Main effects training epoch: 102, train loss: 0.52535, val loss: 0.52603\n",
      "Main effects training epoch: 103, train loss: 0.52503, val loss: 0.52507\n",
      "Main effects training epoch: 104, train loss: 0.52535, val loss: 0.52663\n",
      "Main effects training epoch: 105, train loss: 0.52515, val loss: 0.52742\n",
      "Main effects training epoch: 106, train loss: 0.52573, val loss: 0.52526\n",
      "Main effects training epoch: 107, train loss: 0.52533, val loss: 0.52704\n",
      "Main effects training epoch: 108, train loss: 0.52493, val loss: 0.52527\n",
      "Main effects training epoch: 109, train loss: 0.52501, val loss: 0.52602\n",
      "Main effects training epoch: 110, train loss: 0.52519, val loss: 0.52563\n",
      "Main effects training epoch: 111, train loss: 0.52492, val loss: 0.52622\n",
      "Main effects training epoch: 112, train loss: 0.52531, val loss: 0.52418\n",
      "Main effects training epoch: 113, train loss: 0.52507, val loss: 0.52653\n",
      "Main effects training epoch: 114, train loss: 0.52525, val loss: 0.52634\n",
      "Main effects training epoch: 115, train loss: 0.52520, val loss: 0.52764\n",
      "Main effects training epoch: 116, train loss: 0.52524, val loss: 0.52517\n",
      "Main effects training epoch: 117, train loss: 0.52557, val loss: 0.52837\n",
      "Main effects training epoch: 118, train loss: 0.52588, val loss: 0.52556\n",
      "Main effects training epoch: 119, train loss: 0.52609, val loss: 0.52844\n",
      "Main effects training epoch: 120, train loss: 0.52592, val loss: 0.52603\n",
      "Main effects training epoch: 121, train loss: 0.52503, val loss: 0.52645\n",
      "Main effects training epoch: 122, train loss: 0.52460, val loss: 0.52599\n",
      "Main effects training epoch: 123, train loss: 0.52446, val loss: 0.52617\n",
      "Main effects training epoch: 124, train loss: 0.52474, val loss: 0.52651\n",
      "Main effects training epoch: 125, train loss: 0.52444, val loss: 0.52585\n",
      "Main effects training epoch: 126, train loss: 0.52440, val loss: 0.52614\n",
      "Main effects training epoch: 127, train loss: 0.52446, val loss: 0.52481\n",
      "Main effects training epoch: 128, train loss: 0.52426, val loss: 0.52575\n",
      "Main effects training epoch: 129, train loss: 0.52455, val loss: 0.52488\n",
      "Main effects training epoch: 130, train loss: 0.52490, val loss: 0.52696\n",
      "Main effects training epoch: 131, train loss: 0.52463, val loss: 0.52683\n",
      "Main effects training epoch: 132, train loss: 0.52400, val loss: 0.52645\n",
      "Main effects training epoch: 133, train loss: 0.52382, val loss: 0.52644\n",
      "Main effects training epoch: 134, train loss: 0.52357, val loss: 0.52601\n",
      "Main effects training epoch: 135, train loss: 0.52350, val loss: 0.52583\n",
      "Main effects training epoch: 136, train loss: 0.52424, val loss: 0.52600\n",
      "Main effects training epoch: 137, train loss: 0.52402, val loss: 0.52691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 138, train loss: 0.52370, val loss: 0.52626\n",
      "Main effects training epoch: 139, train loss: 0.52315, val loss: 0.52713\n",
      "Main effects training epoch: 140, train loss: 0.52338, val loss: 0.52674\n",
      "Main effects training epoch: 141, train loss: 0.52320, val loss: 0.52488\n",
      "Main effects training epoch: 142, train loss: 0.52321, val loss: 0.52680\n",
      "Main effects training epoch: 143, train loss: 0.52300, val loss: 0.52655\n",
      "Main effects training epoch: 144, train loss: 0.52299, val loss: 0.52545\n",
      "Main effects training epoch: 145, train loss: 0.52294, val loss: 0.52886\n",
      "Main effects training epoch: 146, train loss: 0.52321, val loss: 0.52503\n",
      "Main effects training epoch: 147, train loss: 0.52268, val loss: 0.52706\n",
      "Main effects training epoch: 148, train loss: 0.52290, val loss: 0.52617\n",
      "Main effects training epoch: 149, train loss: 0.52268, val loss: 0.52668\n",
      "Main effects training epoch: 150, train loss: 0.52277, val loss: 0.52668\n",
      "Main effects training epoch: 151, train loss: 0.52240, val loss: 0.52732\n",
      "Main effects training epoch: 152, train loss: 0.52234, val loss: 0.52649\n",
      "Main effects training epoch: 153, train loss: 0.52274, val loss: 0.52526\n",
      "Main effects training epoch: 154, train loss: 0.52303, val loss: 0.52845\n",
      "Main effects training epoch: 155, train loss: 0.52272, val loss: 0.52602\n",
      "Main effects training epoch: 156, train loss: 0.52271, val loss: 0.52874\n",
      "Main effects training epoch: 157, train loss: 0.52242, val loss: 0.52650\n",
      "Main effects training epoch: 158, train loss: 0.52285, val loss: 0.52811\n",
      "Main effects training epoch: 159, train loss: 0.52269, val loss: 0.52603\n",
      "Main effects training epoch: 160, train loss: 0.52238, val loss: 0.52757\n",
      "Main effects training epoch: 161, train loss: 0.52250, val loss: 0.52708\n",
      "Main effects training epoch: 162, train loss: 0.52241, val loss: 0.52666\n",
      "Main effects training epoch: 163, train loss: 0.52258, val loss: 0.52764\n",
      "Main effects training epoch: 164, train loss: 0.52219, val loss: 0.52612\n",
      "Main effects training epoch: 165, train loss: 0.52264, val loss: 0.52839\n",
      "Main effects training epoch: 166, train loss: 0.52301, val loss: 0.52490\n",
      "Main effects training epoch: 167, train loss: 0.52248, val loss: 0.52870\n",
      "Main effects training epoch: 168, train loss: 0.52224, val loss: 0.52596\n",
      "Main effects training epoch: 169, train loss: 0.52205, val loss: 0.52609\n",
      "Main effects training epoch: 170, train loss: 0.52223, val loss: 0.52641\n",
      "Main effects training epoch: 171, train loss: 0.52362, val loss: 0.52445\n",
      "Main effects training epoch: 172, train loss: 0.52270, val loss: 0.52938\n",
      "Main effects training epoch: 173, train loss: 0.52213, val loss: 0.52598\n",
      "Main effects training epoch: 174, train loss: 0.52191, val loss: 0.52656\n",
      "Main effects training epoch: 175, train loss: 0.52236, val loss: 0.52777\n",
      "Main effects training epoch: 176, train loss: 0.52260, val loss: 0.52548\n",
      "Main effects training epoch: 177, train loss: 0.52211, val loss: 0.52729\n",
      "Main effects training epoch: 178, train loss: 0.52184, val loss: 0.52684\n",
      "Main effects training epoch: 179, train loss: 0.52217, val loss: 0.52607\n",
      "Main effects training epoch: 180, train loss: 0.52213, val loss: 0.52779\n",
      "Main effects training epoch: 181, train loss: 0.52190, val loss: 0.52596\n",
      "Main effects training epoch: 182, train loss: 0.52170, val loss: 0.52542\n",
      "Main effects training epoch: 183, train loss: 0.52188, val loss: 0.52772\n",
      "Main effects training epoch: 184, train loss: 0.52216, val loss: 0.52437\n",
      "Main effects training epoch: 185, train loss: 0.52209, val loss: 0.52893\n",
      "Main effects training epoch: 186, train loss: 0.52156, val loss: 0.52608\n",
      "Main effects training epoch: 187, train loss: 0.52175, val loss: 0.52687\n",
      "Main effects training epoch: 188, train loss: 0.52160, val loss: 0.52490\n",
      "Main effects training epoch: 189, train loss: 0.52150, val loss: 0.52690\n",
      "Main effects training epoch: 190, train loss: 0.52153, val loss: 0.52494\n",
      "Main effects training epoch: 191, train loss: 0.52162, val loss: 0.52570\n",
      "Main effects training epoch: 192, train loss: 0.52138, val loss: 0.52625\n",
      "Main effects training epoch: 193, train loss: 0.52140, val loss: 0.52643\n",
      "Main effects training epoch: 194, train loss: 0.52167, val loss: 0.52678\n",
      "Main effects training epoch: 195, train loss: 0.52195, val loss: 0.52488\n",
      "Main effects training epoch: 196, train loss: 0.52183, val loss: 0.52694\n",
      "Main effects training epoch: 197, train loss: 0.52148, val loss: 0.52544\n",
      "Main effects training epoch: 198, train loss: 0.52119, val loss: 0.52627\n",
      "Main effects training epoch: 199, train loss: 0.52098, val loss: 0.52533\n",
      "Main effects training epoch: 200, train loss: 0.52137, val loss: 0.52624\n",
      "Main effects training epoch: 201, train loss: 0.52108, val loss: 0.52573\n",
      "Main effects training epoch: 202, train loss: 0.52078, val loss: 0.52500\n",
      "Main effects training epoch: 203, train loss: 0.52082, val loss: 0.52442\n",
      "Main effects training epoch: 204, train loss: 0.52073, val loss: 0.52460\n",
      "Main effects training epoch: 205, train loss: 0.52100, val loss: 0.52652\n",
      "Main effects training epoch: 206, train loss: 0.52111, val loss: 0.52385\n",
      "Main effects training epoch: 207, train loss: 0.52098, val loss: 0.52545\n",
      "Main effects training epoch: 208, train loss: 0.52123, val loss: 0.52423\n",
      "Main effects training epoch: 209, train loss: 0.52130, val loss: 0.52663\n",
      "Main effects training epoch: 210, train loss: 0.52059, val loss: 0.52439\n",
      "Main effects training epoch: 211, train loss: 0.52040, val loss: 0.52437\n",
      "Main effects training epoch: 212, train loss: 0.52045, val loss: 0.52404\n",
      "Main effects training epoch: 213, train loss: 0.52049, val loss: 0.52487\n",
      "Main effects training epoch: 214, train loss: 0.52085, val loss: 0.52457\n",
      "Main effects training epoch: 215, train loss: 0.52055, val loss: 0.52501\n",
      "Main effects training epoch: 216, train loss: 0.52039, val loss: 0.52503\n",
      "Main effects training epoch: 217, train loss: 0.52070, val loss: 0.52326\n",
      "Main effects training epoch: 218, train loss: 0.52032, val loss: 0.52531\n",
      "Main effects training epoch: 219, train loss: 0.52037, val loss: 0.52396\n",
      "Main effects training epoch: 220, train loss: 0.52041, val loss: 0.52342\n",
      "Main effects training epoch: 221, train loss: 0.52023, val loss: 0.52419\n",
      "Main effects training epoch: 222, train loss: 0.52001, val loss: 0.52444\n",
      "Main effects training epoch: 223, train loss: 0.52007, val loss: 0.52312\n",
      "Main effects training epoch: 224, train loss: 0.52000, val loss: 0.52391\n",
      "Main effects training epoch: 225, train loss: 0.51997, val loss: 0.52437\n",
      "Main effects training epoch: 226, train loss: 0.52002, val loss: 0.52308\n",
      "Main effects training epoch: 227, train loss: 0.52000, val loss: 0.52381\n",
      "Main effects training epoch: 228, train loss: 0.51989, val loss: 0.52207\n",
      "Main effects training epoch: 229, train loss: 0.51968, val loss: 0.52470\n",
      "Main effects training epoch: 230, train loss: 0.51952, val loss: 0.52373\n",
      "Main effects training epoch: 231, train loss: 0.51964, val loss: 0.52354\n",
      "Main effects training epoch: 232, train loss: 0.51937, val loss: 0.52241\n",
      "Main effects training epoch: 233, train loss: 0.51938, val loss: 0.52237\n",
      "Main effects training epoch: 234, train loss: 0.51961, val loss: 0.52224\n",
      "Main effects training epoch: 235, train loss: 0.51951, val loss: 0.52284\n",
      "Main effects training epoch: 236, train loss: 0.51917, val loss: 0.52296\n",
      "Main effects training epoch: 237, train loss: 0.51936, val loss: 0.52235\n",
      "Main effects training epoch: 238, train loss: 0.51923, val loss: 0.52363\n",
      "Main effects training epoch: 239, train loss: 0.51929, val loss: 0.52261\n",
      "Main effects training epoch: 240, train loss: 0.51920, val loss: 0.52221\n",
      "Main effects training epoch: 241, train loss: 0.51895, val loss: 0.52194\n",
      "Main effects training epoch: 242, train loss: 0.51895, val loss: 0.52062\n",
      "Main effects training epoch: 243, train loss: 0.51883, val loss: 0.52306\n",
      "Main effects training epoch: 244, train loss: 0.51882, val loss: 0.52206\n",
      "Main effects training epoch: 245, train loss: 0.51894, val loss: 0.52311\n",
      "Main effects training epoch: 246, train loss: 0.51905, val loss: 0.52207\n",
      "Main effects training epoch: 247, train loss: 0.51922, val loss: 0.52268\n",
      "Main effects training epoch: 248, train loss: 0.51906, val loss: 0.52054\n",
      "Main effects training epoch: 249, train loss: 0.51857, val loss: 0.52289\n",
      "Main effects training epoch: 250, train loss: 0.51871, val loss: 0.52242\n",
      "Main effects training epoch: 251, train loss: 0.51892, val loss: 0.52110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 252, train loss: 0.51861, val loss: 0.52312\n",
      "Main effects training epoch: 253, train loss: 0.51844, val loss: 0.52141\n",
      "Main effects training epoch: 254, train loss: 0.51822, val loss: 0.52247\n",
      "Main effects training epoch: 255, train loss: 0.51841, val loss: 0.52154\n",
      "Main effects training epoch: 256, train loss: 0.51818, val loss: 0.52164\n",
      "Main effects training epoch: 257, train loss: 0.51823, val loss: 0.52130\n",
      "Main effects training epoch: 258, train loss: 0.51812, val loss: 0.52268\n",
      "Main effects training epoch: 259, train loss: 0.51791, val loss: 0.52026\n",
      "Main effects training epoch: 260, train loss: 0.51803, val loss: 0.52151\n",
      "Main effects training epoch: 261, train loss: 0.51816, val loss: 0.51974\n",
      "Main effects training epoch: 262, train loss: 0.51802, val loss: 0.52000\n",
      "Main effects training epoch: 263, train loss: 0.51811, val loss: 0.52273\n",
      "Main effects training epoch: 264, train loss: 0.51823, val loss: 0.52230\n",
      "Main effects training epoch: 265, train loss: 0.51795, val loss: 0.52059\n",
      "Main effects training epoch: 266, train loss: 0.51760, val loss: 0.52041\n",
      "Main effects training epoch: 267, train loss: 0.51733, val loss: 0.52075\n",
      "Main effects training epoch: 268, train loss: 0.51744, val loss: 0.52002\n",
      "Main effects training epoch: 269, train loss: 0.51729, val loss: 0.51974\n",
      "Main effects training epoch: 270, train loss: 0.51723, val loss: 0.51968\n",
      "Main effects training epoch: 271, train loss: 0.51758, val loss: 0.51954\n",
      "Main effects training epoch: 272, train loss: 0.51740, val loss: 0.52061\n",
      "Main effects training epoch: 273, train loss: 0.51729, val loss: 0.52070\n",
      "Main effects training epoch: 274, train loss: 0.51741, val loss: 0.52005\n",
      "Main effects training epoch: 275, train loss: 0.51722, val loss: 0.52134\n",
      "Main effects training epoch: 276, train loss: 0.51768, val loss: 0.51871\n",
      "Main effects training epoch: 277, train loss: 0.51727, val loss: 0.51881\n",
      "Main effects training epoch: 278, train loss: 0.51763, val loss: 0.52219\n",
      "Main effects training epoch: 279, train loss: 0.51687, val loss: 0.51854\n",
      "Main effects training epoch: 280, train loss: 0.51665, val loss: 0.51836\n",
      "Main effects training epoch: 281, train loss: 0.51681, val loss: 0.52082\n",
      "Main effects training epoch: 282, train loss: 0.51652, val loss: 0.51770\n",
      "Main effects training epoch: 283, train loss: 0.51684, val loss: 0.51817\n",
      "Main effects training epoch: 284, train loss: 0.51681, val loss: 0.52278\n",
      "Main effects training epoch: 285, train loss: 0.51674, val loss: 0.51900\n",
      "Main effects training epoch: 286, train loss: 0.51628, val loss: 0.51816\n",
      "Main effects training epoch: 287, train loss: 0.51613, val loss: 0.51991\n",
      "Main effects training epoch: 288, train loss: 0.51619, val loss: 0.51756\n",
      "Main effects training epoch: 289, train loss: 0.51622, val loss: 0.51928\n",
      "Main effects training epoch: 290, train loss: 0.51646, val loss: 0.51802\n",
      "Main effects training epoch: 291, train loss: 0.51581, val loss: 0.51849\n",
      "Main effects training epoch: 292, train loss: 0.51606, val loss: 0.51772\n",
      "Main effects training epoch: 293, train loss: 0.51573, val loss: 0.51902\n",
      "Main effects training epoch: 294, train loss: 0.51542, val loss: 0.51758\n",
      "Main effects training epoch: 295, train loss: 0.51520, val loss: 0.51774\n",
      "Main effects training epoch: 296, train loss: 0.51601, val loss: 0.51692\n",
      "Main effects training epoch: 297, train loss: 0.51565, val loss: 0.51707\n",
      "Main effects training epoch: 298, train loss: 0.51582, val loss: 0.52031\n",
      "Main effects training epoch: 299, train loss: 0.51539, val loss: 0.51924\n",
      "Main effects training epoch: 300, train loss: 0.51552, val loss: 0.51519\n",
      "##########Stage 1: main effect training stop.##########\n",
      "5 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51683, val loss: 0.52348\n",
      "Main effects tuning epoch: 2, train loss: 0.51697, val loss: 0.52131\n",
      "Main effects tuning epoch: 3, train loss: 0.51667, val loss: 0.52021\n",
      "Main effects tuning epoch: 4, train loss: 0.51632, val loss: 0.52251\n",
      "Main effects tuning epoch: 5, train loss: 0.51631, val loss: 0.52081\n",
      "Main effects tuning epoch: 6, train loss: 0.51650, val loss: 0.52087\n",
      "Main effects tuning epoch: 7, train loss: 0.51609, val loss: 0.52197\n",
      "Main effects tuning epoch: 8, train loss: 0.51630, val loss: 0.52234\n",
      "Main effects tuning epoch: 9, train loss: 0.51657, val loss: 0.51970\n",
      "Main effects tuning epoch: 10, train loss: 0.51582, val loss: 0.52039\n",
      "Main effects tuning epoch: 11, train loss: 0.51574, val loss: 0.52252\n",
      "Main effects tuning epoch: 12, train loss: 0.51618, val loss: 0.51862\n",
      "Main effects tuning epoch: 13, train loss: 0.51597, val loss: 0.52096\n",
      "Main effects tuning epoch: 14, train loss: 0.51604, val loss: 0.52199\n",
      "Main effects tuning epoch: 15, train loss: 0.51542, val loss: 0.51994\n",
      "Main effects tuning epoch: 16, train loss: 0.51540, val loss: 0.51999\n",
      "Main effects tuning epoch: 17, train loss: 0.51547, val loss: 0.51982\n",
      "Main effects tuning epoch: 18, train loss: 0.51550, val loss: 0.51989\n",
      "Main effects tuning epoch: 19, train loss: 0.51527, val loss: 0.52151\n",
      "Main effects tuning epoch: 20, train loss: 0.51515, val loss: 0.51982\n",
      "Main effects tuning epoch: 21, train loss: 0.51604, val loss: 0.52041\n",
      "Main effects tuning epoch: 22, train loss: 0.51523, val loss: 0.52193\n",
      "Main effects tuning epoch: 23, train loss: 0.51549, val loss: 0.51970\n",
      "Main effects tuning epoch: 24, train loss: 0.51557, val loss: 0.51901\n",
      "Main effects tuning epoch: 25, train loss: 0.51566, val loss: 0.52242\n",
      "Main effects tuning epoch: 26, train loss: 0.51529, val loss: 0.52143\n",
      "Main effects tuning epoch: 27, train loss: 0.51534, val loss: 0.52089\n",
      "Main effects tuning epoch: 28, train loss: 0.51485, val loss: 0.52044\n",
      "Main effects tuning epoch: 29, train loss: 0.51457, val loss: 0.52159\n",
      "Main effects tuning epoch: 30, train loss: 0.51474, val loss: 0.51925\n",
      "Main effects tuning epoch: 31, train loss: 0.51499, val loss: 0.52003\n",
      "Main effects tuning epoch: 32, train loss: 0.51578, val loss: 0.52068\n",
      "Main effects tuning epoch: 33, train loss: 0.51507, val loss: 0.52125\n",
      "Main effects tuning epoch: 34, train loss: 0.51476, val loss: 0.51752\n",
      "Main effects tuning epoch: 35, train loss: 0.51494, val loss: 0.52137\n",
      "Main effects tuning epoch: 36, train loss: 0.51464, val loss: 0.52001\n",
      "Main effects tuning epoch: 37, train loss: 0.51442, val loss: 0.51820\n",
      "Main effects tuning epoch: 38, train loss: 0.51483, val loss: 0.51999\n",
      "Main effects tuning epoch: 39, train loss: 0.51466, val loss: 0.52048\n",
      "Main effects tuning epoch: 40, train loss: 0.51456, val loss: 0.51997\n",
      "Main effects tuning epoch: 41, train loss: 0.51442, val loss: 0.52048\n",
      "Main effects tuning epoch: 42, train loss: 0.51467, val loss: 0.51843\n",
      "Main effects tuning epoch: 43, train loss: 0.51509, val loss: 0.51930\n",
      "Main effects tuning epoch: 44, train loss: 0.51476, val loss: 0.52164\n",
      "Main effects tuning epoch: 45, train loss: 0.51421, val loss: 0.52016\n",
      "Main effects tuning epoch: 46, train loss: 0.51411, val loss: 0.51832\n",
      "Main effects tuning epoch: 47, train loss: 0.51409, val loss: 0.52029\n",
      "Main effects tuning epoch: 48, train loss: 0.51421, val loss: 0.52144\n",
      "Main effects tuning epoch: 49, train loss: 0.51394, val loss: 0.51870\n",
      "Main effects tuning epoch: 50, train loss: 0.51384, val loss: 0.51915\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.46676, val loss: 0.48890\n",
      "Interaction training epoch: 2, train loss: 0.46580, val loss: 0.48979\n",
      "Interaction training epoch: 3, train loss: 0.32952, val loss: 0.36334\n",
      "Interaction training epoch: 4, train loss: 0.30992, val loss: 0.33124\n",
      "Interaction training epoch: 5, train loss: 0.30307, val loss: 0.32811\n",
      "Interaction training epoch: 6, train loss: 0.30342, val loss: 0.32488\n",
      "Interaction training epoch: 7, train loss: 0.29916, val loss: 0.32334\n",
      "Interaction training epoch: 8, train loss: 0.29676, val loss: 0.32169\n",
      "Interaction training epoch: 9, train loss: 0.29529, val loss: 0.31942\n",
      "Interaction training epoch: 10, train loss: 0.29158, val loss: 0.31600\n",
      "Interaction training epoch: 11, train loss: 0.29512, val loss: 0.31576\n",
      "Interaction training epoch: 12, train loss: 0.29819, val loss: 0.31870\n",
      "Interaction training epoch: 13, train loss: 0.29326, val loss: 0.31946\n",
      "Interaction training epoch: 14, train loss: 0.29142, val loss: 0.31937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 15, train loss: 0.29604, val loss: 0.32071\n",
      "Interaction training epoch: 16, train loss: 0.28664, val loss: 0.31079\n",
      "Interaction training epoch: 17, train loss: 0.28875, val loss: 0.31058\n",
      "Interaction training epoch: 18, train loss: 0.29025, val loss: 0.31444\n",
      "Interaction training epoch: 19, train loss: 0.28999, val loss: 0.31342\n",
      "Interaction training epoch: 20, train loss: 0.29024, val loss: 0.31883\n",
      "Interaction training epoch: 21, train loss: 0.28717, val loss: 0.30993\n",
      "Interaction training epoch: 22, train loss: 0.28630, val loss: 0.31102\n",
      "Interaction training epoch: 23, train loss: 0.29096, val loss: 0.31306\n",
      "Interaction training epoch: 24, train loss: 0.28824, val loss: 0.31303\n",
      "Interaction training epoch: 25, train loss: 0.28285, val loss: 0.30747\n",
      "Interaction training epoch: 26, train loss: 0.28672, val loss: 0.31133\n",
      "Interaction training epoch: 27, train loss: 0.28771, val loss: 0.31174\n",
      "Interaction training epoch: 28, train loss: 0.29109, val loss: 0.32432\n",
      "Interaction training epoch: 29, train loss: 0.29151, val loss: 0.31786\n",
      "Interaction training epoch: 30, train loss: 0.28594, val loss: 0.30968\n",
      "Interaction training epoch: 31, train loss: 0.28793, val loss: 0.31714\n",
      "Interaction training epoch: 32, train loss: 0.28908, val loss: 0.31058\n",
      "Interaction training epoch: 33, train loss: 0.28501, val loss: 0.31162\n",
      "Interaction training epoch: 34, train loss: 0.28421, val loss: 0.31220\n",
      "Interaction training epoch: 35, train loss: 0.28877, val loss: 0.31161\n",
      "Interaction training epoch: 36, train loss: 0.28144, val loss: 0.31068\n",
      "Interaction training epoch: 37, train loss: 0.28369, val loss: 0.31314\n",
      "Interaction training epoch: 38, train loss: 0.28053, val loss: 0.30823\n",
      "Interaction training epoch: 39, train loss: 0.28098, val loss: 0.30810\n",
      "Interaction training epoch: 40, train loss: 0.27912, val loss: 0.30703\n",
      "Interaction training epoch: 41, train loss: 0.28405, val loss: 0.31392\n",
      "Interaction training epoch: 42, train loss: 0.28313, val loss: 0.31009\n",
      "Interaction training epoch: 43, train loss: 0.27991, val loss: 0.31038\n",
      "Interaction training epoch: 44, train loss: 0.28024, val loss: 0.31217\n",
      "Interaction training epoch: 45, train loss: 0.27900, val loss: 0.31187\n",
      "Interaction training epoch: 46, train loss: 0.28497, val loss: 0.31291\n",
      "Interaction training epoch: 47, train loss: 0.27594, val loss: 0.30888\n",
      "Interaction training epoch: 48, train loss: 0.27689, val loss: 0.30797\n",
      "Interaction training epoch: 49, train loss: 0.27964, val loss: 0.31125\n",
      "Interaction training epoch: 50, train loss: 0.27923, val loss: 0.31213\n",
      "Interaction training epoch: 51, train loss: 0.27511, val loss: 0.30674\n",
      "Interaction training epoch: 52, train loss: 0.28418, val loss: 0.31771\n",
      "Interaction training epoch: 53, train loss: 0.27476, val loss: 0.30461\n",
      "Interaction training epoch: 54, train loss: 0.27642, val loss: 0.31122\n",
      "Interaction training epoch: 55, train loss: 0.27659, val loss: 0.31106\n",
      "Interaction training epoch: 56, train loss: 0.27536, val loss: 0.30809\n",
      "Interaction training epoch: 57, train loss: 0.27882, val loss: 0.30863\n",
      "Interaction training epoch: 58, train loss: 0.27698, val loss: 0.31422\n",
      "Interaction training epoch: 59, train loss: 0.27600, val loss: 0.31108\n",
      "Interaction training epoch: 60, train loss: 0.27605, val loss: 0.30828\n",
      "Interaction training epoch: 61, train loss: 0.27540, val loss: 0.31246\n",
      "Interaction training epoch: 62, train loss: 0.27522, val loss: 0.31285\n",
      "Interaction training epoch: 63, train loss: 0.27844, val loss: 0.31148\n",
      "Interaction training epoch: 64, train loss: 0.27357, val loss: 0.31076\n",
      "Interaction training epoch: 65, train loss: 0.27721, val loss: 0.31521\n",
      "Interaction training epoch: 66, train loss: 0.27165, val loss: 0.30825\n",
      "Interaction training epoch: 67, train loss: 0.27475, val loss: 0.31335\n",
      "Interaction training epoch: 68, train loss: 0.27266, val loss: 0.30958\n",
      "Interaction training epoch: 69, train loss: 0.27336, val loss: 0.31055\n",
      "Interaction training epoch: 70, train loss: 0.27203, val loss: 0.30811\n",
      "Interaction training epoch: 71, train loss: 0.26955, val loss: 0.30921\n",
      "Interaction training epoch: 72, train loss: 0.27261, val loss: 0.30969\n",
      "Interaction training epoch: 73, train loss: 0.27049, val loss: 0.30756\n",
      "Interaction training epoch: 74, train loss: 0.27362, val loss: 0.31318\n",
      "Interaction training epoch: 75, train loss: 0.26883, val loss: 0.30477\n",
      "Interaction training epoch: 76, train loss: 0.27000, val loss: 0.31061\n",
      "Interaction training epoch: 77, train loss: 0.26929, val loss: 0.30546\n",
      "Interaction training epoch: 78, train loss: 0.27069, val loss: 0.31092\n",
      "Interaction training epoch: 79, train loss: 0.26936, val loss: 0.30713\n",
      "Interaction training epoch: 80, train loss: 0.27242, val loss: 0.30885\n",
      "Interaction training epoch: 81, train loss: 0.26891, val loss: 0.30737\n",
      "Interaction training epoch: 82, train loss: 0.27041, val loss: 0.31078\n",
      "Interaction training epoch: 83, train loss: 0.27009, val loss: 0.30990\n",
      "Interaction training epoch: 84, train loss: 0.26589, val loss: 0.30533\n",
      "Interaction training epoch: 85, train loss: 0.27402, val loss: 0.30936\n",
      "Interaction training epoch: 86, train loss: 0.26993, val loss: 0.31239\n",
      "Interaction training epoch: 87, train loss: 0.26926, val loss: 0.31010\n",
      "Interaction training epoch: 88, train loss: 0.26730, val loss: 0.30870\n",
      "Interaction training epoch: 89, train loss: 0.26703, val loss: 0.30623\n",
      "Interaction training epoch: 90, train loss: 0.26370, val loss: 0.30252\n",
      "Interaction training epoch: 91, train loss: 0.26984, val loss: 0.31244\n",
      "Interaction training epoch: 92, train loss: 0.26581, val loss: 0.30628\n",
      "Interaction training epoch: 93, train loss: 0.26900, val loss: 0.30656\n",
      "Interaction training epoch: 94, train loss: 0.26744, val loss: 0.30891\n",
      "Interaction training epoch: 95, train loss: 0.26533, val loss: 0.30615\n",
      "Interaction training epoch: 96, train loss: 0.26814, val loss: 0.31139\n",
      "Interaction training epoch: 97, train loss: 0.26306, val loss: 0.30407\n",
      "Interaction training epoch: 98, train loss: 0.27070, val loss: 0.30957\n",
      "Interaction training epoch: 99, train loss: 0.26720, val loss: 0.31356\n",
      "Interaction training epoch: 100, train loss: 0.27085, val loss: 0.31037\n",
      "Interaction training epoch: 101, train loss: 0.26619, val loss: 0.31064\n",
      "Interaction training epoch: 102, train loss: 0.26372, val loss: 0.30493\n",
      "Interaction training epoch: 103, train loss: 0.26487, val loss: 0.30698\n",
      "Interaction training epoch: 104, train loss: 0.26336, val loss: 0.30473\n",
      "Interaction training epoch: 105, train loss: 0.26520, val loss: 0.30787\n",
      "Interaction training epoch: 106, train loss: 0.26449, val loss: 0.30752\n",
      "Interaction training epoch: 107, train loss: 0.26322, val loss: 0.30966\n",
      "Interaction training epoch: 108, train loss: 0.26247, val loss: 0.30658\n",
      "Interaction training epoch: 109, train loss: 0.26340, val loss: 0.30375\n",
      "Interaction training epoch: 110, train loss: 0.26484, val loss: 0.31099\n",
      "Interaction training epoch: 111, train loss: 0.26145, val loss: 0.30638\n",
      "Interaction training epoch: 112, train loss: 0.26719, val loss: 0.31283\n",
      "Interaction training epoch: 113, train loss: 0.26081, val loss: 0.30256\n",
      "Interaction training epoch: 114, train loss: 0.26483, val loss: 0.31179\n",
      "Interaction training epoch: 115, train loss: 0.26103, val loss: 0.30496\n",
      "Interaction training epoch: 116, train loss: 0.26434, val loss: 0.30540\n",
      "Interaction training epoch: 117, train loss: 0.26123, val loss: 0.30709\n",
      "Interaction training epoch: 118, train loss: 0.26141, val loss: 0.30513\n",
      "Interaction training epoch: 119, train loss: 0.26323, val loss: 0.30877\n",
      "Interaction training epoch: 120, train loss: 0.26093, val loss: 0.30496\n",
      "Interaction training epoch: 121, train loss: 0.25947, val loss: 0.30358\n",
      "Interaction training epoch: 122, train loss: 0.26301, val loss: 0.30460\n",
      "Interaction training epoch: 123, train loss: 0.26057, val loss: 0.30862\n",
      "Interaction training epoch: 124, train loss: 0.26205, val loss: 0.30596\n",
      "Interaction training epoch: 125, train loss: 0.25920, val loss: 0.30250\n",
      "Interaction training epoch: 126, train loss: 0.26051, val loss: 0.30730\n",
      "Interaction training epoch: 127, train loss: 0.26078, val loss: 0.30534\n",
      "Interaction training epoch: 128, train loss: 0.26078, val loss: 0.30689\n",
      "Interaction training epoch: 129, train loss: 0.26099, val loss: 0.30733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 130, train loss: 0.26232, val loss: 0.30448\n",
      "Interaction training epoch: 131, train loss: 0.25843, val loss: 0.30398\n",
      "Interaction training epoch: 132, train loss: 0.26044, val loss: 0.30779\n",
      "Interaction training epoch: 133, train loss: 0.26299, val loss: 0.30792\n",
      "Interaction training epoch: 134, train loss: 0.26000, val loss: 0.30996\n",
      "Interaction training epoch: 135, train loss: 0.26171, val loss: 0.30315\n",
      "Interaction training epoch: 136, train loss: 0.26050, val loss: 0.31090\n",
      "Interaction training epoch: 137, train loss: 0.25817, val loss: 0.30220\n",
      "Interaction training epoch: 138, train loss: 0.26069, val loss: 0.30519\n",
      "Interaction training epoch: 139, train loss: 0.25874, val loss: 0.30727\n",
      "Interaction training epoch: 140, train loss: 0.25845, val loss: 0.30441\n",
      "Interaction training epoch: 141, train loss: 0.26203, val loss: 0.31139\n",
      "Interaction training epoch: 142, train loss: 0.25706, val loss: 0.30534\n",
      "Interaction training epoch: 143, train loss: 0.25775, val loss: 0.30344\n",
      "Interaction training epoch: 144, train loss: 0.25984, val loss: 0.30941\n",
      "Interaction training epoch: 145, train loss: 0.25818, val loss: 0.30700\n",
      "Interaction training epoch: 146, train loss: 0.25747, val loss: 0.30366\n",
      "Interaction training epoch: 147, train loss: 0.26079, val loss: 0.31363\n",
      "Interaction training epoch: 148, train loss: 0.25762, val loss: 0.30547\n",
      "Interaction training epoch: 149, train loss: 0.26127, val loss: 0.31330\n",
      "Interaction training epoch: 150, train loss: 0.25827, val loss: 0.30851\n",
      "Interaction training epoch: 151, train loss: 0.25654, val loss: 0.30371\n",
      "Interaction training epoch: 152, train loss: 0.25731, val loss: 0.30658\n",
      "Interaction training epoch: 153, train loss: 0.25955, val loss: 0.31202\n",
      "Interaction training epoch: 154, train loss: 0.25683, val loss: 0.30719\n",
      "Interaction training epoch: 155, train loss: 0.25691, val loss: 0.30794\n",
      "Interaction training epoch: 156, train loss: 0.25679, val loss: 0.30601\n",
      "Interaction training epoch: 157, train loss: 0.25770, val loss: 0.30606\n",
      "Interaction training epoch: 158, train loss: 0.25795, val loss: 0.31006\n",
      "Interaction training epoch: 159, train loss: 0.25610, val loss: 0.30366\n",
      "Interaction training epoch: 160, train loss: 0.25589, val loss: 0.30917\n",
      "Interaction training epoch: 161, train loss: 0.25693, val loss: 0.30869\n",
      "Interaction training epoch: 162, train loss: 0.25665, val loss: 0.30751\n",
      "Interaction training epoch: 163, train loss: 0.25444, val loss: 0.30148\n",
      "Interaction training epoch: 164, train loss: 0.25681, val loss: 0.30715\n",
      "Interaction training epoch: 165, train loss: 0.25390, val loss: 0.30513\n",
      "Interaction training epoch: 166, train loss: 0.25838, val loss: 0.31403\n",
      "Interaction training epoch: 167, train loss: 0.25448, val loss: 0.30433\n",
      "Interaction training epoch: 168, train loss: 0.25142, val loss: 0.30431\n",
      "Interaction training epoch: 169, train loss: 0.25740, val loss: 0.31051\n",
      "Interaction training epoch: 170, train loss: 0.25442, val loss: 0.30674\n",
      "Interaction training epoch: 171, train loss: 0.25920, val loss: 0.31041\n",
      "Interaction training epoch: 172, train loss: 0.25330, val loss: 0.30516\n",
      "Interaction training epoch: 173, train loss: 0.25869, val loss: 0.31043\n",
      "Interaction training epoch: 174, train loss: 0.25389, val loss: 0.30530\n",
      "Interaction training epoch: 175, train loss: 0.25377, val loss: 0.30557\n",
      "Interaction training epoch: 176, train loss: 0.25330, val loss: 0.30780\n",
      "Interaction training epoch: 177, train loss: 0.25329, val loss: 0.30733\n",
      "Interaction training epoch: 178, train loss: 0.25533, val loss: 0.31232\n",
      "Interaction training epoch: 179, train loss: 0.25070, val loss: 0.30051\n",
      "Interaction training epoch: 180, train loss: 0.25580, val loss: 0.31044\n",
      "Interaction training epoch: 181, train loss: 0.25148, val loss: 0.30589\n",
      "Interaction training epoch: 182, train loss: 0.25430, val loss: 0.30958\n",
      "Interaction training epoch: 183, train loss: 0.25411, val loss: 0.30649\n",
      "Interaction training epoch: 184, train loss: 0.25013, val loss: 0.30217\n",
      "Interaction training epoch: 185, train loss: 0.25431, val loss: 0.31088\n",
      "Interaction training epoch: 186, train loss: 0.24937, val loss: 0.30068\n",
      "Interaction training epoch: 187, train loss: 0.25459, val loss: 0.30703\n",
      "Interaction training epoch: 188, train loss: 0.24965, val loss: 0.30361\n",
      "Interaction training epoch: 189, train loss: 0.25256, val loss: 0.30931\n",
      "Interaction training epoch: 190, train loss: 0.24986, val loss: 0.30172\n",
      "Interaction training epoch: 191, train loss: 0.25472, val loss: 0.31040\n",
      "Interaction training epoch: 192, train loss: 0.24883, val loss: 0.30104\n",
      "Interaction training epoch: 193, train loss: 0.25395, val loss: 0.30803\n",
      "Interaction training epoch: 194, train loss: 0.24877, val loss: 0.30154\n",
      "Interaction training epoch: 195, train loss: 0.24990, val loss: 0.30305\n",
      "Interaction training epoch: 196, train loss: 0.25087, val loss: 0.30716\n",
      "Interaction training epoch: 197, train loss: 0.24902, val loss: 0.30324\n",
      "Interaction training epoch: 198, train loss: 0.25195, val loss: 0.30456\n",
      "Interaction training epoch: 199, train loss: 0.25045, val loss: 0.30504\n",
      "Interaction training epoch: 200, train loss: 0.25171, val loss: 0.30797\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########4 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.25867, val loss: 0.29544\n",
      "Interaction tuning epoch: 2, train loss: 0.25856, val loss: 0.30212\n",
      "Interaction tuning epoch: 3, train loss: 0.25724, val loss: 0.29596\n",
      "Interaction tuning epoch: 4, train loss: 0.25566, val loss: 0.29471\n",
      "Interaction tuning epoch: 5, train loss: 0.25633, val loss: 0.29629\n",
      "Interaction tuning epoch: 6, train loss: 0.25707, val loss: 0.29685\n",
      "Interaction tuning epoch: 7, train loss: 0.25426, val loss: 0.29629\n",
      "Interaction tuning epoch: 8, train loss: 0.25873, val loss: 0.29750\n",
      "Interaction tuning epoch: 9, train loss: 0.25666, val loss: 0.29772\n",
      "Interaction tuning epoch: 10, train loss: 0.25380, val loss: 0.29422\n",
      "Interaction tuning epoch: 11, train loss: 0.26123, val loss: 0.30240\n",
      "Interaction tuning epoch: 12, train loss: 0.25326, val loss: 0.29194\n",
      "Interaction tuning epoch: 13, train loss: 0.25683, val loss: 0.29674\n",
      "Interaction tuning epoch: 14, train loss: 0.25488, val loss: 0.29961\n",
      "Interaction tuning epoch: 15, train loss: 0.25606, val loss: 0.29529\n",
      "Interaction tuning epoch: 16, train loss: 0.25662, val loss: 0.29738\n",
      "Interaction tuning epoch: 17, train loss: 0.25706, val loss: 0.29654\n",
      "Interaction tuning epoch: 18, train loss: 0.25717, val loss: 0.30106\n",
      "Interaction tuning epoch: 19, train loss: 0.25503, val loss: 0.29523\n",
      "Interaction tuning epoch: 20, train loss: 0.25624, val loss: 0.30245\n",
      "Interaction tuning epoch: 21, train loss: 0.25718, val loss: 0.29518\n",
      "Interaction tuning epoch: 22, train loss: 0.25385, val loss: 0.29988\n",
      "Interaction tuning epoch: 23, train loss: 0.25654, val loss: 0.29575\n",
      "Interaction tuning epoch: 24, train loss: 0.25113, val loss: 0.29282\n",
      "Interaction tuning epoch: 25, train loss: 0.25661, val loss: 0.30034\n",
      "Interaction tuning epoch: 26, train loss: 0.25520, val loss: 0.29587\n",
      "Interaction tuning epoch: 27, train loss: 0.25354, val loss: 0.29731\n",
      "Interaction tuning epoch: 28, train loss: 0.25291, val loss: 0.29353\n",
      "Interaction tuning epoch: 29, train loss: 0.25350, val loss: 0.29628\n",
      "Interaction tuning epoch: 30, train loss: 0.25486, val loss: 0.29655\n",
      "Interaction tuning epoch: 31, train loss: 0.25340, val loss: 0.29558\n",
      "Interaction tuning epoch: 32, train loss: 0.25230, val loss: 0.29347\n",
      "Interaction tuning epoch: 33, train loss: 0.25554, val loss: 0.29459\n",
      "Interaction tuning epoch: 34, train loss: 0.25221, val loss: 0.29600\n",
      "Interaction tuning epoch: 35, train loss: 0.25239, val loss: 0.29310\n",
      "Interaction tuning epoch: 36, train loss: 0.25307, val loss: 0.29524\n",
      "Interaction tuning epoch: 37, train loss: 0.25386, val loss: 0.29513\n",
      "Interaction tuning epoch: 38, train loss: 0.25317, val loss: 0.29840\n",
      "Interaction tuning epoch: 39, train loss: 0.25075, val loss: 0.29355\n",
      "Interaction tuning epoch: 40, train loss: 0.25013, val loss: 0.29416\n",
      "Interaction tuning epoch: 41, train loss: 0.25610, val loss: 0.30045\n",
      "Interaction tuning epoch: 42, train loss: 0.25079, val loss: 0.29139\n",
      "Interaction tuning epoch: 43, train loss: 0.25421, val loss: 0.29743\n",
      "Interaction tuning epoch: 44, train loss: 0.25252, val loss: 0.29762\n",
      "Interaction tuning epoch: 45, train loss: 0.25469, val loss: 0.30049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 46, train loss: 0.24912, val loss: 0.29114\n",
      "Interaction tuning epoch: 47, train loss: 0.25373, val loss: 0.29833\n",
      "Interaction tuning epoch: 48, train loss: 0.25171, val loss: 0.29381\n",
      "Interaction tuning epoch: 49, train loss: 0.25032, val loss: 0.29387\n",
      "Interaction tuning epoch: 50, train loss: 0.25295, val loss: 0.30059\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 60.920217752456665\n",
      "After the gam stage, training error is 0.25295 , validation error is 0.30059\n",
      "missing value counts: 99234\n",
      "#####start auto_tuning#####\n",
      "the best shrinkage is 0.125000\n",
      "[SoftImpute] Max Singular Value of X_init = 3.834933\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.232639 validation BCE=0.317270,rank=3\n",
      "[SoftImpute] Iter 1: observed BCE=0.250367 validation BCE=0.296662,rank=3\n",
      "[SoftImpute] Iter 2: observed BCE=0.250514 validation BCE=0.296135,rank=3\n",
      "[SoftImpute] Iter 3: observed BCE=0.250540 validation BCE=0.296195,rank=3\n",
      "[SoftImpute] Iter 4: observed BCE=0.250550 validation BCE=0.296627,rank=3\n",
      "[SoftImpute] Iter 5: observed BCE=0.250431 validation BCE=0.295677,rank=3\n",
      "[SoftImpute] Iter 6: observed BCE=0.250463 validation BCE=0.296261,rank=3\n",
      "[SoftImpute] Iter 7: observed BCE=0.250390 validation BCE=0.296469,rank=3\n",
      "[SoftImpute] Iter 8: observed BCE=0.250405 validation BCE=0.296563,rank=3\n",
      "[SoftImpute] Iter 9: observed BCE=0.250418 validation BCE=0.296720,rank=3\n",
      "[SoftImpute] Iter 10: observed BCE=0.250499 validation BCE=0.296921,rank=3\n",
      "[SoftImpute] Iter 11: observed BCE=0.250594 validation BCE=0.297329,rank=3\n",
      "[SoftImpute] Iter 12: observed BCE=0.250930 validation BCE=0.297888,rank=3\n",
      "[SoftImpute] Iter 13: observed BCE=0.250994 validation BCE=0.298069,rank=3\n",
      "[SoftImpute] Iter 14: observed BCE=0.251067 validation BCE=0.298214,rank=3\n",
      "[SoftImpute] Iter 15: observed BCE=0.251113 validation BCE=0.298295,rank=3\n",
      "[SoftImpute] Iter 16: observed BCE=0.251155 validation BCE=0.298376,rank=3\n",
      "[SoftImpute] Iter 17: observed BCE=0.251174 validation BCE=0.298408,rank=3\n",
      "[SoftImpute] Iter 18: observed BCE=0.251196 validation BCE=0.298456,rank=3\n",
      "[SoftImpute] Iter 19: observed BCE=0.251202 validation BCE=0.298463,rank=3\n",
      "[SoftImpute] Iter 20: observed BCE=0.251214 validation BCE=0.298496,rank=3\n",
      "[SoftImpute] Iter 21: observed BCE=0.251214 validation BCE=0.298490,rank=3\n",
      "[SoftImpute] Iter 22: observed BCE=0.251222 validation BCE=0.298515,rank=3\n",
      "[SoftImpute] Iter 23: observed BCE=0.251219 validation BCE=0.298503,rank=3\n",
      "[SoftImpute] Iter 24: observed BCE=0.251226 validation BCE=0.298524,rank=3\n",
      "[SoftImpute] Iter 25: observed BCE=0.251222 validation BCE=0.298510,rank=3\n",
      "[SoftImpute] Iter 26: observed BCE=0.251228 validation BCE=0.298528,rank=3\n",
      "[SoftImpute] Iter 27: observed BCE=0.251223 validation BCE=0.298513,rank=3\n",
      "[SoftImpute] Iter 28: observed BCE=0.251228 validation BCE=0.298530,rank=3\n",
      "[SoftImpute] Iter 29: observed BCE=0.251223 validation BCE=0.298514,rank=3\n",
      "[SoftImpute] Iter 30: observed BCE=0.251229 validation BCE=0.298531,rank=3\n",
      "[SoftImpute] Iter 31: observed BCE=0.251224 validation BCE=0.298515,rank=3\n",
      "[SoftImpute] Iter 32: observed BCE=0.251229 validation BCE=0.298532,rank=3\n",
      "[SoftImpute] Iter 33: observed BCE=0.251224 validation BCE=0.298515,rank=3\n",
      "[SoftImpute] Iter 34: observed BCE=0.251229 validation BCE=0.298532,rank=3\n",
      "[SoftImpute] Iter 35: observed BCE=0.251224 validation BCE=0.298515,rank=3\n",
      "[SoftImpute] Iter 36: observed BCE=0.251229 validation BCE=0.298532,rank=3\n",
      "[SoftImpute] Iter 37: observed BCE=0.251224 validation BCE=0.298515,rank=3\n",
      "[SoftImpute] Iter 38: observed BCE=0.251229 validation BCE=0.298532,rank=3\n",
      "[SoftImpute] Iter 39: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 40: observed BCE=0.251229 validation BCE=0.298532,rank=3\n",
      "[SoftImpute] Iter 41: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 42: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 43: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 44: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 45: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 46: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 47: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 48: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 49: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 50: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 51: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 52: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 53: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 54: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 55: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 56: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 57: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 58: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 59: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 60: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 61: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 62: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 63: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 64: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 65: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 66: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 67: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 68: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 69: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 70: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 71: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 72: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 73: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 74: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 75: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 76: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 77: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 78: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 79: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 80: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 81: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 82: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 83: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 84: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 85: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 86: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 87: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 88: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 89: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 90: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 91: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 92: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 93: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 94: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 95: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 96: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 97: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 98: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Iter 99: observed BCE=0.251224 validation BCE=0.298516,rank=3\n",
      "[SoftImpute] Iter 100: observed BCE=0.251229 validation BCE=0.298533,rank=3\n",
      "[SoftImpute] Stopped after iteration 100 for lambda=0.076699\n",
      "final num of user group: 7\n",
      "final num of item group: 16\n",
      "change mode state : True\n",
      "time cost: 33.00244975090027\n",
      "After the matrix factor stage, training error is 0.25123, validation error is 0.29853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68275, val loss: 0.68515\n",
      "Main effects training epoch: 2, train loss: 0.67569, val loss: 0.67966\n",
      "Main effects training epoch: 3, train loss: 0.67188, val loss: 0.67752\n",
      "Main effects training epoch: 4, train loss: 0.66603, val loss: 0.67132\n",
      "Main effects training epoch: 5, train loss: 0.65518, val loss: 0.65942\n",
      "Main effects training epoch: 6, train loss: 0.62766, val loss: 0.63106\n",
      "Main effects training epoch: 7, train loss: 0.58017, val loss: 0.58025\n",
      "Main effects training epoch: 8, train loss: 0.54295, val loss: 0.53557\n",
      "Main effects training epoch: 9, train loss: 0.53861, val loss: 0.52614\n",
      "Main effects training epoch: 10, train loss: 0.53310, val loss: 0.52006\n",
      "Main effects training epoch: 11, train loss: 0.53051, val loss: 0.51667\n",
      "Main effects training epoch: 12, train loss: 0.53021, val loss: 0.51919\n",
      "Main effects training epoch: 13, train loss: 0.52954, val loss: 0.51733\n",
      "Main effects training epoch: 14, train loss: 0.52964, val loss: 0.51773\n",
      "Main effects training epoch: 15, train loss: 0.52909, val loss: 0.51667\n",
      "Main effects training epoch: 16, train loss: 0.52890, val loss: 0.51591\n",
      "Main effects training epoch: 17, train loss: 0.52882, val loss: 0.51639\n",
      "Main effects training epoch: 18, train loss: 0.52927, val loss: 0.51778\n",
      "Main effects training epoch: 19, train loss: 0.53007, val loss: 0.51776\n",
      "Main effects training epoch: 20, train loss: 0.52985, val loss: 0.51882\n",
      "Main effects training epoch: 21, train loss: 0.52892, val loss: 0.51670\n",
      "Main effects training epoch: 22, train loss: 0.52868, val loss: 0.51634\n",
      "Main effects training epoch: 23, train loss: 0.52857, val loss: 0.51640\n",
      "Main effects training epoch: 24, train loss: 0.52830, val loss: 0.51678\n",
      "Main effects training epoch: 25, train loss: 0.52841, val loss: 0.51606\n",
      "Main effects training epoch: 26, train loss: 0.52864, val loss: 0.51786\n",
      "Main effects training epoch: 27, train loss: 0.52858, val loss: 0.51678\n",
      "Main effects training epoch: 28, train loss: 0.52833, val loss: 0.51727\n",
      "Main effects training epoch: 29, train loss: 0.52860, val loss: 0.51805\n",
      "Main effects training epoch: 30, train loss: 0.52865, val loss: 0.51722\n",
      "Main effects training epoch: 31, train loss: 0.52823, val loss: 0.51743\n",
      "Main effects training epoch: 32, train loss: 0.52811, val loss: 0.51745\n",
      "Main effects training epoch: 33, train loss: 0.52815, val loss: 0.51642\n",
      "Main effects training epoch: 34, train loss: 0.52822, val loss: 0.51635\n",
      "Main effects training epoch: 35, train loss: 0.52824, val loss: 0.51724\n",
      "Main effects training epoch: 36, train loss: 0.52940, val loss: 0.51766\n",
      "Main effects training epoch: 37, train loss: 0.52835, val loss: 0.51842\n",
      "Main effects training epoch: 38, train loss: 0.52820, val loss: 0.51605\n",
      "Main effects training epoch: 39, train loss: 0.52821, val loss: 0.51873\n",
      "Main effects training epoch: 40, train loss: 0.52795, val loss: 0.51583\n",
      "Main effects training epoch: 41, train loss: 0.52823, val loss: 0.51734\n",
      "Main effects training epoch: 42, train loss: 0.52783, val loss: 0.51745\n",
      "Main effects training epoch: 43, train loss: 0.52857, val loss: 0.51789\n",
      "Main effects training epoch: 44, train loss: 0.52794, val loss: 0.51681\n",
      "Main effects training epoch: 45, train loss: 0.52805, val loss: 0.51790\n",
      "Main effects training epoch: 46, train loss: 0.52758, val loss: 0.51649\n",
      "Main effects training epoch: 47, train loss: 0.52835, val loss: 0.51749\n",
      "Main effects training epoch: 48, train loss: 0.52800, val loss: 0.51748\n",
      "Main effects training epoch: 49, train loss: 0.52839, val loss: 0.51632\n",
      "Main effects training epoch: 50, train loss: 0.52899, val loss: 0.52046\n",
      "Main effects training epoch: 51, train loss: 0.52888, val loss: 0.51723\n",
      "Main effects training epoch: 52, train loss: 0.52774, val loss: 0.51708\n",
      "Main effects training epoch: 53, train loss: 0.52738, val loss: 0.51688\n",
      "Main effects training epoch: 54, train loss: 0.52765, val loss: 0.51716\n",
      "Main effects training epoch: 55, train loss: 0.52836, val loss: 0.51748\n",
      "Main effects training epoch: 56, train loss: 0.52780, val loss: 0.51780\n",
      "Main effects training epoch: 57, train loss: 0.52803, val loss: 0.51788\n",
      "Main effects training epoch: 58, train loss: 0.52759, val loss: 0.51625\n",
      "Main effects training epoch: 59, train loss: 0.52726, val loss: 0.51739\n",
      "Main effects training epoch: 60, train loss: 0.52728, val loss: 0.51743\n",
      "Main effects training epoch: 61, train loss: 0.52747, val loss: 0.51592\n",
      "Main effects training epoch: 62, train loss: 0.52789, val loss: 0.51797\n",
      "Main effects training epoch: 63, train loss: 0.52742, val loss: 0.51806\n",
      "Main effects training epoch: 64, train loss: 0.52736, val loss: 0.51619\n",
      "Main effects training epoch: 65, train loss: 0.52721, val loss: 0.51752\n",
      "Main effects training epoch: 66, train loss: 0.52789, val loss: 0.51742\n",
      "Main effects training epoch: 67, train loss: 0.52747, val loss: 0.51724\n",
      "Main effects training epoch: 68, train loss: 0.52720, val loss: 0.51692\n",
      "Main effects training epoch: 69, train loss: 0.52730, val loss: 0.51794\n",
      "Main effects training epoch: 70, train loss: 0.52716, val loss: 0.51679\n",
      "Main effects training epoch: 71, train loss: 0.52767, val loss: 0.51648\n",
      "Main effects training epoch: 72, train loss: 0.52783, val loss: 0.51904\n",
      "Main effects training epoch: 73, train loss: 0.52742, val loss: 0.51632\n",
      "Main effects training epoch: 74, train loss: 0.52688, val loss: 0.51689\n",
      "Main effects training epoch: 75, train loss: 0.52691, val loss: 0.51755\n",
      "Main effects training epoch: 76, train loss: 0.52758, val loss: 0.51649\n",
      "Main effects training epoch: 77, train loss: 0.52691, val loss: 0.51827\n",
      "Main effects training epoch: 78, train loss: 0.52735, val loss: 0.51669\n",
      "Main effects training epoch: 79, train loss: 0.52712, val loss: 0.51674\n",
      "Main effects training epoch: 80, train loss: 0.52706, val loss: 0.51785\n",
      "Main effects training epoch: 81, train loss: 0.52694, val loss: 0.51671\n",
      "Main effects training epoch: 82, train loss: 0.52669, val loss: 0.51770\n",
      "Main effects training epoch: 83, train loss: 0.52700, val loss: 0.51739\n",
      "Main effects training epoch: 84, train loss: 0.52807, val loss: 0.51993\n",
      "Main effects training epoch: 85, train loss: 0.52771, val loss: 0.51772\n",
      "Main effects training epoch: 86, train loss: 0.52738, val loss: 0.51694\n",
      "Main effects training epoch: 87, train loss: 0.52757, val loss: 0.51911\n",
      "Main effects training epoch: 88, train loss: 0.52674, val loss: 0.51711\n",
      "Main effects training epoch: 89, train loss: 0.52685, val loss: 0.51753\n",
      "Main effects training epoch: 90, train loss: 0.52674, val loss: 0.51688\n",
      "Main effects training epoch: 91, train loss: 0.52679, val loss: 0.51693\n",
      "Main effects training epoch: 92, train loss: 0.52659, val loss: 0.51800\n",
      "Main effects training epoch: 93, train loss: 0.52701, val loss: 0.51688\n",
      "Main effects training epoch: 94, train loss: 0.52724, val loss: 0.51798\n",
      "Main effects training epoch: 95, train loss: 0.52695, val loss: 0.51790\n",
      "Main effects training epoch: 96, train loss: 0.52713, val loss: 0.51866\n",
      "Main effects training epoch: 97, train loss: 0.52650, val loss: 0.51638\n",
      "Main effects training epoch: 98, train loss: 0.52637, val loss: 0.51797\n",
      "Main effects training epoch: 99, train loss: 0.52684, val loss: 0.51690\n",
      "Main effects training epoch: 100, train loss: 0.52744, val loss: 0.51850\n",
      "Main effects training epoch: 101, train loss: 0.52699, val loss: 0.51692\n",
      "Main effects training epoch: 102, train loss: 0.52661, val loss: 0.51800\n",
      "Main effects training epoch: 103, train loss: 0.52864, val loss: 0.51759\n",
      "Main effects training epoch: 104, train loss: 0.52848, val loss: 0.52006\n",
      "Main effects training epoch: 105, train loss: 0.52779, val loss: 0.51828\n",
      "Main effects training epoch: 106, train loss: 0.52731, val loss: 0.51921\n",
      "Main effects training epoch: 107, train loss: 0.52644, val loss: 0.51672\n",
      "Main effects training epoch: 108, train loss: 0.52630, val loss: 0.51805\n",
      "Main effects training epoch: 109, train loss: 0.52640, val loss: 0.51756\n",
      "Main effects training epoch: 110, train loss: 0.52651, val loss: 0.51829\n",
      "Main effects training epoch: 111, train loss: 0.52788, val loss: 0.51816\n",
      "Main effects training epoch: 112, train loss: 0.52623, val loss: 0.51743\n",
      "Main effects training epoch: 113, train loss: 0.52598, val loss: 0.51615\n",
      "Main effects training epoch: 114, train loss: 0.52643, val loss: 0.51773\n",
      "Main effects training epoch: 115, train loss: 0.52654, val loss: 0.51739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 116, train loss: 0.52618, val loss: 0.51805\n",
      "Main effects training epoch: 117, train loss: 0.52601, val loss: 0.51664\n",
      "Main effects training epoch: 118, train loss: 0.52624, val loss: 0.51761\n",
      "Main effects training epoch: 119, train loss: 0.52602, val loss: 0.51832\n",
      "Main effects training epoch: 120, train loss: 0.52588, val loss: 0.51645\n",
      "Main effects training epoch: 121, train loss: 0.52627, val loss: 0.51779\n",
      "Main effects training epoch: 122, train loss: 0.52665, val loss: 0.51917\n",
      "Main effects training epoch: 123, train loss: 0.52592, val loss: 0.51883\n",
      "Main effects training epoch: 124, train loss: 0.52613, val loss: 0.51610\n",
      "Main effects training epoch: 125, train loss: 0.52641, val loss: 0.51874\n",
      "Main effects training epoch: 126, train loss: 0.52640, val loss: 0.51824\n",
      "Main effects training epoch: 127, train loss: 0.52662, val loss: 0.51859\n",
      "Main effects training epoch: 128, train loss: 0.52592, val loss: 0.51695\n",
      "Main effects training epoch: 129, train loss: 0.52590, val loss: 0.51757\n",
      "Main effects training epoch: 130, train loss: 0.52641, val loss: 0.51714\n",
      "Main effects training epoch: 131, train loss: 0.52635, val loss: 0.51807\n",
      "Main effects training epoch: 132, train loss: 0.52643, val loss: 0.51898\n",
      "Main effects training epoch: 133, train loss: 0.52643, val loss: 0.51709\n",
      "Main effects training epoch: 134, train loss: 0.52601, val loss: 0.51861\n",
      "Main effects training epoch: 135, train loss: 0.52552, val loss: 0.51715\n",
      "Main effects training epoch: 136, train loss: 0.52544, val loss: 0.51744\n",
      "Main effects training epoch: 137, train loss: 0.52577, val loss: 0.51914\n",
      "Main effects training epoch: 138, train loss: 0.52570, val loss: 0.51657\n",
      "Main effects training epoch: 139, train loss: 0.52548, val loss: 0.51716\n",
      "Main effects training epoch: 140, train loss: 0.52533, val loss: 0.51732\n",
      "Main effects training epoch: 141, train loss: 0.52552, val loss: 0.51879\n",
      "Early stop at epoch 141, with validation loss: 0.51879\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.52761, val loss: 0.51699\n",
      "Main effects tuning epoch: 2, train loss: 0.52785, val loss: 0.51941\n",
      "Main effects tuning epoch: 3, train loss: 0.52760, val loss: 0.51752\n",
      "Main effects tuning epoch: 4, train loss: 0.52752, val loss: 0.51694\n",
      "Main effects tuning epoch: 5, train loss: 0.52756, val loss: 0.51753\n",
      "Main effects tuning epoch: 6, train loss: 0.52758, val loss: 0.51808\n",
      "Main effects tuning epoch: 7, train loss: 0.52753, val loss: 0.51840\n",
      "Main effects tuning epoch: 8, train loss: 0.52801, val loss: 0.51863\n",
      "Main effects tuning epoch: 9, train loss: 0.52767, val loss: 0.51784\n",
      "Main effects tuning epoch: 10, train loss: 0.52865, val loss: 0.52009\n",
      "Main effects tuning epoch: 11, train loss: 0.52788, val loss: 0.51779\n",
      "Main effects tuning epoch: 12, train loss: 0.52738, val loss: 0.51819\n",
      "Main effects tuning epoch: 13, train loss: 0.52764, val loss: 0.51793\n",
      "Main effects tuning epoch: 14, train loss: 0.52751, val loss: 0.51718\n",
      "Main effects tuning epoch: 15, train loss: 0.52751, val loss: 0.51929\n",
      "Main effects tuning epoch: 16, train loss: 0.52742, val loss: 0.51704\n",
      "Main effects tuning epoch: 17, train loss: 0.52715, val loss: 0.51805\n",
      "Main effects tuning epoch: 18, train loss: 0.52728, val loss: 0.51836\n",
      "Main effects tuning epoch: 19, train loss: 0.52717, val loss: 0.51827\n",
      "Main effects tuning epoch: 20, train loss: 0.52711, val loss: 0.51775\n",
      "Main effects tuning epoch: 21, train loss: 0.52710, val loss: 0.51892\n",
      "Main effects tuning epoch: 22, train loss: 0.52722, val loss: 0.51803\n",
      "Main effects tuning epoch: 23, train loss: 0.52775, val loss: 0.51836\n",
      "Main effects tuning epoch: 24, train loss: 0.52708, val loss: 0.51855\n",
      "Main effects tuning epoch: 25, train loss: 0.52722, val loss: 0.51893\n",
      "Main effects tuning epoch: 26, train loss: 0.52710, val loss: 0.51798\n",
      "Main effects tuning epoch: 27, train loss: 0.52692, val loss: 0.51830\n",
      "Main effects tuning epoch: 28, train loss: 0.52684, val loss: 0.51778\n",
      "Main effects tuning epoch: 29, train loss: 0.52687, val loss: 0.51796\n",
      "Main effects tuning epoch: 30, train loss: 0.52679, val loss: 0.51828\n",
      "Main effects tuning epoch: 31, train loss: 0.52693, val loss: 0.51716\n",
      "Main effects tuning epoch: 32, train loss: 0.52688, val loss: 0.51892\n",
      "Main effects tuning epoch: 33, train loss: 0.52780, val loss: 0.51916\n",
      "Main effects tuning epoch: 34, train loss: 0.52801, val loss: 0.51952\n",
      "Main effects tuning epoch: 35, train loss: 0.52772, val loss: 0.51801\n",
      "Main effects tuning epoch: 36, train loss: 0.52717, val loss: 0.52039\n",
      "Main effects tuning epoch: 37, train loss: 0.52728, val loss: 0.51704\n",
      "Main effects tuning epoch: 38, train loss: 0.52678, val loss: 0.51954\n",
      "Main effects tuning epoch: 39, train loss: 0.52652, val loss: 0.51840\n",
      "Main effects tuning epoch: 40, train loss: 0.52673, val loss: 0.51779\n",
      "Main effects tuning epoch: 41, train loss: 0.52661, val loss: 0.51855\n",
      "Main effects tuning epoch: 42, train loss: 0.52678, val loss: 0.51845\n",
      "Main effects tuning epoch: 43, train loss: 0.52651, val loss: 0.51787\n",
      "Main effects tuning epoch: 44, train loss: 0.52660, val loss: 0.51960\n",
      "Main effects tuning epoch: 45, train loss: 0.52647, val loss: 0.51724\n",
      "Main effects tuning epoch: 46, train loss: 0.52675, val loss: 0.51830\n",
      "Main effects tuning epoch: 47, train loss: 0.52666, val loss: 0.51887\n",
      "Main effects tuning epoch: 48, train loss: 0.52730, val loss: 0.51998\n",
      "Main effects tuning epoch: 49, train loss: 0.52657, val loss: 0.51816\n",
      "Main effects tuning epoch: 50, train loss: 0.52667, val loss: 0.51799\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.49792, val loss: 0.49610\n",
      "Interaction training epoch: 2, train loss: 0.41716, val loss: 0.40130\n",
      "Interaction training epoch: 3, train loss: 0.31770, val loss: 0.31757\n",
      "Interaction training epoch: 4, train loss: 0.30989, val loss: 0.30800\n",
      "Interaction training epoch: 5, train loss: 0.30770, val loss: 0.30289\n",
      "Interaction training epoch: 6, train loss: 0.30529, val loss: 0.29835\n",
      "Interaction training epoch: 7, train loss: 0.30028, val loss: 0.29303\n",
      "Interaction training epoch: 8, train loss: 0.29589, val loss: 0.29403\n",
      "Interaction training epoch: 9, train loss: 0.29447, val loss: 0.29204\n",
      "Interaction training epoch: 10, train loss: 0.29257, val loss: 0.29300\n",
      "Interaction training epoch: 11, train loss: 0.29516, val loss: 0.29313\n",
      "Interaction training epoch: 12, train loss: 0.29317, val loss: 0.29051\n",
      "Interaction training epoch: 13, train loss: 0.29297, val loss: 0.29593\n",
      "Interaction training epoch: 14, train loss: 0.28995, val loss: 0.28732\n",
      "Interaction training epoch: 15, train loss: 0.29091, val loss: 0.29028\n",
      "Interaction training epoch: 16, train loss: 0.29333, val loss: 0.29251\n",
      "Interaction training epoch: 17, train loss: 0.29018, val loss: 0.28850\n",
      "Interaction training epoch: 18, train loss: 0.28789, val loss: 0.28948\n",
      "Interaction training epoch: 19, train loss: 0.28979, val loss: 0.29164\n",
      "Interaction training epoch: 20, train loss: 0.28832, val loss: 0.29060\n",
      "Interaction training epoch: 21, train loss: 0.28661, val loss: 0.28810\n",
      "Interaction training epoch: 22, train loss: 0.28892, val loss: 0.28919\n",
      "Interaction training epoch: 23, train loss: 0.28676, val loss: 0.29179\n",
      "Interaction training epoch: 24, train loss: 0.29142, val loss: 0.29484\n",
      "Interaction training epoch: 25, train loss: 0.28704, val loss: 0.29075\n",
      "Interaction training epoch: 26, train loss: 0.28941, val loss: 0.28944\n",
      "Interaction training epoch: 27, train loss: 0.28902, val loss: 0.29116\n",
      "Interaction training epoch: 28, train loss: 0.28810, val loss: 0.28972\n",
      "Interaction training epoch: 29, train loss: 0.28404, val loss: 0.28919\n",
      "Interaction training epoch: 30, train loss: 0.28732, val loss: 0.29227\n",
      "Interaction training epoch: 31, train loss: 0.29236, val loss: 0.29435\n",
      "Interaction training epoch: 32, train loss: 0.28701, val loss: 0.29356\n",
      "Interaction training epoch: 33, train loss: 0.28764, val loss: 0.28803\n",
      "Interaction training epoch: 34, train loss: 0.28691, val loss: 0.28811\n",
      "Interaction training epoch: 35, train loss: 0.28383, val loss: 0.28842\n",
      "Interaction training epoch: 36, train loss: 0.28671, val loss: 0.28768\n",
      "Interaction training epoch: 37, train loss: 0.28386, val loss: 0.28736\n",
      "Interaction training epoch: 38, train loss: 0.28433, val loss: 0.28983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 39, train loss: 0.28372, val loss: 0.28599\n",
      "Interaction training epoch: 40, train loss: 0.28053, val loss: 0.28294\n",
      "Interaction training epoch: 41, train loss: 0.29043, val loss: 0.29871\n",
      "Interaction training epoch: 42, train loss: 0.28770, val loss: 0.28695\n",
      "Interaction training epoch: 43, train loss: 0.28137, val loss: 0.28672\n",
      "Interaction training epoch: 44, train loss: 0.28098, val loss: 0.28522\n",
      "Interaction training epoch: 45, train loss: 0.28503, val loss: 0.28896\n",
      "Interaction training epoch: 46, train loss: 0.28259, val loss: 0.28306\n",
      "Interaction training epoch: 47, train loss: 0.28023, val loss: 0.28406\n",
      "Interaction training epoch: 48, train loss: 0.28440, val loss: 0.28769\n",
      "Interaction training epoch: 49, train loss: 0.28088, val loss: 0.28296\n",
      "Interaction training epoch: 50, train loss: 0.28122, val loss: 0.28523\n",
      "Interaction training epoch: 51, train loss: 0.28147, val loss: 0.28346\n",
      "Interaction training epoch: 52, train loss: 0.28314, val loss: 0.28718\n",
      "Interaction training epoch: 53, train loss: 0.27851, val loss: 0.27978\n",
      "Interaction training epoch: 54, train loss: 0.27995, val loss: 0.28312\n",
      "Interaction training epoch: 55, train loss: 0.27950, val loss: 0.27882\n",
      "Interaction training epoch: 56, train loss: 0.27937, val loss: 0.28533\n",
      "Interaction training epoch: 57, train loss: 0.27952, val loss: 0.27922\n",
      "Interaction training epoch: 58, train loss: 0.28140, val loss: 0.28529\n",
      "Interaction training epoch: 59, train loss: 0.28186, val loss: 0.27945\n",
      "Interaction training epoch: 60, train loss: 0.27700, val loss: 0.28158\n",
      "Interaction training epoch: 61, train loss: 0.27974, val loss: 0.28217\n",
      "Interaction training epoch: 62, train loss: 0.27939, val loss: 0.28482\n",
      "Interaction training epoch: 63, train loss: 0.27919, val loss: 0.28495\n",
      "Interaction training epoch: 64, train loss: 0.27687, val loss: 0.27769\n",
      "Interaction training epoch: 65, train loss: 0.27696, val loss: 0.27912\n",
      "Interaction training epoch: 66, train loss: 0.28139, val loss: 0.28404\n",
      "Interaction training epoch: 67, train loss: 0.27849, val loss: 0.28120\n",
      "Interaction training epoch: 68, train loss: 0.27646, val loss: 0.28197\n",
      "Interaction training epoch: 69, train loss: 0.27679, val loss: 0.27859\n",
      "Interaction training epoch: 70, train loss: 0.27955, val loss: 0.28216\n",
      "Interaction training epoch: 71, train loss: 0.27546, val loss: 0.27924\n",
      "Interaction training epoch: 72, train loss: 0.28013, val loss: 0.28586\n",
      "Interaction training epoch: 73, train loss: 0.28015, val loss: 0.27830\n",
      "Interaction training epoch: 74, train loss: 0.27500, val loss: 0.28048\n",
      "Interaction training epoch: 75, train loss: 0.27836, val loss: 0.28154\n",
      "Interaction training epoch: 76, train loss: 0.27560, val loss: 0.27898\n",
      "Interaction training epoch: 77, train loss: 0.27519, val loss: 0.28050\n",
      "Interaction training epoch: 78, train loss: 0.27433, val loss: 0.27970\n",
      "Interaction training epoch: 79, train loss: 0.27620, val loss: 0.28161\n",
      "Interaction training epoch: 80, train loss: 0.27352, val loss: 0.27501\n",
      "Interaction training epoch: 81, train loss: 0.27402, val loss: 0.27957\n",
      "Interaction training epoch: 82, train loss: 0.27437, val loss: 0.27573\n",
      "Interaction training epoch: 83, train loss: 0.27433, val loss: 0.28326\n",
      "Interaction training epoch: 84, train loss: 0.27316, val loss: 0.27639\n",
      "Interaction training epoch: 85, train loss: 0.27221, val loss: 0.27845\n",
      "Interaction training epoch: 86, train loss: 0.27111, val loss: 0.27734\n",
      "Interaction training epoch: 87, train loss: 0.27136, val loss: 0.27526\n",
      "Interaction training epoch: 88, train loss: 0.27212, val loss: 0.27917\n",
      "Interaction training epoch: 89, train loss: 0.27069, val loss: 0.27826\n",
      "Interaction training epoch: 90, train loss: 0.27332, val loss: 0.27716\n",
      "Interaction training epoch: 91, train loss: 0.27154, val loss: 0.27924\n",
      "Interaction training epoch: 92, train loss: 0.27079, val loss: 0.28098\n",
      "Interaction training epoch: 93, train loss: 0.27427, val loss: 0.27910\n",
      "Interaction training epoch: 94, train loss: 0.27128, val loss: 0.27627\n",
      "Interaction training epoch: 95, train loss: 0.27053, val loss: 0.27984\n",
      "Interaction training epoch: 96, train loss: 0.27118, val loss: 0.27736\n",
      "Interaction training epoch: 97, train loss: 0.26944, val loss: 0.27876\n",
      "Interaction training epoch: 98, train loss: 0.27053, val loss: 0.28050\n",
      "Interaction training epoch: 99, train loss: 0.26804, val loss: 0.27367\n",
      "Interaction training epoch: 100, train loss: 0.26577, val loss: 0.27682\n",
      "Interaction training epoch: 101, train loss: 0.26789, val loss: 0.27365\n",
      "Interaction training epoch: 102, train loss: 0.26721, val loss: 0.28084\n",
      "Interaction training epoch: 103, train loss: 0.26799, val loss: 0.27868\n",
      "Interaction training epoch: 104, train loss: 0.26446, val loss: 0.27386\n",
      "Interaction training epoch: 105, train loss: 0.26181, val loss: 0.27224\n",
      "Interaction training epoch: 106, train loss: 0.26327, val loss: 0.27041\n",
      "Interaction training epoch: 107, train loss: 0.26092, val loss: 0.27029\n",
      "Interaction training epoch: 108, train loss: 0.26112, val loss: 0.27345\n",
      "Interaction training epoch: 109, train loss: 0.26117, val loss: 0.27081\n",
      "Interaction training epoch: 110, train loss: 0.26371, val loss: 0.26957\n",
      "Interaction training epoch: 111, train loss: 0.25604, val loss: 0.26622\n",
      "Interaction training epoch: 112, train loss: 0.25929, val loss: 0.26934\n",
      "Interaction training epoch: 113, train loss: 0.25856, val loss: 0.27284\n",
      "Interaction training epoch: 114, train loss: 0.25510, val loss: 0.26913\n",
      "Interaction training epoch: 115, train loss: 0.25496, val loss: 0.26517\n",
      "Interaction training epoch: 116, train loss: 0.25581, val loss: 0.26963\n",
      "Interaction training epoch: 117, train loss: 0.25739, val loss: 0.27012\n",
      "Interaction training epoch: 118, train loss: 0.25350, val loss: 0.27117\n",
      "Interaction training epoch: 119, train loss: 0.25512, val loss: 0.26353\n",
      "Interaction training epoch: 120, train loss: 0.25055, val loss: 0.26799\n",
      "Interaction training epoch: 121, train loss: 0.25496, val loss: 0.26452\n",
      "Interaction training epoch: 122, train loss: 0.25253, val loss: 0.27007\n",
      "Interaction training epoch: 123, train loss: 0.25026, val loss: 0.26604\n",
      "Interaction training epoch: 124, train loss: 0.25268, val loss: 0.26844\n",
      "Interaction training epoch: 125, train loss: 0.25345, val loss: 0.26541\n",
      "Interaction training epoch: 126, train loss: 0.25463, val loss: 0.27197\n",
      "Interaction training epoch: 127, train loss: 0.24727, val loss: 0.26063\n",
      "Interaction training epoch: 128, train loss: 0.25113, val loss: 0.26837\n",
      "Interaction training epoch: 129, train loss: 0.25086, val loss: 0.26543\n",
      "Interaction training epoch: 130, train loss: 0.24796, val loss: 0.26618\n",
      "Interaction training epoch: 131, train loss: 0.24988, val loss: 0.26716\n",
      "Interaction training epoch: 132, train loss: 0.24657, val loss: 0.26553\n",
      "Interaction training epoch: 133, train loss: 0.24962, val loss: 0.26819\n",
      "Interaction training epoch: 134, train loss: 0.24722, val loss: 0.26359\n",
      "Interaction training epoch: 135, train loss: 0.24424, val loss: 0.26496\n",
      "Interaction training epoch: 136, train loss: 0.24864, val loss: 0.26529\n",
      "Interaction training epoch: 137, train loss: 0.24585, val loss: 0.26359\n",
      "Interaction training epoch: 138, train loss: 0.24608, val loss: 0.26900\n",
      "Interaction training epoch: 139, train loss: 0.24684, val loss: 0.26557\n",
      "Interaction training epoch: 140, train loss: 0.24358, val loss: 0.26213\n",
      "Interaction training epoch: 141, train loss: 0.24566, val loss: 0.26490\n",
      "Interaction training epoch: 142, train loss: 0.24495, val loss: 0.26616\n",
      "Interaction training epoch: 143, train loss: 0.24299, val loss: 0.26123\n",
      "Interaction training epoch: 144, train loss: 0.24386, val loss: 0.26226\n",
      "Interaction training epoch: 145, train loss: 0.24261, val loss: 0.26206\n",
      "Interaction training epoch: 146, train loss: 0.24372, val loss: 0.26652\n",
      "Interaction training epoch: 147, train loss: 0.24700, val loss: 0.26628\n",
      "Interaction training epoch: 148, train loss: 0.24353, val loss: 0.26557\n",
      "Interaction training epoch: 149, train loss: 0.23999, val loss: 0.25982\n",
      "Interaction training epoch: 150, train loss: 0.24377, val loss: 0.26589\n",
      "Interaction training epoch: 151, train loss: 0.24078, val loss: 0.26123\n",
      "Interaction training epoch: 152, train loss: 0.24477, val loss: 0.26826\n",
      "Interaction training epoch: 153, train loss: 0.24413, val loss: 0.26629\n",
      "Interaction training epoch: 154, train loss: 0.24308, val loss: 0.26528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 155, train loss: 0.24015, val loss: 0.25957\n",
      "Interaction training epoch: 156, train loss: 0.24318, val loss: 0.26188\n",
      "Interaction training epoch: 157, train loss: 0.24102, val loss: 0.26203\n",
      "Interaction training epoch: 158, train loss: 0.24395, val loss: 0.26605\n",
      "Interaction training epoch: 159, train loss: 0.24266, val loss: 0.25945\n",
      "Interaction training epoch: 160, train loss: 0.23884, val loss: 0.26010\n",
      "Interaction training epoch: 161, train loss: 0.24207, val loss: 0.26387\n",
      "Interaction training epoch: 162, train loss: 0.24035, val loss: 0.26318\n",
      "Interaction training epoch: 163, train loss: 0.24050, val loss: 0.26089\n",
      "Interaction training epoch: 164, train loss: 0.24091, val loss: 0.26310\n",
      "Interaction training epoch: 165, train loss: 0.23989, val loss: 0.26496\n",
      "Interaction training epoch: 166, train loss: 0.24063, val loss: 0.26241\n",
      "Interaction training epoch: 167, train loss: 0.24303, val loss: 0.26728\n",
      "Interaction training epoch: 168, train loss: 0.24212, val loss: 0.26345\n",
      "Interaction training epoch: 169, train loss: 0.24007, val loss: 0.26335\n",
      "Interaction training epoch: 170, train loss: 0.23723, val loss: 0.26229\n",
      "Interaction training epoch: 171, train loss: 0.23893, val loss: 0.26377\n",
      "Interaction training epoch: 172, train loss: 0.24232, val loss: 0.26279\n",
      "Interaction training epoch: 173, train loss: 0.23792, val loss: 0.26218\n",
      "Interaction training epoch: 174, train loss: 0.23763, val loss: 0.26278\n",
      "Interaction training epoch: 175, train loss: 0.23781, val loss: 0.26319\n",
      "Interaction training epoch: 176, train loss: 0.23807, val loss: 0.26151\n",
      "Interaction training epoch: 177, train loss: 0.23659, val loss: 0.26139\n",
      "Interaction training epoch: 178, train loss: 0.24052, val loss: 0.26727\n",
      "Interaction training epoch: 179, train loss: 0.23804, val loss: 0.26345\n",
      "Interaction training epoch: 180, train loss: 0.24035, val loss: 0.26146\n",
      "Interaction training epoch: 181, train loss: 0.24021, val loss: 0.26881\n",
      "Interaction training epoch: 182, train loss: 0.23821, val loss: 0.26280\n",
      "Interaction training epoch: 183, train loss: 0.23708, val loss: 0.26396\n",
      "Interaction training epoch: 184, train loss: 0.23702, val loss: 0.26045\n",
      "Interaction training epoch: 185, train loss: 0.23756, val loss: 0.26269\n",
      "Interaction training epoch: 186, train loss: 0.23658, val loss: 0.26063\n",
      "Interaction training epoch: 187, train loss: 0.24119, val loss: 0.26714\n",
      "Interaction training epoch: 188, train loss: 0.23738, val loss: 0.26281\n",
      "Interaction training epoch: 189, train loss: 0.23410, val loss: 0.26105\n",
      "Interaction training epoch: 190, train loss: 0.23608, val loss: 0.25623\n",
      "Interaction training epoch: 191, train loss: 0.23611, val loss: 0.26204\n",
      "Interaction training epoch: 192, train loss: 0.23746, val loss: 0.26344\n",
      "Interaction training epoch: 193, train loss: 0.23780, val loss: 0.26498\n",
      "Interaction training epoch: 194, train loss: 0.23805, val loss: 0.26259\n",
      "Interaction training epoch: 195, train loss: 0.23975, val loss: 0.26715\n",
      "Interaction training epoch: 196, train loss: 0.23918, val loss: 0.26361\n",
      "Interaction training epoch: 197, train loss: 0.23446, val loss: 0.25911\n",
      "Interaction training epoch: 198, train loss: 0.23468, val loss: 0.26098\n",
      "Interaction training epoch: 199, train loss: 0.23558, val loss: 0.26138\n",
      "Interaction training epoch: 200, train loss: 0.23687, val loss: 0.26115\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########2 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.23817, val loss: 0.26084\n",
      "Interaction tuning epoch: 2, train loss: 0.23993, val loss: 0.25893\n",
      "Interaction tuning epoch: 3, train loss: 0.23974, val loss: 0.26340\n",
      "Interaction tuning epoch: 4, train loss: 0.24074, val loss: 0.26049\n",
      "Interaction tuning epoch: 5, train loss: 0.23773, val loss: 0.26325\n",
      "Interaction tuning epoch: 6, train loss: 0.23925, val loss: 0.25985\n",
      "Interaction tuning epoch: 7, train loss: 0.23914, val loss: 0.26328\n",
      "Interaction tuning epoch: 8, train loss: 0.23737, val loss: 0.25623\n",
      "Interaction tuning epoch: 9, train loss: 0.23961, val loss: 0.26102\n",
      "Interaction tuning epoch: 10, train loss: 0.23728, val loss: 0.26121\n",
      "Interaction tuning epoch: 11, train loss: 0.23798, val loss: 0.25766\n",
      "Interaction tuning epoch: 12, train loss: 0.23454, val loss: 0.25618\n",
      "Interaction tuning epoch: 13, train loss: 0.23787, val loss: 0.26193\n",
      "Interaction tuning epoch: 14, train loss: 0.23700, val loss: 0.26031\n",
      "Interaction tuning epoch: 15, train loss: 0.23607, val loss: 0.25869\n",
      "Interaction tuning epoch: 16, train loss: 0.23543, val loss: 0.25784\n",
      "Interaction tuning epoch: 17, train loss: 0.23393, val loss: 0.25512\n",
      "Interaction tuning epoch: 18, train loss: 0.23743, val loss: 0.26040\n",
      "Interaction tuning epoch: 19, train loss: 0.23545, val loss: 0.25724\n",
      "Interaction tuning epoch: 20, train loss: 0.23574, val loss: 0.25833\n",
      "Interaction tuning epoch: 21, train loss: 0.23365, val loss: 0.25486\n",
      "Interaction tuning epoch: 22, train loss: 0.23572, val loss: 0.25683\n",
      "Interaction tuning epoch: 23, train loss: 0.23820, val loss: 0.26392\n",
      "Interaction tuning epoch: 24, train loss: 0.24044, val loss: 0.26385\n",
      "Interaction tuning epoch: 25, train loss: 0.23829, val loss: 0.25849\n",
      "Interaction tuning epoch: 26, train loss: 0.23711, val loss: 0.25986\n",
      "Interaction tuning epoch: 27, train loss: 0.23788, val loss: 0.25888\n",
      "Interaction tuning epoch: 28, train loss: 0.23565, val loss: 0.25942\n",
      "Interaction tuning epoch: 29, train loss: 0.23418, val loss: 0.25854\n",
      "Interaction tuning epoch: 30, train loss: 0.23476, val loss: 0.25510\n",
      "Interaction tuning epoch: 31, train loss: 0.23957, val loss: 0.26643\n",
      "Interaction tuning epoch: 32, train loss: 0.23833, val loss: 0.26298\n",
      "Interaction tuning epoch: 33, train loss: 0.23676, val loss: 0.25603\n",
      "Interaction tuning epoch: 34, train loss: 0.23535, val loss: 0.26350\n",
      "Interaction tuning epoch: 35, train loss: 0.23521, val loss: 0.25866\n",
      "Interaction tuning epoch: 36, train loss: 0.23557, val loss: 0.25700\n",
      "Interaction tuning epoch: 37, train loss: 0.23291, val loss: 0.25616\n",
      "Interaction tuning epoch: 38, train loss: 0.23409, val loss: 0.25812\n",
      "Interaction tuning epoch: 39, train loss: 0.23483, val loss: 0.25595\n",
      "Interaction tuning epoch: 40, train loss: 0.23253, val loss: 0.25821\n",
      "Interaction tuning epoch: 41, train loss: 0.23233, val loss: 0.25209\n",
      "Interaction tuning epoch: 42, train loss: 0.23285, val loss: 0.25583\n",
      "Interaction tuning epoch: 43, train loss: 0.23687, val loss: 0.26463\n",
      "Interaction tuning epoch: 44, train loss: 0.23334, val loss: 0.25249\n",
      "Interaction tuning epoch: 45, train loss: 0.23111, val loss: 0.25717\n",
      "Interaction tuning epoch: 46, train loss: 0.23327, val loss: 0.25607\n",
      "Interaction tuning epoch: 47, train loss: 0.23254, val loss: 0.25412\n",
      "Interaction tuning epoch: 48, train loss: 0.23159, val loss: 0.25527\n",
      "Interaction tuning epoch: 49, train loss: 0.23153, val loss: 0.25567\n",
      "Interaction tuning epoch: 50, train loss: 0.23419, val loss: 0.25760\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 52.152413845062256\n",
      "After the gam stage, training error is 0.23419 , validation error is 0.25760\n",
      "missing value counts: 99255\n",
      "#####start auto_tuning#####\n",
      "the best shrinkage is 0.656250\n",
      "[SoftImpute] Max Singular Value of X_init = 3.763697\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.213098 validation BCE=0.259479,rank=3\n",
      "[SoftImpute] Iter 1: observed BCE=0.213139 validation BCE=0.259354,rank=3\n",
      "[SoftImpute] Iter 2: observed BCE=0.212525 validation BCE=0.258147,rank=3\n",
      "[SoftImpute] Iter 3: observed BCE=0.212309 validation BCE=0.257481,rank=3\n",
      "[SoftImpute] Iter 4: observed BCE=0.211746 validation BCE=0.256813,rank=3\n",
      "[SoftImpute] Iter 5: observed BCE=0.211103 validation BCE=0.256480,rank=3\n",
      "[SoftImpute] Iter 6: observed BCE=0.211063 validation BCE=0.256256,rank=3\n",
      "[SoftImpute] Iter 7: observed BCE=0.210367 validation BCE=0.256242,rank=3\n",
      "[SoftImpute] Iter 8: observed BCE=0.210415 validation BCE=0.255882,rank=3\n",
      "[SoftImpute] Iter 9: observed BCE=0.209945 validation BCE=0.256425,rank=3\n",
      "[SoftImpute] Iter 10: observed BCE=0.209875 validation BCE=0.256251,rank=3\n",
      "[SoftImpute] Iter 11: observed BCE=0.209873 validation BCE=0.256694,rank=3\n",
      "[SoftImpute] Iter 12: observed BCE=0.209876 validation BCE=0.256595,rank=3\n",
      "[SoftImpute] Iter 13: observed BCE=0.209674 validation BCE=0.257554,rank=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 14: observed BCE=0.209698 validation BCE=0.258153,rank=3\n",
      "[SoftImpute] Iter 15: observed BCE=0.209513 validation BCE=0.267075,rank=3\n",
      "[SoftImpute] Iter 16: observed BCE=0.209623 validation BCE=0.266712,rank=3\n",
      "[SoftImpute] Iter 17: observed BCE=0.209435 validation BCE=0.267258,rank=3\n",
      "[SoftImpute] Iter 18: observed BCE=0.209446 validation BCE=0.266664,rank=3\n",
      "[SoftImpute] Iter 19: observed BCE=0.209578 validation BCE=0.267211,rank=3\n",
      "[SoftImpute] Iter 20: observed BCE=0.209494 validation BCE=0.266705,rank=3\n",
      "[SoftImpute] Iter 21: observed BCE=0.209310 validation BCE=0.266926,rank=3\n",
      "[SoftImpute] Iter 22: observed BCE=0.209583 validation BCE=0.266745,rank=3\n",
      "[SoftImpute] Iter 23: observed BCE=0.209403 validation BCE=0.267305,rank=3\n",
      "[SoftImpute] Iter 24: observed BCE=0.209600 validation BCE=0.267249,rank=3\n",
      "[SoftImpute] Iter 25: observed BCE=0.209529 validation BCE=0.267464,rank=3\n",
      "[SoftImpute] Iter 26: observed BCE=0.209606 validation BCE=0.267472,rank=3\n",
      "[SoftImpute] Iter 27: observed BCE=0.209671 validation BCE=0.267471,rank=3\n",
      "[SoftImpute] Iter 28: observed BCE=0.209569 validation BCE=0.267364,rank=3\n",
      "[SoftImpute] Iter 29: observed BCE=0.209523 validation BCE=0.267411,rank=3\n",
      "[SoftImpute] Iter 30: observed BCE=0.209631 validation BCE=0.267534,rank=3\n",
      "[SoftImpute] Iter 31: observed BCE=0.209824 validation BCE=0.267410,rank=3\n",
      "[SoftImpute] Iter 32: observed BCE=0.209553 validation BCE=0.267699,rank=3\n",
      "[SoftImpute] Iter 33: observed BCE=0.209619 validation BCE=0.267461,rank=3\n",
      "[SoftImpute] Iter 34: observed BCE=0.209585 validation BCE=0.267714,rank=3\n",
      "[SoftImpute] Iter 35: observed BCE=0.209449 validation BCE=0.267051,rank=3\n",
      "[SoftImpute] Iter 36: observed BCE=0.209408 validation BCE=0.266866,rank=3\n",
      "[SoftImpute] Iter 37: observed BCE=0.209629 validation BCE=0.266721,rank=3\n",
      "[SoftImpute] Iter 38: observed BCE=0.209521 validation BCE=0.266515,rank=3\n",
      "[SoftImpute] Iter 39: observed BCE=0.209648 validation BCE=0.267078,rank=3\n",
      "[SoftImpute] Iter 40: observed BCE=0.209745 validation BCE=0.266503,rank=3\n",
      "[SoftImpute] Iter 41: observed BCE=0.209812 validation BCE=0.266463,rank=3\n",
      "[SoftImpute] Iter 42: observed BCE=0.209839 validation BCE=0.266786,rank=3\n",
      "[SoftImpute] Iter 43: observed BCE=0.209878 validation BCE=0.266849,rank=3\n",
      "[SoftImpute] Iter 44: observed BCE=0.209846 validation BCE=0.266681,rank=3\n",
      "[SoftImpute] Iter 45: observed BCE=0.210170 validation BCE=0.266686,rank=3\n",
      "[SoftImpute] Iter 46: observed BCE=0.209910 validation BCE=0.266271,rank=3\n",
      "[SoftImpute] Iter 47: observed BCE=0.209625 validation BCE=0.266838,rank=3\n",
      "[SoftImpute] Iter 48: observed BCE=0.210072 validation BCE=0.266467,rank=3\n",
      "[SoftImpute] Iter 49: observed BCE=0.210142 validation BCE=0.266412,rank=3\n",
      "[SoftImpute] Iter 50: observed BCE=0.209882 validation BCE=0.266615,rank=3\n",
      "[SoftImpute] Iter 51: observed BCE=0.209777 validation BCE=0.266709,rank=3\n",
      "[SoftImpute] Iter 52: observed BCE=0.209886 validation BCE=0.266666,rank=3\n",
      "[SoftImpute] Iter 53: observed BCE=0.209925 validation BCE=0.266531,rank=3\n",
      "[SoftImpute] Iter 54: observed BCE=0.209681 validation BCE=0.266349,rank=3\n",
      "[SoftImpute] Iter 55: observed BCE=0.209537 validation BCE=0.266839,rank=3\n",
      "[SoftImpute] Iter 56: observed BCE=0.210020 validation BCE=0.266502,rank=3\n",
      "[SoftImpute] Iter 57: observed BCE=0.210326 validation BCE=0.266404,rank=3\n",
      "[SoftImpute] Iter 58: observed BCE=0.209870 validation BCE=0.266469,rank=3\n",
      "[SoftImpute] Iter 59: observed BCE=0.209739 validation BCE=0.266488,rank=3\n",
      "[SoftImpute] Iter 60: observed BCE=0.209985 validation BCE=0.266693,rank=3\n",
      "[SoftImpute] Iter 61: observed BCE=0.209959 validation BCE=0.266238,rank=3\n",
      "[SoftImpute] Iter 62: observed BCE=0.209781 validation BCE=0.266467,rank=3\n",
      "[SoftImpute] Iter 63: observed BCE=0.209864 validation BCE=0.266612,rank=3\n",
      "[SoftImpute] Iter 64: observed BCE=0.210105 validation BCE=0.266592,rank=3\n",
      "[SoftImpute] Iter 65: observed BCE=0.210091 validation BCE=0.266136,rank=3\n",
      "[SoftImpute] Iter 66: observed BCE=0.209983 validation BCE=0.266536,rank=3\n",
      "[SoftImpute] Iter 67: observed BCE=0.209989 validation BCE=0.266366,rank=3\n",
      "[SoftImpute] Iter 68: observed BCE=0.210058 validation BCE=0.266404,rank=3\n",
      "[SoftImpute] Iter 69: observed BCE=0.210438 validation BCE=0.266529,rank=3\n",
      "[SoftImpute] Iter 70: observed BCE=0.210419 validation BCE=0.266525,rank=3\n",
      "[SoftImpute] Iter 71: observed BCE=0.210213 validation BCE=0.266379,rank=3\n",
      "[SoftImpute] Iter 72: observed BCE=0.210459 validation BCE=0.266604,rank=3\n",
      "[SoftImpute] Iter 73: observed BCE=0.210321 validation BCE=0.266365,rank=3\n",
      "[SoftImpute] Iter 74: observed BCE=0.210460 validation BCE=0.266413,rank=3\n",
      "[SoftImpute] Iter 75: observed BCE=0.210335 validation BCE=0.266694,rank=3\n",
      "[SoftImpute] Iter 76: observed BCE=0.210242 validation BCE=0.266541,rank=3\n",
      "[SoftImpute] Iter 77: observed BCE=0.210035 validation BCE=0.266323,rank=3\n",
      "[SoftImpute] Iter 78: observed BCE=0.210320 validation BCE=0.266542,rank=3\n",
      "[SoftImpute] Iter 79: observed BCE=0.210340 validation BCE=0.266437,rank=3\n",
      "[SoftImpute] Iter 80: observed BCE=0.210532 validation BCE=0.266306,rank=3\n",
      "[SoftImpute] Iter 81: observed BCE=0.210443 validation BCE=0.266454,rank=3\n",
      "[SoftImpute] Iter 82: observed BCE=0.210216 validation BCE=0.266798,rank=3\n",
      "[SoftImpute] Iter 83: observed BCE=0.210046 validation BCE=0.266537,rank=3\n",
      "[SoftImpute] Iter 84: observed BCE=0.210351 validation BCE=0.266332,rank=3\n",
      "[SoftImpute] Iter 85: observed BCE=0.210589 validation BCE=0.266409,rank=3\n",
      "[SoftImpute] Iter 86: observed BCE=0.210615 validation BCE=0.266524,rank=3\n",
      "[SoftImpute] Iter 87: observed BCE=0.210495 validation BCE=0.266278,rank=3\n",
      "[SoftImpute] Iter 88: observed BCE=0.210318 validation BCE=0.266668,rank=3\n",
      "[SoftImpute] Iter 89: observed BCE=0.209987 validation BCE=0.266601,rank=3\n",
      "[SoftImpute] Iter 90: observed BCE=0.210490 validation BCE=0.266466,rank=3\n",
      "[SoftImpute] Iter 91: observed BCE=0.210440 validation BCE=0.266253,rank=3\n",
      "[SoftImpute] Iter 92: observed BCE=0.210233 validation BCE=0.266704,rank=3\n",
      "[SoftImpute] Iter 93: observed BCE=0.210125 validation BCE=0.266423,rank=3\n",
      "[SoftImpute] Iter 94: observed BCE=0.210181 validation BCE=0.266751,rank=3\n",
      "[SoftImpute] Iter 95: observed BCE=0.210533 validation BCE=0.266450,rank=3\n",
      "[SoftImpute] Iter 96: observed BCE=0.210531 validation BCE=0.266449,rank=3\n",
      "[SoftImpute] Iter 97: observed BCE=0.210314 validation BCE=0.266246,rank=3\n",
      "[SoftImpute] Iter 98: observed BCE=0.210135 validation BCE=0.266430,rank=3\n",
      "[SoftImpute] Iter 99: observed BCE=0.210498 validation BCE=0.266292,rank=3\n",
      "[SoftImpute] Iter 100: observed BCE=0.210513 validation BCE=0.266620,rank=3\n",
      "[SoftImpute] Stopped after iteration 100 for lambda=0.075274\n",
      "final num of user group: 10\n",
      "final num of item group: 14\n",
      "change mode state : True\n",
      "time cost: 33.680237770080566\n",
      "After the matrix factor stage, training error is 0.21051, validation error is 0.26662\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gaminet.utils import local_visualize\n",
    "from gaminet.utils import global_visualize_density\n",
    "from gaminet.utils import feature_importance_visualize\n",
    "from gaminet.utils import plot_trajectory\n",
    "from gaminet.utils import plot_regularization\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from lvxnn.LVXNN import LV_XNN\n",
    "from lvxnn.DataReader import data_initialize\n",
    "\n",
    "data= pd.read_csv('data/train.csv')\n",
    "train , test = train_test_split(data,test_size=0.2)\n",
    "\n",
    "list1 = data.columns\n",
    "meta_info = OrderedDict()\n",
    "\n",
    "for i in list1:\n",
    "    meta_info[i]={'type': 'categorical','source':'user'}\n",
    "meta_info['income']={\"type\":\"continues\",'source':'user'}\n",
    "meta_info['cust_seniority']={\"type\":\"continues\",'source':'user'}\n",
    "meta_info['age'] = {\"type\":\"continues\",'source':'user'}\n",
    "meta_info['item'] = {'type': 'categorical','source':'item'}\n",
    "meta_info['user_id']={\"type\":\"id\",'source':'user'}\n",
    "meta_info['item_id']={\"type\":\"id\",'source':'item'}\n",
    "meta_info['target']={\"type\":\"target\",'source':''}\n",
    "\n",
    "\n",
    "tr_x, tr_Xi, tr_y , te_x , te_Xi, te_y, meta_info, model_info = data_initialize(train,test,meta_info,\"Classification\")\n",
    "\n",
    "def auto_test():\n",
    "    cold_auc = []\n",
    "    cold_logloss = []\n",
    "    warm_auc = []\n",
    "    warm_logloss = []\n",
    "    gami_auc = []\n",
    "    gami_logloss = []\n",
    "\n",
    "    for times in range(10):\n",
    "        \n",
    "        print(times)\n",
    "\n",
    "\n",
    "        model = LV_XNN(model_info=model_info, meta_info=meta_info, subnet_arch=[8, 16],interact_arch=[20, 10],activation_func=tf.tanh, batch_size=1000, lr_bp=0.01, auto_tune=True,\n",
    "               interaction_epochs=200,main_effect_epochs=300,tuning_epochs=50,loss_threshold_main=0.01,loss_threshold_inter=0.01,alpha=1,\n",
    "              verbose=True,val_ratio=0.125, early_stop_thres=100,interact_num=10,u_group_num=10,i_group_num=50,scale_ratio=0.7,n_power_iterations=5,n_oversamples=0,\n",
    "              mf_training_iters=1,mf_tuning_iters=100,change_mode=True,convergence_threshold=0.001,max_rank=3,shrinkage_value=20,random_state=times)\n",
    "    \n",
    "        st_time = time.time()\n",
    "        model.fit(tr_x,tr_Xi, tr_y)\n",
    "        ed_time = time.time()\n",
    "        \n",
    "        pred = model.predict(te_x, te_Xi)\n",
    "        \n",
    "        cold_y = te_y[(te_Xi[:,1] == 'cold') | (te_Xi[:,0] == 'cold')]\n",
    "        cold_pred = pred[(te_Xi[:,1] == 'cold') | (te_Xi[:,0] == 'cold')]\n",
    "        warm_y = te_y[(te_Xi[:,1] != 'cold') & (te_Xi[:,0] != 'cold')]\n",
    "        warm_pred = pred[(te_Xi[:,1] != 'cold') & (te_Xi[:,0] != 'cold')]\n",
    "    \n",
    "        cold_auc.append(roc_auc_score(cold_y,cold_pred))\n",
    "        cold_logloss.append(log_loss(cold_y,cold_pred))\n",
    "        warm_auc.append(roc_auc_score(warm_y,warm_pred))\n",
    "        warm_logloss.append(log_loss(warm_y,warm_pred))\n",
    "        gami_auc.append(roc_auc_score(te_y,model.final_gam_model.predict(te_x)))\n",
    "        gami_logloss.append(log_loss(te_y,model.final_gam_model.predict(te_x)))\n",
    "        \n",
    "    i_result = np.array([np.mean(cold_auc),np.mean(cold_logloss),np.mean(warm_auc),np.mean(warm_logloss)]).reshape(1,-1)\n",
    "    result = pd.DataFrame(i_result,columns=['cold_auc','cold_logloss','warm_auc','warm_logloss'])\n",
    "    \n",
    "    g_result = np.array([np.mean(gami_auc),np.mean(gami_logloss)]).reshape(1,-1)\n",
    "    g_result = pd.DataFrame(g_result,columns=['auc','logloss'])\n",
    "    \n",
    "    return result, g_result\n",
    "\n",
    "results, g_result = (auto_test())\n",
    "results.to_csv('result/Classification_LVXNN_result.csv',index=None)\n",
    "g_result.to_csv('result/Classification_gami_result.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "data= pd.read_csv('data/sim_binary_0.9.csv')\n",
    "train , test = train_test_split(data,test_size=0.2)\n",
    "data,val = train_test_split(train,test_size=0.125)\n",
    "\n",
    "x=data.iloc[:,:-1].values\n",
    "y=data.iloc[:,-1].values\n",
    "x_t = test.iloc[:,:-1].values\n",
    "y_t = test.iloc[:,-1].values\n",
    "\n",
    "enc = MinMaxScaler()\n",
    "x = enc.fit_transform(x)\n",
    "x_t = enc.fit_transform(x_t)\n",
    "\n",
    "def auto_test():\n",
    "    auc = []\n",
    "    logloss = []\n",
    "    for times in range(10):\n",
    "        xgb = XGBClassifier(n_jobs=-1)\n",
    "        xgb.fit(x,y)\n",
    "        pred = xgb.predict_proba(x_t)\n",
    "        \n",
    "        auc.append(roc_auc_score(y_t,pred[:,1]))\n",
    "        logloss.append(log_loss(y_t,pred[:,1]))\n",
    "\n",
    "    i_result = np.array([np.mean(auc),np.mean(logloss)]).reshape(1,-1)\n",
    "    result = pd.DataFrame(i_result,columns=['auc','logloss'])\n",
    "    \n",
    "    return result\n",
    "\n",
    "results = (auto_test())\n",
    "results.to_csv('result/Classification_xgboost_result.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "data= pd.read_csv('data/sim_binary_0.9.csv')\n",
    "train , test = train_test_split(data,test_size=0.2)\n",
    "data,val = train_test_split(train,test_size=0.125)\n",
    "\n",
    "Xi = data.iloc[:,-3:-1].values\n",
    "y = data.iloc[:,-1].values\n",
    "Xi_t = test.iloc[:,-3:-1].values\n",
    "y_t = test.iloc[:,-1].values\n",
    "\n",
    "tr_ratings_dict = {'itemID': Xi[:,1].tolist(),\n",
    "                'userID': Xi[:,0].tolist(),\n",
    "                'rating': y.tolist()}\n",
    "\n",
    "tr_df = pd.DataFrame(tr_ratings_dict)\n",
    "\n",
    "# A reader is still needed but only the rating_scale param is requiered.\n",
    "reader = Reader(rating_scale=(y.min(), y.max()))\n",
    "\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "tr_data = Dataset.load_from_df(tr_df[['userID', 'itemID', 'rating']], reader)\n",
    "\n",
    "tr_data = tr_data.build_full_trainset()\n",
    "\n",
    "def auto_test():\n",
    "    auc = []\n",
    "    logloss = []\n",
    "    for j in range(10):\n",
    "        model = SVD(n_factors=3)\n",
    "\n",
    "        model.fit(tr_data)\n",
    "\n",
    "        pred = []\n",
    "        \n",
    "        for i in range(Xi_t.shape[0]):\n",
    "            pred.append(model.predict(Xi_t[i,0],Xi_t[i,1],Xi_t[i,0]).est)\n",
    "    \n",
    "        pred2 = np.array(pred).ravel()\n",
    "\n",
    "        auc.append(roc_auc_score(y_t,pred2))\n",
    "        logloss.append(log_loss(y_t,pred2))\n",
    "    \n",
    "    i_result = np.array([np.mean(auc),np.mean(logloss)]).reshape(1,-1)\n",
    "    result = pd.DataFrame(i_result,columns=['auc','logloss'])\n",
    "    \n",
    "    return result\n",
    "\n",
    "results = (auto_test())\n",
    "results.to_csv('result/Classification_svd_result.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deepfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_size': 3, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002800723B6A8>, 'loss_type': 'logloss', 'epoch': 100, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function log_loss at 0x000002801B31B2F0>, 'random_seed': 2017, 'feature_size': 1110, 'field_size': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_size': 3, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002800723B6A8>, 'loss_type': 'logloss', 'epoch': 100, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function log_loss at 0x000002801B31B2F0>, 'random_seed': 2017, 'feature_size': 1110, 'field_size': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_size': 3, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002800723B6A8>, 'loss_type': 'logloss', 'epoch': 100, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function log_loss at 0x000002801B31B2F0>, 'random_seed': 2017, 'feature_size': 1110, 'field_size': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_size': 3, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002800723B6A8>, 'loss_type': 'logloss', 'epoch': 100, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function log_loss at 0x000002801B31B2F0>, 'random_seed': 2017, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 3, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002800723B6A8>, 'loss_type': 'logloss', 'epoch': 100, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function log_loss at 0x000002801B31B2F0>, 'random_seed': 2017, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 3, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002800723B6A8>, 'loss_type': 'logloss', 'epoch': 100, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function log_loss at 0x000002801B31B2F0>, 'random_seed': 2017, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 3, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002800723B6A8>, 'loss_type': 'logloss', 'epoch': 100, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function log_loss at 0x000002801B31B2F0>, 'random_seed': 2017, 'feature_size': 1110, 'field_size': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_size': 3, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002800723B6A8>, 'loss_type': 'logloss', 'epoch': 100, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function log_loss at 0x000002801B31B2F0>, 'random_seed': 2017, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 3, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002800723B6A8>, 'loss_type': 'logloss', 'epoch': 100, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function log_loss at 0x000002801B31B2F0>, 'random_seed': 2017, 'feature_size': 1110, 'field_size': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_size': 3, 'deep_layers': [32, 32], 'use_deep': True, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002800723B6A8>, 'loss_type': 'logloss', 'epoch': 100, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function log_loss at 0x000002801B31B2F0>, 'random_seed': 2017, 'feature_size': 1110, 'field_size': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_size': 3, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002800723B6A8>, 'loss_type': 'logloss', 'epoch': 100, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function log_loss at 0x000002801B31B2F0>, 'random_seed': 2017, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 3, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002800723B6A8>, 'loss_type': 'logloss', 'epoch': 100, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function log_loss at 0x000002801B31B2F0>, 'random_seed': 2017, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 3, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002800723B6A8>, 'loss_type': 'logloss', 'epoch': 100, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function log_loss at 0x000002801B31B2F0>, 'random_seed': 2017, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 3, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002800723B6A8>, 'loss_type': 'logloss', 'epoch': 100, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function log_loss at 0x000002801B31B2F0>, 'random_seed': 2017, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 3, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002800723B6A8>, 'loss_type': 'logloss', 'epoch': 100, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function log_loss at 0x000002801B31B2F0>, 'random_seed': 2017, 'feature_size': 1110, 'field_size': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_size': 3, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002800723B6A8>, 'loss_type': 'logloss', 'epoch': 100, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function log_loss at 0x000002801B31B2F0>, 'random_seed': 2017, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 3, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002800723B6A8>, 'loss_type': 'logloss', 'epoch': 100, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function log_loss at 0x000002801B31B2F0>, 'random_seed': 2017, 'feature_size': 1110, 'field_size': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_size': 3, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002800723B6A8>, 'loss_type': 'logloss', 'epoch': 100, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function log_loss at 0x000002801B31B2F0>, 'random_seed': 2017, 'feature_size': 1110, 'field_size': 12}\n",
      "{'embedding_size': 3, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002800723B6A8>, 'loss_type': 'logloss', 'epoch': 100, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function log_loss at 0x000002801B31B2F0>, 'random_seed': 2017, 'feature_size': 1110, 'field_size': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\64161\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_size': 3, 'deep_layers': [32, 32], 'use_deep': False, 'use_fm': True, 'deep_layers_activation': <function relu at 0x000002800723B6A8>, 'loss_type': 'logloss', 'epoch': 100, 'batch_size': 1024, 'learning_rate': 0.01, 'optimizer_type': 'adam', 'batch_norm': 0, 'batch_norm_decay': 0.995, 'l2_reg': 0.01, 'verbose': False, 'eval_metric': <function log_loss at 0x000002801B31B2F0>, 'random_seed': 2017, 'feature_size': 1110, 'field_size': 12}\n"
     ]
    }
   ],
   "source": [
    "class config():\n",
    "# set the path-to-files\n",
    "    \n",
    "    TRAIN_FILE = \"../simulation/data/sim_binary_0.9.csv\"\n",
    "    SUB_DIR = \"./result\"\n",
    "    NUM_SPLITS = 3\n",
    "    RANDOM_SEED = 2017\n",
    "\n",
    "# types of columns of the dataset dataframe\n",
    "    CATEGORICAL_COLS = []\n",
    "    NUMERIC_COLS = [\"uf_1\", \"uf_2\", \"uf_3\", \"uf_4\", \"uf_5\", \"if_1\", \"if_2\", \"if_3\", \"if_4\", \"if_5\"]\n",
    "    IGNORE_COLS = [\"target\"]\n",
    "    \n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "sys.path.append('../benchmark/deepfm/')\n",
    "from DataReader import FeatureDictionary, DataParser\n",
    "from DeepFM import DeepFM\n",
    "\n",
    "\n",
    "def _load_data():\n",
    "\n",
    "    dfTrain = pd.read_csv(config.TRAIN_FILE)\n",
    "    dfTrain , dfTest = train_test_split(dfTrain,test_size=0.2)\n",
    "\n",
    "    def preprocess(df):        \n",
    "        cols = [c for c in df.columns if c not in [\"target\"]]\n",
    "        return df\n",
    "\n",
    "    dfTrain = preprocess(dfTrain)\n",
    "    dfTest = preprocess(dfTest)\n",
    "    cols = [c for c in dfTrain.columns if c not in [\"target\"]]\n",
    "\n",
    "\n",
    "    X_train = dfTrain[cols].values\n",
    "    y_train = dfTrain[\"target\"].values\n",
    "    X_test = dfTest[cols].values\n",
    "    \n",
    "    ids_test = dfTest[\"user_id\"].values\n",
    "    idv_test = dfTest[\"item_id\"].values\n",
    "    y_test = dfTest['target'].values\n",
    "        \n",
    "    return dfTrain, dfTest, X_train, y_train, X_test, ids_test,idv_test, y_test\n",
    "\n",
    "\n",
    "def _run_base_model_dfm(dfTrain, dfTest, folds, dfm_params):\n",
    "    fd = FeatureDictionary(dfTrain=dfTrain, dfTest=dfTest,\n",
    "                           numeric_cols=config.NUMERIC_COLS,\n",
    "                           ignore_cols=config.IGNORE_COLS)\n",
    "    data_parser = DataParser(feat_dict=fd)\n",
    "    Xi_train, Xv_train, y_train = data_parser.parse(df=dfTrain, has_label=True)\n",
    "    Xi_test, Xv_test, ids_test,idv_test = data_parser.parse(df=dfTest)\n",
    "    \n",
    "    dfm_params[\"feature_size\"] = fd.feat_dim\n",
    "    dfm_params[\"field_size\"] = len(Xi_train[0])\n",
    "    print(dfm_params)\n",
    "\n",
    "    y_train_meta = np.zeros((dfTrain.shape[0], 1), dtype=float)\n",
    "    y_test_meta = np.zeros((dfTest.shape[0], 1), dtype=float)\n",
    "    _get = lambda x, l: [x[i] for i in l]\n",
    "    gini_results_cv = np.zeros(len(folds), dtype=float)\n",
    "    gini_results_epoch_train = np.zeros((len(folds), dfm_params[\"epoch\"]), dtype=float)\n",
    "    gini_results_epoch_valid = np.zeros((len(folds), dfm_params[\"epoch\"]), dtype=float)\n",
    "    for i, (train_idx, valid_idx) in enumerate(folds):\n",
    "        Xi_train_, Xv_train_, y_train_ = _get(Xi_train, train_idx), _get(Xv_train, train_idx), _get(y_train, train_idx)\n",
    "        Xi_valid_, Xv_valid_, y_valid_ = _get(Xi_train, valid_idx), _get(Xv_train, valid_idx), _get(y_train, valid_idx)\n",
    "\n",
    "        dfm = DeepFM(**dfm_params)\n",
    "        dfm.fit(Xi_train_, Xv_train_, y_train_, Xi_valid_, Xv_valid_, y_valid_)\n",
    "\n",
    "        y_train_meta[valid_idx,0] = dfm.predict(Xi_valid_, Xv_valid_)\n",
    "        y_test_meta[:,0] += dfm.predict(Xi_test, Xv_test)\n",
    "        \n",
    "        gini_results_cv[i] = mean_absolute_error(y_valid_, y_train_meta[valid_idx])\n",
    "        gini_results_epoch_train[i] = dfm.train_result\n",
    "        gini_results_epoch_valid[i] = dfm.valid_result\n",
    "\n",
    "    y_test_meta /= float(len(folds))\n",
    "\n",
    "    # save result\n",
    "    return y_train_meta, y_test_meta\n",
    "\n",
    "# load data\n",
    "dfTrain, dfTest, X_train, y_train, X_test, ids_test ,idv_test, y_test= _load_data()\n",
    "\n",
    "# folds\n",
    "folds = list(KFold(n_splits=config.NUM_SPLITS, shuffle=True,\n",
    "                             random_state=config.RANDOM_SEED).split(X_train, y_train))\n",
    "\n",
    "\n",
    "\n",
    "# ------------------ DeepFM Model ------------------\n",
    "# params\n",
    "dfm_params = {\n",
    "    \"embedding_size\": 3,\n",
    "    \"deep_layers\": [32, 32],\n",
    "    \"use_deep\" : True ,\n",
    "    \"use_fm\" : True , \n",
    "    \"deep_layers_activation\": tf.nn.relu,\n",
    "    \"loss_type\" : \"logloss\",\n",
    "    \"epoch\": 100 ,\n",
    "    \"batch_size\": 1024,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"optimizer_type\": \"adam\",\n",
    "    \"batch_norm\": 0,\n",
    "    \"batch_norm_decay\": 0.995,\n",
    "    \"l2_reg\": 0.01,\n",
    "    \"verbose\": False,\n",
    "    \"eval_metric\": log_loss,\n",
    "    \"random_seed\": config.RANDOM_SEED\n",
    "}\n",
    "\n",
    "def auto_test(deep):\n",
    "    auc = []\n",
    "    logloss = []\n",
    "    dfm_params['use_deep']=deep\n",
    "    \n",
    "    for i in range(10):\n",
    "        y_train_dfm, y_test_dfm = _run_base_model_dfm(dfTrain, dfTest, folds, dfm_params)\n",
    "        auc.append(roc_auc_score(y_test,y_test_dfm))\n",
    "        logloss.append(log_loss(y_test,y_test_dfm))\n",
    "    \n",
    "    i_result = np.array([np.mean(auc),np.mean(logloss)]).reshape(1,-1)\n",
    "    results = pd.DataFrame(i_result,columns=['auc','logloss'])\n",
    "    \n",
    "    return results\n",
    "result_1 = (auto_test(True))\n",
    "result_2 = (auto_test(False))\n",
    "result_1.to_csv('result/Classification_deepfm_result.csv',index=None)\n",
    "result_2.to_csv('result/Classification_fm_result.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cold test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.86 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../..\\lvxnn\\DataReader.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 0.26 MB\n",
      "Decreased by 69.6%\n",
      "Memory usage of dataframe is 0.21 MB\n",
      "Memory usage after optimization is: 0.07 MB\n",
      "Decreased by 69.6%\n",
      "cold start user: 40\n",
      "cold start item: 329\n",
      "0\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.00643, val loss: 3.98251\n",
      "Main effects training epoch: 2, train loss: 3.84931, val loss: 3.81429\n",
      "Main effects training epoch: 3, train loss: 3.64703, val loss: 3.57569\n",
      "Main effects training epoch: 4, train loss: 3.47427, val loss: 3.36377\n",
      "Main effects training epoch: 5, train loss: 3.40606, val loss: 3.29867\n",
      "Main effects training epoch: 6, train loss: 3.36459, val loss: 3.27615\n",
      "Main effects training epoch: 7, train loss: 3.30277, val loss: 3.21485\n",
      "Main effects training epoch: 8, train loss: 3.26879, val loss: 3.18237\n",
      "Main effects training epoch: 9, train loss: 3.21660, val loss: 3.12521\n",
      "Main effects training epoch: 10, train loss: 3.16889, val loss: 3.08685\n",
      "Main effects training epoch: 11, train loss: 3.09692, val loss: 3.01219\n",
      "Main effects training epoch: 12, train loss: 3.05865, val loss: 2.98764\n",
      "Main effects training epoch: 13, train loss: 2.97058, val loss: 2.90726\n",
      "Main effects training epoch: 14, train loss: 2.91190, val loss: 2.85502\n",
      "Main effects training epoch: 15, train loss: 2.81319, val loss: 2.78115\n",
      "Main effects training epoch: 16, train loss: 2.79172, val loss: 2.75715\n",
      "Main effects training epoch: 17, train loss: 2.75773, val loss: 2.73333\n",
      "Main effects training epoch: 18, train loss: 2.68145, val loss: 2.66461\n",
      "Main effects training epoch: 19, train loss: 2.66279, val loss: 2.64589\n",
      "Main effects training epoch: 20, train loss: 2.61127, val loss: 2.59115\n",
      "Main effects training epoch: 21, train loss: 2.60101, val loss: 2.58541\n",
      "Main effects training epoch: 22, train loss: 2.49819, val loss: 2.48185\n",
      "Main effects training epoch: 23, train loss: 2.51852, val loss: 2.49378\n",
      "Main effects training epoch: 24, train loss: 2.46837, val loss: 2.45362\n",
      "Main effects training epoch: 25, train loss: 2.39601, val loss: 2.38562\n",
      "Main effects training epoch: 26, train loss: 2.39512, val loss: 2.36913\n",
      "Main effects training epoch: 27, train loss: 2.35612, val loss: 2.34559\n",
      "Main effects training epoch: 28, train loss: 2.31841, val loss: 2.30668\n",
      "Main effects training epoch: 29, train loss: 2.28650, val loss: 2.26723\n",
      "Main effects training epoch: 30, train loss: 2.32109, val loss: 2.30463\n",
      "Main effects training epoch: 31, train loss: 2.23803, val loss: 2.22584\n",
      "Main effects training epoch: 32, train loss: 2.23900, val loss: 2.22693\n",
      "Main effects training epoch: 33, train loss: 2.17624, val loss: 2.16796\n",
      "Main effects training epoch: 34, train loss: 2.17633, val loss: 2.16663\n",
      "Main effects training epoch: 35, train loss: 2.16082, val loss: 2.15250\n",
      "Main effects training epoch: 36, train loss: 2.10551, val loss: 2.10430\n",
      "Main effects training epoch: 37, train loss: 2.10034, val loss: 2.10174\n",
      "Main effects training epoch: 38, train loss: 2.08214, val loss: 2.08983\n",
      "Main effects training epoch: 39, train loss: 2.05306, val loss: 2.05888\n",
      "Main effects training epoch: 40, train loss: 2.04229, val loss: 2.05377\n",
      "Main effects training epoch: 41, train loss: 2.03840, val loss: 2.04887\n",
      "Main effects training epoch: 42, train loss: 1.99140, val loss: 2.00439\n",
      "Main effects training epoch: 43, train loss: 2.01161, val loss: 2.02922\n",
      "Main effects training epoch: 44, train loss: 1.97985, val loss: 2.00012\n",
      "Main effects training epoch: 45, train loss: 1.94908, val loss: 1.97227\n",
      "Main effects training epoch: 46, train loss: 1.95038, val loss: 1.97129\n",
      "Main effects training epoch: 47, train loss: 1.93308, val loss: 1.95951\n",
      "Main effects training epoch: 48, train loss: 1.90864, val loss: 1.94218\n",
      "Main effects training epoch: 49, train loss: 1.92556, val loss: 1.95580\n",
      "Main effects training epoch: 50, train loss: 1.89287, val loss: 1.92894\n",
      "Main effects training epoch: 51, train loss: 1.87731, val loss: 1.91674\n",
      "Main effects training epoch: 52, train loss: 1.88528, val loss: 1.92228\n",
      "Main effects training epoch: 53, train loss: 1.86219, val loss: 1.90679\n",
      "Main effects training epoch: 54, train loss: 1.86467, val loss: 1.90638\n",
      "Main effects training epoch: 55, train loss: 1.84051, val loss: 1.89157\n",
      "Main effects training epoch: 56, train loss: 1.85655, val loss: 1.89953\n",
      "Main effects training epoch: 57, train loss: 1.82744, val loss: 1.88236\n",
      "Main effects training epoch: 58, train loss: 1.81837, val loss: 1.87220\n",
      "Main effects training epoch: 59, train loss: 1.81822, val loss: 1.87621\n",
      "Main effects training epoch: 60, train loss: 1.79271, val loss: 1.85578\n",
      "Main effects training epoch: 61, train loss: 1.77921, val loss: 1.84754\n",
      "Main effects training epoch: 62, train loss: 1.76436, val loss: 1.82837\n",
      "Main effects training epoch: 63, train loss: 1.76064, val loss: 1.83407\n",
      "Main effects training epoch: 64, train loss: 1.74607, val loss: 1.81318\n",
      "Main effects training epoch: 65, train loss: 1.73763, val loss: 1.79964\n",
      "Main effects training epoch: 66, train loss: 1.74434, val loss: 1.80926\n",
      "Main effects training epoch: 67, train loss: 1.73981, val loss: 1.80728\n",
      "Main effects training epoch: 68, train loss: 1.72813, val loss: 1.79662\n",
      "Main effects training epoch: 69, train loss: 1.72177, val loss: 1.78615\n",
      "Main effects training epoch: 70, train loss: 1.72873, val loss: 1.79227\n",
      "Main effects training epoch: 71, train loss: 1.72582, val loss: 1.78371\n",
      "Main effects training epoch: 72, train loss: 1.72237, val loss: 1.79136\n",
      "Main effects training epoch: 73, train loss: 1.71512, val loss: 1.78029\n",
      "Main effects training epoch: 74, train loss: 1.71550, val loss: 1.78006\n",
      "Main effects training epoch: 75, train loss: 1.70998, val loss: 1.77954\n",
      "Main effects training epoch: 76, train loss: 1.71051, val loss: 1.77184\n",
      "Main effects training epoch: 77, train loss: 1.70748, val loss: 1.78053\n",
      "Main effects training epoch: 78, train loss: 1.71025, val loss: 1.77345\n",
      "Main effects training epoch: 79, train loss: 1.70345, val loss: 1.77462\n",
      "Main effects training epoch: 80, train loss: 1.70256, val loss: 1.77286\n",
      "Main effects training epoch: 81, train loss: 1.70075, val loss: 1.76551\n",
      "Main effects training epoch: 82, train loss: 1.69406, val loss: 1.76857\n",
      "Main effects training epoch: 83, train loss: 1.69159, val loss: 1.76208\n",
      "Main effects training epoch: 84, train loss: 1.68994, val loss: 1.76814\n",
      "Main effects training epoch: 85, train loss: 1.68579, val loss: 1.75771\n",
      "Main effects training epoch: 86, train loss: 1.68922, val loss: 1.76753\n",
      "Main effects training epoch: 87, train loss: 1.68116, val loss: 1.75778\n",
      "Main effects training epoch: 88, train loss: 1.67608, val loss: 1.76350\n",
      "Main effects training epoch: 89, train loss: 1.66433, val loss: 1.74793\n",
      "Main effects training epoch: 90, train loss: 1.66749, val loss: 1.74786\n",
      "Main effects training epoch: 91, train loss: 1.65849, val loss: 1.74901\n",
      "Main effects training epoch: 92, train loss: 1.65853, val loss: 1.74745\n",
      "Main effects training epoch: 93, train loss: 1.66611, val loss: 1.75572\n",
      "Main effects training epoch: 94, train loss: 1.65417, val loss: 1.73682\n",
      "Main effects training epoch: 95, train loss: 1.64454, val loss: 1.74203\n",
      "Main effects training epoch: 96, train loss: 1.64774, val loss: 1.74071\n",
      "Main effects training epoch: 97, train loss: 1.63520, val loss: 1.72539\n",
      "Main effects training epoch: 98, train loss: 1.63695, val loss: 1.72891\n",
      "Main effects training epoch: 99, train loss: 1.63165, val loss: 1.71515\n",
      "Main effects training epoch: 100, train loss: 1.62081, val loss: 1.71720\n",
      "Main effects training epoch: 101, train loss: 1.62360, val loss: 1.71047\n",
      "Main effects training epoch: 102, train loss: 1.61212, val loss: 1.70434\n",
      "Main effects training epoch: 103, train loss: 1.60961, val loss: 1.70613\n",
      "Main effects training epoch: 104, train loss: 1.61114, val loss: 1.71211\n",
      "Main effects training epoch: 105, train loss: 1.60380, val loss: 1.69876\n",
      "Main effects training epoch: 106, train loss: 1.60248, val loss: 1.69992\n",
      "Main effects training epoch: 107, train loss: 1.60465, val loss: 1.70255\n",
      "Main effects training epoch: 108, train loss: 1.59699, val loss: 1.69332\n",
      "Main effects training epoch: 109, train loss: 1.59405, val loss: 1.69949\n",
      "Main effects training epoch: 110, train loss: 1.58877, val loss: 1.69378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 111, train loss: 1.59706, val loss: 1.69846\n",
      "Main effects training epoch: 112, train loss: 1.58556, val loss: 1.68967\n",
      "Main effects training epoch: 113, train loss: 1.57964, val loss: 1.68878\n",
      "Main effects training epoch: 114, train loss: 1.58525, val loss: 1.69017\n",
      "Main effects training epoch: 115, train loss: 1.58037, val loss: 1.69121\n",
      "Main effects training epoch: 116, train loss: 1.58022, val loss: 1.68878\n",
      "Main effects training epoch: 117, train loss: 1.57283, val loss: 1.67991\n",
      "Main effects training epoch: 118, train loss: 1.57187, val loss: 1.68510\n",
      "Main effects training epoch: 119, train loss: 1.57044, val loss: 1.68455\n",
      "Main effects training epoch: 120, train loss: 1.56814, val loss: 1.68247\n",
      "Main effects training epoch: 121, train loss: 1.56821, val loss: 1.68650\n",
      "Main effects training epoch: 122, train loss: 1.56492, val loss: 1.67891\n",
      "Main effects training epoch: 123, train loss: 1.56535, val loss: 1.68061\n",
      "Main effects training epoch: 124, train loss: 1.56491, val loss: 1.68346\n",
      "Main effects training epoch: 125, train loss: 1.56546, val loss: 1.67503\n",
      "Main effects training epoch: 126, train loss: 1.56744, val loss: 1.68024\n",
      "Main effects training epoch: 127, train loss: 1.56484, val loss: 1.68717\n",
      "Main effects training epoch: 128, train loss: 1.55886, val loss: 1.67584\n",
      "Main effects training epoch: 129, train loss: 1.56285, val loss: 1.67631\n",
      "Main effects training epoch: 130, train loss: 1.56343, val loss: 1.68097\n",
      "Main effects training epoch: 131, train loss: 1.55935, val loss: 1.67534\n",
      "Main effects training epoch: 132, train loss: 1.56781, val loss: 1.69232\n",
      "Main effects training epoch: 133, train loss: 1.56955, val loss: 1.67525\n",
      "Main effects training epoch: 134, train loss: 1.56575, val loss: 1.68183\n",
      "Main effects training epoch: 135, train loss: 1.56405, val loss: 1.67581\n",
      "Main effects training epoch: 136, train loss: 1.55643, val loss: 1.67419\n",
      "Main effects training epoch: 137, train loss: 1.56595, val loss: 1.67814\n",
      "Main effects training epoch: 138, train loss: 1.56271, val loss: 1.67337\n",
      "Main effects training epoch: 139, train loss: 1.55841, val loss: 1.67478\n",
      "Main effects training epoch: 140, train loss: 1.55857, val loss: 1.67651\n",
      "Main effects training epoch: 141, train loss: 1.56243, val loss: 1.66856\n",
      "Main effects training epoch: 142, train loss: 1.55646, val loss: 1.66885\n",
      "Main effects training epoch: 143, train loss: 1.55971, val loss: 1.66922\n",
      "Main effects training epoch: 144, train loss: 1.55918, val loss: 1.66627\n",
      "Main effects training epoch: 145, train loss: 1.55159, val loss: 1.65683\n",
      "Main effects training epoch: 146, train loss: 1.55249, val loss: 1.66651\n",
      "Main effects training epoch: 147, train loss: 1.55937, val loss: 1.67155\n",
      "Main effects training epoch: 148, train loss: 1.55151, val loss: 1.66280\n",
      "Main effects training epoch: 149, train loss: 1.54869, val loss: 1.66782\n",
      "Main effects training epoch: 150, train loss: 1.55291, val loss: 1.66458\n",
      "Main effects training epoch: 151, train loss: 1.54704, val loss: 1.65743\n",
      "Main effects training epoch: 152, train loss: 1.54751, val loss: 1.66203\n",
      "Main effects training epoch: 153, train loss: 1.54813, val loss: 1.66172\n",
      "Main effects training epoch: 154, train loss: 1.54737, val loss: 1.65693\n",
      "Main effects training epoch: 155, train loss: 1.54849, val loss: 1.66265\n",
      "Main effects training epoch: 156, train loss: 1.54690, val loss: 1.65190\n",
      "Main effects training epoch: 157, train loss: 1.54625, val loss: 1.66113\n",
      "Main effects training epoch: 158, train loss: 1.54469, val loss: 1.65572\n",
      "Main effects training epoch: 159, train loss: 1.54321, val loss: 1.65746\n",
      "Main effects training epoch: 160, train loss: 1.54781, val loss: 1.65229\n",
      "Main effects training epoch: 161, train loss: 1.54377, val loss: 1.65460\n",
      "Main effects training epoch: 162, train loss: 1.54619, val loss: 1.65544\n",
      "Main effects training epoch: 163, train loss: 1.54365, val loss: 1.66312\n",
      "Main effects training epoch: 164, train loss: 1.54256, val loss: 1.64949\n",
      "Main effects training epoch: 165, train loss: 1.54559, val loss: 1.65221\n",
      "Main effects training epoch: 166, train loss: 1.54191, val loss: 1.65643\n",
      "Main effects training epoch: 167, train loss: 1.54032, val loss: 1.64529\n",
      "Main effects training epoch: 168, train loss: 1.54142, val loss: 1.65299\n",
      "Main effects training epoch: 169, train loss: 1.54160, val loss: 1.65316\n",
      "Main effects training epoch: 170, train loss: 1.54066, val loss: 1.64833\n",
      "Main effects training epoch: 171, train loss: 1.53789, val loss: 1.65051\n",
      "Main effects training epoch: 172, train loss: 1.53884, val loss: 1.64553\n",
      "Main effects training epoch: 173, train loss: 1.53723, val loss: 1.64662\n",
      "Main effects training epoch: 174, train loss: 1.53996, val loss: 1.64820\n",
      "Main effects training epoch: 175, train loss: 1.54526, val loss: 1.65707\n",
      "Main effects training epoch: 176, train loss: 1.54449, val loss: 1.65536\n",
      "Main effects training epoch: 177, train loss: 1.54041, val loss: 1.64837\n",
      "Main effects training epoch: 178, train loss: 1.53651, val loss: 1.64363\n",
      "Main effects training epoch: 179, train loss: 1.53681, val loss: 1.64604\n",
      "Main effects training epoch: 180, train loss: 1.54036, val loss: 1.65311\n",
      "Main effects training epoch: 181, train loss: 1.53406, val loss: 1.64153\n",
      "Main effects training epoch: 182, train loss: 1.53427, val loss: 1.65019\n",
      "Main effects training epoch: 183, train loss: 1.53502, val loss: 1.64340\n",
      "Main effects training epoch: 184, train loss: 1.53739, val loss: 1.64623\n",
      "Main effects training epoch: 185, train loss: 1.53817, val loss: 1.63870\n",
      "Main effects training epoch: 186, train loss: 1.54080, val loss: 1.64459\n",
      "Main effects training epoch: 187, train loss: 1.53591, val loss: 1.64835\n",
      "Main effects training epoch: 188, train loss: 1.53250, val loss: 1.64064\n",
      "Main effects training epoch: 189, train loss: 1.52955, val loss: 1.63301\n",
      "Main effects training epoch: 190, train loss: 1.52867, val loss: 1.63700\n",
      "Main effects training epoch: 191, train loss: 1.53135, val loss: 1.63785\n",
      "Main effects training epoch: 192, train loss: 1.53315, val loss: 1.64032\n",
      "Main effects training epoch: 193, train loss: 1.53184, val loss: 1.63289\n",
      "Main effects training epoch: 194, train loss: 1.52852, val loss: 1.64000\n",
      "Main effects training epoch: 195, train loss: 1.52960, val loss: 1.63495\n",
      "Main effects training epoch: 196, train loss: 1.52433, val loss: 1.62673\n",
      "Main effects training epoch: 197, train loss: 1.52591, val loss: 1.63439\n",
      "Main effects training epoch: 198, train loss: 1.52379, val loss: 1.63141\n",
      "Main effects training epoch: 199, train loss: 1.52837, val loss: 1.63937\n",
      "Main effects training epoch: 200, train loss: 1.52924, val loss: 1.62973\n",
      "Main effects training epoch: 201, train loss: 1.52598, val loss: 1.62981\n",
      "Main effects training epoch: 202, train loss: 1.52512, val loss: 1.62910\n",
      "Main effects training epoch: 203, train loss: 1.52185, val loss: 1.62858\n",
      "Main effects training epoch: 204, train loss: 1.51997, val loss: 1.62791\n",
      "Main effects training epoch: 205, train loss: 1.52261, val loss: 1.62665\n",
      "Main effects training epoch: 206, train loss: 1.52033, val loss: 1.62931\n",
      "Main effects training epoch: 207, train loss: 1.52095, val loss: 1.62353\n",
      "Main effects training epoch: 208, train loss: 1.52158, val loss: 1.63390\n",
      "Main effects training epoch: 209, train loss: 1.52210, val loss: 1.62345\n",
      "Main effects training epoch: 210, train loss: 1.51886, val loss: 1.63025\n",
      "Main effects training epoch: 211, train loss: 1.51648, val loss: 1.62113\n",
      "Main effects training epoch: 212, train loss: 1.52155, val loss: 1.62694\n",
      "Main effects training epoch: 213, train loss: 1.52304, val loss: 1.62538\n",
      "Main effects training epoch: 214, train loss: 1.52230, val loss: 1.62429\n",
      "Main effects training epoch: 215, train loss: 1.51855, val loss: 1.62346\n",
      "Main effects training epoch: 216, train loss: 1.52366, val loss: 1.62821\n",
      "Main effects training epoch: 217, train loss: 1.52225, val loss: 1.63128\n",
      "Main effects training epoch: 218, train loss: 1.52198, val loss: 1.62928\n",
      "Main effects training epoch: 219, train loss: 1.52474, val loss: 1.62576\n",
      "Main effects training epoch: 220, train loss: 1.51881, val loss: 1.62930\n",
      "Main effects training epoch: 221, train loss: 1.51838, val loss: 1.62021\n",
      "Main effects training epoch: 222, train loss: 1.52382, val loss: 1.63345\n",
      "Main effects training epoch: 223, train loss: 1.51392, val loss: 1.61423\n",
      "Main effects training epoch: 224, train loss: 1.51383, val loss: 1.62313\n",
      "Main effects training epoch: 225, train loss: 1.52136, val loss: 1.62899\n",
      "Main effects training epoch: 226, train loss: 1.51554, val loss: 1.62029\n",
      "Main effects training epoch: 227, train loss: 1.52065, val loss: 1.62644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 228, train loss: 1.51788, val loss: 1.61524\n",
      "Main effects training epoch: 229, train loss: 1.51574, val loss: 1.62373\n",
      "Main effects training epoch: 230, train loss: 1.51377, val loss: 1.61144\n",
      "Main effects training epoch: 231, train loss: 1.51573, val loss: 1.61865\n",
      "Main effects training epoch: 232, train loss: 1.51601, val loss: 1.61879\n",
      "Main effects training epoch: 233, train loss: 1.50940, val loss: 1.61326\n",
      "Main effects training epoch: 234, train loss: 1.51159, val loss: 1.61631\n",
      "Main effects training epoch: 235, train loss: 1.51191, val loss: 1.61541\n",
      "Main effects training epoch: 236, train loss: 1.51836, val loss: 1.61806\n",
      "Main effects training epoch: 237, train loss: 1.51139, val loss: 1.60918\n",
      "Main effects training epoch: 238, train loss: 1.51257, val loss: 1.62264\n",
      "Main effects training epoch: 239, train loss: 1.51528, val loss: 1.61095\n",
      "Main effects training epoch: 240, train loss: 1.51112, val loss: 1.61551\n",
      "Main effects training epoch: 241, train loss: 1.50951, val loss: 1.61149\n",
      "Main effects training epoch: 242, train loss: 1.50770, val loss: 1.61187\n",
      "Main effects training epoch: 243, train loss: 1.51485, val loss: 1.61940\n",
      "Main effects training epoch: 244, train loss: 1.50815, val loss: 1.61560\n",
      "Main effects training epoch: 245, train loss: 1.50986, val loss: 1.60560\n",
      "Main effects training epoch: 246, train loss: 1.50879, val loss: 1.60947\n",
      "Main effects training epoch: 247, train loss: 1.50694, val loss: 1.61289\n",
      "Main effects training epoch: 248, train loss: 1.50864, val loss: 1.60817\n",
      "Main effects training epoch: 249, train loss: 1.50917, val loss: 1.61357\n",
      "Main effects training epoch: 250, train loss: 1.50591, val loss: 1.60995\n",
      "Main effects training epoch: 251, train loss: 1.50412, val loss: 1.60871\n",
      "Main effects training epoch: 252, train loss: 1.50287, val loss: 1.60194\n",
      "Main effects training epoch: 253, train loss: 1.51356, val loss: 1.61103\n",
      "Main effects training epoch: 254, train loss: 1.51163, val loss: 1.61586\n",
      "Main effects training epoch: 255, train loss: 1.50616, val loss: 1.60395\n",
      "Main effects training epoch: 256, train loss: 1.50503, val loss: 1.60757\n",
      "Main effects training epoch: 257, train loss: 1.50702, val loss: 1.60487\n",
      "Main effects training epoch: 258, train loss: 1.50493, val loss: 1.60732\n",
      "Main effects training epoch: 259, train loss: 1.50046, val loss: 1.60058\n",
      "Main effects training epoch: 260, train loss: 1.50326, val loss: 1.59778\n",
      "Main effects training epoch: 261, train loss: 1.50672, val loss: 1.61391\n",
      "Main effects training epoch: 262, train loss: 1.50842, val loss: 1.60276\n",
      "Main effects training epoch: 263, train loss: 1.50260, val loss: 1.60344\n",
      "Main effects training epoch: 264, train loss: 1.50780, val loss: 1.61144\n",
      "Main effects training epoch: 265, train loss: 1.50282, val loss: 1.60565\n",
      "Main effects training epoch: 266, train loss: 1.50305, val loss: 1.59530\n",
      "Main effects training epoch: 267, train loss: 1.50397, val loss: 1.60911\n",
      "Main effects training epoch: 268, train loss: 1.50360, val loss: 1.59716\n",
      "Main effects training epoch: 269, train loss: 1.51023, val loss: 1.60571\n",
      "Main effects training epoch: 270, train loss: 1.49962, val loss: 1.60608\n",
      "Main effects training epoch: 271, train loss: 1.50105, val loss: 1.59199\n",
      "Main effects training epoch: 272, train loss: 1.49943, val loss: 1.59862\n",
      "Main effects training epoch: 273, train loss: 1.49522, val loss: 1.59998\n",
      "Main effects training epoch: 274, train loss: 1.49614, val loss: 1.59119\n",
      "Main effects training epoch: 275, train loss: 1.50188, val loss: 1.60908\n",
      "Main effects training epoch: 276, train loss: 1.49932, val loss: 1.58981\n",
      "Main effects training epoch: 277, train loss: 1.50055, val loss: 1.59749\n",
      "Main effects training epoch: 278, train loss: 1.50224, val loss: 1.60649\n",
      "Main effects training epoch: 279, train loss: 1.50703, val loss: 1.61344\n",
      "Main effects training epoch: 280, train loss: 1.51164, val loss: 1.60882\n",
      "Main effects training epoch: 281, train loss: 1.49570, val loss: 1.59554\n",
      "Main effects training epoch: 282, train loss: 1.49475, val loss: 1.59811\n",
      "Main effects training epoch: 283, train loss: 1.49356, val loss: 1.58924\n",
      "Main effects training epoch: 284, train loss: 1.48788, val loss: 1.58816\n",
      "Main effects training epoch: 285, train loss: 1.49762, val loss: 1.59805\n",
      "Main effects training epoch: 286, train loss: 1.49362, val loss: 1.59454\n",
      "Main effects training epoch: 287, train loss: 1.48859, val loss: 1.58927\n",
      "Main effects training epoch: 288, train loss: 1.49397, val loss: 1.59298\n",
      "Main effects training epoch: 289, train loss: 1.48901, val loss: 1.59048\n",
      "Main effects training epoch: 290, train loss: 1.49709, val loss: 1.59320\n",
      "Main effects training epoch: 291, train loss: 1.48725, val loss: 1.58304\n",
      "Main effects training epoch: 292, train loss: 1.49267, val loss: 1.59096\n",
      "Main effects training epoch: 293, train loss: 1.48791, val loss: 1.59172\n",
      "Main effects training epoch: 294, train loss: 1.48740, val loss: 1.58388\n",
      "Main effects training epoch: 295, train loss: 1.48791, val loss: 1.58763\n",
      "Main effects training epoch: 296, train loss: 1.48164, val loss: 1.57752\n",
      "Main effects training epoch: 297, train loss: 1.48273, val loss: 1.58789\n",
      "Main effects training epoch: 298, train loss: 1.49686, val loss: 1.59149\n",
      "Main effects training epoch: 299, train loss: 1.49085, val loss: 1.59326\n",
      "Main effects training epoch: 300, train loss: 1.48143, val loss: 1.57795\n",
      "##########Stage 1: main effect training stop.##########\n",
      "3 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.50652, val loss: 1.59549\n",
      "Main effects tuning epoch: 2, train loss: 1.50834, val loss: 1.59348\n",
      "Main effects tuning epoch: 3, train loss: 1.50562, val loss: 1.58377\n",
      "Main effects tuning epoch: 4, train loss: 1.50620, val loss: 1.58972\n",
      "Main effects tuning epoch: 5, train loss: 1.50602, val loss: 1.59017\n",
      "Main effects tuning epoch: 6, train loss: 1.50634, val loss: 1.58906\n",
      "Main effects tuning epoch: 7, train loss: 1.50491, val loss: 1.58666\n",
      "Main effects tuning epoch: 8, train loss: 1.51424, val loss: 1.59725\n",
      "Main effects tuning epoch: 9, train loss: 1.51776, val loss: 1.59724\n",
      "Main effects tuning epoch: 10, train loss: 1.51541, val loss: 1.59733\n",
      "Main effects tuning epoch: 11, train loss: 1.50006, val loss: 1.57479\n",
      "Main effects tuning epoch: 12, train loss: 1.50037, val loss: 1.57859\n",
      "Main effects tuning epoch: 13, train loss: 1.49950, val loss: 1.58009\n",
      "Main effects tuning epoch: 14, train loss: 1.50045, val loss: 1.57891\n",
      "Main effects tuning epoch: 15, train loss: 1.50213, val loss: 1.58386\n",
      "Main effects tuning epoch: 16, train loss: 1.51392, val loss: 1.59837\n",
      "Main effects tuning epoch: 17, train loss: 1.50131, val loss: 1.58494\n",
      "Main effects tuning epoch: 18, train loss: 1.49776, val loss: 1.57601\n",
      "Main effects tuning epoch: 19, train loss: 1.51233, val loss: 1.60198\n",
      "Main effects tuning epoch: 20, train loss: 1.50198, val loss: 1.58804\n",
      "Main effects tuning epoch: 21, train loss: 1.50396, val loss: 1.57574\n",
      "Main effects tuning epoch: 22, train loss: 1.49631, val loss: 1.58499\n",
      "Main effects tuning epoch: 23, train loss: 1.49806, val loss: 1.57824\n",
      "Main effects tuning epoch: 24, train loss: 1.50047, val loss: 1.58988\n",
      "Main effects tuning epoch: 25, train loss: 1.50048, val loss: 1.57330\n",
      "Main effects tuning epoch: 26, train loss: 1.49641, val loss: 1.57998\n",
      "Main effects tuning epoch: 27, train loss: 1.49913, val loss: 1.59107\n",
      "Main effects tuning epoch: 28, train loss: 1.49872, val loss: 1.57721\n",
      "Main effects tuning epoch: 29, train loss: 1.49268, val loss: 1.57902\n",
      "Main effects tuning epoch: 30, train loss: 1.49729, val loss: 1.57318\n",
      "Main effects tuning epoch: 31, train loss: 1.49374, val loss: 1.58750\n",
      "Main effects tuning epoch: 32, train loss: 1.50145, val loss: 1.58632\n",
      "Main effects tuning epoch: 33, train loss: 1.49160, val loss: 1.57392\n",
      "Main effects tuning epoch: 34, train loss: 1.49714, val loss: 1.57388\n",
      "Main effects tuning epoch: 35, train loss: 1.48971, val loss: 1.57233\n",
      "Main effects tuning epoch: 36, train loss: 1.49242, val loss: 1.57660\n",
      "Main effects tuning epoch: 37, train loss: 1.48802, val loss: 1.57008\n",
      "Main effects tuning epoch: 38, train loss: 1.48870, val loss: 1.57355\n",
      "Main effects tuning epoch: 39, train loss: 1.50100, val loss: 1.58312\n",
      "Main effects tuning epoch: 40, train loss: 1.48819, val loss: 1.56999\n",
      "Main effects tuning epoch: 41, train loss: 1.50270, val loss: 1.57713\n",
      "Main effects tuning epoch: 42, train loss: 1.49514, val loss: 1.58071\n",
      "Main effects tuning epoch: 43, train loss: 1.49041, val loss: 1.57681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects tuning epoch: 44, train loss: 1.49502, val loss: 1.57884\n",
      "Main effects tuning epoch: 45, train loss: 1.49143, val loss: 1.57524\n",
      "Main effects tuning epoch: 46, train loss: 1.48883, val loss: 1.57344\n",
      "Main effects tuning epoch: 47, train loss: 1.49268, val loss: 1.57392\n",
      "Main effects tuning epoch: 48, train loss: 1.49687, val loss: 1.58188\n",
      "Main effects tuning epoch: 49, train loss: 1.49406, val loss: 1.57610\n",
      "Main effects tuning epoch: 50, train loss: 1.50032, val loss: 1.57658\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.40956, val loss: 1.49314\n",
      "Interaction training epoch: 2, train loss: 1.46902, val loss: 1.51986\n",
      "Interaction training epoch: 3, train loss: 1.15048, val loss: 1.18366\n",
      "Interaction training epoch: 4, train loss: 1.13589, val loss: 1.18122\n",
      "Interaction training epoch: 5, train loss: 1.07473, val loss: 1.12770\n",
      "Interaction training epoch: 6, train loss: 0.99921, val loss: 1.03674\n",
      "Interaction training epoch: 7, train loss: 1.00172, val loss: 1.05620\n",
      "Interaction training epoch: 8, train loss: 1.01769, val loss: 1.05947\n",
      "Interaction training epoch: 9, train loss: 0.99403, val loss: 1.03591\n",
      "Interaction training epoch: 10, train loss: 1.00592, val loss: 1.03974\n",
      "Interaction training epoch: 11, train loss: 0.99622, val loss: 1.02490\n",
      "Interaction training epoch: 12, train loss: 0.97908, val loss: 1.02599\n",
      "Interaction training epoch: 13, train loss: 0.98155, val loss: 1.01341\n",
      "Interaction training epoch: 14, train loss: 0.97008, val loss: 1.00133\n",
      "Interaction training epoch: 15, train loss: 0.95424, val loss: 0.97799\n",
      "Interaction training epoch: 16, train loss: 0.95155, val loss: 0.97238\n",
      "Interaction training epoch: 17, train loss: 0.98059, val loss: 1.01282\n",
      "Interaction training epoch: 18, train loss: 0.94549, val loss: 0.96955\n",
      "Interaction training epoch: 19, train loss: 0.93598, val loss: 0.95780\n",
      "Interaction training epoch: 20, train loss: 0.93828, val loss: 0.97372\n",
      "Interaction training epoch: 21, train loss: 0.93293, val loss: 0.96572\n",
      "Interaction training epoch: 22, train loss: 0.95286, val loss: 0.98104\n",
      "Interaction training epoch: 23, train loss: 0.93339, val loss: 0.96433\n",
      "Interaction training epoch: 24, train loss: 0.91513, val loss: 0.95006\n",
      "Interaction training epoch: 25, train loss: 0.92670, val loss: 0.94249\n",
      "Interaction training epoch: 26, train loss: 0.93005, val loss: 0.94204\n",
      "Interaction training epoch: 27, train loss: 0.92169, val loss: 0.95041\n",
      "Interaction training epoch: 28, train loss: 0.94260, val loss: 0.97708\n",
      "Interaction training epoch: 29, train loss: 0.90709, val loss: 0.91891\n",
      "Interaction training epoch: 30, train loss: 0.89673, val loss: 0.91938\n",
      "Interaction training epoch: 31, train loss: 0.91288, val loss: 0.94253\n",
      "Interaction training epoch: 32, train loss: 0.89432, val loss: 0.91265\n",
      "Interaction training epoch: 33, train loss: 0.86775, val loss: 0.89799\n",
      "Interaction training epoch: 34, train loss: 0.89509, val loss: 0.92433\n",
      "Interaction training epoch: 35, train loss: 0.88679, val loss: 0.90835\n",
      "Interaction training epoch: 36, train loss: 0.86156, val loss: 0.89861\n",
      "Interaction training epoch: 37, train loss: 0.88817, val loss: 0.91977\n",
      "Interaction training epoch: 38, train loss: 0.92171, val loss: 0.94598\n",
      "Interaction training epoch: 39, train loss: 0.90946, val loss: 0.93880\n",
      "Interaction training epoch: 40, train loss: 0.88316, val loss: 0.91862\n",
      "Interaction training epoch: 41, train loss: 0.85277, val loss: 0.89711\n",
      "Interaction training epoch: 42, train loss: 0.85934, val loss: 0.89101\n",
      "Interaction training epoch: 43, train loss: 0.88933, val loss: 0.92912\n",
      "Interaction training epoch: 44, train loss: 0.87513, val loss: 0.91175\n",
      "Interaction training epoch: 45, train loss: 0.85681, val loss: 0.89279\n",
      "Interaction training epoch: 46, train loss: 0.84760, val loss: 0.88703\n",
      "Interaction training epoch: 47, train loss: 0.86431, val loss: 0.90668\n",
      "Interaction training epoch: 48, train loss: 0.86026, val loss: 0.89881\n",
      "Interaction training epoch: 49, train loss: 0.86555, val loss: 0.89054\n",
      "Interaction training epoch: 50, train loss: 0.84109, val loss: 0.88072\n",
      "Interaction training epoch: 51, train loss: 0.84642, val loss: 0.87915\n",
      "Interaction training epoch: 52, train loss: 0.83248, val loss: 0.87359\n",
      "Interaction training epoch: 53, train loss: 0.84579, val loss: 0.89416\n",
      "Interaction training epoch: 54, train loss: 0.85331, val loss: 0.89070\n",
      "Interaction training epoch: 55, train loss: 0.85759, val loss: 0.88773\n",
      "Interaction training epoch: 56, train loss: 0.84289, val loss: 0.87615\n",
      "Interaction training epoch: 57, train loss: 0.84675, val loss: 0.89031\n",
      "Interaction training epoch: 58, train loss: 0.82679, val loss: 0.85761\n",
      "Interaction training epoch: 59, train loss: 0.82518, val loss: 0.86964\n",
      "Interaction training epoch: 60, train loss: 0.83318, val loss: 0.87064\n",
      "Interaction training epoch: 61, train loss: 0.85557, val loss: 0.88900\n",
      "Interaction training epoch: 62, train loss: 0.84306, val loss: 0.88361\n",
      "Interaction training epoch: 63, train loss: 0.83336, val loss: 0.86717\n",
      "Interaction training epoch: 64, train loss: 0.82752, val loss: 0.86794\n",
      "Interaction training epoch: 65, train loss: 0.81838, val loss: 0.85637\n",
      "Interaction training epoch: 66, train loss: 0.83686, val loss: 0.86853\n",
      "Interaction training epoch: 67, train loss: 0.83602, val loss: 0.87397\n",
      "Interaction training epoch: 68, train loss: 0.83543, val loss: 0.87308\n",
      "Interaction training epoch: 69, train loss: 0.82697, val loss: 0.86509\n",
      "Interaction training epoch: 70, train loss: 0.83767, val loss: 0.88222\n",
      "Interaction training epoch: 71, train loss: 0.83754, val loss: 0.87786\n",
      "Interaction training epoch: 72, train loss: 0.82683, val loss: 0.87203\n",
      "Interaction training epoch: 73, train loss: 0.83112, val loss: 0.86827\n",
      "Interaction training epoch: 74, train loss: 0.83455, val loss: 0.86857\n",
      "Interaction training epoch: 75, train loss: 0.82355, val loss: 0.86710\n",
      "Interaction training epoch: 76, train loss: 0.83016, val loss: 0.87242\n",
      "Interaction training epoch: 77, train loss: 0.84048, val loss: 0.88576\n",
      "Interaction training epoch: 78, train loss: 0.82393, val loss: 0.86247\n",
      "Interaction training epoch: 79, train loss: 0.81613, val loss: 0.85315\n",
      "Interaction training epoch: 80, train loss: 0.82724, val loss: 0.87836\n",
      "Interaction training epoch: 81, train loss: 0.84099, val loss: 0.87866\n",
      "Interaction training epoch: 82, train loss: 0.83836, val loss: 0.87295\n",
      "Interaction training epoch: 83, train loss: 0.81783, val loss: 0.85521\n",
      "Interaction training epoch: 84, train loss: 0.82666, val loss: 0.87209\n",
      "Interaction training epoch: 85, train loss: 0.84005, val loss: 0.87491\n",
      "Interaction training epoch: 86, train loss: 0.83174, val loss: 0.86576\n",
      "Interaction training epoch: 87, train loss: 0.81651, val loss: 0.86218\n",
      "Interaction training epoch: 88, train loss: 0.81480, val loss: 0.85396\n",
      "Interaction training epoch: 89, train loss: 0.85614, val loss: 0.89144\n",
      "Interaction training epoch: 90, train loss: 0.83109, val loss: 0.87679\n",
      "Interaction training epoch: 91, train loss: 0.82518, val loss: 0.87027\n",
      "Interaction training epoch: 92, train loss: 0.81499, val loss: 0.86139\n",
      "Interaction training epoch: 93, train loss: 0.82309, val loss: 0.86860\n",
      "Interaction training epoch: 94, train loss: 0.82397, val loss: 0.86232\n",
      "Interaction training epoch: 95, train loss: 0.81835, val loss: 0.86587\n",
      "Interaction training epoch: 96, train loss: 0.82623, val loss: 0.86952\n",
      "Interaction training epoch: 97, train loss: 0.81929, val loss: 0.86970\n",
      "Interaction training epoch: 98, train loss: 0.82161, val loss: 0.85792\n",
      "Interaction training epoch: 99, train loss: 0.82247, val loss: 0.86405\n",
      "Interaction training epoch: 100, train loss: 0.81022, val loss: 0.85209\n",
      "Interaction training epoch: 101, train loss: 0.82315, val loss: 0.87212\n",
      "Interaction training epoch: 102, train loss: 0.83412, val loss: 0.87518\n",
      "Interaction training epoch: 103, train loss: 0.81954, val loss: 0.86697\n",
      "Interaction training epoch: 104, train loss: 0.81300, val loss: 0.85138\n",
      "Interaction training epoch: 105, train loss: 0.80942, val loss: 0.85989\n",
      "Interaction training epoch: 106, train loss: 0.81791, val loss: 0.85945\n",
      "Interaction training epoch: 107, train loss: 0.81759, val loss: 0.85588\n",
      "Interaction training epoch: 108, train loss: 0.82010, val loss: 0.86858\n",
      "Interaction training epoch: 109, train loss: 0.81132, val loss: 0.84401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 110, train loss: 0.83717, val loss: 0.88094\n",
      "Interaction training epoch: 111, train loss: 0.81600, val loss: 0.85822\n",
      "Interaction training epoch: 112, train loss: 0.80618, val loss: 0.84638\n",
      "Interaction training epoch: 113, train loss: 0.80709, val loss: 0.85568\n",
      "Interaction training epoch: 114, train loss: 0.81600, val loss: 0.84905\n",
      "Interaction training epoch: 115, train loss: 0.81632, val loss: 0.86988\n",
      "Interaction training epoch: 116, train loss: 0.80968, val loss: 0.84773\n",
      "Interaction training epoch: 117, train loss: 0.80877, val loss: 0.85952\n",
      "Interaction training epoch: 118, train loss: 0.80687, val loss: 0.85056\n",
      "Interaction training epoch: 119, train loss: 0.80957, val loss: 0.85752\n",
      "Interaction training epoch: 120, train loss: 0.81501, val loss: 0.85651\n",
      "Interaction training epoch: 121, train loss: 0.81923, val loss: 0.86564\n",
      "Interaction training epoch: 122, train loss: 0.80755, val loss: 0.84859\n",
      "Interaction training epoch: 123, train loss: 0.81166, val loss: 0.85734\n",
      "Interaction training epoch: 124, train loss: 0.81013, val loss: 0.85555\n",
      "Interaction training epoch: 125, train loss: 0.81091, val loss: 0.85308\n",
      "Interaction training epoch: 126, train loss: 0.80867, val loss: 0.85629\n",
      "Interaction training epoch: 127, train loss: 0.81037, val loss: 0.85780\n",
      "Interaction training epoch: 128, train loss: 0.81253, val loss: 0.85737\n",
      "Interaction training epoch: 129, train loss: 0.80665, val loss: 0.85786\n",
      "Interaction training epoch: 130, train loss: 0.80427, val loss: 0.85136\n",
      "Interaction training epoch: 131, train loss: 0.80781, val loss: 0.85389\n",
      "Interaction training epoch: 132, train loss: 0.82419, val loss: 0.86690\n",
      "Interaction training epoch: 133, train loss: 0.81746, val loss: 0.85865\n",
      "Interaction training epoch: 134, train loss: 0.80749, val loss: 0.85179\n",
      "Interaction training epoch: 135, train loss: 0.81048, val loss: 0.86009\n",
      "Interaction training epoch: 136, train loss: 0.81184, val loss: 0.85719\n",
      "Interaction training epoch: 137, train loss: 0.81149, val loss: 0.85676\n",
      "Interaction training epoch: 138, train loss: 0.80772, val loss: 0.84650\n",
      "Interaction training epoch: 139, train loss: 0.81263, val loss: 0.86131\n",
      "Interaction training epoch: 140, train loss: 0.82019, val loss: 0.86644\n",
      "Interaction training epoch: 141, train loss: 0.80825, val loss: 0.84972\n",
      "Interaction training epoch: 142, train loss: 0.80414, val loss: 0.84837\n",
      "Interaction training epoch: 143, train loss: 0.80359, val loss: 0.85093\n",
      "Interaction training epoch: 144, train loss: 0.81233, val loss: 0.85791\n",
      "Interaction training epoch: 145, train loss: 0.80926, val loss: 0.85513\n",
      "Interaction training epoch: 146, train loss: 0.80006, val loss: 0.85147\n",
      "Interaction training epoch: 147, train loss: 0.80703, val loss: 0.85325\n",
      "Interaction training epoch: 148, train loss: 0.81535, val loss: 0.85725\n",
      "Interaction training epoch: 149, train loss: 0.80375, val loss: 0.85029\n",
      "Interaction training epoch: 150, train loss: 0.80414, val loss: 0.85290\n",
      "Interaction training epoch: 151, train loss: 0.82894, val loss: 0.86931\n",
      "Interaction training epoch: 152, train loss: 0.81451, val loss: 0.86664\n",
      "Interaction training epoch: 153, train loss: 0.81076, val loss: 0.85282\n",
      "Interaction training epoch: 154, train loss: 0.80220, val loss: 0.84632\n",
      "Interaction training epoch: 155, train loss: 0.82089, val loss: 0.86551\n",
      "Interaction training epoch: 156, train loss: 0.80933, val loss: 0.85724\n",
      "Interaction training epoch: 157, train loss: 0.80438, val loss: 0.85361\n",
      "Interaction training epoch: 158, train loss: 0.80491, val loss: 0.85176\n",
      "Interaction training epoch: 159, train loss: 0.81120, val loss: 0.86295\n",
      "Interaction training epoch: 160, train loss: 0.79989, val loss: 0.84536\n",
      "Interaction training epoch: 161, train loss: 0.80234, val loss: 0.85357\n",
      "Interaction training epoch: 162, train loss: 0.81037, val loss: 0.85848\n",
      "Interaction training epoch: 163, train loss: 0.81419, val loss: 0.86560\n",
      "Interaction training epoch: 164, train loss: 0.80728, val loss: 0.84883\n",
      "Interaction training epoch: 165, train loss: 0.81221, val loss: 0.86247\n",
      "Interaction training epoch: 166, train loss: 0.81487, val loss: 0.85769\n",
      "Interaction training epoch: 167, train loss: 0.80699, val loss: 0.85687\n",
      "Interaction training epoch: 168, train loss: 0.79950, val loss: 0.84287\n",
      "Interaction training epoch: 169, train loss: 0.80100, val loss: 0.85322\n",
      "Interaction training epoch: 170, train loss: 0.81637, val loss: 0.86149\n",
      "Interaction training epoch: 171, train loss: 0.80393, val loss: 0.85089\n",
      "Interaction training epoch: 172, train loss: 0.79365, val loss: 0.83904\n",
      "Interaction training epoch: 173, train loss: 0.81176, val loss: 0.85970\n",
      "Interaction training epoch: 174, train loss: 0.80301, val loss: 0.85232\n",
      "Interaction training epoch: 175, train loss: 0.80116, val loss: 0.84683\n",
      "Interaction training epoch: 176, train loss: 0.81326, val loss: 0.86029\n",
      "Interaction training epoch: 177, train loss: 0.81704, val loss: 0.86040\n",
      "Interaction training epoch: 178, train loss: 0.80098, val loss: 0.84745\n",
      "Interaction training epoch: 179, train loss: 0.81235, val loss: 0.85552\n",
      "Interaction training epoch: 180, train loss: 0.80835, val loss: 0.85639\n",
      "Interaction training epoch: 181, train loss: 0.80479, val loss: 0.85767\n",
      "Interaction training epoch: 182, train loss: 0.79743, val loss: 0.83575\n",
      "Interaction training epoch: 183, train loss: 0.79694, val loss: 0.84273\n",
      "Interaction training epoch: 184, train loss: 0.79892, val loss: 0.85056\n",
      "Interaction training epoch: 185, train loss: 0.80041, val loss: 0.85110\n",
      "Interaction training epoch: 186, train loss: 0.80182, val loss: 0.85149\n",
      "Interaction training epoch: 187, train loss: 0.79689, val loss: 0.84648\n",
      "Interaction training epoch: 188, train loss: 0.80978, val loss: 0.85604\n",
      "Interaction training epoch: 189, train loss: 0.80610, val loss: 0.85084\n",
      "Interaction training epoch: 190, train loss: 0.79734, val loss: 0.85203\n",
      "Interaction training epoch: 191, train loss: 0.80876, val loss: 0.86221\n",
      "Interaction training epoch: 192, train loss: 0.79890, val loss: 0.84818\n",
      "Interaction training epoch: 193, train loss: 0.80842, val loss: 0.85520\n",
      "Interaction training epoch: 194, train loss: 0.80154, val loss: 0.84554\n",
      "Interaction training epoch: 195, train loss: 0.79483, val loss: 0.84356\n",
      "Interaction training epoch: 196, train loss: 0.79826, val loss: 0.84428\n",
      "Interaction training epoch: 197, train loss: 0.79959, val loss: 0.85096\n",
      "Interaction training epoch: 198, train loss: 0.79740, val loss: 0.84526\n",
      "Interaction training epoch: 199, train loss: 0.80686, val loss: 0.84751\n",
      "Interaction training epoch: 200, train loss: 0.80577, val loss: 0.86060\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########2 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.80570, val loss: 0.84916\n",
      "Interaction tuning epoch: 2, train loss: 0.80524, val loss: 0.85347\n",
      "Interaction tuning epoch: 3, train loss: 0.80136, val loss: 0.85081\n",
      "Interaction tuning epoch: 4, train loss: 0.80698, val loss: 0.85694\n",
      "Interaction tuning epoch: 5, train loss: 0.80669, val loss: 0.85476\n",
      "Interaction tuning epoch: 6, train loss: 0.80080, val loss: 0.84848\n",
      "Interaction tuning epoch: 7, train loss: 0.80122, val loss: 0.85215\n",
      "Interaction tuning epoch: 8, train loss: 0.80040, val loss: 0.85415\n",
      "Interaction tuning epoch: 9, train loss: 0.80221, val loss: 0.85062\n",
      "Interaction tuning epoch: 10, train loss: 0.80450, val loss: 0.85713\n",
      "Interaction tuning epoch: 11, train loss: 0.79896, val loss: 0.84491\n",
      "Interaction tuning epoch: 12, train loss: 0.80202, val loss: 0.85504\n",
      "Interaction tuning epoch: 13, train loss: 0.80700, val loss: 0.85342\n",
      "Interaction tuning epoch: 14, train loss: 0.79986, val loss: 0.84970\n",
      "Interaction tuning epoch: 15, train loss: 0.80393, val loss: 0.85525\n",
      "Interaction tuning epoch: 16, train loss: 0.79974, val loss: 0.85368\n",
      "Interaction tuning epoch: 17, train loss: 0.80172, val loss: 0.85238\n",
      "Interaction tuning epoch: 18, train loss: 0.80453, val loss: 0.84990\n",
      "Interaction tuning epoch: 19, train loss: 0.80417, val loss: 0.85146\n",
      "Interaction tuning epoch: 20, train loss: 0.79664, val loss: 0.84916\n",
      "Interaction tuning epoch: 21, train loss: 0.80322, val loss: 0.84778\n",
      "Interaction tuning epoch: 22, train loss: 0.80561, val loss: 0.85637\n",
      "Interaction tuning epoch: 23, train loss: 0.79833, val loss: 0.84399\n",
      "Interaction tuning epoch: 24, train loss: 0.79969, val loss: 0.84994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 25, train loss: 0.80481, val loss: 0.85554\n",
      "Interaction tuning epoch: 26, train loss: 0.80254, val loss: 0.85458\n",
      "Interaction tuning epoch: 27, train loss: 0.79875, val loss: 0.84839\n",
      "Interaction tuning epoch: 28, train loss: 0.80513, val loss: 0.85831\n",
      "Interaction tuning epoch: 29, train loss: 0.80753, val loss: 0.84875\n",
      "Interaction tuning epoch: 30, train loss: 0.79658, val loss: 0.85137\n",
      "Interaction tuning epoch: 31, train loss: 0.79727, val loss: 0.85291\n",
      "Interaction tuning epoch: 32, train loss: 0.79993, val loss: 0.84851\n",
      "Interaction tuning epoch: 33, train loss: 0.80092, val loss: 0.85302\n",
      "Interaction tuning epoch: 34, train loss: 0.79704, val loss: 0.84780\n",
      "Interaction tuning epoch: 35, train loss: 0.80439, val loss: 0.85229\n",
      "Interaction tuning epoch: 36, train loss: 0.80572, val loss: 0.85794\n",
      "Interaction tuning epoch: 37, train loss: 0.80353, val loss: 0.85577\n",
      "Interaction tuning epoch: 38, train loss: 0.79578, val loss: 0.84485\n",
      "Interaction tuning epoch: 39, train loss: 0.80244, val loss: 0.84865\n",
      "Interaction tuning epoch: 40, train loss: 0.80553, val loss: 0.85673\n",
      "Interaction tuning epoch: 41, train loss: 0.80656, val loss: 0.85327\n",
      "Interaction tuning epoch: 42, train loss: 0.80376, val loss: 0.85687\n",
      "Interaction tuning epoch: 43, train loss: 0.79281, val loss: 0.84164\n",
      "Interaction tuning epoch: 44, train loss: 0.79850, val loss: 0.84979\n",
      "Interaction tuning epoch: 45, train loss: 0.80399, val loss: 0.85821\n",
      "Interaction tuning epoch: 46, train loss: 0.79414, val loss: 0.84265\n",
      "Interaction tuning epoch: 47, train loss: 0.79762, val loss: 0.84737\n",
      "Interaction tuning epoch: 48, train loss: 0.79840, val loss: 0.84865\n",
      "Interaction tuning epoch: 49, train loss: 0.80174, val loss: 0.85633\n",
      "Interaction tuning epoch: 50, train loss: 0.79198, val loss: 0.84100\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 71.6136863231659\n",
      "After the gam stage, training error is 0.79198 , validation error is 0.84100\n",
      "missing value counts: 92883\n",
      "[SoftImpute] Max Singular Value of X_init = 19.838216\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed MAE=0.748390 validation MAE=0.833842,rank=3\n",
      "[SoftImpute] Iter 1: observed MAE=0.784432 validation MAE=0.835588,rank=3\n",
      "[SoftImpute] Iter 2: observed MAE=0.785486 validation MAE=0.835245,rank=3\n",
      "[SoftImpute] Iter 3: observed MAE=0.784212 validation MAE=0.834049,rank=3\n",
      "[SoftImpute] Iter 4: observed MAE=0.789290 validation MAE=0.840025,rank=3\n",
      "[SoftImpute] Iter 5: observed MAE=0.789239 validation MAE=0.840108,rank=3\n",
      "[SoftImpute] Iter 6: observed MAE=0.790036 validation MAE=0.840757,rank=3\n",
      "[SoftImpute] Iter 7: observed MAE=0.790071 validation MAE=0.840795,rank=3\n",
      "[SoftImpute] Iter 8: observed MAE=0.790087 validation MAE=0.840627,rank=3\n",
      "[SoftImpute] Iter 9: observed MAE=0.790114 validation MAE=0.840646,rank=3\n",
      "[SoftImpute] Iter 10: observed MAE=0.790137 validation MAE=0.840658,rank=3\n",
      "[SoftImpute] Iter 11: observed MAE=0.790159 validation MAE=0.840667,rank=3\n",
      "[SoftImpute] Iter 12: observed MAE=0.790179 validation MAE=0.840673,rank=3\n",
      "[SoftImpute] Iter 13: observed MAE=0.790199 validation MAE=0.840678,rank=3\n",
      "[SoftImpute] Iter 14: observed MAE=0.790218 validation MAE=0.840682,rank=3\n",
      "[SoftImpute] Iter 15: observed MAE=0.790379 validation MAE=0.840798,rank=3\n",
      "[SoftImpute] Iter 16: observed MAE=0.790411 validation MAE=0.840791,rank=3\n",
      "[SoftImpute] Iter 17: observed MAE=0.790444 validation MAE=0.840790,rank=3\n",
      "[SoftImpute] Iter 18: observed MAE=0.790477 validation MAE=0.840791,rank=3\n",
      "[SoftImpute] Iter 19: observed MAE=0.790510 validation MAE=0.840792,rank=3\n",
      "[SoftImpute] Iter 20: observed MAE=0.790543 validation MAE=0.840795,rank=3\n",
      "[SoftImpute] Iter 21: observed MAE=0.790577 validation MAE=0.840798,rank=3\n",
      "[SoftImpute] Iter 22: observed MAE=0.790610 validation MAE=0.840801,rank=3\n",
      "[SoftImpute] Iter 23: observed MAE=0.790643 validation MAE=0.840805,rank=3\n",
      "[SoftImpute] Iter 24: observed MAE=0.790676 validation MAE=0.840808,rank=3\n",
      "[SoftImpute] Iter 25: observed MAE=0.790707 validation MAE=0.840812,rank=3\n",
      "[SoftImpute] Iter 26: observed MAE=0.790738 validation MAE=0.840816,rank=3\n",
      "[SoftImpute] Iter 27: observed MAE=0.790768 validation MAE=0.840819,rank=3\n",
      "[SoftImpute] Iter 28: observed MAE=0.790796 validation MAE=0.840822,rank=3\n",
      "[SoftImpute] Iter 29: observed MAE=0.790822 validation MAE=0.840825,rank=3\n",
      "[SoftImpute] Iter 30: observed MAE=0.790847 validation MAE=0.840828,rank=3\n",
      "[SoftImpute] Iter 31: observed MAE=0.790870 validation MAE=0.840831,rank=3\n",
      "[SoftImpute] Iter 32: observed MAE=0.790891 validation MAE=0.840834,rank=3\n",
      "[SoftImpute] Iter 33: observed MAE=0.790910 validation MAE=0.840836,rank=3\n",
      "[SoftImpute] Iter 34: observed MAE=0.790928 validation MAE=0.840838,rank=3\n",
      "[SoftImpute] Iter 35: observed MAE=0.790944 validation MAE=0.840840,rank=3\n",
      "[SoftImpute] Iter 36: observed MAE=0.790958 validation MAE=0.840842,rank=3\n",
      "[SoftImpute] Iter 37: observed MAE=0.790971 validation MAE=0.840844,rank=3\n",
      "[SoftImpute] Iter 38: observed MAE=0.790983 validation MAE=0.840845,rank=3\n",
      "[SoftImpute] Iter 39: observed MAE=0.790993 validation MAE=0.840846,rank=3\n",
      "[SoftImpute] Iter 40: observed MAE=0.791002 validation MAE=0.840848,rank=3\n",
      "[SoftImpute] Iter 41: observed MAE=0.791010 validation MAE=0.840849,rank=3\n",
      "[SoftImpute] Iter 42: observed MAE=0.791017 validation MAE=0.840850,rank=3\n",
      "[SoftImpute] Iter 43: observed MAE=0.791023 validation MAE=0.840850,rank=3\n",
      "[SoftImpute] Iter 44: observed MAE=0.791028 validation MAE=0.840851,rank=3\n",
      "[SoftImpute] Iter 45: observed MAE=0.791033 validation MAE=0.840852,rank=3\n",
      "[SoftImpute] Iter 46: observed MAE=0.791037 validation MAE=0.840852,rank=3\n",
      "[SoftImpute] Iter 47: observed MAE=0.791041 validation MAE=0.840853,rank=3\n",
      "[SoftImpute] Iter 48: observed MAE=0.791044 validation MAE=0.840853,rank=3\n",
      "[SoftImpute] Iter 49: observed MAE=0.791047 validation MAE=0.840853,rank=3\n",
      "[SoftImpute] Iter 50: observed MAE=0.791049 validation MAE=0.840854,rank=3\n",
      "[SoftImpute] Iter 51: observed MAE=0.791051 validation MAE=0.840854,rank=3\n",
      "[SoftImpute] Iter 52: observed MAE=0.791053 validation MAE=0.840854,rank=3\n",
      "[SoftImpute] Iter 53: observed MAE=0.791055 validation MAE=0.840854,rank=3\n",
      "[SoftImpute] Iter 54: observed MAE=0.791056 validation MAE=0.840855,rank=3\n",
      "[SoftImpute] Iter 55: observed MAE=0.791057 validation MAE=0.840855,rank=3\n",
      "[SoftImpute] Iter 56: observed MAE=0.791058 validation MAE=0.840855,rank=3\n",
      "[SoftImpute] Iter 57: observed MAE=0.791059 validation MAE=0.840855,rank=3\n",
      "[SoftImpute] Stopped after iteration 57 for lambda=0.396764\n",
      "final num of user group: 9\n",
      "final num of item group: 17\n",
      "change mode state : True\n",
      "time cost: 7.406018972396851\n",
      "After the matrix factor stage, training error is 0.79106, validation error is 0.84086\n",
      "1\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.04223, val loss: 4.07966\n",
      "Main effects training epoch: 2, train loss: 3.87743, val loss: 3.90888\n",
      "Main effects training epoch: 3, train loss: 3.69330, val loss: 3.70849\n",
      "Main effects training epoch: 4, train loss: 3.45182, val loss: 3.44803\n",
      "Main effects training epoch: 5, train loss: 3.28371, val loss: 3.26471\n",
      "Main effects training epoch: 6, train loss: 3.28085, val loss: 3.25302\n",
      "Main effects training epoch: 7, train loss: 3.28264, val loss: 3.27286\n",
      "Main effects training epoch: 8, train loss: 3.24228, val loss: 3.23382\n",
      "Main effects training epoch: 9, train loss: 3.24982, val loss: 3.22751\n",
      "Main effects training epoch: 10, train loss: 3.15367, val loss: 3.13339\n",
      "Main effects training epoch: 11, train loss: 3.11592, val loss: 3.09664\n",
      "Main effects training epoch: 12, train loss: 2.97341, val loss: 2.95106\n",
      "Main effects training epoch: 13, train loss: 2.94780, val loss: 2.92757\n",
      "Main effects training epoch: 14, train loss: 2.91572, val loss: 2.90291\n",
      "Main effects training epoch: 15, train loss: 2.86215, val loss: 2.84341\n",
      "Main effects training epoch: 16, train loss: 2.84414, val loss: 2.82600\n",
      "Main effects training epoch: 17, train loss: 2.81244, val loss: 2.79447\n",
      "Main effects training epoch: 18, train loss: 2.73655, val loss: 2.70742\n",
      "Main effects training epoch: 19, train loss: 2.67624, val loss: 2.64986\n",
      "Main effects training epoch: 20, train loss: 2.59558, val loss: 2.56727\n",
      "Main effects training epoch: 21, train loss: 2.54938, val loss: 2.51977\n",
      "Main effects training epoch: 22, train loss: 2.53833, val loss: 2.50210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 23, train loss: 2.49214, val loss: 2.46272\n",
      "Main effects training epoch: 24, train loss: 2.45752, val loss: 2.43230\n",
      "Main effects training epoch: 25, train loss: 2.44392, val loss: 2.41804\n",
      "Main effects training epoch: 26, train loss: 2.38365, val loss: 2.35116\n",
      "Main effects training epoch: 27, train loss: 2.34172, val loss: 2.31160\n",
      "Main effects training epoch: 28, train loss: 2.33911, val loss: 2.30844\n",
      "Main effects training epoch: 29, train loss: 2.32689, val loss: 2.29223\n",
      "Main effects training epoch: 30, train loss: 2.28543, val loss: 2.24907\n",
      "Main effects training epoch: 31, train loss: 2.24514, val loss: 2.20920\n",
      "Main effects training epoch: 32, train loss: 2.22641, val loss: 2.19495\n",
      "Main effects training epoch: 33, train loss: 2.18305, val loss: 2.14395\n",
      "Main effects training epoch: 34, train loss: 2.19697, val loss: 2.15616\n",
      "Main effects training epoch: 35, train loss: 2.17140, val loss: 2.13563\n",
      "Main effects training epoch: 36, train loss: 2.12048, val loss: 2.07707\n",
      "Main effects training epoch: 37, train loss: 2.09984, val loss: 2.05799\n",
      "Main effects training epoch: 38, train loss: 2.10031, val loss: 2.05785\n",
      "Main effects training epoch: 39, train loss: 2.06489, val loss: 2.02353\n",
      "Main effects training epoch: 40, train loss: 2.04812, val loss: 1.99928\n",
      "Main effects training epoch: 41, train loss: 2.04157, val loss: 2.00095\n",
      "Main effects training epoch: 42, train loss: 2.00709, val loss: 1.96324\n",
      "Main effects training epoch: 43, train loss: 2.02512, val loss: 1.98174\n",
      "Main effects training epoch: 44, train loss: 1.98058, val loss: 1.93926\n",
      "Main effects training epoch: 45, train loss: 1.97243, val loss: 1.92693\n",
      "Main effects training epoch: 46, train loss: 1.96603, val loss: 1.92497\n",
      "Main effects training epoch: 47, train loss: 1.93875, val loss: 1.89712\n",
      "Main effects training epoch: 48, train loss: 1.93934, val loss: 1.89765\n",
      "Main effects training epoch: 49, train loss: 1.90895, val loss: 1.86692\n",
      "Main effects training epoch: 50, train loss: 1.91698, val loss: 1.87667\n",
      "Main effects training epoch: 51, train loss: 1.88837, val loss: 1.84628\n",
      "Main effects training epoch: 52, train loss: 1.89208, val loss: 1.84611\n",
      "Main effects training epoch: 53, train loss: 1.87853, val loss: 1.83974\n",
      "Main effects training epoch: 54, train loss: 1.87249, val loss: 1.83024\n",
      "Main effects training epoch: 55, train loss: 1.87281, val loss: 1.83041\n",
      "Main effects training epoch: 56, train loss: 1.85484, val loss: 1.81158\n",
      "Main effects training epoch: 57, train loss: 1.85025, val loss: 1.80603\n",
      "Main effects training epoch: 58, train loss: 1.84757, val loss: 1.80897\n",
      "Main effects training epoch: 59, train loss: 1.83170, val loss: 1.78358\n",
      "Main effects training epoch: 60, train loss: 1.82744, val loss: 1.78377\n",
      "Main effects training epoch: 61, train loss: 1.82899, val loss: 1.78603\n",
      "Main effects training epoch: 62, train loss: 1.81803, val loss: 1.77439\n",
      "Main effects training epoch: 63, train loss: 1.81467, val loss: 1.77027\n",
      "Main effects training epoch: 64, train loss: 1.80949, val loss: 1.76540\n",
      "Main effects training epoch: 65, train loss: 1.80963, val loss: 1.76372\n",
      "Main effects training epoch: 66, train loss: 1.79602, val loss: 1.75155\n",
      "Main effects training epoch: 67, train loss: 1.79487, val loss: 1.75186\n",
      "Main effects training epoch: 68, train loss: 1.79450, val loss: 1.75028\n",
      "Main effects training epoch: 69, train loss: 1.78984, val loss: 1.74133\n",
      "Main effects training epoch: 70, train loss: 1.78356, val loss: 1.73913\n",
      "Main effects training epoch: 71, train loss: 1.78359, val loss: 1.73978\n",
      "Main effects training epoch: 72, train loss: 1.77448, val loss: 1.72845\n",
      "Main effects training epoch: 73, train loss: 1.77125, val loss: 1.72627\n",
      "Main effects training epoch: 74, train loss: 1.76538, val loss: 1.71749\n",
      "Main effects training epoch: 75, train loss: 1.75560, val loss: 1.71707\n",
      "Main effects training epoch: 76, train loss: 1.74426, val loss: 1.69830\n",
      "Main effects training epoch: 77, train loss: 1.73591, val loss: 1.70488\n",
      "Main effects training epoch: 78, train loss: 1.71879, val loss: 1.68571\n",
      "Main effects training epoch: 79, train loss: 1.71715, val loss: 1.69002\n",
      "Main effects training epoch: 80, train loss: 1.71029, val loss: 1.67808\n",
      "Main effects training epoch: 81, train loss: 1.70504, val loss: 1.68429\n",
      "Main effects training epoch: 82, train loss: 1.71274, val loss: 1.68761\n",
      "Main effects training epoch: 83, train loss: 1.70179, val loss: 1.67192\n",
      "Main effects training epoch: 84, train loss: 1.70153, val loss: 1.68157\n",
      "Main effects training epoch: 85, train loss: 1.70116, val loss: 1.67593\n",
      "Main effects training epoch: 86, train loss: 1.69519, val loss: 1.67033\n",
      "Main effects training epoch: 87, train loss: 1.69848, val loss: 1.67978\n",
      "Main effects training epoch: 88, train loss: 1.69601, val loss: 1.67527\n",
      "Main effects training epoch: 89, train loss: 1.69200, val loss: 1.66027\n",
      "Main effects training epoch: 90, train loss: 1.69288, val loss: 1.67640\n",
      "Main effects training epoch: 91, train loss: 1.68699, val loss: 1.65836\n",
      "Main effects training epoch: 92, train loss: 1.68260, val loss: 1.66387\n",
      "Main effects training epoch: 93, train loss: 1.67721, val loss: 1.65116\n",
      "Main effects training epoch: 94, train loss: 1.68508, val loss: 1.65280\n",
      "Main effects training epoch: 95, train loss: 1.67541, val loss: 1.65340\n",
      "Main effects training epoch: 96, train loss: 1.66602, val loss: 1.63217\n",
      "Main effects training epoch: 97, train loss: 1.67896, val loss: 1.65276\n",
      "Main effects training epoch: 98, train loss: 1.66150, val loss: 1.62831\n",
      "Main effects training epoch: 99, train loss: 1.66068, val loss: 1.62832\n",
      "Main effects training epoch: 100, train loss: 1.65030, val loss: 1.62344\n",
      "Main effects training epoch: 101, train loss: 1.65101, val loss: 1.61832\n",
      "Main effects training epoch: 102, train loss: 1.64728, val loss: 1.60524\n",
      "Main effects training epoch: 103, train loss: 1.64934, val loss: 1.62268\n",
      "Main effects training epoch: 104, train loss: 1.64123, val loss: 1.60679\n",
      "Main effects training epoch: 105, train loss: 1.64142, val loss: 1.61238\n",
      "Main effects training epoch: 106, train loss: 1.63985, val loss: 1.61096\n",
      "Main effects training epoch: 107, train loss: 1.64506, val loss: 1.62322\n",
      "Main effects training epoch: 108, train loss: 1.63935, val loss: 1.60210\n",
      "Main effects training epoch: 109, train loss: 1.64466, val loss: 1.60182\n",
      "Main effects training epoch: 110, train loss: 1.63705, val loss: 1.60383\n",
      "Main effects training epoch: 111, train loss: 1.63522, val loss: 1.59933\n",
      "Main effects training epoch: 112, train loss: 1.63972, val loss: 1.62121\n",
      "Main effects training epoch: 113, train loss: 1.63628, val loss: 1.59408\n",
      "Main effects training epoch: 114, train loss: 1.63435, val loss: 1.60852\n",
      "Main effects training epoch: 115, train loss: 1.63179, val loss: 1.60065\n",
      "Main effects training epoch: 116, train loss: 1.62937, val loss: 1.59625\n",
      "Main effects training epoch: 117, train loss: 1.63144, val loss: 1.59289\n",
      "Main effects training epoch: 118, train loss: 1.63759, val loss: 1.60904\n",
      "Main effects training epoch: 119, train loss: 1.62991, val loss: 1.59576\n",
      "Main effects training epoch: 120, train loss: 1.62703, val loss: 1.58815\n",
      "Main effects training epoch: 121, train loss: 1.62540, val loss: 1.59424\n",
      "Main effects training epoch: 122, train loss: 1.62653, val loss: 1.59211\n",
      "Main effects training epoch: 123, train loss: 1.62903, val loss: 1.60126\n",
      "Main effects training epoch: 124, train loss: 1.62727, val loss: 1.58747\n",
      "Main effects training epoch: 125, train loss: 1.63253, val loss: 1.61188\n",
      "Main effects training epoch: 126, train loss: 1.62346, val loss: 1.57829\n",
      "Main effects training epoch: 127, train loss: 1.62107, val loss: 1.59240\n",
      "Main effects training epoch: 128, train loss: 1.61850, val loss: 1.59418\n",
      "Main effects training epoch: 129, train loss: 1.62062, val loss: 1.58456\n",
      "Main effects training epoch: 130, train loss: 1.61498, val loss: 1.58427\n",
      "Main effects training epoch: 131, train loss: 1.61731, val loss: 1.58645\n",
      "Main effects training epoch: 132, train loss: 1.61294, val loss: 1.57843\n",
      "Main effects training epoch: 133, train loss: 1.61951, val loss: 1.58846\n",
      "Main effects training epoch: 134, train loss: 1.61025, val loss: 1.57842\n",
      "Main effects training epoch: 135, train loss: 1.60873, val loss: 1.57313\n",
      "Main effects training epoch: 136, train loss: 1.60679, val loss: 1.57638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 137, train loss: 1.61046, val loss: 1.58125\n",
      "Main effects training epoch: 138, train loss: 1.60513, val loss: 1.57605\n",
      "Main effects training epoch: 139, train loss: 1.61142, val loss: 1.57079\n",
      "Main effects training epoch: 140, train loss: 1.60409, val loss: 1.58642\n",
      "Main effects training epoch: 141, train loss: 1.60384, val loss: 1.56865\n",
      "Main effects training epoch: 142, train loss: 1.60099, val loss: 1.57256\n",
      "Main effects training epoch: 143, train loss: 1.59979, val loss: 1.57677\n",
      "Main effects training epoch: 144, train loss: 1.60047, val loss: 1.57468\n",
      "Main effects training epoch: 145, train loss: 1.59964, val loss: 1.56900\n",
      "Main effects training epoch: 146, train loss: 1.59742, val loss: 1.57449\n",
      "Main effects training epoch: 147, train loss: 1.59981, val loss: 1.57554\n",
      "Main effects training epoch: 148, train loss: 1.60723, val loss: 1.57359\n",
      "Main effects training epoch: 149, train loss: 1.59916, val loss: 1.57293\n",
      "Main effects training epoch: 150, train loss: 1.60263, val loss: 1.58600\n",
      "Main effects training epoch: 151, train loss: 1.59666, val loss: 1.57065\n",
      "Main effects training epoch: 152, train loss: 1.59972, val loss: 1.57017\n",
      "Main effects training epoch: 153, train loss: 1.60047, val loss: 1.58931\n",
      "Main effects training epoch: 154, train loss: 1.60092, val loss: 1.57732\n",
      "Main effects training epoch: 155, train loss: 1.59151, val loss: 1.56434\n",
      "Main effects training epoch: 156, train loss: 1.60466, val loss: 1.58376\n",
      "Main effects training epoch: 157, train loss: 1.59942, val loss: 1.57015\n",
      "Main effects training epoch: 158, train loss: 1.59854, val loss: 1.57791\n",
      "Main effects training epoch: 159, train loss: 1.59112, val loss: 1.55875\n",
      "Main effects training epoch: 160, train loss: 1.58885, val loss: 1.57038\n",
      "Main effects training epoch: 161, train loss: 1.58970, val loss: 1.56208\n",
      "Main effects training epoch: 162, train loss: 1.58787, val loss: 1.57418\n",
      "Main effects training epoch: 163, train loss: 1.58615, val loss: 1.56481\n",
      "Main effects training epoch: 164, train loss: 1.58966, val loss: 1.56586\n",
      "Main effects training epoch: 165, train loss: 1.58523, val loss: 1.56797\n",
      "Main effects training epoch: 166, train loss: 1.58557, val loss: 1.56406\n",
      "Main effects training epoch: 167, train loss: 1.58578, val loss: 1.57371\n",
      "Main effects training epoch: 168, train loss: 1.58301, val loss: 1.55939\n",
      "Main effects training epoch: 169, train loss: 1.57784, val loss: 1.56035\n",
      "Main effects training epoch: 170, train loss: 1.57909, val loss: 1.56051\n",
      "Main effects training epoch: 171, train loss: 1.58128, val loss: 1.57517\n",
      "Main effects training epoch: 172, train loss: 1.57871, val loss: 1.55674\n",
      "Main effects training epoch: 173, train loss: 1.57736, val loss: 1.55728\n",
      "Main effects training epoch: 174, train loss: 1.58693, val loss: 1.56704\n",
      "Main effects training epoch: 175, train loss: 1.57091, val loss: 1.55412\n",
      "Main effects training epoch: 176, train loss: 1.57016, val loss: 1.55434\n",
      "Main effects training epoch: 177, train loss: 1.56952, val loss: 1.55953\n",
      "Main effects training epoch: 178, train loss: 1.56656, val loss: 1.54792\n",
      "Main effects training epoch: 179, train loss: 1.57451, val loss: 1.54795\n",
      "Main effects training epoch: 180, train loss: 1.56888, val loss: 1.55947\n",
      "Main effects training epoch: 181, train loss: 1.57219, val loss: 1.56599\n",
      "Main effects training epoch: 182, train loss: 1.57319, val loss: 1.54312\n",
      "Main effects training epoch: 183, train loss: 1.56677, val loss: 1.56093\n",
      "Main effects training epoch: 184, train loss: 1.57276, val loss: 1.55867\n",
      "Main effects training epoch: 185, train loss: 1.56792, val loss: 1.54668\n",
      "Main effects training epoch: 186, train loss: 1.57371, val loss: 1.55728\n",
      "Main effects training epoch: 187, train loss: 1.57425, val loss: 1.55996\n",
      "Main effects training epoch: 188, train loss: 1.57578, val loss: 1.56903\n",
      "Main effects training epoch: 189, train loss: 1.56844, val loss: 1.55234\n",
      "Main effects training epoch: 190, train loss: 1.56576, val loss: 1.55202\n",
      "Main effects training epoch: 191, train loss: 1.56391, val loss: 1.54499\n",
      "Main effects training epoch: 192, train loss: 1.56063, val loss: 1.54553\n",
      "Main effects training epoch: 193, train loss: 1.56763, val loss: 1.56414\n",
      "Main effects training epoch: 194, train loss: 1.56895, val loss: 1.54388\n",
      "Main effects training epoch: 195, train loss: 1.56613, val loss: 1.54831\n",
      "Main effects training epoch: 196, train loss: 1.55802, val loss: 1.54822\n",
      "Main effects training epoch: 197, train loss: 1.56427, val loss: 1.54527\n",
      "Main effects training epoch: 198, train loss: 1.58275, val loss: 1.57974\n",
      "Main effects training epoch: 199, train loss: 1.56505, val loss: 1.54440\n",
      "Main effects training epoch: 200, train loss: 1.56899, val loss: 1.55795\n",
      "Main effects training epoch: 201, train loss: 1.55998, val loss: 1.54056\n",
      "Main effects training epoch: 202, train loss: 1.57008, val loss: 1.55679\n",
      "Main effects training epoch: 203, train loss: 1.55814, val loss: 1.54032\n",
      "Main effects training epoch: 204, train loss: 1.55633, val loss: 1.55273\n",
      "Main effects training epoch: 205, train loss: 1.55377, val loss: 1.54554\n",
      "Main effects training epoch: 206, train loss: 1.55982, val loss: 1.53282\n",
      "Main effects training epoch: 207, train loss: 1.56489, val loss: 1.53622\n",
      "Main effects training epoch: 208, train loss: 1.56584, val loss: 1.56922\n",
      "Main effects training epoch: 209, train loss: 1.57152, val loss: 1.54016\n",
      "Main effects training epoch: 210, train loss: 1.56175, val loss: 1.56414\n",
      "Main effects training epoch: 211, train loss: 1.55249, val loss: 1.53276\n",
      "Main effects training epoch: 212, train loss: 1.57704, val loss: 1.57135\n",
      "Main effects training epoch: 213, train loss: 1.56361, val loss: 1.54749\n",
      "Main effects training epoch: 214, train loss: 1.55555, val loss: 1.54328\n",
      "Main effects training epoch: 215, train loss: 1.56441, val loss: 1.54633\n",
      "Main effects training epoch: 216, train loss: 1.55334, val loss: 1.54499\n",
      "Main effects training epoch: 217, train loss: 1.55412, val loss: 1.53989\n",
      "Main effects training epoch: 218, train loss: 1.55515, val loss: 1.54670\n",
      "Main effects training epoch: 219, train loss: 1.55351, val loss: 1.54116\n",
      "Main effects training epoch: 220, train loss: 1.54775, val loss: 1.53053\n",
      "Main effects training epoch: 221, train loss: 1.55035, val loss: 1.53750\n",
      "Main effects training epoch: 222, train loss: 1.55452, val loss: 1.55095\n",
      "Main effects training epoch: 223, train loss: 1.54943, val loss: 1.52755\n",
      "Main effects training epoch: 224, train loss: 1.55720, val loss: 1.54448\n",
      "Main effects training epoch: 225, train loss: 1.56014, val loss: 1.55598\n",
      "Main effects training epoch: 226, train loss: 1.56343, val loss: 1.53233\n",
      "Main effects training epoch: 227, train loss: 1.55587, val loss: 1.55030\n",
      "Main effects training epoch: 228, train loss: 1.55747, val loss: 1.55708\n",
      "Main effects training epoch: 229, train loss: 1.55433, val loss: 1.52117\n",
      "Main effects training epoch: 230, train loss: 1.55739, val loss: 1.56235\n",
      "Main effects training epoch: 231, train loss: 1.55518, val loss: 1.52951\n",
      "Main effects training epoch: 232, train loss: 1.54796, val loss: 1.54615\n",
      "Main effects training epoch: 233, train loss: 1.54669, val loss: 1.54354\n",
      "Main effects training epoch: 234, train loss: 1.54453, val loss: 1.51876\n",
      "Main effects training epoch: 235, train loss: 1.54236, val loss: 1.53464\n",
      "Main effects training epoch: 236, train loss: 1.54506, val loss: 1.53001\n",
      "Main effects training epoch: 237, train loss: 1.54084, val loss: 1.52578\n",
      "Main effects training epoch: 238, train loss: 1.54366, val loss: 1.53055\n",
      "Main effects training epoch: 239, train loss: 1.53805, val loss: 1.52206\n",
      "Main effects training epoch: 240, train loss: 1.54082, val loss: 1.52826\n",
      "Main effects training epoch: 241, train loss: 1.53572, val loss: 1.51819\n",
      "Main effects training epoch: 242, train loss: 1.53754, val loss: 1.52794\n",
      "Main effects training epoch: 243, train loss: 1.53604, val loss: 1.53022\n",
      "Main effects training epoch: 244, train loss: 1.53456, val loss: 1.52713\n",
      "Main effects training epoch: 245, train loss: 1.55148, val loss: 1.54659\n",
      "Main effects training epoch: 246, train loss: 1.55179, val loss: 1.54236\n",
      "Main effects training epoch: 247, train loss: 1.53435, val loss: 1.51600\n",
      "Main effects training epoch: 248, train loss: 1.53376, val loss: 1.51308\n",
      "Main effects training epoch: 249, train loss: 1.53092, val loss: 1.53767\n",
      "Main effects training epoch: 250, train loss: 1.54054, val loss: 1.51216\n",
      "Main effects training epoch: 251, train loss: 1.52801, val loss: 1.53285\n",
      "Main effects training epoch: 252, train loss: 1.52931, val loss: 1.52258\n",
      "Main effects training epoch: 253, train loss: 1.54492, val loss: 1.52972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 254, train loss: 1.52470, val loss: 1.50903\n",
      "Main effects training epoch: 255, train loss: 1.52218, val loss: 1.51488\n",
      "Main effects training epoch: 256, train loss: 1.52990, val loss: 1.51890\n",
      "Main effects training epoch: 257, train loss: 1.52550, val loss: 1.52931\n",
      "Main effects training epoch: 258, train loss: 1.52752, val loss: 1.51996\n",
      "Main effects training epoch: 259, train loss: 1.53208, val loss: 1.52644\n",
      "Main effects training epoch: 260, train loss: 1.52008, val loss: 1.51478\n",
      "Main effects training epoch: 261, train loss: 1.52521, val loss: 1.49710\n",
      "Main effects training epoch: 262, train loss: 1.51657, val loss: 1.50802\n",
      "Main effects training epoch: 263, train loss: 1.52436, val loss: 1.51226\n",
      "Main effects training epoch: 264, train loss: 1.52030, val loss: 1.51078\n",
      "Main effects training epoch: 265, train loss: 1.51571, val loss: 1.51284\n",
      "Main effects training epoch: 266, train loss: 1.51608, val loss: 1.50220\n",
      "Main effects training epoch: 267, train loss: 1.51972, val loss: 1.51637\n",
      "Main effects training epoch: 268, train loss: 1.51501, val loss: 1.50468\n",
      "Main effects training epoch: 269, train loss: 1.51146, val loss: 1.50246\n",
      "Main effects training epoch: 270, train loss: 1.51761, val loss: 1.51946\n",
      "Main effects training epoch: 271, train loss: 1.52180, val loss: 1.50980\n",
      "Main effects training epoch: 272, train loss: 1.50940, val loss: 1.48736\n",
      "Main effects training epoch: 273, train loss: 1.50898, val loss: 1.51206\n",
      "Main effects training epoch: 274, train loss: 1.51927, val loss: 1.50631\n",
      "Main effects training epoch: 275, train loss: 1.52166, val loss: 1.52586\n",
      "Main effects training epoch: 276, train loss: 1.51000, val loss: 1.49341\n",
      "Main effects training epoch: 277, train loss: 1.50806, val loss: 1.50211\n",
      "Main effects training epoch: 278, train loss: 1.50899, val loss: 1.50494\n",
      "Main effects training epoch: 279, train loss: 1.50842, val loss: 1.49881\n",
      "Main effects training epoch: 280, train loss: 1.51115, val loss: 1.50731\n",
      "Main effects training epoch: 281, train loss: 1.51860, val loss: 1.51458\n",
      "Main effects training epoch: 282, train loss: 1.50875, val loss: 1.50216\n",
      "Main effects training epoch: 283, train loss: 1.51664, val loss: 1.51455\n",
      "Main effects training epoch: 284, train loss: 1.51948, val loss: 1.50433\n",
      "Main effects training epoch: 285, train loss: 1.52929, val loss: 1.51252\n",
      "Main effects training epoch: 286, train loss: 1.53375, val loss: 1.53879\n",
      "Main effects training epoch: 287, train loss: 1.50750, val loss: 1.49880\n",
      "Main effects training epoch: 288, train loss: 1.50873, val loss: 1.50691\n",
      "Main effects training epoch: 289, train loss: 1.51643, val loss: 1.50506\n",
      "Main effects training epoch: 290, train loss: 1.51712, val loss: 1.49842\n",
      "Main effects training epoch: 291, train loss: 1.50490, val loss: 1.50579\n",
      "Main effects training epoch: 292, train loss: 1.50937, val loss: 1.51035\n",
      "Main effects training epoch: 293, train loss: 1.51472, val loss: 1.48836\n",
      "Main effects training epoch: 294, train loss: 1.50984, val loss: 1.51100\n",
      "Main effects training epoch: 295, train loss: 1.50741, val loss: 1.48871\n",
      "Main effects training epoch: 296, train loss: 1.51113, val loss: 1.51519\n",
      "Main effects training epoch: 297, train loss: 1.50793, val loss: 1.49115\n",
      "Main effects training epoch: 298, train loss: 1.50425, val loss: 1.51459\n",
      "Main effects training epoch: 299, train loss: 1.50506, val loss: 1.49452\n",
      "Main effects training epoch: 300, train loss: 1.50817, val loss: 1.49762\n",
      "##########Stage 1: main effect training stop.##########\n",
      "4 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.54960, val loss: 1.48615\n",
      "Main effects tuning epoch: 2, train loss: 1.55096, val loss: 1.50088\n",
      "Main effects tuning epoch: 3, train loss: 1.54918, val loss: 1.51007\n",
      "Main effects tuning epoch: 4, train loss: 1.55754, val loss: 1.50672\n",
      "Main effects tuning epoch: 5, train loss: 1.55703, val loss: 1.49695\n",
      "Main effects tuning epoch: 6, train loss: 1.55386, val loss: 1.50811\n",
      "Main effects tuning epoch: 7, train loss: 1.55213, val loss: 1.51090\n",
      "Main effects tuning epoch: 8, train loss: 1.55114, val loss: 1.49406\n",
      "Main effects tuning epoch: 9, train loss: 1.54940, val loss: 1.50433\n",
      "Main effects tuning epoch: 10, train loss: 1.55124, val loss: 1.51332\n",
      "Main effects tuning epoch: 11, train loss: 1.55487, val loss: 1.50028\n",
      "Main effects tuning epoch: 12, train loss: 1.54857, val loss: 1.52270\n",
      "Main effects tuning epoch: 13, train loss: 1.54962, val loss: 1.49129\n",
      "Main effects tuning epoch: 14, train loss: 1.54468, val loss: 1.50953\n",
      "Main effects tuning epoch: 15, train loss: 1.55351, val loss: 1.51280\n",
      "Main effects tuning epoch: 16, train loss: 1.54271, val loss: 1.49697\n",
      "Main effects tuning epoch: 17, train loss: 1.55096, val loss: 1.49643\n",
      "Main effects tuning epoch: 18, train loss: 1.55380, val loss: 1.49601\n",
      "Main effects tuning epoch: 19, train loss: 1.54178, val loss: 1.49735\n",
      "Main effects tuning epoch: 20, train loss: 1.54321, val loss: 1.50220\n",
      "Main effects tuning epoch: 21, train loss: 1.54141, val loss: 1.50117\n",
      "Main effects tuning epoch: 22, train loss: 1.55004, val loss: 1.49036\n",
      "Main effects tuning epoch: 23, train loss: 1.54161, val loss: 1.49042\n",
      "Main effects tuning epoch: 24, train loss: 1.55168, val loss: 1.51238\n",
      "Main effects tuning epoch: 25, train loss: 1.54281, val loss: 1.49590\n",
      "Main effects tuning epoch: 26, train loss: 1.54950, val loss: 1.49675\n",
      "Main effects tuning epoch: 27, train loss: 1.54194, val loss: 1.49507\n",
      "Main effects tuning epoch: 28, train loss: 1.54412, val loss: 1.50409\n",
      "Main effects tuning epoch: 29, train loss: 1.54153, val loss: 1.49464\n",
      "Main effects tuning epoch: 30, train loss: 1.54306, val loss: 1.49519\n",
      "Main effects tuning epoch: 31, train loss: 1.54634, val loss: 1.51330\n",
      "Main effects tuning epoch: 32, train loss: 1.53811, val loss: 1.48742\n",
      "Main effects tuning epoch: 33, train loss: 1.53771, val loss: 1.50170\n",
      "Main effects tuning epoch: 34, train loss: 1.54941, val loss: 1.49075\n",
      "Main effects tuning epoch: 35, train loss: 1.53992, val loss: 1.49327\n",
      "Main effects tuning epoch: 36, train loss: 1.55232, val loss: 1.51730\n",
      "Main effects tuning epoch: 37, train loss: 1.53817, val loss: 1.49318\n",
      "Main effects tuning epoch: 38, train loss: 1.53650, val loss: 1.48829\n",
      "Main effects tuning epoch: 39, train loss: 1.53611, val loss: 1.49468\n",
      "Main effects tuning epoch: 40, train loss: 1.53853, val loss: 1.49104\n",
      "Main effects tuning epoch: 41, train loss: 1.54458, val loss: 1.49971\n",
      "Main effects tuning epoch: 42, train loss: 1.54176, val loss: 1.48615\n",
      "Main effects tuning epoch: 43, train loss: 1.54086, val loss: 1.50161\n",
      "Main effects tuning epoch: 44, train loss: 1.53832, val loss: 1.48027\n",
      "Main effects tuning epoch: 45, train loss: 1.53863, val loss: 1.49629\n",
      "Main effects tuning epoch: 46, train loss: 1.53771, val loss: 1.49186\n",
      "Main effects tuning epoch: 47, train loss: 1.55377, val loss: 1.50762\n",
      "Main effects tuning epoch: 48, train loss: 1.54224, val loss: 1.48777\n",
      "Main effects tuning epoch: 49, train loss: 1.53991, val loss: 1.48900\n",
      "Main effects tuning epoch: 50, train loss: 1.53415, val loss: 1.48893\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.50300, val loss: 1.46636\n",
      "Interaction training epoch: 2, train loss: 1.98298, val loss: 1.96205\n",
      "Interaction training epoch: 3, train loss: 1.10640, val loss: 1.07391\n",
      "Interaction training epoch: 4, train loss: 1.12826, val loss: 1.11375\n",
      "Interaction training epoch: 5, train loss: 0.99531, val loss: 0.99541\n",
      "Interaction training epoch: 6, train loss: 1.01851, val loss: 1.02100\n",
      "Interaction training epoch: 7, train loss: 0.96429, val loss: 0.96994\n",
      "Interaction training epoch: 8, train loss: 0.96704, val loss: 0.97530\n",
      "Interaction training epoch: 9, train loss: 0.97107, val loss: 0.97649\n",
      "Interaction training epoch: 10, train loss: 0.96740, val loss: 0.98442\n",
      "Interaction training epoch: 11, train loss: 0.96406, val loss: 0.99903\n",
      "Interaction training epoch: 12, train loss: 0.95352, val loss: 0.96594\n",
      "Interaction training epoch: 13, train loss: 0.95175, val loss: 0.96694\n",
      "Interaction training epoch: 14, train loss: 0.92298, val loss: 0.91711\n",
      "Interaction training epoch: 15, train loss: 0.92712, val loss: 0.92360\n",
      "Interaction training epoch: 16, train loss: 0.91504, val loss: 0.92330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 17, train loss: 0.91612, val loss: 0.91561\n",
      "Interaction training epoch: 18, train loss: 0.91686, val loss: 0.92031\n",
      "Interaction training epoch: 19, train loss: 0.93247, val loss: 0.91048\n",
      "Interaction training epoch: 20, train loss: 0.93007, val loss: 0.90682\n",
      "Interaction training epoch: 21, train loss: 0.90731, val loss: 0.91628\n",
      "Interaction training epoch: 22, train loss: 0.97570, val loss: 0.98547\n",
      "Interaction training epoch: 23, train loss: 0.89123, val loss: 0.88724\n",
      "Interaction training epoch: 24, train loss: 0.92186, val loss: 0.91618\n",
      "Interaction training epoch: 25, train loss: 0.89965, val loss: 0.88558\n",
      "Interaction training epoch: 26, train loss: 0.90680, val loss: 0.89431\n",
      "Interaction training epoch: 27, train loss: 0.89809, val loss: 0.88465\n",
      "Interaction training epoch: 28, train loss: 0.90357, val loss: 0.88167\n",
      "Interaction training epoch: 29, train loss: 0.88837, val loss: 0.88997\n",
      "Interaction training epoch: 30, train loss: 0.92647, val loss: 0.90858\n",
      "Interaction training epoch: 31, train loss: 0.89775, val loss: 0.91848\n",
      "Interaction training epoch: 32, train loss: 0.87649, val loss: 0.87387\n",
      "Interaction training epoch: 33, train loss: 0.90596, val loss: 0.88304\n",
      "Interaction training epoch: 34, train loss: 0.87684, val loss: 0.84953\n",
      "Interaction training epoch: 35, train loss: 0.88311, val loss: 0.88016\n",
      "Interaction training epoch: 36, train loss: 0.87114, val loss: 0.85459\n",
      "Interaction training epoch: 37, train loss: 0.87559, val loss: 0.86565\n",
      "Interaction training epoch: 38, train loss: 0.89355, val loss: 0.89822\n",
      "Interaction training epoch: 39, train loss: 0.85122, val loss: 0.84798\n",
      "Interaction training epoch: 40, train loss: 0.86859, val loss: 0.87647\n",
      "Interaction training epoch: 41, train loss: 0.89243, val loss: 0.88632\n",
      "Interaction training epoch: 42, train loss: 0.89746, val loss: 0.86720\n",
      "Interaction training epoch: 43, train loss: 0.87564, val loss: 0.87086\n",
      "Interaction training epoch: 44, train loss: 0.86708, val loss: 0.86468\n",
      "Interaction training epoch: 45, train loss: 0.87308, val loss: 0.85955\n",
      "Interaction training epoch: 46, train loss: 0.84913, val loss: 0.84583\n",
      "Interaction training epoch: 47, train loss: 0.85226, val loss: 0.85624\n",
      "Interaction training epoch: 48, train loss: 0.86052, val loss: 0.85766\n",
      "Interaction training epoch: 49, train loss: 0.84531, val loss: 0.84882\n",
      "Interaction training epoch: 50, train loss: 0.84673, val loss: 0.83435\n",
      "Interaction training epoch: 51, train loss: 0.85187, val loss: 0.84461\n",
      "Interaction training epoch: 52, train loss: 0.84598, val loss: 0.84876\n",
      "Interaction training epoch: 53, train loss: 0.83651, val loss: 0.83178\n",
      "Interaction training epoch: 54, train loss: 0.85355, val loss: 0.87172\n",
      "Interaction training epoch: 55, train loss: 0.85184, val loss: 0.85014\n",
      "Interaction training epoch: 56, train loss: 0.82788, val loss: 0.81876\n",
      "Interaction training epoch: 57, train loss: 0.84993, val loss: 0.84133\n",
      "Interaction training epoch: 58, train loss: 0.84069, val loss: 0.83600\n",
      "Interaction training epoch: 59, train loss: 0.83090, val loss: 0.85003\n",
      "Interaction training epoch: 60, train loss: 0.83659, val loss: 0.83370\n",
      "Interaction training epoch: 61, train loss: 0.85014, val loss: 0.84420\n",
      "Interaction training epoch: 62, train loss: 0.83523, val loss: 0.83437\n",
      "Interaction training epoch: 63, train loss: 0.83069, val loss: 0.82954\n",
      "Interaction training epoch: 64, train loss: 0.84099, val loss: 0.83793\n",
      "Interaction training epoch: 65, train loss: 0.85011, val loss: 0.84056\n",
      "Interaction training epoch: 66, train loss: 0.83980, val loss: 0.84510\n",
      "Interaction training epoch: 67, train loss: 0.84038, val loss: 0.84875\n",
      "Interaction training epoch: 68, train loss: 0.83491, val loss: 0.81603\n",
      "Interaction training epoch: 69, train loss: 0.83617, val loss: 0.83030\n",
      "Interaction training epoch: 70, train loss: 0.82252, val loss: 0.81009\n",
      "Interaction training epoch: 71, train loss: 0.82483, val loss: 0.82685\n",
      "Interaction training epoch: 72, train loss: 0.83353, val loss: 0.83123\n",
      "Interaction training epoch: 73, train loss: 0.82274, val loss: 0.82495\n",
      "Interaction training epoch: 74, train loss: 0.82216, val loss: 0.82524\n",
      "Interaction training epoch: 75, train loss: 0.82332, val loss: 0.81921\n",
      "Interaction training epoch: 76, train loss: 0.83069, val loss: 0.82572\n",
      "Interaction training epoch: 77, train loss: 0.82688, val loss: 0.81635\n",
      "Interaction training epoch: 78, train loss: 0.82473, val loss: 0.81675\n",
      "Interaction training epoch: 79, train loss: 0.82014, val loss: 0.81819\n",
      "Interaction training epoch: 80, train loss: 0.82549, val loss: 0.82215\n",
      "Interaction training epoch: 81, train loss: 0.82335, val loss: 0.81984\n",
      "Interaction training epoch: 82, train loss: 0.83812, val loss: 0.84778\n",
      "Interaction training epoch: 83, train loss: 0.82567, val loss: 0.82831\n",
      "Interaction training epoch: 84, train loss: 0.82478, val loss: 0.82505\n",
      "Interaction training epoch: 85, train loss: 0.82334, val loss: 0.81659\n",
      "Interaction training epoch: 86, train loss: 0.82643, val loss: 0.82701\n",
      "Interaction training epoch: 87, train loss: 0.81962, val loss: 0.80785\n",
      "Interaction training epoch: 88, train loss: 0.82191, val loss: 0.82314\n",
      "Interaction training epoch: 89, train loss: 0.81735, val loss: 0.82370\n",
      "Interaction training epoch: 90, train loss: 0.81508, val loss: 0.80141\n",
      "Interaction training epoch: 91, train loss: 0.81188, val loss: 0.80991\n",
      "Interaction training epoch: 92, train loss: 0.81977, val loss: 0.82266\n",
      "Interaction training epoch: 93, train loss: 0.82718, val loss: 0.82240\n",
      "Interaction training epoch: 94, train loss: 0.82661, val loss: 0.81321\n",
      "Interaction training epoch: 95, train loss: 0.81214, val loss: 0.81696\n",
      "Interaction training epoch: 96, train loss: 0.82335, val loss: 0.82698\n",
      "Interaction training epoch: 97, train loss: 0.83580, val loss: 0.81568\n",
      "Interaction training epoch: 98, train loss: 0.81878, val loss: 0.81617\n",
      "Interaction training epoch: 99, train loss: 0.82223, val loss: 0.82136\n",
      "Interaction training epoch: 100, train loss: 0.81607, val loss: 0.81933\n",
      "Interaction training epoch: 101, train loss: 0.81045, val loss: 0.80311\n",
      "Interaction training epoch: 102, train loss: 0.81910, val loss: 0.81196\n",
      "Interaction training epoch: 103, train loss: 0.81492, val loss: 0.82080\n",
      "Interaction training epoch: 104, train loss: 0.81560, val loss: 0.81255\n",
      "Interaction training epoch: 105, train loss: 0.81602, val loss: 0.80742\n",
      "Interaction training epoch: 106, train loss: 0.80749, val loss: 0.80726\n",
      "Interaction training epoch: 107, train loss: 0.80470, val loss: 0.81275\n",
      "Interaction training epoch: 108, train loss: 0.81093, val loss: 0.79883\n",
      "Interaction training epoch: 109, train loss: 0.81385, val loss: 0.80644\n",
      "Interaction training epoch: 110, train loss: 0.80554, val loss: 0.81468\n",
      "Interaction training epoch: 111, train loss: 0.82526, val loss: 0.83393\n",
      "Interaction training epoch: 112, train loss: 0.82158, val loss: 0.80160\n",
      "Interaction training epoch: 113, train loss: 0.80872, val loss: 0.81828\n",
      "Interaction training epoch: 114, train loss: 0.81722, val loss: 0.82083\n",
      "Interaction training epoch: 115, train loss: 0.80783, val loss: 0.80899\n",
      "Interaction training epoch: 116, train loss: 0.80657, val loss: 0.81246\n",
      "Interaction training epoch: 117, train loss: 0.80965, val loss: 0.80914\n",
      "Interaction training epoch: 118, train loss: 0.82091, val loss: 0.80501\n",
      "Interaction training epoch: 119, train loss: 0.80720, val loss: 0.81347\n",
      "Interaction training epoch: 120, train loss: 0.80805, val loss: 0.81670\n",
      "Interaction training epoch: 121, train loss: 0.80856, val loss: 0.80265\n",
      "Interaction training epoch: 122, train loss: 0.80578, val loss: 0.80732\n",
      "Interaction training epoch: 123, train loss: 0.81223, val loss: 0.81320\n",
      "Interaction training epoch: 124, train loss: 0.80120, val loss: 0.79909\n",
      "Interaction training epoch: 125, train loss: 0.80659, val loss: 0.80152\n",
      "Interaction training epoch: 126, train loss: 0.81043, val loss: 0.81074\n",
      "Interaction training epoch: 127, train loss: 0.80831, val loss: 0.81943\n",
      "Interaction training epoch: 128, train loss: 0.80822, val loss: 0.81346\n",
      "Interaction training epoch: 129, train loss: 0.84383, val loss: 0.83057\n",
      "Interaction training epoch: 130, train loss: 0.81879, val loss: 0.84084\n",
      "Interaction training epoch: 131, train loss: 0.81194, val loss: 0.80244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 132, train loss: 0.81890, val loss: 0.81283\n",
      "Interaction training epoch: 133, train loss: 0.81035, val loss: 0.81357\n",
      "Interaction training epoch: 134, train loss: 0.81110, val loss: 0.82411\n",
      "Interaction training epoch: 135, train loss: 0.80075, val loss: 0.79736\n",
      "Interaction training epoch: 136, train loss: 0.80986, val loss: 0.81610\n",
      "Interaction training epoch: 137, train loss: 0.80896, val loss: 0.80957\n",
      "Interaction training epoch: 138, train loss: 0.81193, val loss: 0.82008\n",
      "Interaction training epoch: 139, train loss: 0.80362, val loss: 0.79398\n",
      "Interaction training epoch: 140, train loss: 0.80529, val loss: 0.80662\n",
      "Interaction training epoch: 141, train loss: 0.80128, val loss: 0.80082\n",
      "Interaction training epoch: 142, train loss: 0.80063, val loss: 0.79646\n",
      "Interaction training epoch: 143, train loss: 0.79664, val loss: 0.80153\n",
      "Interaction training epoch: 144, train loss: 0.79712, val loss: 0.80035\n",
      "Interaction training epoch: 145, train loss: 0.81568, val loss: 0.82627\n",
      "Interaction training epoch: 146, train loss: 0.80391, val loss: 0.81027\n",
      "Interaction training epoch: 147, train loss: 0.80176, val loss: 0.80351\n",
      "Interaction training epoch: 148, train loss: 0.81148, val loss: 0.82319\n",
      "Interaction training epoch: 149, train loss: 0.80099, val loss: 0.79900\n",
      "Interaction training epoch: 150, train loss: 0.80499, val loss: 0.80281\n",
      "Interaction training epoch: 151, train loss: 0.79349, val loss: 0.80218\n",
      "Interaction training epoch: 152, train loss: 0.80050, val loss: 0.80462\n",
      "Interaction training epoch: 153, train loss: 0.80286, val loss: 0.80320\n",
      "Interaction training epoch: 154, train loss: 0.79481, val loss: 0.80446\n",
      "Interaction training epoch: 155, train loss: 0.79334, val loss: 0.79643\n",
      "Interaction training epoch: 156, train loss: 0.80441, val loss: 0.81319\n",
      "Interaction training epoch: 157, train loss: 0.79868, val loss: 0.80766\n",
      "Interaction training epoch: 158, train loss: 0.79688, val loss: 0.80375\n",
      "Interaction training epoch: 159, train loss: 0.79299, val loss: 0.79432\n",
      "Interaction training epoch: 160, train loss: 0.79935, val loss: 0.81762\n",
      "Interaction training epoch: 161, train loss: 0.79816, val loss: 0.80640\n",
      "Interaction training epoch: 162, train loss: 0.79551, val loss: 0.80000\n",
      "Interaction training epoch: 163, train loss: 0.80506, val loss: 0.81585\n",
      "Interaction training epoch: 164, train loss: 0.79603, val loss: 0.79512\n",
      "Interaction training epoch: 165, train loss: 0.79694, val loss: 0.80761\n",
      "Interaction training epoch: 166, train loss: 0.79853, val loss: 0.79633\n",
      "Interaction training epoch: 167, train loss: 0.79499, val loss: 0.80243\n",
      "Interaction training epoch: 168, train loss: 0.79527, val loss: 0.81211\n",
      "Interaction training epoch: 169, train loss: 0.79719, val loss: 0.79095\n",
      "Interaction training epoch: 170, train loss: 0.79618, val loss: 0.79023\n",
      "Interaction training epoch: 171, train loss: 0.79827, val loss: 0.80225\n",
      "Interaction training epoch: 172, train loss: 0.79193, val loss: 0.80790\n",
      "Interaction training epoch: 173, train loss: 0.79959, val loss: 0.80456\n",
      "Interaction training epoch: 174, train loss: 0.79232, val loss: 0.80435\n",
      "Interaction training epoch: 175, train loss: 0.79926, val loss: 0.80222\n",
      "Interaction training epoch: 176, train loss: 0.80323, val loss: 0.80622\n",
      "Interaction training epoch: 177, train loss: 0.79582, val loss: 0.80745\n",
      "Interaction training epoch: 178, train loss: 0.80514, val loss: 0.80088\n",
      "Interaction training epoch: 179, train loss: 0.80297, val loss: 0.81976\n",
      "Interaction training epoch: 180, train loss: 0.80113, val loss: 0.80323\n",
      "Interaction training epoch: 181, train loss: 0.79739, val loss: 0.80007\n",
      "Interaction training epoch: 182, train loss: 0.79178, val loss: 0.80241\n",
      "Interaction training epoch: 183, train loss: 0.79798, val loss: 0.80384\n",
      "Interaction training epoch: 184, train loss: 0.80385, val loss: 0.81029\n",
      "Interaction training epoch: 185, train loss: 0.79006, val loss: 0.79485\n",
      "Interaction training epoch: 186, train loss: 0.79209, val loss: 0.79923\n",
      "Interaction training epoch: 187, train loss: 0.79466, val loss: 0.80236\n",
      "Interaction training epoch: 188, train loss: 0.79183, val loss: 0.79190\n",
      "Interaction training epoch: 189, train loss: 0.79392, val loss: 0.80153\n",
      "Interaction training epoch: 190, train loss: 0.78907, val loss: 0.79515\n",
      "Interaction training epoch: 191, train loss: 0.78584, val loss: 0.79930\n",
      "Interaction training epoch: 192, train loss: 0.78798, val loss: 0.78823\n",
      "Interaction training epoch: 193, train loss: 0.79118, val loss: 0.80524\n",
      "Interaction training epoch: 194, train loss: 0.79883, val loss: 0.80325\n",
      "Interaction training epoch: 195, train loss: 0.79424, val loss: 0.79748\n",
      "Interaction training epoch: 196, train loss: 0.79125, val loss: 0.79000\n",
      "Interaction training epoch: 197, train loss: 0.80028, val loss: 0.80331\n",
      "Interaction training epoch: 198, train loss: 0.79685, val loss: 0.80967\n",
      "Interaction training epoch: 199, train loss: 0.78787, val loss: 0.79527\n",
      "Interaction training epoch: 200, train loss: 0.80193, val loss: 0.80962\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########3 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.80566, val loss: 0.79724\n",
      "Interaction tuning epoch: 2, train loss: 0.80705, val loss: 0.81016\n",
      "Interaction tuning epoch: 3, train loss: 0.82141, val loss: 0.83536\n",
      "Interaction tuning epoch: 4, train loss: 0.79770, val loss: 0.79951\n",
      "Interaction tuning epoch: 5, train loss: 0.80783, val loss: 0.81281\n",
      "Interaction tuning epoch: 6, train loss: 0.80742, val loss: 0.80656\n",
      "Interaction tuning epoch: 7, train loss: 0.79344, val loss: 0.79044\n",
      "Interaction tuning epoch: 8, train loss: 0.80206, val loss: 0.79647\n",
      "Interaction tuning epoch: 9, train loss: 0.79806, val loss: 0.80205\n",
      "Interaction tuning epoch: 10, train loss: 0.80599, val loss: 0.81231\n",
      "Interaction tuning epoch: 11, train loss: 0.79228, val loss: 0.79392\n",
      "Interaction tuning epoch: 12, train loss: 0.79758, val loss: 0.80238\n",
      "Interaction tuning epoch: 13, train loss: 0.79724, val loss: 0.80168\n",
      "Interaction tuning epoch: 14, train loss: 0.79424, val loss: 0.80222\n",
      "Interaction tuning epoch: 15, train loss: 0.79757, val loss: 0.79889\n",
      "Interaction tuning epoch: 16, train loss: 0.80166, val loss: 0.80783\n",
      "Interaction tuning epoch: 17, train loss: 0.80554, val loss: 0.80632\n",
      "Interaction tuning epoch: 18, train loss: 0.79674, val loss: 0.79565\n",
      "Interaction tuning epoch: 19, train loss: 0.79448, val loss: 0.79143\n",
      "Interaction tuning epoch: 20, train loss: 0.78910, val loss: 0.79428\n",
      "Interaction tuning epoch: 21, train loss: 0.79598, val loss: 0.80381\n",
      "Interaction tuning epoch: 22, train loss: 0.80382, val loss: 0.80007\n",
      "Interaction tuning epoch: 23, train loss: 0.79980, val loss: 0.79858\n",
      "Interaction tuning epoch: 24, train loss: 0.80334, val loss: 0.80314\n",
      "Interaction tuning epoch: 25, train loss: 0.79936, val loss: 0.80466\n",
      "Interaction tuning epoch: 26, train loss: 0.79589, val loss: 0.81033\n",
      "Interaction tuning epoch: 27, train loss: 0.78894, val loss: 0.78865\n",
      "Interaction tuning epoch: 28, train loss: 0.80214, val loss: 0.80731\n",
      "Interaction tuning epoch: 29, train loss: 0.79355, val loss: 0.79505\n",
      "Interaction tuning epoch: 30, train loss: 0.79644, val loss: 0.80528\n",
      "Interaction tuning epoch: 31, train loss: 0.79169, val loss: 0.79180\n",
      "Interaction tuning epoch: 32, train loss: 0.79619, val loss: 0.79972\n",
      "Interaction tuning epoch: 33, train loss: 0.79104, val loss: 0.79329\n",
      "Interaction tuning epoch: 34, train loss: 0.79320, val loss: 0.79852\n",
      "Interaction tuning epoch: 35, train loss: 0.79204, val loss: 0.79351\n",
      "Interaction tuning epoch: 36, train loss: 0.79903, val loss: 0.80141\n",
      "Interaction tuning epoch: 37, train loss: 0.78904, val loss: 0.79424\n",
      "Interaction tuning epoch: 38, train loss: 0.78971, val loss: 0.79133\n",
      "Interaction tuning epoch: 39, train loss: 0.79870, val loss: 0.79386\n",
      "Interaction tuning epoch: 40, train loss: 0.78967, val loss: 0.78793\n",
      "Interaction tuning epoch: 41, train loss: 0.78853, val loss: 0.79526\n",
      "Interaction tuning epoch: 42, train loss: 0.78741, val loss: 0.78981\n",
      "Interaction tuning epoch: 43, train loss: 0.79499, val loss: 0.80615\n",
      "Interaction tuning epoch: 44, train loss: 0.78970, val loss: 0.78823\n",
      "Interaction tuning epoch: 45, train loss: 0.79424, val loss: 0.79442\n",
      "Interaction tuning epoch: 46, train loss: 0.79025, val loss: 0.78687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 47, train loss: 0.78789, val loss: 0.79451\n",
      "Interaction tuning epoch: 48, train loss: 0.79437, val loss: 0.79882\n",
      "Interaction tuning epoch: 49, train loss: 0.78887, val loss: 0.78376\n",
      "Interaction tuning epoch: 50, train loss: 0.78911, val loss: 0.79385\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 76.66828918457031\n",
      "After the gam stage, training error is 0.78911 , validation error is 0.79385\n",
      "missing value counts: 92883\n",
      "[SoftImpute] Max Singular Value of X_init = 19.468633\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed MAE=0.749429 validation MAE=0.788932,rank=3\n",
      "[SoftImpute] Iter 1: observed MAE=0.785148 validation MAE=0.790145,rank=3\n",
      "[SoftImpute] Iter 2: observed MAE=0.783864 validation MAE=0.788697,rank=3\n",
      "[SoftImpute] Iter 3: observed MAE=0.785087 validation MAE=0.789001,rank=3\n",
      "[SoftImpute] Iter 4: observed MAE=0.785035 validation MAE=0.789540,rank=3\n",
      "[SoftImpute] Iter 5: observed MAE=0.785648 validation MAE=0.789846,rank=3\n",
      "[SoftImpute] Iter 6: observed MAE=0.785742 validation MAE=0.789882,rank=3\n",
      "[SoftImpute] Iter 7: observed MAE=0.785573 validation MAE=0.789705,rank=3\n",
      "[SoftImpute] Iter 8: observed MAE=0.785513 validation MAE=0.789400,rank=3\n",
      "[SoftImpute] Iter 9: observed MAE=0.785350 validation MAE=0.789169,rank=3\n",
      "[SoftImpute] Iter 10: observed MAE=0.786017 validation MAE=0.790278,rank=3\n",
      "[SoftImpute] Iter 11: observed MAE=0.786125 validation MAE=0.790281,rank=3\n",
      "[SoftImpute] Iter 12: observed MAE=0.786566 validation MAE=0.790846,rank=3\n",
      "[SoftImpute] Iter 13: observed MAE=0.786702 validation MAE=0.790973,rank=3\n",
      "[SoftImpute] Iter 14: observed MAE=0.786850 validation MAE=0.791117,rank=3\n",
      "[SoftImpute] Iter 15: observed MAE=0.787011 validation MAE=0.791276,rank=3\n",
      "[SoftImpute] Iter 16: observed MAE=0.787187 validation MAE=0.791453,rank=3\n",
      "[SoftImpute] Iter 17: observed MAE=0.787375 validation MAE=0.791656,rank=3\n",
      "[SoftImpute] Iter 18: observed MAE=0.787571 validation MAE=0.791874,rank=3\n",
      "[SoftImpute] Iter 19: observed MAE=0.787764 validation MAE=0.792094,rank=3\n",
      "[SoftImpute] Iter 20: observed MAE=0.787948 validation MAE=0.792307,rank=3\n",
      "[SoftImpute] Iter 21: observed MAE=0.788115 validation MAE=0.792497,rank=3\n",
      "[SoftImpute] Iter 22: observed MAE=0.788254 validation MAE=0.792658,rank=3\n",
      "[SoftImpute] Iter 23: observed MAE=0.788362 validation MAE=0.792786,rank=3\n",
      "[SoftImpute] Iter 24: observed MAE=0.788443 validation MAE=0.792882,rank=3\n",
      "[SoftImpute] Iter 25: observed MAE=0.788500 validation MAE=0.792951,rank=3\n",
      "[SoftImpute] Iter 26: observed MAE=0.788539 validation MAE=0.793000,rank=3\n",
      "[SoftImpute] Iter 27: observed MAE=0.788566 validation MAE=0.793033,rank=3\n",
      "[SoftImpute] Iter 28: observed MAE=0.788583 validation MAE=0.793056,rank=3\n",
      "[SoftImpute] Iter 29: observed MAE=0.788595 validation MAE=0.793071,rank=3\n",
      "[SoftImpute] Iter 30: observed MAE=0.788602 validation MAE=0.793080,rank=3\n",
      "[SoftImpute] Iter 31: observed MAE=0.788607 validation MAE=0.793087,rank=3\n",
      "[SoftImpute] Iter 32: observed MAE=0.788611 validation MAE=0.793091,rank=3\n",
      "[SoftImpute] Iter 33: observed MAE=0.788613 validation MAE=0.793094,rank=3\n",
      "[SoftImpute] Iter 34: observed MAE=0.788614 validation MAE=0.793096,rank=3\n",
      "[SoftImpute] Iter 35: observed MAE=0.788615 validation MAE=0.793097,rank=3\n",
      "[SoftImpute] Iter 36: observed MAE=0.788616 validation MAE=0.793098,rank=3\n",
      "[SoftImpute] Stopped after iteration 36 for lambda=0.389373\n",
      "final num of user group: 10\n",
      "final num of item group: 18\n",
      "change mode state : True\n",
      "time cost: 5.2383270263671875\n",
      "After the matrix factor stage, training error is 0.78862, validation error is 0.79310\n",
      "2\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 4.00780, val loss: 3.99576\n",
      "Main effects training epoch: 2, train loss: 3.80269, val loss: 3.80276\n",
      "Main effects training epoch: 3, train loss: 3.60809, val loss: 3.62139\n",
      "Main effects training epoch: 4, train loss: 3.42625, val loss: 3.46432\n",
      "Main effects training epoch: 5, train loss: 3.33490, val loss: 3.39180\n",
      "Main effects training epoch: 6, train loss: 3.36558, val loss: 3.41659\n",
      "Main effects training epoch: 7, train loss: 3.33883, val loss: 3.37470\n",
      "Main effects training epoch: 8, train loss: 3.23579, val loss: 3.26507\n",
      "Main effects training epoch: 9, train loss: 3.18826, val loss: 3.21804\n",
      "Main effects training epoch: 10, train loss: 3.15775, val loss: 3.18927\n",
      "Main effects training epoch: 11, train loss: 3.10340, val loss: 3.13226\n",
      "Main effects training epoch: 12, train loss: 3.06054, val loss: 3.08763\n",
      "Main effects training epoch: 13, train loss: 3.00048, val loss: 3.03387\n",
      "Main effects training epoch: 14, train loss: 2.89737, val loss: 2.92432\n",
      "Main effects training epoch: 15, train loss: 2.82016, val loss: 2.84382\n",
      "Main effects training epoch: 16, train loss: 2.77997, val loss: 2.80106\n",
      "Main effects training epoch: 17, train loss: 2.74610, val loss: 2.76206\n",
      "Main effects training epoch: 18, train loss: 2.65991, val loss: 2.66842\n",
      "Main effects training epoch: 19, train loss: 2.58698, val loss: 2.59852\n",
      "Main effects training epoch: 20, train loss: 2.52903, val loss: 2.54219\n",
      "Main effects training epoch: 21, train loss: 2.52142, val loss: 2.52747\n",
      "Main effects training epoch: 22, train loss: 2.43216, val loss: 2.44164\n",
      "Main effects training epoch: 23, train loss: 2.41149, val loss: 2.42752\n",
      "Main effects training epoch: 24, train loss: 2.35504, val loss: 2.35877\n",
      "Main effects training epoch: 25, train loss: 2.34069, val loss: 2.34761\n",
      "Main effects training epoch: 26, train loss: 2.30942, val loss: 2.32172\n",
      "Main effects training epoch: 27, train loss: 2.28274, val loss: 2.28884\n",
      "Main effects training epoch: 28, train loss: 2.24703, val loss: 2.25550\n",
      "Main effects training epoch: 29, train loss: 2.25479, val loss: 2.26640\n",
      "Main effects training epoch: 30, train loss: 2.19539, val loss: 2.20208\n",
      "Main effects training epoch: 31, train loss: 2.17079, val loss: 2.17604\n",
      "Main effects training epoch: 32, train loss: 2.16131, val loss: 2.16962\n",
      "Main effects training epoch: 33, train loss: 2.10997, val loss: 2.11260\n",
      "Main effects training epoch: 34, train loss: 2.11321, val loss: 2.11890\n",
      "Main effects training epoch: 35, train loss: 2.09960, val loss: 2.10537\n",
      "Main effects training epoch: 36, train loss: 2.05095, val loss: 2.05329\n",
      "Main effects training epoch: 37, train loss: 2.06332, val loss: 2.07528\n",
      "Main effects training epoch: 38, train loss: 2.03248, val loss: 2.03246\n",
      "Main effects training epoch: 39, train loss: 2.00752, val loss: 2.01487\n",
      "Main effects training epoch: 40, train loss: 1.99962, val loss: 2.00270\n",
      "Main effects training epoch: 41, train loss: 1.96840, val loss: 1.97093\n",
      "Main effects training epoch: 42, train loss: 1.96160, val loss: 1.96723\n",
      "Main effects training epoch: 43, train loss: 1.95548, val loss: 1.95695\n",
      "Main effects training epoch: 44, train loss: 1.91815, val loss: 1.91876\n",
      "Main effects training epoch: 45, train loss: 1.92060, val loss: 1.92551\n",
      "Main effects training epoch: 46, train loss: 1.89976, val loss: 1.89751\n",
      "Main effects training epoch: 47, train loss: 1.87868, val loss: 1.88338\n",
      "Main effects training epoch: 48, train loss: 1.88476, val loss: 1.88732\n",
      "Main effects training epoch: 49, train loss: 1.85571, val loss: 1.85504\n",
      "Main effects training epoch: 50, train loss: 1.85987, val loss: 1.85153\n",
      "Main effects training epoch: 51, train loss: 1.83941, val loss: 1.84137\n",
      "Main effects training epoch: 52, train loss: 1.83060, val loss: 1.82488\n",
      "Main effects training epoch: 53, train loss: 1.81907, val loss: 1.81431\n",
      "Main effects training epoch: 54, train loss: 1.82049, val loss: 1.81354\n",
      "Main effects training epoch: 55, train loss: 1.80928, val loss: 1.80558\n",
      "Main effects training epoch: 56, train loss: 1.79225, val loss: 1.77900\n",
      "Main effects training epoch: 57, train loss: 1.79486, val loss: 1.78654\n",
      "Main effects training epoch: 58, train loss: 1.78454, val loss: 1.77152\n",
      "Main effects training epoch: 59, train loss: 1.77867, val loss: 1.76575\n",
      "Main effects training epoch: 60, train loss: 1.78587, val loss: 1.77357\n",
      "Main effects training epoch: 61, train loss: 1.77793, val loss: 1.76853\n",
      "Main effects training epoch: 62, train loss: 1.76195, val loss: 1.74491\n",
      "Main effects training epoch: 63, train loss: 1.76082, val loss: 1.74302\n",
      "Main effects training epoch: 64, train loss: 1.75563, val loss: 1.73889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 65, train loss: 1.76280, val loss: 1.74481\n",
      "Main effects training epoch: 66, train loss: 1.75289, val loss: 1.73550\n",
      "Main effects training epoch: 67, train loss: 1.75204, val loss: 1.73036\n",
      "Main effects training epoch: 68, train loss: 1.75216, val loss: 1.73852\n",
      "Main effects training epoch: 69, train loss: 1.74681, val loss: 1.72774\n",
      "Main effects training epoch: 70, train loss: 1.74038, val loss: 1.72486\n",
      "Main effects training epoch: 71, train loss: 1.74344, val loss: 1.71961\n",
      "Main effects training epoch: 72, train loss: 1.73123, val loss: 1.71218\n",
      "Main effects training epoch: 73, train loss: 1.73382, val loss: 1.71462\n",
      "Main effects training epoch: 74, train loss: 1.72889, val loss: 1.70695\n",
      "Main effects training epoch: 75, train loss: 1.72364, val loss: 1.69997\n",
      "Main effects training epoch: 76, train loss: 1.72953, val loss: 1.71102\n",
      "Main effects training epoch: 77, train loss: 1.71547, val loss: 1.69192\n",
      "Main effects training epoch: 78, train loss: 1.72249, val loss: 1.70268\n",
      "Main effects training epoch: 79, train loss: 1.71960, val loss: 1.70369\n",
      "Main effects training epoch: 80, train loss: 1.72253, val loss: 1.70149\n",
      "Main effects training epoch: 81, train loss: 1.71547, val loss: 1.70048\n",
      "Main effects training epoch: 82, train loss: 1.71560, val loss: 1.69452\n",
      "Main effects training epoch: 83, train loss: 1.70969, val loss: 1.69403\n",
      "Main effects training epoch: 84, train loss: 1.70994, val loss: 1.68483\n",
      "Main effects training epoch: 85, train loss: 1.70374, val loss: 1.68712\n",
      "Main effects training epoch: 86, train loss: 1.70833, val loss: 1.69111\n",
      "Main effects training epoch: 87, train loss: 1.70219, val loss: 1.68559\n",
      "Main effects training epoch: 88, train loss: 1.69871, val loss: 1.67988\n",
      "Main effects training epoch: 89, train loss: 1.69812, val loss: 1.68276\n",
      "Main effects training epoch: 90, train loss: 1.69560, val loss: 1.68099\n",
      "Main effects training epoch: 91, train loss: 1.69482, val loss: 1.67780\n",
      "Main effects training epoch: 92, train loss: 1.69510, val loss: 1.67837\n",
      "Main effects training epoch: 93, train loss: 1.69245, val loss: 1.67788\n",
      "Main effects training epoch: 94, train loss: 1.69871, val loss: 1.69175\n",
      "Main effects training epoch: 95, train loss: 1.68888, val loss: 1.66893\n",
      "Main effects training epoch: 96, train loss: 1.68943, val loss: 1.67363\n",
      "Main effects training epoch: 97, train loss: 1.68803, val loss: 1.68472\n",
      "Main effects training epoch: 98, train loss: 1.68267, val loss: 1.66611\n",
      "Main effects training epoch: 99, train loss: 1.67859, val loss: 1.67450\n",
      "Main effects training epoch: 100, train loss: 1.67445, val loss: 1.66120\n",
      "Main effects training epoch: 101, train loss: 1.67601, val loss: 1.67294\n",
      "Main effects training epoch: 102, train loss: 1.66986, val loss: 1.66728\n",
      "Main effects training epoch: 103, train loss: 1.67355, val loss: 1.66836\n",
      "Main effects training epoch: 104, train loss: 1.66057, val loss: 1.66482\n",
      "Main effects training epoch: 105, train loss: 1.65546, val loss: 1.65580\n",
      "Main effects training epoch: 106, train loss: 1.65067, val loss: 1.66738\n",
      "Main effects training epoch: 107, train loss: 1.64701, val loss: 1.65304\n",
      "Main effects training epoch: 108, train loss: 1.63866, val loss: 1.65781\n",
      "Main effects training epoch: 109, train loss: 1.63186, val loss: 1.64927\n",
      "Main effects training epoch: 110, train loss: 1.63971, val loss: 1.66398\n",
      "Main effects training epoch: 111, train loss: 1.63637, val loss: 1.65487\n",
      "Main effects training epoch: 112, train loss: 1.62983, val loss: 1.64273\n",
      "Main effects training epoch: 113, train loss: 1.62382, val loss: 1.63834\n",
      "Main effects training epoch: 114, train loss: 1.62500, val loss: 1.64156\n",
      "Main effects training epoch: 115, train loss: 1.62259, val loss: 1.62904\n",
      "Main effects training epoch: 116, train loss: 1.62144, val loss: 1.64200\n",
      "Main effects training epoch: 117, train loss: 1.62813, val loss: 1.65196\n",
      "Main effects training epoch: 118, train loss: 1.61798, val loss: 1.63604\n",
      "Main effects training epoch: 119, train loss: 1.61616, val loss: 1.63190\n",
      "Main effects training epoch: 120, train loss: 1.61408, val loss: 1.62739\n",
      "Main effects training epoch: 121, train loss: 1.61135, val loss: 1.62880\n",
      "Main effects training epoch: 122, train loss: 1.61650, val loss: 1.63731\n",
      "Main effects training epoch: 123, train loss: 1.61437, val loss: 1.62904\n",
      "Main effects training epoch: 124, train loss: 1.61213, val loss: 1.62737\n",
      "Main effects training epoch: 125, train loss: 1.61051, val loss: 1.62634\n",
      "Main effects training epoch: 126, train loss: 1.62488, val loss: 1.62209\n",
      "Main effects training epoch: 127, train loss: 1.63330, val loss: 1.65892\n",
      "Main effects training epoch: 128, train loss: 1.61237, val loss: 1.61913\n",
      "Main effects training epoch: 129, train loss: 1.61065, val loss: 1.61756\n",
      "Main effects training epoch: 130, train loss: 1.61353, val loss: 1.63453\n",
      "Main effects training epoch: 131, train loss: 1.60719, val loss: 1.61549\n",
      "Main effects training epoch: 132, train loss: 1.60785, val loss: 1.62481\n",
      "Main effects training epoch: 133, train loss: 1.61015, val loss: 1.62120\n",
      "Main effects training epoch: 134, train loss: 1.60522, val loss: 1.62113\n",
      "Main effects training epoch: 135, train loss: 1.60450, val loss: 1.62143\n",
      "Main effects training epoch: 136, train loss: 1.60357, val loss: 1.61258\n",
      "Main effects training epoch: 137, train loss: 1.60095, val loss: 1.61881\n",
      "Main effects training epoch: 138, train loss: 1.60042, val loss: 1.61260\n",
      "Main effects training epoch: 139, train loss: 1.60031, val loss: 1.62039\n",
      "Main effects training epoch: 140, train loss: 1.60390, val loss: 1.61594\n",
      "Main effects training epoch: 141, train loss: 1.60625, val loss: 1.61987\n",
      "Main effects training epoch: 142, train loss: 1.60204, val loss: 1.62155\n",
      "Main effects training epoch: 143, train loss: 1.59932, val loss: 1.61452\n",
      "Main effects training epoch: 144, train loss: 1.59957, val loss: 1.61799\n",
      "Main effects training epoch: 145, train loss: 1.60008, val loss: 1.62980\n",
      "Main effects training epoch: 146, train loss: 1.59889, val loss: 1.61172\n",
      "Main effects training epoch: 147, train loss: 1.59952, val loss: 1.62275\n",
      "Main effects training epoch: 148, train loss: 1.59497, val loss: 1.60507\n",
      "Main effects training epoch: 149, train loss: 1.59441, val loss: 1.61373\n",
      "Main effects training epoch: 150, train loss: 1.59274, val loss: 1.61077\n",
      "Main effects training epoch: 151, train loss: 1.58779, val loss: 1.60530\n",
      "Main effects training epoch: 152, train loss: 1.58966, val loss: 1.60031\n",
      "Main effects training epoch: 153, train loss: 1.59696, val loss: 1.62282\n",
      "Main effects training epoch: 154, train loss: 1.58573, val loss: 1.60317\n",
      "Main effects training epoch: 155, train loss: 1.59362, val loss: 1.61324\n",
      "Main effects training epoch: 156, train loss: 1.59266, val loss: 1.60872\n",
      "Main effects training epoch: 157, train loss: 1.59020, val loss: 1.60542\n",
      "Main effects training epoch: 158, train loss: 1.58567, val loss: 1.60182\n",
      "Main effects training epoch: 159, train loss: 1.58557, val loss: 1.60602\n",
      "Main effects training epoch: 160, train loss: 1.58423, val loss: 1.60256\n",
      "Main effects training epoch: 161, train loss: 1.58396, val loss: 1.59702\n",
      "Main effects training epoch: 162, train loss: 1.58695, val loss: 1.60894\n",
      "Main effects training epoch: 163, train loss: 1.58861, val loss: 1.60253\n",
      "Main effects training epoch: 164, train loss: 1.58587, val loss: 1.60476\n",
      "Main effects training epoch: 165, train loss: 1.58894, val loss: 1.60208\n",
      "Main effects training epoch: 166, train loss: 1.58730, val loss: 1.61231\n",
      "Main effects training epoch: 167, train loss: 1.58688, val loss: 1.59538\n",
      "Main effects training epoch: 168, train loss: 1.58099, val loss: 1.60200\n",
      "Main effects training epoch: 169, train loss: 1.58554, val loss: 1.59997\n",
      "Main effects training epoch: 170, train loss: 1.58348, val loss: 1.59547\n",
      "Main effects training epoch: 171, train loss: 1.58876, val loss: 1.60789\n",
      "Main effects training epoch: 172, train loss: 1.58826, val loss: 1.60268\n",
      "Main effects training epoch: 173, train loss: 1.58343, val loss: 1.59552\n",
      "Main effects training epoch: 174, train loss: 1.57947, val loss: 1.59767\n",
      "Main effects training epoch: 175, train loss: 1.58171, val loss: 1.59964\n",
      "Main effects training epoch: 176, train loss: 1.58249, val loss: 1.60350\n",
      "Main effects training epoch: 177, train loss: 1.57952, val loss: 1.60120\n",
      "Main effects training epoch: 178, train loss: 1.57706, val loss: 1.59520\n",
      "Main effects training epoch: 179, train loss: 1.57820, val loss: 1.59768\n",
      "Main effects training epoch: 180, train loss: 1.57688, val loss: 1.59292\n",
      "Main effects training epoch: 181, train loss: 1.57739, val loss: 1.59451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 182, train loss: 1.58123, val loss: 1.60092\n",
      "Main effects training epoch: 183, train loss: 1.58010, val loss: 1.59740\n",
      "Main effects training epoch: 184, train loss: 1.57761, val loss: 1.59894\n",
      "Main effects training epoch: 185, train loss: 1.57997, val loss: 1.59317\n",
      "Main effects training epoch: 186, train loss: 1.58088, val loss: 1.59644\n",
      "Main effects training epoch: 187, train loss: 1.57844, val loss: 1.60082\n",
      "Main effects training epoch: 188, train loss: 1.57553, val loss: 1.58986\n",
      "Main effects training epoch: 189, train loss: 1.57995, val loss: 1.60038\n",
      "Main effects training epoch: 190, train loss: 1.57518, val loss: 1.58750\n",
      "Main effects training epoch: 191, train loss: 1.57839, val loss: 1.60193\n",
      "Main effects training epoch: 192, train loss: 1.57505, val loss: 1.59138\n",
      "Main effects training epoch: 193, train loss: 1.57623, val loss: 1.58547\n",
      "Main effects training epoch: 194, train loss: 1.58293, val loss: 1.60626\n",
      "Main effects training epoch: 195, train loss: 1.58332, val loss: 1.59443\n",
      "Main effects training epoch: 196, train loss: 1.57642, val loss: 1.58645\n",
      "Main effects training epoch: 197, train loss: 1.58492, val loss: 1.61438\n",
      "Main effects training epoch: 198, train loss: 1.57670, val loss: 1.58960\n",
      "Main effects training epoch: 199, train loss: 1.57872, val loss: 1.60039\n",
      "Main effects training epoch: 200, train loss: 1.57411, val loss: 1.58532\n",
      "Main effects training epoch: 201, train loss: 1.57998, val loss: 1.60165\n",
      "Main effects training epoch: 202, train loss: 1.58871, val loss: 1.61993\n",
      "Main effects training epoch: 203, train loss: 1.58208, val loss: 1.60462\n",
      "Main effects training epoch: 204, train loss: 1.58956, val loss: 1.60753\n",
      "Main effects training epoch: 205, train loss: 1.58471, val loss: 1.60665\n",
      "Main effects training epoch: 206, train loss: 1.58050, val loss: 1.60249\n",
      "Main effects training epoch: 207, train loss: 1.58123, val loss: 1.59482\n",
      "Main effects training epoch: 208, train loss: 1.58648, val loss: 1.61554\n",
      "Main effects training epoch: 209, train loss: 1.58638, val loss: 1.60965\n",
      "Main effects training epoch: 210, train loss: 1.58653, val loss: 1.61166\n",
      "Main effects training epoch: 211, train loss: 1.58393, val loss: 1.60315\n",
      "Main effects training epoch: 212, train loss: 1.58320, val loss: 1.60433\n",
      "Main effects training epoch: 213, train loss: 1.58286, val loss: 1.60725\n",
      "Main effects training epoch: 214, train loss: 1.58296, val loss: 1.59824\n",
      "Main effects training epoch: 215, train loss: 1.58070, val loss: 1.60169\n",
      "Main effects training epoch: 216, train loss: 1.58224, val loss: 1.59961\n",
      "Main effects training epoch: 217, train loss: 1.58018, val loss: 1.60095\n",
      "Main effects training epoch: 218, train loss: 1.57498, val loss: 1.59555\n",
      "Main effects training epoch: 219, train loss: 1.57773, val loss: 1.60399\n",
      "Main effects training epoch: 220, train loss: 1.57583, val loss: 1.59526\n",
      "Main effects training epoch: 221, train loss: 1.57950, val loss: 1.59925\n",
      "Main effects training epoch: 222, train loss: 1.57825, val loss: 1.60109\n",
      "Main effects training epoch: 223, train loss: 1.57670, val loss: 1.59862\n",
      "Main effects training epoch: 224, train loss: 1.57449, val loss: 1.59836\n",
      "Main effects training epoch: 225, train loss: 1.57412, val loss: 1.58924\n",
      "Main effects training epoch: 226, train loss: 1.57270, val loss: 1.59137\n",
      "Main effects training epoch: 227, train loss: 1.57995, val loss: 1.60268\n",
      "Main effects training epoch: 228, train loss: 1.57669, val loss: 1.59610\n",
      "Main effects training epoch: 229, train loss: 1.57499, val loss: 1.59147\n",
      "Main effects training epoch: 230, train loss: 1.57450, val loss: 1.59136\n",
      "Main effects training epoch: 231, train loss: 1.57075, val loss: 1.59483\n",
      "Main effects training epoch: 232, train loss: 1.57134, val loss: 1.58644\n",
      "Main effects training epoch: 233, train loss: 1.57384, val loss: 1.59370\n",
      "Main effects training epoch: 234, train loss: 1.57439, val loss: 1.59713\n",
      "Main effects training epoch: 235, train loss: 1.56433, val loss: 1.58172\n",
      "Main effects training epoch: 236, train loss: 1.56681, val loss: 1.58366\n",
      "Main effects training epoch: 237, train loss: 1.57481, val loss: 1.58951\n",
      "Main effects training epoch: 238, train loss: 1.56855, val loss: 1.59244\n",
      "Main effects training epoch: 239, train loss: 1.56840, val loss: 1.58763\n",
      "Main effects training epoch: 240, train loss: 1.56097, val loss: 1.58282\n",
      "Main effects training epoch: 241, train loss: 1.56015, val loss: 1.57838\n",
      "Main effects training epoch: 242, train loss: 1.56269, val loss: 1.57366\n",
      "Main effects training epoch: 243, train loss: 1.56438, val loss: 1.59010\n",
      "Main effects training epoch: 244, train loss: 1.55936, val loss: 1.57202\n",
      "Main effects training epoch: 245, train loss: 1.55132, val loss: 1.56642\n",
      "Main effects training epoch: 246, train loss: 1.54826, val loss: 1.55498\n",
      "Main effects training epoch: 247, train loss: 1.54997, val loss: 1.56619\n",
      "Main effects training epoch: 248, train loss: 1.55373, val loss: 1.57202\n",
      "Main effects training epoch: 249, train loss: 1.54863, val loss: 1.55842\n",
      "Main effects training epoch: 250, train loss: 1.55190, val loss: 1.56330\n",
      "Main effects training epoch: 251, train loss: 1.54791, val loss: 1.56095\n",
      "Main effects training epoch: 252, train loss: 1.55116, val loss: 1.55618\n",
      "Main effects training epoch: 253, train loss: 1.54191, val loss: 1.55228\n",
      "Main effects training epoch: 254, train loss: 1.54405, val loss: 1.55827\n",
      "Main effects training epoch: 255, train loss: 1.53789, val loss: 1.55029\n",
      "Main effects training epoch: 256, train loss: 1.54508, val loss: 1.56053\n",
      "Main effects training epoch: 257, train loss: 1.55436, val loss: 1.55938\n",
      "Main effects training epoch: 258, train loss: 1.54556, val loss: 1.56648\n",
      "Main effects training epoch: 259, train loss: 1.54520, val loss: 1.55805\n",
      "Main effects training epoch: 260, train loss: 1.53853, val loss: 1.55411\n",
      "Main effects training epoch: 261, train loss: 1.54307, val loss: 1.56202\n",
      "Main effects training epoch: 262, train loss: 1.53959, val loss: 1.55356\n",
      "Main effects training epoch: 263, train loss: 1.54302, val loss: 1.54788\n",
      "Main effects training epoch: 264, train loss: 1.53835, val loss: 1.55242\n",
      "Main effects training epoch: 265, train loss: 1.53951, val loss: 1.55837\n",
      "Main effects training epoch: 266, train loss: 1.55299, val loss: 1.57193\n",
      "Main effects training epoch: 267, train loss: 1.53656, val loss: 1.54613\n",
      "Main effects training epoch: 268, train loss: 1.53169, val loss: 1.54262\n",
      "Main effects training epoch: 269, train loss: 1.53904, val loss: 1.55306\n",
      "Main effects training epoch: 270, train loss: 1.53946, val loss: 1.56442\n",
      "Main effects training epoch: 271, train loss: 1.53439, val loss: 1.54909\n",
      "Main effects training epoch: 272, train loss: 1.53501, val loss: 1.54531\n",
      "Main effects training epoch: 273, train loss: 1.53036, val loss: 1.54365\n",
      "Main effects training epoch: 274, train loss: 1.53399, val loss: 1.53666\n",
      "Main effects training epoch: 275, train loss: 1.53436, val loss: 1.55051\n",
      "Main effects training epoch: 276, train loss: 1.53591, val loss: 1.54966\n",
      "Main effects training epoch: 277, train loss: 1.53752, val loss: 1.54856\n",
      "Main effects training epoch: 278, train loss: 1.52887, val loss: 1.55098\n",
      "Main effects training epoch: 279, train loss: 1.52896, val loss: 1.55706\n",
      "Main effects training epoch: 280, train loss: 1.52969, val loss: 1.53298\n",
      "Main effects training epoch: 281, train loss: 1.53007, val loss: 1.54674\n",
      "Main effects training epoch: 282, train loss: 1.53016, val loss: 1.53918\n",
      "Main effects training epoch: 283, train loss: 1.53523, val loss: 1.54680\n",
      "Main effects training epoch: 284, train loss: 1.52678, val loss: 1.53231\n",
      "Main effects training epoch: 285, train loss: 1.53389, val loss: 1.55674\n",
      "Main effects training epoch: 286, train loss: 1.54057, val loss: 1.55779\n",
      "Main effects training epoch: 287, train loss: 1.54766, val loss: 1.56423\n",
      "Main effects training epoch: 288, train loss: 1.54380, val loss: 1.55501\n",
      "Main effects training epoch: 289, train loss: 1.52756, val loss: 1.53205\n",
      "Main effects training epoch: 290, train loss: 1.54252, val loss: 1.54476\n",
      "Main effects training epoch: 291, train loss: 1.53651, val loss: 1.55565\n",
      "Main effects training epoch: 292, train loss: 1.52380, val loss: 1.53920\n",
      "Main effects training epoch: 293, train loss: 1.52231, val loss: 1.52908\n",
      "Main effects training epoch: 294, train loss: 1.52690, val loss: 1.53786\n",
      "Main effects training epoch: 295, train loss: 1.52792, val loss: 1.53521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 296, train loss: 1.52722, val loss: 1.54064\n",
      "Main effects training epoch: 297, train loss: 1.52512, val loss: 1.53607\n",
      "Main effects training epoch: 298, train loss: 1.53591, val loss: 1.53294\n",
      "Main effects training epoch: 299, train loss: 1.53643, val loss: 1.54739\n",
      "Main effects training epoch: 300, train loss: 1.52329, val loss: 1.53060\n",
      "##########Stage 1: main effect training stop.##########\n",
      "2 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 1.52379, val loss: 1.53053\n",
      "Main effects tuning epoch: 2, train loss: 1.53079, val loss: 1.54162\n",
      "Main effects tuning epoch: 3, train loss: 1.52223, val loss: 1.53516\n",
      "Main effects tuning epoch: 4, train loss: 1.53471, val loss: 1.54545\n",
      "Main effects tuning epoch: 5, train loss: 1.52439, val loss: 1.54264\n",
      "Main effects tuning epoch: 6, train loss: 1.52119, val loss: 1.52289\n",
      "Main effects tuning epoch: 7, train loss: 1.52570, val loss: 1.53474\n",
      "Main effects tuning epoch: 8, train loss: 1.52358, val loss: 1.53849\n",
      "Main effects tuning epoch: 9, train loss: 1.53259, val loss: 1.55325\n",
      "Main effects tuning epoch: 10, train loss: 1.52916, val loss: 1.54256\n",
      "Main effects tuning epoch: 11, train loss: 1.53004, val loss: 1.53029\n",
      "Main effects tuning epoch: 12, train loss: 1.53394, val loss: 1.56081\n",
      "Main effects tuning epoch: 13, train loss: 1.52060, val loss: 1.53363\n",
      "Main effects tuning epoch: 14, train loss: 1.52031, val loss: 1.52585\n",
      "Main effects tuning epoch: 15, train loss: 1.52093, val loss: 1.52962\n",
      "Main effects tuning epoch: 16, train loss: 1.51822, val loss: 1.52715\n",
      "Main effects tuning epoch: 17, train loss: 1.51821, val loss: 1.52952\n",
      "Main effects tuning epoch: 18, train loss: 1.51466, val loss: 1.52751\n",
      "Main effects tuning epoch: 19, train loss: 1.52172, val loss: 1.54323\n",
      "Main effects tuning epoch: 20, train loss: 1.52935, val loss: 1.52394\n",
      "Main effects tuning epoch: 21, train loss: 1.52098, val loss: 1.53241\n",
      "Main effects tuning epoch: 22, train loss: 1.51916, val loss: 1.53331\n",
      "Main effects tuning epoch: 23, train loss: 1.51559, val loss: 1.52765\n",
      "Main effects tuning epoch: 24, train loss: 1.51264, val loss: 1.51446\n",
      "Main effects tuning epoch: 25, train loss: 1.51906, val loss: 1.53519\n",
      "Main effects tuning epoch: 26, train loss: 1.51460, val loss: 1.52018\n",
      "Main effects tuning epoch: 27, train loss: 1.51572, val loss: 1.53142\n",
      "Main effects tuning epoch: 28, train loss: 1.51618, val loss: 1.52659\n",
      "Main effects tuning epoch: 29, train loss: 1.51629, val loss: 1.51470\n",
      "Main effects tuning epoch: 30, train loss: 1.51763, val loss: 1.53385\n",
      "Main effects tuning epoch: 31, train loss: 1.52070, val loss: 1.54407\n",
      "Main effects tuning epoch: 32, train loss: 1.51369, val loss: 1.52205\n",
      "Main effects tuning epoch: 33, train loss: 1.53658, val loss: 1.54800\n",
      "Main effects tuning epoch: 34, train loss: 1.52197, val loss: 1.53071\n",
      "Main effects tuning epoch: 35, train loss: 1.50894, val loss: 1.51765\n",
      "Main effects tuning epoch: 36, train loss: 1.51066, val loss: 1.52019\n",
      "Main effects tuning epoch: 37, train loss: 1.50987, val loss: 1.51265\n",
      "Main effects tuning epoch: 38, train loss: 1.51773, val loss: 1.53946\n",
      "Main effects tuning epoch: 39, train loss: 1.50860, val loss: 1.51604\n",
      "Main effects tuning epoch: 40, train loss: 1.51351, val loss: 1.51590\n",
      "Main effects tuning epoch: 41, train loss: 1.50532, val loss: 1.52256\n",
      "Main effects tuning epoch: 42, train loss: 1.51293, val loss: 1.52732\n",
      "Main effects tuning epoch: 43, train loss: 1.51802, val loss: 1.52129\n",
      "Main effects tuning epoch: 44, train loss: 1.52136, val loss: 1.53150\n",
      "Main effects tuning epoch: 45, train loss: 1.51849, val loss: 1.52504\n",
      "Main effects tuning epoch: 46, train loss: 1.53487, val loss: 1.54049\n",
      "Main effects tuning epoch: 47, train loss: 1.50822, val loss: 1.51639\n",
      "Main effects tuning epoch: 48, train loss: 1.50909, val loss: 1.51231\n",
      "Main effects tuning epoch: 49, train loss: 1.50894, val loss: 1.51961\n",
      "Main effects tuning epoch: 50, train loss: 1.50683, val loss: 1.51231\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 1.45925, val loss: 1.48268\n",
      "Interaction training epoch: 2, train loss: 1.29567, val loss: 1.29028\n",
      "Interaction training epoch: 3, train loss: 1.15923, val loss: 1.15125\n",
      "Interaction training epoch: 4, train loss: 1.07358, val loss: 1.12904\n",
      "Interaction training epoch: 5, train loss: 1.03564, val loss: 1.04241\n",
      "Interaction training epoch: 6, train loss: 1.02009, val loss: 1.05732\n",
      "Interaction training epoch: 7, train loss: 0.99047, val loss: 0.99542\n",
      "Interaction training epoch: 8, train loss: 0.98504, val loss: 0.97793\n",
      "Interaction training epoch: 9, train loss: 0.95992, val loss: 0.97786\n",
      "Interaction training epoch: 10, train loss: 0.95961, val loss: 0.98577\n",
      "Interaction training epoch: 11, train loss: 0.94814, val loss: 0.99544\n",
      "Interaction training epoch: 12, train loss: 0.94598, val loss: 0.97303\n",
      "Interaction training epoch: 13, train loss: 0.92864, val loss: 0.97348\n",
      "Interaction training epoch: 14, train loss: 0.92698, val loss: 0.98300\n",
      "Interaction training epoch: 15, train loss: 0.91412, val loss: 0.96828\n",
      "Interaction training epoch: 16, train loss: 0.92041, val loss: 0.99146\n",
      "Interaction training epoch: 17, train loss: 0.89188, val loss: 0.93914\n",
      "Interaction training epoch: 18, train loss: 0.88718, val loss: 0.94243\n",
      "Interaction training epoch: 19, train loss: 0.87795, val loss: 0.92820\n",
      "Interaction training epoch: 20, train loss: 0.86708, val loss: 0.92619\n",
      "Interaction training epoch: 21, train loss: 0.87223, val loss: 0.91883\n",
      "Interaction training epoch: 22, train loss: 0.92302, val loss: 0.99708\n",
      "Interaction training epoch: 23, train loss: 0.87426, val loss: 0.94561\n",
      "Interaction training epoch: 24, train loss: 0.87308, val loss: 0.94122\n",
      "Interaction training epoch: 25, train loss: 0.85696, val loss: 0.92805\n",
      "Interaction training epoch: 26, train loss: 0.86112, val loss: 0.91811\n",
      "Interaction training epoch: 27, train loss: 0.84025, val loss: 0.90428\n",
      "Interaction training epoch: 28, train loss: 0.88315, val loss: 0.92287\n",
      "Interaction training epoch: 29, train loss: 0.86837, val loss: 0.94415\n",
      "Interaction training epoch: 30, train loss: 0.86416, val loss: 0.90858\n",
      "Interaction training epoch: 31, train loss: 0.84067, val loss: 0.90146\n",
      "Interaction training epoch: 32, train loss: 0.86117, val loss: 0.91528\n",
      "Interaction training epoch: 33, train loss: 0.85608, val loss: 0.90494\n",
      "Interaction training epoch: 34, train loss: 0.84643, val loss: 0.92279\n",
      "Interaction training epoch: 35, train loss: 0.85171, val loss: 0.90606\n",
      "Interaction training epoch: 36, train loss: 0.86146, val loss: 0.92565\n",
      "Interaction training epoch: 37, train loss: 0.82933, val loss: 0.88834\n",
      "Interaction training epoch: 38, train loss: 0.85233, val loss: 0.91496\n",
      "Interaction training epoch: 39, train loss: 0.85860, val loss: 0.92729\n",
      "Interaction training epoch: 40, train loss: 0.83413, val loss: 0.90030\n",
      "Interaction training epoch: 41, train loss: 0.85286, val loss: 0.90327\n",
      "Interaction training epoch: 42, train loss: 0.84757, val loss: 0.91775\n",
      "Interaction training epoch: 43, train loss: 0.84666, val loss: 0.91740\n",
      "Interaction training epoch: 44, train loss: 0.83399, val loss: 0.89913\n",
      "Interaction training epoch: 45, train loss: 0.82954, val loss: 0.89609\n",
      "Interaction training epoch: 46, train loss: 0.82677, val loss: 0.89020\n",
      "Interaction training epoch: 47, train loss: 0.83585, val loss: 0.90725\n",
      "Interaction training epoch: 48, train loss: 0.82588, val loss: 0.87863\n",
      "Interaction training epoch: 49, train loss: 0.83230, val loss: 0.89547\n",
      "Interaction training epoch: 50, train loss: 0.82885, val loss: 0.89676\n",
      "Interaction training epoch: 51, train loss: 0.82029, val loss: 0.87316\n",
      "Interaction training epoch: 52, train loss: 0.83336, val loss: 0.87857\n",
      "Interaction training epoch: 53, train loss: 0.82595, val loss: 0.88029\n",
      "Interaction training epoch: 54, train loss: 0.82082, val loss: 0.87702\n",
      "Interaction training epoch: 55, train loss: 0.82001, val loss: 0.88662\n",
      "Interaction training epoch: 56, train loss: 0.82556, val loss: 0.86101\n",
      "Interaction training epoch: 57, train loss: 0.83231, val loss: 0.89629\n",
      "Interaction training epoch: 58, train loss: 0.81973, val loss: 0.87353\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6429af04a840>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[0mlii\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mauto_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlii\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'result/wc_test.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    223\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m                        copy=copy, sort=sort)\n\u001b[0m\u001b[0;32m    226\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[0mobjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m             \u001b[0mobjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-6429af04a840>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[0mlii\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mauto_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlii\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'result/wc_test.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-6429af04a840>\u001b[0m in \u001b[0;36mauto_test\u001b[1;34m(alpha)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mst_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtr_Xi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0med_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\索信达\\代码\\lvxnn_0526\\lvxnn\\LVXNN.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, xx, Xi, y)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0mst_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[0mval_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mval_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\索信达\\代码\\lvxnn_0526\\lvxnn\\gaminet.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, train_x, train_y)\u001b[0m\n\u001b[0;32m    748\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"#\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"Stage 2: interaction training start.\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"#\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_interaction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_interaction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"#\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"Stage 2: interaction training stop.\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"#\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\索信达\\代码\\lvxnn_0526\\lvxnn\\gaminet.py\u001b[0m in \u001b[0;36mfit_interaction\u001b[1;34m(self, tr_x, tr_y, val_x, val_y)\u001b[0m\n\u001b[0;32m    541\u001b[0m                 \u001b[0mbatch_xx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m                 \u001b[0mbatch_yy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 543\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_interaction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_xx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_yy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merr_train_interaction_training\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmain_effect_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minteraction_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gaminet.utils import local_visualize\n",
    "from gaminet.utils import global_visualize_density\n",
    "from gaminet.utils import feature_importance_visualize\n",
    "from gaminet.utils import plot_trajectory\n",
    "from gaminet.utils import plot_regularization\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from lvxnn.LVXNN import LV_XNN\n",
    "from lvxnn.DataReader import data_initialize\n",
    "\n",
    "\n",
    "data= pd.read_csv('data/sim_0.9.csv')\n",
    "train , test = train_test_split(data,test_size=0.2)\n",
    "\n",
    "#list1 = data.columns\n",
    "meta_info = OrderedDict()\n",
    "\n",
    "meta_info['uf_1']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_2']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_3']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_4']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_5']={'type': 'continues','source':'user'}\n",
    "meta_info['if_1']={'type': 'continues','source':'item'}\n",
    "meta_info['if_2']={'type': 'continues','source':'item'}\n",
    "meta_info['if_3']={'type': 'continues','source':'item'}\n",
    "meta_info['if_4']={'type': 'continues','source':'item'}\n",
    "meta_info['if_5']={'type': 'continues','source':'item'}\n",
    "meta_info['user_id']={\"type\":\"id\",'source':'user'}\n",
    "meta_info['item_id']={\"type\":\"id\",'source':'item'}\n",
    "meta_info['target']={\"type\":\"target\",'source':''}\n",
    "\n",
    "\n",
    "tr_x, tr_Xi, tr_y , te_x , te_Xi, te_y, meta_info, model_info = data_initialize(train,test,meta_info,\"Regression\")\n",
    "\n",
    "def auto_test(alpha):\n",
    "    cold_mae = []\n",
    "    cold_rmse = []\n",
    "    warm_mae = []\n",
    "    warm_rmse = []\n",
    "    \n",
    "\n",
    "    for times in range(10):\n",
    "        \n",
    "        print(times)\n",
    "\n",
    "\n",
    "        model = LV_XNN(model_info=model_info, meta_info=meta_info, subnet_arch=[8, 16],interact_arch=[20, 10],activation_func=tf.tanh, batch_size=1000, lr_bp=0.01, auto_tune=False,\n",
    "               interaction_epochs=200,main_effect_epochs=300,tuning_epochs=50,loss_threshold_main=0.01,loss_threshold_inter=0.01,alpha=1,\n",
    "              verbose=True,val_ratio=0.125, early_stop_thres=100,interact_num=10,u_group_num=10,i_group_num=50,scale_ratio=alpha,n_power_iterations=5,n_oversamples=0,\n",
    "              mf_training_iters=1,mf_tuning_iters=400,change_mode=True,convergence_threshold=0.001,max_rank=3,shrinkage_value=20,random_state=times)\n",
    "    \n",
    "        st_time = time.time()\n",
    "        model.fit(tr_x,tr_Xi, tr_y)\n",
    "        ed_time = time.time()\n",
    "        \n",
    "        pred = model.predict(te_x, te_Xi)\n",
    "        \n",
    "        cold_y = te_y[(te_Xi[:,1] == 'cold') | (te_Xi[:,0] == 'cold')]\n",
    "        cold_pred = pred[(te_Xi[:,1] == 'cold') | (te_Xi[:,0] == 'cold')]\n",
    "        warm_y = te_y[(te_Xi[:,1] != 'cold') & (te_Xi[:,0] != 'cold')]\n",
    "        warm_pred = pred[(te_Xi[:,1] != 'cold') & (te_Xi[:,0] != 'cold')]\n",
    "        \n",
    "        cold_mae.append(mean_absolute_error(cold_y,cold_pred))\n",
    "        cold_rmse.append(mean_squared_error(cold_y,cold_pred)**0.5)\n",
    "        warm_mae.append(mean_absolute_error(te_y,pred))\n",
    "        warm_rmse.append(mean_squared_error(te_y,pred)**0.5)\n",
    "        \n",
    "    i_result = np.array([np.mean(cold_mae),np.mean(cold_rmse),np.mean(warm_mae),np.mean(warm_rmse)]).reshape(1,-1)\n",
    "    result = pd.DataFrame(i_result,columns=['cold_mae','cold_rmse','warm_mae','warm_rmse'])\n",
    "    \n",
    "    return result\n",
    "lii = [0,0.2,0.4,0.6,0.8,1]\n",
    "results = (auto_test(alpha) for alpha in lii)\n",
    "results = pd.concat(results)\n",
    "results.to_csv('result/wc_test.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.86 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../..\\lvxnn\\DataReader.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[col] = df[col].astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 0.26 MB\n",
      "Decreased by 69.6%\n",
      "Memory usage of dataframe is 0.21 MB\n",
      "Memory usage after optimization is: 0.07 MB\n",
      "Decreased by 69.6%\n",
      "cold start user: 40\n",
      "cold start item: 315\n",
      "0\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68488, val loss: 0.68424\n",
      "Main effects training epoch: 2, train loss: 0.67897, val loss: 0.68003\n",
      "Main effects training epoch: 3, train loss: 0.67209, val loss: 0.67596\n",
      "Main effects training epoch: 4, train loss: 0.66609, val loss: 0.67227\n",
      "Main effects training epoch: 5, train loss: 0.65428, val loss: 0.66347\n",
      "Main effects training epoch: 6, train loss: 0.63348, val loss: 0.64240\n",
      "Main effects training epoch: 7, train loss: 0.59636, val loss: 0.60298\n",
      "Main effects training epoch: 8, train loss: 0.55908, val loss: 0.55570\n",
      "Main effects training epoch: 9, train loss: 0.54213, val loss: 0.52746\n",
      "Main effects training epoch: 10, train loss: 0.53248, val loss: 0.50761\n",
      "Main effects training epoch: 11, train loss: 0.52876, val loss: 0.50058\n",
      "Main effects training epoch: 12, train loss: 0.52881, val loss: 0.50016\n",
      "Main effects training epoch: 13, train loss: 0.52579, val loss: 0.49930\n",
      "Main effects training epoch: 14, train loss: 0.52529, val loss: 0.49633\n",
      "Main effects training epoch: 15, train loss: 0.52492, val loss: 0.49867\n",
      "Main effects training epoch: 16, train loss: 0.52436, val loss: 0.49592\n",
      "Main effects training epoch: 17, train loss: 0.52432, val loss: 0.49557\n",
      "Main effects training epoch: 18, train loss: 0.52425, val loss: 0.49812\n",
      "Main effects training epoch: 19, train loss: 0.52401, val loss: 0.49674\n",
      "Main effects training epoch: 20, train loss: 0.52518, val loss: 0.49780\n",
      "Main effects training epoch: 21, train loss: 0.52428, val loss: 0.49714\n",
      "Main effects training epoch: 22, train loss: 0.52377, val loss: 0.49614\n",
      "Main effects training epoch: 23, train loss: 0.52395, val loss: 0.49656\n",
      "Main effects training epoch: 24, train loss: 0.52422, val loss: 0.49745\n",
      "Main effects training epoch: 25, train loss: 0.52554, val loss: 0.49583\n",
      "Main effects training epoch: 26, train loss: 0.52459, val loss: 0.49967\n",
      "Main effects training epoch: 27, train loss: 0.52581, val loss: 0.49550\n",
      "Main effects training epoch: 28, train loss: 0.52443, val loss: 0.49991\n",
      "Main effects training epoch: 29, train loss: 0.52517, val loss: 0.49561\n",
      "Main effects training epoch: 30, train loss: 0.52429, val loss: 0.49715\n",
      "Main effects training epoch: 31, train loss: 0.52631, val loss: 0.49799\n",
      "Main effects training epoch: 32, train loss: 0.52438, val loss: 0.49770\n",
      "Main effects training epoch: 33, train loss: 0.52328, val loss: 0.49530\n",
      "Main effects training epoch: 34, train loss: 0.52336, val loss: 0.49611\n",
      "Main effects training epoch: 35, train loss: 0.52426, val loss: 0.49691\n",
      "Main effects training epoch: 36, train loss: 0.52519, val loss: 0.49596\n",
      "Main effects training epoch: 37, train loss: 0.52567, val loss: 0.49980\n",
      "Main effects training epoch: 38, train loss: 0.52524, val loss: 0.49774\n",
      "Main effects training epoch: 39, train loss: 0.52384, val loss: 0.49759\n",
      "Main effects training epoch: 40, train loss: 0.52355, val loss: 0.49496\n",
      "Main effects training epoch: 41, train loss: 0.52321, val loss: 0.49596\n",
      "Main effects training epoch: 42, train loss: 0.52283, val loss: 0.49597\n",
      "Main effects training epoch: 43, train loss: 0.52332, val loss: 0.49458\n",
      "Main effects training epoch: 44, train loss: 0.52322, val loss: 0.49715\n",
      "Main effects training epoch: 45, train loss: 0.52296, val loss: 0.49447\n",
      "Main effects training epoch: 46, train loss: 0.52257, val loss: 0.49432\n",
      "Main effects training epoch: 47, train loss: 0.52228, val loss: 0.49558\n",
      "Main effects training epoch: 48, train loss: 0.52256, val loss: 0.49489\n",
      "Main effects training epoch: 49, train loss: 0.52270, val loss: 0.49624\n",
      "Main effects training epoch: 50, train loss: 0.52305, val loss: 0.49428\n",
      "Main effects training epoch: 51, train loss: 0.52242, val loss: 0.49547\n",
      "Main effects training epoch: 52, train loss: 0.52248, val loss: 0.49694\n",
      "Main effects training epoch: 53, train loss: 0.52352, val loss: 0.49415\n",
      "Main effects training epoch: 54, train loss: 0.52328, val loss: 0.49766\n",
      "Main effects training epoch: 55, train loss: 0.52260, val loss: 0.49392\n",
      "Main effects training epoch: 56, train loss: 0.52257, val loss: 0.49730\n",
      "Main effects training epoch: 57, train loss: 0.52245, val loss: 0.49385\n",
      "Main effects training epoch: 58, train loss: 0.52311, val loss: 0.49840\n",
      "Main effects training epoch: 59, train loss: 0.52326, val loss: 0.49401\n",
      "Main effects training epoch: 60, train loss: 0.52207, val loss: 0.49570\n",
      "Main effects training epoch: 61, train loss: 0.52192, val loss: 0.49468\n",
      "Main effects training epoch: 62, train loss: 0.52238, val loss: 0.49521\n",
      "Main effects training epoch: 63, train loss: 0.52213, val loss: 0.49398\n",
      "Main effects training epoch: 64, train loss: 0.52278, val loss: 0.49738\n",
      "Main effects training epoch: 65, train loss: 0.52264, val loss: 0.49355\n",
      "Main effects training epoch: 66, train loss: 0.52239, val loss: 0.49773\n",
      "Main effects training epoch: 67, train loss: 0.52156, val loss: 0.49374\n",
      "Main effects training epoch: 68, train loss: 0.52125, val loss: 0.49415\n",
      "Main effects training epoch: 69, train loss: 0.52135, val loss: 0.49329\n",
      "Main effects training epoch: 70, train loss: 0.52149, val loss: 0.49428\n",
      "Main effects training epoch: 71, train loss: 0.52156, val loss: 0.49417\n",
      "Main effects training epoch: 72, train loss: 0.52195, val loss: 0.49577\n",
      "Main effects training epoch: 73, train loss: 0.52182, val loss: 0.49400\n",
      "Main effects training epoch: 74, train loss: 0.52167, val loss: 0.49584\n",
      "Main effects training epoch: 75, train loss: 0.52130, val loss: 0.49482\n",
      "Main effects training epoch: 76, train loss: 0.52116, val loss: 0.49385\n",
      "Main effects training epoch: 77, train loss: 0.52099, val loss: 0.49287\n",
      "Main effects training epoch: 78, train loss: 0.52125, val loss: 0.49515\n",
      "Main effects training epoch: 79, train loss: 0.52089, val loss: 0.49379\n",
      "Main effects training epoch: 80, train loss: 0.52108, val loss: 0.49395\n",
      "Main effects training epoch: 81, train loss: 0.52103, val loss: 0.49464\n",
      "Main effects training epoch: 82, train loss: 0.52114, val loss: 0.49281\n",
      "Main effects training epoch: 83, train loss: 0.52137, val loss: 0.49600\n",
      "Main effects training epoch: 84, train loss: 0.52115, val loss: 0.49284\n",
      "Main effects training epoch: 85, train loss: 0.52101, val loss: 0.49643\n",
      "Main effects training epoch: 86, train loss: 0.52112, val loss: 0.49385\n",
      "Main effects training epoch: 87, train loss: 0.52068, val loss: 0.49390\n",
      "Main effects training epoch: 88, train loss: 0.52110, val loss: 0.49331\n",
      "Main effects training epoch: 89, train loss: 0.52076, val loss: 0.49515\n",
      "Main effects training epoch: 90, train loss: 0.52137, val loss: 0.49224\n",
      "Main effects training epoch: 91, train loss: 0.52049, val loss: 0.49586\n",
      "Main effects training epoch: 92, train loss: 0.52008, val loss: 0.49404\n",
      "Main effects training epoch: 93, train loss: 0.52003, val loss: 0.49224\n",
      "Main effects training epoch: 94, train loss: 0.52000, val loss: 0.49403\n",
      "Main effects training epoch: 95, train loss: 0.51978, val loss: 0.49288\n",
      "Main effects training epoch: 96, train loss: 0.51960, val loss: 0.49247\n",
      "Main effects training epoch: 97, train loss: 0.51994, val loss: 0.49437\n",
      "Main effects training epoch: 98, train loss: 0.51962, val loss: 0.49151\n",
      "Main effects training epoch: 99, train loss: 0.51933, val loss: 0.49297\n",
      "Main effects training epoch: 100, train loss: 0.51934, val loss: 0.49278\n",
      "Main effects training epoch: 101, train loss: 0.51914, val loss: 0.49377\n",
      "Main effects training epoch: 102, train loss: 0.51890, val loss: 0.49190\n",
      "Main effects training epoch: 103, train loss: 0.51918, val loss: 0.49248\n",
      "Main effects training epoch: 104, train loss: 0.51900, val loss: 0.49178\n",
      "Main effects training epoch: 105, train loss: 0.51869, val loss: 0.49247\n",
      "Main effects training epoch: 106, train loss: 0.51847, val loss: 0.49264\n",
      "Main effects training epoch: 107, train loss: 0.51874, val loss: 0.49144\n",
      "Main effects training epoch: 108, train loss: 0.51889, val loss: 0.49303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 109, train loss: 0.51782, val loss: 0.49032\n",
      "Main effects training epoch: 110, train loss: 0.51835, val loss: 0.48990\n",
      "Main effects training epoch: 111, train loss: 0.51760, val loss: 0.49092\n",
      "Main effects training epoch: 112, train loss: 0.51778, val loss: 0.48986\n",
      "Main effects training epoch: 113, train loss: 0.51764, val loss: 0.49099\n",
      "Main effects training epoch: 114, train loss: 0.51790, val loss: 0.49041\n",
      "Main effects training epoch: 115, train loss: 0.51771, val loss: 0.49102\n",
      "Main effects training epoch: 116, train loss: 0.51786, val loss: 0.48986\n",
      "Main effects training epoch: 117, train loss: 0.51736, val loss: 0.49026\n",
      "Main effects training epoch: 118, train loss: 0.51775, val loss: 0.49270\n",
      "Main effects training epoch: 119, train loss: 0.51808, val loss: 0.49011\n",
      "Main effects training epoch: 120, train loss: 0.51761, val loss: 0.49044\n",
      "Main effects training epoch: 121, train loss: 0.51718, val loss: 0.48969\n",
      "Main effects training epoch: 122, train loss: 0.51724, val loss: 0.48976\n",
      "Main effects training epoch: 123, train loss: 0.51708, val loss: 0.49089\n",
      "Main effects training epoch: 124, train loss: 0.51712, val loss: 0.48802\n",
      "Main effects training epoch: 125, train loss: 0.51733, val loss: 0.49081\n",
      "Main effects training epoch: 126, train loss: 0.51727, val loss: 0.48883\n",
      "Main effects training epoch: 127, train loss: 0.51755, val loss: 0.49165\n",
      "Main effects training epoch: 128, train loss: 0.51715, val loss: 0.48858\n",
      "Main effects training epoch: 129, train loss: 0.51691, val loss: 0.49003\n",
      "Main effects training epoch: 130, train loss: 0.51670, val loss: 0.49043\n",
      "Main effects training epoch: 131, train loss: 0.51671, val loss: 0.48902\n",
      "Main effects training epoch: 132, train loss: 0.51658, val loss: 0.48841\n",
      "Main effects training epoch: 133, train loss: 0.51707, val loss: 0.49083\n",
      "Main effects training epoch: 134, train loss: 0.51646, val loss: 0.48841\n",
      "Main effects training epoch: 135, train loss: 0.51650, val loss: 0.48996\n",
      "Main effects training epoch: 136, train loss: 0.51689, val loss: 0.48839\n",
      "Main effects training epoch: 137, train loss: 0.51649, val loss: 0.48976\n",
      "Main effects training epoch: 138, train loss: 0.51675, val loss: 0.48977\n",
      "Main effects training epoch: 139, train loss: 0.51712, val loss: 0.49126\n",
      "Main effects training epoch: 140, train loss: 0.51728, val loss: 0.48913\n",
      "Main effects training epoch: 141, train loss: 0.51709, val loss: 0.49035\n",
      "Main effects training epoch: 142, train loss: 0.51669, val loss: 0.48828\n",
      "Main effects training epoch: 143, train loss: 0.51727, val loss: 0.49065\n",
      "Main effects training epoch: 144, train loss: 0.51844, val loss: 0.49056\n",
      "Main effects training epoch: 145, train loss: 0.51685, val loss: 0.49086\n",
      "Main effects training epoch: 146, train loss: 0.51726, val loss: 0.48940\n",
      "Main effects training epoch: 147, train loss: 0.51716, val loss: 0.48974\n",
      "Main effects training epoch: 148, train loss: 0.51674, val loss: 0.48965\n",
      "Main effects training epoch: 149, train loss: 0.51636, val loss: 0.49099\n",
      "Main effects training epoch: 150, train loss: 0.51592, val loss: 0.48948\n",
      "Main effects training epoch: 151, train loss: 0.51580, val loss: 0.48817\n",
      "Main effects training epoch: 152, train loss: 0.51602, val loss: 0.48895\n",
      "Main effects training epoch: 153, train loss: 0.51620, val loss: 0.48999\n",
      "Main effects training epoch: 154, train loss: 0.51586, val loss: 0.48875\n",
      "Main effects training epoch: 155, train loss: 0.51570, val loss: 0.48861\n",
      "Main effects training epoch: 156, train loss: 0.51611, val loss: 0.48798\n",
      "Main effects training epoch: 157, train loss: 0.51552, val loss: 0.48877\n",
      "Main effects training epoch: 158, train loss: 0.51543, val loss: 0.48827\n",
      "Main effects training epoch: 159, train loss: 0.51561, val loss: 0.48956\n",
      "Main effects training epoch: 160, train loss: 0.51634, val loss: 0.48939\n",
      "Main effects training epoch: 161, train loss: 0.51581, val loss: 0.48966\n",
      "Main effects training epoch: 162, train loss: 0.51592, val loss: 0.48891\n",
      "Main effects training epoch: 163, train loss: 0.51579, val loss: 0.48835\n",
      "Main effects training epoch: 164, train loss: 0.51642, val loss: 0.49139\n",
      "Main effects training epoch: 165, train loss: 0.51693, val loss: 0.48970\n",
      "Main effects training epoch: 166, train loss: 0.51630, val loss: 0.48808\n",
      "Main effects training epoch: 167, train loss: 0.51623, val loss: 0.49147\n",
      "Main effects training epoch: 168, train loss: 0.51598, val loss: 0.48821\n",
      "Main effects training epoch: 169, train loss: 0.51539, val loss: 0.48980\n",
      "Main effects training epoch: 170, train loss: 0.51513, val loss: 0.48814\n",
      "Main effects training epoch: 171, train loss: 0.51516, val loss: 0.48904\n",
      "Main effects training epoch: 172, train loss: 0.51511, val loss: 0.48843\n",
      "Main effects training epoch: 173, train loss: 0.51498, val loss: 0.48834\n",
      "Main effects training epoch: 174, train loss: 0.51523, val loss: 0.48842\n",
      "Main effects training epoch: 175, train loss: 0.51540, val loss: 0.49044\n",
      "Main effects training epoch: 176, train loss: 0.51496, val loss: 0.48910\n",
      "Main effects training epoch: 177, train loss: 0.51511, val loss: 0.48952\n",
      "Main effects training epoch: 178, train loss: 0.51531, val loss: 0.48756\n",
      "Main effects training epoch: 179, train loss: 0.51516, val loss: 0.48946\n",
      "Main effects training epoch: 180, train loss: 0.51515, val loss: 0.49097\n",
      "Main effects training epoch: 181, train loss: 0.51493, val loss: 0.48787\n",
      "Main effects training epoch: 182, train loss: 0.51450, val loss: 0.48818\n",
      "Main effects training epoch: 183, train loss: 0.51456, val loss: 0.48867\n",
      "Main effects training epoch: 184, train loss: 0.51452, val loss: 0.48944\n",
      "Main effects training epoch: 185, train loss: 0.51465, val loss: 0.48735\n",
      "Main effects training epoch: 186, train loss: 0.51448, val loss: 0.48992\n",
      "Main effects training epoch: 187, train loss: 0.51471, val loss: 0.48906\n",
      "Main effects training epoch: 188, train loss: 0.51434, val loss: 0.48744\n",
      "Main effects training epoch: 189, train loss: 0.51455, val loss: 0.49049\n",
      "Main effects training epoch: 190, train loss: 0.51497, val loss: 0.48877\n",
      "Main effects training epoch: 191, train loss: 0.51469, val loss: 0.49060\n",
      "Main effects training epoch: 192, train loss: 0.51400, val loss: 0.48761\n",
      "Main effects training epoch: 193, train loss: 0.51450, val loss: 0.48794\n",
      "Main effects training epoch: 194, train loss: 0.51449, val loss: 0.49099\n",
      "Main effects training epoch: 195, train loss: 0.51398, val loss: 0.48901\n",
      "Main effects training epoch: 196, train loss: 0.51388, val loss: 0.48737\n",
      "Main effects training epoch: 197, train loss: 0.51383, val loss: 0.48859\n",
      "Main effects training epoch: 198, train loss: 0.51394, val loss: 0.48905\n",
      "Main effects training epoch: 199, train loss: 0.51413, val loss: 0.48933\n",
      "Main effects training epoch: 200, train loss: 0.51458, val loss: 0.48767\n",
      "Main effects training epoch: 201, train loss: 0.51390, val loss: 0.48968\n",
      "Main effects training epoch: 202, train loss: 0.51364, val loss: 0.48849\n",
      "Main effects training epoch: 203, train loss: 0.51364, val loss: 0.48753\n",
      "Main effects training epoch: 204, train loss: 0.51356, val loss: 0.48932\n",
      "Main effects training epoch: 205, train loss: 0.51343, val loss: 0.48832\n",
      "Main effects training epoch: 206, train loss: 0.51365, val loss: 0.49039\n",
      "Main effects training epoch: 207, train loss: 0.51377, val loss: 0.48795\n",
      "Main effects training epoch: 208, train loss: 0.51335, val loss: 0.48853\n",
      "Main effects training epoch: 209, train loss: 0.51364, val loss: 0.48939\n",
      "Main effects training epoch: 210, train loss: 0.51325, val loss: 0.48803\n",
      "Main effects training epoch: 211, train loss: 0.51352, val loss: 0.48614\n",
      "Main effects training epoch: 212, train loss: 0.51327, val loss: 0.49013\n",
      "Main effects training epoch: 213, train loss: 0.51289, val loss: 0.48791\n",
      "Main effects training epoch: 214, train loss: 0.51353, val loss: 0.48854\n",
      "Main effects training epoch: 215, train loss: 0.51308, val loss: 0.48883\n",
      "Main effects training epoch: 216, train loss: 0.51297, val loss: 0.48752\n",
      "Main effects training epoch: 217, train loss: 0.51305, val loss: 0.48982\n",
      "Main effects training epoch: 218, train loss: 0.51276, val loss: 0.48742\n",
      "Main effects training epoch: 219, train loss: 0.51303, val loss: 0.48900\n",
      "Main effects training epoch: 220, train loss: 0.51259, val loss: 0.48877\n",
      "Main effects training epoch: 221, train loss: 0.51239, val loss: 0.48718\n",
      "Main effects training epoch: 222, train loss: 0.51252, val loss: 0.48905\n",
      "Main effects training epoch: 223, train loss: 0.51241, val loss: 0.48671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 224, train loss: 0.51279, val loss: 0.48922\n",
      "Main effects training epoch: 225, train loss: 0.51273, val loss: 0.48742\n",
      "Main effects training epoch: 226, train loss: 0.51241, val loss: 0.48849\n",
      "Main effects training epoch: 227, train loss: 0.51228, val loss: 0.48761\n",
      "Main effects training epoch: 228, train loss: 0.51195, val loss: 0.48725\n",
      "Main effects training epoch: 229, train loss: 0.51193, val loss: 0.48769\n",
      "Main effects training epoch: 230, train loss: 0.51217, val loss: 0.48744\n",
      "Main effects training epoch: 231, train loss: 0.51204, val loss: 0.48551\n",
      "Main effects training epoch: 232, train loss: 0.51263, val loss: 0.49026\n",
      "Main effects training epoch: 233, train loss: 0.51186, val loss: 0.48740\n",
      "Main effects training epoch: 234, train loss: 0.51164, val loss: 0.48717\n",
      "Main effects training epoch: 235, train loss: 0.51204, val loss: 0.48672\n",
      "Main effects training epoch: 236, train loss: 0.51228, val loss: 0.48760\n",
      "Main effects training epoch: 237, train loss: 0.51179, val loss: 0.48810\n",
      "Main effects training epoch: 238, train loss: 0.51215, val loss: 0.48787\n",
      "Main effects training epoch: 239, train loss: 0.51334, val loss: 0.48908\n",
      "Main effects training epoch: 240, train loss: 0.51153, val loss: 0.48723\n",
      "Main effects training epoch: 241, train loss: 0.51127, val loss: 0.48804\n",
      "Main effects training epoch: 242, train loss: 0.51165, val loss: 0.48581\n",
      "Main effects training epoch: 243, train loss: 0.51161, val loss: 0.48761\n",
      "Main effects training epoch: 244, train loss: 0.51165, val loss: 0.48709\n",
      "Main effects training epoch: 245, train loss: 0.51141, val loss: 0.48893\n",
      "Main effects training epoch: 246, train loss: 0.51114, val loss: 0.48587\n",
      "Main effects training epoch: 247, train loss: 0.51203, val loss: 0.48628\n",
      "Main effects training epoch: 248, train loss: 0.51322, val loss: 0.48867\n",
      "Main effects training epoch: 249, train loss: 0.51257, val loss: 0.49010\n",
      "Main effects training epoch: 250, train loss: 0.51203, val loss: 0.48576\n",
      "Main effects training epoch: 251, train loss: 0.51090, val loss: 0.48581\n",
      "Main effects training epoch: 252, train loss: 0.51049, val loss: 0.48681\n",
      "Main effects training epoch: 253, train loss: 0.51052, val loss: 0.48655\n",
      "Main effects training epoch: 254, train loss: 0.51049, val loss: 0.48622\n",
      "Main effects training epoch: 255, train loss: 0.51047, val loss: 0.48455\n",
      "Main effects training epoch: 256, train loss: 0.51037, val loss: 0.48543\n",
      "Main effects training epoch: 257, train loss: 0.51016, val loss: 0.48530\n",
      "Main effects training epoch: 258, train loss: 0.51002, val loss: 0.48522\n",
      "Main effects training epoch: 259, train loss: 0.51008, val loss: 0.48584\n",
      "Main effects training epoch: 260, train loss: 0.50997, val loss: 0.48500\n",
      "Main effects training epoch: 261, train loss: 0.51011, val loss: 0.48601\n",
      "Main effects training epoch: 262, train loss: 0.50999, val loss: 0.48468\n",
      "Main effects training epoch: 263, train loss: 0.50977, val loss: 0.48559\n",
      "Main effects training epoch: 264, train loss: 0.50977, val loss: 0.48386\n",
      "Main effects training epoch: 265, train loss: 0.51007, val loss: 0.48609\n",
      "Main effects training epoch: 266, train loss: 0.51022, val loss: 0.48515\n",
      "Main effects training epoch: 267, train loss: 0.50973, val loss: 0.48656\n",
      "Main effects training epoch: 268, train loss: 0.50976, val loss: 0.48355\n",
      "Main effects training epoch: 269, train loss: 0.50983, val loss: 0.48540\n",
      "Main effects training epoch: 270, train loss: 0.51025, val loss: 0.48481\n",
      "Main effects training epoch: 271, train loss: 0.51014, val loss: 0.48769\n",
      "Main effects training epoch: 272, train loss: 0.50977, val loss: 0.48254\n",
      "Main effects training epoch: 273, train loss: 0.50972, val loss: 0.48592\n",
      "Main effects training epoch: 274, train loss: 0.50965, val loss: 0.48492\n",
      "Main effects training epoch: 275, train loss: 0.50986, val loss: 0.48749\n",
      "Main effects training epoch: 276, train loss: 0.50968, val loss: 0.48393\n",
      "Main effects training epoch: 277, train loss: 0.51047, val loss: 0.48685\n",
      "Main effects training epoch: 278, train loss: 0.50994, val loss: 0.48336\n",
      "Main effects training epoch: 279, train loss: 0.51000, val loss: 0.48606\n",
      "Main effects training epoch: 280, train loss: 0.51026, val loss: 0.48503\n",
      "Main effects training epoch: 281, train loss: 0.50920, val loss: 0.48430\n",
      "Main effects training epoch: 282, train loss: 0.50973, val loss: 0.48414\n",
      "Main effects training epoch: 283, train loss: 0.50992, val loss: 0.48688\n",
      "Main effects training epoch: 284, train loss: 0.50913, val loss: 0.48253\n",
      "Main effects training epoch: 285, train loss: 0.50906, val loss: 0.48657\n",
      "Main effects training epoch: 286, train loss: 0.50843, val loss: 0.48334\n",
      "Main effects training epoch: 287, train loss: 0.50869, val loss: 0.48413\n",
      "Main effects training epoch: 288, train loss: 0.50843, val loss: 0.48272\n",
      "Main effects training epoch: 289, train loss: 0.50828, val loss: 0.48256\n",
      "Main effects training epoch: 290, train loss: 0.50847, val loss: 0.48462\n",
      "Main effects training epoch: 291, train loss: 0.50858, val loss: 0.48382\n",
      "Main effects training epoch: 292, train loss: 0.50939, val loss: 0.48416\n",
      "Main effects training epoch: 293, train loss: 0.50876, val loss: 0.48286\n",
      "Main effects training epoch: 294, train loss: 0.50834, val loss: 0.48376\n",
      "Main effects training epoch: 295, train loss: 0.50794, val loss: 0.48261\n",
      "Main effects training epoch: 296, train loss: 0.50845, val loss: 0.48441\n",
      "Main effects training epoch: 297, train loss: 0.50804, val loss: 0.48219\n",
      "Main effects training epoch: 298, train loss: 0.50793, val loss: 0.48343\n",
      "Main effects training epoch: 299, train loss: 0.50829, val loss: 0.48266\n",
      "Main effects training epoch: 300, train loss: 0.50801, val loss: 0.48311\n",
      "##########Stage 1: main effect training stop.##########\n",
      "6 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.50878, val loss: 0.48330\n",
      "Main effects tuning epoch: 2, train loss: 0.50888, val loss: 0.48365\n",
      "Main effects tuning epoch: 3, train loss: 0.50851, val loss: 0.48428\n",
      "Main effects tuning epoch: 4, train loss: 0.50837, val loss: 0.48402\n",
      "Main effects tuning epoch: 5, train loss: 0.50849, val loss: 0.48269\n",
      "Main effects tuning epoch: 6, train loss: 0.50841, val loss: 0.48362\n",
      "Main effects tuning epoch: 7, train loss: 0.50904, val loss: 0.48304\n",
      "Main effects tuning epoch: 8, train loss: 0.50859, val loss: 0.48454\n",
      "Main effects tuning epoch: 9, train loss: 0.50910, val loss: 0.48440\n",
      "Main effects tuning epoch: 10, train loss: 0.50906, val loss: 0.48514\n",
      "Main effects tuning epoch: 11, train loss: 0.50863, val loss: 0.48239\n",
      "Main effects tuning epoch: 12, train loss: 0.50818, val loss: 0.48282\n",
      "Main effects tuning epoch: 13, train loss: 0.50788, val loss: 0.48348\n",
      "Main effects tuning epoch: 14, train loss: 0.50836, val loss: 0.48261\n",
      "Main effects tuning epoch: 15, train loss: 0.50816, val loss: 0.48404\n",
      "Main effects tuning epoch: 16, train loss: 0.50810, val loss: 0.48226\n",
      "Main effects tuning epoch: 17, train loss: 0.50797, val loss: 0.48238\n",
      "Main effects tuning epoch: 18, train loss: 0.50790, val loss: 0.48307\n",
      "Main effects tuning epoch: 19, train loss: 0.50805, val loss: 0.48265\n",
      "Main effects tuning epoch: 20, train loss: 0.50790, val loss: 0.48273\n",
      "Main effects tuning epoch: 21, train loss: 0.50751, val loss: 0.48219\n",
      "Main effects tuning epoch: 22, train loss: 0.50797, val loss: 0.48296\n",
      "Main effects tuning epoch: 23, train loss: 0.50787, val loss: 0.48319\n",
      "Main effects tuning epoch: 24, train loss: 0.50790, val loss: 0.48053\n",
      "Main effects tuning epoch: 25, train loss: 0.50772, val loss: 0.48334\n",
      "Main effects tuning epoch: 26, train loss: 0.50801, val loss: 0.48350\n",
      "Main effects tuning epoch: 27, train loss: 0.50770, val loss: 0.48243\n",
      "Main effects tuning epoch: 28, train loss: 0.50761, val loss: 0.48209\n",
      "Main effects tuning epoch: 29, train loss: 0.50821, val loss: 0.48052\n",
      "Main effects tuning epoch: 30, train loss: 0.50760, val loss: 0.48426\n",
      "Main effects tuning epoch: 31, train loss: 0.50777, val loss: 0.48287\n",
      "Main effects tuning epoch: 32, train loss: 0.50770, val loss: 0.48210\n",
      "Main effects tuning epoch: 33, train loss: 0.50778, val loss: 0.48131\n",
      "Main effects tuning epoch: 34, train loss: 0.50784, val loss: 0.48268\n",
      "Main effects tuning epoch: 35, train loss: 0.50742, val loss: 0.48333\n",
      "Main effects tuning epoch: 36, train loss: 0.50831, val loss: 0.48349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects tuning epoch: 37, train loss: 0.50753, val loss: 0.48178\n",
      "Main effects tuning epoch: 38, train loss: 0.50717, val loss: 0.48116\n",
      "Main effects tuning epoch: 39, train loss: 0.50745, val loss: 0.48326\n",
      "Main effects tuning epoch: 40, train loss: 0.50722, val loss: 0.48308\n",
      "Main effects tuning epoch: 41, train loss: 0.50737, val loss: 0.48003\n",
      "Main effects tuning epoch: 42, train loss: 0.50741, val loss: 0.48380\n",
      "Main effects tuning epoch: 43, train loss: 0.50776, val loss: 0.48034\n",
      "Main effects tuning epoch: 44, train loss: 0.50782, val loss: 0.48515\n",
      "Main effects tuning epoch: 45, train loss: 0.50765, val loss: 0.48079\n",
      "Main effects tuning epoch: 46, train loss: 0.50751, val loss: 0.48264\n",
      "Main effects tuning epoch: 47, train loss: 0.50723, val loss: 0.48099\n",
      "Main effects tuning epoch: 48, train loss: 0.50714, val loss: 0.48225\n",
      "Main effects tuning epoch: 49, train loss: 0.50736, val loss: 0.48232\n",
      "Main effects tuning epoch: 50, train loss: 0.50733, val loss: 0.48213\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.46986, val loss: 0.45074\n",
      "Interaction training epoch: 2, train loss: 0.38934, val loss: 0.40292\n",
      "Interaction training epoch: 3, train loss: 0.33970, val loss: 0.34585\n",
      "Interaction training epoch: 4, train loss: 0.30589, val loss: 0.31508\n",
      "Interaction training epoch: 5, train loss: 0.30131, val loss: 0.30922\n",
      "Interaction training epoch: 6, train loss: 0.29611, val loss: 0.30582\n",
      "Interaction training epoch: 7, train loss: 0.29437, val loss: 0.30801\n",
      "Interaction training epoch: 8, train loss: 0.29330, val loss: 0.30879\n",
      "Interaction training epoch: 9, train loss: 0.28488, val loss: 0.29540\n",
      "Interaction training epoch: 10, train loss: 0.28500, val loss: 0.29197\n",
      "Interaction training epoch: 11, train loss: 0.29151, val loss: 0.30120\n",
      "Interaction training epoch: 12, train loss: 0.28300, val loss: 0.29286\n",
      "Interaction training epoch: 13, train loss: 0.27812, val loss: 0.28951\n",
      "Interaction training epoch: 14, train loss: 0.28621, val loss: 0.29226\n",
      "Interaction training epoch: 15, train loss: 0.28022, val loss: 0.29077\n",
      "Interaction training epoch: 16, train loss: 0.27936, val loss: 0.29166\n",
      "Interaction training epoch: 17, train loss: 0.28102, val loss: 0.29755\n",
      "Interaction training epoch: 18, train loss: 0.27582, val loss: 0.28388\n",
      "Interaction training epoch: 19, train loss: 0.27892, val loss: 0.28510\n",
      "Interaction training epoch: 20, train loss: 0.28262, val loss: 0.29742\n",
      "Interaction training epoch: 21, train loss: 0.27895, val loss: 0.29021\n",
      "Interaction training epoch: 22, train loss: 0.27960, val loss: 0.28914\n",
      "Interaction training epoch: 23, train loss: 0.27748, val loss: 0.29333\n",
      "Interaction training epoch: 24, train loss: 0.27203, val loss: 0.28251\n",
      "Interaction training epoch: 25, train loss: 0.27596, val loss: 0.28283\n",
      "Interaction training epoch: 26, train loss: 0.27590, val loss: 0.28699\n",
      "Interaction training epoch: 27, train loss: 0.27308, val loss: 0.28357\n",
      "Interaction training epoch: 28, train loss: 0.27309, val loss: 0.28418\n",
      "Interaction training epoch: 29, train loss: 0.27541, val loss: 0.28430\n",
      "Interaction training epoch: 30, train loss: 0.27086, val loss: 0.28114\n",
      "Interaction training epoch: 31, train loss: 0.28005, val loss: 0.28759\n",
      "Interaction training epoch: 32, train loss: 0.27625, val loss: 0.28759\n",
      "Interaction training epoch: 33, train loss: 0.26847, val loss: 0.27919\n",
      "Interaction training epoch: 34, train loss: 0.27811, val loss: 0.28896\n",
      "Interaction training epoch: 35, train loss: 0.27341, val loss: 0.28682\n",
      "Interaction training epoch: 36, train loss: 0.27013, val loss: 0.28175\n",
      "Interaction training epoch: 37, train loss: 0.26980, val loss: 0.27882\n",
      "Interaction training epoch: 38, train loss: 0.27451, val loss: 0.28858\n",
      "Interaction training epoch: 39, train loss: 0.26995, val loss: 0.28310\n",
      "Interaction training epoch: 40, train loss: 0.26870, val loss: 0.27959\n",
      "Interaction training epoch: 41, train loss: 0.27004, val loss: 0.28376\n",
      "Interaction training epoch: 42, train loss: 0.27217, val loss: 0.28153\n",
      "Interaction training epoch: 43, train loss: 0.27237, val loss: 0.28525\n",
      "Interaction training epoch: 44, train loss: 0.27093, val loss: 0.28690\n",
      "Interaction training epoch: 45, train loss: 0.27116, val loss: 0.28087\n",
      "Interaction training epoch: 46, train loss: 0.26736, val loss: 0.28017\n",
      "Interaction training epoch: 47, train loss: 0.26949, val loss: 0.28327\n",
      "Interaction training epoch: 48, train loss: 0.26752, val loss: 0.27817\n",
      "Interaction training epoch: 49, train loss: 0.26372, val loss: 0.27487\n",
      "Interaction training epoch: 50, train loss: 0.26202, val loss: 0.27693\n",
      "Interaction training epoch: 51, train loss: 0.27247, val loss: 0.28476\n",
      "Interaction training epoch: 52, train loss: 0.26470, val loss: 0.28051\n",
      "Interaction training epoch: 53, train loss: 0.26667, val loss: 0.28047\n",
      "Interaction training epoch: 54, train loss: 0.26552, val loss: 0.27822\n",
      "Interaction training epoch: 55, train loss: 0.26607, val loss: 0.28112\n",
      "Interaction training epoch: 56, train loss: 0.26544, val loss: 0.27620\n",
      "Interaction training epoch: 57, train loss: 0.27127, val loss: 0.28621\n",
      "Interaction training epoch: 58, train loss: 0.26385, val loss: 0.27960\n",
      "Interaction training epoch: 59, train loss: 0.26334, val loss: 0.27669\n",
      "Interaction training epoch: 60, train loss: 0.26236, val loss: 0.27825\n",
      "Interaction training epoch: 61, train loss: 0.26347, val loss: 0.28094\n",
      "Interaction training epoch: 62, train loss: 0.26215, val loss: 0.27782\n",
      "Interaction training epoch: 63, train loss: 0.26357, val loss: 0.27830\n",
      "Interaction training epoch: 64, train loss: 0.26475, val loss: 0.28268\n",
      "Interaction training epoch: 65, train loss: 0.26299, val loss: 0.27672\n",
      "Interaction training epoch: 66, train loss: 0.26991, val loss: 0.28696\n",
      "Interaction training epoch: 67, train loss: 0.26299, val loss: 0.27158\n",
      "Interaction training epoch: 68, train loss: 0.26655, val loss: 0.28435\n",
      "Interaction training epoch: 69, train loss: 0.26127, val loss: 0.27913\n",
      "Interaction training epoch: 70, train loss: 0.25968, val loss: 0.27210\n",
      "Interaction training epoch: 71, train loss: 0.26461, val loss: 0.27995\n",
      "Interaction training epoch: 72, train loss: 0.26338, val loss: 0.27959\n",
      "Interaction training epoch: 73, train loss: 0.25939, val loss: 0.27512\n",
      "Interaction training epoch: 74, train loss: 0.25909, val loss: 0.27260\n",
      "Interaction training epoch: 75, train loss: 0.25830, val loss: 0.27621\n",
      "Interaction training epoch: 76, train loss: 0.26224, val loss: 0.27842\n",
      "Interaction training epoch: 77, train loss: 0.26374, val loss: 0.27872\n",
      "Interaction training epoch: 78, train loss: 0.26385, val loss: 0.27910\n",
      "Interaction training epoch: 79, train loss: 0.25804, val loss: 0.27754\n",
      "Interaction training epoch: 80, train loss: 0.25544, val loss: 0.26911\n",
      "Interaction training epoch: 81, train loss: 0.25929, val loss: 0.27865\n",
      "Interaction training epoch: 82, train loss: 0.26080, val loss: 0.27478\n",
      "Interaction training epoch: 83, train loss: 0.25739, val loss: 0.27493\n",
      "Interaction training epoch: 84, train loss: 0.25492, val loss: 0.26942\n",
      "Interaction training epoch: 85, train loss: 0.25387, val loss: 0.27023\n",
      "Interaction training epoch: 86, train loss: 0.25557, val loss: 0.27317\n",
      "Interaction training epoch: 87, train loss: 0.25899, val loss: 0.27930\n",
      "Interaction training epoch: 88, train loss: 0.25210, val loss: 0.26965\n",
      "Interaction training epoch: 89, train loss: 0.25811, val loss: 0.27293\n",
      "Interaction training epoch: 90, train loss: 0.25565, val loss: 0.27485\n",
      "Interaction training epoch: 91, train loss: 0.25406, val loss: 0.27181\n",
      "Interaction training epoch: 92, train loss: 0.25918, val loss: 0.27839\n",
      "Interaction training epoch: 93, train loss: 0.25430, val loss: 0.27096\n",
      "Interaction training epoch: 94, train loss: 0.25364, val loss: 0.27325\n",
      "Interaction training epoch: 95, train loss: 0.25640, val loss: 0.27259\n",
      "Interaction training epoch: 96, train loss: 0.25226, val loss: 0.27265\n",
      "Interaction training epoch: 97, train loss: 0.25323, val loss: 0.27116\n",
      "Interaction training epoch: 98, train loss: 0.25324, val loss: 0.26912\n",
      "Interaction training epoch: 99, train loss: 0.25640, val loss: 0.27925\n",
      "Interaction training epoch: 100, train loss: 0.25174, val loss: 0.26916\n",
      "Interaction training epoch: 101, train loss: 0.25522, val loss: 0.27091\n",
      "Interaction training epoch: 102, train loss: 0.24973, val loss: 0.27094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 103, train loss: 0.25175, val loss: 0.26563\n",
      "Interaction training epoch: 104, train loss: 0.25246, val loss: 0.27555\n",
      "Interaction training epoch: 105, train loss: 0.25041, val loss: 0.26922\n",
      "Interaction training epoch: 106, train loss: 0.25349, val loss: 0.27122\n",
      "Interaction training epoch: 107, train loss: 0.25425, val loss: 0.27402\n",
      "Interaction training epoch: 108, train loss: 0.25139, val loss: 0.26708\n",
      "Interaction training epoch: 109, train loss: 0.25453, val loss: 0.27381\n",
      "Interaction training epoch: 110, train loss: 0.25001, val loss: 0.27087\n",
      "Interaction training epoch: 111, train loss: 0.25111, val loss: 0.26772\n",
      "Interaction training epoch: 112, train loss: 0.25190, val loss: 0.27301\n",
      "Interaction training epoch: 113, train loss: 0.24793, val loss: 0.26750\n",
      "Interaction training epoch: 114, train loss: 0.25727, val loss: 0.27526\n",
      "Interaction training epoch: 115, train loss: 0.25035, val loss: 0.27687\n",
      "Interaction training epoch: 116, train loss: 0.24868, val loss: 0.26580\n",
      "Interaction training epoch: 117, train loss: 0.24681, val loss: 0.26836\n",
      "Interaction training epoch: 118, train loss: 0.24348, val loss: 0.25958\n",
      "Interaction training epoch: 119, train loss: 0.24896, val loss: 0.26984\n",
      "Interaction training epoch: 120, train loss: 0.24988, val loss: 0.26693\n",
      "Interaction training epoch: 121, train loss: 0.24620, val loss: 0.26687\n",
      "Interaction training epoch: 122, train loss: 0.24636, val loss: 0.26815\n",
      "Interaction training epoch: 123, train loss: 0.25637, val loss: 0.27790\n",
      "Interaction training epoch: 124, train loss: 0.24255, val loss: 0.26006\n",
      "Interaction training epoch: 125, train loss: 0.25583, val loss: 0.27504\n",
      "Interaction training epoch: 126, train loss: 0.24600, val loss: 0.26532\n",
      "Interaction training epoch: 127, train loss: 0.24545, val loss: 0.25913\n",
      "Interaction training epoch: 128, train loss: 0.24431, val loss: 0.26713\n",
      "Interaction training epoch: 129, train loss: 0.24464, val loss: 0.26464\n",
      "Interaction training epoch: 130, train loss: 0.24474, val loss: 0.26238\n",
      "Interaction training epoch: 131, train loss: 0.24311, val loss: 0.26228\n",
      "Interaction training epoch: 132, train loss: 0.24045, val loss: 0.26210\n",
      "Interaction training epoch: 133, train loss: 0.25285, val loss: 0.27241\n",
      "Interaction training epoch: 134, train loss: 0.24476, val loss: 0.26794\n",
      "Interaction training epoch: 135, train loss: 0.24598, val loss: 0.26272\n",
      "Interaction training epoch: 136, train loss: 0.24498, val loss: 0.26234\n",
      "Interaction training epoch: 137, train loss: 0.24492, val loss: 0.26338\n",
      "Interaction training epoch: 138, train loss: 0.24698, val loss: 0.26449\n",
      "Interaction training epoch: 139, train loss: 0.24467, val loss: 0.26888\n",
      "Interaction training epoch: 140, train loss: 0.24028, val loss: 0.25547\n",
      "Interaction training epoch: 141, train loss: 0.24457, val loss: 0.26482\n",
      "Interaction training epoch: 142, train loss: 0.24255, val loss: 0.26409\n",
      "Interaction training epoch: 143, train loss: 0.24131, val loss: 0.25929\n",
      "Interaction training epoch: 144, train loss: 0.24422, val loss: 0.26287\n",
      "Interaction training epoch: 145, train loss: 0.25501, val loss: 0.27896\n",
      "Interaction training epoch: 146, train loss: 0.23780, val loss: 0.25635\n",
      "Interaction training epoch: 147, train loss: 0.24362, val loss: 0.26268\n",
      "Interaction training epoch: 148, train loss: 0.24254, val loss: 0.26189\n",
      "Interaction training epoch: 149, train loss: 0.24174, val loss: 0.26245\n",
      "Interaction training epoch: 150, train loss: 0.24147, val loss: 0.25888\n",
      "Interaction training epoch: 151, train loss: 0.24048, val loss: 0.26285\n",
      "Interaction training epoch: 152, train loss: 0.24268, val loss: 0.26332\n",
      "Interaction training epoch: 153, train loss: 0.24337, val loss: 0.25968\n",
      "Interaction training epoch: 154, train loss: 0.24337, val loss: 0.26815\n",
      "Interaction training epoch: 155, train loss: 0.24218, val loss: 0.25415\n",
      "Interaction training epoch: 156, train loss: 0.24500, val loss: 0.26643\n",
      "Interaction training epoch: 157, train loss: 0.24541, val loss: 0.26394\n",
      "Interaction training epoch: 158, train loss: 0.24644, val loss: 0.26373\n",
      "Interaction training epoch: 159, train loss: 0.23998, val loss: 0.25975\n",
      "Interaction training epoch: 160, train loss: 0.23893, val loss: 0.25773\n",
      "Interaction training epoch: 161, train loss: 0.24215, val loss: 0.25810\n",
      "Interaction training epoch: 162, train loss: 0.24095, val loss: 0.26766\n",
      "Interaction training epoch: 163, train loss: 0.24019, val loss: 0.25694\n",
      "Interaction training epoch: 164, train loss: 0.24301, val loss: 0.26274\n",
      "Interaction training epoch: 165, train loss: 0.23725, val loss: 0.26160\n",
      "Interaction training epoch: 166, train loss: 0.24040, val loss: 0.25506\n",
      "Interaction training epoch: 167, train loss: 0.24343, val loss: 0.26068\n",
      "Interaction training epoch: 168, train loss: 0.23954, val loss: 0.25908\n",
      "Interaction training epoch: 169, train loss: 0.24196, val loss: 0.26234\n",
      "Interaction training epoch: 170, train loss: 0.24871, val loss: 0.26579\n",
      "Interaction training epoch: 171, train loss: 0.23887, val loss: 0.26225\n",
      "Interaction training epoch: 172, train loss: 0.24095, val loss: 0.25690\n",
      "Interaction training epoch: 173, train loss: 0.23821, val loss: 0.25809\n",
      "Interaction training epoch: 174, train loss: 0.23807, val loss: 0.25896\n",
      "Interaction training epoch: 175, train loss: 0.23638, val loss: 0.26177\n",
      "Interaction training epoch: 176, train loss: 0.23823, val loss: 0.25343\n",
      "Interaction training epoch: 177, train loss: 0.23888, val loss: 0.26592\n",
      "Interaction training epoch: 178, train loss: 0.24047, val loss: 0.25263\n",
      "Interaction training epoch: 179, train loss: 0.23661, val loss: 0.25780\n",
      "Interaction training epoch: 180, train loss: 0.23808, val loss: 0.26032\n",
      "Interaction training epoch: 181, train loss: 0.23896, val loss: 0.26213\n",
      "Interaction training epoch: 182, train loss: 0.23920, val loss: 0.25267\n",
      "Interaction training epoch: 183, train loss: 0.24215, val loss: 0.26862\n",
      "Interaction training epoch: 184, train loss: 0.23724, val loss: 0.25706\n",
      "Interaction training epoch: 185, train loss: 0.24047, val loss: 0.26085\n",
      "Interaction training epoch: 186, train loss: 0.23801, val loss: 0.25789\n",
      "Interaction training epoch: 187, train loss: 0.23648, val loss: 0.25821\n",
      "Interaction training epoch: 188, train loss: 0.23473, val loss: 0.25458\n",
      "Interaction training epoch: 189, train loss: 0.23397, val loss: 0.25622\n",
      "Interaction training epoch: 190, train loss: 0.24284, val loss: 0.26013\n",
      "Interaction training epoch: 191, train loss: 0.23847, val loss: 0.25912\n",
      "Interaction training epoch: 192, train loss: 0.23421, val loss: 0.25582\n",
      "Interaction training epoch: 193, train loss: 0.24150, val loss: 0.26394\n",
      "Interaction training epoch: 194, train loss: 0.23510, val loss: 0.25769\n",
      "Interaction training epoch: 195, train loss: 0.23896, val loss: 0.25670\n",
      "Interaction training epoch: 196, train loss: 0.23568, val loss: 0.25737\n",
      "Interaction training epoch: 197, train loss: 0.23640, val loss: 0.26102\n",
      "Interaction training epoch: 198, train loss: 0.23406, val loss: 0.25641\n",
      "Interaction training epoch: 199, train loss: 0.23637, val loss: 0.25972\n",
      "Interaction training epoch: 200, train loss: 0.23824, val loss: 0.25940\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########2 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.23723, val loss: 0.25751\n",
      "Interaction tuning epoch: 2, train loss: 0.23831, val loss: 0.26047\n",
      "Interaction tuning epoch: 3, train loss: 0.23645, val loss: 0.25350\n",
      "Interaction tuning epoch: 4, train loss: 0.23524, val loss: 0.25358\n",
      "Interaction tuning epoch: 5, train loss: 0.23582, val loss: 0.25799\n",
      "Interaction tuning epoch: 6, train loss: 0.23787, val loss: 0.25857\n",
      "Interaction tuning epoch: 7, train loss: 0.23821, val loss: 0.25942\n",
      "Interaction tuning epoch: 8, train loss: 0.23514, val loss: 0.25289\n",
      "Interaction tuning epoch: 9, train loss: 0.23548, val loss: 0.25462\n",
      "Interaction tuning epoch: 10, train loss: 0.23511, val loss: 0.25254\n",
      "Interaction tuning epoch: 11, train loss: 0.24092, val loss: 0.26264\n",
      "Interaction tuning epoch: 12, train loss: 0.23319, val loss: 0.25139\n",
      "Interaction tuning epoch: 13, train loss: 0.23511, val loss: 0.25590\n",
      "Interaction tuning epoch: 14, train loss: 0.23598, val loss: 0.25345\n",
      "Interaction tuning epoch: 15, train loss: 0.24531, val loss: 0.26573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction tuning epoch: 16, train loss: 0.23698, val loss: 0.25653\n",
      "Interaction tuning epoch: 17, train loss: 0.23473, val loss: 0.26013\n",
      "Interaction tuning epoch: 18, train loss: 0.23363, val loss: 0.25206\n",
      "Interaction tuning epoch: 19, train loss: 0.24119, val loss: 0.26252\n",
      "Interaction tuning epoch: 20, train loss: 0.24950, val loss: 0.26910\n",
      "Interaction tuning epoch: 21, train loss: 0.24292, val loss: 0.26189\n",
      "Interaction tuning epoch: 22, train loss: 0.23371, val loss: 0.25455\n",
      "Interaction tuning epoch: 23, train loss: 0.23168, val loss: 0.25125\n",
      "Interaction tuning epoch: 24, train loss: 0.23621, val loss: 0.25631\n",
      "Interaction tuning epoch: 25, train loss: 0.23010, val loss: 0.25083\n",
      "Interaction tuning epoch: 26, train loss: 0.23797, val loss: 0.25449\n",
      "Interaction tuning epoch: 27, train loss: 0.24458, val loss: 0.27031\n",
      "Interaction tuning epoch: 28, train loss: 0.23540, val loss: 0.25282\n",
      "Interaction tuning epoch: 29, train loss: 0.22983, val loss: 0.25146\n",
      "Interaction tuning epoch: 30, train loss: 0.23539, val loss: 0.25849\n",
      "Interaction tuning epoch: 31, train loss: 0.23621, val loss: 0.25432\n",
      "Interaction tuning epoch: 32, train loss: 0.23564, val loss: 0.25814\n",
      "Interaction tuning epoch: 33, train loss: 0.23167, val loss: 0.25406\n",
      "Interaction tuning epoch: 34, train loss: 0.24298, val loss: 0.26146\n",
      "Interaction tuning epoch: 35, train loss: 0.23218, val loss: 0.25179\n",
      "Interaction tuning epoch: 36, train loss: 0.25537, val loss: 0.27729\n",
      "Interaction tuning epoch: 37, train loss: 0.24638, val loss: 0.26776\n",
      "Interaction tuning epoch: 38, train loss: 0.23279, val loss: 0.25583\n",
      "Interaction tuning epoch: 39, train loss: 0.23193, val loss: 0.25062\n",
      "Interaction tuning epoch: 40, train loss: 0.23792, val loss: 0.26263\n",
      "Interaction tuning epoch: 41, train loss: 0.23944, val loss: 0.25564\n",
      "Interaction tuning epoch: 42, train loss: 0.23469, val loss: 0.25666\n",
      "Interaction tuning epoch: 43, train loss: 0.24967, val loss: 0.26965\n",
      "Interaction tuning epoch: 44, train loss: 0.23467, val loss: 0.25808\n",
      "Interaction tuning epoch: 45, train loss: 0.23354, val loss: 0.25370\n",
      "Interaction tuning epoch: 46, train loss: 0.26114, val loss: 0.28222\n",
      "Interaction tuning epoch: 47, train loss: 0.23019, val loss: 0.25100\n",
      "Interaction tuning epoch: 48, train loss: 0.23430, val loss: 0.25529\n",
      "Interaction tuning epoch: 49, train loss: 0.23167, val loss: 0.25059\n",
      "Interaction tuning epoch: 50, train loss: 0.23843, val loss: 0.26423\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 65.3522937297821\n",
      "After the gam stage, training error is 0.23843 , validation error is 0.26423\n",
      "missing value counts: 99241\n",
      "[SoftImpute] Max Singular Value of X_init = 3.709529\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.217774 validation BCE=0.261911,rank=3\n",
      "[SoftImpute] Iter 1: observed BCE=0.236056 validation BCE=0.266736,rank=3\n",
      "[SoftImpute] Iter 2: observed BCE=0.236473 validation BCE=0.264933,rank=3\n",
      "[SoftImpute] Iter 3: observed BCE=0.236360 validation BCE=0.263278,rank=3\n",
      "[SoftImpute] Iter 4: observed BCE=0.236826 validation BCE=0.262497,rank=3\n",
      "[SoftImpute] Iter 5: observed BCE=0.237821 validation BCE=0.261590,rank=3\n",
      "[SoftImpute] Iter 6: observed BCE=0.237709 validation BCE=0.260442,rank=3\n",
      "[SoftImpute] Iter 7: observed BCE=0.237818 validation BCE=0.259949,rank=3\n",
      "[SoftImpute] Iter 8: observed BCE=0.238093 validation BCE=0.259256,rank=3\n",
      "[SoftImpute] Iter 9: observed BCE=0.238065 validation BCE=0.258475,rank=3\n",
      "[SoftImpute] Iter 10: observed BCE=0.238335 validation BCE=0.258030,rank=3\n",
      "[SoftImpute] Iter 11: observed BCE=0.239530 validation BCE=0.258460,rank=3\n",
      "[SoftImpute] Iter 12: observed BCE=0.239768 validation BCE=0.258824,rank=3\n",
      "[SoftImpute] Iter 13: observed BCE=0.239758 validation BCE=0.258490,rank=3\n",
      "[SoftImpute] Iter 14: observed BCE=0.240237 validation BCE=0.258827,rank=3\n",
      "[SoftImpute] Iter 15: observed BCE=0.240516 validation BCE=0.257828,rank=3\n",
      "[SoftImpute] Iter 16: observed BCE=0.241985 validation BCE=0.257314,rank=3\n",
      "[SoftImpute] Iter 17: observed BCE=0.242150 validation BCE=0.257146,rank=3\n",
      "[SoftImpute] Iter 18: observed BCE=0.243096 validation BCE=0.257211,rank=3\n",
      "[SoftImpute] Iter 19: observed BCE=0.243971 validation BCE=0.257010,rank=3\n",
      "[SoftImpute] Iter 20: observed BCE=0.244158 validation BCE=0.256912,rank=3\n",
      "[SoftImpute] Iter 21: observed BCE=0.244372 validation BCE=0.256839,rank=3\n",
      "[SoftImpute] Iter 22: observed BCE=0.244615 validation BCE=0.256793,rank=3\n",
      "[SoftImpute] Iter 23: observed BCE=0.244888 validation BCE=0.256772,rank=3\n",
      "[SoftImpute] Iter 24: observed BCE=0.245199 validation BCE=0.256773,rank=3\n",
      "[SoftImpute] Iter 25: observed BCE=0.245575 validation BCE=0.256792,rank=3\n",
      "[SoftImpute] Iter 26: observed BCE=0.247221 validation BCE=0.256829,rank=3\n",
      "[SoftImpute] Iter 27: observed BCE=0.247534 validation BCE=0.256885,rank=3\n",
      "[SoftImpute] Iter 28: observed BCE=0.247873 validation BCE=0.256964,rank=3\n",
      "[SoftImpute] Iter 29: observed BCE=0.248318 validation BCE=0.257036,rank=3\n",
      "[SoftImpute] Iter 30: observed BCE=0.248293 validation BCE=0.257226,rank=3\n",
      "[SoftImpute] Iter 31: observed BCE=0.249684 validation BCE=0.257333,rank=3\n",
      "[SoftImpute] Iter 32: observed BCE=0.250015 validation BCE=0.257455,rank=3\n",
      "[SoftImpute] Iter 33: observed BCE=0.250360 validation BCE=0.257594,rank=3\n",
      "[SoftImpute] Iter 34: observed BCE=0.250718 validation BCE=0.257747,rank=3\n",
      "[SoftImpute] Iter 35: observed BCE=0.251089 validation BCE=0.257917,rank=3\n",
      "[SoftImpute] Iter 36: observed BCE=0.250562 validation BCE=0.259161,rank=3\n",
      "[SoftImpute] Iter 37: observed BCE=0.251061 validation BCE=0.259377,rank=3\n",
      "[SoftImpute] Iter 38: observed BCE=0.251665 validation BCE=0.259593,rank=3\n",
      "[SoftImpute] Iter 39: observed BCE=0.253360 validation BCE=0.259835,rank=3\n",
      "[SoftImpute] Iter 40: observed BCE=0.254470 validation BCE=0.258406,rank=3\n",
      "[SoftImpute] Iter 41: observed BCE=0.254931 validation BCE=0.258662,rank=3\n",
      "[SoftImpute] Iter 42: observed BCE=0.255628 validation BCE=0.258925,rank=3\n",
      "[SoftImpute] Iter 43: observed BCE=0.257131 validation BCE=0.259186,rank=3\n",
      "[SoftImpute] Iter 44: observed BCE=0.257624 validation BCE=0.259460,rank=3\n",
      "[SoftImpute] Iter 45: observed BCE=0.258160 validation BCE=0.259746,rank=3\n",
      "[SoftImpute] Iter 46: observed BCE=0.258844 validation BCE=0.260046,rank=3\n",
      "[SoftImpute] Iter 47: observed BCE=0.260392 validation BCE=0.260360,rank=3\n",
      "[SoftImpute] Iter 48: observed BCE=0.260885 validation BCE=0.260687,rank=3\n",
      "[SoftImpute] Iter 49: observed BCE=0.261402 validation BCE=0.261031,rank=3\n",
      "[SoftImpute] Iter 50: observed BCE=0.261962 validation BCE=0.261394,rank=3\n",
      "[SoftImpute] Iter 51: observed BCE=0.262717 validation BCE=0.261780,rank=3\n",
      "[SoftImpute] Iter 52: observed BCE=0.264208 validation BCE=0.262193,rank=3\n",
      "[SoftImpute] Iter 53: observed BCE=0.264722 validation BCE=0.262638,rank=3\n",
      "[SoftImpute] Iter 54: observed BCE=0.265259 validation BCE=0.263130,rank=3\n",
      "[SoftImpute] Iter 55: observed BCE=0.265839 validation BCE=0.263693,rank=3\n",
      "[SoftImpute] Iter 56: observed BCE=0.266542 validation BCE=0.264384,rank=3\n",
      "[SoftImpute] Iter 57: observed BCE=0.268305 validation BCE=0.265386,rank=3\n",
      "[SoftImpute] Iter 58: observed BCE=0.270016 validation BCE=0.274787,rank=3\n",
      "[SoftImpute] Iter 59: observed BCE=0.270501 validation BCE=0.275077,rank=3\n",
      "[SoftImpute] Iter 60: observed BCE=0.270992 validation BCE=0.275370,rank=3\n",
      "[SoftImpute] Iter 61: observed BCE=0.271490 validation BCE=0.275667,rank=3\n",
      "[SoftImpute] Iter 62: observed BCE=0.271996 validation BCE=0.275968,rank=3\n",
      "[SoftImpute] Iter 63: observed BCE=0.272509 validation BCE=0.276274,rank=3\n",
      "[SoftImpute] Iter 64: observed BCE=0.273033 validation BCE=0.276583,rank=3\n",
      "[SoftImpute] Iter 65: observed BCE=0.273570 validation BCE=0.276895,rank=3\n",
      "[SoftImpute] Iter 66: observed BCE=0.274125 validation BCE=0.277210,rank=3\n",
      "[SoftImpute] Iter 67: observed BCE=0.274708 validation BCE=0.277529,rank=3\n",
      "[SoftImpute] Iter 68: observed BCE=0.275351 validation BCE=0.277852,rank=3\n",
      "[SoftImpute] Iter 69: observed BCE=0.277136 validation BCE=0.278177,rank=3\n",
      "[SoftImpute] Iter 70: observed BCE=0.277814 validation BCE=0.278507,rank=3\n",
      "[SoftImpute] Iter 71: observed BCE=0.279420 validation BCE=0.278839,rank=3\n",
      "[SoftImpute] Iter 72: observed BCE=0.279926 validation BCE=0.279176,rank=3\n",
      "[SoftImpute] Iter 73: observed BCE=0.280520 validation BCE=0.279114,rank=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 74: observed BCE=0.281024 validation BCE=0.279460,rank=3\n",
      "[SoftImpute] Iter 75: observed BCE=0.281531 validation BCE=0.279807,rank=3\n",
      "[SoftImpute] Iter 76: observed BCE=0.282042 validation BCE=0.280157,rank=3\n",
      "[SoftImpute] Iter 77: observed BCE=0.282558 validation BCE=0.280508,rank=3\n",
      "[SoftImpute] Iter 78: observed BCE=0.283078 validation BCE=0.280861,rank=3\n",
      "[SoftImpute] Iter 79: observed BCE=0.283604 validation BCE=0.281216,rank=3\n",
      "[SoftImpute] Iter 80: observed BCE=0.284135 validation BCE=0.281573,rank=3\n",
      "[SoftImpute] Iter 81: observed BCE=0.284671 validation BCE=0.281932,rank=3\n",
      "[SoftImpute] Iter 82: observed BCE=0.285216 validation BCE=0.282293,rank=3\n",
      "[SoftImpute] Iter 83: observed BCE=0.285771 validation BCE=0.282657,rank=3\n",
      "[SoftImpute] Iter 84: observed BCE=0.286340 validation BCE=0.283023,rank=3\n",
      "[SoftImpute] Iter 85: observed BCE=0.286931 validation BCE=0.283390,rank=3\n",
      "[SoftImpute] Iter 86: observed BCE=0.287574 validation BCE=0.283759,rank=3\n",
      "[SoftImpute] Iter 87: observed BCE=0.289299 validation BCE=0.284130,rank=3\n",
      "[SoftImpute] Iter 88: observed BCE=0.289843 validation BCE=0.284504,rank=3\n",
      "[SoftImpute] Iter 89: observed BCE=0.290394 validation BCE=0.284879,rank=3\n",
      "[SoftImpute] Iter 90: observed BCE=0.290952 validation BCE=0.285257,rank=3\n",
      "[SoftImpute] Iter 91: observed BCE=0.291519 validation BCE=0.285636,rank=3\n",
      "[SoftImpute] Iter 92: observed BCE=0.292100 validation BCE=0.286015,rank=3\n",
      "[SoftImpute] Iter 93: observed BCE=0.292702 validation BCE=0.286396,rank=3\n",
      "[SoftImpute] Iter 94: observed BCE=0.293352 validation BCE=0.286779,rank=3\n",
      "[SoftImpute] Iter 95: observed BCE=0.295103 validation BCE=0.287162,rank=3\n",
      "[SoftImpute] Iter 96: observed BCE=0.295662 validation BCE=0.287546,rank=3\n",
      "[SoftImpute] Iter 97: observed BCE=0.296226 validation BCE=0.287930,rank=3\n",
      "[SoftImpute] Iter 98: observed BCE=0.296799 validation BCE=0.288316,rank=3\n",
      "[SoftImpute] Iter 99: observed BCE=0.297382 validation BCE=0.288702,rank=3\n",
      "[SoftImpute] Iter 100: observed BCE=0.297981 validation BCE=0.289088,rank=3\n",
      "[SoftImpute] Stopped after iteration 100 for lambda=0.074191\n",
      "final num of user group: 14\n",
      "final num of item group: 7\n",
      "change mode state : True\n",
      "time cost: 11.969252109527588\n",
      "After the matrix factor stage, training error is 0.29798, validation error is 0.28909\n",
      "1\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68158, val loss: 0.68679\n",
      "Main effects training epoch: 2, train loss: 0.67754, val loss: 0.68624\n",
      "Main effects training epoch: 3, train loss: 0.67263, val loss: 0.67940\n",
      "Main effects training epoch: 4, train loss: 0.66755, val loss: 0.67461\n",
      "Main effects training epoch: 5, train loss: 0.65772, val loss: 0.66465\n",
      "Main effects training epoch: 6, train loss: 0.63824, val loss: 0.64489\n",
      "Main effects training epoch: 7, train loss: 0.59904, val loss: 0.60330\n",
      "Main effects training epoch: 8, train loss: 0.55095, val loss: 0.55055\n",
      "Main effects training epoch: 9, train loss: 0.53072, val loss: 0.52691\n",
      "Main effects training epoch: 10, train loss: 0.52682, val loss: 0.51885\n",
      "Main effects training epoch: 11, train loss: 0.52510, val loss: 0.51663\n",
      "Main effects training epoch: 12, train loss: 0.52332, val loss: 0.51679\n",
      "Main effects training epoch: 13, train loss: 0.52251, val loss: 0.51987\n",
      "Main effects training epoch: 14, train loss: 0.52416, val loss: 0.52057\n",
      "Main effects training epoch: 15, train loss: 0.52146, val loss: 0.51833\n",
      "Main effects training epoch: 16, train loss: 0.52133, val loss: 0.51657\n",
      "Main effects training epoch: 17, train loss: 0.52088, val loss: 0.51767\n",
      "Main effects training epoch: 18, train loss: 0.52068, val loss: 0.51698\n",
      "Main effects training epoch: 19, train loss: 0.52093, val loss: 0.51864\n",
      "Main effects training epoch: 20, train loss: 0.52114, val loss: 0.51655\n",
      "Main effects training epoch: 21, train loss: 0.52156, val loss: 0.51911\n",
      "Main effects training epoch: 22, train loss: 0.52121, val loss: 0.51650\n",
      "Main effects training epoch: 23, train loss: 0.52058, val loss: 0.51658\n",
      "Main effects training epoch: 24, train loss: 0.52024, val loss: 0.51738\n",
      "Main effects training epoch: 25, train loss: 0.52036, val loss: 0.51745\n",
      "Main effects training epoch: 26, train loss: 0.52015, val loss: 0.51697\n",
      "Main effects training epoch: 27, train loss: 0.52019, val loss: 0.51610\n",
      "Main effects training epoch: 28, train loss: 0.52020, val loss: 0.51841\n",
      "Main effects training epoch: 29, train loss: 0.51995, val loss: 0.51563\n",
      "Main effects training epoch: 30, train loss: 0.52005, val loss: 0.51596\n",
      "Main effects training epoch: 31, train loss: 0.52099, val loss: 0.52000\n",
      "Main effects training epoch: 32, train loss: 0.52078, val loss: 0.51520\n",
      "Main effects training epoch: 33, train loss: 0.52121, val loss: 0.51895\n",
      "Main effects training epoch: 34, train loss: 0.52016, val loss: 0.51790\n",
      "Main effects training epoch: 35, train loss: 0.51957, val loss: 0.51625\n",
      "Main effects training epoch: 36, train loss: 0.51972, val loss: 0.51552\n",
      "Main effects training epoch: 37, train loss: 0.51956, val loss: 0.51531\n",
      "Main effects training epoch: 38, train loss: 0.51950, val loss: 0.51641\n",
      "Main effects training epoch: 39, train loss: 0.51945, val loss: 0.51677\n",
      "Main effects training epoch: 40, train loss: 0.51943, val loss: 0.51685\n",
      "Main effects training epoch: 41, train loss: 0.51961, val loss: 0.51454\n",
      "Main effects training epoch: 42, train loss: 0.51980, val loss: 0.51934\n",
      "Main effects training epoch: 43, train loss: 0.52014, val loss: 0.51564\n",
      "Main effects training epoch: 44, train loss: 0.51936, val loss: 0.51658\n",
      "Main effects training epoch: 45, train loss: 0.51969, val loss: 0.51683\n",
      "Main effects training epoch: 46, train loss: 0.51905, val loss: 0.51593\n",
      "Main effects training epoch: 47, train loss: 0.51944, val loss: 0.51568\n",
      "Main effects training epoch: 48, train loss: 0.51924, val loss: 0.51661\n",
      "Main effects training epoch: 49, train loss: 0.51941, val loss: 0.51788\n",
      "Main effects training epoch: 50, train loss: 0.51964, val loss: 0.51564\n",
      "Main effects training epoch: 51, train loss: 0.52057, val loss: 0.51938\n",
      "Main effects training epoch: 52, train loss: 0.51994, val loss: 0.51453\n",
      "Main effects training epoch: 53, train loss: 0.52001, val loss: 0.51944\n",
      "Main effects training epoch: 54, train loss: 0.52015, val loss: 0.51486\n",
      "Main effects training epoch: 55, train loss: 0.51975, val loss: 0.51970\n",
      "Main effects training epoch: 56, train loss: 0.52004, val loss: 0.51588\n",
      "Main effects training epoch: 57, train loss: 0.51930, val loss: 0.51784\n",
      "Main effects training epoch: 58, train loss: 0.51919, val loss: 0.51450\n",
      "Main effects training epoch: 59, train loss: 0.52030, val loss: 0.52055\n",
      "Main effects training epoch: 60, train loss: 0.51918, val loss: 0.51503\n",
      "Main effects training epoch: 61, train loss: 0.52033, val loss: 0.51935\n",
      "Main effects training epoch: 62, train loss: 0.51860, val loss: 0.51517\n",
      "Main effects training epoch: 63, train loss: 0.51867, val loss: 0.51606\n",
      "Main effects training epoch: 64, train loss: 0.51850, val loss: 0.51720\n",
      "Main effects training epoch: 65, train loss: 0.51823, val loss: 0.51656\n",
      "Main effects training epoch: 66, train loss: 0.51855, val loss: 0.51389\n",
      "Main effects training epoch: 67, train loss: 0.51853, val loss: 0.51897\n",
      "Main effects training epoch: 68, train loss: 0.51841, val loss: 0.51574\n",
      "Main effects training epoch: 69, train loss: 0.51824, val loss: 0.51478\n",
      "Main effects training epoch: 70, train loss: 0.51852, val loss: 0.51751\n",
      "Main effects training epoch: 71, train loss: 0.51873, val loss: 0.51619\n",
      "Main effects training epoch: 72, train loss: 0.51842, val loss: 0.51534\n",
      "Main effects training epoch: 73, train loss: 0.51829, val loss: 0.51735\n",
      "Main effects training epoch: 74, train loss: 0.51806, val loss: 0.51525\n",
      "Main effects training epoch: 75, train loss: 0.51797, val loss: 0.51619\n",
      "Main effects training epoch: 76, train loss: 0.51770, val loss: 0.51537\n",
      "Main effects training epoch: 77, train loss: 0.51775, val loss: 0.51609\n",
      "Main effects training epoch: 78, train loss: 0.51767, val loss: 0.51697\n",
      "Main effects training epoch: 79, train loss: 0.51755, val loss: 0.51510\n",
      "Main effects training epoch: 80, train loss: 0.51786, val loss: 0.51562\n",
      "Main effects training epoch: 81, train loss: 0.51754, val loss: 0.51516\n",
      "Main effects training epoch: 82, train loss: 0.51746, val loss: 0.51547\n",
      "Main effects training epoch: 83, train loss: 0.51772, val loss: 0.51622\n",
      "Main effects training epoch: 84, train loss: 0.51813, val loss: 0.51743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 85, train loss: 0.51758, val loss: 0.51401\n",
      "Main effects training epoch: 86, train loss: 0.51737, val loss: 0.51635\n",
      "Main effects training epoch: 87, train loss: 0.51746, val loss: 0.51628\n",
      "Main effects training epoch: 88, train loss: 0.51712, val loss: 0.51587\n",
      "Main effects training epoch: 89, train loss: 0.51712, val loss: 0.51588\n",
      "Main effects training epoch: 90, train loss: 0.51726, val loss: 0.51555\n",
      "Main effects training epoch: 91, train loss: 0.51721, val loss: 0.51721\n",
      "Main effects training epoch: 92, train loss: 0.51707, val loss: 0.51476\n",
      "Main effects training epoch: 93, train loss: 0.51697, val loss: 0.51644\n",
      "Main effects training epoch: 94, train loss: 0.51685, val loss: 0.51583\n",
      "Main effects training epoch: 95, train loss: 0.51690, val loss: 0.51435\n",
      "Main effects training epoch: 96, train loss: 0.51672, val loss: 0.51614\n",
      "Main effects training epoch: 97, train loss: 0.51664, val loss: 0.51513\n",
      "Main effects training epoch: 98, train loss: 0.51699, val loss: 0.51745\n",
      "Main effects training epoch: 99, train loss: 0.51654, val loss: 0.51435\n",
      "Main effects training epoch: 100, train loss: 0.51645, val loss: 0.51480\n",
      "Main effects training epoch: 101, train loss: 0.51638, val loss: 0.51647\n",
      "Main effects training epoch: 102, train loss: 0.51625, val loss: 0.51599\n",
      "Main effects training epoch: 103, train loss: 0.51656, val loss: 0.51324\n",
      "Main effects training epoch: 104, train loss: 0.51632, val loss: 0.51548\n",
      "Main effects training epoch: 105, train loss: 0.51598, val loss: 0.51593\n",
      "Main effects training epoch: 106, train loss: 0.51624, val loss: 0.51433\n",
      "Main effects training epoch: 107, train loss: 0.51585, val loss: 0.51591\n",
      "Main effects training epoch: 108, train loss: 0.51595, val loss: 0.51727\n",
      "Main effects training epoch: 109, train loss: 0.51592, val loss: 0.51328\n",
      "Main effects training epoch: 110, train loss: 0.51638, val loss: 0.51656\n",
      "Main effects training epoch: 111, train loss: 0.51575, val loss: 0.51439\n",
      "Main effects training epoch: 112, train loss: 0.51553, val loss: 0.51517\n",
      "Main effects training epoch: 113, train loss: 0.51538, val loss: 0.51477\n",
      "Main effects training epoch: 114, train loss: 0.51545, val loss: 0.51534\n",
      "Main effects training epoch: 115, train loss: 0.51537, val loss: 0.51620\n",
      "Main effects training epoch: 116, train loss: 0.51553, val loss: 0.51298\n",
      "Main effects training epoch: 117, train loss: 0.51556, val loss: 0.51566\n",
      "Main effects training epoch: 118, train loss: 0.51544, val loss: 0.51358\n",
      "Main effects training epoch: 119, train loss: 0.51557, val loss: 0.51656\n",
      "Main effects training epoch: 120, train loss: 0.51521, val loss: 0.51633\n",
      "Main effects training epoch: 121, train loss: 0.51498, val loss: 0.51445\n",
      "Main effects training epoch: 122, train loss: 0.51543, val loss: 0.51396\n",
      "Main effects training epoch: 123, train loss: 0.51510, val loss: 0.51558\n",
      "Main effects training epoch: 124, train loss: 0.51512, val loss: 0.51246\n",
      "Main effects training epoch: 125, train loss: 0.51484, val loss: 0.51646\n",
      "Main effects training epoch: 126, train loss: 0.51469, val loss: 0.51348\n",
      "Main effects training epoch: 127, train loss: 0.51472, val loss: 0.51587\n",
      "Main effects training epoch: 128, train loss: 0.51516, val loss: 0.51486\n",
      "Main effects training epoch: 129, train loss: 0.51551, val loss: 0.51702\n",
      "Main effects training epoch: 130, train loss: 0.51463, val loss: 0.51428\n",
      "Main effects training epoch: 131, train loss: 0.51445, val loss: 0.51584\n",
      "Main effects training epoch: 132, train loss: 0.51478, val loss: 0.51529\n",
      "Main effects training epoch: 133, train loss: 0.51476, val loss: 0.51540\n",
      "Main effects training epoch: 134, train loss: 0.51482, val loss: 0.51474\n",
      "Main effects training epoch: 135, train loss: 0.51570, val loss: 0.51493\n",
      "Main effects training epoch: 136, train loss: 0.51518, val loss: 0.51493\n",
      "Main effects training epoch: 137, train loss: 0.51407, val loss: 0.51379\n",
      "Main effects training epoch: 138, train loss: 0.51379, val loss: 0.51586\n",
      "Main effects training epoch: 139, train loss: 0.51381, val loss: 0.51191\n",
      "Main effects training epoch: 140, train loss: 0.51345, val loss: 0.51419\n",
      "Main effects training epoch: 141, train loss: 0.51384, val loss: 0.51637\n",
      "Main effects training epoch: 142, train loss: 0.51365, val loss: 0.51300\n",
      "Main effects training epoch: 143, train loss: 0.51338, val loss: 0.51363\n",
      "Main effects training epoch: 144, train loss: 0.51345, val loss: 0.51347\n",
      "Main effects training epoch: 145, train loss: 0.51367, val loss: 0.51251\n",
      "Main effects training epoch: 146, train loss: 0.51366, val loss: 0.51672\n",
      "Main effects training epoch: 147, train loss: 0.51349, val loss: 0.51237\n",
      "Main effects training epoch: 148, train loss: 0.51307, val loss: 0.51467\n",
      "Main effects training epoch: 149, train loss: 0.51321, val loss: 0.51395\n",
      "Main effects training epoch: 150, train loss: 0.51358, val loss: 0.51523\n",
      "Main effects training epoch: 151, train loss: 0.51360, val loss: 0.51338\n",
      "Main effects training epoch: 152, train loss: 0.51326, val loss: 0.51516\n",
      "Main effects training epoch: 153, train loss: 0.51280, val loss: 0.51338\n",
      "Main effects training epoch: 154, train loss: 0.51290, val loss: 0.51489\n",
      "Main effects training epoch: 155, train loss: 0.51368, val loss: 0.51395\n",
      "Main effects training epoch: 156, train loss: 0.51327, val loss: 0.51352\n",
      "Main effects training epoch: 157, train loss: 0.51371, val loss: 0.51697\n",
      "Main effects training epoch: 158, train loss: 0.51303, val loss: 0.51330\n",
      "Main effects training epoch: 159, train loss: 0.51282, val loss: 0.51414\n",
      "Main effects training epoch: 160, train loss: 0.51264, val loss: 0.51284\n",
      "Main effects training epoch: 161, train loss: 0.51254, val loss: 0.51502\n",
      "Main effects training epoch: 162, train loss: 0.51248, val loss: 0.51346\n",
      "Main effects training epoch: 163, train loss: 0.51300, val loss: 0.51320\n",
      "Main effects training epoch: 164, train loss: 0.51299, val loss: 0.51754\n",
      "Main effects training epoch: 165, train loss: 0.51284, val loss: 0.51186\n",
      "Main effects training epoch: 166, train loss: 0.51227, val loss: 0.51541\n",
      "Main effects training epoch: 167, train loss: 0.51248, val loss: 0.51372\n",
      "Main effects training epoch: 168, train loss: 0.51293, val loss: 0.51411\n",
      "Main effects training epoch: 169, train loss: 0.51279, val loss: 0.51343\n",
      "Main effects training epoch: 170, train loss: 0.51254, val loss: 0.51642\n",
      "Main effects training epoch: 171, train loss: 0.51219, val loss: 0.51126\n",
      "Main effects training epoch: 172, train loss: 0.51226, val loss: 0.51607\n",
      "Main effects training epoch: 173, train loss: 0.51229, val loss: 0.51358\n",
      "Main effects training epoch: 174, train loss: 0.51218, val loss: 0.51278\n",
      "Main effects training epoch: 175, train loss: 0.51214, val loss: 0.51480\n",
      "Main effects training epoch: 176, train loss: 0.51163, val loss: 0.51460\n",
      "Main effects training epoch: 177, train loss: 0.51172, val loss: 0.51431\n",
      "Main effects training epoch: 178, train loss: 0.51219, val loss: 0.51239\n",
      "Main effects training epoch: 179, train loss: 0.51159, val loss: 0.51530\n",
      "Main effects training epoch: 180, train loss: 0.51166, val loss: 0.51422\n",
      "Main effects training epoch: 181, train loss: 0.51124, val loss: 0.51367\n",
      "Main effects training epoch: 182, train loss: 0.51129, val loss: 0.51325\n",
      "Main effects training epoch: 183, train loss: 0.51115, val loss: 0.51429\n",
      "Main effects training epoch: 184, train loss: 0.51116, val loss: 0.51363\n",
      "Main effects training epoch: 185, train loss: 0.51140, val loss: 0.51465\n",
      "Main effects training epoch: 186, train loss: 0.51148, val loss: 0.51272\n",
      "Main effects training epoch: 187, train loss: 0.51158, val loss: 0.51544\n",
      "Main effects training epoch: 188, train loss: 0.51119, val loss: 0.51274\n",
      "Main effects training epoch: 189, train loss: 0.51130, val loss: 0.51422\n",
      "Main effects training epoch: 190, train loss: 0.51139, val loss: 0.51539\n",
      "Main effects training epoch: 191, train loss: 0.51156, val loss: 0.51399\n",
      "Main effects training epoch: 192, train loss: 0.51073, val loss: 0.51401\n",
      "Main effects training epoch: 193, train loss: 0.51100, val loss: 0.51489\n",
      "Main effects training epoch: 194, train loss: 0.51071, val loss: 0.51352\n",
      "Main effects training epoch: 195, train loss: 0.51060, val loss: 0.51367\n",
      "Main effects training epoch: 196, train loss: 0.51109, val loss: 0.51353\n",
      "Main effects training epoch: 197, train loss: 0.51063, val loss: 0.51335\n",
      "Main effects training epoch: 198, train loss: 0.51042, val loss: 0.51367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 199, train loss: 0.51085, val loss: 0.51465\n",
      "Main effects training epoch: 200, train loss: 0.51037, val loss: 0.51524\n",
      "Main effects training epoch: 201, train loss: 0.51067, val loss: 0.51175\n",
      "Main effects training epoch: 202, train loss: 0.51051, val loss: 0.51448\n",
      "Main effects training epoch: 203, train loss: 0.51012, val loss: 0.51439\n",
      "Main effects training epoch: 204, train loss: 0.51016, val loss: 0.51242\n",
      "Main effects training epoch: 205, train loss: 0.51012, val loss: 0.51319\n",
      "Main effects training epoch: 206, train loss: 0.51022, val loss: 0.51544\n",
      "Main effects training epoch: 207, train loss: 0.51060, val loss: 0.51370\n",
      "Main effects training epoch: 208, train loss: 0.51029, val loss: 0.51317\n",
      "Main effects training epoch: 209, train loss: 0.51017, val loss: 0.51410\n",
      "Main effects training epoch: 210, train loss: 0.51018, val loss: 0.51505\n",
      "Main effects training epoch: 211, train loss: 0.50999, val loss: 0.51351\n",
      "Main effects training epoch: 212, train loss: 0.50992, val loss: 0.51467\n",
      "Main effects training epoch: 213, train loss: 0.50984, val loss: 0.51463\n",
      "Main effects training epoch: 214, train loss: 0.50968, val loss: 0.51368\n",
      "Main effects training epoch: 215, train loss: 0.50983, val loss: 0.51394\n",
      "Main effects training epoch: 216, train loss: 0.51039, val loss: 0.51308\n",
      "Main effects training epoch: 217, train loss: 0.51010, val loss: 0.51631\n",
      "Main effects training epoch: 218, train loss: 0.51095, val loss: 0.51147\n",
      "Main effects training epoch: 219, train loss: 0.50965, val loss: 0.51411\n",
      "Main effects training epoch: 220, train loss: 0.50989, val loss: 0.51501\n",
      "Main effects training epoch: 221, train loss: 0.50975, val loss: 0.51408\n",
      "Main effects training epoch: 222, train loss: 0.50983, val loss: 0.51362\n",
      "Main effects training epoch: 223, train loss: 0.50968, val loss: 0.51384\n",
      "Main effects training epoch: 224, train loss: 0.51037, val loss: 0.51468\n",
      "Main effects training epoch: 225, train loss: 0.51028, val loss: 0.51462\n",
      "Main effects training epoch: 226, train loss: 0.51083, val loss: 0.51384\n",
      "Main effects training epoch: 227, train loss: 0.51039, val loss: 0.51481\n",
      "Main effects training epoch: 228, train loss: 0.50970, val loss: 0.51575\n",
      "Main effects training epoch: 229, train loss: 0.51002, val loss: 0.51225\n",
      "Main effects training epoch: 230, train loss: 0.51010, val loss: 0.51554\n",
      "Main effects training epoch: 231, train loss: 0.50979, val loss: 0.51419\n",
      "Main effects training epoch: 232, train loss: 0.50974, val loss: 0.51528\n",
      "Main effects training epoch: 233, train loss: 0.50965, val loss: 0.51231\n",
      "Main effects training epoch: 234, train loss: 0.50941, val loss: 0.51409\n",
      "Main effects training epoch: 235, train loss: 0.50948, val loss: 0.51417\n",
      "Main effects training epoch: 236, train loss: 0.50927, val loss: 0.51448\n",
      "Main effects training epoch: 237, train loss: 0.50954, val loss: 0.51547\n",
      "Main effects training epoch: 238, train loss: 0.51000, val loss: 0.51332\n",
      "Main effects training epoch: 239, train loss: 0.50985, val loss: 0.51514\n",
      "Main effects training epoch: 240, train loss: 0.51047, val loss: 0.51386\n",
      "Main effects training epoch: 241, train loss: 0.50998, val loss: 0.51563\n",
      "Main effects training epoch: 242, train loss: 0.50996, val loss: 0.51453\n",
      "Main effects training epoch: 243, train loss: 0.50961, val loss: 0.51364\n",
      "Main effects training epoch: 244, train loss: 0.50943, val loss: 0.51561\n",
      "Main effects training epoch: 245, train loss: 0.50949, val loss: 0.51657\n",
      "Main effects training epoch: 246, train loss: 0.51035, val loss: 0.51105\n",
      "Main effects training epoch: 247, train loss: 0.51000, val loss: 0.51811\n",
      "Main effects training epoch: 248, train loss: 0.50958, val loss: 0.51298\n",
      "Main effects training epoch: 249, train loss: 0.50942, val loss: 0.51345\n",
      "Main effects training epoch: 250, train loss: 0.50940, val loss: 0.51473\n",
      "Main effects training epoch: 251, train loss: 0.50922, val loss: 0.51423\n",
      "Main effects training epoch: 252, train loss: 0.50924, val loss: 0.51386\n",
      "Main effects training epoch: 253, train loss: 0.50934, val loss: 0.51487\n",
      "Main effects training epoch: 254, train loss: 0.50958, val loss: 0.51433\n",
      "Main effects training epoch: 255, train loss: 0.50930, val loss: 0.51572\n",
      "Main effects training epoch: 256, train loss: 0.50945, val loss: 0.51290\n",
      "Main effects training epoch: 257, train loss: 0.50939, val loss: 0.51439\n",
      "Main effects training epoch: 258, train loss: 0.50901, val loss: 0.51445\n",
      "Main effects training epoch: 259, train loss: 0.50923, val loss: 0.51347\n",
      "Main effects training epoch: 260, train loss: 0.50966, val loss: 0.51433\n",
      "Main effects training epoch: 261, train loss: 0.50955, val loss: 0.51561\n",
      "Main effects training epoch: 262, train loss: 0.50893, val loss: 0.51365\n",
      "Main effects training epoch: 263, train loss: 0.50903, val loss: 0.51468\n",
      "Main effects training epoch: 264, train loss: 0.50949, val loss: 0.51336\n",
      "Main effects training epoch: 265, train loss: 0.50922, val loss: 0.51650\n",
      "Main effects training epoch: 266, train loss: 0.50910, val loss: 0.51348\n",
      "Main effects training epoch: 267, train loss: 0.50974, val loss: 0.51393\n",
      "Main effects training epoch: 268, train loss: 0.50938, val loss: 0.51379\n",
      "Main effects training epoch: 269, train loss: 0.50929, val loss: 0.51686\n",
      "Main effects training epoch: 270, train loss: 0.50894, val loss: 0.51406\n",
      "Main effects training epoch: 271, train loss: 0.50893, val loss: 0.51356\n",
      "Main effects training epoch: 272, train loss: 0.50897, val loss: 0.51552\n",
      "Main effects training epoch: 273, train loss: 0.50896, val loss: 0.51224\n",
      "Main effects training epoch: 274, train loss: 0.50923, val loss: 0.51672\n",
      "Main effects training epoch: 275, train loss: 0.50874, val loss: 0.51423\n",
      "Main effects training epoch: 276, train loss: 0.50881, val loss: 0.51410\n",
      "Main effects training epoch: 277, train loss: 0.50885, val loss: 0.51518\n",
      "Main effects training epoch: 278, train loss: 0.50939, val loss: 0.51355\n",
      "Main effects training epoch: 279, train loss: 0.50924, val loss: 0.51556\n",
      "Main effects training epoch: 280, train loss: 0.50905, val loss: 0.51571\n",
      "Main effects training epoch: 281, train loss: 0.50889, val loss: 0.51223\n",
      "Main effects training epoch: 282, train loss: 0.50890, val loss: 0.51594\n",
      "Main effects training epoch: 283, train loss: 0.50902, val loss: 0.51486\n",
      "Main effects training epoch: 284, train loss: 0.50849, val loss: 0.51448\n",
      "Main effects training epoch: 285, train loss: 0.50857, val loss: 0.51441\n",
      "Main effects training epoch: 286, train loss: 0.50863, val loss: 0.51361\n",
      "Main effects training epoch: 287, train loss: 0.50875, val loss: 0.51545\n",
      "Main effects training epoch: 288, train loss: 0.50853, val loss: 0.51439\n",
      "Main effects training epoch: 289, train loss: 0.50866, val loss: 0.51351\n",
      "Main effects training epoch: 290, train loss: 0.50867, val loss: 0.51647\n",
      "Main effects training epoch: 291, train loss: 0.50920, val loss: 0.51414\n",
      "Main effects training epoch: 292, train loss: 0.51004, val loss: 0.51475\n",
      "Main effects training epoch: 293, train loss: 0.51037, val loss: 0.51548\n",
      "Main effects training epoch: 294, train loss: 0.50895, val loss: 0.51559\n",
      "Main effects training epoch: 295, train loss: 0.50856, val loss: 0.51408\n",
      "Main effects training epoch: 296, train loss: 0.50855, val loss: 0.51458\n",
      "Main effects training epoch: 297, train loss: 0.50831, val loss: 0.51442\n",
      "Main effects training epoch: 298, train loss: 0.50841, val loss: 0.51369\n",
      "Main effects training epoch: 299, train loss: 0.50946, val loss: 0.51831\n",
      "Main effects training epoch: 300, train loss: 0.50924, val loss: 0.51183\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51062, val loss: 0.51408\n",
      "Main effects tuning epoch: 2, train loss: 0.51096, val loss: 0.51437\n",
      "Main effects tuning epoch: 3, train loss: 0.51176, val loss: 0.51268\n",
      "Main effects tuning epoch: 4, train loss: 0.51069, val loss: 0.51501\n",
      "Main effects tuning epoch: 5, train loss: 0.51075, val loss: 0.51352\n",
      "Main effects tuning epoch: 6, train loss: 0.51066, val loss: 0.51261\n",
      "Main effects tuning epoch: 7, train loss: 0.51058, val loss: 0.51476\n",
      "Main effects tuning epoch: 8, train loss: 0.51053, val loss: 0.51304\n",
      "Main effects tuning epoch: 9, train loss: 0.51057, val loss: 0.51501\n",
      "Main effects tuning epoch: 10, train loss: 0.51042, val loss: 0.51276\n",
      "Main effects tuning epoch: 11, train loss: 0.51048, val loss: 0.51386\n",
      "Main effects tuning epoch: 12, train loss: 0.51062, val loss: 0.51234\n",
      "Main effects tuning epoch: 13, train loss: 0.51072, val loss: 0.51415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects tuning epoch: 14, train loss: 0.51065, val loss: 0.51442\n",
      "Main effects tuning epoch: 15, train loss: 0.51056, val loss: 0.51440\n",
      "Main effects tuning epoch: 16, train loss: 0.51050, val loss: 0.51300\n",
      "Main effects tuning epoch: 17, train loss: 0.51055, val loss: 0.51318\n",
      "Main effects tuning epoch: 18, train loss: 0.51059, val loss: 0.51377\n",
      "Main effects tuning epoch: 19, train loss: 0.51071, val loss: 0.51353\n",
      "Main effects tuning epoch: 20, train loss: 0.51034, val loss: 0.51381\n",
      "Main effects tuning epoch: 21, train loss: 0.51045, val loss: 0.51344\n",
      "Main effects tuning epoch: 22, train loss: 0.51049, val loss: 0.51336\n",
      "Main effects tuning epoch: 23, train loss: 0.51042, val loss: 0.51354\n",
      "Main effects tuning epoch: 24, train loss: 0.51052, val loss: 0.51359\n",
      "Main effects tuning epoch: 25, train loss: 0.51041, val loss: 0.51442\n",
      "Main effects tuning epoch: 26, train loss: 0.51066, val loss: 0.51279\n",
      "Main effects tuning epoch: 27, train loss: 0.51059, val loss: 0.51316\n",
      "Main effects tuning epoch: 28, train loss: 0.51056, val loss: 0.51461\n",
      "Main effects tuning epoch: 29, train loss: 0.51046, val loss: 0.51291\n",
      "Main effects tuning epoch: 30, train loss: 0.51044, val loss: 0.51423\n",
      "Main effects tuning epoch: 31, train loss: 0.51032, val loss: 0.51273\n",
      "Main effects tuning epoch: 32, train loss: 0.51080, val loss: 0.51290\n",
      "Main effects tuning epoch: 33, train loss: 0.51059, val loss: 0.51371\n",
      "Main effects tuning epoch: 34, train loss: 0.51037, val loss: 0.51226\n",
      "Main effects tuning epoch: 35, train loss: 0.51097, val loss: 0.51469\n",
      "Main effects tuning epoch: 36, train loss: 0.51047, val loss: 0.51401\n",
      "Main effects tuning epoch: 37, train loss: 0.51058, val loss: 0.51296\n",
      "Main effects tuning epoch: 38, train loss: 0.51095, val loss: 0.51520\n",
      "Main effects tuning epoch: 39, train loss: 0.51040, val loss: 0.51289\n",
      "Main effects tuning epoch: 40, train loss: 0.51071, val loss: 0.51428\n",
      "Main effects tuning epoch: 41, train loss: 0.51030, val loss: 0.51292\n",
      "Main effects tuning epoch: 42, train loss: 0.51027, val loss: 0.51338\n",
      "Main effects tuning epoch: 43, train loss: 0.51030, val loss: 0.51242\n",
      "Main effects tuning epoch: 44, train loss: 0.51048, val loss: 0.51435\n",
      "Main effects tuning epoch: 45, train loss: 0.51044, val loss: 0.51341\n",
      "Main effects tuning epoch: 46, train loss: 0.51105, val loss: 0.51331\n",
      "Main effects tuning epoch: 47, train loss: 0.51129, val loss: 0.51583\n",
      "Main effects tuning epoch: 48, train loss: 0.51092, val loss: 0.51274\n",
      "Main effects tuning epoch: 49, train loss: 0.51072, val loss: 0.51279\n",
      "Main effects tuning epoch: 50, train loss: 0.51072, val loss: 0.51330\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.46832, val loss: 0.47243\n",
      "Interaction training epoch: 2, train loss: 0.33709, val loss: 0.31216\n",
      "Interaction training epoch: 3, train loss: 0.30771, val loss: 0.28999\n",
      "Interaction training epoch: 4, train loss: 0.31194, val loss: 0.28831\n",
      "Interaction training epoch: 5, train loss: 0.31055, val loss: 0.28928\n",
      "Interaction training epoch: 6, train loss: 0.30771, val loss: 0.28784\n",
      "Interaction training epoch: 7, train loss: 0.29762, val loss: 0.27560\n",
      "Interaction training epoch: 8, train loss: 0.28805, val loss: 0.27074\n",
      "Interaction training epoch: 9, train loss: 0.29398, val loss: 0.27376\n",
      "Interaction training epoch: 10, train loss: 0.29557, val loss: 0.27523\n",
      "Interaction training epoch: 11, train loss: 0.28574, val loss: 0.26791\n",
      "Interaction training epoch: 12, train loss: 0.28828, val loss: 0.26505\n",
      "Interaction training epoch: 13, train loss: 0.28032, val loss: 0.26390\n",
      "Interaction training epoch: 14, train loss: 0.28340, val loss: 0.26799\n",
      "Interaction training epoch: 15, train loss: 0.28264, val loss: 0.26307\n",
      "Interaction training epoch: 16, train loss: 0.27977, val loss: 0.26210\n",
      "Interaction training epoch: 17, train loss: 0.27300, val loss: 0.25454\n",
      "Interaction training epoch: 18, train loss: 0.26716, val loss: 0.25098\n",
      "Interaction training epoch: 19, train loss: 0.26475, val loss: 0.25264\n",
      "Interaction training epoch: 20, train loss: 0.26293, val loss: 0.24730\n",
      "Interaction training epoch: 21, train loss: 0.26187, val loss: 0.24839\n",
      "Interaction training epoch: 22, train loss: 0.25997, val loss: 0.25177\n",
      "Interaction training epoch: 23, train loss: 0.26048, val loss: 0.24647\n",
      "Interaction training epoch: 24, train loss: 0.26283, val loss: 0.25016\n",
      "Interaction training epoch: 25, train loss: 0.26231, val loss: 0.24614\n",
      "Interaction training epoch: 26, train loss: 0.26077, val loss: 0.24816\n",
      "Interaction training epoch: 27, train loss: 0.25782, val loss: 0.24390\n",
      "Interaction training epoch: 28, train loss: 0.25622, val loss: 0.24567\n",
      "Interaction training epoch: 29, train loss: 0.26155, val loss: 0.24605\n",
      "Interaction training epoch: 30, train loss: 0.25861, val loss: 0.24861\n",
      "Interaction training epoch: 31, train loss: 0.25930, val loss: 0.24339\n",
      "Interaction training epoch: 32, train loss: 0.25857, val loss: 0.24863\n",
      "Interaction training epoch: 33, train loss: 0.25676, val loss: 0.24556\n",
      "Interaction training epoch: 34, train loss: 0.25799, val loss: 0.24565\n",
      "Interaction training epoch: 35, train loss: 0.25428, val loss: 0.24130\n",
      "Interaction training epoch: 36, train loss: 0.25443, val loss: 0.24075\n",
      "Interaction training epoch: 37, train loss: 0.25562, val loss: 0.24400\n",
      "Interaction training epoch: 38, train loss: 0.25540, val loss: 0.24342\n",
      "Interaction training epoch: 39, train loss: 0.25284, val loss: 0.24101\n",
      "Interaction training epoch: 40, train loss: 0.25215, val loss: 0.23999\n",
      "Interaction training epoch: 41, train loss: 0.25286, val loss: 0.24161\n",
      "Interaction training epoch: 42, train loss: 0.25824, val loss: 0.24684\n",
      "Interaction training epoch: 43, train loss: 0.25346, val loss: 0.24206\n",
      "Interaction training epoch: 44, train loss: 0.25304, val loss: 0.23943\n",
      "Interaction training epoch: 45, train loss: 0.25282, val loss: 0.24208\n",
      "Interaction training epoch: 46, train loss: 0.25468, val loss: 0.24391\n",
      "Interaction training epoch: 47, train loss: 0.25546, val loss: 0.24948\n",
      "Interaction training epoch: 48, train loss: 0.25411, val loss: 0.24719\n",
      "Interaction training epoch: 49, train loss: 0.25187, val loss: 0.24019\n",
      "Interaction training epoch: 50, train loss: 0.25463, val loss: 0.24104\n",
      "Interaction training epoch: 51, train loss: 0.25496, val loss: 0.24530\n",
      "Interaction training epoch: 52, train loss: 0.25257, val loss: 0.23919\n",
      "Interaction training epoch: 53, train loss: 0.25330, val loss: 0.24539\n",
      "Interaction training epoch: 54, train loss: 0.25034, val loss: 0.24146\n",
      "Interaction training epoch: 55, train loss: 0.25040, val loss: 0.24008\n",
      "Interaction training epoch: 56, train loss: 0.24888, val loss: 0.24016\n",
      "Interaction training epoch: 57, train loss: 0.25235, val loss: 0.24298\n",
      "Interaction training epoch: 58, train loss: 0.25328, val loss: 0.24179\n",
      "Interaction training epoch: 59, train loss: 0.24950, val loss: 0.24143\n",
      "Interaction training epoch: 60, train loss: 0.25143, val loss: 0.24124\n",
      "Interaction training epoch: 61, train loss: 0.24834, val loss: 0.24194\n",
      "Interaction training epoch: 62, train loss: 0.24907, val loss: 0.24090\n",
      "Interaction training epoch: 63, train loss: 0.24962, val loss: 0.24344\n",
      "Interaction training epoch: 64, train loss: 0.25051, val loss: 0.24193\n",
      "Interaction training epoch: 65, train loss: 0.24819, val loss: 0.24070\n",
      "Interaction training epoch: 66, train loss: 0.24755, val loss: 0.23882\n",
      "Interaction training epoch: 67, train loss: 0.24932, val loss: 0.24151\n",
      "Interaction training epoch: 68, train loss: 0.24818, val loss: 0.24259\n",
      "Interaction training epoch: 69, train loss: 0.24740, val loss: 0.24060\n",
      "Interaction training epoch: 70, train loss: 0.24721, val loss: 0.23988\n",
      "Interaction training epoch: 71, train loss: 0.25405, val loss: 0.24372\n",
      "Interaction training epoch: 72, train loss: 0.25098, val loss: 0.24241\n",
      "Interaction training epoch: 73, train loss: 0.24671, val loss: 0.23989\n",
      "Interaction training epoch: 74, train loss: 0.24947, val loss: 0.24066\n",
      "Interaction training epoch: 75, train loss: 0.24711, val loss: 0.23988\n",
      "Interaction training epoch: 76, train loss: 0.24911, val loss: 0.24332\n",
      "Interaction training epoch: 77, train loss: 0.24721, val loss: 0.23881\n",
      "Interaction training epoch: 78, train loss: 0.24536, val loss: 0.23802\n",
      "Interaction training epoch: 79, train loss: 0.24612, val loss: 0.24063\n",
      "Interaction training epoch: 80, train loss: 0.25172, val loss: 0.24216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 81, train loss: 0.25630, val loss: 0.24641\n",
      "Interaction training epoch: 82, train loss: 0.25188, val loss: 0.24453\n",
      "Interaction training epoch: 83, train loss: 0.24728, val loss: 0.23971\n",
      "Interaction training epoch: 84, train loss: 0.24733, val loss: 0.24343\n",
      "Interaction training epoch: 85, train loss: 0.24553, val loss: 0.24014\n",
      "Interaction training epoch: 86, train loss: 0.24431, val loss: 0.23855\n",
      "Interaction training epoch: 87, train loss: 0.25068, val loss: 0.24088\n",
      "Interaction training epoch: 88, train loss: 0.24620, val loss: 0.24118\n",
      "Interaction training epoch: 89, train loss: 0.24626, val loss: 0.24442\n",
      "Interaction training epoch: 90, train loss: 0.24407, val loss: 0.23966\n",
      "Interaction training epoch: 91, train loss: 0.24654, val loss: 0.23933\n",
      "Interaction training epoch: 92, train loss: 0.25018, val loss: 0.24914\n",
      "Interaction training epoch: 93, train loss: 0.24299, val loss: 0.23782\n",
      "Interaction training epoch: 94, train loss: 0.24190, val loss: 0.24017\n",
      "Interaction training epoch: 95, train loss: 0.24501, val loss: 0.23819\n",
      "Interaction training epoch: 96, train loss: 0.24340, val loss: 0.24079\n",
      "Interaction training epoch: 97, train loss: 0.24390, val loss: 0.24054\n",
      "Interaction training epoch: 98, train loss: 0.24418, val loss: 0.23703\n",
      "Interaction training epoch: 99, train loss: 0.24546, val loss: 0.24322\n",
      "Interaction training epoch: 100, train loss: 0.24505, val loss: 0.24392\n",
      "Interaction training epoch: 101, train loss: 0.24366, val loss: 0.24071\n",
      "Interaction training epoch: 102, train loss: 0.24217, val loss: 0.23836\n",
      "Interaction training epoch: 103, train loss: 0.24387, val loss: 0.23921\n",
      "Interaction training epoch: 104, train loss: 0.24249, val loss: 0.23796\n",
      "Interaction training epoch: 105, train loss: 0.24261, val loss: 0.23638\n",
      "Interaction training epoch: 106, train loss: 0.24432, val loss: 0.24128\n",
      "Interaction training epoch: 107, train loss: 0.24650, val loss: 0.24338\n",
      "Interaction training epoch: 108, train loss: 0.24727, val loss: 0.24721\n",
      "Interaction training epoch: 109, train loss: 0.24204, val loss: 0.23810\n",
      "Interaction training epoch: 110, train loss: 0.24764, val loss: 0.23988\n",
      "Interaction training epoch: 111, train loss: 0.24306, val loss: 0.23854\n",
      "Interaction training epoch: 112, train loss: 0.24405, val loss: 0.24251\n",
      "Interaction training epoch: 113, train loss: 0.24313, val loss: 0.24132\n",
      "Interaction training epoch: 114, train loss: 0.24341, val loss: 0.23749\n",
      "Interaction training epoch: 115, train loss: 0.24155, val loss: 0.23608\n",
      "Interaction training epoch: 116, train loss: 0.24406, val loss: 0.24504\n",
      "Interaction training epoch: 117, train loss: 0.24187, val loss: 0.23737\n",
      "Interaction training epoch: 118, train loss: 0.24068, val loss: 0.23641\n",
      "Interaction training epoch: 119, train loss: 0.24204, val loss: 0.23538\n",
      "Interaction training epoch: 120, train loss: 0.24171, val loss: 0.23720\n",
      "Interaction training epoch: 121, train loss: 0.24076, val loss: 0.23582\n",
      "Interaction training epoch: 122, train loss: 0.24122, val loss: 0.23666\n",
      "Interaction training epoch: 123, train loss: 0.24031, val loss: 0.23847\n",
      "Interaction training epoch: 124, train loss: 0.24011, val loss: 0.23699\n",
      "Interaction training epoch: 125, train loss: 0.23790, val loss: 0.23159\n",
      "Interaction training epoch: 126, train loss: 0.23986, val loss: 0.23857\n",
      "Interaction training epoch: 127, train loss: 0.23952, val loss: 0.23489\n",
      "Interaction training epoch: 128, train loss: 0.24072, val loss: 0.23588\n",
      "Interaction training epoch: 129, train loss: 0.23975, val loss: 0.23738\n",
      "Interaction training epoch: 130, train loss: 0.23857, val loss: 0.23686\n",
      "Interaction training epoch: 131, train loss: 0.24204, val loss: 0.23679\n",
      "Interaction training epoch: 132, train loss: 0.24558, val loss: 0.23761\n",
      "Interaction training epoch: 133, train loss: 0.25082, val loss: 0.25673\n",
      "Interaction training epoch: 134, train loss: 0.23770, val loss: 0.23240\n",
      "Interaction training epoch: 135, train loss: 0.23954, val loss: 0.23444\n",
      "Interaction training epoch: 136, train loss: 0.23864, val loss: 0.23499\n",
      "Interaction training epoch: 137, train loss: 0.23804, val loss: 0.23336\n",
      "Interaction training epoch: 138, train loss: 0.23634, val loss: 0.23176\n",
      "Interaction training epoch: 139, train loss: 0.23841, val loss: 0.23528\n",
      "Interaction training epoch: 140, train loss: 0.23856, val loss: 0.23276\n",
      "Interaction training epoch: 141, train loss: 0.24088, val loss: 0.24022\n",
      "Interaction training epoch: 142, train loss: 0.23595, val loss: 0.23314\n",
      "Interaction training epoch: 143, train loss: 0.23893, val loss: 0.23526\n",
      "Interaction training epoch: 144, train loss: 0.23930, val loss: 0.23460\n",
      "Interaction training epoch: 145, train loss: 0.23818, val loss: 0.23729\n",
      "Interaction training epoch: 146, train loss: 0.24252, val loss: 0.23728\n",
      "Interaction training epoch: 147, train loss: 0.23977, val loss: 0.23899\n",
      "Interaction training epoch: 148, train loss: 0.23607, val loss: 0.23418\n",
      "Interaction training epoch: 149, train loss: 0.23771, val loss: 0.23055\n",
      "Interaction training epoch: 150, train loss: 0.23612, val loss: 0.23540\n",
      "Interaction training epoch: 151, train loss: 0.23543, val loss: 0.23487\n",
      "Interaction training epoch: 152, train loss: 0.23580, val loss: 0.23426\n",
      "Interaction training epoch: 153, train loss: 0.23634, val loss: 0.22823\n",
      "Interaction training epoch: 154, train loss: 0.23552, val loss: 0.23373\n",
      "Interaction training epoch: 155, train loss: 0.23540, val loss: 0.23239\n",
      "Interaction training epoch: 156, train loss: 0.23526, val loss: 0.23318\n",
      "Interaction training epoch: 157, train loss: 0.23469, val loss: 0.23360\n",
      "Interaction training epoch: 158, train loss: 0.23613, val loss: 0.23694\n",
      "Interaction training epoch: 159, train loss: 0.23804, val loss: 0.23304\n",
      "Interaction training epoch: 160, train loss: 0.23491, val loss: 0.23162\n",
      "Interaction training epoch: 161, train loss: 0.23759, val loss: 0.23999\n",
      "Interaction training epoch: 162, train loss: 0.23386, val loss: 0.23126\n",
      "Interaction training epoch: 163, train loss: 0.23413, val loss: 0.23019\n",
      "Interaction training epoch: 164, train loss: 0.23549, val loss: 0.23492\n",
      "Interaction training epoch: 165, train loss: 0.23371, val loss: 0.23234\n",
      "Interaction training epoch: 166, train loss: 0.23707, val loss: 0.23472\n",
      "Interaction training epoch: 167, train loss: 0.23347, val loss: 0.23206\n",
      "Interaction training epoch: 168, train loss: 0.23322, val loss: 0.22814\n",
      "Interaction training epoch: 169, train loss: 0.23286, val loss: 0.23621\n",
      "Interaction training epoch: 170, train loss: 0.23558, val loss: 0.23186\n",
      "Interaction training epoch: 171, train loss: 0.23487, val loss: 0.23112\n",
      "Interaction training epoch: 172, train loss: 0.23408, val loss: 0.23466\n",
      "Interaction training epoch: 173, train loss: 0.23465, val loss: 0.23272\n",
      "Interaction training epoch: 174, train loss: 0.23204, val loss: 0.23019\n",
      "Interaction training epoch: 175, train loss: 0.23256, val loss: 0.23530\n",
      "Interaction training epoch: 176, train loss: 0.23360, val loss: 0.23251\n",
      "Interaction training epoch: 177, train loss: 0.23205, val loss: 0.23024\n",
      "Interaction training epoch: 178, train loss: 0.23111, val loss: 0.23293\n",
      "Interaction training epoch: 179, train loss: 0.23301, val loss: 0.23071\n",
      "Interaction training epoch: 180, train loss: 0.23506, val loss: 0.23297\n",
      "Interaction training epoch: 181, train loss: 0.23180, val loss: 0.23045\n",
      "Interaction training epoch: 182, train loss: 0.22949, val loss: 0.23037\n",
      "Interaction training epoch: 183, train loss: 0.23185, val loss: 0.22971\n",
      "Interaction training epoch: 184, train loss: 0.23053, val loss: 0.23222\n",
      "Interaction training epoch: 185, train loss: 0.22875, val loss: 0.22969\n",
      "Interaction training epoch: 186, train loss: 0.22942, val loss: 0.23062\n",
      "Interaction training epoch: 187, train loss: 0.23304, val loss: 0.23038\n",
      "Interaction training epoch: 188, train loss: 0.23428, val loss: 0.23643\n",
      "Interaction training epoch: 189, train loss: 0.23002, val loss: 0.23262\n",
      "Interaction training epoch: 190, train loss: 0.22947, val loss: 0.23385\n",
      "Interaction training epoch: 191, train loss: 0.22855, val loss: 0.23072\n",
      "Interaction training epoch: 192, train loss: 0.23080, val loss: 0.22959\n",
      "Interaction training epoch: 193, train loss: 0.23109, val loss: 0.23268\n",
      "Interaction training epoch: 194, train loss: 0.23118, val loss: 0.23298\n",
      "Interaction training epoch: 195, train loss: 0.22810, val loss: 0.22785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 196, train loss: 0.22952, val loss: 0.22918\n",
      "Interaction training epoch: 197, train loss: 0.23192, val loss: 0.23398\n",
      "Interaction training epoch: 198, train loss: 0.23277, val loss: 0.23469\n",
      "Interaction training epoch: 199, train loss: 0.22962, val loss: 0.22804\n",
      "Interaction training epoch: 200, train loss: 0.23031, val loss: 0.23269\n",
      "##########Stage 2: interaction training stop.##########\n",
      "##########2 interactions are pruned, start tuning.##########\n",
      "Interaction tuning epoch: 1, train loss: 0.23676, val loss: 0.23677\n",
      "Interaction tuning epoch: 2, train loss: 0.23532, val loss: 0.23635\n",
      "Interaction tuning epoch: 3, train loss: 0.23348, val loss: 0.23065\n",
      "Interaction tuning epoch: 4, train loss: 0.23372, val loss: 0.23439\n",
      "Interaction tuning epoch: 5, train loss: 0.23059, val loss: 0.23119\n",
      "Interaction tuning epoch: 6, train loss: 0.23331, val loss: 0.23262\n",
      "Interaction tuning epoch: 7, train loss: 0.23545, val loss: 0.23751\n",
      "Interaction tuning epoch: 8, train loss: 0.23142, val loss: 0.22870\n",
      "Interaction tuning epoch: 9, train loss: 0.23269, val loss: 0.23709\n",
      "Interaction tuning epoch: 10, train loss: 0.23316, val loss: 0.23142\n",
      "Interaction tuning epoch: 11, train loss: 0.23283, val loss: 0.23244\n",
      "Interaction tuning epoch: 12, train loss: 0.22980, val loss: 0.22946\n",
      "Interaction tuning epoch: 13, train loss: 0.23329, val loss: 0.23205\n",
      "Interaction tuning epoch: 14, train loss: 0.23041, val loss: 0.23260\n",
      "Interaction tuning epoch: 15, train loss: 0.23048, val loss: 0.23046\n",
      "Interaction tuning epoch: 16, train loss: 0.23238, val loss: 0.23358\n",
      "Interaction tuning epoch: 17, train loss: 0.23038, val loss: 0.23575\n",
      "Interaction tuning epoch: 18, train loss: 0.23156, val loss: 0.23178\n",
      "Interaction tuning epoch: 19, train loss: 0.23113, val loss: 0.23545\n",
      "Interaction tuning epoch: 20, train loss: 0.23031, val loss: 0.23022\n",
      "Interaction tuning epoch: 21, train loss: 0.23150, val loss: 0.23207\n",
      "Interaction tuning epoch: 22, train loss: 0.23150, val loss: 0.23278\n",
      "Interaction tuning epoch: 23, train loss: 0.23060, val loss: 0.23380\n",
      "Interaction tuning epoch: 24, train loss: 0.23011, val loss: 0.23452\n",
      "Interaction tuning epoch: 25, train loss: 0.23018, val loss: 0.23103\n",
      "Interaction tuning epoch: 26, train loss: 0.22857, val loss: 0.23341\n",
      "Interaction tuning epoch: 27, train loss: 0.22754, val loss: 0.22981\n",
      "Interaction tuning epoch: 28, train loss: 0.23056, val loss: 0.23064\n",
      "Interaction tuning epoch: 29, train loss: 0.22817, val loss: 0.22982\n",
      "Interaction tuning epoch: 30, train loss: 0.23206, val loss: 0.23950\n",
      "Interaction tuning epoch: 31, train loss: 0.23331, val loss: 0.23177\n",
      "Interaction tuning epoch: 32, train loss: 0.23452, val loss: 0.24026\n",
      "Interaction tuning epoch: 33, train loss: 0.22774, val loss: 0.22937\n",
      "Interaction tuning epoch: 34, train loss: 0.22949, val loss: 0.22957\n",
      "Interaction tuning epoch: 35, train loss: 0.23118, val loss: 0.23945\n",
      "Interaction tuning epoch: 36, train loss: 0.22682, val loss: 0.22774\n",
      "Interaction tuning epoch: 37, train loss: 0.22695, val loss: 0.23116\n",
      "Interaction tuning epoch: 38, train loss: 0.22815, val loss: 0.23047\n",
      "Interaction tuning epoch: 39, train loss: 0.23002, val loss: 0.23271\n",
      "Interaction tuning epoch: 40, train loss: 0.22841, val loss: 0.23042\n",
      "Interaction tuning epoch: 41, train loss: 0.22913, val loss: 0.22951\n",
      "Interaction tuning epoch: 42, train loss: 0.22982, val loss: 0.23319\n",
      "Interaction tuning epoch: 43, train loss: 0.22808, val loss: 0.23316\n",
      "Interaction tuning epoch: 44, train loss: 0.22991, val loss: 0.23116\n",
      "Interaction tuning epoch: 45, train loss: 0.22848, val loss: 0.23125\n",
      "Interaction tuning epoch: 46, train loss: 0.22642, val loss: 0.23151\n",
      "Interaction tuning epoch: 47, train loss: 0.22907, val loss: 0.23132\n",
      "Interaction tuning epoch: 48, train loss: 0.22732, val loss: 0.22820\n",
      "Interaction tuning epoch: 49, train loss: 0.22978, val loss: 0.23482\n",
      "Interaction tuning epoch: 50, train loss: 0.22598, val loss: 0.22805\n",
      "####################GAMI-Net training finished.####################\n",
      "time cost: 76.48421049118042\n",
      "After the gam stage, training error is 0.22598 , validation error is 0.22805\n",
      "missing value counts: 99283\n",
      "[SoftImpute] Max Singular Value of X_init = 3.567053\n",
      "#####mf_training#####\n",
      "[SoftImpute] Iter 1: observed BCE=0.206532 validation BCE=0.227845,rank=3\n",
      "[SoftImpute] Iter 1: observed BCE=0.225120 validation BCE=0.228117,rank=3\n",
      "[SoftImpute] Iter 2: observed BCE=0.225172 validation BCE=0.227985,rank=3\n",
      "[SoftImpute] Iter 3: observed BCE=0.225134 validation BCE=0.227997,rank=3\n",
      "[SoftImpute] Iter 4: observed BCE=0.225096 validation BCE=0.228041,rank=3\n",
      "[SoftImpute] Iter 5: observed BCE=0.225100 validation BCE=0.228249,rank=3\n",
      "[SoftImpute] Iter 6: observed BCE=0.225080 validation BCE=0.228275,rank=3\n",
      "[SoftImpute] Iter 7: observed BCE=0.225060 validation BCE=0.228295,rank=3\n",
      "[SoftImpute] Iter 8: observed BCE=0.225044 validation BCE=0.228329,rank=3\n",
      "[SoftImpute] Iter 9: observed BCE=0.225040 validation BCE=0.228345,rank=3\n",
      "[SoftImpute] Iter 10: observed BCE=0.225091 validation BCE=0.228258,rank=3\n",
      "[SoftImpute] Iter 11: observed BCE=0.225084 validation BCE=0.228270,rank=3\n",
      "[SoftImpute] Iter 12: observed BCE=0.225133 validation BCE=0.228236,rank=3\n",
      "[SoftImpute] Iter 13: observed BCE=0.225204 validation BCE=0.228249,rank=3\n",
      "[SoftImpute] Iter 14: observed BCE=0.225212 validation BCE=0.228234,rank=3\n",
      "[SoftImpute] Iter 15: observed BCE=0.225217 validation BCE=0.228231,rank=3\n",
      "[SoftImpute] Iter 16: observed BCE=0.225221 validation BCE=0.228230,rank=3\n",
      "[SoftImpute] Iter 17: observed BCE=0.225225 validation BCE=0.228229,rank=3\n",
      "[SoftImpute] Iter 18: observed BCE=0.225228 validation BCE=0.228228,rank=3\n",
      "[SoftImpute] Iter 19: observed BCE=0.225231 validation BCE=0.228228,rank=3\n",
      "[SoftImpute] Iter 20: observed BCE=0.225234 validation BCE=0.228227,rank=3\n",
      "[SoftImpute] Iter 21: observed BCE=0.225237 validation BCE=0.228227,rank=3\n",
      "[SoftImpute] Iter 22: observed BCE=0.225239 validation BCE=0.228226,rank=3\n",
      "[SoftImpute] Iter 23: observed BCE=0.225241 validation BCE=0.228226,rank=3\n",
      "[SoftImpute] Iter 24: observed BCE=0.225243 validation BCE=0.228225,rank=3\n",
      "[SoftImpute] Iter 25: observed BCE=0.225245 validation BCE=0.228225,rank=3\n",
      "[SoftImpute] Iter 26: observed BCE=0.225247 validation BCE=0.228224,rank=3\n",
      "[SoftImpute] Iter 27: observed BCE=0.225248 validation BCE=0.228224,rank=3\n",
      "[SoftImpute] Iter 28: observed BCE=0.225250 validation BCE=0.228224,rank=3\n",
      "[SoftImpute] Iter 29: observed BCE=0.225251 validation BCE=0.228223,rank=3\n",
      "[SoftImpute] Iter 30: observed BCE=0.225252 validation BCE=0.228223,rank=3\n",
      "[SoftImpute] Iter 31: observed BCE=0.225253 validation BCE=0.228223,rank=3\n",
      "[SoftImpute] Iter 32: observed BCE=0.225254 validation BCE=0.228223,rank=3\n",
      "[SoftImpute] Iter 33: observed BCE=0.225254 validation BCE=0.228222,rank=3\n",
      "[SoftImpute] Iter 34: observed BCE=0.225255 validation BCE=0.228222,rank=3\n",
      "[SoftImpute] Iter 35: observed BCE=0.225256 validation BCE=0.228222,rank=3\n",
      "[SoftImpute] Iter 36: observed BCE=0.225256 validation BCE=0.228222,rank=3\n",
      "[SoftImpute] Iter 37: observed BCE=0.225257 validation BCE=0.228222,rank=3\n",
      "[SoftImpute] Stopped after iteration 37 for lambda=0.071341\n",
      "final num of user group: 24\n",
      "final num of item group: 24\n",
      "change mode state : True\n",
      "time cost: 12.487675666809082\n",
      "After the matrix factor stage, training error is 0.22526, validation error is 0.22822\n",
      "2\n",
      "ListWrapper(['uf_1', 'uf_2', 'uf_3', 'uf_4', 'uf_5', 'if_1', 'if_2', 'if_3', 'if_4', 'if_5'])\n",
      "####################GAMI-Net training start.####################\n",
      "##########Stage 1: main effect training start.##########\n",
      "Main effects training epoch: 1, train loss: 0.68311, val loss: 0.68560\n",
      "Main effects training epoch: 2, train loss: 0.67682, val loss: 0.68195\n",
      "Main effects training epoch: 3, train loss: 0.67320, val loss: 0.68057\n",
      "Main effects training epoch: 4, train loss: 0.66746, val loss: 0.67428\n",
      "Main effects training epoch: 5, train loss: 0.65814, val loss: 0.66481\n",
      "Main effects training epoch: 6, train loss: 0.64028, val loss: 0.64834\n",
      "Main effects training epoch: 7, train loss: 0.61254, val loss: 0.62333\n",
      "Main effects training epoch: 8, train loss: 0.58255, val loss: 0.59867\n",
      "Main effects training epoch: 9, train loss: 0.54471, val loss: 0.56311\n",
      "Main effects training epoch: 10, train loss: 0.52227, val loss: 0.54338\n",
      "Main effects training epoch: 11, train loss: 0.52252, val loss: 0.54596\n",
      "Main effects training epoch: 12, train loss: 0.51924, val loss: 0.54187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 13, train loss: 0.52061, val loss: 0.54417\n",
      "Main effects training epoch: 14, train loss: 0.51796, val loss: 0.53943\n",
      "Main effects training epoch: 15, train loss: 0.51842, val loss: 0.53930\n",
      "Main effects training epoch: 16, train loss: 0.51856, val loss: 0.54151\n",
      "Main effects training epoch: 17, train loss: 0.51848, val loss: 0.53983\n",
      "Main effects training epoch: 18, train loss: 0.51730, val loss: 0.54023\n",
      "Main effects training epoch: 19, train loss: 0.51717, val loss: 0.53923\n",
      "Main effects training epoch: 20, train loss: 0.51701, val loss: 0.53927\n",
      "Main effects training epoch: 21, train loss: 0.51703, val loss: 0.53943\n",
      "Main effects training epoch: 22, train loss: 0.51713, val loss: 0.53990\n",
      "Main effects training epoch: 23, train loss: 0.51745, val loss: 0.53987\n",
      "Main effects training epoch: 24, train loss: 0.51718, val loss: 0.53959\n",
      "Main effects training epoch: 25, train loss: 0.51726, val loss: 0.53925\n",
      "Main effects training epoch: 26, train loss: 0.51679, val loss: 0.53882\n",
      "Main effects training epoch: 27, train loss: 0.51764, val loss: 0.54076\n",
      "Main effects training epoch: 28, train loss: 0.51701, val loss: 0.53895\n",
      "Main effects training epoch: 29, train loss: 0.51673, val loss: 0.53927\n",
      "Main effects training epoch: 30, train loss: 0.51689, val loss: 0.53981\n",
      "Main effects training epoch: 31, train loss: 0.51675, val loss: 0.53858\n",
      "Main effects training epoch: 32, train loss: 0.51667, val loss: 0.54014\n",
      "Main effects training epoch: 33, train loss: 0.51660, val loss: 0.53882\n",
      "Main effects training epoch: 34, train loss: 0.51658, val loss: 0.53929\n",
      "Main effects training epoch: 35, train loss: 0.51665, val loss: 0.53863\n",
      "Main effects training epoch: 36, train loss: 0.51645, val loss: 0.53955\n",
      "Main effects training epoch: 37, train loss: 0.51735, val loss: 0.53970\n",
      "Main effects training epoch: 38, train loss: 0.51681, val loss: 0.53939\n",
      "Main effects training epoch: 39, train loss: 0.51688, val loss: 0.53858\n",
      "Main effects training epoch: 40, train loss: 0.51632, val loss: 0.53915\n",
      "Main effects training epoch: 41, train loss: 0.51678, val loss: 0.53848\n",
      "Main effects training epoch: 42, train loss: 0.51667, val loss: 0.53856\n",
      "Main effects training epoch: 43, train loss: 0.51692, val loss: 0.53868\n",
      "Main effects training epoch: 44, train loss: 0.51636, val loss: 0.53977\n",
      "Main effects training epoch: 45, train loss: 0.51661, val loss: 0.53909\n",
      "Main effects training epoch: 46, train loss: 0.51812, val loss: 0.53956\n",
      "Main effects training epoch: 47, train loss: 0.51966, val loss: 0.54333\n",
      "Main effects training epoch: 48, train loss: 0.51938, val loss: 0.53992\n",
      "Main effects training epoch: 49, train loss: 0.51728, val loss: 0.53982\n",
      "Main effects training epoch: 50, train loss: 0.51670, val loss: 0.53840\n",
      "Main effects training epoch: 51, train loss: 0.51616, val loss: 0.53856\n",
      "Main effects training epoch: 52, train loss: 0.51652, val loss: 0.54001\n",
      "Main effects training epoch: 53, train loss: 0.51700, val loss: 0.53831\n",
      "Main effects training epoch: 54, train loss: 0.51738, val loss: 0.53951\n",
      "Main effects training epoch: 55, train loss: 0.51655, val loss: 0.53902\n",
      "Main effects training epoch: 56, train loss: 0.51643, val loss: 0.53793\n",
      "Main effects training epoch: 57, train loss: 0.51603, val loss: 0.53924\n",
      "Main effects training epoch: 58, train loss: 0.51569, val loss: 0.53821\n",
      "Main effects training epoch: 59, train loss: 0.51571, val loss: 0.53774\n",
      "Main effects training epoch: 60, train loss: 0.51578, val loss: 0.53731\n",
      "Main effects training epoch: 61, train loss: 0.51579, val loss: 0.53881\n",
      "Main effects training epoch: 62, train loss: 0.51588, val loss: 0.53827\n",
      "Main effects training epoch: 63, train loss: 0.51671, val loss: 0.53983\n",
      "Main effects training epoch: 64, train loss: 0.51629, val loss: 0.53829\n",
      "Main effects training epoch: 65, train loss: 0.51573, val loss: 0.53856\n",
      "Main effects training epoch: 66, train loss: 0.51618, val loss: 0.53780\n",
      "Main effects training epoch: 67, train loss: 0.51637, val loss: 0.53842\n",
      "Main effects training epoch: 68, train loss: 0.51559, val loss: 0.53706\n",
      "Main effects training epoch: 69, train loss: 0.51574, val loss: 0.53889\n",
      "Main effects training epoch: 70, train loss: 0.51590, val loss: 0.53747\n",
      "Main effects training epoch: 71, train loss: 0.51550, val loss: 0.53808\n",
      "Main effects training epoch: 72, train loss: 0.51564, val loss: 0.53763\n",
      "Main effects training epoch: 73, train loss: 0.51552, val loss: 0.53880\n",
      "Main effects training epoch: 74, train loss: 0.51551, val loss: 0.53733\n",
      "Main effects training epoch: 75, train loss: 0.51550, val loss: 0.53744\n",
      "Main effects training epoch: 76, train loss: 0.51504, val loss: 0.53723\n",
      "Main effects training epoch: 77, train loss: 0.51504, val loss: 0.53728\n",
      "Main effects training epoch: 78, train loss: 0.51525, val loss: 0.53729\n",
      "Main effects training epoch: 79, train loss: 0.51518, val loss: 0.53824\n",
      "Main effects training epoch: 80, train loss: 0.51502, val loss: 0.53795\n",
      "Main effects training epoch: 81, train loss: 0.51512, val loss: 0.53729\n",
      "Main effects training epoch: 82, train loss: 0.51502, val loss: 0.53739\n",
      "Main effects training epoch: 83, train loss: 0.51523, val loss: 0.53723\n",
      "Main effects training epoch: 84, train loss: 0.51504, val loss: 0.53684\n",
      "Main effects training epoch: 85, train loss: 0.51488, val loss: 0.53720\n",
      "Main effects training epoch: 86, train loss: 0.51479, val loss: 0.53689\n",
      "Main effects training epoch: 87, train loss: 0.51533, val loss: 0.53845\n",
      "Main effects training epoch: 88, train loss: 0.51581, val loss: 0.53859\n",
      "Main effects training epoch: 89, train loss: 0.51571, val loss: 0.53814\n",
      "Main effects training epoch: 90, train loss: 0.51513, val loss: 0.53739\n",
      "Main effects training epoch: 91, train loss: 0.51449, val loss: 0.53567\n",
      "Main effects training epoch: 92, train loss: 0.51485, val loss: 0.53706\n",
      "Main effects training epoch: 93, train loss: 0.51473, val loss: 0.53802\n",
      "Main effects training epoch: 94, train loss: 0.51458, val loss: 0.53612\n",
      "Main effects training epoch: 95, train loss: 0.51475, val loss: 0.53768\n",
      "Main effects training epoch: 96, train loss: 0.51509, val loss: 0.53647\n",
      "Main effects training epoch: 97, train loss: 0.51476, val loss: 0.53627\n",
      "Main effects training epoch: 98, train loss: 0.51491, val loss: 0.53877\n",
      "Main effects training epoch: 99, train loss: 0.51509, val loss: 0.53576\n",
      "Main effects training epoch: 100, train loss: 0.51434, val loss: 0.53709\n",
      "Main effects training epoch: 101, train loss: 0.51418, val loss: 0.53635\n",
      "Main effects training epoch: 102, train loss: 0.51417, val loss: 0.53507\n",
      "Main effects training epoch: 103, train loss: 0.51438, val loss: 0.53828\n",
      "Main effects training epoch: 104, train loss: 0.51452, val loss: 0.53546\n",
      "Main effects training epoch: 105, train loss: 0.51398, val loss: 0.53668\n",
      "Main effects training epoch: 106, train loss: 0.51421, val loss: 0.53614\n",
      "Main effects training epoch: 107, train loss: 0.51396, val loss: 0.53628\n",
      "Main effects training epoch: 108, train loss: 0.51401, val loss: 0.53592\n",
      "Main effects training epoch: 109, train loss: 0.51366, val loss: 0.53550\n",
      "Main effects training epoch: 110, train loss: 0.51359, val loss: 0.53594\n",
      "Main effects training epoch: 111, train loss: 0.51366, val loss: 0.53599\n",
      "Main effects training epoch: 112, train loss: 0.51374, val loss: 0.53533\n",
      "Main effects training epoch: 113, train loss: 0.51354, val loss: 0.53551\n",
      "Main effects training epoch: 114, train loss: 0.51368, val loss: 0.53666\n",
      "Main effects training epoch: 115, train loss: 0.51338, val loss: 0.53524\n",
      "Main effects training epoch: 116, train loss: 0.51335, val loss: 0.53441\n",
      "Main effects training epoch: 117, train loss: 0.51322, val loss: 0.53525\n",
      "Main effects training epoch: 118, train loss: 0.51307, val loss: 0.53428\n",
      "Main effects training epoch: 119, train loss: 0.51302, val loss: 0.53476\n",
      "Main effects training epoch: 120, train loss: 0.51309, val loss: 0.53509\n",
      "Main effects training epoch: 121, train loss: 0.51269, val loss: 0.53493\n",
      "Main effects training epoch: 122, train loss: 0.51273, val loss: 0.53271\n",
      "Main effects training epoch: 123, train loss: 0.51270, val loss: 0.53531\n",
      "Main effects training epoch: 124, train loss: 0.51238, val loss: 0.53262\n",
      "Main effects training epoch: 125, train loss: 0.51214, val loss: 0.53374\n",
      "Main effects training epoch: 126, train loss: 0.51236, val loss: 0.53320\n",
      "Main effects training epoch: 127, train loss: 0.51212, val loss: 0.53229\n",
      "Main effects training epoch: 128, train loss: 0.51187, val loss: 0.53285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 129, train loss: 0.51188, val loss: 0.53275\n",
      "Main effects training epoch: 130, train loss: 0.51180, val loss: 0.53139\n",
      "Main effects training epoch: 131, train loss: 0.51189, val loss: 0.53190\n",
      "Main effects training epoch: 132, train loss: 0.51222, val loss: 0.53156\n",
      "Main effects training epoch: 133, train loss: 0.51156, val loss: 0.53241\n",
      "Main effects training epoch: 134, train loss: 0.51152, val loss: 0.53253\n",
      "Main effects training epoch: 135, train loss: 0.51153, val loss: 0.53206\n",
      "Main effects training epoch: 136, train loss: 0.51159, val loss: 0.53000\n",
      "Main effects training epoch: 137, train loss: 0.51134, val loss: 0.53165\n",
      "Main effects training epoch: 138, train loss: 0.51104, val loss: 0.53104\n",
      "Main effects training epoch: 139, train loss: 0.51119, val loss: 0.53055\n",
      "Main effects training epoch: 140, train loss: 0.51122, val loss: 0.53006\n",
      "Main effects training epoch: 141, train loss: 0.51103, val loss: 0.53053\n",
      "Main effects training epoch: 142, train loss: 0.51097, val loss: 0.53182\n",
      "Main effects training epoch: 143, train loss: 0.51110, val loss: 0.52953\n",
      "Main effects training epoch: 144, train loss: 0.51080, val loss: 0.53072\n",
      "Main effects training epoch: 145, train loss: 0.51069, val loss: 0.52912\n",
      "Main effects training epoch: 146, train loss: 0.51102, val loss: 0.53002\n",
      "Main effects training epoch: 147, train loss: 0.51108, val loss: 0.53069\n",
      "Main effects training epoch: 148, train loss: 0.51076, val loss: 0.53008\n",
      "Main effects training epoch: 149, train loss: 0.51063, val loss: 0.53007\n",
      "Main effects training epoch: 150, train loss: 0.51079, val loss: 0.52965\n",
      "Main effects training epoch: 151, train loss: 0.51076, val loss: 0.52944\n",
      "Main effects training epoch: 152, train loss: 0.51062, val loss: 0.53086\n",
      "Main effects training epoch: 153, train loss: 0.51088, val loss: 0.53003\n",
      "Main effects training epoch: 154, train loss: 0.51096, val loss: 0.53036\n",
      "Main effects training epoch: 155, train loss: 0.51069, val loss: 0.52860\n",
      "Main effects training epoch: 156, train loss: 0.51052, val loss: 0.52979\n",
      "Main effects training epoch: 157, train loss: 0.51033, val loss: 0.52940\n",
      "Main effects training epoch: 158, train loss: 0.51044, val loss: 0.52995\n",
      "Main effects training epoch: 159, train loss: 0.51033, val loss: 0.52980\n",
      "Main effects training epoch: 160, train loss: 0.51027, val loss: 0.52780\n",
      "Main effects training epoch: 161, train loss: 0.51016, val loss: 0.52924\n",
      "Main effects training epoch: 162, train loss: 0.51031, val loss: 0.52830\n",
      "Main effects training epoch: 163, train loss: 0.51047, val loss: 0.53024\n",
      "Main effects training epoch: 164, train loss: 0.51006, val loss: 0.52997\n",
      "Main effects training epoch: 165, train loss: 0.51002, val loss: 0.52890\n",
      "Main effects training epoch: 166, train loss: 0.51014, val loss: 0.52799\n",
      "Main effects training epoch: 167, train loss: 0.51039, val loss: 0.52805\n",
      "Main effects training epoch: 168, train loss: 0.50998, val loss: 0.52984\n",
      "Main effects training epoch: 169, train loss: 0.51015, val loss: 0.52758\n",
      "Main effects training epoch: 170, train loss: 0.51003, val loss: 0.52916\n",
      "Main effects training epoch: 171, train loss: 0.50992, val loss: 0.52793\n",
      "Main effects training epoch: 172, train loss: 0.51038, val loss: 0.52949\n",
      "Main effects training epoch: 173, train loss: 0.51107, val loss: 0.52795\n",
      "Main effects training epoch: 174, train loss: 0.51110, val loss: 0.53205\n",
      "Main effects training epoch: 175, train loss: 0.51002, val loss: 0.52687\n",
      "Main effects training epoch: 176, train loss: 0.51001, val loss: 0.52963\n",
      "Main effects training epoch: 177, train loss: 0.50987, val loss: 0.52699\n",
      "Main effects training epoch: 178, train loss: 0.50995, val loss: 0.52842\n",
      "Main effects training epoch: 179, train loss: 0.51024, val loss: 0.52899\n",
      "Main effects training epoch: 180, train loss: 0.51025, val loss: 0.52722\n",
      "Main effects training epoch: 181, train loss: 0.50957, val loss: 0.52810\n",
      "Main effects training epoch: 182, train loss: 0.50977, val loss: 0.52904\n",
      "Main effects training epoch: 183, train loss: 0.50980, val loss: 0.52733\n",
      "Main effects training epoch: 184, train loss: 0.50936, val loss: 0.52817\n",
      "Main effects training epoch: 185, train loss: 0.50956, val loss: 0.52663\n",
      "Main effects training epoch: 186, train loss: 0.50939, val loss: 0.52788\n",
      "Main effects training epoch: 187, train loss: 0.50945, val loss: 0.52748\n",
      "Main effects training epoch: 188, train loss: 0.50952, val loss: 0.52692\n",
      "Main effects training epoch: 189, train loss: 0.50941, val loss: 0.52895\n",
      "Main effects training epoch: 190, train loss: 0.50953, val loss: 0.52699\n",
      "Main effects training epoch: 191, train loss: 0.50918, val loss: 0.52799\n",
      "Main effects training epoch: 192, train loss: 0.50927, val loss: 0.52769\n",
      "Main effects training epoch: 193, train loss: 0.50930, val loss: 0.52778\n",
      "Main effects training epoch: 194, train loss: 0.50917, val loss: 0.52723\n",
      "Main effects training epoch: 195, train loss: 0.50920, val loss: 0.52755\n",
      "Main effects training epoch: 196, train loss: 0.50899, val loss: 0.52646\n",
      "Main effects training epoch: 197, train loss: 0.50926, val loss: 0.52642\n",
      "Main effects training epoch: 198, train loss: 0.50957, val loss: 0.52849\n",
      "Main effects training epoch: 199, train loss: 0.50966, val loss: 0.52626\n",
      "Main effects training epoch: 200, train loss: 0.50937, val loss: 0.52753\n",
      "Main effects training epoch: 201, train loss: 0.50917, val loss: 0.52773\n",
      "Main effects training epoch: 202, train loss: 0.50988, val loss: 0.52624\n",
      "Main effects training epoch: 203, train loss: 0.50932, val loss: 0.52785\n",
      "Main effects training epoch: 204, train loss: 0.50919, val loss: 0.52624\n",
      "Main effects training epoch: 205, train loss: 0.50885, val loss: 0.52689\n",
      "Main effects training epoch: 206, train loss: 0.50892, val loss: 0.52636\n",
      "Main effects training epoch: 207, train loss: 0.50889, val loss: 0.52776\n",
      "Main effects training epoch: 208, train loss: 0.50988, val loss: 0.52681\n",
      "Main effects training epoch: 209, train loss: 0.50992, val loss: 0.52861\n",
      "Main effects training epoch: 210, train loss: 0.50972, val loss: 0.52659\n",
      "Main effects training epoch: 211, train loss: 0.50994, val loss: 0.52913\n",
      "Main effects training epoch: 212, train loss: 0.51079, val loss: 0.52664\n",
      "Main effects training epoch: 213, train loss: 0.51031, val loss: 0.52898\n",
      "Main effects training epoch: 214, train loss: 0.51124, val loss: 0.52722\n",
      "Main effects training epoch: 215, train loss: 0.50973, val loss: 0.52781\n",
      "Main effects training epoch: 216, train loss: 0.50937, val loss: 0.52546\n",
      "Main effects training epoch: 217, train loss: 0.50888, val loss: 0.52734\n",
      "Main effects training epoch: 218, train loss: 0.50882, val loss: 0.52716\n",
      "Main effects training epoch: 219, train loss: 0.50862, val loss: 0.52576\n",
      "Main effects training epoch: 220, train loss: 0.50849, val loss: 0.52629\n",
      "Main effects training epoch: 221, train loss: 0.50853, val loss: 0.52704\n",
      "Main effects training epoch: 222, train loss: 0.50844, val loss: 0.52662\n",
      "Main effects training epoch: 223, train loss: 0.50852, val loss: 0.52566\n",
      "Main effects training epoch: 224, train loss: 0.50845, val loss: 0.52672\n",
      "Main effects training epoch: 225, train loss: 0.50855, val loss: 0.52576\n",
      "Main effects training epoch: 226, train loss: 0.50892, val loss: 0.52683\n",
      "Main effects training epoch: 227, train loss: 0.50823, val loss: 0.52606\n",
      "Main effects training epoch: 228, train loss: 0.50831, val loss: 0.52622\n",
      "Main effects training epoch: 229, train loss: 0.50852, val loss: 0.52555\n",
      "Main effects training epoch: 230, train loss: 0.50829, val loss: 0.52628\n",
      "Main effects training epoch: 231, train loss: 0.50857, val loss: 0.52581\n",
      "Main effects training epoch: 232, train loss: 0.50839, val loss: 0.52666\n",
      "Main effects training epoch: 233, train loss: 0.50835, val loss: 0.52543\n",
      "Main effects training epoch: 234, train loss: 0.50976, val loss: 0.52664\n",
      "Main effects training epoch: 235, train loss: 0.50842, val loss: 0.52612\n",
      "Main effects training epoch: 236, train loss: 0.50796, val loss: 0.52613\n",
      "Main effects training epoch: 237, train loss: 0.50863, val loss: 0.52427\n",
      "Main effects training epoch: 238, train loss: 0.50841, val loss: 0.52677\n",
      "Main effects training epoch: 239, train loss: 0.50814, val loss: 0.52600\n",
      "Main effects training epoch: 240, train loss: 0.50831, val loss: 0.52735\n",
      "Main effects training epoch: 241, train loss: 0.50811, val loss: 0.52569\n",
      "Main effects training epoch: 242, train loss: 0.50861, val loss: 0.52493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects training epoch: 243, train loss: 0.50817, val loss: 0.52657\n",
      "Main effects training epoch: 244, train loss: 0.50827, val loss: 0.52585\n",
      "Main effects training epoch: 245, train loss: 0.50912, val loss: 0.52791\n",
      "Main effects training epoch: 246, train loss: 0.50815, val loss: 0.52502\n",
      "Main effects training epoch: 247, train loss: 0.50798, val loss: 0.52625\n",
      "Main effects training epoch: 248, train loss: 0.50838, val loss: 0.52561\n",
      "Main effects training epoch: 249, train loss: 0.50789, val loss: 0.52431\n",
      "Main effects training epoch: 250, train loss: 0.50812, val loss: 0.52711\n",
      "Main effects training epoch: 251, train loss: 0.50788, val loss: 0.52544\n",
      "Main effects training epoch: 252, train loss: 0.50765, val loss: 0.52525\n",
      "Main effects training epoch: 253, train loss: 0.50776, val loss: 0.52598\n",
      "Main effects training epoch: 254, train loss: 0.50768, val loss: 0.52443\n",
      "Main effects training epoch: 255, train loss: 0.50740, val loss: 0.52468\n",
      "Main effects training epoch: 256, train loss: 0.50764, val loss: 0.52627\n",
      "Main effects training epoch: 257, train loss: 0.50761, val loss: 0.52546\n",
      "Main effects training epoch: 258, train loss: 0.50771, val loss: 0.52533\n",
      "Main effects training epoch: 259, train loss: 0.50764, val loss: 0.52499\n",
      "Main effects training epoch: 260, train loss: 0.50758, val loss: 0.52517\n",
      "Main effects training epoch: 261, train loss: 0.50763, val loss: 0.52441\n",
      "Main effects training epoch: 262, train loss: 0.50758, val loss: 0.52527\n",
      "Main effects training epoch: 263, train loss: 0.50761, val loss: 0.52616\n",
      "Main effects training epoch: 264, train loss: 0.50765, val loss: 0.52483\n",
      "Main effects training epoch: 265, train loss: 0.50786, val loss: 0.52559\n",
      "Main effects training epoch: 266, train loss: 0.50743, val loss: 0.52538\n",
      "Main effects training epoch: 267, train loss: 0.50751, val loss: 0.52425\n",
      "Main effects training epoch: 268, train loss: 0.50766, val loss: 0.52626\n",
      "Main effects training epoch: 269, train loss: 0.50721, val loss: 0.52492\n",
      "Main effects training epoch: 270, train loss: 0.50732, val loss: 0.52381\n",
      "Main effects training epoch: 271, train loss: 0.50728, val loss: 0.52528\n",
      "Main effects training epoch: 272, train loss: 0.50717, val loss: 0.52439\n",
      "Main effects training epoch: 273, train loss: 0.50713, val loss: 0.52437\n",
      "Main effects training epoch: 274, train loss: 0.50707, val loss: 0.52482\n",
      "Main effects training epoch: 275, train loss: 0.50701, val loss: 0.52344\n",
      "Main effects training epoch: 276, train loss: 0.50709, val loss: 0.52563\n",
      "Main effects training epoch: 277, train loss: 0.50748, val loss: 0.52513\n",
      "Main effects training epoch: 278, train loss: 0.50737, val loss: 0.52531\n",
      "Main effects training epoch: 279, train loss: 0.50767, val loss: 0.52449\n",
      "Main effects training epoch: 280, train loss: 0.50791, val loss: 0.52479\n",
      "Main effects training epoch: 281, train loss: 0.50714, val loss: 0.52542\n",
      "Main effects training epoch: 282, train loss: 0.50715, val loss: 0.52412\n",
      "Main effects training epoch: 283, train loss: 0.50709, val loss: 0.52485\n",
      "Main effects training epoch: 284, train loss: 0.50716, val loss: 0.52502\n",
      "Main effects training epoch: 285, train loss: 0.50782, val loss: 0.52512\n",
      "Main effects training epoch: 286, train loss: 0.50725, val loss: 0.52460\n",
      "Main effects training epoch: 287, train loss: 0.50744, val loss: 0.52578\n",
      "Main effects training epoch: 288, train loss: 0.50746, val loss: 0.52317\n",
      "Main effects training epoch: 289, train loss: 0.50763, val loss: 0.52676\n",
      "Main effects training epoch: 290, train loss: 0.50714, val loss: 0.52407\n",
      "Main effects training epoch: 291, train loss: 0.50706, val loss: 0.52601\n",
      "Main effects training epoch: 292, train loss: 0.50678, val loss: 0.52304\n",
      "Main effects training epoch: 293, train loss: 0.50654, val loss: 0.52391\n",
      "Main effects training epoch: 294, train loss: 0.50672, val loss: 0.52269\n",
      "Main effects training epoch: 295, train loss: 0.50689, val loss: 0.52615\n",
      "Main effects training epoch: 296, train loss: 0.50650, val loss: 0.52405\n",
      "Main effects training epoch: 297, train loss: 0.50683, val loss: 0.52384\n",
      "Main effects training epoch: 298, train loss: 0.50687, val loss: 0.52474\n",
      "Main effects training epoch: 299, train loss: 0.50666, val loss: 0.52323\n",
      "Main effects training epoch: 300, train loss: 0.50632, val loss: 0.52371\n",
      "##########Stage 1: main effect training stop.##########\n",
      "7 main effects are pruned, start tuning.##########\n",
      "Main effects tuning epoch: 1, train loss: 0.51032, val loss: 0.52772\n",
      "Main effects tuning epoch: 2, train loss: 0.51003, val loss: 0.52736\n",
      "Main effects tuning epoch: 3, train loss: 0.51001, val loss: 0.52727\n",
      "Main effects tuning epoch: 4, train loss: 0.51021, val loss: 0.52748\n",
      "Main effects tuning epoch: 5, train loss: 0.51076, val loss: 0.52728\n",
      "Main effects tuning epoch: 6, train loss: 0.51025, val loss: 0.52876\n",
      "Main effects tuning epoch: 7, train loss: 0.51021, val loss: 0.52665\n",
      "Main effects tuning epoch: 8, train loss: 0.50984, val loss: 0.52740\n",
      "Main effects tuning epoch: 9, train loss: 0.50980, val loss: 0.52743\n",
      "Main effects tuning epoch: 10, train loss: 0.50975, val loss: 0.52702\n",
      "Main effects tuning epoch: 11, train loss: 0.50996, val loss: 0.52754\n",
      "Main effects tuning epoch: 12, train loss: 0.51004, val loss: 0.52790\n",
      "Main effects tuning epoch: 13, train loss: 0.50988, val loss: 0.52615\n",
      "Main effects tuning epoch: 14, train loss: 0.50955, val loss: 0.52635\n",
      "Main effects tuning epoch: 15, train loss: 0.50996, val loss: 0.52876\n",
      "Main effects tuning epoch: 16, train loss: 0.51011, val loss: 0.52583\n",
      "Main effects tuning epoch: 17, train loss: 0.50963, val loss: 0.52848\n",
      "Main effects tuning epoch: 18, train loss: 0.51019, val loss: 0.52686\n",
      "Main effects tuning epoch: 19, train loss: 0.51027, val loss: 0.52654\n",
      "Main effects tuning epoch: 20, train loss: 0.50977, val loss: 0.52747\n",
      "Main effects tuning epoch: 21, train loss: 0.50983, val loss: 0.52693\n",
      "Main effects tuning epoch: 22, train loss: 0.50965, val loss: 0.52837\n",
      "Main effects tuning epoch: 23, train loss: 0.50964, val loss: 0.52570\n",
      "Main effects tuning epoch: 24, train loss: 0.50925, val loss: 0.52668\n",
      "Main effects tuning epoch: 25, train loss: 0.50946, val loss: 0.52629\n",
      "Main effects tuning epoch: 26, train loss: 0.50969, val loss: 0.52858\n",
      "Main effects tuning epoch: 27, train loss: 0.50889, val loss: 0.52648\n",
      "Main effects tuning epoch: 28, train loss: 0.50903, val loss: 0.52709\n",
      "Main effects tuning epoch: 29, train loss: 0.50906, val loss: 0.52573\n",
      "Main effects tuning epoch: 30, train loss: 0.50929, val loss: 0.52776\n",
      "Main effects tuning epoch: 31, train loss: 0.50960, val loss: 0.52498\n",
      "Main effects tuning epoch: 32, train loss: 0.50944, val loss: 0.52881\n",
      "Main effects tuning epoch: 33, train loss: 0.50920, val loss: 0.52536\n",
      "Main effects tuning epoch: 34, train loss: 0.50882, val loss: 0.52593\n",
      "Main effects tuning epoch: 35, train loss: 0.50922, val loss: 0.52756\n",
      "Main effects tuning epoch: 36, train loss: 0.50888, val loss: 0.52639\n",
      "Main effects tuning epoch: 37, train loss: 0.50859, val loss: 0.52662\n",
      "Main effects tuning epoch: 38, train loss: 0.50850, val loss: 0.52579\n",
      "Main effects tuning epoch: 39, train loss: 0.50855, val loss: 0.52702\n",
      "Main effects tuning epoch: 40, train loss: 0.50840, val loss: 0.52549\n",
      "Main effects tuning epoch: 41, train loss: 0.50851, val loss: 0.52625\n",
      "Main effects tuning epoch: 42, train loss: 0.50890, val loss: 0.52634\n",
      "Main effects tuning epoch: 43, train loss: 0.50872, val loss: 0.52671\n",
      "Main effects tuning epoch: 44, train loss: 0.50871, val loss: 0.52553\n",
      "Main effects tuning epoch: 45, train loss: 0.50862, val loss: 0.52733\n",
      "Main effects tuning epoch: 46, train loss: 0.50845, val loss: 0.52537\n",
      "Main effects tuning epoch: 47, train loss: 0.50866, val loss: 0.52727\n",
      "Main effects tuning epoch: 48, train loss: 0.50890, val loss: 0.52608\n",
      "Main effects tuning epoch: 49, train loss: 0.50923, val loss: 0.52639\n",
      "Main effects tuning epoch: 50, train loss: 0.50859, val loss: 0.52696\n",
      "##########Stage 2: interaction training start.##########\n",
      "Interaction training epoch: 1, train loss: 0.50873, val loss: 0.52470\n",
      "Interaction training epoch: 2, train loss: 0.51572, val loss: 0.53857\n",
      "Interaction training epoch: 3, train loss: 0.32832, val loss: 0.35041\n",
      "Interaction training epoch: 4, train loss: 0.31754, val loss: 0.34945\n",
      "Interaction training epoch: 5, train loss: 0.29991, val loss: 0.31902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction training epoch: 6, train loss: 0.29298, val loss: 0.31870\n",
      "Interaction training epoch: 7, train loss: 0.29322, val loss: 0.31305\n",
      "Interaction training epoch: 8, train loss: 0.28243, val loss: 0.30734\n",
      "Interaction training epoch: 9, train loss: 0.28556, val loss: 0.31156\n",
      "Interaction training epoch: 10, train loss: 0.27711, val loss: 0.29956\n",
      "Interaction training epoch: 11, train loss: 0.28555, val loss: 0.30675\n",
      "Interaction training epoch: 12, train loss: 0.28202, val loss: 0.30306\n",
      "Interaction training epoch: 13, train loss: 0.28012, val loss: 0.30921\n",
      "Interaction training epoch: 14, train loss: 0.27400, val loss: 0.30028\n",
      "Interaction training epoch: 15, train loss: 0.28211, val loss: 0.30644\n",
      "Interaction training epoch: 16, train loss: 0.27816, val loss: 0.30463\n",
      "Interaction training epoch: 17, train loss: 0.27371, val loss: 0.30457\n",
      "Interaction training epoch: 18, train loss: 0.27857, val loss: 0.30849\n",
      "Interaction training epoch: 19, train loss: 0.27286, val loss: 0.30503\n",
      "Interaction training epoch: 20, train loss: 0.27018, val loss: 0.30697\n",
      "Interaction training epoch: 21, train loss: 0.26970, val loss: 0.30171\n",
      "Interaction training epoch: 22, train loss: 0.27359, val loss: 0.30520\n",
      "Interaction training epoch: 23, train loss: 0.27451, val loss: 0.30982\n",
      "Interaction training epoch: 24, train loss: 0.26856, val loss: 0.30499\n",
      "Interaction training epoch: 25, train loss: 0.26955, val loss: 0.30586\n",
      "Interaction training epoch: 26, train loss: 0.26614, val loss: 0.29744\n",
      "Interaction training epoch: 27, train loss: 0.27349, val loss: 0.31143\n",
      "Interaction training epoch: 28, train loss: 0.26860, val loss: 0.30666\n",
      "Interaction training epoch: 29, train loss: 0.26889, val loss: 0.31056\n",
      "Interaction training epoch: 30, train loss: 0.27113, val loss: 0.30815\n",
      "Interaction training epoch: 31, train loss: 0.26442, val loss: 0.30518\n",
      "Interaction training epoch: 32, train loss: 0.26539, val loss: 0.30680\n",
      "Interaction training epoch: 33, train loss: 0.26584, val loss: 0.30548\n",
      "Interaction training epoch: 34, train loss: 0.26438, val loss: 0.30618\n",
      "Interaction training epoch: 35, train loss: 0.26340, val loss: 0.30612\n",
      "Interaction training epoch: 36, train loss: 0.26466, val loss: 0.30352\n",
      "Interaction training epoch: 37, train loss: 0.26500, val loss: 0.30798\n",
      "Interaction training epoch: 38, train loss: 0.26447, val loss: 0.30822\n",
      "Interaction training epoch: 39, train loss: 0.26335, val loss: 0.30367\n",
      "Interaction training epoch: 40, train loss: 0.26487, val loss: 0.31215\n",
      "Interaction training epoch: 41, train loss: 0.26075, val loss: 0.30140\n",
      "Interaction training epoch: 42, train loss: 0.26282, val loss: 0.30665\n",
      "Interaction training epoch: 43, train loss: 0.26109, val loss: 0.30676\n",
      "Interaction training epoch: 44, train loss: 0.25805, val loss: 0.30253\n",
      "Interaction training epoch: 45, train loss: 0.25890, val loss: 0.30440\n",
      "Interaction training epoch: 46, train loss: 0.25892, val loss: 0.30369\n",
      "Interaction training epoch: 47, train loss: 0.25930, val loss: 0.30510\n",
      "Interaction training epoch: 48, train loss: 0.25817, val loss: 0.30596\n",
      "Interaction training epoch: 49, train loss: 0.25673, val loss: 0.30287\n",
      "Interaction training epoch: 50, train loss: 0.25593, val loss: 0.30087\n",
      "Interaction training epoch: 51, train loss: 0.25586, val loss: 0.30297\n",
      "Interaction training epoch: 52, train loss: 0.25931, val loss: 0.30669\n",
      "Interaction training epoch: 53, train loss: 0.25987, val loss: 0.30747\n",
      "Interaction training epoch: 54, train loss: 0.25648, val loss: 0.29876\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a4bd4c180459>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[0mlii\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mauto_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlii\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'result/wc_test_class.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    223\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m                        copy=copy, sort=sort)\n\u001b[0m\u001b[0;32m    226\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[0mobjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m             \u001b[0mobjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-a4bd4c180459>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[0mlii\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mauto_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlii\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'result/wc_test_class.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-a4bd4c180459>\u001b[0m in \u001b[0;36mauto_test\u001b[1;34m(alpha)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mst_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtr_Xi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0med_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\索信达\\代码\\lvxnn_0526\\lvxnn\\LVXNN.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, xx, Xi, y)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0mst_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[0mval_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mval_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\索信达\\代码\\lvxnn_0526\\lvxnn\\gaminet.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, train_x, train_y)\u001b[0m\n\u001b[0;32m    748\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"#\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"Stage 2: interaction training start.\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"#\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_interaction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_interaction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"#\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"Stage 2: interaction training stop.\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"#\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\索信达\\代码\\lvxnn_0526\\lvxnn\\gaminet.py\u001b[0m in \u001b[0;36mfit_interaction\u001b[1;34m(self, tr_x, tr_y, val_x, val_y)\u001b[0m\n\u001b[0;32m    541\u001b[0m                 \u001b[0mbatch_xx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m                 \u001b[0mbatch_yy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 543\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_interaction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_xx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_yy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merr_train_interaction_training\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmain_effect_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minteraction_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score,log_loss\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gaminet.utils import local_visualize\n",
    "from gaminet.utils import global_visualize_density\n",
    "from gaminet.utils import feature_importance_visualize\n",
    "from gaminet.utils import plot_trajectory\n",
    "from gaminet.utils import plot_regularization\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from lvxnn.LVXNN import LV_XNN\n",
    "from lvxnn.DataReader import data_initialize\n",
    "\n",
    "\n",
    "data= pd.read_csv('data/sim_binary_0.9.csv')\n",
    "train , test = train_test_split(data,test_size=0.2)\n",
    "\n",
    "#list1 = data.columns\n",
    "meta_info = OrderedDict()\n",
    "\n",
    "meta_info['uf_1']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_2']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_3']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_4']={'type': 'continues','source':'user'}\n",
    "meta_info['uf_5']={'type': 'continues','source':'user'}\n",
    "meta_info['if_1']={'type': 'continues','source':'item'}\n",
    "meta_info['if_2']={'type': 'continues','source':'item'}\n",
    "meta_info['if_3']={'type': 'continues','source':'item'}\n",
    "meta_info['if_4']={'type': 'continues','source':'item'}\n",
    "meta_info['if_5']={'type': 'continues','source':'item'}\n",
    "meta_info['user_id']={\"type\":\"id\",'source':'user'}\n",
    "meta_info['item_id']={\"type\":\"id\",'source':'item'}\n",
    "meta_info['target']={\"type\":\"target\",'source':''}\n",
    "\n",
    "\n",
    "tr_x, tr_Xi, tr_y , te_x , te_Xi, te_y, meta_info, model_info = data_initialize(train,test,meta_info,\"Classification\")\n",
    "\n",
    "def auto_test(alpha):\n",
    "    cold_auc = []\n",
    "    cold_loss = []\n",
    "    warm_auc = []\n",
    "    warm_loss = []\n",
    "    \n",
    "\n",
    "    for times in range(10):\n",
    "        \n",
    "        print(times)\n",
    "\n",
    "\n",
    "        model = LV_XNN(model_info=model_info, meta_info=meta_info, subnet_arch=[8, 16],interact_arch=[20, 10],activation_func=tf.tanh, batch_size=1000, lr_bp=0.01, auto_tune=False,\n",
    "               interaction_epochs=200,main_effect_epochs=300,tuning_epochs=50,loss_threshold_main=0.01,loss_threshold_inter=0.01,alpha=1,\n",
    "              verbose=True,val_ratio=0.125, early_stop_thres=100,interact_num=10,u_group_num=30,i_group_num=50,scale_ratio=alpha,n_power_iterations=5,n_oversamples=0,\n",
    "              mf_training_iters=1,mf_tuning_iters=100,change_mode=True,convergence_threshold=0.001,max_rank=3,shrinkage_value=20,random_state=times)\n",
    "    \n",
    "        st_time = time.time()\n",
    "        model.fit(tr_x,tr_Xi, tr_y)\n",
    "        ed_time = time.time()\n",
    "        \n",
    "        pred = model.predict(te_x, te_Xi)\n",
    "        \n",
    "        cold_y = te_y[(te_Xi[:,1] == 'cold') | (te_Xi[:,0] == 'cold')]\n",
    "        cold_pred = pred[(te_Xi[:,1] == 'cold') | (te_Xi[:,0] == 'cold')]\n",
    "        warm_y = te_y[(te_Xi[:,1] != 'cold') & (te_Xi[:,0] != 'cold')]\n",
    "        warm_pred = pred[(te_Xi[:,1] != 'cold') & (te_Xi[:,0] != 'cold')]\n",
    "        \n",
    "        cold_auc.append(roc_auc_score(cold_y,cold_pred))\n",
    "        cold_loss.append(log_loss(cold_y,cold_pred))\n",
    "        warm_auc.append(roc_auc_score(te_y,pred))\n",
    "        warm_loss.append(log_loss(te_y,pred))\n",
    "        \n",
    "\n",
    "        \n",
    "    i_result = np.array([np.mean(cold_auc),np.mean(cold_loss),np.mean(warm_auc),np.mean(warm_loss)]).reshape(1,-1)\n",
    "    result = pd.DataFrame(i_result,columns=['cold_auc','cold_loss','warm_auc','warm_loss'])\n",
    "    \n",
    "    return result\n",
    "\n",
    "lii = [0,0.2,0.4,0.6,0.8,1]\n",
    "results = (auto_test(alpha) for alpha in lii)\n",
    "results = pd.concat(results)\n",
    "results.to_csv('result/wc_test_class.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
